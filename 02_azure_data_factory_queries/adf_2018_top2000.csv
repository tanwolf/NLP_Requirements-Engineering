QuestionId,QuestionTitle,QuestionBody,QuestionTags,Date,ViewCount,Score,NumberOfAnswers,AnswerId,AnswerBody
"59529880","Read API data from HTTPS webservice using Azure Data Factory","<p>I am trying to ingest data from Web services API, source is XML format as Azure Data Factory does not support XML format, I am reading it in binary format loading into Azure data lake gen2, Then converting into xml, csv using Azure Databricks.</p>

<p>Problem:
I am able to read the tokenid from API but not actual data means using token id i have to get the actual data.Please can anyone help on this.
Note: Used HTTP linked service to ingest the data.
All connectors are fine and test connections are successful.
Below is the error message i can see from ADF.</p>

<pre><code>""errorCode"": ""2200"",
""message"": ""Failure happened on 'Sink' side. ErrorCode=UserErrorFailedToReadHttpFile,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The remote server returned an error: (500) Internal Server Error.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Net.WebException,Message=The remote server returned an error: (500) Internal Server Error.,Source=System,'"",
""failureType"": ""UserError"",
""target"": ""Copy data1"",
""details"": []
</code></pre>

<p>}</p>

<p>My apologies if anyone do not understand the problem, I can post clearly.
Thanks,</p>
","<azure-data-lake><azure-data-factory>","2019-12-30 11:13:47","505","1","1","59560563","<p>Based on your description, i think you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-webhook-activity"" rel=""nofollow noreferrer"">WehHook Activity</a>,not Web Activity.</p>

<p>Using the WehHook activity, you can call an endpoint and pass a callback URL. The pipeline run waits for the callback to be invoked before proceeding to the next activity.</p>

<p>The actual step is passing an additional property “callBackUri”(Ingest Data API) in the body to the url endpoint(Get Token API), and will expect this uri to be invoked before the timeout value specified.</p>

<hr>

<p>Updates:</p>

<p>Finally,the OP chosen databricks as solution and it works fine.</p>
"
"59519005","Handling Multiple tables Using Data flow dynamically","<p>I tried to use <a href=""http://microsoft-bitools.blogspot.com/2019/05/azure-incremental-load-using-adf-data.html"" rel=""nofollow noreferrer"">this</a> article in successfully copying data from one table to another using Dataflows in Data factory. Now my scenario is to handle multiple tables in the DB. the above example is for one of the table.</p>

<p>I tried to follow the next article (<a href=""http://microsoft-bitools.blogspot.com/2019/06/staging-with-azure-data-factory-foreach.html"" rel=""nofollow noreferrer"">link</a>) in same series and have created View and For each loop but now wondering how should I put the input in Data Flow activity. </p>

<p>Any ideas or if any one tried the same thing.</p>

<p>Thanks</p>
","<azure><azure-data-factory><dataflow>","2019-12-29 11:32:53","1016","0","1","59529865","<p>You will need to use a parameterized dataset that uses a dataset parameter for the name of the table. Then, pass a string parameter from the Foreach activity that contains the table name into the dataset parameter for that data flow activity. This will all be accomplished from the pipeline.</p>
"
"59517614","Can't connect using windows authentication to azure cube over Azure SSIS /SSIS run time created in ADF","<p>I have SSIS package created locally which connects to azure cube. In order to connect to azure cube I am using windows authentication as shown below:</p>

<p><a href=""https://i.stack.imgur.com/K3DqJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K3DqJ.png"" alt=""enter image description here""></a></p>

<p>As shown above I am able to connect to Azure cube locally (i.e. on my desktop version of SSDT) via windows authentication. </p>

<p>Now I have created a azure SSIS runtime in ADF (Please note this azure subscription of ADF is different than that of the azure cube which i am connecting to). Now when i deploy the local (i.e. desktop version ) of SSIS package to the azure SSIS and try to run trigger the SSIS package via azure catlog in SSMS or through azure ADF i am getting the below connection failure message</p>

<p>1) Error which i get when i try to run the package on SSIS catlog over azure server
<a href=""https://i.stack.imgur.com/ylRiZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ylRiZ.png"" alt=""enter image description here""></a></p>

<p>2) Error which i get when i try to run the deployed SSIS package via ADf created.
<a href=""https://i.stack.imgur.com/PRuRY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PRuRY.png"" alt=""enter image description here""></a> </p>

<p>Error details</p>

<p>Output</p>

<p>{
    ""SsisDBOperationGuid"": ""ed2ca785-a942-434a-955a-9c1f0b5ed3b7"",
    ""SsisDBOperationId"": 26,
    ""OperationErrorMessages"": ""12/29/2019 7:23:47 AM +00:00 : Data Flow Task:Error: ADO NET Source has failed to acquire the connection {280FE230-2294-49D8-A947-D2EA8A7F29A2} with the following error message: \""COM error: COM error: mscorlib; Exception has been thrown by the target of an invocation..\"".\r\n\n12/29/2019 7:23:47 AM +00:00 : Data Flow Task:Error: ADO NET Source failed validation and returned error code 0xC0208449.\r\n\n12/29/2019 7:23:47 AM +00:00 : Data Flow Task:Error: One or more component failed validation.\r\n\n"",
    ""effectiveIntegrationRuntime"": ""SSSISruntime (West US 2)"",
    ""executionDuration"": 6,
    ""durationInQueue"": {
        ""integrationRuntimeQueue"": 1
    }
}</p>

<p><strong>Even though i am using windows authentication in ADF  connection which uses the same credentials by which visual studio package is running locally i am getting the above error.
Could you please let me know how can i run a ssis pakage package in ssis azure runtime by windows authention mode and can connect to the azure cube?
Any help would be greatly appreciated</strong></p>
","<azure><ssis><azure-active-directory><windows-authentication><azure-data-factory>","2019-12-29 07:34:38","387","0","1","62092519","<p><strong>You will need to refer this section:</strong></p>

<p>Use Windows authentication in staging tasks</p>

<p>If the staging tasks on your self-hosted IR require Windows authentication, configure your SSIS packages to use the same Windows authentication.</p>

<p>Your staging tasks will be invoked with the self-hosted IR service account (NT SERVICE\DIAHostService, by default), and your data stores will be accessed with the Windows authentication account. Both accounts require certain security policies to be assigned to them. On the self-hosted IR machine, go to Local Security Policy > Local Policies > User Rights Assignment, and then do the following:</p>

<p>Assign the Adjust memory quotas for a process and Replace a process level token policies to the self-hosted IR service account. This should occur automatically when you install your self-hosted IR with the default service account. If it doesn't, assign those policies manually. If you use a different service account, assign the same policies to it.</p>

<p>Assign the Log on as a service policy to the Windows Authentication account.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/self-hosted-integration-runtime-proxy-ssis"" rel=""nofollow noreferrer"">MSFT Document</a></p>
"
"59505781","GetMetadata to get the full file directory in Azure Data Factory","<p>I am working through a use case where I want to load all the folder names that were loaded into an Azure Database into a different ""control"" table, but am having problems with using GetMetadata activity properly. </p>

<p>The purpose of this use case would be to skip all of the older folders (that were already loaded) and only focus on the new folder and get the "".gz"" file and load it into an Azure Database. Oh a high level I thought I would use GetMetadata activity to send all of the folder names to a stored procedure. That stored procedure would then load those folder names with a status of '1' (meaning successful).</p>

<p>That table would then be used in a separate pipeline that is used to load files into a database. I would use a Lookup activity to compare against already loaded folders and if one of them don't match then that would be the folder to get the file from (the source is an S3 bucket).</p>

<p>The folder structure is nested in the YYYY/MM/DD format (ex: 2019/12/27 where each day a new folder is created and a ""gz"" file is placed there).</p>

<p>I created an ADF pipeline using the ""GetMetadata"" activity pointing to the blob storage that has already had the folders loaded into it.</p>

<p><a href=""https://i.stack.imgur.com/4EtqG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4EtqG.png"" alt=""enter image description here""></a></p>

<p>However, when I run this pipeline I only get the top three folder names: 2019, 2018, 2017.</p>

<p><a href=""https://i.stack.imgur.com/EwlB5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EwlB5.png"" alt=""enter image description here""></a></p>

<p>Is it possible to to not only get the top level folder name, but go down all the way down to day level? so instead of the output being ""2019"" it would be ""2019/12/26"" and then next one would be ""2019/12/27"" plus all of the months and days from 2017 and 2018. </p>

<p>If anyone faced this issue any insight would be greatly appreciated.</p>

<p>Thank you</p>
","<azure-data-factory>","2019-12-27 20:36:14","6924","3","2","59540694","<p>Based on the statements in the Get-Metadata Activity doc,<code>childItems</code> only returns elements from the specific path,won’t include items in subfolders.</p>

<p><a href=""https://i.stack.imgur.com/lJaHI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lJaHI.png"" alt=""enter image description here""></a></p>

<p>I supposed that you have to use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">ForEach Activity</a> to loop the <code>childItems</code> array layer by layer to flatten all structure. At the same time,use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-set-variable-activity"" rel=""nofollow noreferrer"">Set Variable Activity</a> to concat the complete folder path. Then use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-if-condition-activity"" rel=""nofollow noreferrer"">IfCondition Activity</a>,when you detect the element type is <code>file</code>,not <code>folder</code>,you could call the SP you mentioned in your question. </p>
"
"59505781","GetMetadata to get the full file directory in Azure Data Factory","<p>I am working through a use case where I want to load all the folder names that were loaded into an Azure Database into a different ""control"" table, but am having problems with using GetMetadata activity properly. </p>

<p>The purpose of this use case would be to skip all of the older folders (that were already loaded) and only focus on the new folder and get the "".gz"" file and load it into an Azure Database. Oh a high level I thought I would use GetMetadata activity to send all of the folder names to a stored procedure. That stored procedure would then load those folder names with a status of '1' (meaning successful).</p>

<p>That table would then be used in a separate pipeline that is used to load files into a database. I would use a Lookup activity to compare against already loaded folders and if one of them don't match then that would be the folder to get the file from (the source is an S3 bucket).</p>

<p>The folder structure is nested in the YYYY/MM/DD format (ex: 2019/12/27 where each day a new folder is created and a ""gz"" file is placed there).</p>

<p>I created an ADF pipeline using the ""GetMetadata"" activity pointing to the blob storage that has already had the folders loaded into it.</p>

<p><a href=""https://i.stack.imgur.com/4EtqG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4EtqG.png"" alt=""enter image description here""></a></p>

<p>However, when I run this pipeline I only get the top three folder names: 2019, 2018, 2017.</p>

<p><a href=""https://i.stack.imgur.com/EwlB5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EwlB5.png"" alt=""enter image description here""></a></p>

<p>Is it possible to to not only get the top level folder name, but go down all the way down to day level? so instead of the output being ""2019"" it would be ""2019/12/26"" and then next one would be ""2019/12/27"" plus all of the months and days from 2017 and 2018. </p>

<p>If anyone faced this issue any insight would be greatly appreciated.</p>

<p>Thank you</p>
","<azure-data-factory>","2019-12-27 20:36:14","6924","3","2","67404361","<p>you can also use a wildcard placeholder in this case, if you have a defined and nonchanging folder structure.</p>
<p>Use as directory: <em>storageroot</em> / * / * / * / <em>filename</em></p>
<p>For example I used csvFiles / * / * / * / * / * / * / *.csv
to get all files that have this structure:</p>
<p>csvFiles / topic / subtopic / country / year / month / day</p>
<p><a href=""https://i.stack.imgur.com/zUlX2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zUlX2.png"" alt=""example for wildcards in data source path"" /></a></p>
<p>Then you get <strong>all</strong> files in this folder structure.</p>
"
"59501736","how to pass in an expression through a parameter","<p>Suppose I have a <code>foreach</code> inside of a pipe:</p>

<p><a href=""https://i.stack.imgur.com/RSwqi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RSwqi.png"" alt=""enter image description here""></a></p>

<p>I'd like to iterate through the following:</p>

<p><code>@split(split(item().name,'_')[4],'-')[1]</code></p>

<p>However, I'd like to pass this formula in through a parameter.</p>

<p>I've defined a parameter <code>myExpression</code> with the desired value in the pipeline, and attempting to reference it like so:</p>

<p><a href=""https://i.stack.imgur.com/BJvMh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BJvMh.png"" alt=""enter image description here""></a></p>

<p>Note that the full expression would be: <code>{@pipeline().parameters.myExpression}</code></p>

<p>However, data factory does not execute that expression, rather it just accepts it as a verbatim string: </p>

<blockquote>
  <p>{@pipeline().parameters.myExpression}</p>
</blockquote>

<p><strong>How do we pass in an expression from parameters from within the pipeline?</strong></p>
","<azure-data-factory>","2019-12-27 13:49:12","892","0","2","59563212","<p>your description lacks a lot of context of what are you trying to do. I can only presume that you generate array in one pipeline and you want to iterate it in another. Looking at your print screen it looks like you typed in your value, therefore output is a plain text. you should hit dynamic context
<a href=""https://i.stack.imgur.com/LPk5y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LPk5y.png"" alt=""enter image description here""></a></p>

<p>so it would look like this:</p>

<p><a href=""https://i.stack.imgur.com/e3xx6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e3xx6.png"" alt=""enter image description here""></a></p>
"
"59501736","how to pass in an expression through a parameter","<p>Suppose I have a <code>foreach</code> inside of a pipe:</p>

<p><a href=""https://i.stack.imgur.com/RSwqi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RSwqi.png"" alt=""enter image description here""></a></p>

<p>I'd like to iterate through the following:</p>

<p><code>@split(split(item().name,'_')[4],'-')[1]</code></p>

<p>However, I'd like to pass this formula in through a parameter.</p>

<p>I've defined a parameter <code>myExpression</code> with the desired value in the pipeline, and attempting to reference it like so:</p>

<p><a href=""https://i.stack.imgur.com/BJvMh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BJvMh.png"" alt=""enter image description here""></a></p>

<p>Note that the full expression would be: <code>{@pipeline().parameters.myExpression}</code></p>

<p>However, data factory does not execute that expression, rather it just accepts it as a verbatim string: </p>

<blockquote>
  <p>{@pipeline().parameters.myExpression}</p>
</blockquote>

<p><strong>How do we pass in an expression from parameters from within the pipeline?</strong></p>
","<azure-data-factory>","2019-12-27 13:49:12","892","0","2","59577168","<p>When you define a parameter like this and pass the value, what you are doing is is send a string input, as the textbox doesn't accept expression. The only way to pass expression to a parameter is to pass it from another pipeline. Another issue we have is one ADF limitation -  there can not be nested iterations. Calling a second pipeline solves both the issues.</p>

<p>Split your flow in two pipelines. </p>

<p>First (parent) pipeline - Keep all steps up until generating the array over which iteration has to happen. </p>

<pre><code>@split(split(item().name,'_')[4],'-')[1]
</code></pre>

<p>Then, inside a for each loop, invoke an ""Execute pipeline"" activity. In there, pass the expression that you desire in a similar fashion- </p>

<p><a href=""https://i.stack.imgur.com/z5XeK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z5XeK.png"" alt=""enter image description here""></a></p>

<p>In the child pipeline, define a string parameter to absorb the passed value. Then, use <strong>@pipeline().parameters.ParamName</strong> to use it in there.</p>

<p>HTH</p>
"
"59489796","How we can create VM ware using Azure data Factory","<p><strong>I am trying to create Vmware using VMImage through Azure data factory!</strong></p>

<p>I am able to create manually by using Resource Manager, but i want to automate this process through Azure Data Factory.</p>
","<azure-devops><azure-active-directory><azure-data-factory><azure-virtual-machine><azure-vm-role>","2019-12-26 14:31:15","89","0","2","59489818","<p>If you can call a Azure Runbook from Azure Automation, then consider it done!
Also you can call the run book by a webjobs [Link] (<a href=""https://learn.microsoft.com/en-us/azure/app-service/webjobs-create"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/app-service/webjobs-create</a>)</p>
"
"59489796","How we can create VM ware using Azure data Factory","<p><strong>I am trying to create Vmware using VMImage through Azure data factory!</strong></p>

<p>I am able to create manually by using Resource Manager, but i want to automate this process through Azure Data Factory.</p>
","<azure-devops><azure-active-directory><azure-data-factory><azure-virtual-machine><azure-vm-role>","2019-12-26 14:31:15","89","0","2","59492671","<p>This does not seem like an appropriate use of Azure Data Factory (ADF) which is primarily an ELT tool.  Consider <a href=""https://azure.microsoft.com/en-gb/services/devops/"" rel=""nofollow noreferrer"">Azure DevOps</a>, <a href=""https://azure.microsoft.com/en-gb/services/azure-arc/"" rel=""nofollow noreferrer"">Azure Arc</a> or <a href=""https://www.terraform.io/"" rel=""nofollow noreferrer"">Terraform</a> which would be more appropriate for an Infrastructure as Code approach.</p>

<p>Or maybe just a bit of Powershell!?</p>
"
"59487594","Azure Data Flow create many databases in SQL Elastic Pool from one database","<p>I'm quite new in 'Azure'. Is it possible to create many databases in SQL Elastic Pool by Azure Data Factory (Data Flow)?  </p>

<p>Given:</p>

<blockquote>
  <h2>Table</h2>
  
  <p>Row1 | Value1<br>
  Row2 | Value2</p>
</blockquote>

<p>After execution I need to get two databases in specific elastic pool. One DB named: ""Value1"", second DB named: ""Value2"".</p>
","<azure><azure-elasticpool><azure-data-factory>","2019-12-26 11:11:39","75","0","1","59497917","<p>According my experience, Data Flow doesn't have the feature or component can help you create the databases. We can not do that with Data Flow.</p>

<p>With Data Factory, I have an idea is that you can create a stored procedure which create database in elastic pool with source column data in sink database, using Foreach to copy data from source table and call the stored procedure in sink.</p>

<p>Hope this helps.</p>
"
"59485248","Azure Databricks Python Job","<p>I have a requirement to parse a lot of small unstructured files in near real-time inside Azure and load the parsed data into a SQL database. I chose Python <em>(because I don't think any Spark cluster or big data would suite considering the volume of source files and their size)</em> and the parsing logic has been already written. I am looking forward to schedule this python script in different ways using Azure PaaS</p>

<ol>
<li>Azure Data Factory</li>
<li>Azure Databricks</li>
<li>Both 1+2</li>
</ol>

<p>May I ask what's the implication of running a Python notebook activity from Azure Data Factory pointing to Azure Databricks? Would I be able to fully leverage the potential of the cluster (Driver &amp; Workers)?</p>

<p>Also, please suggest me if you think the script has to be converted to PySpark to meet my use case requirement to run in Azure Databricks? The only hesitation here is the files are in KB and they are unstructured. </p>
","<python><azure><azure-data-factory><azure-databricks>","2019-12-26 07:27:31","489","1","1","59517813","<p>If the script is pure Python then it would only run on the driver node of the Databricks cluster making it very expensive (and slow due to cluster startup times). </p>

<p>You could rewrite as pyspark but if the data volumes are as low as you say then this is still expensive and slow. The smallest cluster will consume two vm’s - each with 4 cores. </p>

<p>I would look at using Azure Functions instead. Python is now an option: <a href=""https://learn.microsoft.com/en-us/azure/python/tutorial-vs-code-serverless-python-01"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/python/tutorial-vs-code-serverless-python-01</a></p>

<p>Azure Functions also have great integration with Azure Data Factory so your workflow would still work. </p>
"
"59484041","Azure Data Factory error while fetching pipeline RunId","<p>Azure Data Factory Pipeline throws error ""Failed to convert the value in 'schema' property to 'System.String' type. Please make sure the payload structure and value are correct"" If I use the queries with Dynamic content - </p>

<p>SELECT @pipeline().RunId AS RunId </p>

<p>OR</p>

<p>SELECT '@{pipeline().RunId}' AS RunId </p>

<p>in a Lookup Activity. I need the Run Id GUID.</p>
","<azure-data-factory>","2019-12-26 05:05:17","920","0","1","59489163","<p>Any time I have to do this, I simply create a new variable in the pipeline and use a SetVariable activity. Then, select the variable you want to assign and for the value ""@pipeline().RunId"".</p>

<p><a href=""https://i.stack.imgur.com/Mfhch.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Mfhch.png"" alt=""Set variable activity""></a></p>

<p>Then, to use the value simple use ""@variables('PipeId')"" where you need (replace PipeId with your own variable name).</p>

<p>I suggest reading this to get a better grasp of expressions and functions in data factory: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions</a></p>

<p>Hope this helped!!</p>
"
"59471464","is it possible to copy only the first few bytes of a file?","<p>I have a copy activity where the source is sftp and the sink is blob storage. </p>

<p><strong>Is it possible to ONLY copy the first few hundred bytes of a file?</strong></p>
","<azure-data-factory>","2019-12-24 16:38:26","63","0","1","59476511","<p>I see the answer for your another SO thread <a href=""https://stackoverflow.com/questions/59470838/how-to-output-variable-to-a-file"">how to output variable to a file?</a>, its solution is also suitable for the current scenario to return the first few hundred bytes of a file in Azure Functions to Copy Activity.</p>

<p>If you don't want to use this way, I think the only solution is to follow the offical document <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer""><code>Use custom activities in an Azure Data Factory pipeline</code></a> to create a custom activity in programming to realize the feature you want.</p>
"
"59470838","how to output variable to a file?","<p>I've got a <code>Get metadata</code> activity that goes to an sftp server and lists the files:</p>

<p><a href=""https://i.stack.imgur.com/OiT7t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OiT7t.png"" alt=""enter image description here""></a></p>

<p><strong>Is it possible to output this list to a file without using a function??</strong></p>
","<azure-data-factory>","2019-12-24 15:30:46","2035","2","2","59474591","<blockquote>
  <p>Is it possible to output this list to a file?</p>
</blockquote>

<p>Yes, you could connect an Azure Function Activity with Get Metadata Activity.Just pass the file list as parameter into Azure Function: <code>@activity('Get Metadata1').output</code></p>

<p><a href=""https://i.stack.imgur.com/ZH3TZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZH3TZ.png"" alt=""enter image description here""></a></p>

<p>Then save the data as your need inside function.(Such as call the blob storage sdk to save the data into blob storage) Surely,<code>Web Activity</code> is another option to call your custom api to save the data.</p>
"
"59470838","how to output variable to a file?","<p>I've got a <code>Get metadata</code> activity that goes to an sftp server and lists the files:</p>

<p><a href=""https://i.stack.imgur.com/OiT7t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OiT7t.png"" alt=""enter image description here""></a></p>

<p><strong>Is it possible to output this list to a file without using a function??</strong></p>
","<azure-data-factory>","2019-12-24 15:30:46","2035","2","2","59493970","<p>Is there a particular reason you have to log the output to a file? If you just call a Get Metadata task in Azure Data Factory it will be logged as part of the pipeline run and default logging anyway.  You can then access those logs if required.</p>

<p>Alternately, a common pattern I use with <em>Get Metadata</em> task is a <em>For Each</em> loop and then host multiple tasks within the <em>For Each</em>, eg a Copy task to move the files from SFTP to wherever you want, and a Stored Proc task for custom logging to a database.  Pass the filename from the <em>Get Metadata</em> task to the <em>Stored Proc</em> task as a parameter.  Something like this:</p>

<p><a href=""https://i.stack.imgur.com/7WLUO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7WLUO.png"" alt=""ADF pattern""></a></p>
"
"59467960","Stored procedure on azure data factory is failing with error message : The length of execution output is over limit (around 1M currently)","<p>I am running this stored procedure on azure data factory. The stored procedure perform some computation on 10 millions rows and takes about 90 minutes (Yup. I know I need to profile it). This stored procedure is working fine when I run it from SSMS but when triggering or debugging it on the data factory it throws this error:</p>

<blockquote>
  <p>""errorCode"": ""2001"",<br>
  ""message"": ""The length of execution output is over limit (around 1M currently). "",<br>
  ""failureType"": ""UserError"",<br>
  ""target"": ""Booking Flattening"",  </p>
</blockquote>

<p>Is it a configuration issue ? Any pointers you have would be greatly helpful.
I am running azure data factory version 2 and SQL Server 2014 on Azure</p>
","<sql-server><azure><stored-procedures><azure-data-factory>","2019-12-24 11:13:35","1496","1","1","59474689","<p>You could reference this GitHub blog:<a href=""https://github.com/MicrosoftDocs/azure-docs/issues/40088"" rel=""nofollow noreferrer"">The length of execution output is over limit (around 1M currently), Azure Data Factory #40088</a>.</p>

<p>Azure MSFT said: ""There currently is bug with handling very large monitoring outputs"".</p>

<p>Actually, as I searched a lot, not only Stored Procedure has the ""The length of execution output is over limit (around 1M currently)."" error, other actives also have the same error, such as Function, Data Flow, Lookup and so on.</p>

<p>We don't know when Data Factory product team will solve the Data Factory output limitation. </p>

<p>Someone has post the question in Data Factory feedback, and it's voted up 18 times. But still has no reply. Please see here:<a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/37843276-remove-output-limitations-on-web-and-azure-function"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory/suggestions/37843276-remove-output-limitations-on-web-and-azure-function</a></p>

<p>Most suggestions are using the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">For-Each</a> active to help you avoid the output limit error.</p>

<p>Reference: <a href=""https://stackoverflow.com/questions/52843154/web-activity-throws-overlimit-error-when-calling-rest-api"">Web activity throws overlimit error when calling rest api</a></p>

<p>Hope this helps.</p>
"
"59464898","Azure Data Factory same pipeline for multiple for trigger parallel?","<p>Suppose, we have parameter pipeline in Azure Data Factory which copy data from Dataset_DL_XXX (Data Lake) to Dataset_DB_XXX (Azure Database). this activity starts at 10:00 pm. At 10:05 pm we have 1 more request to copy data from Dataset_DL_YYY (Data Lake) to Dataset_DB_YYY (Azure Database).</p>

<p>Can we use the same pipeline to trigger multiple tasks? or do we have any other approach?</p>

<p>Thank You.  </p>
","<azure-data-factory>","2019-12-24 06:47:07","1822","0","1","59527851","<blockquote>
  <p>Can we use the same pipeline to trigger multiple tasks?</p>
</blockquote>

<p>Based on my researching, it should be possible if you use control flow in the pipeline.My idea as below:</p>

<p>1.Firstly, please see the statements from this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers"" rel=""nofollow noreferrer"">doc</a>:</p>

<blockquote>
  <p>A pipeline run in Azure Data Factory defines an instance of a pipeline
  execution. For example, say you have a pipeline that executes at 8:00
  AM, 9:00 AM, and 10:00 AM. In this case, there are three separate runs
  of the pipeline, or pipeline runs. Each pipeline run has a unique
  pipeline run ID. A run ID is a GUID that uniquely defines that
  particular pipeline run.</p>
</blockquote>

<p>So you could trigger the same pipeline at 10 pm and 10.05 pm.Then are different pipeline runs.</p>

<p>2.Get the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables"" rel=""nofollow noreferrer"">trigger time</a> inside the pipeline:<code>@pipeline().TriggerTime</code></p>

<p>3.Use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-if-condition-activity"" rel=""nofollow noreferrer"">If-Condition Activity</a> to control that which copy activity should be executed this time.</p>
"
"59463256","How to specify query for Azure Data Factory Source (Azure Table Storage) for yesterday's records","<p>I am copying records from an Azure Storage Table (source) to Azure Storage Table (sink) everyday. So if I am executing the query on December 24th 2019 (UTC) for instance, then I want to copy records for December 23rd 2019 (UTC).  The query works and is doing what I intend it to do. Here is the query:</p>

<pre><code>Timestamp ge datetime'2019-12-23T00:00Z' and Timestamp lt datetime'2019-12-24T00:00Z'
</code></pre>

<p>In the query above, the Timestamp column is automatically stamped in the Azure Storage Table when a new record is inserted in it. That is how Azure Storage Table works.</p>

<p>And here is the screenshot of the Data Factory Pipeline:</p>

<p><a href=""https://i.stack.imgur.com/sogcI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sogcI.png"" alt=""enter image description here""></a></p>

<p>I want to parameterize the query now. That is: if the query is run on 24th December, 2019 then it should copy 23rd December 2019's records and keep sliding as it executes everyday on a schedule. I don't know how to do that. I know that there is a utcNow function and there is a subtractFromTime Function.  I just don't know how to put it together.</p>
","<azure><parameters><azure-table-storage><azure-data-factory>","2019-12-24 02:45:28","1780","0","2","59463422","<p>You can do something like this:</p>

<pre><code>addDays(startOfDay(utcNow()), -1)
</code></pre>

<p>this would find the start of the previous day</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#date-functions"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#date-functions</a></p>
"
"59463256","How to specify query for Azure Data Factory Source (Azure Table Storage) for yesterday's records","<p>I am copying records from an Azure Storage Table (source) to Azure Storage Table (sink) everyday. So if I am executing the query on December 24th 2019 (UTC) for instance, then I want to copy records for December 23rd 2019 (UTC).  The query works and is doing what I intend it to do. Here is the query:</p>

<pre><code>Timestamp ge datetime'2019-12-23T00:00Z' and Timestamp lt datetime'2019-12-24T00:00Z'
</code></pre>

<p>In the query above, the Timestamp column is automatically stamped in the Azure Storage Table when a new record is inserted in it. That is how Azure Storage Table works.</p>

<p>And here is the screenshot of the Data Factory Pipeline:</p>

<p><a href=""https://i.stack.imgur.com/sogcI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sogcI.png"" alt=""enter image description here""></a></p>

<p>I want to parameterize the query now. That is: if the query is run on 24th December, 2019 then it should copy 23rd December 2019's records and keep sliding as it executes everyday on a schedule. I don't know how to do that. I know that there is a utcNow function and there is a subtractFromTime Function.  I just don't know how to put it together.</p>
","<azure><parameters><azure-table-storage><azure-data-factory>","2019-12-24 02:45:28","1780","0","2","59463785","<p>@4c74356b41, Thank you for your kind support.  Based on your answers and some more googling, I was able to piece it together.  Here is the final expression:</p>

<pre><code>Timestamp ge @{concat('datetime','''',addDays(startOfDay(utcNow()), -1),'''')} and Timestamp lt @{concat('datetime','''',startOfDay(utcNow()),'''')}
</code></pre>
"
"59462120","Unable to connect to Oracle on Azure Data Factory","<p>I'm trying to connect to my on-premise Oracle database in order to migrate and copy some tables over to Azure SQL, but am not able to do so despite making sure all the connection parameters match the provided values in tnsnames.</p>

<p>Am I missing something? The error says the socket is closed but haven't gotten any useful information other than <a href=""https://social.msdn.microsoft.com/Forums/en-US/28b6de2b-592b-4a39-b4fe-de0551f563da/socket-closed-error-when-connecting-on-prem-oracle-to-data-factory"" rel=""nofollow noreferrer"">this prior issue, but doesn't contain any solution.</a> I currently use Oracle 11.2.0.3 so the ADF connector should support this version.</p>

<p>Not sure what else I need to check. Any thoughts would be greatly appreciated! </p>

<p><a href=""https://i.stack.imgur.com/aiuEQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aiuEQ.png"" alt=""enter image description here""></a></p>
","<oracle><oracle-adf><azure-data-factory><connector>","2019-12-23 23:00:13","3082","1","2","59501009","<p>Do you have any proxy or firewall configured?
Have you tried creating the linked service and then testing the connection? Sometimes it occurred to me that I failed to test the connection of a new linked service but when creating it and retesting the connection is successful ...</p>
"
"59462120","Unable to connect to Oracle on Azure Data Factory","<p>I'm trying to connect to my on-premise Oracle database in order to migrate and copy some tables over to Azure SQL, but am not able to do so despite making sure all the connection parameters match the provided values in tnsnames.</p>

<p>Am I missing something? The error says the socket is closed but haven't gotten any useful information other than <a href=""https://social.msdn.microsoft.com/Forums/en-US/28b6de2b-592b-4a39-b4fe-de0551f563da/socket-closed-error-when-connecting-on-prem-oracle-to-data-factory"" rel=""nofollow noreferrer"">this prior issue, but doesn't contain any solution.</a> I currently use Oracle 11.2.0.3 so the ADF connector should support this version.</p>

<p>Not sure what else I need to check. Any thoughts would be greatly appreciated! </p>

<p><a href=""https://i.stack.imgur.com/aiuEQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aiuEQ.png"" alt=""enter image description here""></a></p>
","<oracle><oracle-adf><azure-data-factory><connector>","2019-12-23 23:00:13","3082","1","2","59557388","<p>Your screenprint shows you are using the <em>AutoResolveIntegrationRuntime</em> but as you say your Oracle db is on-premises you would need a Self-hosted Integration runtime (SHIR) as per <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-oracle#prerequisites"" rel=""nofollow noreferrer"">this article</a>.  You would still need an SHIR for an IaaS Oracle db.  Ideally the SHIR should be 'close' to the datasource so probably on-premises in the same network.</p>
"
"59457647","Azure Data Factory Mapping Data Flow to CSV sink results in zero-byte files","<p>I'm sharpening up my Azure Data Factory chops, comparing Copy Activity performance vs Mapping Data Flows writing to a single CSV file in Azure Blob Storage.</p>

<p>When I write to a Single CSV through an Azure Blob Storage Linked Service (azureBlobLinkedService) through a Dataset (azureBlobSingleCSVFileNameDataset), using a Copy Activity get the output in the blob storage container that I expect.  For example, an output file of MyData.csv in the container MyContainer under the folder /output/csv/singleFiles.</p>

<p>When I write to a Single CSV through the same Blob Storage Linked Service, but through a different DataSet (azureBlobSingleCSVNoFileNameDataset), using a Mapping Data Flow I get the following:</p>

<ul>
<li>MyContainer/output/csv/singleFiles (zero length file)</li>
<li>MyContainer/output/csv/singleFiles/MyData.csv (contains the data that I expect)</li>
</ul>

<p>I don't understand why I'm getting the zero length files produced when using a Mapping Data Flow.</p>

<p>Here are my source files:</p>

<p>linkedService/azureBlobLinkedService</p>

<pre><code>{
    ""name"": ""azureBlobLinkedService"",
    ""type"": ""Microsoft.DataFactory/factories/linkedservices"",
    ""properties"": {
        ""type"": ""AzureBlobStorage"",
        ""parameters"": {
            ""azureBlobConnectionStringSecretName"": {
                ""type"": ""string""
            }
        },
        ""annotations"": [],
        ""typeProperties"": {
            ""connectionString"": {
                ""type"": ""AzureKeyVaultSecret"",
                ""store"": {
                    ""referenceName"": ""AzureKeyVaultLinkedService"",
                    ""type"": ""LinkedServiceReference""
                },
                ""secretName"": ""@{linkedService().azureBlobConnectionStringSecretName}""
            }
        }
    }
}
</code></pre>

<p>dataset/azureBlobSingleCSVFileNameDataset</p>

<pre><code>{
    ""name"": ""azureBlobSingleCSVFileNameDataset"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""azureBlobLinkedService"",
            ""type"": ""LinkedServiceReference"",
            ""parameters"": {
                ""azureBlobConnectionStringSecretName"": {
                    ""value"": ""@dataset().azureBlobConnectionStringSecretName"",
                    ""type"": ""Expression""
                }
            }
        },
        ""parameters"": {
            ""azureBlobConnectionStringSecretName"": {
                ""type"": ""string""
            },
            ""azureBlobSingleCSVFileName"": {
                ""type"": ""string""
            },
            ""azureBlobSingleCSVFolderPath"": {
                ""type"": ""string""
            },
            ""azureBlobSingleCSVContainerName"": {
                ""type"": ""string""
            }
        },
        ""annotations"": [],
        ""type"": ""DelimitedText"",
        ""typeProperties"": {
            ""location"": {
                ""type"": ""AzureBlobStorageLocation"",
                ""fileName"": {
                    ""value"": ""@dataset().azureBlobSingleCSVFileName"",
                    ""type"": ""Expression""
                },
                ""folderPath"": {
                    ""value"": ""@dataset().azureBlobSingleCSVFolderPath"",
                    ""type"": ""Expression""
                },
                ""container"": {
                    ""value"": ""@dataset().azureBlobSingleCSVContainerName"",
                    ""type"": ""Expression""
                }
            },
            ""columnDelimiter"": "","",
            ""escapeChar"": ""\\"",
            ""firstRowAsHeader"": true,
            ""quoteChar"": ""\""""
        },
        ""schema"": []
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p>pipeline/Azure SQL Table to Blob Single CSV Copy Pipeline (this produces expected results)</p>

<pre><code>{
    ""name"": ""Azure SQL Table to Blob Single CSV Copy Pipeline"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""Copy Azure SQL Table to Blob Single CSV"",
                ""type"": ""Copy"",
                ""dependsOn"": [],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""userProperties"": [],
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""AzureSqlSource"",
                        ""queryTimeout"": ""02:00:00""
                    },
                    ""sink"": {
                        ""type"": ""DelimitedTextSink"",
                        ""storeSettings"": {
                            ""type"": ""AzureBlobStorageWriteSettings""
                        },
                        ""formatSettings"": {
                            ""type"": ""DelimitedTextWriteSettings"",
                            ""quoteAllText"": true,
                            ""fileExtension"": "".csv""
                        }
                    },
                    ""enableStaging"": false
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""azureSqlDatabaseTableDataset"",
                        ""type"": ""DatasetReference"",
                        ""parameters"": {
                            ""azureSqlDatabaseConnectionStringSecretName"": {
                                ""value"": ""@pipeline().parameters.sourceAzureSqlDatabaseConnectionStringSecretName"",
                                ""type"": ""Expression""
                            },
                            ""azureSqlDatabaseTableSchemaName"": {
                                ""value"": ""@pipeline().parameters.sourceAzureSqlDatabaseTableSchemaName"",
                                ""type"": ""Expression""
                            },
                            ""azureSqlDatabaseTableTableName"": {
                                ""value"": ""@pipeline().parameters.sourceAzureSqlDatabaseTableTableName"",
                                ""type"": ""Expression""
                            }
                        }
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""azureBlobSingleCSVFileNameDataset"",
                        ""type"": ""DatasetReference"",
                        ""parameters"": {
                            ""azureBlobConnectionStringSecretName"": {
                                ""value"": ""@pipeline().parameters.sinkAzureBlobConnectionStringSecretName"",
                                ""type"": ""Expression""
                            },
                            ""azureBlobSingleCSVFileName"": {
                                ""value"": ""@pipeline().parameters.sinkAzureBlobSingleCSVFileName"",
                                ""type"": ""Expression""
                            },
                            ""azureBlobSingleCSVFolderPath"": {
                                ""value"": ""@pipeline().parameters.sinkAzureBlobSingleCSVFolderPath"",
                                ""type"": ""Expression""
                            },
                            ""azureBlobSingleCSVContainerName"": {
                                ""value"": ""@pipeline().parameters.sinkAzureBlobSingleCSVContainerName"",
                                ""type"": ""Expression""
                            }
                        }
                    }
                ]
            }
        ],
        ""parameters"": {
            ""sourceAzureSqlDatabaseConnectionStringSecretName"": {
                ""type"": ""string""
            },
            ""sourceAzureSqlDatabaseTableSchemaName"": {
                ""type"": ""string""
            },
            ""sourceAzureSqlDatabaseTableTableName"": {
                ""type"": ""string""
            },
            ""sinkAzureBlobConnectionStringSecretName"": {
                ""type"": ""string""
            },
            ""sinkAzureBlobSingleCSVContainerName"": {
                ""type"": ""string""
            },
            ""sinkAzureBlobSingleCSVFolderPath"": {
                ""type"": ""string""
            },
            ""sinkAzureBlobSingleCSVFileName"": {
                ""type"": ""string""
            }
        },
        ""annotations"": []
    },
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>

<p>dataset/azureBlobSingleCSVNoFileNameDataset: (no filename in dataset required by mapping data flow, set in the mapping data flow)</p>

<pre><code>{
    ""name"": ""azureBlobSingleCSVNoFileNameDataset"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""azureBlobLinkedService"",
            ""type"": ""LinkedServiceReference"",
            ""parameters"": {
                ""azureBlobConnectionStringSecretName"": {
                    ""value"": ""@dataset().azureBlobConnectionStringSecretName"",
                    ""type"": ""Expression""
                }
            }
        },
        ""parameters"": {
            ""azureBlobConnectionStringSecretName"": {
                ""type"": ""string""
            },
            ""azureBlobSingleCSVFolderPath"": {
                ""type"": ""string""
            },
            ""azureBlobSingleCSVContainerName"": {
                ""type"": ""string""
            }
        },
        ""annotations"": [],
        ""type"": ""DelimitedText"",
        ""typeProperties"": {
            ""location"": {
                ""type"": ""AzureBlobStorageLocation"",
                ""folderPath"": {
                    ""value"": ""@dataset().azureBlobSingleCSVFolderPath"",
                    ""type"": ""Expression""
                },
                ""container"": {
                    ""value"": ""@dataset().azureBlobSingleCSVContainerName"",
                    ""type"": ""Expression""
                }
            },
            ""columnDelimiter"": "","",
            ""escapeChar"": ""\\"",
            ""firstRowAsHeader"": true,
            ""quoteChar"": ""\""""
        },
        ""schema"": []
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p>dataflow/azureSqlDatabaseTableToAzureBlobSingleCSVDataFlow</p>

<pre><code>{
    ""name"": ""azureSqlDatabaseTableToAzureBlobSingleCSVDataFlow"",
    ""properties"": {
        ""type"": ""MappingDataFlow"",
        ""typeProperties"": {
            ""sources"": [
                {
                    ""dataset"": {
                        ""referenceName"": ""azureSqlDatabaseTableDataset"",
                        ""type"": ""DatasetReference""
                    },
                    ""name"": ""readFromAzureSqlDatabase""
                }
            ],
            ""sinks"": [
                {
                    ""dataset"": {
                        ""referenceName"": ""azureBlobSingleCSVNoFileNameDataset"",
                        ""type"": ""DatasetReference""
                    },
                    ""name"": ""writeToAzureBlobSingleCSV""
                }
            ],
            ""transformations"": [
                {
                    ""name"": ""enrichWithRuntimeMetadata""
                }
            ],
            ""script"": ""\nparameters{\n\tsourceConnectionSecretName as string,\n\tsinkConnectionStringSecretName as string,\n\tsourceObjectName as string,\n\tsinkObjectName as string,\n\tdataFactoryName as string,\n\tdataFactoryPipelineName as string,\n\tdataFactoryPipelineRunId as string,\n\tsinkFileNameNoPath as string\n}\nsource(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tisolationLevel: 'READ_UNCOMMITTED',\n\tformat: 'table') ~&gt; readFromAzureSqlDatabase\nreadFromAzureSqlDatabase derive({__sourceConnectionStringSecretName} = $sourceConnectionSecretName,\n\t\t{__sinkConnectionStringSecretName} = $sinkConnectionStringSecretName,\n\t\t{__sourceObjectName} = $sourceObjectName,\n\t\t{__sinkObjectName} = $sinkObjectName,\n\t\t{__dataFactoryName} = $dataFactoryName,\n\t\t{__dataFactoryPipelineName} = $dataFactoryPipelineName,\n\t\t{__dataFactoryPipelineRunId} = $dataFactoryPipelineRunId) ~&gt; enrichWithRuntimeMetadata\nenrichWithRuntimeMetadata sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tpartitionFileNames:[($sinkFileNameNoPath)],\n\tpartitionBy('hash', 1),\n\tquoteAll: true) ~&gt; writeToAzureBlobSingleCSV""
        }
    }
}
</code></pre>

<p>pipeline/Azure SQL Table to Blob Single CSV Data Flow Pipeline (this produces the expected results, plus zero-byte file at folder paths.)</p>

<pre><code>{
    ""name"": ""Azure SQL Table to Blob Single CSV Data Flow Pipeline"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""Copy Sql Database Table To Blob Single CSV Data Flow"",
                ""type"": ""ExecuteDataFlow"",
                ""dependsOn"": [],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""userProperties"": [],
                ""typeProperties"": {
                    ""dataflow"": {
                        ""referenceName"": ""azureSqlDatabaseTableToAzureBlobSingleCSVDataFlow"",
                        ""type"": ""DataFlowReference"",
                        ""parameters"": {
                            ""sourceConnectionSecretName"": {
                                ""value"": ""'@{pipeline().parameters.sourceAzureSqlDatabaseConnectionStringSecretName}'"",
                                ""type"": ""Expression""
                            },
                            ""sinkConnectionStringSecretName"": {
                                ""value"": ""'@{pipeline().parameters.sinkAzureBlobConnectionStringSecretName}'"",
                                ""type"": ""Expression""
                            },
                            ""sourceObjectName"": {
                                ""value"": ""'@{concat('[', pipeline().parameters.sourceAzureSqlDatabaseTableSchemaName, '].[', pipeline().parameters.sourceAzureSqlDatabaseTableTableName, ']')}'"",
                                ""type"": ""Expression""
                            },
                            ""sinkObjectName"": {
                                ""value"": ""'@{concat(pipeline().parameters.sinkAzureBlobSingleCSVContainerName, '/', pipeline().parameters.sinkAzureBlobSingleCSVFolderPath, '/', \npipeline().parameters.sinkAzureBlobSingleCSVFileName)}'"",
                                ""type"": ""Expression""
                            },
                            ""dataFactoryName"": {
                                ""value"": ""'@{pipeline().DataFactory}'"",
                                ""type"": ""Expression""
                            },
                            ""dataFactoryPipelineName"": {
                                ""value"": ""'@{pipeline().Pipeline}'"",
                                ""type"": ""Expression""
                            },
                            ""dataFactoryPipelineRunId"": {
                                ""value"": ""'@{pipeline().RunId}'"",
                                ""type"": ""Expression""
                            },
                            ""sinkFileNameNoPath"": {
                                ""value"": ""'@{pipeline().parameters.sinkAzureBlobSingleCSVFileName}'"",
                                ""type"": ""Expression""
                            }
                        },
                        ""datasetParameters"": {
                            ""readFromAzureSqlDatabase"": {
                                ""azureSqlDatabaseConnectionStringSecretName"": {
                                    ""value"": ""@pipeline().parameters.sourceAzureSqlDatabaseConnectionStringSecretName"",
                                    ""type"": ""Expression""
                                },
                                ""azureSqlDatabaseTableSchemaName"": {
                                    ""value"": ""@pipeline().parameters.sourceAzureSqlDatabaseTableSchemaName"",
                                    ""type"": ""Expression""
                                },
                                ""azureSqlDatabaseTableTableName"": {
                                    ""value"": ""@pipeline().parameters.sourceAzureSqlDatabaseTableTableName"",
                                    ""type"": ""Expression""
                                }
                            },
                            ""writeToAzureBlobSingleCSV"": {
                                ""azureBlobConnectionStringSecretName"": {
                                    ""value"": ""@pipeline().parameters.sinkAzureBlobConnectionStringSecretName"",
                                    ""type"": ""Expression""
                                },
                                ""azureBlobSingleCSVFolderPath"": {
                                    ""value"": ""@pipeline().parameters.sinkAzureBlobSingleCSVFolderPath"",
                                    ""type"": ""Expression""
                                },
                                ""azureBlobSingleCSVContainerName"": {
                                    ""value"": ""@pipeline().parameters.sinkAzureBlobSingleCSVContainerName"",
                                    ""type"": ""Expression""
                                }
                            }
                        }
                    },
                    ""compute"": {
                        ""coreCount"": 8,
                        ""computeType"": ""General""
                    }
                }
            }
        ],
        ""parameters"": {
            ""sourceAzureSqlDatabaseConnectionStringSecretName"": {
                ""type"": ""string""
            },
            ""sourceAzureSqlDatabaseTableSchemaName"": {
                ""type"": ""string""
            },
            ""sourceAzureSqlDatabaseTableTableName"": {
                ""type"": ""string""
            },
            ""sinkAzureBlobConnectionStringSecretName"": {
                ""type"": ""string""
            },
            ""sinkAzureBlobSingleCSVContainerName"": {
                ""type"": ""string""
            },
            ""sinkAzureBlobSingleCSVFolderPath"": {
                ""type"": ""string""
            },
            ""sinkAzureBlobSingleCSVFileName"": {
                ""type"": ""string""
            }
        },
        ""annotations"": []
    },
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>
","<azure-data-factory><azure-blob-storage>","2019-12-23 15:21:10","2147","6","1","61082973","<p>The reason to get 0 lengths (bytes) files means that while your pipeline may have run with success, it did not return or produce any output.</p>

<p>One of the better techniques is to preview the output of every stage to be sure that each stage there is an expected output.</p>
"
"59453326","Azure Data Factory Mapping Data Flows VS SSIS","<p>Its not really a coding problem so I hope its fine to ask here, otherwise I will just delete my post.</p>

<p>We have recently moved to the cloud and are trying out ADF data flows.I have expected it to be a bit more efficient. However, I don't really understand currently how are those superior to the SSIS packages. I have tried playing around with different settings and so far it appears to me as slower( cluster has to start up ) and more expensive solution. So now I am kind of in doubt whether just to leave my SSIS packages in there and trigger them from ADF pipelines or actually re-write them to data flows. The only pro that I see for now is that data flows could be more intuitive for non-technical people. </p>

<p>So I would be grateful if someone could explain to me ""when"" it's a right thing to actually use them ? </p>

<p>Thank you in advance!  </p>
","<azure><ssis><azure-data-factory><dataflow>","2019-12-23 10:00:36","503","1","1","59550512","<p>I think it all depends on your real scenario.</p>

<p>SSIS is SQL Server Integration Services and is part of the SQL Server product distribution, has been around since 2005 as SSIS and before that as DTS.</p>

<p>ADF mapping data flows, Cloud-based PaaS service for data integration.</p>

<p>Both can be used to integrate and transform data across on-prem and cloud data stores. However, SSIS is built primarily as an on-prem service while ADF has a scale-out data movement service in Azure.</p>

<p>You can lift &amp; shift SSIS jobs into the cloud using ADF or build new ETL jobs in ADF as Cloud-first jobs.</p>

<p>More clues,please refer to this case:<a href=""https://stackoverflow.com/questions/35985352/azure-data-factories-vs-ssis"">Azure Data Factories vs SSIS</a></p>
"
"59449188","Copy millions of files form root AZStorage Blob to subfolders","<p>I’ve got multiple Azure storage blob containers each with over 1M JSON files include the root. Impossible to work with (no shocker) so trying to use Data Factory to move them to multiple folders using a timestamp in the files to create a YYYY-MM-DD/HH folder setup as a partition system. But every approach I’ve tried fails with timeouts / too many item limits. Need to open each file, get the timestamp, and use it to move the file to a dynamic path using the timestamp data. Ideas? </p>

<p><em>UPDATE: I was able to get around this, but I wouldn't call it a ""answer"" so I'll just update the question. To create smaller collections, I parameterized the pipeline to accept a file name wildcard. I then created another pipeline that uses an array of 0-9,a-z to use that as an parameter on the dataset. Brute force workaround... assume there's got to be a better solution, but this works for now.</em></p>
","<azure><azure-data-factory><azure-blob-storage>","2019-12-23 00:52:00","843","7","3","60984434","<p>Read doc: <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/team-data-science-process/move-azure-blob"" rel=""nofollow noreferrer"">Move data to and from Azure Blob storage</a></p>

<p>The following articles describe how to move data to and from Azure Blob storage using different technologies.</p>

<ul>
<li><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/team-data-science-process/move-data-to-azure-blob-using-azure-storage-explorer"" rel=""nofollow noreferrer"">Azure Storage-Explorer</a></li>
<li><a href=""https://learn.microsoft.com/azure/storage/common/storage-use-azcopy-v10"" rel=""nofollow noreferrer"">AzCopy</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/team-data-science-process/move-data-to-azure-blob-using-python"" rel=""nofollow noreferrer"">Python-SDK</a> (Others:  <a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-overview"" rel=""nofollow noreferrer"">.NET, Java, Node.js, Python, Go, PHP, Ruby.</a>)</li>
<li><a href=""https://learn.microsoft.com/en-us/azure/machine-learning/team-data-science-process/move-data-to-azure-blob-using-ssis"" rel=""nofollow noreferrer"">SSIS</a></li>
</ul>

<hr>

<p>In your case, I would suggest you to use SDK, which supports <a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-overview"" rel=""nofollow noreferrer"">.NET, Java, Node.js, Python, Go, PHP, Ruby.</a></p>

<p>Believe me , if you want to migrate your datas from AzureBlob , <code>DataFactory</code> is not a good way, it makes the problem more complicated.
( This is my suggestion after I migrated over 100 million JSON-files (over 2TB) from AzureBlob)</p>
"
"59449188","Copy millions of files form root AZStorage Blob to subfolders","<p>I’ve got multiple Azure storage blob containers each with over 1M JSON files include the root. Impossible to work with (no shocker) so trying to use Data Factory to move them to multiple folders using a timestamp in the files to create a YYYY-MM-DD/HH folder setup as a partition system. But every approach I’ve tried fails with timeouts / too many item limits. Need to open each file, get the timestamp, and use it to move the file to a dynamic path using the timestamp data. Ideas? </p>

<p><em>UPDATE: I was able to get around this, but I wouldn't call it a ""answer"" so I'll just update the question. To create smaller collections, I parameterized the pipeline to accept a file name wildcard. I then created another pipeline that uses an array of 0-9,a-z to use that as an parameter on the dataset. Brute force workaround... assume there's got to be a better solution, but this works for now.</em></p>
","<azure><azure-data-factory><azure-blob-storage>","2019-12-23 00:52:00","843","7","3","61213578","<p>If you have time... I would do the following:</p>

<p>Create an Azure Function to read the file and get your timestamp and do your move operation. scope the function just to use a single file.  Then use events (EventGrid) in the storage account to trigger the function on create of a blob.  Then you know for any new files it will move the file to the right spot.  (Remember you need to reach a million executions in the consumption model for functions to start billing, so this is a low cost option).</p>

<p>For the current files, create another function (or if you want some more control, use a logic app, but your cost will be a bit more) and set your parralelism on the function or logic app to a low amount (to keep an eye on your executions). that run a simple for each with limits that run your first function.  This will slowly move your files out of that container eventually getting you into a reasonable item count to work with on with stuff like ADF.  This might just solve your problem for the long run as any new files will be categorized accordingly, and your backlog is slowly being moved as required.  If you need to update a DB with a pointer to where your file lives you could put that piece of code also in your function or logic app.  Just my two cents :)</p>
"
"59449188","Copy millions of files form root AZStorage Blob to subfolders","<p>I’ve got multiple Azure storage blob containers each with over 1M JSON files include the root. Impossible to work with (no shocker) so trying to use Data Factory to move them to multiple folders using a timestamp in the files to create a YYYY-MM-DD/HH folder setup as a partition system. But every approach I’ve tried fails with timeouts / too many item limits. Need to open each file, get the timestamp, and use it to move the file to a dynamic path using the timestamp data. Ideas? </p>

<p><em>UPDATE: I was able to get around this, but I wouldn't call it a ""answer"" so I'll just update the question. To create smaller collections, I parameterized the pipeline to accept a file name wildcard. I then created another pipeline that uses an array of 0-9,a-z to use that as an parameter on the dataset. Brute force workaround... assume there's got to be a better solution, but this works for now.</em></p>
","<azure><azure-data-factory><azure-blob-storage>","2019-12-23 00:52:00","843","7","3","61229522","<p>It is not clear if you are using the <a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-namespace"" rel=""nofollow noreferrer"">hierarchical folder structure</a> provided by Azure Data Lake Storage Gen2, the generation 1 simulates a folders structure but it is not optimum.</p>

<p>There are several advantages on the ADLSV2 that should help in your case mainly related to move operations.</p>

<p>To migrate from ADLS Gen 1 to ADLS Gen 2 have a look <a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-migrate-gen1-to-gen2"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Additionally, you may explore optimizations on your specific case with the following paper <a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-performance-tuning-guidance"" rel=""nofollow noreferrer"">here</a>. </p>
"
"59449055","Azure Table storage to Azure Table storage using Azure Data Factory using DateTime Parameterized query","<p>I have an Azure Table storage where a few records are added every day (usually 3-5).  There are days when no records can be added, so the volume is very low.  Here is the structure of table with the pre-defined PartitionKey, RowKey, and Timestamp columns:</p>

<p><a href=""https://i.stack.imgur.com/sp7m0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sp7m0.png"" alt=""Screenshot showing Data in Azure Table""></a></p>

<p>I need to query this table from the Azure Data Factory for the previous day's records.  So for the example data shown below, I could be querying for 2019-04-25 data on 2019-04-26.  The reason being, one day's staleness does not make a difference and that way, I don't have to worry about 'Watermarks' etc.  I simply query for the data for the previous day and copy it to an identical Azure Table in Azure Data Lake Storage Gen 2.</p>

<p>I know that I need to specify a parameterized query based on the 'Timestamp' column for the previous day but do not know how to specify it.</p>

<p>Please advise.</p>
","<parameter-passing><azure-table-storage><azure-data-factory>","2019-12-23 00:18:58","2221","1","1","59551293","<p>You could set query sql in the copy activity table storage source.For your needs,it should be like:</p>

<pre><code>time gt datetime'2019-04-25T00:00:00' and time le datetime'2019-04-2T00:00:00'
</code></pre>

<p>My sample data as below:</p>

<p><a href=""https://i.stack.imgur.com/1ttSq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1ttSq.png"" alt=""enter image description here""></a></p>

<p>Preview data as below:</p>

<p><a href=""https://i.stack.imgur.com/rjcSH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rjcSH.png"" alt=""enter image description here""></a></p>

<p>Pls see some examples in this doc: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-table-storage#azuretablesourcequery-examples"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-table-storage#azuretablesourcequery-examples</a>.</p>
"
"59432145","Is there a way to preserve indexes and keys of SQL table when performing a copy activity using azure data factory","<p>Trying to perform a copy activity from onprem sql to azure sql.
The source database table has few indexes and keys and when I perform Copy activity to azure sql by Auto-generate new table, indexes and keys are missing on destination table.</p>
","<sql-server><azure><azure-sql-database><azure-data-factory>","2019-12-20 22:56:36","725","0","1","59464800","<p>Based on the parameter statements in this official <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#azure-sql-database-as-the-sink"" rel=""nofollow noreferrer"">document</a>,there is no guarantee that the index and key will be transferred in ADF copy activity.</p>

<p>As you mentioned in your comment,you could have to create them by yourself,such as in the stored procedure which could be executed in ADF copy activity.</p>

<p>More clues,please refer to these threads:</p>

<p>1.<a href=""https://www.sqlserverlogexplorer.com/copy-table-one-database-another-database/"" rel=""nofollow noreferrer"">https://www.sqlserverlogexplorer.com/copy-table-one-database-another-database/</a></p>

<p>2.<a href=""https://stackoverflow.com/questions/652148/how-to-copy-indexes-from-one-table-to-another-in-sql-server"">How to copy indexes from one table to another in SQL Server</a></p>
"
"59428193","Calling a stored procedure in ADFv2 with output (JSON) from an Azure Function call. Passing type, not JSON string?","<p>I'm creating a pipeline in ADFv2 that calls an Azure Function that outputs JSON. I'm then passing that JSON into a stored procedure Activity, but the stored procedure isn't getting the JSON text, it's getting the object type. I've been able to pass a single JSON Value successfully. </p>

<p>Here are some of the attempts and their results:</p>

<pre><code>@activity('AzureFunction').output                      - Dictionary Error
@activity('AzureFunction').output.FileList             - List Error
@activity('AzureFunction').output.fileList[0].FileName - Sent name of file (Hospital_Overview.csv)
@activity('AzureFunction').output.firstRow             - Cannot be evaluated becauase property firstRow doesn't exist
@activity('AzureFunction').output.fileList             - List Error
@activity('AzureFunction').output.production           - Sent value of production JSON node (False)
@activity('AzureFunction').output()                    - Invalid Template
@activity('AzureFunction').output.response             - activity('AzureFunction').output.response' cannot be evaluated because property 'response' doesn't exist, available properties are 'fileList, production, effectiveIntegrationRuntime, executionDuration, durationInQueue, billingReference'
@activity('AzureFunction').output.ToString()           - unable to parse
@activity('AzureFunction').output.value                - The expression 'activity('AzureFunction').output.value' cannot be evaluated because property 'value' doesn't exist, available properties are 'fileList, production, effectiveIntegrationRuntime, executionDuration, durationInQueue, billingReference'
</code></pre>

<p>The JSON being sent to the stored procedure is as follows:</p>

<pre><code>{
""fileList"": [
    {
        ""FileName"": ""File1.csv"",
        ""FileDate"": ""2019-12-13T11:26:54Z"",
        ""Downloaded"": false,
    },
    {
        ""FileName"": ""File2"",
        ""FileDate"": ""2019-12-13T11:29:26Z"",
        ""Downloaded"": false,
    }
],
""production"": false,
""effectiveIntegrationRuntime"": ""DefaultIntegrationRuntime (East US 2)"",
""executionDuration"": 6,
""durationInQueue"": {
    ""integrationRuntimeQueue"": 0
},
""billingReference"": {
    ""activityType"": ""ExternalActivity"",
    ""billableDuration"": {
        ""Managed"": 0.016666666666666666
    }
}
}
</code></pre>

<p>I expected the following options to work:</p>

<pre><code>@activity('AzureFunction').output
</code></pre>

<p>The stored procedure gives me the following error message:</p>

<blockquote>
  <p>Error number: 13609. Severity: 16. State: 3. Procedure: dbo.Control_Insert_FromAzureFunction. Line: 76.<br>
  Message: JSON text is not properly formatted. Unexpected character 'S' is found at position 0..</p>
</blockquote>

<p>I define the incoming JSON as <code>NVARCHAR(MAX)</code> and log it so I can see what I'm getting:</p>

<pre><code>System.Collections.Generic.Dictionary`2[System.String,System.Object]
</code></pre>

<p>So in the Azure SQL database wasn't passed text, it was passed an object. I either need to know how to convert the object in Azure SQL database to a string, or just pass the JSON text from the stored procedure activity. </p>

<p>This is the stored procedure I'm using:</p>

<pre><code>CREATE PROCEDURE [dbo].[Control_Insert_FromAzureFunction] 
    @json NVARCHAR(MAX)
AS
BEGIN
    DECLARE @RC INT = 0,
            @StatusMessage VARCHAR(MAX) = 'Pending...',
            @ErrorDescription VARCHAR(MAX) = '',
            @ProcedureName VARCHAR(MAX) = object_name(@@PROCID),
            @LogId INT = 0, 
            @ProcessQueueId INT = 0;

    BEGIN TRY
        IF object_id('tempdb..#File') IS NOT NULL
        BEGIN
            DROP TABLE #File;
        END;

        CREATE TABLE #File
        (
            [FileName]   VARCHAR(512) NOT NULL, 
            [FileDate]   DATETIME2    NOT NULL,
            [Downloaded] BIT          NOT NULL
        );

        INSERT INTO #File ([FileName], [OutputFileName], [FileDate], [Downloaded], 
                           [SFTPElapsedTime], [SFTPStartTime], [SFTPEndTime])
            SELECT 
                json_value(files.[Value], '$.FileName') AS [FileName],
                CAST(json_value(files.[Value], '$.FileDate') AS DATETIME2) AS [FileDate],
                CAST(json_value(files.[Value], '$.Downloaded') AS BIT) AS [Downloaded]
            FROM 
                OPENJSON(@json, '$.fileStatus') AS files;
    END TRY
    BEGIN CATCH
        SET @RC = -1;

        SET @StatusMessage = @json;

        EXEC [dbo].[Log_Merge] @LogId = @LogId OUT
                                    , @ProcessQueueId = @ProcessQueueId
                                    , @ProcedureName = @ProcedureName
                                    , @StatusMessage = @StatusMessage
                                    , @ErrorDescription = @ErrorDescription
                                    , @ReturnCode = @RC;
    END CATCH;
END
</code></pre>
","<azure><azure-sql-database><azure-functions><azure-data-factory>","2019-12-20 16:05:37","826","0","1","59461294","<p>I needed to wrap the object in the string function.  It looks like primitive types like string and int send their value, but a Dictionary sends the object type unless you specifically ask for string value.  Hopefully this helps someone else struggling with this in the future.</p>

<pre><code>@string(activity('GetDefinitiveDataFunction').output)
</code></pre>
"
"59421171","I wanted to fetch the data from google dashboards using Azure data factory","<p>I wanted to fetch the data from google dashboards and load it to azure data warehouse using azure data factory.
Is there any way I can follow to load the data to data warehouse using azure data factory.</p>
","<azure><azure-data-factory>","2019-12-20 07:10:15","57","0","1","59450678","<p>Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">ADF copy activity support connector list</a>, there is no google dashboard connector.So,maybe you have to consider 2 ways:</p>

<p>1.If you could fetch data from google dashboards via REST API,then you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">Web Activity</a> in ADF and connect it with Copy Activity.</p>

<p>2.If not,i notice that ADF supports <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-google-cloud-storage"" rel=""nofollow noreferrer"">google could storage</a>.So you could try to sync your data into it and use copy activity.</p>
"
"59420307","How can i check Data sync group from SSMS?","<p>I have seen some  Azure DataSync tables and procedures in my Database.I'm not sure from where which database they are loaded. Is there any way to check the data sync group from SSMS? </p>
","<sql-server><azure><azure-sql-database><azure-web-app-service><azure-data-factory>","2019-12-20 05:34:31","217","1","1","59424657","<p>The following query tells you which database is the hub database. Remove the SyncGroupName on the WHERE clause to bring information of all sync groups.</p>

<pre><code>SELECT ud.server as HubServer, ud.[database] as HubDatabase, [db_schema] 
FROM [dss].[syncgroup] as sg 
INNER JOIN [dss].[userdatabase] as ud on sg.hub_memberid = ud.id 
LEFT JOIN [dss].[syncgroupmember] as m on sg.id = m.syncgroupid 
WHERE [db_schema] IS NOT NULL AND sg.name = 'syncGoupName' 
</code></pre>

<p>The following query tells you all member databases participating in the sync. Remove the SyncGroupName on the WHERE clause to bring information of all sync groups.</p>

<pre><code>SELECT ud2.[server] as MemberServer ,ud2.[database] as MemberDatabase, [db_schema] 
FROM [dss].[syncgroup] as sg 
LEFT JOIN [dss].[syncgroupmember] as m on sg.id = m.syncgroupid 
LEFT JOIN [dss].[userdatabase] as ud2 on m.databaseid = ud2.id 
WHERE [db_schema] IS NOT NULL AND sg.name = 'syncGoupName' 
</code></pre>

<p>List all stored procedures related to SQL Data Sync.</p>

<pre><code>SELECT 'DataSync.' + name FROM sys.procedures WHERE schema_id = SCHEMA_ID('DataSync')
</code></pre>

<p>List all tracking tables used by SQL Data Sync.</p>

<pre><code>SELECT '['+TABLE_SCHEMA+'].['+ TABLE_NAME + ']' as FullTableName, TABLE_NAME 
FROM INFORMATION_SCHEMA.TABLES 
WHERE TABLE_NAME LIKE '%_dss_tracking'
</code></pre>
"
"59412849","Azure Data factory and Data flow taking too much time to process data from staging to Database","<p>So I have one data factory which runs every day, and it selects data from oracle on-premise database around 80M records and moves it to parquet file, which is taking around 2 hours I want to speed up this process... also the data flow process which insert and update data in db</p>

<p>parquet file setting </p>

<p><a href=""https://i.stack.imgur.com/tvBxt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tvBxt.png"" alt=""taking to much time enter image description here""></a></p>

<p>Next step is from parquet file it call the data flow which move data as upsert to database but this also taking too much time </p>

<p>data flow  Setting </p>

<p><a href=""https://i.stack.imgur.com/HY8iT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HY8iT.png"" alt=""enter image description here""></a></p>

<p>Let me know which compute type for data flow </p>

<p>Memory Optimized 
Computed Optimized 
General Purpose</p>

<p><a href=""https://i.stack.imgur.com/xXGuU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xXGuU.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/q2EcW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q2EcW.png"" alt=""enter image description here""></a></p>

<p><strong>After Round Robin</strong>  Update</p>

<p><a href=""https://i.stack.imgur.com/ZslNo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZslNo.png"" alt=""enter image description here""></a></p>

<p>Sink Time </p>

<p><a href=""https://i.stack.imgur.com/407ra.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/407ra.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2019-12-19 15:39:15","1771","1","3","59430009","<p>Can you open the monitoring detailed execution plan for the data flow? Click on each stage in your data flow and look to see where the bulk of the time is being spent. You should see on the top of the view how much time was spent setting-up the compute environment, how much time was taken to read your source, and also check the total write time on your sinks.</p>

<p>I have some examples of how to view and optimize this <a href=""https://www.youtube.com/watch?v=QasncXQjLow"" rel=""nofollow noreferrer"">here</a>.</p>
"
"59412849","Azure Data factory and Data flow taking too much time to process data from staging to Database","<p>So I have one data factory which runs every day, and it selects data from oracle on-premise database around 80M records and moves it to parquet file, which is taking around 2 hours I want to speed up this process... also the data flow process which insert and update data in db</p>

<p>parquet file setting </p>

<p><a href=""https://i.stack.imgur.com/tvBxt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tvBxt.png"" alt=""taking to much time enter image description here""></a></p>

<p>Next step is from parquet file it call the data flow which move data as upsert to database but this also taking too much time </p>

<p>data flow  Setting </p>

<p><a href=""https://i.stack.imgur.com/HY8iT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HY8iT.png"" alt=""enter image description here""></a></p>

<p>Let me know which compute type for data flow </p>

<p>Memory Optimized 
Computed Optimized 
General Purpose</p>

<p><a href=""https://i.stack.imgur.com/xXGuU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xXGuU.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/q2EcW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q2EcW.png"" alt=""enter image description here""></a></p>

<p><strong>After Round Robin</strong>  Update</p>

<p><a href=""https://i.stack.imgur.com/ZslNo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZslNo.png"" alt=""enter image description here""></a></p>

<p>Sink Time </p>

<p><a href=""https://i.stack.imgur.com/407ra.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/407ra.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2019-12-19 15:39:15","1771","1","3","59461467","<p>Well, I would surmise that 45 min to stuff 85M files into a SQL DB is not horrible.  You can break the task down into chunks and see what's taking the longest time to complete.  Do you have access to Databricks?  I do a lot of pre-processing with Databricks, and I have found Spark to be super-super-fast!!  If you can pre-process in Databricks and push everything into your SQL world, you may have an optimal solution there.</p>
"
"59412849","Azure Data factory and Data flow taking too much time to process data from staging to Database","<p>So I have one data factory which runs every day, and it selects data from oracle on-premise database around 80M records and moves it to parquet file, which is taking around 2 hours I want to speed up this process... also the data flow process which insert and update data in db</p>

<p>parquet file setting </p>

<p><a href=""https://i.stack.imgur.com/tvBxt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tvBxt.png"" alt=""taking to much time enter image description here""></a></p>

<p>Next step is from parquet file it call the data flow which move data as upsert to database but this also taking too much time </p>

<p>data flow  Setting </p>

<p><a href=""https://i.stack.imgur.com/HY8iT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HY8iT.png"" alt=""enter image description here""></a></p>

<p>Let me know which compute type for data flow </p>

<p>Memory Optimized 
Computed Optimized 
General Purpose</p>

<p><a href=""https://i.stack.imgur.com/xXGuU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xXGuU.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/q2EcW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q2EcW.png"" alt=""enter image description here""></a></p>

<p><strong>After Round Robin</strong>  Update</p>

<p><a href=""https://i.stack.imgur.com/ZslNo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZslNo.png"" alt=""enter image description here""></a></p>

<p>Sink Time </p>

<p><a href=""https://i.stack.imgur.com/407ra.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/407ra.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2019-12-19 15:39:15","1771","1","3","59496874","<p>As per the documentation - <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-performance#partitioning-on-sink"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-performance#partitioning-on-sink</a> can you try modifying your <code>partition</code> settings under <code>Optimize</code> tab of your Sink ?</p>

<p>I faced similar issue with the default partitioning setting, where the data load was taking close to 30+ mins for 1M records itself, after changing the partition strategy to <code>round robin</code> and provided number of partitions as <code>5</code> (for my case) load is happening in less than a min. </p>

<p>Try experimenting with both Source partition (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-performance#partitioning-on-source"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-performance#partitioning-on-source</a>) &amp; Sink partition settings to come up with the optimum strategy. That should improve the data load time</p>
"
"59410120","Azure Batch Service cannot find Az.DataFactory cmdlets when run as a Custom Activity","<p>I am attempting to automate the activation of SSIS Integration Runtimes by running a pipeline containing a Custom Activity in Azure Data Factory.</p>

<p>I have set the Batch Service up with a linked storage account and have successfully started to run a .ps1 file in the linked storage account. I know it find the file OK because I can see a node is running and I get an adfjob set of logs in my storage account.</p>

<p>The Powershell script is a simple one liner:</p>

<pre><code>Start-AzDataFactoryV2IntegrationRuntime -Name SSIS -ResourceGroupName &lt;RG Name&gt; -DataFactoryName &lt;ADF Name&gt; -Force
</code></pre>

<p>However, the output log file says that it cannot find the cmdlet:</p>

<blockquote>
  <p>The term 'Start-AzDataFactoryV2IntegrationRuntime' is  not recognized
  as the name of a cmdlet, function, script file, or operable program.
  Check the  spelling of the name, or if a path was included, verify
  that the path is correct and try again.</p>
</blockquote>

<p>So I take it from the log that Powershell is available on the node but the <code>Az</code> module is not.  I find this extremely surprising given it's an Azure Batch Service node.  I've tried adding an <code>Install-Module Az ...</code> to the start of this script, the result is it appears to be hanging and I don't know how to track if it is doing anything or not, but in any case I cancelled after 8mins because I'm pretty sure it would have installed by then.</p>

<p>So I am therefore wondering where the Az module should be installed and how to go about doing so?</p>
","<azure><powershell><azure-data-factory><azure-batch>","2019-12-19 12:46:45","285","0","1","59420363","<p>You could install the <a href=""https://learn.microsoft.com/en-us/powershell/azure/new-azureps-module-az?view=azps-2.8.0"" rel=""nofollow noreferrer""><code>Az</code></a> module with your Batch <a href=""https://learn.microsoft.com/en-us/azure/batch/batch-api-basics#start-task"" rel=""nofollow noreferrer""><strong>Start task</strong></a>  in order for your task to use it.</p>

<blockquote>
  <p>By associating a start task with a pool, you can prepare the operating environment of its nodes. For example, you can perform actions such as installing the applications that your tasks run, or starting background processes.</p>
</blockquote>
"
"59409491","Why Azure Data Factory when we have Polybase","<p>Our requirement is to take data from Blob Storage and convert to other tabular form. This can be achieved with Sql DW using polybase. What really is the role of Azure data factory in such cases? </p>

<p>I know the same objective can be met with Azure data factory. But isn't Azure DW with Polybase the easiest option and cost effective? </p>
","<azure-data-factory><azure-synapse>","2019-12-19 12:08:02","4341","3","2","59409997","<p>Polybase can only really do one thing - load data to Azure Synapse Analytics (formerly known as Azure SQL Data Warehouse) or SQL Server with Polybase enabled.  More precisely, Polybase acts as a virtualisation layer for flat files stored in storage or data lake allowing them to be presented in the database as external tables or make them available for load into the database as a physical table, eg via CTAS.</p>

<p>It does that very well and is the recommended way to load data into Synapse / Warehouse assuming you select the right DWU and resource class for your data load.</p>

<p>Polybase has a couple of nice extra features around rejecting error rows and a few different file types / separators (comma, pipe etc), but really that’s it.</p>

<p>So if all you need to do is load some files or put a virtualisation layer on, then yes you could just use Polybase.</p>

<p>However let me ask you a few questions:  </p>

<ul>
<li>how do you intend to do your orchestration?</li>
<li>how do you intend to do your scheduling?  Event or time-based?</li>
<li>how do you intend to notify someone when the job is complete or errors?</li>
<li>how do you plan to start multiple loads simultaneously?  Azure Data Factory (ADF) has a great For Each task which can execute up to 20 tasks in parallel for example</li>
<li>do you want to run some stored procedures before or after the load?  Or any other ELT activities?</li>
</ul>

<p>Hopefully that helps explain the place of Polybase which is normally at least a piece of the puzzle.  You might look at more lightweight options than ADF like Logic Apps, Azure Run Books etc if you have a more lightweight ELT process.</p>

<p>HTH</p>
"
"59409491","Why Azure Data Factory when we have Polybase","<p>Our requirement is to take data from Blob Storage and convert to other tabular form. This can be achieved with Sql DW using polybase. What really is the role of Azure data factory in such cases? </p>

<p>I know the same objective can be met with Azure data factory. But isn't Azure DW with Polybase the easiest option and cost effective? </p>
","<azure-data-factory><azure-synapse>","2019-12-19 12:08:02","4341","3","2","59410028","<p>On the face if it, sure just use Polybase, it's simple.  It uses TSQL.  But, how much data are we talking about?  Different formats? Will it need cleaning or transformation?  As those needs grow, the answer may change.  If you have a lot of data to transform, maybe you don't want to spend your DW CPU cycles on transformation instead of querying?  So,like most thing here the answer... depends. You could use Polybase, SSIS, ADF, HDInsight, etc... James Serra does a good job laying out all the options. 
 <a href=""https://www.jamesserra.com/archive/2019/01/what-product-to-use-to-transform-my-data/"" rel=""nofollow noreferrer"">https://www.jamesserra.com/archive/2019/01/what-product-to-use-to-transform-my-data/</a></p>
"
"59407504","Azure Data factory Copy data Blob to Cosmos db - need help skipping 2MB files","<p>I have a Azure Data Factory Copy Activity within a pipeline - I'm copying data from Blob container /jsons in multiple virtual folders/ to Cosmos DB. However, fringe cases exist and cannot be escaped, where files larger than 2MB are placed in the Blob storage. When the copy activity picks them, the transfer /and subsequent pipeline activities/ fail as I hit the 2MB hard limit for CosmosDB.
I have tried setting up a lookup activity / get metadata but can't seem to address properly the relevant (size) property and the output necessary for the delete activity.</p>

<p>Can anyone advise on a an approach on how to handle this? </p>

<p>Thank you.</p>
","<azure><azure-data-factory>","2019-12-19 10:14:08","195","0","1","59419067","<p>It should be possible to get the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity#metadata-options"" rel=""nofollow noreferrer"">size</a> of files in Get Metadata activity.But please note it is in bytes and only could be applied on the file.</p>

<p><a href=""https://i.stack.imgur.com/ouxcE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ouxcE.png"" alt=""enter image description here""></a></p>

<p>As i know,no way to avoid 2mb limitation of cosmos db document.You could refer to this case:<a href=""https://stackoverflow.com/questions/58653153/what-is-the-size-limit-of-a-single-document-stored-in-azure-cosmos-db"">What is the size limit of a single document stored in Azure Cosmos DB</a></p>
"
"59405300","How do we create a generic mapping dataflow in datafactory that will dynamically extract data from different tables with different schema?","<p>I am trying to create a azure datafactory mapping dataflow that is generic for all tables. I am going to pass table name, the primary column for join purpose and other columns to be used in groupBy and aggregate functions as parameters to the DF.
<a href=""https://i.stack.imgur.com/DZ6mj.png"" rel=""nofollow noreferrer"">parameters to df</a></p>

<p>I am unable to refernce this parameter in groupBy
<a href=""https://i.stack.imgur.com/wV5BB.png"" rel=""nofollow noreferrer"">Error: DF-AGG-003 - Groupby should reference atleast one column -
MapDrifted1 aggregate(
) ~> Aggregate1,[486 619]</a></p>

<p>Has anyone tried this scenario? Please help if you have some knowledge on this or if it can be handled in u-sql script.</p>
","<azure><u-sql><azure-data-factory>","2019-12-19 07:46:45","554","2","1","59430192","<p>We need to first lookup your parameter string name from your incoming source data to locate the metadata and assign it.</p>

<p>Just add a Derived Column previous to your Aggregate and it will work. Call the column 'groupbycol' in your Derived Column and use this formula: byName($group1).</p>

<p>In your Agg, select 'groupbycol' as your groupby column.</p>
"
"59399491","Is there a way to set different permissions to pipelines and datasets in the same Azure Data Factory?","<p>I'm trying to set RBAC to the components of a Data Factory so every different user group can work just with the pipeline they have to. Is there a way to achieve this?</p>

<p>Thank you very much in advance!</p>
","<azure><data-science><azure-data-factory><azure-cloud-services><azure-rbac>","2019-12-18 20:22:52","1409","1","1","59400283","<p>Here is a list of the <a href=""https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles"" rel=""nofollow noreferrer"">built in roles</a>. I don't see what you are looking for exactly. It might be worth looking at creating a custom role specific for what you are looking for.</p>

<p>Most likely would need to build a custom role looking at which <a href=""https://learn.microsoft.com/en-us/azure/templates/microsoft.datafactory/2018-06-01/factories/dataflows"" rel=""nofollow noreferrer"">features of Data Factory</a> you'd like to grant permissions to.</p>

<p>And then <a href=""https://learn.microsoft.com/en-us/azure/role-based-access-control/custom-roles"" rel=""nofollow noreferrer"">create a custom role</a> which can be deploy via ARM or Powershell.</p>
"
"59399063","Consuming files from external network at scale in Azure data pipeline","<p>What is the best way to consume files from an external network at a scale? The goal here is to build data pipeline in Azure using Data Factory, ADLS and Databricks which can scale for almost real time consumption for analytics purpose.</p>
","<architecture><analytics><azure-data-factory><azure-data-lake><azure-databricks>","2019-12-18 19:43:19","47","-1","1","59484712","<p>As i know, you could use Databricks Notebook Activity in ADF pipeline:<a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-databricks-notebook"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-databricks-notebook</a></p>

<p>Then you could configure min workers and max workers in databricks cluster.Please see <a href=""https://docs.databricks.com/clusters/configure.html#how-autoscaling-behaves"" rel=""nofollow noreferrer"">How autoscaling behaviors</a>:</p>

<p><a href=""https://i.stack.imgur.com/YMBbI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YMBbI.png"" alt=""enter image description here""></a></p>
"
"59398935","How to handle failures in parallel copy activities in ADF v2?","<p>I'm planning on iterating over a list of connection strings to upsert data from one source to multiple sinks in parallel. So, I'll abstract the copy activity and parameterize the connection string value here. The main pipeline will then iterate over the connection strings and pass on the value to the abstracted copy activity.</p>

<p><strong>Scenario</strong>: One of those parallel copy activities fails. How to handle the individual failures?</p>

<ul>
<li><p><strong>Option 1</strong>: Abort all the parallel pipelines in order to maintain my
sinks in a consistent state.  Is this possible to do in Azure data
factory? </p></li>
<li><p><strong>Option 2</strong>: If I choose to let the other pipelines write data
      to the sink, will I have to handle the failed slices manually? I'm
      choosing a tumbling window trigger for the main pipeline here.</p></li>
</ul>

<p>Please help me gain clarity over handling it in the best way possible.</p>
","<azure><azure-data-factory>","2019-12-18 19:32:22","1202","1","1","59405355","<p>A best practice is to have your activities in foreach loop in the separate pipeline, so then you call execute pipeline activity in your foreach. 
That way each execution will be treated separately and in monitoring view, you will have a clear overview of which one failed.</p>
"
"59397506","Azure Data Factory Copy Data pipeline just stuck at In Progress status; but not even loading any data","<p>Really confused to what is going on.</p>

<p><strong>Source:</strong><br/> 
Azure Table Storage - I have done the ""Preview Data"" and I can see the data. Added couple of filters in <strong>Query</strong> window as shown in the description.<br/> 
<strong>Target</strong><br/> 
Azure SQL Server<br/> 
<strong>Mapping</strong><br/> 
Successful<br/> 
<strong>Pipeline Status</strong><br/> 
In Progress for last 15 minutes. I don't mind it but it hasn't loaded any data in the SQL destination so far
<a href=""https://i.stack.imgur.com/AYX51.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AYX51.png"" alt=""Image of the Pipeline""></a>
<a href=""https://i.stack.imgur.com/f5JFO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f5JFO.png"" alt=""enter image description here""></a></p>
","<azure><azure-sql-database><azure-data-factory>","2019-12-18 17:42:50","1560","1","2","59398079","<p>If you click on the pipeline name, it will drill down to the activity level monitoring and you can see the details for each activity</p>

<p><a href=""https://i.stack.imgur.com/xdJM3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xdJM3.png"" alt=""Activity level monitoring""></a></p>

<p>Once you click on the details button, it will show you the copy info</p>

<p><a href=""https://i.stack.imgur.com/M0tBk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M0tBk.png"" alt=""Copy info""></a></p>

<p>If this didn't help, please provide your pipeline run id and/or activity runid and I'll take a look in our logs.</p>

<p>Thanks,
Paco</p>
"
"59397506","Azure Data Factory Copy Data pipeline just stuck at In Progress status; but not even loading any data","<p>Really confused to what is going on.</p>

<p><strong>Source:</strong><br/> 
Azure Table Storage - I have done the ""Preview Data"" and I can see the data. Added couple of filters in <strong>Query</strong> window as shown in the description.<br/> 
<strong>Target</strong><br/> 
Azure SQL Server<br/> 
<strong>Mapping</strong><br/> 
Successful<br/> 
<strong>Pipeline Status</strong><br/> 
In Progress for last 15 minutes. I don't mind it but it hasn't loaded any data in the SQL destination so far
<a href=""https://i.stack.imgur.com/AYX51.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AYX51.png"" alt=""Image of the Pipeline""></a>
<a href=""https://i.stack.imgur.com/f5JFO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f5JFO.png"" alt=""enter image description here""></a></p>
","<azure><azure-sql-database><azure-data-factory>","2019-12-18 17:42:50","1560","1","2","59527563","<p>I think the error is happened in your Source dataset query:</p>

<p>When you first choose the table storage as Source dataset, you can see all the data with <code>Preview data</code>, for example:
<a href=""https://i.stack.imgur.com/qQtCO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qQtCO.png"" alt=""enter image description here""></a></p>

<p>But when you add the query filter, the data will changed in <code>Preview data</code>:</p>

<p>I added the filter: <code>PartitionKey eq '1' and RowKey eq '1'</code></p>

<p>Click the <code>Preview data</code> again to check if the filter works:
<a href=""https://i.stack.imgur.com/5mCga.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5mCga.png"" alt=""enter image description here""></a> </p>

<p>The filtered data is the data which will be transfer to Azure SQL.</p>

<p>As you know, Table storage only support you query with <code>PartitionKey</code>, <code>RowKey</code> and <code>Timestamp</code>, there isn't column like <code>DSP_Status</code>.</p>

<p>That means you query <code>DSP_Status eq '***' and Timestamp ge datatime '2019-05-01'</code> is wrong, the filter result must be null.</p>

<p>That's why no data is loaded to the SQL destination.</p>

<p>Please see:<a href=""https://learn.microsoft.com/en-in/azure/storage/tables/table-storage-design-for-query"" rel=""nofollow noreferrer"">Design for querying</a></p>

<p>Hope this helps.</p>
"
"59393196","Dataset not getting created via Powershell for type=AzureSqlMITable","<p>Issue: error on ADF when trying to create ADF Components such as dataset for AzureSQLMITable via powershell </p>

<p>Analysis:</p>

<p>Error is reproducable on BuildServer (run via DevOps) &amp; locally via Windows PowerShell.</p>

<p>Error is not reproducible in Azure Cloudshell &amp; Powershell core with same set of commands</p>

<p>Error on ADF for the dataset:</p>

<blockquote>
  <p>Could not load resource #datasetname. Please ensure no mistakes in the JSON and that referenced resources exist. Status: UnknownError, Possible reason: Fetch failed for named: dataset$#datasetname. Adapter not found. Type: dataset.</p>
</blockquote>

<p>If manually pasted the file(jsonfile) in ADF it works as expected without error</p>

<p>Expected resolution: How to make it work with WindowsPowershell?</p>

<p>Json file:</p>

<pre><code>{
    ""name"": ""#datasetname"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""&lt;connection name&gt;"",
            ""type"": ""LinkedServiceReference""
        },
        ""annotations"": [],
        ""type"": ""AzureSqlMITable"",
        ""schema"": [],
         ""typeProperties"": {

             ""tableName"": {
                ""value"": ""&lt;StoredProcedure_Name_Name&gt;"",
                ""type"": ""Expression""
            }
        }  
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p>Powershell commands:</p>

<pre><code>Connect-AzureRmAccount

$BaseFolder=&lt;FilePath&gt;

$file = Get-ChildItem $BaseFolder -Recurse -Include *.json -Filter #somefilter -ErrorAction Stop

Set-AzureRmDataFactoryV2Dataset -DataFactoryName &lt;datafactoryname&gt; -Name ($file.BaseName) -ResourceGroupName &lt;resourcegroupname&gt; -DefinitionFile $file.FullName -Force -ErrorAction Stop 
</code></pre>
","<azure><powershell><azure-sql-database><azure-data-factory>","2019-12-18 13:35:15","226","1","1","59558599","<p>Resolved with Az Commands using powershell core commands(via Azure Powershell task in CICD instead of Powershell script which was Windows one).</p>

<p>The point here is that henceforth people should use Powershell core(Az) as it will have new feature and windows powershell(AzureRm) will only have bug fixes</p>
"
"59382234","Expression of type 'String' does not match the field 'expression' on If Condition in ADFv2","<p>I'm adding an If Condition to my ADFv2 Pipeline based on the output of an activity.  I'm simply checking if a file is downloaded and if so I want to load the table with the downloaded file.</p>

<p>My Expression looks like this:</p>

<pre><code>@{equals(activity('RunFunction').output.files[1].Downloaded,true)}
</code></pre>

<p>And I'm getting the following warning:</p>

<pre><code>Expression of type 'String' does not match the field 'expression' 
</code></pre>

<p>How do I update my code to remove this warning?</p>
","<azure><azure-data-factory>","2019-12-17 21:19:21","15725","3","1","59386498","<p>Why you have to add <code>{ }</code> in your expression? Please remove it,<code>@equals(A,B)</code> is correct format.</p>

<pre><code>@equals(activity('Azure Function1').output.files[1].Downloaded,true)
</code></pre>
"
"59378581","Adfv2 reference child pipeline variable in master pipeline","<p>I have a pipeline that executes another pipeline in azure data factory v2. In the executed (child) pipeline I assign a value to a variable I want returned in the master pipeline, is this possible?
Thanks</p>
","<variables><pipeline><azure-data-factory>","2019-12-17 16:41:25","909","1","1","59381766","<p>Pipelines are independent entities - while you can execute ""child"" pipelines, there is no functional connection between the two. One way around this is to have the child pipeline write the value to some form of intermediate storage (blob storage, a SQL table, etc), and then have the ""parent"" pipeline read the value after the child pipeline completes. You should also make sure the Execute Pipeline activity has the ""Wait on completion"" property checked. If you don't want the value retained in the storage medium, you could have the parent pipeline delete the data once it has processed it.</p>
"
"59359219","Where is the compression happening with Azure Data Factory?","<p>If you have a pipeline with a copy data activity, which copies data from an on-prem SQL server to an azure data lake storage account - and you choose some form of compression.</p>

<p><strong>Where is the compression happening?</strong> Is the data transfered over the internet to the cloud, and then compressed. Or does it get compressed on-prem by the self-hosted integration runtime?</p>
","<azure><azure-data-factory>","2019-12-16 15:02:24","289","0","1","59359434","<blockquote>
  <p>Where is the compression happening?</p>
</blockquote>

<p>The compression happens on the self-hosted Integration Runtime.</p>

<p>If it is a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance#how-staged-copy-works"" rel=""nofollow noreferrer"">staged copy</a>, the IR will compress the data when writing to the intermediate blob storage account.  And if it's not a staged copy, the IR writes the files directly to ADLS.</p>
"
"59358188","Pass output from one pipeline run and use as parameter in another pipeline","<p>The way my ADF setup currently works, is that I have multiple pipelines, each containing atleast one activity. Then I have one big pipeline that sort of chains these pipelines together.</p>

<p>However, now in the big ""master"" pipeline, I would like to use the output of an activity from one pipeline and then pass it to another pipeline. All of this orchestrated from the ""master"" pipeline.</p>

<p>My ""master"" pipeline would look something like this:</p>

<p><a href=""https://i.stack.imgur.com/pSjHT.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/pSjHT.png"" alt=""enter image description here""></a></p>

<p>What I have tried to do is adding a parameter to ""Execute Pipeline2"", and I have tried passing:</p>

<pre><code>@activity('Execute Pipeline1').output.pipeline.runId.output.runOutput
@activity('Execute Pipeline1').output.pipelineRunId.output.runOutput
@activity('Execute Pipeline1').output.runOutput
</code></pre>

<p>How would one go about doing this?</p>
","<azure-data-factory>","2019-12-16 14:00:52","3955","6","1","59361569","<p>unfortunately we don't have a way to pass the output of an activity across pipelines. Right now pipelines don't have outputs (only activities).
We have a workitem that will allow a user to choose what should be the output for a pipeline (imagine a pipeline with 40 activities, user would be able to choose the output of activity 3 as pipeline output). However, this workitem is in very early stages so don't expect to see this soon.</p>

<p>For now, the only way would be to save the output that you want in storage (blob, for example) and then read it and pass it to the other pipeline. Another method could be a web activity that gets the pipeline run (passing run id) and you get the output using ADF SDK or REST API, and then you pass that to the next Execute Pipeline activity.</p>
"
"59344959","Getting error on null and empty string while copying a csv file from blob container to Azure SQL DB","<p>I tried all combination on the datatype of my data but each time my data factory pipeline is giving me this error:</p>

<p>{
    ""errorCode"": ""2200"",
    ""message"": ""ErrorCode=UserErrorColumnNameNotAllowNull,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Empty or Null string found in Column Name 2. Please make sure column name not null and try again.,Source=Microsoft.DataTransfer.Common,'"",
    ""failureType"": ""UserError"",
    ""target"": ""xxx"",
    ""details"": []
}</p>

<p>My Copy data source code is something like this:<code>{
    ""name"": ""xxx"",
    ""description"": ""uuu"",
    ""type"": ""Copy"",
    ""dependsOn"": [],
    ""policy"": {
        ""timeout"": ""7.00:00:00"",
        ""retry"": 0,
        ""retryIntervalInSeconds"": 30,
        ""secureOutput"": false,
        ""secureInput"": false
    },
    ""userProperties"": [],
    ""typeProperties"": {
        ""source"": {
            ""type"": ""DelimitedTextSource"",
            ""storeSettings"": {
                ""type"": ""AzureBlobStorageReadSettings"",
                ""recursive"": true,
                ""wildcardFileName"": ""*""
            },
            ""formatSettings"": {
                ""type"": ""DelimitedTextReadSettings""
            }
        },
        ""sink"": {
            ""type"": ""AzureSqlSink""
        },
        ""enableStaging"": false,
        ""translator"": {
            ""type"": ""TabularTranslator"",
            ""mappings"": [
                {
                    ""source"": {
                        ""name"": ""populationId"",
                        ""type"": ""Guid""
                    },
                    ""sink"": {
                        ""name"": ""PopulationID"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""inputTime"",
                        ""type"": ""DateTime""
                    },
                    ""sink"": {
                        ""name"": ""inputTime"",
                        ""type"": ""DateTime""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""inputCount"",
                        ""type"": ""Decimal""
                    },
                    ""sink"": {
                        ""name"": ""inputCount"",
                        ""type"": ""Decimal""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""inputBiomass"",
                        ""type"": ""Decimal""
                    },
                    ""sink"": {
                        ""name"": ""inputBiomass"",
                        ""type"": ""Decimal""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""inputNumber"",
                        ""type"": ""Decimal""
                    },
                    ""sink"": {
                        ""name"": ""inputNumber"",
                        ""type"": ""Decimal""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""utcOffset"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""utcOffset"",
                        ""type"": ""Int32""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""fishGroupName"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""fishgroupname"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""yearClass"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""yearclass"",
                        ""type"": ""String""
                    }
                }
            ]
        }
    },
    ""inputs"": [
        {
            ""referenceName"": ""DelimitedTextFTDimensions"",
            ""type"": ""DatasetReference""
        }
    ],
    ""outputs"": [
        {
            ""referenceName"": ""AzureSqlTable1"",
            ""type"": ""DatasetReference""
        }
    ]
}</code>
Can anyone please help me understand the issue. I see in some blogs they ask me use treatnullasempty but I am not allowed to modify the JSON. is there a way to do that??</p>
","<azure-data-factory><azure-blob-storage>","2019-12-15 14:43:57","609","0","2","59369678","<p>I suggest to using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"" rel=""nofollow noreferrer"">Data Flow DerivedColumn</a>, DerivedColumn can help you build expression to replace the null column.</p>

<p>For example:
<a href=""https://i.stack.imgur.com/Pan2z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Pan2z.png"" alt=""enter image description here""></a></p>

<p>Derived Column, if <code>Column_2</code> is null =<code>true</code>, return 'dd' :</p>

<pre><code>iifNull(Column_2,'dd')
</code></pre>

<p><a href=""https://i.stack.imgur.com/rsob9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rsob9.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/eZxs2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eZxs2.png"" alt=""enter image description here""></a></p>

<p>Mapping the column
<a href=""https://i.stack.imgur.com/RwyJd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RwyJd.png"" alt=""enter image description here""></a></p>

<p>Reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions"" rel=""nofollow noreferrer"">Data transformation expressions in mapping data flow</a></p>

<p>Hope this helps.</p>
"
"59344959","Getting error on null and empty string while copying a csv file from blob container to Azure SQL DB","<p>I tried all combination on the datatype of my data but each time my data factory pipeline is giving me this error:</p>

<p>{
    ""errorCode"": ""2200"",
    ""message"": ""ErrorCode=UserErrorColumnNameNotAllowNull,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Empty or Null string found in Column Name 2. Please make sure column name not null and try again.,Source=Microsoft.DataTransfer.Common,'"",
    ""failureType"": ""UserError"",
    ""target"": ""xxx"",
    ""details"": []
}</p>

<p>My Copy data source code is something like this:<code>{
    ""name"": ""xxx"",
    ""description"": ""uuu"",
    ""type"": ""Copy"",
    ""dependsOn"": [],
    ""policy"": {
        ""timeout"": ""7.00:00:00"",
        ""retry"": 0,
        ""retryIntervalInSeconds"": 30,
        ""secureOutput"": false,
        ""secureInput"": false
    },
    ""userProperties"": [],
    ""typeProperties"": {
        ""source"": {
            ""type"": ""DelimitedTextSource"",
            ""storeSettings"": {
                ""type"": ""AzureBlobStorageReadSettings"",
                ""recursive"": true,
                ""wildcardFileName"": ""*""
            },
            ""formatSettings"": {
                ""type"": ""DelimitedTextReadSettings""
            }
        },
        ""sink"": {
            ""type"": ""AzureSqlSink""
        },
        ""enableStaging"": false,
        ""translator"": {
            ""type"": ""TabularTranslator"",
            ""mappings"": [
                {
                    ""source"": {
                        ""name"": ""populationId"",
                        ""type"": ""Guid""
                    },
                    ""sink"": {
                        ""name"": ""PopulationID"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""inputTime"",
                        ""type"": ""DateTime""
                    },
                    ""sink"": {
                        ""name"": ""inputTime"",
                        ""type"": ""DateTime""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""inputCount"",
                        ""type"": ""Decimal""
                    },
                    ""sink"": {
                        ""name"": ""inputCount"",
                        ""type"": ""Decimal""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""inputBiomass"",
                        ""type"": ""Decimal""
                    },
                    ""sink"": {
                        ""name"": ""inputBiomass"",
                        ""type"": ""Decimal""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""inputNumber"",
                        ""type"": ""Decimal""
                    },
                    ""sink"": {
                        ""name"": ""inputNumber"",
                        ""type"": ""Decimal""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""utcOffset"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""utcOffset"",
                        ""type"": ""Int32""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""fishGroupName"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""fishgroupname"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""yearClass"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""yearclass"",
                        ""type"": ""String""
                    }
                }
            ]
        }
    },
    ""inputs"": [
        {
            ""referenceName"": ""DelimitedTextFTDimensions"",
            ""type"": ""DatasetReference""
        }
    ],
    ""outputs"": [
        {
            ""referenceName"": ""AzureSqlTable1"",
            ""type"": ""DatasetReference""
        }
    ]
}</code>
Can anyone please help me understand the issue. I see in some blogs they ask me use treatnullasempty but I am not allowed to modify the JSON. is there a way to do that??</p>
","<azure-data-factory><azure-blob-storage>","2019-12-15 14:43:57","609","0","2","59371373","<p>fixed it.it was a easy fix as one of my column in destination was marked as not null, i changed it as null and it worked.</p>
"
"59342602","write data to text file in azure data factory version 2","<p>It's seem ADF v2 does not support writing data to TEXT file (.TXT).</p>
<ol>
<li><p>After select File System</p>
<p><img src=""https://i.stack.imgur.com/dSThe.png"" alt=""FileSystem"" /></p>
</li>
<li><p>But don't see TextFormat at the next screen</p>
<p><img src=""https://i.stack.imgur.com/z8qF0.png"" alt=""TextFormat "" /></p>
</li>
</ol>
<p>So do we any method to write data to TEXT file ?</p>
<p>Thanks,
Thai</p>
","<azure><azure-data-factory>","2019-12-15 09:21:18","5325","0","2","59357984","<p>I think what you are looking for is DelimitedText dataset. You can specify extension as part of the file name</p>
"
"59342602","write data to text file in azure data factory version 2","<p>It's seem ADF v2 does not support writing data to TEXT file (.TXT).</p>
<ol>
<li><p>After select File System</p>
<p><img src=""https://i.stack.imgur.com/dSThe.png"" alt=""FileSystem"" /></p>
</li>
<li><p>But don't see TextFormat at the next screen</p>
<p><img src=""https://i.stack.imgur.com/z8qF0.png"" alt=""TextFormat "" /></p>
</li>
</ol>
<p>So do we any method to write data to TEXT file ?</p>
<p>Thanks,
Thai</p>
","<azure><azure-data-factory>","2019-12-15 09:21:18","5325","0","2","59366477","<p>Data Factory only support these 6 file formats:
<a href=""https://i.stack.imgur.com/urqFe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/urqFe.png"" alt=""enter image description here""></a></p>

<p>Please see: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs"" rel=""nofollow noreferrer"">Supported file formats and compression codecs in Azure Data Factory</a>.</p>

<p>If we want to write data to a txt file, the only format we can using is Delimited text, when the pipeline finished, you will get a txt file. </p>

<p>Reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-delimited-text"" rel=""nofollow noreferrer"">Delimited text</a>: Follow this article when you want to parse the delimited text files or write the data into delimited text format.</p>

<p>For example, I create a pipeline to copy data from Azure SQL to Blob, choose DelimitedText format as Sink dataset:
<a href=""https://i.stack.imgur.com/Qf0u5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qf0u5.png"" alt=""enter image description here""></a></p>

<p>The txt file I get in Blob Storeage:</p>

<p><a href=""https://i.stack.imgur.com/gOp3P.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gOp3P.png"" alt=""enter image description here""></a></p>

<p>Hope this helps</p>
"
"59341935","Easiest way to fail pipeline in Data Factory?","<p>I have a data factory pipeline that has an &quot;If Condition&quot; Activity and I want the pipeline to fail on a certain condition. What is the best way to achieve this? There is no fail activity..</p>
","<azure-data-factory>","2019-12-15 07:18:27","1356","3","2","59349606","<p><strong>Update September 2021: There is now a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-fail-activity?"" rel=""nofollow noreferrer"">Fail</a> activity in ADF.</strong></p>
<p>First of all, please vote for this <a href=""https://feedback.azure.com/d365community/idea/2d90df95-7226-ec11-b6e6-000d3a4f032c"" rel=""nofollow noreferrer"">feedback</a>. This is a common need.</p>
<p>There are several workarounds such as a Web Activity to throw an error by trying to connect to https://ThrowAnError or a Stored Procedure activity which executes a <code>raiserror</code> function in Azure SQL Database. But that’s as close as we can get to solving your problem currently.</p>
"
"59341935","Easiest way to fail pipeline in Data Factory?","<p>I have a data factory pipeline that has an &quot;If Condition&quot; Activity and I want the pipeline to fail on a certain condition. What is the best way to achieve this? There is no fail activity..</p>
","<azure-data-factory>","2019-12-15 07:18:27","1356","3","2","69731682","<p>They've just added a &quot;Fail Activity&quot; in Data Factory and Synapse Analytics:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-fail-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-fail-activity</a></p>
<p>It's found in the &quot;General&quot; category in &quot;Activities&quot; in the Data Factory Studio.</p>
"
"59329300","Updating Azure Table Storage Field in Azure Data Factory","<p>I have a Pipeline in Azure Data Factory.
The Pipeline Includes 3 Activities.
1. Lookup ---> Get Content from Table Storage
2. Web Activity --> Call a REST Endpoint (parameter is passed from the result of the lookup activity)
3. Copy Activity --> Copy the REST response payload into CSV.</p>

<p>The thing is I need to flag the Table Storage Row as Success or Fail based on the Web Activity if it returned Response 200 or not.</p>

<p>So my question is that is there any Activity in Azure Data Factory that lets you update a Table storage field.</p>
","<azure><azure-table-storage><azure-data-factory>","2019-12-13 20:18:19","963","0","2","59349591","<p>According my experience,  there isn't any Active in Data Factory support update a Table storage field for now.</p>

<p>Hope this helps.</p>
"
"59329300","Updating Azure Table Storage Field in Azure Data Factory","<p>I have a Pipeline in Azure Data Factory.
The Pipeline Includes 3 Activities.
1. Lookup ---> Get Content from Table Storage
2. Web Activity --> Call a REST Endpoint (parameter is passed from the result of the lookup activity)
3. Copy Activity --> Copy the REST response payload into CSV.</p>

<p>The thing is I need to flag the Table Storage Row as Success or Fail based on the Web Activity if it returned Response 200 or not.</p>

<p>So my question is that is there any Activity in Azure Data Factory that lets you update a Table storage field.</p>
","<azure><azure-table-storage><azure-data-factory>","2019-12-13 20:18:19","963","0","2","59350130","<p>Sorry I don't have enough reputation for comment, that why I am writing in answer section.</p>

<p>Because there is no activity such as update a Table storage field, so this could be help you to solve your problem.
With <code>Update Entity</code> you can updates an existing entity in a table.</p>

<p>Ref : <a href=""https://learn.microsoft.com/en-us/rest/api/storageservices/update-entity2"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/storageservices/update-entity2</a></p>

<p>Hope this is helpful. </p>
"
"59323766","How to fix the data factory v2 adf_publish branch being out of sync with the master branch in azure devops","<p>Recently I ran into the issue with not being able to publish in azure data factory integrated with azure devops/git. This happened because we tried using powershell to automatically create pipelines based on a json template. When this is done in the data factory using <strong>Set-AzDataFactoryV2Pipeline</strong>, you by-pass the azure devops integration and the pipeline gets published right away without any commits or pull requests. Below is the error message</p>

<p><strong>Publishing Error</strong></p>

<p><em>The publish branch is out of sync with the collaboration branch. This is likely due to publishing outside of Git mode. To recover from this state, please refer to our Git troubleshooting guide</em></p>
","<azure-devops><azure-data-factory>","2019-12-13 13:42:04","9761","3","4","59323767","<p>The MS GIT troubleshooting guide suggests some hardcore measures to resolve this out-of-sync issues (by deleting and re-creating the repo I believe). In this case, there's an easier and less hardcore way of solving this.</p>

<p>You simply need to:</p>

<ul>
<li>Create a new branch from your master branch in data factory</li>
<li>Create the same pipeline you created via Set-AzDataFactoryV2Pipeline</li>
<li>Create a pull request and merge it into master</li>
</ul>

<p>Voila, you'll hopefully be able to publish again as it now will consider the branches to be in sync again</p>
"
"59323766","How to fix the data factory v2 adf_publish branch being out of sync with the master branch in azure devops","<p>Recently I ran into the issue with not being able to publish in azure data factory integrated with azure devops/git. This happened because we tried using powershell to automatically create pipelines based on a json template. When this is done in the data factory using <strong>Set-AzDataFactoryV2Pipeline</strong>, you by-pass the azure devops integration and the pipeline gets published right away without any commits or pull requests. Below is the error message</p>

<p><strong>Publishing Error</strong></p>

<p><em>The publish branch is out of sync with the collaboration branch. This is likely due to publishing outside of Git mode. To recover from this state, please refer to our Git troubleshooting guide</em></p>
","<azure-devops><azure-data-factory>","2019-12-13 13:42:04","9761","3","4","61482678","<p>Micosoft now provides guidance on resolving this issue:</p>

<p>From:  <a href=""https://learn.microsoft.com/en-us/azure/data-factory/source-control#stale-publish-branch"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/source-control#stale-publish-branch</a></p>

<blockquote>
  <h1>Stale publish branch</h1>
  
  <p>If the publish branch is out of sync with the
  master branch and contains out-of-date resources despite a recent
  publish, try following these steps:</p>
  
  <ul>
  <li>Remove your current Git repository </li>
  <li>Reconfigure Git with the same
  settings, but make sure Import existing Data Factory resources to
  repository is selected and choose New branch </li>
  <li>Create a pull request to
  merge the changes to the collaboration branch</li>
  </ul>
</blockquote>
"
"59323766","How to fix the data factory v2 adf_publish branch being out of sync with the master branch in azure devops","<p>Recently I ran into the issue with not being able to publish in azure data factory integrated with azure devops/git. This happened because we tried using powershell to automatically create pipelines based on a json template. When this is done in the data factory using <strong>Set-AzDataFactoryV2Pipeline</strong>, you by-pass the azure devops integration and the pipeline gets published right away without any commits or pull requests. Below is the error message</p>

<p><strong>Publishing Error</strong></p>

<p><em>The publish branch is out of sync with the collaboration branch. This is likely due to publishing outside of Git mode. To recover from this state, please refer to our Git troubleshooting guide</em></p>
","<azure-devops><azure-data-factory>","2019-12-13 13:42:04","9761","3","4","61603004","<p>remove your git repo from data factory and create a new with exact same setting.
Go to azure devops and create a new pull request to merge new branch into master.</p>

<p>Link: <a href=""https://www.datastackpros.com/2020/05/how-to-fix-data-factory-adfpublish.html"" rel=""nofollow noreferrer"">https://www.datastackpros.com/2020/05/how-to-fix-data-factory-adfpublish.html</a></p>
"
"59323766","How to fix the data factory v2 adf_publish branch being out of sync with the master branch in azure devops","<p>Recently I ran into the issue with not being able to publish in azure data factory integrated with azure devops/git. This happened because we tried using powershell to automatically create pipelines based on a json template. When this is done in the data factory using <strong>Set-AzDataFactoryV2Pipeline</strong>, you by-pass the azure devops integration and the pipeline gets published right away without any commits or pull requests. Below is the error message</p>

<p><strong>Publishing Error</strong></p>

<p><em>The publish branch is out of sync with the collaboration branch. This is likely due to publishing outside of Git mode. To recover from this state, please refer to our Git troubleshooting guide</em></p>
","<azure-devops><azure-data-factory>","2019-12-13 13:42:04","9761","3","4","74545156","<p>under manage -&gt; git configuration -&gt; over write live mode. Use this option this will reset the data factory with the live code.</p>
"
"59320936","Creating daily to monthly trigger dependency in ADFv2","<p>I have a tumbling window trigger <strong>trg-Daily</strong> that has a start date boundary at 12:00AM and it executes once every 24 hours (frequency is <strong>Hour</strong> and interval is <strong>24</strong>). So basically it executes a daily job. Now I a pipeline that I want to execute once a month -- so basically I want to create another tumbling window trigger <strong>trg-Monthly</strong> that will depend on the daily trigger <strong>trg-Daily</strong> in such a way that it should execute when all the days in a given month are processed. How to define this? Specifically I would like to know what should be the following values of the trigger <strong>trg-Monthly</strong> to make it happen?</p>

<ol>
<li>frequency</li>
<li>interval</li>
<li>window Size (for dependsOn section)</li>
<li>offset ( for dependsOn section)</li>
</ol>

<p>This used to happen easily out of the box in ADFv1 , not sure how to implement that in ADFv2. </p>
","<azure-data-factory>","2019-12-13 10:45:31","398","1","1","59368490","<p>It seems that monthly trigger is not supported in ADF v2 so far(never get any response from official team yet now) based on this <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/33869644-adfv2-tumbling-window-trigger-month-and-year-fre"" rel=""nofollow noreferrer"">feedback</a>.</p>

<p>Just provide a workaround here,using <a href=""https://learn.microsoft.com/en-us/azure/connectors/connectors-native-recurrence"" rel=""nofollow noreferrer"">Recurrence Schedule trigger</a> in Logic App service which supports monthly trigger:</p>

<p><a href=""https://i.stack.imgur.com/cpyha.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cpyha.png"" alt=""enter image description here""></a></p>

<p>Then use ADF connector action to trigger your specific pipeline:</p>

<p><a href=""https://i.stack.imgur.com/84PiM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/84PiM.png"" alt=""enter image description here""></a></p>
"
"59318859","ADF v2 - Web Activity- POST output not retrievable","<p>With Azure Data Factory v2, I created Web Activity using the POST method and got the desired response output.
But can't get the rows data from the output response in the next activity.</p>

<p>How do I reference columns in the rows in this output?
The data in the rows doesn't have any headers.</p>

<pre><code>{
""Tables"": [
    {
        ""TableName"": ""Table_0"",
        ""Columns"": [
            {
                ""ColumnName"": ""MyFieldA"",
                ""DataType"": ""String"",
                ""ColumnType"": ""string""
            },
            {
                ""ColumnName"": ""MyFieldB"",
                ""DataType"": ""String"",
                ""ColumnType"": ""string""
            }
        ],
        ""Rows"": [
            [
                ""ABCDEF"",
                ""AAAABBBBBCCCDDDDD""
            ],
            [
                ""CCCCCCC"",
                ""CCCCCCC""
            ],
</code></pre>

<p>I can't reference the value in the rows 
I've tried numerous things
e.g. @activity('WebActivity').output.Rows</p>

<p>Nothing seems to work.
What's the point of getting a response from a web activity and then not being able to reference the output in data factory?</p>
","<azure-data-factory>","2019-12-13 08:40:19","338","0","2","59327702","<p>For your example.</p>

<p>@activity('WebActivity').output.Tables[0].Rows
will return the following:</p>

<pre><code>[
    [
        ""ABCDEF"",
        ""AAAABBBBBCCCDDDDD""
    ],
    [
        ""CCCCCCC"",
        ""CCCCCCC""
    ]
]
</code></pre>

<p>If you want to access even deeper, you just need to specify the index.
@activity('WebActivity').output.Tables[0].Rows[0][0] will return ABCDEF</p>

<p>If you need to automate this, you can have a pattern of foreach with an execute pipeline inside passing an array as a parameter until you get to the properties that you need.</p>
"
"59318859","ADF v2 - Web Activity- POST output not retrievable","<p>With Azure Data Factory v2, I created Web Activity using the POST method and got the desired response output.
But can't get the rows data from the output response in the next activity.</p>

<p>How do I reference columns in the rows in this output?
The data in the rows doesn't have any headers.</p>

<pre><code>{
""Tables"": [
    {
        ""TableName"": ""Table_0"",
        ""Columns"": [
            {
                ""ColumnName"": ""MyFieldA"",
                ""DataType"": ""String"",
                ""ColumnType"": ""string""
            },
            {
                ""ColumnName"": ""MyFieldB"",
                ""DataType"": ""String"",
                ""ColumnType"": ""string""
            }
        ],
        ""Rows"": [
            [
                ""ABCDEF"",
                ""AAAABBBBBCCCDDDDD""
            ],
            [
                ""CCCCCCC"",
                ""CCCCCCC""
            ],
</code></pre>

<p>I can't reference the value in the rows 
I've tried numerous things
e.g. @activity('WebActivity').output.Rows</p>

<p>Nothing seems to work.
What's the point of getting a response from a web activity and then not being able to reference the output in data factory?</p>
","<azure-data-factory>","2019-12-13 08:40:19","338","0","2","59332063","<p>Thanks Pacodel!!! You've helped me out.
And to use in the a For Each Loop and Array, when I pass in the Rows to my Execute pipeline activity @activity('WebActivity').output.Tables[0].Rows:</p>

<pre><code>[
   [
    ""ABCDEF"",
    ""AAAABBBBBCCCDDDDD""
   ],
   [
    ""CCCCCCC"",
    ""CCCCCCC""
   ]
]
</code></pre>

<p>I can use the following to reference the rows:</p>

<pre><code>@{item()[0]}
@{item()[1]}
</code></pre>

<p>I use the @item to populate parameters in a stored procedure activity which loads my table</p>

<p>Thanks</p>
"
"59307337","Azure Data Factory - Azure SQL Database as JSON API to Power BI","<p>I want to us Azure Data Factory for Data Management and Link it with Power BI Desktop. </p>

<p>I am new to Azure and I don't have an in depth tech background, so this might be a quite trivial question. However, I plan to do the following in several stages and for Stage 1 I want: </p>

<p>STAGE 1</p>

<p>1.1 Connect 1 Azure DB that consists of one table to my Data Factory.</p>

<p>1.2 If possible represent this table as JSON and have an API that lets me connect with Power BI desktop.</p>

<p>Stage 2: </p>

<p>2.1 Connect multiple Azure DBs to ADF, transform and then</p>

<p>2.2 = 1.2 (do the same as above 1.2)</p>

<p>Stage 3 to Stage n: 
- I want to create everything as ""flexible"" as possible since I don't know where exactly this project will go. </p>

<p>I am also open to any suggestions that would do the trick better then how I picture it. Open for any suggestion on how to pull it off. </p>

<p>Many thanks in advance! I appreciate your time and effort</p>

<p>Best, 
MLU</p>
","<azure><powerbi><etl><azure-data-factory>","2019-12-12 14:58:22","214","0","1","59307940","<p>I'm going to try to address some of your issues, but for most StackOverflow questions, you are probably going to need a bit more focused technical request.</p>

<p>First - there is no connection between Azure Data Factory (ADF) and Power BI (PBI). ADF is an ETL/ELT orchestration tool, so its primary function is to organize data operations. </p>

<p>Second - ADF connecting to databases (or other data sources, like Blob Storage) is a primary capability. This is done via a mechanism called Linked Services (found under the Connections section in the web UI). For your request, you would create such a connection to your SQL table and one to a destination (like Blob Storage) for the transformed JSON.</p>

<p>Third - there are several ways to perform such a transform. The easiest one to get started with is the Copy activity, which reads from a ""Source"" (your DB) and writes to a ""Sink"" (JSON in Blob Storage). In ADF, the definition of the source and sink data structure/format is known as a ""DataSet"". Much of the control for this kind of operation is defined by the underlying DataSets.</p>

<p>Fourth - Copy activity is limited in the amount of transformation it can perform. When you hit those boundaries, you'll want to investigate Azure Data Factory Data Flows (ADFDF). Data Flows provide a visual wrapper around Azure Data Bricks, and are well beyond the scope of this answer. Suffice it to say they give you far more capabilities to address complex problems.</p>

<p>PBI is a consumer - it reads data sources and does stuff with it. ADF is not a data source, hence no real connection between the two. As a data consumer, PBI can easily read your DB(s), so the real question is why would you want/need to convert it to JSON in the first place? It feels like you are going down an unnecessary rabbit hole: unless you have a particular need for a JSON transformation, I would bypass that altogether.</p>
"
"59305255","Create a Data Pipeline in Azure","<p>I've a class which make some extract, transform an load to a dataset located in a different JSON files.</p>

<p>This process work Ok. But, I've the necessity to process manually every month. I submitt an spark application in intelliJ (and submit an Scalla Singleton Object with the transformation)</p>

<p>So, I'm trying to automate this process. But, I didn't find documentation or a tutorial to known what is the best service to accomplish this objective.</p>

<p>The processs Should:</p>

<ol>
<li>Create a HDInsight Spark Cluster</li>
<li>Run The process (An Scala Class)</li>
<li>Delete the HDInsight Spark Cluster created before</li>
</ol>

<p>I've searched but the links I find (looking for ""Create on demand HD insight spark cluster"") are the following:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/48165947/access-datalake-from-azure-datafactory-v2-using-on-demand-hd-insight-cluster"">Access datalake from Azure datafactory V2 using on demand HD Insight
cluster</a></li>
<li><a href=""https://stackoverflow.com/questions/43545182/how-to-create-azure-on-demand-hd-insight-spark-cluster-using-data-factory"">How to create Azure on demand HD insight Spark cluster    using Data
Factory</a></li>
</ul>

<p>Other options I've searched:</p>

<ul>
<li><a href=""https://blogs.technet.microsoft.com/dominiquevives/2018/07/02/host-and-run-your-powershell-scripts-in-azure/"" rel=""nofollow noreferrer"">Host and run your PowerShell scripts in Azure</a></li>
<li>Azure Logic Apps</li>
<li>Azure Automation</li>
</ul>

<p>Thanks!</p>
","<azure><powershell><apache-spark><azure-data-factory>","2019-12-12 13:03:09","105","0","1","59318978","<p>Here are the process which you want to </p>

<ol>
<li>Create a HDInsight Spark Cluster</li>
</ol>

<p>Using power shell it should be easy to create HDInsight cluster, here is a sample code:</p>

<pre><code>### Create a Spark 2.3 cluster in Azure HDInsight

# Default cluster size (# of worker nodes), version, and type
$clusterSizeInNodes = ""1""
$clusterVersion = ""3.6""
$clusterType = ""Spark""

# Create the resource group
$resourceGroupName = Read-Host -Prompt ""Enter the resource group name""
$location = Read-Host -Prompt ""Enter the Azure region to create resources in, such as 'Central US'""
$defaultStorageAccountName = Read-Host -Prompt ""Enter the default storage account name""

New-AzResourceGroup -Name $resourceGroupName -Location $location

# Create an Azure storage account and container
# Note: Storage account kind BlobStorage can only be used as secondary storage for HDInsight clusters.
New-AzStorageAccount `
    -ResourceGroupName $resourceGroupName `
    -Name $defaultStorageAccountName `
    -Location $location `
    -SkuName Standard_LRS `
    -Kind StorageV2 `
    -EnableHttpsTrafficOnly 1

$defaultStorageAccountKey = (Get-AzStorageAccountKey `
                                -ResourceGroupName $resourceGroupName `
                                -Name $defaultStorageAccountName)[0].Value

$defaultStorageContext = New-AzStorageContext `
                                -StorageAccountName $defaultStorageAccountName `
                                -StorageAccountKey $defaultStorageAccountKey

# Create a Spark 2.3 cluster
$clusterName = Read-Host -Prompt ""Enter the name of the HDInsight cluster""

# Cluster login is used to secure HTTPS services hosted on the cluster
$httpCredential = Get-Credential -Message ""Enter Cluster login credentials"" -UserName ""admin""

# SSH user is used to remotely connect to the cluster using SSH clients
$sshCredentials = Get-Credential -Message ""Enter SSH user credentials"" -UserName ""sshuser""

# Set the storage container name to the cluster name
$defaultBlobContainerName = $clusterName

# Create a blob container. This holds the default data store for the cluster.
New-AzStorageContainer `
    -Name $clusterName `
    -Context $defaultStorageContext

$sparkConfig = New-Object ""System.Collections.Generic.Dictionary``2[System.String,System.String]""
$sparkConfig.Add(""spark"", ""2.3"")

# Create the HDInsight cluster
New-AzHDInsightCluster `
    -ResourceGroupName $resourceGroupName `
    -ClusterName $clusterName `
    -Location $location `
    -ClusterSizeInNodes $clusterSizeInNodes `
    -ClusterType $clusterType `
    -OSType ""Linux"" `
    -Version $clusterVersion `
    -ComponentVersion $sparkConfig `
    -HttpCredential $httpCredential `
    -DefaultStorageAccountName ""$defaultStorageAccountName.blob.core.windows.net"" `
    -DefaultStorageAccountKey $defaultStorageAccountKey `
    -DefaultStorageContainer $clusterName `
    -SshCredential $sshCredentials

Get-AzHDInsightCluster `
    -ResourceGroupName $resourceGroupName `
    -ClusterName $clusterName
</code></pre>

<ol start=""2"">
<li>Run The process (An Scala Class)</li>
</ol>

<p>You can refer this link to submit an application job remotely to the Spark cluster:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-create-standalone-application#run-the-application-on-the-apache-spark-cluster"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-create-standalone-application#run-the-application-on-the-apache-spark-cluster</a> </p>

<ol start=""3"">
<li>Delete the HDInsight Spark Cluster created before</li>
</ol>

<p>Cleaning up the cluster , you can achieve it using powershell, here is a sample code for the same;</p>

<pre><code># Removes the specified HDInsight cluster from the current subscription.
Remove-AzHDInsightCluster `
    -ResourceGroupName $resourceGroupName `
    -ClusterName $clusterName

# Removes the specified storage container.
Remove-AzStorageContainer `
    -Name $clusterName `
    -Context $defaultStorageContext

# Removes a Storage account from Azure.
Remove-AzStorageAccount `
    -ResourceGroupName $resourceGroupName `
    -Name $defaultStorageAccountName

# Removes a resource group.
Remove-AzResourceGroup `
    -Name $resourceGroupName
</code></pre>

<p>Additional reference:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-jupyter-spark-sql-use-powershell"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-jupyter-spark-sql-use-powershell</a></p>

<p><a href=""https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/data-factory/v1/data-factory-build-your-first-pipeline-using-powershell.md"" rel=""nofollow noreferrer"">https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/data-factory/v1/data-factory-build-your-first-pipeline-using-powershell.md</a></p>

<p>Hope it helps.</p>
"
"59292922","Azure Data Factory - Limit the number of Databricks pipeline running at the same time","<p>I am using ADF to execute Databricks notebook. At this time, I have 6 pipelines, and they are executed consequently.</p>

<p>Specifically, after the former is done, the latter is executed with multiple parameters by the loop box, and this keeps going. For example, after the first pipeline is done, it will trigger 3 instances of the second pipeline with different parameters, and each of these instances will trigger multiple instances of the third pipeline. As a result, the deeper I go, the more pipelines I have to run.</p>

<p>The issue with me is: when each pipeline is executed, it will ask Databricks to allocate a cluster to run. However, Databricks limits the number of cores to be used for each workspace, which causes the pipeline instance to fail to run.</p>

<p>My question is: is there any solution to control the number of pipeline instance running at the same time, or any solution to handle my issue?</p>

<p>Thanks in advance :-)</p>
","<azure><azure-data-factory><azure-databricks>","2019-12-11 19:40:11","3615","2","2","59299170","<blockquote>
  <p>Why this issue occurs?</p>
</blockquote>

<p><strong>Note:</strong> Creating a Databricks clusters always have a dependency with the number of cores available in the subscription.</p>

<blockquote>
  <p>Before creating any databricks cluster, make sure number of cores are
  available in the region selected and the VM Family vCPUs.</p>
</blockquote>

<p>You can checkout the core limit of your subscription by going to <strong>Azure Portal</strong> => <strong>Subscriptions</strong> => <strong>Select your subscription</strong> => Settings “<strong>Usage + quotes</strong>” => Checkout the usage quota available for each regions.</p>

<p><a href=""https://i.stack.imgur.com/23NXx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/23NXx.png"" alt=""enter image description here""></a></p>

<p><strong>Example:</strong> If your subscription has <strong>> 72 cores</strong> which results in <strong>success</strong> of ADF runs else results in <strong>failure</strong>.</p>

<pre><code>Activity Validate failed: Databricks execution failed with error message: Unexpected failure while waiting for the cluster to be ready. Cause Unexpected state for cluster (job-200-run-1):  Could not launch cluster due to cloud provider failures. azure_error_code: OperationNotAllowed, azure_error_message: Operation results in exceeding quota limits of Core. Maximum allowed: 350, Current in use: 344
</code></pre>

<p>I’m trying to create 6 pipelines with databricks clusters with 2 worker nodes each. Which means it requires</p>

<p><strong>(6 pipelines) * (1 Driver Node + 2 Worker Node) * (4 cores) = 72 cores</strong>. </p>

<p>Above calculation used with <strong>VM Size Standard_DS3_v2 which has 4 cores</strong>. </p>

<blockquote>
  <p><strong>Note:</strong> To create a databricks spark cluster which requires more than 4 cores i.e. (Minimum 4 cores for Driver type and 4 cores for
  Worker type).</p>
</blockquote>

<p><strong>Resolutions for this issue:</strong></p>

<ol>
<li>Increase the core limits by raising the ticket to billing and subscription team to higher limit. With this option once you use it you will be charged for the used cores only.</li>
<li>Limit your job frequency so that limited number of clusters/ consider using  a single job for copying multiple file so that you can limit the cluster creations which will exhaust your cores under your subscription.</li>
</ol>

<p>To request an increase of one or more resources that support such an increase, submit an 
<a href=""https://ms.portal.azure.com/#blade/Microsoft_Azure_Support/HelpAndSupportBlade/newsupportrequest"" rel=""nofollow noreferrer"">Azure support request</a> (select ""Quota"" for Issue type).</p>

<p><strong>Issue type:</strong> Service and subscription limits (quotas)</p>

<p><strong>Reference:</strong> <a href=""https://learn.microsoft.com/en-us/azure/azure-supportability/regional-quota-requests"" rel=""nofollow noreferrer"">Total regional vCPU limit increases</a></p>

<p>Hope this helps. Do let us know if you any further queries.</p>

<hr>

<p>Do click on ""Mark as Answer"" and Upvote on the post that helps you, this can be beneficial to other community members.</p>
"
"59292922","Azure Data Factory - Limit the number of Databricks pipeline running at the same time","<p>I am using ADF to execute Databricks notebook. At this time, I have 6 pipelines, and they are executed consequently.</p>

<p>Specifically, after the former is done, the latter is executed with multiple parameters by the loop box, and this keeps going. For example, after the first pipeline is done, it will trigger 3 instances of the second pipeline with different parameters, and each of these instances will trigger multiple instances of the third pipeline. As a result, the deeper I go, the more pipelines I have to run.</p>

<p>The issue with me is: when each pipeline is executed, it will ask Databricks to allocate a cluster to run. However, Databricks limits the number of cores to be used for each workspace, which causes the pipeline instance to fail to run.</p>

<p>My question is: is there any solution to control the number of pipeline instance running at the same time, or any solution to handle my issue?</p>

<p>Thanks in advance :-)</p>
","<azure><azure-data-factory><azure-databricks>","2019-12-11 19:40:11","3615","2","2","59868203","<p>You can limit number of activities being run in parallel at each foreach level by setting - Batch Count Parameter. ( Found under settings tab on foreach loop)</p>

<p><strong>batchCount- Batch count to be used for controlling the number of parallel execution (when isSequential is set to false).</strong></p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity</a></p>

<p>If not able to set limit at overall Pipeline level, try arriving at a minimum value of batch count in your each nested foreach loops.</p>
"
"59288923","I'm having issues with DevOps production deployment - Unable to edit or replace deployment","<p>Up until yesterday morning I was able to deploy data factory v2 changes in my release pipeline. Then last night during deployment I received an error that the connection was forced closed. Now when I try to deploy to the production environment, I get this error: ""Unable to edit or replace deployment 'ArmTemplate_18': previous deployment from '12/10/2019 10:19:27 PM' is still active (expiration time is '12/17/2019 10:19:23 PM')"". Am I supposed to wait a week for this error to clear itself?</p>
","<azure-devops><azure-data-factory>","2019-12-11 15:18:49","648","2","1","59297348","<p>This message indicates that there’s another deployment going on, with the same name, in the same ARM Resource Group. In order to perform your new deployment, you’ll need to either:</p>

<ul>
<li>Wait for the existing deployment to complete</li>
<li>Stop the in-progress / active deployment</li>
</ul>

<p>You can stop an active deployment by using the <strong><a href=""https://learn.microsoft.com/en-us/powershell/module/azurerm.resources/stop-azurermresourcegroupdeployment?view=azurermps-6.13.0"" rel=""nofollow noreferrer"">Stop-AzureRmResourceGroupDeployment</a></strong> PowerShell command or the azure group deployment stop command in the xPlat CLI tool. Please refer to this <a href=""https://stackoverflow.com/questions/32222691/cancelling-azure-resource-manager-deployment"">case</a>.</p>

<p>Or you can open target Resource Group on the azure portal, go to Deployment tab, find not completed deployments, cancel it, start new deploy. You can refer to this <a href=""https://github.com/Azure/service-fabric-mesh-preview/issues/257"" rel=""nofollow noreferrer"">issue</a> for details.</p>

<p>In addition, there is a recently <a href=""https://status.dev.azure.com/_history"" rel=""nofollow noreferrer"">event</a> of availability degradation of Azure DevOps .This could also have an impact. Now the engineers have mitigated this event. </p>
"
"59276032","How to know if Web activity(ADF V2) ran Analysis Services models process succesfully?","<p>I found a tutorial that guided me on how to request an Analysis Services process by using Web Activity requesting an HTTP/REST post method, calling the Logic App service.
<a href=""https://marczak.io/posts/2019/06/logic-apps-refresh-analysis-services/"" rel=""nofollow noreferrer"">https://marczak.io/posts/2019/06/logic-apps-refresh-analysis-services/</a></p>

<p>However, I need the activity to wait for the end of the processing so we can proceed with the rest of the flow (once it is asynchronous it jumps to the next step). </p>

<p>Can someone help or provide me with any alternative?</p>

<p>I appreciate it!</p>
","<azure-data-factory><azure-logic-apps><azure-analysis-services>","2019-12-10 22:02:56","51","0","1","59305119","<p>My preferred approach is to use native Web Activities in ADF rather than adding Logic Apps to the architecture. This <a href=""https://github.com/furmangg/automating-azure-analysis-services/blob/master/README.md#processazureas"" rel=""nofollow noreferrer"">sample</a> includes an Until loop to wait until the processing succeeds. </p>
"
"59273831","How to unzip files from a zipped folder at FTP location using ADFv2","<p>I have a zipped folder at FTP location and want to unzip it at blob location. But how to achieve the same? I have no idea on it. Please tell me hows to achieve the same.</p>
","<azure><azure-data-factory>","2019-12-10 19:07:06","145","0","1","59280521","<p>Please reference this tutorial: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-ftp"" rel=""nofollow noreferrer"">Copy data from FTP server by using Azure Data Factory</a>.</p>

<p>This article outlines how to copy data from FTP server.</p>

<p>Specifically, this FTP connector supports:</p>

<ul>
<li>Copying files using Basic or Anonymous authentication.</li>
<li>Copying files as-is or parsing files with the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs"" rel=""nofollow noreferrer"">supported file formats
and compression codecs</a>.</li>
</ul>

<p>Azure Data Factory supports compress/decompress zip data during copy. Legacy compress model are still supported as-is for backward compatibility.</p>

<p><a href=""https://i.stack.imgur.com/sPNoT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sPNoT.png"" alt=""enter image description here""></a></p>

<p>You can follow this steps:</p>

<p><a href=""https://i.stack.imgur.com/KH3KB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KH3KB.png"" alt=""enter image description here""></a></p>

<p>Choose the file <code>Compress type</code>, Data Factory will help unzip it.
<a href=""https://i.stack.imgur.com/6h4WY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6h4WY.png"" alt=""enter image description here""></a></p>

<p>Reference tutorial:</p>

<ol>
<li><a href=""https://www.red-gate.com/simple-talk/cloud/cloud-data/using-copy-wizard-azure-data-factory/"" rel=""nofollow noreferrer"">Using the Copy Wizard for the Azure Data Factory</a></li>
<li><a href=""https://www.future-processing.pl/blog/moving-data-processing/"" rel=""nofollow noreferrer"">Moving data processing to the cloud – part 1 – Azure Data
Factory</a></li>
</ol>

<p>Hope this helps.</p>
"
"59261747","Log Pipeline Activity to Database after execution","<p>Currently, we are monitoring DataFlow Taks using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-visually"" rel=""nofollow noreferrer"">Visual Link</a>
How to implement the log analysis and operations management to get desired Monitor pipeline runs and Monitor activity runs</p>

<p>Using ADF-  Need Activity Runs information Inserted into a database table for each pipeline, enabling to create power bi report and share with monitoring team with my own charts and kpis</p>

<p>Kindly provide the Gitcode or steps to move the Monitor pipeline /activity  runs into the database.
Also, is there any quick way to export the result to excel</p>
","<azure><azure-data-factory><azure-log-analytics><log-analysis><azure-oms>","2019-12-10 06:53:49","460","0","1","59453029","<p>According to your description,i think the ADF <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor"" rel=""nofollow noreferrer"">azure monitor</a> is your appropriate option. With Monitor, you can route diagnostic logs for analysis. Data Factory stores pipeline-run data for only 45 days.You have 3 options to store them:</p>

<p>1.Storage Account</p>

<p>2.Event hub</p>

<p>3.Log Analytics</p>

<p>You said your desired destination is database table.So, if the source data resides in blob storage then use ADF copy activity or <a href=""https://cn.bing.com/search?q=azure%20function%20blob%20trigger&amp;PC=U531&amp;cvid=c211b26a9bb94442a2681136d0a3bc7a&amp;FORM=ANNTA9"" rel=""nofollow noreferrer"">Azure Blob Storage Trigger Function</a>.If it is in the event hub,you could transfer them into db via <a href=""https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-inputs"" rel=""nofollow noreferrer"">ASA Job</a>.</p>
"
"59255137","How to pass JSON into an Azure Function with embedded dynamic content in Azure Data Factory V2","<p>In ADFv2 I'm looking up a date and passing it to an Azure Function.  I can pass just the data like so:</p>

<pre><code>@activity('GetLastDateProcessed').output.firstRow.LastDateProcessed
</code></pre>

<p>However if I embed this into a JSON string like this:</p>

<pre><code>{""lastProcessDate"":""@activity('GetLastDateProcessed').output.firstRow.LastDateProcessed""}
</code></pre>

<p>I get this {""lastProcessDate"":""@activity('GetLastDateProcessed').output.firstRow.LastDateProcessed""} instead of {""lastProcessDate"":""2019-11-13""} as input into function.</p>

<p>Last I've tried to use a parameter with no success also.</p>

<pre><code>@concat('{""lastProcessDate"":""', string(pipeline().parameters.lastProcessDate), '""}')
</code></pre>

<p>The problem here is the parameter was not set.  I set the parameter like this: </p>

<pre><code>@activity('GetLastDateProcessed').output.firstRow.LastDateProcessed
</code></pre>

<p><a href=""https://i.stack.imgur.com/QgDvy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QgDvy.png"" alt=""enter image description here""></a>
However this is a default value and is never dynamically updated.  If I can update this string then the @concat method will work, but haven't been able to figure out how to dynamically update a parameter for the pipeline.  </p>

<p>Another option could be a pipeline variable, but I don't know how to reference the variable.</p>

<p>How do I concat strings together with dynamic content?</p>
","<azure><azure-functions><azure-data-factory>","2019-12-09 19:06:37","3912","4","3","59255626","<p>I was able to get this to work by creating a second pipeline.  This is not optimal, but works for people running into this same issue.  Hopefully someone finds a better solution than this!</p>

<p>From the first pipeline I set the second pipelines parameter with this:</p>

<pre><code>@activity('GetLastDateProcessed').output.firstRow.LastDateProcessed
</code></pre>

<p>I named the parameter in the second pipeline lastProcessDate so then this worked:</p>

<pre><code>@concat('{""lastProcessDate"":""', string(pipeline().parameters.lastProcessDate), '""}')
</code></pre>

<p>This is not straight forward and can't be how Microsoft is expecting us to solve this!</p>
"
"59255137","How to pass JSON into an Azure Function with embedded dynamic content in Azure Data Factory V2","<p>In ADFv2 I'm looking up a date and passing it to an Azure Function.  I can pass just the data like so:</p>

<pre><code>@activity('GetLastDateProcessed').output.firstRow.LastDateProcessed
</code></pre>

<p>However if I embed this into a JSON string like this:</p>

<pre><code>{""lastProcessDate"":""@activity('GetLastDateProcessed').output.firstRow.LastDateProcessed""}
</code></pre>

<p>I get this {""lastProcessDate"":""@activity('GetLastDateProcessed').output.firstRow.LastDateProcessed""} instead of {""lastProcessDate"":""2019-11-13""} as input into function.</p>

<p>Last I've tried to use a parameter with no success also.</p>

<pre><code>@concat('{""lastProcessDate"":""', string(pipeline().parameters.lastProcessDate), '""}')
</code></pre>

<p>The problem here is the parameter was not set.  I set the parameter like this: </p>

<pre><code>@activity('GetLastDateProcessed').output.firstRow.LastDateProcessed
</code></pre>

<p><a href=""https://i.stack.imgur.com/QgDvy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QgDvy.png"" alt=""enter image description here""></a>
However this is a default value and is never dynamically updated.  If I can update this string then the @concat method will work, but haven't been able to figure out how to dynamically update a parameter for the pipeline.  </p>

<p>Another option could be a pipeline variable, but I don't know how to reference the variable.</p>

<p>How do I concat strings together with dynamic content?</p>
","<azure><azure-functions><azure-data-factory>","2019-12-09 19:06:37","3912","4","3","62912830","<p>I think what you are missing is that when you use the at-sign '@' in the json string you should follow it with a curly bracket '{'</p>
<p>In your example it will look something like this:</p>
<pre><code>{&quot;lastProcessDate&quot;:&quot;@{activity('GetLastDateProcessed').output.firstRow.LastDateProcessed}&quot;}
</code></pre>
<p>here is the source (found it in the comments):
<a href=""https://azure.microsoft.com/en-us/blog/azure-functions-now-supported-as-a-step-in-azure-data-factory-pipelines/#:%7E:text=Azure%20Data%20Factory%20(ADF)%20is,in%20your%20data%20factory%20pipelines"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/blog/azure-functions-now-supported-as-a-step-in-azure-data-factory-pipelines/#:~:text=Azure%20Data%20Factory%20(ADF)%20is,in%20your%20data%20factory%20pipelines</a>.</p>
"
"59255137","How to pass JSON into an Azure Function with embedded dynamic content in Azure Data Factory V2","<p>In ADFv2 I'm looking up a date and passing it to an Azure Function.  I can pass just the data like so:</p>

<pre><code>@activity('GetLastDateProcessed').output.firstRow.LastDateProcessed
</code></pre>

<p>However if I embed this into a JSON string like this:</p>

<pre><code>{""lastProcessDate"":""@activity('GetLastDateProcessed').output.firstRow.LastDateProcessed""}
</code></pre>

<p>I get this {""lastProcessDate"":""@activity('GetLastDateProcessed').output.firstRow.LastDateProcessed""} instead of {""lastProcessDate"":""2019-11-13""} as input into function.</p>

<p>Last I've tried to use a parameter with no success also.</p>

<pre><code>@concat('{""lastProcessDate"":""', string(pipeline().parameters.lastProcessDate), '""}')
</code></pre>

<p>The problem here is the parameter was not set.  I set the parameter like this: </p>

<pre><code>@activity('GetLastDateProcessed').output.firstRow.LastDateProcessed
</code></pre>

<p><a href=""https://i.stack.imgur.com/QgDvy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QgDvy.png"" alt=""enter image description here""></a>
However this is a default value and is never dynamically updated.  If I can update this string then the @concat method will work, but haven't been able to figure out how to dynamically update a parameter for the pipeline.  </p>

<p>Another option could be a pipeline variable, but I don't know how to reference the variable.</p>

<p>How do I concat strings together with dynamic content?</p>
","<azure><azure-functions><azure-data-factory>","2019-12-09 19:06:37","3912","4","3","65293766","<p>I was able to achieve this with command.</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;storedprocedure&quot;:&quot;storedProcName&quot;,
  &quot;params&quot;:&quot;@{variables('currentDt')}&quot;
}
</code></pre>
"
"59249764","How to give the databricks filepath in data factory","<p><a href=""https://i.stack.imgur.com/qIm52.jpg"" rel=""nofollow noreferrer"">enter image description here</a><a href=""https://i.stack.imgur.com/nZJBV.jpg"" rel=""nofollow noreferrer"">I'm retrieving notebook path using stored procedure then executing that Databricks notebook using data factory but I'm getting error as message"": ""Invalid notebook_path: {\""effectiveIntegrationRuntime\"":\""DefaultIntegrationRuntime (West Europe)\"",\""executionDuration\"":0,\""durationInQueue\"":{\""integrationRuntimeQueue\"":1},\""billingReference\"":{\""activityType\"":\""ExternalActivity\"",\""billableDuration\"":{\""Managed\"":0.016666666666666666}}}. Only absolute paths are currently supported. Paths must begin with '/'.  . Please help me on this</a></p>
","<python><python-3.x><azure-data-factory><azure-databricks>","2019-12-09 13:18:36","960","0","2","59261936","<p>Make sure you have specified the notebook path in the Databricks activity.</p>

<p><a href=""https://i.stack.imgur.com/DSbhi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DSbhi.png"" alt=""enter image description here""></a></p>

<p>This is the excepted behaviour in Azure Data Factory.</p>

<p><strong>Error code:3201</strong></p>

<ul>
<li><strong>Message:</strong> Missing required field: settings.task.notebook_task.notebook_path.</li>
<li><strong>Cause:</strong> Bad authoring: Notebook path not specified correctly.</li>
<li><strong>Recommendation:</strong> Specify the notebook path in the Databricks activity.</li>
</ul>

<p>For more details, refer ""<a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-databricks-notebook"" rel=""nofollow noreferrer"">Run a Databricks notebook with the Databricks Notebook activity in Azure Data Factory</a>""</p>

<p>Hope this helps.</p>
"
"59249764","How to give the databricks filepath in data factory","<p><a href=""https://i.stack.imgur.com/qIm52.jpg"" rel=""nofollow noreferrer"">enter image description here</a><a href=""https://i.stack.imgur.com/nZJBV.jpg"" rel=""nofollow noreferrer"">I'm retrieving notebook path using stored procedure then executing that Databricks notebook using data factory but I'm getting error as message"": ""Invalid notebook_path: {\""effectiveIntegrationRuntime\"":\""DefaultIntegrationRuntime (West Europe)\"",\""executionDuration\"":0,\""durationInQueue\"":{\""integrationRuntimeQueue\"":1},\""billingReference\"":{\""activityType\"":\""ExternalActivity\"",\""billableDuration\"":{\""Managed\"":0.016666666666666666}}}. Only absolute paths are currently supported. Paths must begin with '/'.  . Please help me on this</a></p>
","<python><python-3.x><azure-data-factory><azure-databricks>","2019-12-09 13:18:36","960","0","2","59330875","<p>There are two pieces to this problem:</p>

<ol>
<li>Retrieving the returned value from the stored procedure</li>
<li>Using the returned value in the Databricks activity</li>
</ol>

<p>Part 1:
Please try using your stored proc from inside a Lookup Activity, instead of a Stored Proc Activity.  The difference is that a Lookup Activity expects data to be returned, and exposes that.  Stored Proc Activity does not require data to be returned, and so may not expose any returned values. (See the 'Stored Procedure' option under 'Use Query' in Settings of the Lookup Activity.)</p>

<p>To ease the development / troublshooting process, connect a Set Variable activity to the Lookup Activity.  The Lookup activity returns the data in an array, so lets use an array type variable. <code>@activity('Lookup1').output.value</code>.</p>

<p>An array is not suitable for use in the Databricks activity, so lets extract the path from the array.  Connect a second Set Variable activity to the first Set Variable activity, using string type variable.  Use a Debug run and inspect the output of the first Set Variable activity; determine if there is a key-value pair.  In my test case, I had a key-value pair where the key was the empty string.  <code>@string(first(variables('arrayoutput'))[''])</code></p>

<p>Part 2:
Now that the second variable contains the path, lets parameterize and use the Databricks activity.  If you path does not start with '/' then you will need to prepend onw with this dynamic expression <code>@concat('/',variables('singleoutput'))</code> in the Notebook path in Settings of the Databricks Notebook activity.</p>
"
"59246102","Azure Data Factory Copy Data schema mapping with sink Datalake","<p>I have a simple pipeline , with copydata. I am trying to pull data from sql to data lake . I have created two datasets one for sql and other for datalake. </p>

<p>Data lake schema are defined in dataset are overridden when we run the pipeline and columns have string data type.</p>

<p>I tried to change the schema data type here and after running the pipelines they are overridden in datasets.</p>

<p><a href=""https://i.stack.imgur.com/iG1kh.jpg"" rel=""nofollow noreferrer"">Schema Mapping in CopyData</a></p>

<p><a href=""https://i.stack.imgur.com/dBn3I.jpg"" rel=""nofollow noreferrer"">DataSet Schema definition</a></p>
","<azure><azure-data-factory>","2019-12-09 09:40:01","314","1","1","59262924","<p>When we copy data from SQL to Azure Data Lake Storage, no matter what the column data type is in SQL Source dataset, it all will change to String in Data Lake Storage Sink dataset. Sink dataset will create a flat file(txt, json and so on) to store the data.</p>

<p><a href=""https://i.stack.imgur.com/hOi5i.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hOi5i.png"" alt=""enter image description here""></a></p>

<p>For example, when you copy data from a csv file as Source dataset, import the schema from source file, all the column data types are String. We only can change these String column  data type during SQL Sink dataset schema mapping.</p>

<p>We can not not change the data types for datesets when using sink as Data lake Storage.</p>

<p>Hope this helps.</p>
"
"59240875","Manager Identity error when running a pipeline in Data Factory with Staging Blob Storage + Polybase to a Azure DataWarehouse","<p>I am getting this error message when trying to use Polybase + Enabling Staging Blob Storage to pull data from Azure SQL to Azure DW Database.</p>

<p>Managed Service Identity has not been enabled on this server. Please enable Managed Service Identity and try again.</p>

<p>I've tried to create a Managed Identity user assigning it to a web service and giving it permission linking this user to a AAD Group that has owner permission in the database, but it doesn't work.</p>

<p><a href=""https://i.stack.imgur.com/9il3L.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9il3L.png"" alt=""enter image description here""></a></p>
","<azure-data-factory><staging><azure-synapse><polybase>","2019-12-08 23:30:13","3217","0","3","59294472","<p>Is your sink , source and blob are part of the same subscription ? Since the error is complaining about the SQDWH access , I think you need to check this out . </p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse#managed-identity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse#managed-identity</a>.</p>

<p>Do let me know how it goes .</p>

<p>I am assuming that you are using the authentication type = managed identity on the sink side . </p>

<p>We need to the give the adequate permission to the ADF . Navigate to the below  on Azure portal .
Dashboard->SQL data warehouses->&lt;> - Query editor (preview)</p>

<pre><code>CREATE USER [your ADF Name ] FROM EXTERNAL PROVIDER;
EXEC sp_addrolemember db_owner, [your ADF Name];
</code></pre>

<p>Now execute the pipeline , it should work fine . </p>

<p>I wanted to repro your issue and so I deliberately removed the permission for the ADF </p>

<pre><code>EXEC sp_droprolemember db_owner, [your ADF Name];
</code></pre>

<p>I see the below error ( this is not whay you reported ) </p>

<pre><code>{
    ""errorCode"": ""2200"",
    ""message"": ""ErrorCode=FailedDbOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Error happened when loading data into SQL Data Warehouse.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Data.SqlClient.SqlException,Message=User does not have permission to perform this action.,Source=.Net SqlClient Data Provider,SqlErrorNumber=15247,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=15247,State=1,Message=User does not have permission to perform this action.,},],'"",
    ""failureType"": ""UserError"",
    ""target"": ""Copy data1"",
    ""details"": []
}
</code></pre>

<p>I think that there is something else going on with your pipeline . </p>
"
"59240875","Manager Identity error when running a pipeline in Data Factory with Staging Blob Storage + Polybase to a Azure DataWarehouse","<p>I am getting this error message when trying to use Polybase + Enabling Staging Blob Storage to pull data from Azure SQL to Azure DW Database.</p>

<p>Managed Service Identity has not been enabled on this server. Please enable Managed Service Identity and try again.</p>

<p>I've tried to create a Managed Identity user assigning it to a web service and giving it permission linking this user to a AAD Group that has owner permission in the database, but it doesn't work.</p>

<p><a href=""https://i.stack.imgur.com/9il3L.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9il3L.png"" alt=""enter image description here""></a></p>
","<azure-data-factory><staging><azure-synapse><polybase>","2019-12-08 23:30:13","3217","0","3","59358373","<p>You need to create a user and login with db_owner permissions. Login with the user to test if it actually works.
PS DF Polybase is not that great as you might think... Under the hood it shards the files to smaller ones, dumps the data to tempdb tables, only then loads it to your table, which takes quite a while if you have billions of records.
I did a lot of tests with it and we ended up using CTAS directly, as it turned out to be 10x faster.
<a href=""https://learn.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-develop-ctas"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-develop-ctas</a></p>
"
"59240875","Manager Identity error when running a pipeline in Data Factory with Staging Blob Storage + Polybase to a Azure DataWarehouse","<p>I am getting this error message when trying to use Polybase + Enabling Staging Blob Storage to pull data from Azure SQL to Azure DW Database.</p>

<p>Managed Service Identity has not been enabled on this server. Please enable Managed Service Identity and try again.</p>

<p>I've tried to create a Managed Identity user assigning it to a web service and giving it permission linking this user to a AAD Group that has owner permission in the database, but it doesn't work.</p>

<p><a href=""https://i.stack.imgur.com/9il3L.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9il3L.png"" alt=""enter image description here""></a></p>
","<azure-data-factory><staging><azure-synapse><polybase>","2019-12-08 23:30:13","3217","0","3","59383462","<p>First, logon in Azure PowerShell with your username/password .
Secondly, run the following command to assign your DB DataWarehouse Server.</p>

<p>Set-AzSqlServer -ResourceGroup  -ServerName   -AssignIdentity</p>
"
"59227458","Sink JSON into SQL Server database using ADFv2 copy data task with stored procedure","<p>I'm trying to retrieve JSON from a REST API and store it directly in a Azure SQL database using the Data Copy task. However I can't get the stored procedure to work since the ""Table Type"" and ""Table Type Parameter"" doesn't seem to ""support"" JSON as input.</p>

<p>My stored procedure looks like this:</p>

<pre><code>CREATE PROCEDURE [dbo].[InsertNewCustomers]
    @JSONObject NVARCHAR(MAX)
AS
BEGIN
    SET NOCOUNT ON;

    INSERT INTO [tmp].[Customer] ([customerNumber], [name])
        SELECT [customerNumber], [name]
        FROM OPENJSON(@JSONObject, '$.collection')
             WITH (
                    [customerNumber] [NVARCHAR](250) '$.customerNumber',
                    [name] [NVARCHAR](250) '$.name'
                  )
END
</code></pre>

<p>Now, using that stored procedure needs a table type which seems to only support a SQL Server table type and not JSON... or how ?</p>

<p><a href=""https://i.stack.imgur.com/02sZl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/02sZl.png"" alt=""enter image description here""></a></p>
","<json><sql-server><azure-data-factory>","2019-12-07 15:22:06","505","2","1","59250297","<p>Have you checked the example at : <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#invoke-a-stored-procedure-from-a-sql-sink"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#invoke-a-stored-procedure-from-a-sql-sink</a></p>

<pre><code>--In your database, define the table type with the same name as sqlWriterTableType.
---The schema of the table type is the same as the schema returned by your input data.

CREATE TYPE [dbo].[JSONObjectType] AS TABLE(
    JSONObject NVARCHAR(MAX) NOT NULL
);
GO

CREATE PROCEDURE [dbo].[InsertNewCustomers]
    @JSONTable [dbo].[JSONObjectType] READONLY
AS
BEGIN
    SET NOCOUNT ON;

    --INSERT INTO [tmp].[Customer] ([customerNumber], [name]) --..testing
        SELECT [customerNumber], [name]
        FROM @JSONTable AS t
        CROSS APPLY OPENJSON(t.JSONObject, '$.collection')
             WITH (
                    [customerNumber] [NVARCHAR](250) '$.customerNumber',
                    [name] [NVARCHAR](250) '$.name'
                  )
END
GO
</code></pre>

<p>--test</p>

<pre><code>DECLARE @input AS [dbo].[JSONObjectType];

INSERT INTO @input(JSONObject) 
VALUES(N'{""collection"" : {""customerNumber"" : ""A"", ""name"" : ""Smith""}'), (N'{""collection"" : {""customerNumber"" : ""B"", ""name"" : ""Jones""}');

EXEC [dbo].[InsertNewCustomers] @JSONTable = @input;
</code></pre>
"
"59209158","Unable to turn on debug for ADFv2 Mapping Data Flow","<p>Today, I'm not able to turn on debug on my ADFv2 mapping data flow. I'm getting the error below:
I already tried creating a new IR set to auto resolve and also set to same region as my ADFv2 - still no good.</p>

<blockquote>
  <p>Failed to setup debug session. Error: No connection is available to service this operation: SETEX dfProd_dataflowdebugsession26a9d381-5924-4c1f-b36f-cf41e2eab60e_f96ba068-97be-478a-8ed6-012a7ec58d80; IOCP: (Busy=0,Free=100,Min=50,Max=100), WORKER: (Busy=7,Free=93,Min=50,Max=100), Local-CPU: n/a</p>
</blockquote>
","<azure-data-factory>","2019-12-06 08:04:08","823","0","2","59221252","<p>What Azure Region is this factory in?</p>
"
"59209158","Unable to turn on debug for ADFv2 Mapping Data Flow","<p>Today, I'm not able to turn on debug on my ADFv2 mapping data flow. I'm getting the error below:
I already tried creating a new IR set to auto resolve and also set to same region as my ADFv2 - still no good.</p>

<blockquote>
  <p>Failed to setup debug session. Error: No connection is available to service this operation: SETEX dfProd_dataflowdebugsession26a9d381-5924-4c1f-b36f-cf41e2eab60e_f96ba068-97be-478a-8ed6-012a7ec58d80; IOCP: (Busy=0,Free=100,Min=50,Max=100), WORKER: (Busy=7,Free=93,Min=50,Max=100), Local-CPU: n/a</p>
</blockquote>
","<azure-data-factory>","2019-12-06 08:04:08","823","0","2","59241731","<p>Congratulations that your error solved now.</p>

<p>I think maybe it's caused by the cache. Refresh the Data Factory and try again.</p>

<p>You tried again today and it worked.</p>

<p>I post this as the answer to make others know the error is gone.</p>
"
"59207113","Getting number of days in a month in ADFv2","<p>After looking into the ADFv2 expression language <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">documentation</a>,I didn't find any function that can return number of days in a given input month. Something like this is quite essential so it's quite surprising that there is no support for this. Is there any way to make this work? Even there is no function to get last day in a month , at least that way one could take the diff and calculate days.</p>
","<azure-data-factory>","2019-12-06 04:57:33","363","1","1","59208193","<p>Agree with that ""Why there is no endOfMonth,only startOfMonth,though it is essential i think???"" You could refer to the solution provided in this thread which has same requirement as you:<a href=""https://stackoverflow.com/questions/57622042/how-can-i-get-the-last-day-of-a-month-in-dynamic-content-in-adf2"">How can I get the last day of a month in dynamic content in ADF2?</a></p>

<p>The solution is summed up in two points</p>

<p>1.Store the dates and their respective end of month in a table or file,namely a reference data source,then use LookUp Activity.</p>

<p>2.Or use built-in functions in destination,such as in sql server.</p>

<p>In addition,my idea is using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-if-condition-activity"" rel=""nofollow noreferrer"">If-Condition Activity</a> to construct such feature.Judge whether it is in 1,3,5,7,8,10,12 month or not.(Plus whether it is leap year for Feb.)</p>
"
"59206092","ADFv1 dependency problem for copy activity","<p>I have a single activity pipeline already in ADFv1 producing an AzureBlob dataset D1. Now I want to create either another dependent activity or another dependent pipeline (anything is fine) in such a way that the child activity will be a copy activity -- so it should only detect if D1 is Ready , and if it is , it should perform Blob to Blob copy operation but the source dataset won't be D1, it will be a different AzureBlob dataset altogether, call it Dx . So the status of D1 is only going to be used as a trigger point. How to do this? If I simple provide D1 as input to the copy activity, it will try to copy D1 data itself, which is not what I want -- it should detect the status of D1 and then start copying contents of Dx to target dataset of the copy activity.</p>
","<azure-data-factory>","2019-12-06 02:41:10","40","0","1","59210861","<p>Per my knowledge,there is no such exist built-in feature in ADF V1.So, i just provide my idea here for your reference.</p>

<p>Firstly,i don't know what's the standard of <code>D1 is Ready</code>,because dataset has so status in ADF.I guess you definitely know your own logic of that status.(Something like file name matches some format? Or it's specific time!) So,no matter what it is, just create a normal copy activity named Dx.</p>

<p>Then you could add some triggers on your D1.Here, maybe you have 2 options:</p>

<p>1.Azure Function Blob Trigger:<a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-triggered-function"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-triggered-function</a></p>

<p>It's the real-time trigger,every inputs in your blob storage will trigger the function method.You could judge the <code>Ready</code> status and call the Pipeline Runs REST API in function inside.Negative point is that you need to authenticate when you call the REST API. Positive point is that you could debug your code as you want.</p>

<p>2.Logic App Blob Trigger:<a href=""https://learn.microsoft.com/en-us/azure/connectors/connectors-create-api-azureblobstorage#add-blob-storage-trigger"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/connectors/connectors-create-api-azureblobstorage#add-blob-storage-trigger</a></p>

<p>As i know,it's not real-time trigger,timely scan instead.Then you judge the <code>Ready</code> status and run the pipeline directly in the <a href=""https://learn.microsoft.com/en-us/connectors/azuredatafactory/"" rel=""nofollow noreferrer"">ADF connector</a>.Negative point is that it's difficult debugging in logic app logical judgment. Positive point is that you don't worry about ADF REST API auth!</p>
"
"59205250","Executing ForEach activity against a hardcoded array in ADFv2","<p>I have a set of hardcoded values for which I need to execute ForEach activity. I can create the array in the following manner and specify this as an expression in the Items setting for the ForEach activity</p>

<p>createArray('obj1', 'obj2','obj3')</p>

<p>But I get the following warning:-</p>

<p>Expression of type: 'String' does not match the field: 'items'</p>

<p>Is there any way workaround for this so that  ForEach activity will work for this?</p>
","<azure-data-factory>","2019-12-06 00:36:12","1297","2","1","59205607","<p>I think <code>createArray</code> built-in function could be implemented.</p>

<p><a href=""https://i.stack.imgur.com/G5RzS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G5RzS.png"" alt=""enter image description here""></a></p>

<p>Inside ForEach Avtivity,i just configure one SetVariableActivity.</p>

<p><a href=""https://i.stack.imgur.com/UtI5F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UtI5F.png"" alt=""enter image description here""></a></p>

<p>Result:</p>

<p><a href=""https://i.stack.imgur.com/fjC8D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fjC8D.png"" alt=""enter image description here""></a></p>
"
"59185874","Azure Data Factory Copy Activity error mapping JSON to SQL","<p>I have an Azure Data Factory Copy Activity that is using a REST request to elastic search as the Source and attempting to map the response to a SQL table as the Sink. Everything works fine except when it attempts to map the <code>data</code> field that contains the dynamic JSON. I get the following error:</p>

<blockquote>
  <p>{
      ""errorCode"": ""2200"",
      ""message"": ""ErrorCode=UserErrorUnsupportedHierarchicalComplexValue,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The retrieved type of data JObject with value {\""name\"":\""department\""} is not supported yet, please either remove the targeted column or enable skip incompatible row to skip them.,Source=Microsoft.DataTransfer.Common,'"",
      ""failureType"": ""UserError"",
      ""target"": ""CopyContents_Paged"",
      ""details"": []
  }</p>
</blockquote>

<p>Here's an example of my mapping configuration:</p>

<pre><code>    ""type"": ""TabularTranslator"",
    ""mappings"": [
        {
            ""source"": {
                ""path"": ""['_source']['id']""
            },
            ""sink"": {
                ""name"": ""ContentItemId"",
                ""type"": ""String""
            }
        },
        {
            ""source"": {
                ""path"": ""['_source']['status']""
            },
            ""sink"": {
                ""name"": ""Status"",
                ""type"": ""Int32""
            }
        },
        {
            ""source"": {
                ""path"": ""['_source']['data']""
            },
            ""sink"": {
                ""name"": ""Data"",
                ""type"": ""String""
            }
        }
    ],
    ""collectionReference"": ""$['hits']['hits']""
}
</code></pre>

<p>The JSON in the <code>data</code> object is dynamic so I'm unable to do an explicit mapping for the nested fields within it. That's why I'm trying to just store the entire JSON object under <code>data</code> in a column of a SQL table.</p>

<p>How can I adjust my mapping configuration to allow this to work properly?</p>
","<azure><azure-data-factory>","2019-12-04 23:06:46","4786","4","2","59185954","<p>I have the same problem a few days ago. You need to convert your JSON object to a Json String. It will solve your mapping problem (UserErrorUnsupportedHierarchicalComplexValue).</p>

<p>Try it and tell me if also resolves your error.</p>
"
"59185874","Azure Data Factory Copy Activity error mapping JSON to SQL","<p>I have an Azure Data Factory Copy Activity that is using a REST request to elastic search as the Source and attempting to map the response to a SQL table as the Sink. Everything works fine except when it attempts to map the <code>data</code> field that contains the dynamic JSON. I get the following error:</p>

<blockquote>
  <p>{
      ""errorCode"": ""2200"",
      ""message"": ""ErrorCode=UserErrorUnsupportedHierarchicalComplexValue,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The retrieved type of data JObject with value {\""name\"":\""department\""} is not supported yet, please either remove the targeted column or enable skip incompatible row to skip them.,Source=Microsoft.DataTransfer.Common,'"",
      ""failureType"": ""UserError"",
      ""target"": ""CopyContents_Paged"",
      ""details"": []
  }</p>
</blockquote>

<p>Here's an example of my mapping configuration:</p>

<pre><code>    ""type"": ""TabularTranslator"",
    ""mappings"": [
        {
            ""source"": {
                ""path"": ""['_source']['id']""
            },
            ""sink"": {
                ""name"": ""ContentItemId"",
                ""type"": ""String""
            }
        },
        {
            ""source"": {
                ""path"": ""['_source']['status']""
            },
            ""sink"": {
                ""name"": ""Status"",
                ""type"": ""Int32""
            }
        },
        {
            ""source"": {
                ""path"": ""['_source']['data']""
            },
            ""sink"": {
                ""name"": ""Data"",
                ""type"": ""String""
            }
        }
    ],
    ""collectionReference"": ""$['hits']['hits']""
}
</code></pre>

<p>The JSON in the <code>data</code> object is dynamic so I'm unable to do an explicit mapping for the nested fields within it. That's why I'm trying to just store the entire JSON object under <code>data</code> in a column of a SQL table.</p>

<p>How can I adjust my mapping configuration to allow this to work properly?</p>
","<azure><azure-data-factory>","2019-12-04 23:06:46","4786","4","2","59273833","<p>I posted this question on the MSDN forums and I was told that if you are using a tabular sink you can set this option <code>""mapComplexValuesToString"": true</code> and it should allow complex JSON properties to get mapped correctly. This resolved my ADF copy activity issue.</p>
"
"59171161","Download dynamically generated file using Azure Datafactory","<p>I am trying to download a file from a URL.</p>

<pre><code>https://example.com/report_view.php?reportcode=59281&amp;action=exportcsv&amp;casino=2210&amp;reportby=daily&amp;datetype=1
</code></pre>

<p>The above URL generates a report.csv file everytime it is hit. The URL when run in a regular browser downloads a file. Nothing loads in the window.</p>

<p>I am trying to download the file using Copy Activity in Azure data factory.</p>

<p>Source: HTTP request >> Delimited Text</p>

<p>Sink: Azure Blob Storage.</p>

<p>But the copy activity is failing to download the file.</p>

<p>How to ensure that the file get's in the blob storage?</p>
","<azure><azure-blob-storage><azure-data-factory>","2019-12-04 07:44:50","587","0","1","59171385","<p>I tested in the copy activity and it works for me.</p>

<p>Source Dataset,Http Request:</p>

<p><a href=""https://i.stack.imgur.com/FWY20.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FWY20.png"" alt=""enter image description here""></a></p>

<p>Sink Dataset,Azure Blob Storage:</p>

<p><a href=""https://i.stack.imgur.com/TEXnb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TEXnb.png"" alt=""enter image description here""></a></p>

<p>Result,never set any file name,so it generates random file name:</p>

<p><a href=""https://i.stack.imgur.com/Kp0Np.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Kp0Np.png"" alt=""enter image description here""></a></p>
"
"59166829","Can connect to Azure SQL using ADFv2 connection using Service Principal but not through SSMS","<p>I am able to connect to a specific Azure SQL Database using ADFv2 connection with Authentication Type as Service Principal. But I am trying to figure out how to connect to the same Azure SQL DB using SSMS using the service principal. If I just try to use the service principal ID as 'Login' and its secret as Password in the SSMS login screen (Authentication: SQL Server Authentication), it doesn't work. There is no option in SSMS to select service principal authentication. Is there any workaround for this?</p>
","<azure-active-directory><azure-sql-database><ssms><azure-data-factory>","2019-12-03 23:19:31","461","0","1","59167776","<p>You can not connect to the Azure SQL database with Service Principal in SSMS. As you found that there is no option can support this.</p>

<p>I found a Azure Support blog that give the answer for you:</p>

<p><strong>Just to mention that there it not possible to use SQL SERVER Management Studio to connect using Service Principals and you need to use a C# to be able to connect using it.</strong></p>

<p>Reference: <a href=""https://techcommunity.microsoft.com/t5/Azure-Database-Support-Blog/Lesson-Learned-49-Does-Azure-SQL-Database-support-Azure-Active/ba-p/369055"" rel=""nofollow noreferrer"">Lesson Learned #49: Does Azure SQL Database support Azure Active Directory connections using Service Principals?</a></p>

<p>Hope this helps.</p>
"
"59166159","Conditional mapping in azure data factory","<p>I have created a pipeline in azure data factory which copy data from a collection (named <code>source</code>) from an azure table storage to a cosmos db storage (with mongoDB API) containing collection named <code>destination</code>. </p>

<p>I have an issue on how to copy an attribute named <code>EndDate</code> which is a date object. 
Indeed, I have to map this attribute to <code>endDate</code> attribute in mongo which is an object containing <code>date</code> property. I think this is necessary to be interpreted as a date by mongoDB. <a href=""https://i.stack.imgur.com/uTpsa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uTpsa.png"" alt=""azure data factory mapping""></a></p>

<p>The exactly mapping is the following :
<a href=""https://i.stack.imgur.com/5IHaN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5IHaN.png"" alt=""advanced mapping""></a></p>

<p>The problem here is I have some <code>EndDate</code> with null values. And <code>endDate['date']</code> can't receive null values because mongo throws an error (in fact mongo uses the function <code>ISODate(date)</code> to convert a string to a date and <code>ISODate(null)</code> can't work). </p>

<p>So my idea is to : </p>

<ul>
<li>map <code>EndDate</code> to <code>endDate['date']</code> when its value is not null</li>
<li>map <code>EndDate</code> to <code>endDate</code> otherwise</li>
</ul>

<p>Do you know how I can add this conditional mapping in azure data factory ? </p>

<p>Thanks by advance</p>
","<mongodb><azure><mapping><azure-table-storage><azure-data-factory>","2019-12-03 22:12:02","822","4","1","59170412","<p>As i know,you can't implement such conditional mapping in copy activity because it has some limitations or rules.Please refer to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#alternative-column-mapping"" rel=""nofollow noreferrer"">paragraph</a>.</p>

<p>I suggest you getting idea of <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-data-flow"" rel=""nofollow noreferrer"">Data Flows</a> in ADF which could replace Copy Activity when you have some logical or complex needs in columns mapping.For your scenario,maybe <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-conditional-split"" rel=""nofollow noreferrer"">Conditional Split</a> is your option!</p>

<p><a href=""https://i.stack.imgur.com/soSwp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/soSwp.png"" alt=""enter image description here""></a></p>
"
"59155575","ADF deployment is failing if the ADF objects are renamed","<p>We've have renamed our ADF pipelines in the code and tried deploying them using ""<code>-role: ansible-role-azure-adf-deploy</code>"" role. But, the build is failing as it fails to delete the existing ADF objects. I've tried using the parameter ""<code>azure_adf_max_delete_retries: 2</code>"", yet the old ADF objects are not deleted. Is there any other way instead of deleting the ADF instance manually ?
Following is a log URL for your reference :</p>

<p><a href=""https://dev.azure.com/chevron/CGM-ST-Pond/_build/results?buildId=1184234&amp;view=logs&amp;j=0dfcbfa6-5050-52a7-7298-8f7d8bed78bb&amp;t=9d2bf12b-b622-5483-377c-729016318825"" rel=""nofollow noreferrer"">https://dev.azure.com/chevron/CGM-ST-Pond/_build/results?buildId=1184234&amp;view=logs&amp;j=0dfcbfa6-5050-52a7-7298-8f7d8bed78bb&amp;t=9d2bf12b-b622-5483-377c-729016318825</a></p>

<p>Could someone help ? Thanks !!</p>
","<ansible><azure-data-factory><ansible-role>","2019-12-03 11:00:04","157","0","1","59294645","<p>Your question is not very clear ( the link does not take you anywhere ) , but i think you are trying to deploy a ADF using ansible and AFAIK that this is not supported  . Please do let me know as to how you initially deployed the ADF ?</p>
"
"59154568","Azure Data Factory V2 Copy Activity with Rest API giving one row for nested JSON","<p>I am trying to flatten a nested JSON returned from a Rest source. The pipeline code is as follows.
The problem here is this pipeline returns only first object from JSON dataset and skips all the rest of the rows.
Can you please guide me on how to iterate over nested objects.</p>

<p>Thanks
Sameet</p>

<pre><code>{
    ""name"": ""STG_NCR2"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""Copy data1"",
                ""type"": ""Copy"",
                ""dependsOn"": [],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""userProperties"": [],
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""RestSource"",
                        ""httpRequestTimeout"": ""00:01:40"",
                        ""requestInterval"": ""00.00:00:00.010"",
                        ""requestMethod"": ""GET"",
                        ""additionalHeaders"": {
                            ""OData-MaxVersion"": ""4.0"",
                            ""OData-Version"": ""4.0"",
                            ""Prefer"": ""odata.include-annotations=*""
                        }
                    },
                    ""sink"": {
                        ""type"": ""AzureSqlSink""
                    },
                    ""enableStaging"": false,
                    ""translator"": {
                        ""type"": ""TabularTranslator"",
                        ""mappings"": [
                            {
                                ""source"": {
                                    ""path"": ""$['value'][0]['tco_ncrid']""
                                },
                                ""sink"": {
                                    ""name"": ""NCRID""
                                }
                            },
                            {
                                ""source"": {
                                    ""path"": ""['tco_name']""
                                },
                                ""sink"": {
                                    ""name"": ""EquipmentSerialNumber""
                                }
                            }
                        ],
                        ""collectionReference"": ""$['value'][0]['tco_ncr_tco_equipment']""
                    }
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""Rest_PowerApps_NCR"",
                        ""type"": ""DatasetReference""
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""Prestaging_PowerApps_NCREquipments"",
                        ""type"": ""DatasetReference""
                    }
                ]
            }
        ],
        ""annotations"": []
    }
}
</code></pre>

<p>The JSON is in the following format</p>

<pre><code>[ 
   { 
      ""value"":[ 
         { 
            ""tco_ncrid"":""abc-123"",
            ""tco_ncr_tco_equipment"":[ 
               { 
                  ""tco_name"":""abc""
               }
            ]
         },
         { 
            ""tco_ncrid"":""abc-456"",
            ""tco_ncr_tco_equipment"":[ 
               { 
                  ""tco_name"":""xyz""
               },
               { 
                  ""tco_name"":""yzx""
               }
            }
         ]
      ]
   }
]
</code></pre>
","<json><azure><azure-data-factory>","2019-12-03 10:07:27","1016","0","1","59166408","<p>This can be resolved by amending the translator property as follows.</p>

<pre><code>""translator"": {
                    ""type"": ""TabularTranslator"",
                    ""mappings"": [
                        {
                            ""source"": {
                                ""path"": ""$.['value'][0].['tco_ncrid']""
                            },
                            ""sink"": {
                                ""name"": ""NCRID"",
                                ""type"": ""String""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""$.['value'][0].['tco_text_id']""
                            },
                            ""sink"": {
                                ""name"": ""EquipmentDescription"",
                                ""type"": ""String""
                            }
                        },
                        {
                            ""source"": {
                                ""path"": ""['tco_name']""
                            },
                            ""sink"": {
                                ""name"": ""EquipmentSerialNumber"",
                                ""type"": ""String""
                            }
                        }
                    ],
                    ""collectionReference"": ""$.['value'][*].['tco_ncr_tco_equipment']""
                }
</code></pre>

<p>This code forces the pipeline to iterate over nested array but as you can see that the NCRID is hardcoded to first element of the value array. This is not exactly what I want as I am looking for all Equipment Serial Numbers against every NCRID. Still researching...</p>
"
"59148844","Will my Shared Self-Hosted Integration Runtime be available to other Azure Data Factories during a regional Azure outage?","<p>I have 3 Azure Data Factories.</p>

<ul>
<li><strong>DF_Shared_IR</strong>:

<ul>
<li>Empty, save for a Shared Self-Hosted Integration Runtime (on-premise, 2 nodes).</li>
<li>Deployed in region 1 (Australia East).</li>
</ul></li>
<li><strong>DF_1</strong>:

<ul>
<li>Has pipelines which perform hourly data loads and other ETL jobs.</li>
<li>Has a Linked Integration Runtime (referencing the IR hosted within <strong>DF_Shared_IR</strong>).</li>
<li>Deployed in region 1 (Australia East).</li>
</ul></li>
<li><strong>DF_2</strong>:

<ul>
<li>Is a replica of <strong>DF_1</strong>.</li>
<li>Deployed in region 2 (Australia Southeast).</li>
</ul></li>
</ul>

<p>In the unlikely event of a regional outage in region 1 (in which <strong>DF_1</strong> and <strong>DF_Shared_IR</strong> are impacted), I want to enable all triggers within <strong>DF_2</strong> and resume normal hourly data loads.</p>

<p>I have reviewed the documentation <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#high-availability-and-scalability"" rel=""nofollow noreferrer"">here</a>. It is clear to me that I can make my Shared Self Hosted Integration Runtime highly-available by adding multiple nodes - however it is unclear whether the Azure components which enable communication to the on-premise Integration Runtime are also highly available.</p>

<p>Will the Shared Self-Hosted Integration Runtime (hosted within <strong>DF_Shared_IR</strong>) be available to <strong>DF_2</strong> in this disaster recovery scenario? If not how can I support this scenario, and ensure service continuity?</p>
","<azure-data-factory>","2019-12-03 01:07:31","479","1","1","60032254","<p>TL;DR: Yes. </p>

<p>After opening a ticket with Azure regarding this question, Microsoft Support advised the data movement service within Data Factory which enables connectivity to the on-premise Integration Runtime is considered highly available.</p>

<p>Within Data Factory, you can review the Service URLs in your self-hosted IR settings.</p>

<ul>
<li><p>Click <strong>View Service URLs</strong>.
<a href=""https://i.stack.imgur.com/OFCqE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OFCqE.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/Wfwa6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Wfwa6.png"" alt=""enter image description here""></a></p></li>
<li><p>Observe the service URLs with the domain name *.servicebus.windows.net which are required by the self-hosted integration runtime to connect to data movement services in Azure Data Factory.</p></li>
</ul>

<p><a href=""https://i.stack.imgur.com/UDfQm.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UDfQm.jpg"" alt=""enter image description here""></a></p>

<p>The listed URLs represent services from two regions (sg3 and hk2). One is default and the other provides high availability. This design ensures connectivity remains available between your self-hosted IR and data movement service in Data Factory while either of the regions meet outage.</p>
"
"59148772","Can we configure ADF to copy each fileShare to another account?connect to (//storageAccount/) not (//Storageaccount/FileShare)","<p><strong>I want to copy all my file shares to another account</strong> </p>

<p>Things that I have tried </p>

<ol>
<li>I have tried using storage Explorer but this has size limitations</li>
<li>Azcopy this will fail abruptly with some errors</li>
<li><p>ADF this works but I need to do this for every fileshare</p>

<p>I wanted to know can I get the list of fileshares in ADF and feed it to for-each and perform the copy ? </p></li>
</ol>

<p>I am not sure how to achieve or use foreach to get fileshares in  data factory</p>

<p>I feel that there is no way to get connected to storage account without specifying the fileShare, If I give the name of file share then , I can access only content inside it, I am trying to figure out a way to copy all file share in storage account, that is to connect to //storageacount/ not //storageaccount/FileShare </p>
","<azure><azure-data-factory>","2019-12-03 00:56:47","183","0","1","59149687","<blockquote>
  <p>I wanted to know can I get the list of fileshares in ADF and feed it
  to for-each and perform the copy ?</p>
</blockquote>

<p>If you select <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-file-storage#azure-file-storage-as-source"" rel=""nofollow noreferrer"">recursive</a> as true,the files and sub-files will be all copied into new locations.</p>

<p><a href=""https://i.stack.imgur.com/ibKAV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ibKAV.png"" alt=""enter image description here""></a></p>

<p>If you do want to get list of fileshares, you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">Get Metadata Activity</a> and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">ForEach Activity</a>. </p>

<p>1.Get folder list:<a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity#get-a-folders-metadata"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity#get-a-folders-metadata</a></p>

<p>2.Loop the output of Get Metadata Activity: <code>@activity('Get Metadata1').output</code></p>
"
"59136134","Azure self-hosted integration runtime on a standalone machine","<p>We have an on-premises MS-SQL Server where all the data is stored, which is also a backend for an application. From this on-premises server, we would like to copy data to Azure Data Lake using Data Factory service. This would require installation of Azure self-hosted integration runtime on application backend server.</p>

<p>Is there any other way, like, to create a standalone server for IR installation and use this IR for data copy activity from application backend to Data Lake?</p>
","<azure><azure-data-factory>","2019-12-02 09:41:32","1466","0","2","59136244","<p>I dont see a problem with that, you dont have to install it on the same server. Point number 3 talks about this:</p>

<blockquote>
  <p>The self-hosted integration runtime doesn't need to be on the same
  machine as the data source. However, having the self-hosted
  integration runtime close to the data source reduces the time for the
  self-hosted integration runtime to connect to the data source. We
  recommend that you install the self-hosted integration runtime on a
  machine that differs from the one that hosts the on-premises data
  source. When the self-hosted integration runtime and data source are
  on different machines, the self-hosted integration runtime doesn't
  compete with the data source for resources.</p>
</blockquote>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#considerations-for-using-a-self-hosted-ir"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#considerations-for-using-a-self-hosted-ir</a></p>
"
"59136134","Azure self-hosted integration runtime on a standalone machine","<p>We have an on-premises MS-SQL Server where all the data is stored, which is also a backend for an application. From this on-premises server, we would like to copy data to Azure Data Lake using Data Factory service. This would require installation of Azure self-hosted integration runtime on application backend server.</p>

<p>Is there any other way, like, to create a standalone server for IR installation and use this IR for data copy activity from application backend to Data Lake?</p>
","<azure><azure-data-factory>","2019-12-02 09:41:32","1466","0","2","59139149","<p>Install IR on the on-premise machine and then configure it using Launch Configuration Manager. Doesn't need to be on the same machine as the data source. Details can be found <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-hybrid-copy-powershell#create-linked-services"" rel=""nofollow noreferrer"">here</a>.</p>

<p><a href=""https://i.stack.imgur.com/QbWuC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QbWuC.png"" alt=""enter image description here""></a></p>
"
"59134930","how to get file Count in Azure data factory and do activities based on it","<p>I have a sftp server folder from where I am copying all the child files to a file system(destination). After the copy activity is finished, I have to delete the source folder. But before deleting the source folder, I need to confirm that number of files in the source was copied to the destination folder.</p>

<p>Is there a way to check whether the number of files in source and destination are same, and if they are same proceed, else exit,</p>

<p>Thanks</p>
","<azure><azure-data-factory>","2019-12-02 08:19:30","3495","0","2","59135502","<p>When the copy active completed, you can open the <code>Succeeded</code> details to check whether the number of files in source and destination are same:
<a href=""https://i.stack.imgur.com/Kg95o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Kg95o.png"" alt=""enter image description here""></a> </p>

<p>Details:
<a href=""https://i.stack.imgur.com/z6ern.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z6ern.png"" alt=""enter image description here""></a></p>

<p>If you don't know how to find the details,  you can follow these steps:</p>

<p><strong>Data Factor</strong>--<strong>Monitor</strong>--<strong>Pipeline runs</strong>, choose the pipeline:
<a href=""https://i.stack.imgur.com/2IQ6m.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2IQ6m.png"" alt=""enter image description here""></a></p>

<p><strong>View acitve runs</strong>:
<a href=""https://i.stack.imgur.com/VDFXd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VDFXd.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"59134930","how to get file Count in Azure data factory and do activities based on it","<p>I have a sftp server folder from where I am copying all the child files to a file system(destination). After the copy activity is finished, I have to delete the source folder. But before deleting the source folder, I need to confirm that number of files in the source was copied to the destination folder.</p>

<p>Is there a way to check whether the number of files in source and destination are same, and if they are same proceed, else exit,</p>

<p>Thanks</p>
","<azure><azure-data-factory>","2019-12-02 08:19:30","3495","0","2","59358666","<p>you can do it with ""Get Metadata"" . Create ""Get Metadata"" dataset, add ""child items"", it will output an array with all names of the files, you can use length dynamic content to check what is the size of the array (number of files), or use for each to loop through all files names and check if you have identical in your other source.</p>
"
"59118389","Azure Data Factory Mapping Data Flow: Epoch timestamp to Datetime","<p>I have a JSON-based source I'd like to transform using ADF Mapping Data Flow. I have a string containing an epoch timestamp value that I want to transform to Datetime value to later sink it into Parquet file. </p>

<p>Do you know a way? Docs of this language are <a href=""https://learn.microsoft.com/azure/data-factory/data-flow-expression-functions"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Source file:</p>

<pre><code>{
  ""timestamp"":""1574127407"",
  ""name"":""D.A.""
}
</code></pre>
","<azure><azure-data-factory>","2019-11-30 17:05:33","6579","2","2","59129897","<p>Use toTimestamp() and set the formatting you wish as 2nd parameter</p>

<p><code>toTimestamp(1574127407*1000l)</code></p>

<p>From string:</p>

<p><code>toTimestamp(toInteger(toString(byName('timestamp')))*1000l,'yyyy-MM-dd HH:mm:ss')</code></p>
"
"59118389","Azure Data Factory Mapping Data Flow: Epoch timestamp to Datetime","<p>I have a JSON-based source I'd like to transform using ADF Mapping Data Flow. I have a string containing an epoch timestamp value that I want to transform to Datetime value to later sink it into Parquet file. </p>

<p>Do you know a way? Docs of this language are <a href=""https://learn.microsoft.com/azure/data-factory/data-flow-expression-functions"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Source file:</p>

<pre><code>{
  ""timestamp"":""1574127407"",
  ""name"":""D.A.""
}
</code></pre>
","<azure><azure-data-factory>","2019-11-30 17:05:33","6579","2","2","72560836","<p>I have came across various epoch timestamp values which are of 13 digits i.e., they even have milliseconds detailed information.</p>
<p>In such case, converting to integer using 'toInteger' won't serve the purpose instead this will keep the values as NULL. So, to fix this issue, we need to convert it to Long using toLong as below:</p>
<pre><code>toTimestamp(toLong(toString(created)),'yyyy-MM-dd HH:mm:ss')
</code></pre>
<p>In above expression, 'created' is a field whose value is 13-digit epoch timestamp, something like this created='1635359043307'.</p>
<p>Here, toTimestamp returns the Date Timestamp with above-mentioned date format.</p>
<p>FYI, you can use this site <a href=""https://www.epochconverter.com/"" rel=""nofollow noreferrer"">https://www.epochconverter.com/</a> to check epoch timestamp to human date.</p>
"
"59115519","Azure Data Factory Copy activity not applying compression for avro","<p>I am applying ""avroCompressionCodec"": ""snappy"" for avro dataset. However Copy activity in Azure Data Factory V2 pipeline ignores compression. At design time it shows a warning - The compression type will be ignored when using Avro dataset as copy destination.</p>

<p>Do we know how to apply compression in this case?</p>
","<copy><compression><azure-pipelines><avro><azure-data-factory>","2019-11-30 11:03:38","748","1","1","59133722","<p>As it is stated in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#compression-support"" rel=""nofollow noreferrer"">official documents</a>. Compression settings are not currently supported for AvroFormat data.</p>

<blockquote>
  <p>Compression settings are not supported for data in the AvroFormat, OrcFormat, or ParquetFormat. When reading files in these formats, Data Factory detects and uses the compression codec in the metadata. When writing to files in these formats, Data Factory chooses the default compression codec for that format. For example, ZLIB for OrcFormat and SNAPPY for ParquetFormat.</p>
</blockquote>

<p>You can post your idea to Microsoft <a href=""https://feedback.azure.com/forums/270578-data-factory"" rel=""nofollow noreferrer"">here</a>, asking for the feature to support compression settings for AvroFormat data.</p>
"
"59114668","Not able to specify window for Tumbling Window trigger dependency in ADFv2","<p>I have a very simple pipeline that I have setup to test tumbling window trigger dependency. So the pipeline has a single Wait activity. Here is the pipeline code:-</p>

<pre><code>    {
    ""name"": ""pl-something"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""Wait1"",
                ""type"": ""Wait"",
                ""dependsOn"": [],
                ""userProperties"": [],
                ""typeProperties"": {
                    ""waitTimeInSeconds"": 25
                }
            }
        ],
        ""parameters"": {
            ""date_id"": {
                ""type"": ""string""
            }
        },
        ""annotations"": []
    },
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>

<p>I have created the following hourly trigger on it which just executes it at hourly intervals:-</p>

<pre><code>    {
    ""name"": ""trg-hourly"",
    ""properties"": {
        ""annotations"": [],
        ""runtimeState"": ""Started"",
        ""pipeline"": {
            ""pipelineReference"": {
                ""referenceName"": ""pl-something"",
                ""type"": ""PipelineReference""
            },
            ""parameters"": {
                ""date_id"": ""@formatDateTime(triggerOutputs().windowStartTime, 'yyyyMMddHH')""
            }
        },
        ""type"": ""TumblingWindowTrigger"",
        ""typeProperties"": {
            ""frequency"": ""Hour"",
            ""interval"": 1,
            ""startTime"": ""2019-11-01T00:00:00.000Z"",
            ""delay"": ""00:00:00"",
            ""maxConcurrency"": 1,
            ""retryPolicy"": {
                ""intervalInSeconds"": 30
            },
            ""dependsOn"": []
        }
    }
}
</code></pre>

<p>The parameter date_id exists so I know exactly which hourly window a trigger instance is running for. Now this executes fine. My goal is to create another trigger on the same pipeline but which will execute as a daily thing and which depends on the hourly trigger. So that unless all the 24 hours in a day are processed , the daily trigger should not run. So in the screenshow below you can see how I am trying to setup this new trigger dependent on the hourly trigger (trg-hourly), but somehow the 'OK' button is not activated whenever I try to specify 24 hours window and you can see the error too that the window size is not valid. There is no json to show , since it's not even allowing me to create the trigger. What's the issue here?</p>

<p><a href=""https://i.stack.imgur.com/Kbt5X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Kbt5X.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2019-11-30 08:58:35","458","0","1","59149878","<p>Maybe it is expecting <code>1.00:00:00</code> instead of <code>0.24:00:00</code> because there are 24 hours in a day.</p>
"
"59104693","Data Factory Release Pipeline Issues","<p><strong>The Goal</strong></p>

<p>The goal is to use a Release Pipeline inside Azure DevOps to copy the contents of a DEV ADF environment to a PROD ADF environment. This should work by monitoring the adf_publish branch for changes and publishing the ARM template files to the PROD ADF environment.</p>

<p><strong>The Issue</strong></p>

<p>Although the pipeline reports a success, when I check the PROD environment it's empty still. </p>

<p><a href=""https://i.stack.imgur.com/bnIml.png"" rel=""nofollow noreferrer"">Pipeline Results</a></p>

<p><a href=""https://i.stack.imgur.com/tKm9S.png"" rel=""nofollow noreferrer"">Logs</a></p>

<p><strong>Environment Structure</strong></p>

<p>The two ADF environments, DEV and PROD, are in the same resource group in the Azure Portal and are backed up to seperate repos: ADFV2-DEV and ADFV2-PRD respectively. Each has a master branch.</p>

<p><strong>What I've Tried</strong></p>

<p>When I removed PROD from source control, meaning all changes are saved inside ADF with no branching/git/etc. then the release pipeline works as expected. As such, I thought maybe the issue was permissions inside Azure DevOps.</p>

<p>I tried giving blanket ""Allow"" permissions to every group/user for every permission to see if that was where the issue lay, however there was no change in result.</p>

<p><a href=""https://i.stack.imgur.com/HHebv.png"" rel=""nofollow noreferrer"">Branch Permissions</a></p>

<p>As such, I <em>think</em> the issue lies with the fact that the PROD ADF environment is in an Azure Git Repo, however I can't narrow down what's causing the issue.</p>

<p>Any help with this would be greatly appreciated. There's somewhat of a need to have both ADF environments backed up in source control.</p>

<p><strong>Note</strong></p>

<p>This is my first stackoverflow question so I apologize in advance for any glaring errors or misteps. Please let me know if I've crossed any lines and I'll gladly fix/expand/etc.</p>
","<git><azure><azure-devops><continuous-integration><azure-data-factory>","2019-11-29 12:20:30","293","1","1","59138093","<p>After reading more documentation, it seems the issue lies with git integration in the PROD environment. In <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">this microsoft doc</a> the following is stated:</p>

<blockquote>
  <p>Git Integration. You're only required to configure your Development data factory with Git integration. Changes to Test and Production are deployed via CI/CD, and don't need Git integration.</p>
</blockquote>

<p>Removing git integration from the PROD environment appears to resolve the issue. I was hoping this could be fixed without removing git, however it appears to not be needed.</p>
"
"59103273","How to add headers to Common Data Service data store in Microsoft Azure Data Factory","<p>I am new to Azure Data Factory and have a requirement to pull data from Common Data Service (PowerApps) application to Azure SQL Database.
I can pull data directly from the entities using a linked service. The problem arises when I want to include headers such as </p>

<pre><code>OData-MaxVersion:4.0
OData-Version:4.0
Prefer:odata.include-annotations=*
</code></pre>

<p>into my pipeline. The reason for adding these headers is that my entity uses values from different option sets and when I use the above headers in my web api calls, I get the response with formatted values as well.
Please advise how can I get around this problem using Common Data Service connectors available in Azure Data Factory.</p>
","<azure><dynamics-crm><azure-data-factory><powerapps><common-data-service>","2019-11-29 10:43:10","368","1","1","59232740","<p>Those headers will expand the attributes like Picklist (optionset) for the formatted values in CRM web api. Are you getting the picklist integer column values in query results?</p>

<p>Because there seems to be couple of issues <a href=""https://github.com/MicrosoftDocs/azure-docs/issues/28027"" rel=""nofollow noreferrer"">opened in github </a> stating the results are missing. You should try schema/mapping to bring those attributes. <a href=""https://community.dynamics.com/365/b/ajitpatra365crm/posts/azure-copy-data-from-d365-ce-to-azure-sql-database-using-azure-data-factory"" rel=""nofollow noreferrer"">Read more </a>.</p>

<p>Or use the OData connector instead of Dynamics/CDS connectors.</p>
"
"59102898","ADF copy activity is failing while converting from string to Decimal","<p>I have a copy activity in Azure Data Factory that takes CSV file as source and SQL table as destination. </p>

<p>Source data type is 'string' and destination is 'Decimal' </p>

<p>Let's take an example of source value ,</p>

<p><strong>100612324864664000.00</strong></p>

<p>This is the error message I am getting ,</p>

<p><strong>Message=Column 'xyz' contains an invalid value '1.0061232486466467E+17'.,Source=Microsoft.DataTransfer.ServiceLibrary,''Type=System.ArgumentException,Message=Input string was 
not in a correct format.Couldn't store &lt;1.0061232486466467E+17> in xyz Column. Expected type is Decimal.</strong></p>
","<type-conversion><azure-data-factory>","2019-11-29 10:18:49","4567","1","2","59135124","<p>I tested your description and it works for me.</p>

<p>Source dataset:</p>

<p><a href=""https://i.stack.imgur.com/YSmus.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YSmus.png"" alt=""enter image description here""></a></p>

<p>Sink dataset:</p>

<p><a href=""https://i.stack.imgur.com/vs4Rc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vs4Rc.png"" alt=""enter image description here""></a></p>

<p>Pipeline mapping:</p>

<p><a href=""https://i.stack.imgur.com/2tqx2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2tqx2.png"" alt=""enter image description here""></a></p>

<p>Result:</p>

<p><a href=""https://i.stack.imgur.com/2AAhx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2AAhx.png"" alt=""enter image description here""></a></p>
"
"59102898","ADF copy activity is failing while converting from string to Decimal","<p>I have a copy activity in Azure Data Factory that takes CSV file as source and SQL table as destination. </p>

<p>Source data type is 'string' and destination is 'Decimal' </p>

<p>Let's take an example of source value ,</p>

<p><strong>100612324864664000.00</strong></p>

<p>This is the error message I am getting ,</p>

<p><strong>Message=Column 'xyz' contains an invalid value '1.0061232486466467E+17'.,Source=Microsoft.DataTransfer.ServiceLibrary,''Type=System.ArgumentException,Message=Input string was 
not in a correct format.Couldn't store &lt;1.0061232486466467E+17> in xyz Column. Expected type is Decimal.</strong></p>
","<type-conversion><azure-data-factory>","2019-11-29 10:18:49","4567","1","2","62000766","<p>Have the same problem. You need to use 'format' in your ""toDecimal"" method. eg:</p>

<pre><code>toDecimal(""123.45"",10,4,'#.##')
</code></pre>
"
"59098013","Data Factory Error trying to use Staging Blob Storage to pull data from Azure SQL to Azure SQL Data Warehouse","<p>I am trying to use the Staging Blob Storage to get data to Azure SQL Data Warehouse. 
It is using Managed Identity.
The first step is working fine when getting the data from Azure SQL DB and writing the files under the Blob Storage ""Staging"" Account. But when trying to add the rows in the SQL DW it is giving an error pointing to the Data Factory Managed Identity Application ID that is 98421a8f-73a4-4ef1-aa79-8a852e452b77 as described below.</p>

<p>The SQL DB and DW DB are not in the same SQL Server.</p>

<p>I have also added the Data Factory App to the DW Server by the role assignment under Access Control (IAM)... </p>

<p>Thanks any help/guess!</p>

<p>Error message:</p>

<p>{
    ""errorCode"": ""2200"",
    ""message"": ""ErrorCode=FailedDbOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Error happened when loading data into SQL Data Warehouse.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Data.SqlClient.SqlException,Message=The specified schema name \""98421a8f-73a4-4ef1-aa79-8a852e452b77@68a5e094-6fa7-4c99-bc18-154866e518f0\"" either does not exist or you do not have permission to use it.,Source=.Net SqlClient Data Provider,SqlErrorNumber=2760,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=2760,State=1,Message=The specified schema name \""98421a8f-73a4-4ef1-aa79-8a852e452b77@68a5e094-6fa7-4c99-bc18-154866e518f0\"" either does not exist or you do not have permission to use it.,},],'"",
    ""failureType"": ""UserError"",
    ""target"": ""Copy_8jp"",
    ""details"": []
}</p>
","<azure-data-factory><azure-synapse>","2019-11-29 02:17:53","426","1","1","59240807","<p>Found the solution creating a new user for the Data Factory App and adding it as a owner </p>

<p>CREATE USER [XXXDW-DataFactory] for EXTERNAL PROVIDER</p>

<p>EXEC sp_addrolemember db_owner, [XXXDW-DataFactory];</p>

<p>GRANT CONTROL ON DATABASE::[DW] to [XXXDW-DataFactory]</p>
"
"59085983","Pagination rules - HTTP response header to make next request","<p>I have a ""copy data"" activity making a REST call to get some data in json format. The data should then be transfered to a SQL database.</p>

<p>The problem is I can only get a certain amount of data for the specified HTTP request. For this I need to implement pagination rules, and I have tried to understand this documentation: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest#pagination-support"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-rest#pagination-support</a></p>

<p>The HTTP response returns the absolute URL for the next request in the header, with the field named ""Link"". As I can see in the documentation, it is supposed to be possible to get the value from ""Link"" and put it into a pagination rule.</p>

<p>As stated in the documentation:</p>

<p><strong>Next request’s absolute or relative URL = header value in current response headers</strong></p>

<p>It says supported pagination keys are <strong>AbsoluteUrl</strong>, with the value of this should be set like this: </p>

<p><strong>Headers.response_header OR Headers['response_header']</strong></p>

<p>Where the response_header is defined like this in the docs:</p>

<p><strong>""response_header"" is user-defined which references one header name in the current HTTP response, the value of which will be used to issue next request.</strong></p>

<p>What I can't seem to understand is how this ""response_header"" can be set to reference the HTTP response header value of ""Link"".</p>
","<azure-data-factory>","2019-11-28 09:51:31","1411","2","1","64375889","<p>You need to replace the 'response_header' placeholder with your header name.
In your case - 'Link'</p>
<p><a href=""https://i.stack.imgur.com/luVzS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/luVzS.png"" alt=""enter image description here"" /></a></p>
<p>Or in code editor:</p>
<pre><code>&quot;paginationRules&quot;: {
   &quot;AbsoluteUrl&quot;: &quot;Headers['Link']&quot;
 }
</code></pre>
"
"59085000","Method to put alerts on long running azure data factory pipeline","<p>I have some data factory pipelines which may sometimes run beyond 2 hours when copying data from blob into SQL. The time period is variable, but I'd like to be notified/alerted when any pipeline runs beyond 2 hours.</p>

<p>What are possible ways of doing this?</p>

<p>What I have tried so far:</p>

<ul>
<li>Explored the adf metrics on which I can put an alert rule. But there seems to be none which talks about active run's duration.</li>
<li>I was hoping to get Pipeline's duration value as we see it on the monitor tab in adf.azure.com and use this to put some sort of alert.</li>
<li>I was also thinking if I can get pipeline start time then maybe i can calculate from current time the total run time and put some alert on top of that.</li>
</ul>

<p><a href=""https://i.stack.imgur.com/SgUNK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/SgUNK.png"" alt=""enter image description here""></a></p>
","<azure-data-factory><azure-log-analytics><azure-monitoring>","2019-11-28 08:58:28","2741","5","3","59289053","<p>One way of doing it work-around wise would be to log a timestamp in your SQL database as a first step in your pipeline and then keep track of the load by monitoring the sessions in your database engine.</p>
"
"59085000","Method to put alerts on long running azure data factory pipeline","<p>I have some data factory pipelines which may sometimes run beyond 2 hours when copying data from blob into SQL. The time period is variable, but I'd like to be notified/alerted when any pipeline runs beyond 2 hours.</p>

<p>What are possible ways of doing this?</p>

<p>What I have tried so far:</p>

<ul>
<li>Explored the adf metrics on which I can put an alert rule. But there seems to be none which talks about active run's duration.</li>
<li>I was hoping to get Pipeline's duration value as we see it on the monitor tab in adf.azure.com and use this to put some sort of alert.</li>
<li>I was also thinking if I can get pipeline start time then maybe i can calculate from current time the total run time and put some alert on top of that.</li>
</ul>

<p><a href=""https://i.stack.imgur.com/SgUNK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/SgUNK.png"" alt=""enter image description here""></a></p>
","<azure-data-factory><azure-log-analytics><azure-monitoring>","2019-11-28 08:58:28","2741","5","3","59290603","<p>We do something like this to track running pipelines and manage execution concurrency. I find Logic Apps and Azure Functions great tools for creating these kinds of solutions. Here is a rough outline of how we handle this:</p>

<ol>
<li>A set of Azure Functions (AF) that leverage the
<a href=""https://github.com/Azure/azure-sdk-for-net"" rel=""nofollow noreferrer"">Microsoft.Azure.Management.DataFactory SDK</a>. The relevant code is at the bottom of this post.</li>
<li>A log of pipeline executions in a SQL Server table. The table includes the PipelineId
and Status, and some other information. You would need to INSERT to this table whenever you create a pipeline. We use a separate Logic App that calls an AF to execute the pipeline using the ""RunPipelineAsync"" method in the code below, capture the new PipelineId (RunId), and send it to a Stored Procedure to log the PipelineId.</li>
<li>A Logic App running on a recurrence trigger(every 3 minutes) that 
a) calls a Stored Procedure that polls the table (#2 above) and returns all pipelines with Status = ""InProgress""; 
b) foreach over the returned list and call an AF (#1 above) that checks the current status of the pipeline using the ""GetPipelineInfoAsync"" method in the code below; 
and 
c) calls another Stored Procedure to update the status in the table.</li>
</ol>

<p>You could do something similar to this and use the ""DurationInMS"" to generate appropriate actions based on status = ""InProgress"" and total running time > {desired alert threshold}.</p>

<p>Here is the DataFactoryHelper class I use:</p>

<pre><code>using Microsoft.IdentityModel.Clients.ActiveDirectory;
using Microsoft.Rest;
using Microsoft.Azure.Management.ResourceManager;
using Microsoft.Azure.Management.DataFactory;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace AzureUtilities.DataFactory
{
    public class DataFactoryHelper
    {
        private ClientCredential Credentials { get; set; }
        private string KeyVaultUrl { get; set; }
        private string TenantId { get; set; }
        private string SubscriptionId { get; set; }

        private DataFactoryManagementClient _client = null;
        private DataFactoryManagementClient Client
        {
            get {
                if (_client == null)
                {
                    var context = new AuthenticationContext(""https://login.windows.net/"" + TenantId);
                    AuthenticationResult result = context.AcquireTokenAsync(""https://management.azure.com/"", Credentials).Result;
                    ServiceClientCredentials cred = new TokenCredentials(result.AccessToken);
                    _client = new DataFactoryManagementClient(cred) { SubscriptionId = SubscriptionId };
                }

                return _client;
            }
        }

        public DataFactoryHelper(string servicePrincipalId, string servicePrincipalKey, string tenantId, string subscriptionId)
        {
            Credentials = new ClientCredential(servicePrincipalId, servicePrincipalKey);
            TenantId = tenantId;
            SubscriptionId = subscriptionId;
        }

        public async Task&lt;string&gt; RunPipelineAsync(string resourceGroupName,
                                                   string dataFactoryName,
                                                   string pipelineName,
                                                   Dictionary&lt;string, object&gt; parameters = null,
                                                   Dictionary&lt;string, List&lt;string&gt;&gt; customHeaders = null)
        {
            var runResponse = await Client.Pipelines.CreateRunWithHttpMessagesAsync(resourceGroupName, dataFactoryName, pipelineName, parameters: parameters , customHeaders: customHeaders);
            return runResponse.Body.RunId;
        }

        public async Task&lt;object&gt; GetPipelineInfoAsync(string resourceGroup, string dataFactory, string runId)
        {
            var info = await Client.PipelineRuns.GetAsync(resourceGroup, dataFactory, runId);
            return new
            {
                RunId = info.RunId,
                PipelineName = info.PipelineName,
                InvokedBy = info.InvokedBy.Name,
                LastUpdated = info.LastUpdated,
                RunStart = info.RunStart,
                RunEnd = info.RunEnd,
                DurationInMs = info.DurationInMs,
                Status = info.Status,
                Message = info.Message
            };
        }
    }
}
</code></pre>
"
"59085000","Method to put alerts on long running azure data factory pipeline","<p>I have some data factory pipelines which may sometimes run beyond 2 hours when copying data from blob into SQL. The time period is variable, but I'd like to be notified/alerted when any pipeline runs beyond 2 hours.</p>

<p>What are possible ways of doing this?</p>

<p>What I have tried so far:</p>

<ul>
<li>Explored the adf metrics on which I can put an alert rule. But there seems to be none which talks about active run's duration.</li>
<li>I was hoping to get Pipeline's duration value as we see it on the monitor tab in adf.azure.com and use this to put some sort of alert.</li>
<li>I was also thinking if I can get pipeline start time then maybe i can calculate from current time the total run time and put some alert on top of that.</li>
</ul>

<p><a href=""https://i.stack.imgur.com/SgUNK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/SgUNK.png"" alt=""enter image description here""></a></p>
","<azure-data-factory><azure-log-analytics><azure-monitoring>","2019-11-28 08:58:28","2741","5","3","74760428","<p>Since September 2022 it is possible to define an elapsed time after which ADF will record a metric in Azure Monitor. Alerting can be triggered from there. This is configured on the Settings tab of a pipeline. Details are found at <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-operationalize-pipelines#preemptive-warnings-for-long-running-jobs"" rel=""nofollow noreferrer"">this link</a>.</p>
"
"59071389","Updating ADFv2 pipeline after creating a trigger against it","<p>If I have a pipeline against which I have created a trigger (say TumblingWindow type) , at some stage if I update the pipeline, will the subsequent execution of the trigger refer to the new pipeline? Or it will keep using the version of the pipeline which was there when the trigger was created?</p>
","<azure-data-factory>","2019-11-27 13:31:39","34","0","1","59076480","<p>New pipeline runs will use the new version of the pipeline, no matter if the run comes from a Manual run or a trigger run.</p>

<p>If there are InProgress Pipeline runs, those will continue running with the same version as they started.</p>
"
"59067400","User Property defined for an ADFv2 activity not being displayed in montior","<p>I created a very simple pipeline just to verify that User Property defined for an activity are being displayed in monitor when the pipeline runs. But it appears that no matter how many user properties I define for the activity, they are not being shown in monitor. I read that the whole purpose of user property is for monitoring, if that itself is not happening I am wondering what is the use of this. So the pipeline just one parameter and one Wait activity. The activity has 2 user properties, one of the properties is getting assigned value of the pipeline parameter while the other user property has a hardcoded string as a value.</p>

<p>Here is the pipeline json:-</p>

<pre><code>    {
    ""name"": ""pipeline1"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""act-Wait"",
                ""type"": ""Wait"",
                ""dependsOn"": [],
                ""userProperties"": [
                    {
                        ""name"": ""UserProperty1"",
                        ""value"": ""@pipeline().parameters.sometable""
                    },
                    {
                        ""name"": ""UserProperty2"",
                        ""value"": ""hardcodedstring""
                    }
                ],
                ""typeProperties"": {
                    ""waitTimeInSeconds"": 1
                }
            }
        ],
        ""parameters"": {
            ""sometable"": {
                ""type"": ""string""
            }
        },
        ""annotations"": []
    }
}
</code></pre>

<p>When I run the pipeline in debug mode, I instantiate value for the pipeline parameter too, but in the monitoring window (that appears at the lower side of a screen) I don't see any of the user properties defined for the activity. I do see everything else about the activity and its successful status etc.</p>
","<azure-data-factory>","2019-11-27 09:52:32","847","0","1","59076747","<p>User properties is at the activity level, you would need to click on ""View Activity Runs"" for the pipeline to be able to see it. You can choose to add it as a column for the monitoring view.</p>

<p><a href=""https://i.stack.imgur.com/Z9Qfv.png"" rel=""nofollow noreferrer"">User Properties screenshot</a></p>
"
"59064152","Start antivirus scan from a script on Azure Linux VM","<p>I have a shell script inside Azure Linux vm. The <strong>Azure Linux VM</strong> has <strong>Sophos</strong> installed in it.The shell script can scan the files present in a directory by using <strong>savscan</strong> command. Challenge is , we are using <strong>azure data factory</strong> pipeline, which needs to call the <strong>azure function</strong>. The azure function should be able to ssh to linux vm, and execute the script. The function should pass the parameters like file path  for the <strong>azure shared file storage</strong> where <strong>Sophos</strong> needs to perform the scan.</p>

<p>I understand thepipeline can call the <strong>http triggered azure function</strong>. But how can we ssh into a vm and run the scripts remotely from the azure function. Also the parameters for the file path will come from data factory.</p>

<pre class=""lang-sh prettyprint-override""><code>inotifywait -mr -e close_write ""/xyz/abc/"" |
while read dir eve file; do
    echo ""new file '$path$file' detected - start scan""
    savscan -eec $path$file
    if [ $? -eq 0 ]
    then
      echo ""1""
    else 
      echo ""0""
    fi
done
</code></pre>
","<linux><azure><virtual-machine><azure-functions><azure-data-factory>","2019-11-27 06:17:32","361","3","2","59082145","<p>Seems there is something wrong with your <code>ScriptPath</code> param . If you are using Azure Automation, we can't place static script files in it , but we can download our script first and place it in ""c:/temp"" folder of Azure automation. </p>

<p>I did some tests on my side , I placed my scripts in Azure storage account , before I need to run this script, I will download it to Azure automation temp folder so that I can specify a path to run it . </p>

<p>Try the PS below in Automation: </p>

<pre><code>$appid = ""&lt;your Azure application ID&gt;""
$passwd = ""&lt;your Azure application password&gt;""
$tenant = ""&lt;tenant&gt;""


$storageName = ""&lt;storage name&gt;""
$containerName = ""&lt;container name&gt;""
$scrtptName = ""&lt;script name&gt;""
$storageResourceGroupName = ""&lt;storage group name&gt;""

$vmName = ""&lt;vm name&gt;""
$VMResourceGroupName = ""&lt;vm group name&gt;""

$secpasswd = ConvertTo-SecureString -String $passwd -AsPlainText -Force
$cred = New-Object Management.Automation.PSCredential ($appid , $secpasswd)
login-AzAccount -Credential $cred -Tenant $tenant  -ServicePrincipal

$storage = Get-AzStorageAccount -ResourceGroupName $storageResourceGroupName -Name $storageName
Get-AzStorageBlobContent -Container $containerName -Blob $scrtptName -Context $storage.Context -Destination ""c:/temp""

$scriptPath = ""c:/temp/$scrtptName"" 

$result = Invoke-AzVMRunCommand -VMname $vmName -ResourceGroupName $VMResourceGroupName  -CommandId 'RunPowerShellScript' -ScriptPath $scriptPath
echo $result.Value

Remove-Item $scriptPath -Force
</code></pre>

<p>Modules I imported :
<a href=""https://i.stack.imgur.com/5MYuy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5MYuy.png"" alt=""enter image description here""></a></p>

<p>I place my script in my storage account, in this case , it is used for download something :
<a href=""https://i.stack.imgur.com/v3puk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v3puk.png"" alt=""enter image description here""></a></p>

<p>My test script content : </p>

<pre><code>$url = ""https://download.microsoft.com/download/1/E/7/1E7B1181-3974-4B29-9A47-CC857B271AA2/English/X64/msodbcsql.msi""
$outpath = ""c:/odbc.msi""

Invoke-WebRequest -Uri $url -OutFile $outpath
</code></pre>

<p><strong>Test on Azure automation and its result :</strong> 
<a href=""https://i.stack.imgur.com/qQOQ6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qQOQ6.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/oQ2P8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oQ2P8.png"" alt=""enter image description here""></a>
As you can see the file has been download successfully . </p>

<p>Btw, there is no need to use remote powershell here , you can use run command feature of Azure VMs to run your scripts on Azure VMs directly . </p>
"
"59064152","Start antivirus scan from a script on Azure Linux VM","<p>I have a shell script inside Azure Linux vm. The <strong>Azure Linux VM</strong> has <strong>Sophos</strong> installed in it.The shell script can scan the files present in a directory by using <strong>savscan</strong> command. Challenge is , we are using <strong>azure data factory</strong> pipeline, which needs to call the <strong>azure function</strong>. The azure function should be able to ssh to linux vm, and execute the script. The function should pass the parameters like file path  for the <strong>azure shared file storage</strong> where <strong>Sophos</strong> needs to perform the scan.</p>

<p>I understand thepipeline can call the <strong>http triggered azure function</strong>. But how can we ssh into a vm and run the scripts remotely from the azure function. Also the parameters for the file path will come from data factory.</p>

<pre class=""lang-sh prettyprint-override""><code>inotifywait -mr -e close_write ""/xyz/abc/"" |
while read dir eve file; do
    echo ""new file '$path$file' detected - start scan""
    savscan -eec $path$file
    if [ $? -eq 0 ]
    then
      echo ""1""
    else 
      echo ""0""
    fi
done
</code></pre>
","<linux><azure><virtual-machine><azure-functions><azure-data-factory>","2019-11-27 06:17:32","361","3","2","61647722","<p>Nice work Stanley, I will test it too on an Ubuntu Machine. Hope it works :)</p>
"
"59058269","azure create function for ADFv2","<p>I'd like to create an Azure function that is triggered by ADFv2.  I've found all sorts of information about pointing the ADFv2 Function widget to your existing Azure Function, but I'm struggling with how this function is triggered.  My functions are currently triggered via a timer.  I've also seen references that the return value needs to be JSON but currently my trigger returns System.Threading.Tasks.Task.  My function is written in C#.</p>

<p>I currently plan to get all the values from Environment.GetEnvironmentVariable, but would like to know how to pull values passed from ADFv2 also.  </p>

<p>What type of trigger do I create a my C# trigger with to use with ADFv2?  What return value do I need to define?  And if anything other than Task, how do I do that?  Finally how to I read parameters passed from ADFv2?</p>
","<c#><azure><azure-functions><azure-data-factory>","2019-11-26 19:38:55","62","0","1","59086896","<p>The Azure Function activity allows you to run Azure Functions in a Data Factory pipeline. To run an Azure Function, you need to create a linked service connection and an activity that specifies the Azure Function that you plan to execute.</p>

<p><a href=""https://i.stack.imgur.com/S9QvE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S9QvE.png"" alt=""enter image description here""></a></p>

<p>The return type of the Azure function has to be a valid JObject. (Keep in mind that JArray is not a JObject.) Any return type other than JObject fails and raises the user error Response Content is not a valid JObject. It's a restriction from Data Factory side.</p>

<p>I don't think , there is any restriction on the trigger side, you can use any of the triggers you have, from the output side, object should be of type JObject.</p>

<p>Additional Reference:</p>

<p><a href=""https://visualbi.com/blogs/microsoft/azure/execute-azure-functions-azure-data-factory/"" rel=""nofollow noreferrer"">https://visualbi.com/blogs/microsoft/azure/execute-azure-functions-azure-data-factory/</a></p>

<p>for the passing parameter, here is how you can achieve that:</p>

<p><a href=""https://i.stack.imgur.com/OUAeV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OUAeV.png"" alt=""enter image description here""></a></p>

<p><a href=""https://azure.microsoft.com/en-in/blog/azure-functions-now-supported-as-a-step-in-azure-data-factory-pipelines/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-in/blog/azure-functions-now-supported-as-a-step-in-azure-data-factory-pipelines/</a></p>

<p>Hope it helps.</p>
"
"59054724","AZURE COSMOS DB with Read and Write enabled on two regions,can we have ADF pipeline running on both?","<p>We are planning to have few operations of ADF pipeline jobs pointing to one primary region of Cosmos DB and other pipeline jobs pointing to another region. we have enabled read and write on both region of Cosmos DB. kindly suggest,thanks!</p>
","<azure><azure-cosmosdb><azure-data-factory>","2019-11-26 15:54:07","177","0","1","59059692","<p>You can set the preferred region setting on the source using Copy Activity or Data Flows</p>
"
"59053229","SSIS fails citing invalid meta data for a specific table","<p>I recently faced an issue where SSIS packages ran locally, but when promoted as ISPAC files to Azure Data Factories live, failed due to invalid meta data.  I removed the strangly built step with a sproc, only for the next step along to start complaining. </p>
","<azure><ssis><azure-data-factory>","2019-11-26 14:32:47","8","0","1","59053230","<p>The solution was simple- my connection string on the live environment (populated by Octopus during deployment) didn't include the ""Provider=SQLNCLI11.1;""  - without it SSIS falls back to the generic OLEDB driver which means it cannot access some of the SQL specific calls it seems to require to query table meta data in the packages.</p>
"
"59052795","Azure data factory pipeline: conditional checking on variable activity","<p>I have a web activity to call a REST API and save it output into a table. But one of its value will not available always. So we need to do a conditional checking while setting its output into a variable activity.
you can see how we have done that in the variable activity. </p>

<p>This is the rest APIs output.</p>

<pre><code>{
 ""value"": {
     ""id"": ""464a115fd3cb"",
     ""runId"": ""464a115fd3cb"",
     ""parameters"": {},
     ""invokedBy"": {
         ""id"": ""99448303872CU28"",
         ""name"": ""TRIGGER_TIMESHEET_API"",
         ""invokedByType"": ""ScheduleTrigger""
     },
     ""isLatest"": true
 },
 ""continuationToken"": ""+RID:~sj5QALRCCB4w5hYAAAAADQ"",
 ""ADFWebActivityResponseHeaders"": {
     ""Pragma"": ""no-cache""

 }
</code></pre>

<p>}</p>

<p>Here ""continuationToken"" will not be a part of all the API responses. So if this value is available in the API response, we need to set that in the variable activity.
In the attached screenshot, you can see that we are setting the variable. But if that key is not available in the API response, it will throw an error. 
So we are looking for a solution to check whether that key is existing in the JSON output.
<a href=""https://i.stack.imgur.com/4AG0v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4AG0v.png"" alt=""enter image description here""></a>  </p>

<p>Any help appreciated.</p>
","<azure><rest><api><azure-data-factory>","2019-11-26 14:10:28","4527","1","1","59065529","<p>I think you almost get your goal already,please use Set Variable Activity and If-Condition Activity:</p>

<p><a href=""https://i.stack.imgur.com/dALBu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dALBu.png"" alt=""enter image description here""></a></p>

<p>Set Variable Activity:</p>

<p><a href=""https://i.stack.imgur.com/eoprH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eoprH.png"" alt=""enter image description here""></a></p>

<p>If-Condition Activity to judge the name is empty or not:</p>

<p><a href=""https://i.stack.imgur.com/fVoZD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fVoZD.png"" alt=""enter image description here""></a></p>

<p>Then you could configure the True Activity and False Activity:</p>

<p><a href=""https://i.stack.imgur.com/1T1g9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1T1g9.png"" alt=""enter image description here""></a></p>
"
"59052635","Unzip gzip files in Azure Data factory","<p>I am wondering if it is possible to set up a source and sick in ADF that will unzip a gzip file and shows the extracted txt file. What happened is that the sink was incorrectly defined where both the source/sink had gzip compression. </p>

<p>So what ended up is that ""fil1.gz"" is now ""file1.gz.gz"".</p>

<p>This is how the file looks in Azure blob:
<a href=""https://i.stack.imgur.com/eoq1K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eoq1K.png"" alt=""enter image description here""></a></p>

<p>This is how the file looks like in an S3 bucket (the end is cut off, but the end is ""txt.gz""):
<a href=""https://i.stack.imgur.com/Qt9Ih.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qt9Ih.png"" alt=""enter image description here""></a></p>

<p>I saw that in COPY there is Zipdeflate and deflate compression, but I get an error that it does not support this type of activity.</p>

<p>I created a sink in an ADF pipeline where I am trying to unzip it. In the datasource screen I used Zipdeflate, but it puts the file name with ""deflate"" extention, and not with the 'txt'.</p>

<p><a href=""https://i.stack.imgur.com/KDbWz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KDbWz.png"" alt=""enter image description here""></a></p>

<p>Thank you</p>
","<azure-data-factory>","2019-11-26 14:00:50","4260","1","1","59359383","<p>create a ""copy data"" object
Source:
as your extenstion is gz, you should choose GZip as compresion type, tick binary copy
Target:
Blob Storage Binary
compresion- none</p>

<p>Such copy pipeline will unzip your text file(s)</p>
"
"59050977","How to rename a folder in ADF(Azure data factory) pipeline","<p>I am relatively new to azure. I just want to know is there any way to rename a folder using azure pipeline. Any help will be appreciated</p>
","<azure><azure-data-factory>","2019-11-26 12:27:10","668","0","1","59065762","<p>Per my understanding, i think your question is equivalent to asking:</p>

<blockquote>
  <p>how to execute windows commands or scripts to manipulate vm file
  system in ADF.</p>
</blockquote>

<p>There is no direct way based on my knowledge because ADF is used for data transfer mostly. As a workaround,you could consider the agency model.</p>

<p>1.Create an API and expose it in the on-premise environment. Its responsibility is to accept the parameters and rename the local folder according to the specified parameters.</p>

<p>2.Use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">Web Activity</a> to call the expose API,pass the target folder name as parameters to that API.</p>

<p>Anyway, ADF has little value in this process, unless you have to do something else in ADF.</p>
"
"59050227","Facing issue in applying Dynamic Content in Azure REST data factory","<p>I have one lookup activity and one Copy data Activity which is using REST linked service to get data from one Rest API. </p>

<p>As, I need to do incremental data, I need to pass max id in the url. So, I have set relative URL as </p>

<p>?id=@dataset().parameters.id</p>

<p>And I am settings Dataset parameter value from Pipeline. Pipeline is working fine but only issue I am facing here is, it is sending to the server - 
@body('dataset46621dd59dc847cd812d0ef8472587e0').parameters.id</p>

<p>So, my issue here is - it is not evaluating expression and setting parameter value. </p>

<p>Please let me know, where I am doing mistake here?</p>

<p>Thanks</p>
","<azure><azure-data-factory>","2019-11-26 11:44:23","354","0","1","59080699","<p>Please use built-in <code>concat()</code> function to construct your relative url:</p>

<pre><code>@concat('?id=',dataset().param)
</code></pre>

<p><a href=""https://i.stack.imgur.com/ZDHU3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZDHU3.png"" alt=""enter image description here""></a></p>
"
"59048453","How to submit cosmos script using ADL","<p>I am working on submitting a cosmos script using azure data lake, i was created a ""Data Lake Analytics"" with the name ""cpprodactivityreportsc08"" in azure portal and trying to submit cosmos script by selecting target VC is""cpprodactivityreportsc08"" through Visual studio, but i am getting below error.</p>

<p>""Job type 'Scope' is not supported by API-version '2017-09-01-preview'""</p>
","<azure-cosmosdb><azure-data-lake><azure-data-factory><azure-data-lake-gen2>","2019-11-26 10:10:44","700","1","1","59151810","<p>It seems that Job type 'Scope' is not supported by <code>API-version '2017-09-01-preview'</code>.</p>

<p>Please try with <code>API-version '2018-06-01'</code>. </p>
"
"59047873","How to write <= condition in Join transformation in mapping data flow","<p>So its looks like I can only select <code>==</code> option when I want to join 2 data source or stream.
But I want to put some <code>&lt;,&gt;,&lt;=,&gt;=</code> condition when I joint 2 data source.
However there is a way to do it by using <code>Cross Join</code> but cross join is not an option in my case, I have to use <code>Outer Join</code>. Is there any option or suggestion that can help my problem? </p>

<p><a href=""https://i.stack.imgur.com/00Frz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/00Frz.png"" alt=""enter image description here""></a>
Here is a sample <code>SQL query</code> that i actually want to gain, maybe it can help you to understand otherwise you can ignore it just focus the ADF part.</p>

<p><code>Select a.col1,a.col2,a.col3,sum(a.col4),b.col0
from T1 a, T2 b
where a.col5 &gt;=b.col1
and a.col5 &lt;= b.col2
group by a.col1,a.col2,a.col3,b.col0</code>  </p>
","<azure><azure-data-factory>","2019-11-26 09:39:35","662","0","1","59065131","<p>Yes, as you said, Data flow <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-join"" rel=""nofollow noreferrer"">Join</a> only support ""<code>==</code>"" option, we can not change it.</p>

<p>I'm afraid there is no option or good suggestions that can help you solve problem in Data Factory.</p>
"
"59043388","Import Azure Monitor log data into Azure Data Factory","<p>Is it possible to connect Azure Data Factory to the Azure Monitor logs to extract the data?
You can connect from Power BI as described here: 
<a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/platform/powerbi"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-monitor/platform/powerbi</a></p>

<p>But I want to be able to connect from Data Factory to the log.</p>
","<azure-data-factory><azure-monitoring>","2019-11-26 03:35:39","2323","2","4","59045234","<p>Per my knowledge,there is no direct way which is similar to the PB way in the link you mentioned in your question in the ADF. Based on this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor#diagnostic-logs"" rel=""nofollow noreferrer"">document</a> and ADF portal UI, we could store the log in three residences:</p>

<ul>
<li>Azure Storage Account.</li>
<li>Event Hub.</li>
<li>Log Analytics.</li>
</ul>

<p><a href=""https://i.stack.imgur.com/6NAg5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6NAg5.png"" alt=""enter image description here""></a></p>

<ul>
<li>For storage account ,you could access them in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage"" rel=""nofollow noreferrer"">copy activity</a>.</li>
<li>For Event hub, maybe you could use <a href=""https://learn.microsoft.com/en-us/rest/api/eventhub/"" rel=""nofollow noreferrer"">Event Hub REST API</a> in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest"" rel=""nofollow noreferrer"">REST dataset</a> and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">ADF Web Activity</a> or you could get an idea of Azure Stream Analytics.</li>
<li><p>For Log Analytics, you could use <a href=""https://dev.loganalytics.io/"" rel=""nofollow noreferrer"">Log Analytics REST API</a> in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest"" rel=""nofollow noreferrer"">REST dataset</a> and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">ADF Web Activity</a>：</p>

<p>POST <a href=""https://api.loganalytics.io/v1/workspaces/DEMO_WORKSPACE/query"" rel=""nofollow noreferrer"">https://api.loganalytics.io/v1/workspaces/DEMO_WORKSPACE/query</a></p>

<pre><code>X-Api-Key: DEMO_KEY
Content-Type: application/json

{
    ""query"": ""AzureActivity | summarize count() by Category""
}
</code></pre></li>
</ul>
"
"59043388","Import Azure Monitor log data into Azure Data Factory","<p>Is it possible to connect Azure Data Factory to the Azure Monitor logs to extract the data?
You can connect from Power BI as described here: 
<a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/platform/powerbi"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-monitor/platform/powerbi</a></p>

<p>But I want to be able to connect from Data Factory to the log.</p>
","<azure-data-factory><azure-monitoring>","2019-11-26 03:35:39","2323","2","4","59151502","<p>Partial Answer:
I have been able to use 2 Web Activities in a pipeline. 1 which gets the bearer token and 2 which then uses the bearer token to carry out the GET command.  BUT now the question is how can I use the Output from the Web Activity in a subsequent Copy Activity so I can load the data into SQL ????</p>
"
"59043388","Import Azure Monitor log data into Azure Data Factory","<p>Is it possible to connect Azure Data Factory to the Azure Monitor logs to extract the data?
You can connect from Power BI as described here: 
<a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/platform/powerbi"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-monitor/platform/powerbi</a></p>

<p>But I want to be able to connect from Data Factory to the log.</p>
","<azure-data-factory><azure-monitoring>","2019-11-26 03:35:39","2323","2","4","59153872","<p>Got is all working. I have done the following: </p>

<ol>
<li>Create a Pipeline which contains 2 Web Activities, 1 For Each Loop &amp; Call to stored procedure to insert the data</li>
<li>First Web Activity gets the bearer token</li>
<li>Second Web Activity calls the REST API GET and has a Header name Authorization which brings in the access_token for the first web activity Bearer {access_token}</li>
<li>Then A For Each Loop which I pass the output for the second Web Activity</li>
<li>Stored procedure Activity which passes in all my fields into an insert stored procedure </li>
</ol>

<p>Finally that all worked. I had a lot of trouble using the Copy Activity so resorted to the For Each Loop and stored procedure call to insert each record from the output of the REST API call in the web activity.</p>

<p>I will post more detailed info once I get some sleep!</p>
"
"59043388","Import Azure Monitor log data into Azure Data Factory","<p>Is it possible to connect Azure Data Factory to the Azure Monitor logs to extract the data?
You can connect from Power BI as described here: 
<a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/platform/powerbi"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-monitor/platform/powerbi</a></p>

<p>But I want to be able to connect from Data Factory to the log.</p>
","<azure-data-factory><azure-monitoring>","2019-11-26 03:35:39","2323","2","4","70887873","<p>There are two methods witch depend on the method of authentication to the API.</p>
<p>The first is with a service principle, High level steps described above. This Blog on the topic is also useful: <a href=""https://datasavvy.me/2020/12/24/retrieving-log-analytics-data-with-data-factory/comment-page-1/#comment-28467"" rel=""nofollow noreferrer"">https://datasavvy.me/2020/12/24/retrieving-log-analytics-data-with-data-factory/comment-page-1/#comment-28467</a></p>
<p>Second is with Managed Identity:</p>
<ol>
<li>first give ADF access to Log Analytics using IAM <a href=""https://stackoverflow.com/questions/58317140/how-can-i-use-this-api-in-azure-data-factory/58317404#58317404"">How can I use this API in Azure Data Factory</a></li>
<li>Then connect to Log Analytic API with Web activity or a copy activity (these are the two i got working).</li>
</ol>
<p><strong>Web Activity</strong></p>
<p><a href=""https://i.stack.imgur.com/j4HCx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j4HCx.png"" alt=""enter image description here"" /></a></p>
<p>URL: <a href=""https://api.loganalytics.io/v1/workspaces/%5BWorkspace"" rel=""nofollow noreferrer"">https://api.loganalytics.io/v1/workspaces/[Workspace</a> ID]/query</p>
<p>Body: {&quot;query&quot;:&quot;search '*'| where TimeGenerated &gt;= datetime(@{pipeline().parameters.it_startDate}) and TimeGenerated &lt; datetime(@{pipeline().parameters.it_endDate}) | distinct $table &quot;}</p>
<p><strong>Copy Activity</strong></p>
<p>First the linked service.
<a href=""https://i.stack.imgur.com/QlEEn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QlEEn.png"" alt=""enter image description here"" /></a></p>
<p>ADF Datasets:
<a href=""https://i.stack.imgur.com/kAiKa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kAiKa.png"" alt=""enter image description here"" /></a></p>
<p>Base URL: URL: <a href=""https://api.loganalytics.io/v1/workspaces/%5BWorkspace"" rel=""nofollow noreferrer"">https://api.loganalytics.io/v1/workspaces/[Workspace</a> ID]/</p>
<p>Copy Source:
<a href=""https://i.stack.imgur.com/Ejefd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ejefd.png"" alt=""enter image description here"" /></a></p>
<p>Body: {
&quot;query&quot;: &quot;@{item()[0]} | where TimeGenerated &gt;= datetime(@{pipeline().parameters.it_startDate}) and TimeGenerated &lt; datetime(@{pipeline().parameters.it_endDate})&quot;
}</p>
<p>Additional:
The body code above, gets a list of the table names in log analytics using the web activity. Which I then pass to the Copy Activity to exports copy of the data for each table.</p>
"
"59043359","De-Serialize multi line JSON to C# object","<pre><code> {""ItemName"":""8"",""Id"":1}
 {""ItemName"":""9"",""Id"":2}
</code></pre>

<p>I am reading json file from blob, each line has above format and row is not even split by comma and also there is no square brackets in a file.</p>

<p>When i try setting SupportMultipleContent true in jsontextreader i get following exception:</p>

<pre><code> Cannot deserialize the current JSON object (e.g. {""name"":""value""}) into type 'System.Collections.Generic.List`1[ValueDTO]' because the type requires a JSON array (e.g. [1,2,3]) to deserialize correctly.
To fix this error either change the JSON to a JSON array (e.g. [1,2,3]) or change the deserialized type so that it is a normal .NET type (e.g. not a primitive type like integer, not a collection type like an array or List&lt;T&gt;) that can be deserialized from a JSON object. JsonObjectAttribute can also be added to the type to force it to deserialize from a JSON object.
Path 'ItemName', line 1, position 12.
</code></pre>

<p>Alternatively if parsing of such json is not possible then how would I configure a datafactory in azure to have the file in correct json format.</p>

<p>Code:</p>

<pre><code>using (var sr = new StreamReader(stream))
{
    using (var jsonTextReader = new JsonTextReader(sr))
    {
        jsonTextReader.SupportMultipleContent = true;
        while (jsonTextReader.Read())
        {
            var data = serializer.Deserialize&lt;T&gt;(jsonTextReader);
            result.Add(data);
        }

    }
}
</code></pre>

<blockquote>
  <p>Json has no explicit \n character</p>
</blockquote>
","<c#><.net><json><.net-core><azure-data-factory>","2019-11-26 03:32:44","3090","1","3","59043740","<p>Each line of your JSON is, by itself, a JSON object. You do not have a JSON array containing all of the objects.</p>

<p>To read it, you just need to rewrite your method to deserialize each line individually:</p>

<pre><code>private static List&lt;ValueDTO&gt; LoadItems(Stream stream)
{
    var result = new List&lt;ValueDTO&gt;();
    using (var reader = new StreamReader(stream))
    {
        string line = null;
        while ((line = reader.ReadLine()) != null)
        {
            if (!string.IsNullOrEmpty(line))
            {
                result.Add(JsonConvert.DeserializeObject&lt;ValueDTO&gt;(line));
            }
        }
    }
    return result;
}
</code></pre>

<p><a href=""https://rextester.com/JUM78348"" rel=""nofollow noreferrer"">Try it online</a></p>
"
"59043359","De-Serialize multi line JSON to C# object","<pre><code> {""ItemName"":""8"",""Id"":1}
 {""ItemName"":""9"",""Id"":2}
</code></pre>

<p>I am reading json file from blob, each line has above format and row is not even split by comma and also there is no square brackets in a file.</p>

<p>When i try setting SupportMultipleContent true in jsontextreader i get following exception:</p>

<pre><code> Cannot deserialize the current JSON object (e.g. {""name"":""value""}) into type 'System.Collections.Generic.List`1[ValueDTO]' because the type requires a JSON array (e.g. [1,2,3]) to deserialize correctly.
To fix this error either change the JSON to a JSON array (e.g. [1,2,3]) or change the deserialized type so that it is a normal .NET type (e.g. not a primitive type like integer, not a collection type like an array or List&lt;T&gt;) that can be deserialized from a JSON object. JsonObjectAttribute can also be added to the type to force it to deserialize from a JSON object.
Path 'ItemName', line 1, position 12.
</code></pre>

<p>Alternatively if parsing of such json is not possible then how would I configure a datafactory in azure to have the file in correct json format.</p>

<p>Code:</p>

<pre><code>using (var sr = new StreamReader(stream))
{
    using (var jsonTextReader = new JsonTextReader(sr))
    {
        jsonTextReader.SupportMultipleContent = true;
        while (jsonTextReader.Read())
        {
            var data = serializer.Deserialize&lt;T&gt;(jsonTextReader);
            result.Add(data);
        }

    }
}
</code></pre>

<blockquote>
  <p>Json has no explicit \n character</p>
</blockquote>
","<c#><.net><json><.net-core><azure-data-factory>","2019-11-26 03:32:44","3090","1","3","59047809","<p>Please use Jsonconvert deserialization method.</p>

<pre><code> List&lt;object&gt; myDeserializedObjList = (List&lt;object&gt;)Newtonsoft.Json.JsonConvert.DeserializeObject(responseContent, typeof(List&lt;object&gt;));
</code></pre>
"
"59043359","De-Serialize multi line JSON to C# object","<pre><code> {""ItemName"":""8"",""Id"":1}
 {""ItemName"":""9"",""Id"":2}
</code></pre>

<p>I am reading json file from blob, each line has above format and row is not even split by comma and also there is no square brackets in a file.</p>

<p>When i try setting SupportMultipleContent true in jsontextreader i get following exception:</p>

<pre><code> Cannot deserialize the current JSON object (e.g. {""name"":""value""}) into type 'System.Collections.Generic.List`1[ValueDTO]' because the type requires a JSON array (e.g. [1,2,3]) to deserialize correctly.
To fix this error either change the JSON to a JSON array (e.g. [1,2,3]) or change the deserialized type so that it is a normal .NET type (e.g. not a primitive type like integer, not a collection type like an array or List&lt;T&gt;) that can be deserialized from a JSON object. JsonObjectAttribute can also be added to the type to force it to deserialize from a JSON object.
Path 'ItemName', line 1, position 12.
</code></pre>

<p>Alternatively if parsing of such json is not possible then how would I configure a datafactory in azure to have the file in correct json format.</p>

<p>Code:</p>

<pre><code>using (var sr = new StreamReader(stream))
{
    using (var jsonTextReader = new JsonTextReader(sr))
    {
        jsonTextReader.SupportMultipleContent = true;
        while (jsonTextReader.Read())
        {
            var data = serializer.Deserialize&lt;T&gt;(jsonTextReader);
            result.Add(data);
        }

    }
}
</code></pre>

<blockquote>
  <p>Json has no explicit \n character</p>
</blockquote>
","<c#><.net><json><.net-core><azure-data-factory>","2019-11-26 03:32:44","3090","1","3","59052013","<p>Following code works on my machine. I am using <code>Newtonsoft.Json</code> version <code>12.0.3</code> targeting <code>netcoreapp3.0</code>.</p>

<pre class=""lang-cs prettyprint-override""><code>using System;
using System.Collections.Generic;
using System.IO;
using Newtonsoft.Json;

namespace ConsoleApp1
{
    class Program
    {
        static void Main(string[] args)
        {
            string json = @""
{'ItemName':'8','Id':1}
{'ItemName':'9','Id':2}
"";

            var items = new List&lt;Item&gt;();

            var serializer = new JsonSerializer();

            using (var sr = new StringReader(json))
            {
                using (var jsonTextReader = new JsonTextReader(sr))
                {
                    jsonTextReader.SupportMultipleContent = true;
                    while (jsonTextReader.Read())
                    {
                        var data = serializer.Deserialize&lt;Item&gt;(jsonTextReader);
                        items.Add(data);
                    }

                }
            }

            foreach (Item item in items)
            {
                Console.WriteLine($""{item.Id}: {item.ItemName}"");
            }
        }
    }

    public class Item
    {
        public string ItemName { get; set; }
        public int Id { get; set; }
    }
}

</code></pre>
"
"59032384","Column delimiter error in copy data in azure data fabric","<p>i'm facing a problem while I'm trying to create a ""copy data"" action in azure data fabric. I'm getting the following error ""Column delimiter cannot be empty string or multi-character string"" when setting the column delimiter to no delimiter for the source file as I want the whole line to be treated as one column.</p>

<p><a href=""https://i.stack.imgur.com/gx3dM.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/gx3dM.png</a></p>

<p>Am I missing something here? Why I'm getting this error?</p>
","<azure><azure-data-factory>","2019-11-25 12:57:19","1341","0","1","59133666","<p>As i know, only one character is allowed.Multiple character delimiter is not supported which is stated in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#text-format"" rel=""nofollow noreferrer"">doc</a>.</p>

<p>If you don't want any format change, you could use binary copy.</p>

<p>Otherwise, maybe you could consider <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">custom activity</a>.</p>
"
"59030961","How can I minimize time while copying data using pipeline from cosmos source to cosmos sink in azure data factory","<p>I am working on copying data from one Azure Cosmos database to another Azure Cosmos Database. The RUs set at source and destination are 800 and 15000 respectively and the container that I am copying has 3GB data and 4 million documents.</p>

<p>While copying through Azure data factory directly from source to destination, it takes forever so I added staging (Azure blob storage) and set the DTUI and parallelization as auto. Copying is quick to Blob storage but while copying from blob storage to Cosmos, the throughput is 3Mbps which keeps reducing to 600 kbps as data size increases.</p>

<p>How can I improve the throughput and minimize time taken for copying. It took me 1 hr 15 mins to copy 3GB data.</p>
","<azure-cosmosdb><azure-data-factory><data-migration>","2019-11-25 11:39:26","233","0","1","59035615","<p>Try this data loading operation using data flows. When using CosmosDB as source or sink in a data flow, you can set the RU throughout for the operation, enabling scale-up during the pipeline run.</p>

<p>We also allow throttling limits per Spark partition. Because data flows uses the CosmosDB Spark bulk API, you can control Spark paritioning in your source &amp; sink as well as CosmosDB throughput. You'll see partitioning options in the Optimize tab.</p>

<p>Those additional controls may give you better performance for your operation. You should test this with a smaller subset of your data against the copy operation rather than trying to load the fill 3GB/4M dataset for this testing.</p>
"
"59030836","Delete a pipeline from Datafactory IDE","<p>I am pretty new to azure datafactory. I have managed to create pipelines and sucessfully deploy them.</p>

<p>However I want now to delete one of them. It seems it is not possible with the default IDE:</p>

<p><a href=""https://i.stack.imgur.com/sR3Wq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sR3Wq.png"" alt=""enter image description here""></a></p>

<p>Am I missing something?</p>
","<azure><azure-pipelines><azure-data-factory>","2019-11-25 11:32:41","114","1","3","59043473","<p>If you are on ADF v2 , this should not be an issue , Select the pipeline and try to delete that , see the screenshot below
<a href=""https://i.stack.imgur.com/t0xBF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t0xBF.png"" alt=""enter image description here""></a></p>
"
"59030836","Delete a pipeline from Datafactory IDE","<p>I am pretty new to azure datafactory. I have managed to create pipelines and sucessfully deploy them.</p>

<p>However I want now to delete one of them. It seems it is not possible with the default IDE:</p>

<p><a href=""https://i.stack.imgur.com/sR3Wq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sR3Wq.png"" alt=""enter image description here""></a></p>

<p>Am I missing something?</p>
","<azure><azure-pipelines><azure-data-factory>","2019-11-25 11:32:41","114","1","3","59142877","<p>Actually the main problem was that I was using the version1 of datafactory</p>
"
"59030836","Delete a pipeline from Datafactory IDE","<p>I am pretty new to azure datafactory. I have managed to create pipelines and sucessfully deploy them.</p>

<p>However I want now to delete one of them. It seems it is not possible with the default IDE:</p>

<p><a href=""https://i.stack.imgur.com/sR3Wq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sR3Wq.png"" alt=""enter image description here""></a></p>

<p>Am I missing something?</p>
","<azure><azure-pipelines><azure-data-factory>","2019-11-25 11:32:41","114","1","3","59271011","<p>At the moment there is very little functionality left to manage DataFactory v1 via UI.</p>

<p>You can use the following PowerShell script to remove the pipeline:</p>

<pre><code>Connect-AzureRmAccount
Remove-AzureRmDataFactoryPipeline -ResourceGroupName ""name"" -DataFactoryName ""name"" -Name ""name""
</code></pre>
"
"59028768","Choosing ""As data in column"" as file name option under Sink Settings for Data Flow is writing data at root of blob storage in Azure Data Factory V2","<p>That’s what I am currently trying to do in Data Flow:</p>

<ol>
<li>Read table from SQL DWH (FileName, ProductID, MachinesCount, UsersCount, LastUsed)</li>
<li>In my Sink settings, I have set “File name option” to “As data in column” and gave FileName column.</li>
</ol>

<p>In my Sink dataset(Azure Blob Storage), I have specified container to “referencedata” and folderPath to “mostused/accounts/newaccounts”. Now whenever Data Flow runs it just writes data on root of Blob Container which is “referencedata” and ignoring folderPath.</p>

<p>Extra information that could be useful: When Data Flow is actually running its writing data inside “mostused/accounts/newaccounts” just like how Azure Databricks does but then moves it to root of Blob Container. I think the logic that copies Azure Databricks multi partitioned files into more human readable files is buggy and its moving files to root of Blob container.</p>
","<azure-data-factory>","2019-11-25 09:34:46","604","0","1","59028769","<p>Found the solution. When choosing “As data in column”, you need to specify the folder path from the container. So if your filename was file.json and you wish to write it in directory “mostused/accounts/newaccounts”, you need to have the column value be “mostused/accounts/newaccounts/file.json”.</p>

<p>I understand this is a confusing experience and Microsoft is working to improve it.</p>
"
"59028023","Linked Service parameterization not working for Linked Service of type Azure Data Explorer (Kusto)","<p>I initially successfully created the following linked service in ADFv2 of type AzureDataExplorer for accessing my database in ADX called CustomerDB:-</p>

<pre><code>{
""name"": ""ls_AzureDataExplorer"",
""properties"": {
    ""type"": ""AzureDataExplorer"",
    ""annotations"": [],
    ""typeProperties"": {
        ""endpoint"": ""https://mycluster.xxxxmaskingregionxxxx.kusto.windows.net"",
        ""tenant"": ""xxxxmaskingtenantidxxxx"",
        ""servicePrincipalId"": ""xxxxmaskingspxxxx"",
        ""servicePrincipalKey"": {
            ""type"": ""AzureKeyVaultSecret"",
            ""store"": {
                ""referenceName"": ""ls_AzureKeyVault_MyKeyVault"",
                ""type"": ""LinkedServiceReference""
            },
            ""secretName"": ""MySecret""
        },
        ""database"": ""CustomerDB""
    }
},
""type"": ""Microsoft.DataFactory/factories/linkedservices""
</code></pre>

<p>}</p>

<p>This worked smoothly. Some values I had to mask for obvious reasons but just wanted to say that there is no issue with this connection. Now inspired from this Microsoft <a href=""https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services"" rel=""nofollow noreferrer"">documentation</a> I am trying to create a generic version of this linked service, which makes sense because otherwise if I have 10 databases in the cluster, I will have to create 10 different linked services.</p>

<p>So I tried to create the parameterized version in the following manner:-</p>

<pre><code>{
""name"": ""ls_AzureDataExplorer_Generic"",
""properties"": {
    ""type"": ""AzureDataExplorer"",
    ""annotations"": [],
    ""typeProperties"": {
        ""endpoint"": ""https://mycluster.xxxxmaskingregionxxxx.kusto.windows.net"",
        ""tenant"": ""xxxxmaskingtenantidxxxx"",
        ""servicePrincipalId"": ""xxxxmaskingspxxxx"",
        ""servicePrincipalKey"": {
            ""type"": ""AzureKeyVaultSecret"",
            ""store"": {
                ""referenceName"": ""ls_AzureKeyVault_MyKeyVault"",
                ""type"": ""LinkedServiceReference""
            },
            ""secretName"": ""MySecret""
        },
        ""database"": ""@{linkedService().DBName}""
    }
},
""type"": ""Microsoft.DataFactory/factories/linkedservices""
</code></pre>

<p>}</p>

<p>But while publishing the changes I keep getting the following error:-</p>

<p><a href=""https://i.stack.imgur.com/FXuk9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FXuk9.png"" alt=""enter image description here""></a></p>

<p>Is there any solution to this? </p>

<p>The article clearly says that:- </p>

<blockquote>
  <p>For all other data stores, you can parameterize the linked service by selecting the Code icon on the Connections tab and using the JSON editor</p>
</blockquote>

<p>So as per that my changes should have been published successfully. But I keep getting the error.</p>
","<azure-data-factory><azure-data-explorer>","2019-11-25 08:49:13","269","0","1","59032655","<p>It appears I need to specify the parameter elsewhere in the same JSON. The followed worked:-</p>

<pre><code>{ 
""name"": ""ls_AzureDataExplorer_Generic"",
""properties"": {
    ""parameters"": {
                ""DBName"": {
                    ""type"": ""string""
                }
            },
    ""type"": ""AzureDataExplorer"",
    ""annotations"": [],
    ""typeProperties"": {
        ""endpoint"": ""https://mycluster.xxxxmaskingregionxxxx.kusto.windows.net"",
        ""tenant"": ""xxxxmaskingtenantidxxxx"",
        ""servicePrincipalId"": ""xxxxmaskingspxxxx"",
        ""servicePrincipalKey"": {
            ""type"": ""AzureKeyVaultSecret"",
            ""store"": {
                ""referenceName"": ""ls_AzureKeyVault_MyKeyVault"",
                ""type"": ""LinkedServiceReference""
            },
            ""secretName"": ""MySecret""
        },
        ""database"": ""@{linkedService().DBName}""
    }
},
""type"": ""Microsoft.DataFactory/factories/linkedservices""
}
</code></pre>
"
"59027769","Azure DataFactory copy job copies extension in the name of the destination file","<p>In Azure Datafactory I copy zipped files from one server and unzip them inside another server.
The source file has the following name and extension ""sourcefile.zip"" with '.zip' not being in the name and actually the extension (checked it with the option hide extensions in file explorer). When the copy job is finished The folder gets decompressed but keeps the '.zip' in the name but is not a zipped folder.</p>

<p>Inside my job file I do a copy job from the dataset of the source server and get the ""@item"" that is given by the foreach loop and concatenate the "".zip"" in the wildcardfileName to capture the actual zip file. because the foreach does not give me the full name of the file and if I don't mention the "".zip"" inside the wildcard I get the error <code>Could not find file</code></p>

<pre><code>   {
                        ""name"": ""CopyDataToFileServer"",
                        ""type"": ""Copy"",
                        ""dependsOn"": [
                            {
                                ""activity"": ""CopyDataToIRserver"",
                                ""dependencyConditions"": [
                                    ""Succeeded""
                                ]
                            }
                        ],
                        ""policy"": {
                            ""timeout"": ""7.00:00:00"",
                            ""retry"": 0,
                            ""retryIntervalInSeconds"": 30,
                            ""secureOutput"": false,
                            ""secureInput"": false
                        },
                        ""userProperties"": [],
                        ""typeProperties"": {
                            ""source"": {
                                ""type"": ""BinarySource"",
                                ""storeSettings"": {
                                    ""type"": ""FileServerReadSettings"",
                                    ""recursive"": true,
                                    ""wildcardFileName"": {
                                        ""value"": ""@{concat(item(),'.zip')}"",
                                        ""type"": ""Expression""
                                    }
                                }
                            },
                            ""sink"": {
                                ""type"": ""BinarySink"",
                                ""storeSettings"": {
                                    ""type"": ""FileServerWriteSettings"",
                                    ""copyBehavior"": ""PreserveHierarchy""
                                }
                            },
                            ""enableStaging"": false
                        },
                        ""inputs"": [
                            {
                                ""referenceName"": ""IntegrationRuntimeStorageZip"",
                                ""type"": ""DatasetReference""
                            }
                        ],
                        ""outputs"": [
                            {
                                ""referenceName"": ""FileServer"",
                                ""type"": ""DatasetReference""
                            }
                        ]
                    },
</code></pre>
","<azure><azure-data-factory>","2019-11-25 08:31:47","59","2","1","59036160","<p>@Bilal: Can you paste the JSON for the pipeline ? Also, is this the JSON for the copy data Activity ? If so, looks incomplete with a comma at the end. </p>

<p>Please provide the above, so we can try to help you.</p>
"
"59016588","Azure Data Factory - Copy Activity - Auto Create Table doesn't work","<p>ADF Copy Activity does not work when you select Auto Create Table in the Sink settings.
More specifically in my scenario I am using dynamic script to copy tables from the SQL Server ContosoRetailDW database to SQL Azure.
In cases where my source table has field like geometry or has a nvarchar field which contains brackets in the string then it fails.
Failed tables to date include DimCustomer, DimEmployee, DimGeography.
The same code works for most other tables.
Not sure this feature was tested before release.</p>

<p>Sample Code in Copy Activity:</p>

<p>Source Query:</p>

<pre><code>SELECT * FROM  @{item().SourceSchema}.@{item().SourceObject} 
</code></pre>

<p>Sink Settings:</p>

<pre><code>Table Option: ""Auto Create Table"" is selected
Pre-Copy Script: DROP TABLE IF EXISTS @{item().SinkSchema}.@{item().SinkObject}
</code></pre>

<p>Like I said this works for most tables when I run it but fails where tables have a geometry field or a nvarchar field string which has things like brackets in the string.
Summary of failed tables:</p>

<ul>
<li>DimCustomer - Field : Phone DataType: nvarchar(20) -- failing as there is a bracket in the string</li>
<li>DimEmoloyee - Field : Phone DataType: nvarchar(25) -- failing as there is a bracket in the string</li>
<li>DimGeography - Field: Geometry  DataType: Geometry  -- Failing in this data type</li>
</ul>

<p>Will this feature be fixed in a near future release?</p>
","<azure><azure-data-factory>","2019-11-24 10:11:49","5815","2","1","59043523","<p>I am not sure if you have seen this but the copy activity does not support all Data types . Please find the list mentioned <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#data-type-mapping"" rel=""nofollow noreferrer"">here</a> </p>
"
"59013837","Triggering Kusto commands using 'ADX Command' activity in ADFv2 vs calling WebAPI on it","<p>In ADFv2 (Azure Data Factory V2) if we need to trigger a command on an ADX (Azure Data Explorer) cluster , we have two choices:-</p>

<ol>
<li>Use 'Azure Data Explorer Commmand' activity</li>
<li>Use POST method provided in the 'WebActivity'  activity</li>
</ol>

<p>Having figured out that both the methods work I would say from development/maintenance point of view the first method sounds more slick and systematic especially because it is out of the box feature to support Kusto in ADFv2. Is there any scenario where the Web Activity method would be more preferable or more performant? I am trying to figure out if it's alright to simply use the ADX Command activity all the time to run any Kusto command from ADFv2 instead of ever using the Web activity,</p>
","<azure-data-factory><azure-data-explorer>","2019-11-24 01:18:51","285","0","1","59017107","<p>It is indeed recommended to use the ""Azure Data Explorer Command"" activity:</p>

<ol>
<li>That activity is more comfortable, as you don't have to construct by yourself a the HTTP request.</li>
<li>That command takes care of few things for you, such as:

<ul>
<li>In case you are running an async command, it will poll the Operations table until your async command is completed.</li>
<li>Logging.</li>
<li>Error handling.</li>
</ul></li>
<li>In addition, you should take into consideration that the result format will be different between both cases, and that each activity has its own limits in terms of response size and timeout.</li>
</ol>
"
"59006136","Is it possible to add existing ADFv2 activities to IF Condition activity as True & False activities?","<p>Say I have a pipeline and there are already two activities in the pipeline as act-A and act-B. Now I add a new activity which is 'IF Condition' activity. Normally I always have to create 'new' activities to be added to IF Condition activity as True &amp; False activities. How can I select existing activities act-A and act-B as true and false activities for the IF Condition activity?</p>
","<azure-data-factory>","2019-11-23 09:11:54","129","0","1","59006217","<p>Looks like cutting and pasting of existing activity works , though the UI doesn't explicitly allows this.</p>
"
"59006049","ADFv2 Date functions not accepting variables as arguments","<p>As per <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#date-functions"" rel=""nofollow noreferrer"">this</a> I am trying to use following date functions within pipeline JSON:-</p>

<pre><code>adddays('2015-03-15T13:27:36Z', -int(variables('NumberOfDaysToSubtract')))
adddays('2015-03-15T13:27:36Z', -variables('NumberOfDaysToSubtract'))
adddays('2015-03-15T13:27:36Z', (-1*variables('NumberOfDaysToSubtract')))
</code></pre>

<p>None of the 3 approaches worked, the expressions are being treated as invalid. Only hardcoded value as shown below works:-</p>

<pre><code>adddays('2015-03-15T13:27:36Z', -10)
</code></pre>

<p>I don't want to hardcode this value. It's smooth if I can somehow use the variable NumberOfDaysToSubtract that I have already defined for the pipeline with the default value as 10.</p>
","<azure-data-factory>","2019-11-23 08:59:57","57","0","1","59006058","<p>What I figured is that setting the variable default value to <strong>-10</strong> and then using the following expression worked:-</p>

<pre><code>adddays('2015-03-15T13:27:36Z', int(variables('NumberOfDaysToSubtract')))
</code></pre>
"
"59001989","Getting actual trigger run start time for Tumbling Window trigger","<p>I am interested in getting actual run start time for Tumbling Window trigger. I don't want Schedule Trigger. My scenario demands for Tumbling Window trigger specifically, but also some logic also requires knowing exactly at what time a triggered run started. As per the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables"" rel=""nofollow noreferrer"">documentation</a> I tried using <code>@pipeline().TriggerTime</code> , basically I passed it as a value to one of the pipeline parameters, but then it was not converted into a value -- then I realized the scope of this expression is within pipeline so I can't use it in a trigger. @trigger().outputs.windowStartTime can be used in a trigger but it doesn't serve my purpose -- I am not looking for a window start time , which is fixed no matter when a trigger is executed. I want actual run start time for Tumbling Window trigger. Is there any solution to this?</p>
","<azure-data-factory>","2019-11-22 21:30:01","549","0","1","59002474","<p>One solution I found is that we create Append Variable activity and call @pipeline().TriggerTime in the value section of the activity. Since this is part of the pipeline, it gets converted into a value there.</p>

<p>Another solution is to simply call utcnow() in the append variable activity.</p>
"
"58999993","How to load avro files from blob storage to azure data factory MOVING DATA FLOW?","<p>How to load avro files from blob storage to azure data factory MOVING DATA FLOW?
I'm trying to load but unable to import schema and preview. 
My avro files in blob are result of event hub capture function. 
I have to move data from azure blob to azure sql db using azure data factory's moving data flow.</p>
","<avro><azure-data-factory><azure-blob-storage><azure-eventhub>","2019-11-22 18:40:17","3227","0","1","59026650","<p>Data Factory support <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs"" rel=""nofollow noreferrer"">Avro files</a>.</p>

<p>In Data Flow, create a new Source from Azure Blob Storage:
<a href=""https://i.stack.imgur.com/FGpHO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FGpHO.png"" alt=""enter image description here""></a></p>

<p>Choose the Avro format for your files:
<a href=""https://i.stack.imgur.com/gftsL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gftsL.png"" alt=""enter image description here""></a></p>

<p>Choose the file path:</p>

<p><a href=""https://i.stack.imgur.com/TYchd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TYchd.png"" alt=""enter image description here""></a></p>

<p>Then you can add the sink to Azure SQL dataset:</p>

<p><a href=""https://i.stack.imgur.com/UahUT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UahUT.png"" alt=""enter image description here""></a></p>

<p>Here is another tool may be helpful for you: <a href=""https://www.striim.com/integrations/avro-azure-sql-database/"" rel=""nofollow noreferrer"">Load AVRO Files Data to Azure SQL Database in Real Time</a></p>

<p>Hope this helps.</p>
"
"58998139","Copy data in Azure SQL DW by using ADF(Staged copy by using Polybase dosen't work according to documentation)","<p>Trying to load csv files in the data lake(gen2) to Azure Synapse by using Azure Data Factory. The source file has ""(double quote) as an escape character. This falls outside the data limitations of directly connecting polybase to Data Lake. I setup the staged copy by the following the documentation</p>

<pre><code>""enableStaging"": true,
  ""stagingSettings"": {
                                ""linkedServiceName"": {
                                    ""referenceName"": ""LS_StagedCopy"",
                                    ""type"": ""LinkedServiceReference""
                                },
                                ""path"": ""myContainer/myPath"",
                                ""enableCompression"": false
                            }
</code></pre>

<p>After I debug the pipeline, I am still getting </p>

<pre><code>{Class=16,Number=107090,State=1,Message=HdfsBridge::recordReaderFillBuffer - Unexpected error encountered filling record reader buffer: HadoopExecutionException: Too many columns in the line.,},],
</code></pre>

<p>I do see ADF creating a temporary folder in the path I supplied in the staged copy, but it looks like it not performing the required transformation to load data. Am I missing anything? </p>

<p>Link to doc <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse"" rel=""nofollow noreferrer"">Copy and transform data in Azure SQL Data Warehouse by using Azure Data Factory</a></p>
","<azure><azure-data-factory><azure-synapse><polybase>","2019-11-22 16:24:29","521","2","1","59359560","<p>Most likely the problem is your data. Check your delimiter. Hope its not "","" or something obvious like this. Its a common problem when one column has a text with many "","" ADF will interpret it as a new column.
Test it with a smaller clean csv and go from there.</p>
"
"58994521","Wanted to get output of databricks activity in the ADF pipeline so that I can use those output parameter in further ADF Activity","<p>I am having a data bricks activity which I am using in ADF and I wanted to get the run output in further activity's like there is one file which I am using in data bricks to get all the days from the column and now I wanted to get all these days as output in data factory parameter so that I can use these day's as parameters in pre-copy script to delete the specific day of data.</p>
","<azure-data-factory><azure-databricks>","2019-11-22 12:42:09","4206","2","2","59033192","<p>If the parameter you want to pass is small, you can do so by using: <code>dbutils.notebook.exit(""returnValue"")</code> (see this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-databricks-notebook#passing-parameters-between-notebooks-and-data-factory"" rel=""nofollow noreferrer"">link</a>).</p>

<p>For a larger set of inputs, I would write the input values from Databricks into a file and iterate (<code>ForEach</code>) over the different values in ADF. </p>
"
"58994521","Wanted to get output of databricks activity in the ADF pipeline so that I can use those output parameter in further ADF Activity","<p>I am having a data bricks activity which I am using in ADF and I wanted to get the run output in further activity's like there is one file which I am using in data bricks to get all the days from the column and now I wanted to get all these days as output in data factory parameter so that I can use these day's as parameters in pre-copy script to delete the specific day of data.</p>
","<azure-data-factory><azure-databricks>","2019-11-22 12:42:09","4206","2","2","59395697","<p>another option would be to return the list of days as a json string and then use that for example:</p>

<pre><code>import json

dates = ['2017-12-11', '2017-12-10', '2017-12-09', '2017-12-08', '2017-12-07']
return_json = json.dumps(dates)

dbutils.notebook.exit(return_json)
</code></pre>

<p>You can then either operate on the output using a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">foreach Activity</a> or do additional processing using the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-append-variable-activity"" rel=""nofollow noreferrer"">Append Variable</a> or <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-set-variable-activity"" rel=""nofollow noreferrer"">
Set Variable</a> Activities</p>

<p>Hope this helps</p>
"
"58993778","Azure data Factory save CSV from URL","<p>I need to download a CSV file from a URL using Azure Data Factory v2. </p>

<p>The URL is: <a href=""https://api.worldtradingdata.com/api/v1/history?symbol=SNAP&amp;output=csv&amp;sort=newest&amp;api_token=demo"" rel=""nofollow noreferrer"">https://api.worldtradingdata.com/api/v1/history?symbol=SNAP&amp;output=csv&amp;sort=newest&amp;api_token=demo</a></p>

<p>Do you know how to do this. I was thinking about downloading it to Blob storage but am unsure what connection to use?</p>

<p>Thanks,
Bob</p>
","<azure-data-factory>","2019-11-22 11:55:27","3425","2","1","58994238","<p>This is easy thanks to the HTTP connector, here is a tutorial: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-http"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-http</a></p>

<p>The tutorial guides you in creating a linked service, a dataset for that linked service and finally do a copy activity using that dataset!!</p>

<p>Should be fairly easy to follow, but if you have any questions be sure to reply me and ask away!</p>

<p>Hope this helped!</p>
"
"58976284","How to escape json in dynamic content in ADF V2 in Azure?","<p>I'm calling Azure function and I'm building the request body using dynamic content.</p>

<p>This is how I build it:</p>

<pre><code>{
  ""test"": ""Test1"",
  ""data"": ""@{activity('Upload SKU').output}""
}
</code></pre>

<p>I have problem with the ""data"" node. <code>@{activity('Upload SKU').output</code> is a json string.
So the dynamic content creates ""mess"". It doesn't escape it.</p>

<p>It creates this:</p>

<pre><code>{
  ""test"": ""Test1"",
  ""data"": ""{""a"": ""1""}""
}
</code></pre>

<p><strong>How to escape <code>@{activity('Upload SKU').output</code> so that <code>{""a"": ""1""}</code> creates <code>{\""a\"": \""1\""}</code> so that it can be treated as a string and not as a node under <code>""data""</code>.</strong></p>

<p>This is what I want to achieve:</p>

<pre><code>{
  ""test"": ""Test1"",
  ""data"": ""{\""a\"": \""1\""}""
}
</code></pre>
","<azure><azure-data-factory>","2019-11-21 13:33:58","2270","4","1","58986555","<p>You could get some clues from my previous case:<a href=""https://stackoverflow.com/questions/58917900/error-badrequest-when-calling-azure-function-in-adf/58949987#58949987"">Error &quot;BadRequest&quot; when calling Azure Function in ADF</a>.</p>

<p>The solution is using <code>@json()</code> and <code>@concat()</code> in dynamic content.For you, the entire <code>data</code> may looks like:</p>

<pre><code>@json(concat('{""test"": ""Test1,""data"":""',@{activity('Upload SKU').output,'""}'))
</code></pre>

<hr>

<p>Just for summary:</p>

<p>It turns out there is error in ADF expression editor because when @Hooch put exactly the same expression directly into ""body"" field without using expression editor it works.</p>
"
"58970695","Functions in Azure Data Factory Pipeline debugging Internal Server Error","<p>I followed exactly the ADF Azure Functions Tutorial at Microsoft Website and when I'm debugging my azure Function, I'm getting after debugging</p>

<pre><code>{
    ""errorCode"": ""3608"",
    ""message"": ""Call to provided Azure function '' failed with status-'InternalServerError' and message - 'Invoking Azure function failed with HttpStatusCode - InternalServerError.'."",
    ""failureType"": ""UserError"",
    ""target"": ""azuretestFunction"",
    ""details"": []
}
</code></pre>

<p>Usually when the '<em>Function Name</em>' is wrong, you'll get a '<strong>Not Found</strong>', but if function name is correct, I got a '<strong>Internal Server Error</strong>'.</p>

<p>On postman azure function works gorgeous.</p>

<p>Any Idea?</p>
","<python><azure><azure-functions><azure-data-factory>","2019-11-21 08:49:13","3883","1","1","58974514","<p>its resolved, it was my mystake --> Got a internal server error cause of missing body parameter. So Internal SErver Error could mean, anything is going wrong with your own code.</p>
"
"58967814","Not able to write data to Snowflake database using ODBC connector from Azure Data Factory","<p>I am trying to read data from Azure Data Lake Storage and write to Snowflake database using Azure Data Factory pipeline.
But I am getting below error. At the same time I am able to read it from Snowflake. I do have write privilege at the database side. I have Integration Runtime installed on the Azure VM, ODBC driver installed on the VM. Able to establish the connection successfully. </p>

<pre><code>Message': 'Failure happened on 'Sink' side. ErrorCode=SystemErrorOdbcWriteError,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR [07009] [Snowflake][ODBC] (10670) Invalid descriptor index, descriptor record does not exist, or descriptor record was not properly initialized.,Source=Microsoft.DataTransfer.ClientLibrary.Odbc.OdbcConnector,''Type=Microsoft.DataTransfer.ClientLibrary.Odbc.Exceptions.OdbcException,Message=ERROR [07009] [Snowflake][ODBC] (10670) Invalid descriptor index, descriptor record does not exist, or descriptor record was not properly initialized.,Source=Snowflake,'',
'EventType': 0,
'Category': 5,
'Data': {
'FailedOdbcDbOperation': 'ERROR [07009] [Snowflake][ODBC] (10670) Invalid descriptor index, descriptor record does not exist, or descriptor record was not properly initialized.',
'FailureInitiator': 'Sink'
},
'MsgId': null,
</code></pre>
","<azure><azure-data-factory><snowflake-cloud-data-platform>","2019-11-21 05:23:11","950","0","2","58984659","<p>There is a feature for integrating with this tool being worked on. I do not have an eta.</p>
"
"58967814","Not able to write data to Snowflake database using ODBC connector from Azure Data Factory","<p>I am trying to read data from Azure Data Lake Storage and write to Snowflake database using Azure Data Factory pipeline.
But I am getting below error. At the same time I am able to read it from Snowflake. I do have write privilege at the database side. I have Integration Runtime installed on the Azure VM, ODBC driver installed on the VM. Able to establish the connection successfully. </p>

<pre><code>Message': 'Failure happened on 'Sink' side. ErrorCode=SystemErrorOdbcWriteError,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR [07009] [Snowflake][ODBC] (10670) Invalid descriptor index, descriptor record does not exist, or descriptor record was not properly initialized.,Source=Microsoft.DataTransfer.ClientLibrary.Odbc.OdbcConnector,''Type=Microsoft.DataTransfer.ClientLibrary.Odbc.Exceptions.OdbcException,Message=ERROR [07009] [Snowflake][ODBC] (10670) Invalid descriptor index, descriptor record does not exist, or descriptor record was not properly initialized.,Source=Snowflake,'',
'EventType': 0,
'Category': 5,
'Data': {
'FailedOdbcDbOperation': 'ERROR [07009] [Snowflake][ODBC] (10670) Invalid descriptor index, descriptor record does not exist, or descriptor record was not properly initialized.',
'FailureInitiator': 'Sink'
},
'MsgId': null,
</code></pre>
","<azure><azure-data-factory><snowflake-cloud-data-platform>","2019-11-21 05:23:11","950","0","2","59095809","<p>I just created a temp solution that could trigger a COPY function from snowflake. You would have to export your data as a file in to a Azure Staging area from ADF then use this function to make a call to Snowflake to ingest the new files.</p>

<p>It essentially allows you to pass connection parameters plus a SQL command to snowflake from ADF.</p>

<p><a href=""https://github.com/NickAkincilar/Snowflake-Azure-DataFactory-Connector"" rel=""nofollow noreferrer"">https://github.com/NickAkincilar/Snowflake-Azure-DataFactory-Connector</a></p>

<p>[snowflake-datawarehouse]</p>
"
"58961505","Insert copied data from XML file into Insert SQL query","<p>I have a process in which all I manually copy/paste all the data from a .xml file and paste into an insert statement that puts the data into a single row on a sql table</p>

<p>Sample XML:</p>

<pre><code>&lt;Data_Export&gt;
    &lt;Header&gt;
        &lt;Sample_Data&gt; This is the data string &lt;/sample_data&gt;
    &lt;/Header&gt;
&lt;/Data_Export&gt;
</code></pre>

<p>Insert query:</p>

<pre><code>INSERT INTO [dbo.table] ([XML])
VALUES ('&lt;Data_Export&gt;
             &lt;Header&gt;
                &lt;Sample_Data&gt; This is the data string &lt;/sample_data&gt;
             &lt;/Header&gt;
          &lt;/Data_Export&gt;')
</code></pre>

<p>I'm looking for a way to automate this piece of the process- so that instead of manually copy/pasting the XML data I can have insert into the single SQL row as part of a SSIS or Data Factory process. </p>
","<sql-server><ssis><azure-data-factory>","2019-11-20 18:56:46","311","1","1","58961603","<p>If the directory and file name is known,</p>

<p><strong>Example</strong></p>

<pre><code>Declare @XML xml; 
Select @XML = BulkColumn FROM  OPENROWSET(BULK 'c:\somedir\data.xml', SINGLE_BLOB) x; 

INSERT INTO [dbo.table] ([XML]) Values ( @XML )
</code></pre>
"
"58956627","How to call a REST API using Azure Data Factory Pipelines?","<p>I would like to execute a REST API with oauth authentication using ADF Pipeline. Without oauth2, I could call any REST APIs. So here my question is, does this ADF pipeline support REST APis with oauth2 authentication ? if yes. Please provide a solution</p>

<p>The API which is am trying, you can find from this URL
<a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipelineruns/querybyfactory"" rel=""noreferrer"">https://learn.microsoft.com/en-us/rest/api/datafactory/pipelineruns/querybyfactory</a></p>

<p>Thanks<br>
Binu</p>
","<azure><rest><api><oauth-2.0><azure-data-factory>","2019-11-20 14:26:13","20329","5","1","58965969","<p>You could call the REST API with a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""noreferrer""><code>Web activity</code></a> in the pipeline, select the Authentication with <code>MSI</code> in the web activity.</p>

<p><a href=""https://i.stack.imgur.com/Fg8IZ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Fg8IZ.png"" alt=""enter image description here""></a></p>

<p>Navigate to your subscription or ADFv2 in the portal -> <code>Access control (IAM)</code> -> <code>Add</code> -> <code>Add role assignment</code> -> search for the name of your ADFv2 and add it as an <code>Owner/Contributor</code> role in the subscription. Then the MSI of your ADFv2 will be able to call the rest api.</p>

<p>For exampple, my ADFv2 named <code>joyfactory</code>, just add the MSI with the same name as an <code>Owner</code> or <code>Contributor</code>.</p>

<p><a href=""https://i.stack.imgur.com/1Enug.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1Enug.png"" alt=""enter image description here""></a></p>

<p>After adding it, check it in the <code>Role assignments</code> , it should be like below.</p>

<p><a href=""https://i.stack.imgur.com/cATdK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/cATdK.png"" alt=""enter image description here""></a></p>

<p>For more details, check this <a href=""https://stackoverflow.com/questions/58317140/how-can-i-use-this-api-in-azure-data-factory/58317404#58317404""><strong>similar issue</strong></a>.</p>
"
"58954750","Azure Services - Data Security","<p>May I ask what is the security protocol (Https/TCPIP etc) applied in the following scenarios in Azure? I need these details to write my design document.</p>

<ol>
<li>Between Azure Services 

<ul>
<li>Azure Data Factory interacting with Azure Storage </li>
<li>Azure Databricks interacting with Azure Storage</li>
</ul></li>
<li>Azure Python SDK connecting to Storage Account (Is it TCP/IP ?)</li>
</ol>

<p>If there is any support page in MS Azure, please direct me there.</p>
","<azure><azure-data-factory>","2019-11-20 12:49:05","30","0","1","58955755","<ol>
<li><p>Inside the Azure data centers used TLS/SSL for communication between
services and you can read about it ""Encryption of data in transit""
section on <a href=""https://learn.microsoft.com/en-us/azure/security/fundamentals/encryption-overview"" rel=""nofollow noreferrer"">this page</a>.   </p></li>
<li><p>The main SDK implementations are wrappers around the REST API and
<a href=""https://learn.microsoft.com/en-us/rest/api/azure/"" rel=""nofollow noreferrer"">Python SDK</a> is one of them.</p></li>
</ol>
"
"58952237","Azure Data Factory Copy using Variable","<p>I am coping data from a rest api to an azure SQL database. The copy is working find but there is a column which isn't being return within the api.</p>

<p>What I want to do is to add this column to the source. I've got a variable called symbol which I want to use as the source column. However, this isn't working:</p>

<p><a href=""https://i.stack.imgur.com/tuE4D.gif"" rel=""nofollow noreferrer"">Mapping</a></p>

<p>Any ideas?</p>
","<azure-data-factory>","2019-11-20 10:34:16","4252","2","3","58970961","<p>Per my knowledge, the copy activity may can't meet your requirements.Please see the error conditions in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#alternative-column-mapping"" rel=""nofollow noreferrer"">link</a>:</p>

<ul>
<li>Source data store query result does not have a column name that is
specified in the input dataset ""structure"" section.</li>
<li>Sink data store (if with pre-defined schema) does not have a column
name that is specified in the output dataset ""structure"" section.</li>
<li>Either fewer columns or more columns in the ""structure"" of sink
dataset than specified in the mapping.</li>
<li>Duplicate mapping.</li>
</ul>

<p>I think <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-overview"" rel=""nofollow noreferrer"">Mapping Data Flow</a> is your choice.You could add a <code>derived column</code> before the sink dataset and create a parameter named <code>Symbol</code>.</p>

<p><a href=""https://i.stack.imgur.com/hAyiB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hAyiB.png"" alt=""enter image description here""></a></p>

<p>Then set the <code>derived column</code> as the value of <code>Symbol</code>.</p>

<p><a href=""https://i.stack.imgur.com/TEExO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TEExO.png"" alt=""enter image description here""></a></p>
"
"58952237","Azure Data Factory Copy using Variable","<p>I am coping data from a rest api to an azure SQL database. The copy is working find but there is a column which isn't being return within the api.</p>

<p>What I want to do is to add this column to the source. I've got a variable called symbol which I want to use as the source column. However, this isn't working:</p>

<p><a href=""https://i.stack.imgur.com/tuE4D.gif"" rel=""nofollow noreferrer"">Mapping</a></p>

<p>Any ideas?</p>
","<azure-data-factory>","2019-11-20 10:34:16","4252","2","3","58972806","<p>You can use the Copy Activity with a stored proc sink to do that.  See my answer <a href=""https://stackoverflow.com/questions/58141391/using-adf-how-to-get-file-names-loaded-into-a-sql-server-table/58155361#58155361"">here</a> for more info.</p>
"
"58952237","Azure Data Factory Copy using Variable","<p>I am coping data from a rest api to an azure SQL database. The copy is working find but there is a column which isn't being return within the api.</p>

<p>What I want to do is to add this column to the source. I've got a variable called symbol which I want to use as the source column. However, this isn't working:</p>

<p><a href=""https://i.stack.imgur.com/tuE4D.gif"" rel=""nofollow noreferrer"">Mapping</a></p>

<p>Any ideas?</p>
","<azure-data-factory>","2019-11-20 10:34:16","4252","2","3","62701317","<p>This functionality is available using the &quot;Additional Columns&quot; feature of the Copy Activity.</p>
<p>If you navigate to the &quot;Source&quot; area, the bottom of the page will show you an area where you can add Additional Columns.  Clicking the &quot;New&quot; button will let you enter a name and a value (which can be dynamic), which will be added to the output.</p>
<p><a href=""https://i.stack.imgur.com/7YKFn.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7YKFn.png"" alt=""Screen grab of copy activity"" /></a></p>
<p>Source(s):
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#add-additional-columns-during-copy"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#add-additional-columns-during-copy</a></p>
"
"58936922","Azure Data Factory architecture with Azure SQL database to Power BI","<p>I'm no MS expert - recently hopped onto the Azure train and apologies in advance if I get some information wrong.</p>

<p>Basically need some input in <strong>Azure's architecture</strong> utilising <strong>Azure Data Factory</strong> (as the ETL/ELT tool) and <strong>Azure SQL database</strong> (as the storage), to a BI output - Power BI. My situation is this;</p>

<ul>
<li>I have on-premise data sources such as <em>Oracle DB, Oracle Cloud SSAS, MS SQL server db</em></li>
<li>I'd like to have a MS cloud infrastructure solution for reporting purposes. </li>
<li>No data migration needed - merely pumping on-prem data onto cloud and producing a BI reporting solution</li>
</ul>

<p>Based on my limited knowledge and Google research, Azure Data Factory caters for all my on-prem sources, as well as the future cloud Azure SQL database. If future analysis is needed, <strong>Azure Storage</strong> and <strong>Azure Databricks</strong> can be added in to this architecture. I have sketched out the architecture of my proposed solution.
<a href=""https://i.stack.imgur.com/NVctm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NVctm.png"" alt=""Proposed Azure Architecture""></a></p>

<p>Just confirming my understanding</p>

<ol>
<li><strong>Without</strong> Azure Storage &amp; Databricks (the 2 pink boxes), the 2 Azure component (DF &amp; SQL database) is sufficient to take data from on-premise sources, process on cloud &amp; output into Power BI.</li>
<li><strong>With</strong> Azure Storage &amp; Databricks (the 2 pink boxes), processing will be more efficient as their summarised function is to store training data models &amp; act as an analytics processing engine.</li>
<li><strong>Azure SQL database</strong> is more suitable, as compared to <strong>Azure SQL datawarehouse</strong> as my data sources does not exceed 1TB; cost-wise is cheaper AND one of my data sources contain data from call centers, hence OLTP is more suitable. Plus I have Azure Databricks to support the analytical bit that SQL datawarehouse does (OLAP).</li>
</ol>

<p>Any other comments to help me understand this whole architecture will be great!</p>
","<azure><azure-sql-database><azure-storage><azure-data-factory><azure-databricks>","2019-11-19 15:00:07","362","2","2","58936990","<p>I am a new learner of Azure. I was wondering if we have @Query (value=""..."") kind or any equivalence for DocumentDb (CosmosDB). Because, the documentDB does not take @Query. I am looking to convert the sql query (From jpa to cosmosDB).</p>
"
"58936922","Azure Data Factory architecture with Azure SQL database to Power BI","<p>I'm no MS expert - recently hopped onto the Azure train and apologies in advance if I get some information wrong.</p>

<p>Basically need some input in <strong>Azure's architecture</strong> utilising <strong>Azure Data Factory</strong> (as the ETL/ELT tool) and <strong>Azure SQL database</strong> (as the storage), to a BI output - Power BI. My situation is this;</p>

<ul>
<li>I have on-premise data sources such as <em>Oracle DB, Oracle Cloud SSAS, MS SQL server db</em></li>
<li>I'd like to have a MS cloud infrastructure solution for reporting purposes. </li>
<li>No data migration needed - merely pumping on-prem data onto cloud and producing a BI reporting solution</li>
</ul>

<p>Based on my limited knowledge and Google research, Azure Data Factory caters for all my on-prem sources, as well as the future cloud Azure SQL database. If future analysis is needed, <strong>Azure Storage</strong> and <strong>Azure Databricks</strong> can be added in to this architecture. I have sketched out the architecture of my proposed solution.
<a href=""https://i.stack.imgur.com/NVctm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NVctm.png"" alt=""Proposed Azure Architecture""></a></p>

<p>Just confirming my understanding</p>

<ol>
<li><strong>Without</strong> Azure Storage &amp; Databricks (the 2 pink boxes), the 2 Azure component (DF &amp; SQL database) is sufficient to take data from on-premise sources, process on cloud &amp; output into Power BI.</li>
<li><strong>With</strong> Azure Storage &amp; Databricks (the 2 pink boxes), processing will be more efficient as their summarised function is to store training data models &amp; act as an analytics processing engine.</li>
<li><strong>Azure SQL database</strong> is more suitable, as compared to <strong>Azure SQL datawarehouse</strong> as my data sources does not exceed 1TB; cost-wise is cheaper AND one of my data sources contain data from call centers, hence OLTP is more suitable. Plus I have Azure Databricks to support the analytical bit that SQL datawarehouse does (OLAP).</li>
</ol>

<p>Any other comments to help me understand this whole architecture will be great!</p>
","<azure><azure-sql-database><azure-storage><azure-data-factory><azure-databricks>","2019-11-19 15:00:07","362","2","2","58938831","<p>Taking data from on-prem or IaaS sources like SQL on a VM, Oracle etc, requires a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime"" rel=""nofollow noreferrer"">Self-Hosted Integration Runtime</a> (SHIR).</p>

<p>Please review the <a href=""https://azure.microsoft.com/en-gb/solutions/architecture/modern-data-warehouse/"" rel=""nofollow noreferrer"">Modern Data Warehouse</a> pattern which sounds similar to what you are proposing.</p>
"
"58931406","Azure Data Factory - Event based triggers on multiple files/blobs","<p>I am invoking an ADF V2 pipeline via an event based trigger when new files/blobs are created in a folder within a blob container. </p>

<p>Blob Container structure:</p>

<p><em>BlobContainer -></em>
<em>FolderName -></em> </p>

<p><em>-> File1.csv</em></p>

<p><em>-> File2.csv</em></p>

<p><em>-> File3.csv</em></p>

<p>I've created the trigger with below configuration:</p>

<p>Container Name: <em>BlobContainer</em></p>

<p>Blob path begins with: <em>FolderName/</em></p>

<p>Blob path ends with: <em>.csv</em></p>

<p>Event Checked:Blob Created</p>

<p><a href=""https://i.stack.imgur.com/ykrSB.png"" rel=""nofollow noreferrer"">Trigger Screenshot</a></p>

<p><strong>Problem:</strong> Three csv files are created in the folder on ad hoc basis. The trigger that invokes the pipeline runs 3 times (probably because 3 blobs are created). The pipeline actually move the files in another blob container. So the 1st trigger run succeeds and remaining 2 fails because the files have been moved already. <strong>However how can I configure the trigger so that it only run once per folder even though 3 files are created within it?</strong> </p>

<p>Because the files are generated together, I am required to move them together into a new location using ADF.</p>
","<azure-data-factory><azure-blob-storage><azure-blob-trigger>","2019-11-19 10:08:06","3808","4","1","58940704","<p>Your blobEventTrigger triggered the pipeline for each file, For it, you can use a 'lookup activity' which gets the filenames and then use filter activity, which filtered the required filename and gives the filterdItemCounts attribute that could be checked in the IF Activity. When there is no file the filterdItemCounts returns '0'and your pipeline not triggered.</p>

<p>Summary- 
Lookup Activity -> Filter Activity -> IF Activity -> Your Pipeline</p>
"
"58923843","How to create and set a variable in azure data factory v2","<p>How do I set an variable of type array of strings in ADF v2 so that I can iterate over each element?  Each element represents a database name?</p>

<p>The 'Set Variable' activity has a dropdown but displays 'no results found'?</p>

<p><a href=""https://i.stack.imgur.com/q8OqT.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/q8OqT.png"" alt=""screen shot""></a>
Many thanks,</p>
","<azure><azure-data-factory>","2019-11-18 22:29:41","17539","9","1","58923966","<p>from the official doc:</p>

<blockquote>
  <p>Use the Set Variable activity to set the value of an existing variable
  of type String, Bool, or Array defined in a Data Factory pipeline.</p>
</blockquote>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-set-variable-activity"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-set-variable-activity</a></p>

<p>You'll need to use the pipelines Variable first:</p>

<p><a href=""https://i.stack.imgur.com/eJ71L.gif"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/eJ71L.gif"" alt=""enter image description here""></a></p>

<p>You can also find a good tutorial in here: <a href=""https://www.mssqltips.com/sqlservertip/6136/azure-data-factory-pipeline-variables/"" rel=""noreferrer"">https://www.mssqltips.com/sqlservertip/6136/azure-data-factory-pipeline-variables/</a></p>
"
"58918399","Azure Data Factory ""flatten hierarchy""","<p>I was hoping someone went through the same process and can help me see if the following scenario is possible.</p>

<p>I currently build out a pipeline that copies from an S3 bucket. That bucket contains a large number of folders. Does Azure Data factory have a way, when copying data from the S3 bucket, to them disregard the folders and just copy the files themselves? I have read that the COPY activity has ""flatten hierarchy"", but the big limitation that I see is that all the files are renamed and I am never sure if those are all of the files that are contained in those folders since it mentions that it only does it ""in the first level of target folder"".</p>

<p>The other problem is that the S3 bucket has nested folders (ex: ""domain/yyyy/mm/dd/file"") and some folders contain data and some do not. The only advantage is that all of those files contain the same schema.</p>

<p>The end result of this pipeline would be the following:</p>

<p>1) COPY files from S3 bucket without copying the folder structure
2) Load the files into an Azure Database</p>

<p>If anyone has done something similar with Azure Data Factory or with another tool I would greatly appreciate your insight.</p>
","<azure><azure-data-factory>","2019-11-18 15:52:03","7816","2","1","58930002","<p>vlado101,firstly,i have to say that the <code>""flatten hierarchy""</code> which you mentioned in your question is for <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#blob-storage-as-a-sink-type"" rel=""nofollow noreferrer"">sink</a>,not source:</p>

<p><a href=""https://i.stack.imgur.com/jSZWL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jSZWL.png"" alt=""enter image description here""></a></p>

<p>Since your destination is SQL DB, i think this copy behavior is not related to your requirements. Based on my test(blob storage,not aws s3,sorry for that because i don't have asw services):</p>

<p>2 json files resident in the subfolder:</p>

<p><a href=""https://i.stack.imgur.com/xrI5K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xrI5K.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/HWq49.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HWq49.png"" alt=""enter image description here""></a></p>

<p>I configured source dataset:</p>

<p><a href=""https://i.stack.imgur.com/DHEOU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DHEOU.png"" alt=""enter image description here""></a></p>

<p>Please make sure the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#legacy-source-model"" rel=""nofollow noreferrer"">recursive</a> is selected as true(Indicates whether the data is read recursively from the subfolders or only from the specified folder. Note that when recursive is set to true and the sink is a file-based store, an empty folder or subfolder isn't copied or created at the sink) and preview source data as below</p>

<p><a href=""https://i.stack.imgur.com/nCZWX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nCZWX.png"" alt=""enter image description here""></a></p>

<p>Execute copy activity, all my data in the subfolder files will be transferred into destination sql db table:</p>

<p><a href=""https://i.stack.imgur.com/bKet8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bKet8.png"" alt=""enter image description here""></a></p>

<p>Surely,this test is based on blob storage,not s3 bucket. I believe they are similar,you could test it.Any concern,please let me know.</p>
"
"58917900","Error ""BadRequest"" when calling Azure Function in ADF","<p>I am creating an extensive data factory work flow that will create and fill a data warehouse for multiple customers automatic, however i'm running into an error. I am going to post the questions first, since the remaining info is a bit long. Keep in mind i'm new to data factory and JSON coding.</p>

<p><strong>Questions &amp; comments</strong></p>

<ul>
<li>How do i correctly pass the parameter through to an Execute Pipeline activity?</li>
<li>How do i add said parameter to an Azure Function activity?</li>
</ul>

<p>The issue may lie with correctly passing the parameter through, or it may lie in picking it up - i can't seem to determine which one. If you spot an error with the current setup, dont hesitate to let me know - all help is appreciated</p>

<p><strong>The Error</strong></p>

<pre><code>{
""errorCode"": ""BadRequest"",
""message"": ""Operation on target FetchEntries failed: Call to provided Azure function
'' failed with status-'BadRequest' and message -
'{\""Message\"":\""Please pass 'customerId' on the query string or in the request body\""}'."",
""failureType"": ""UserError"",
""target"": ""ExecuteFullLoad""
}
</code></pre>

<p><strong>The Setup:</strong></p>

<p>The whole setup starts with a function call to get new customers from an online economic platform. It the writes them to a SQL table, from which they are processed and loaded into the final table, after which a new pipeline is executed. This process works perfectly. From there the following pipeline is executed:</p>

<p><a href=""https://i.stack.imgur.com/pTqo4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pTqo4.png"" alt=""enter image description here""></a></p>

<p>As you can see it all works well until the ForEach loop tries to execute another pipeline, that contains an azure function that calls a .NET scripted function that fills said warehouse (complex i know). This azure function needs a customerid to retrieve tokens and load the data into the warehouse. I'm trying to pass those tokens from the InternalCustomerID lookup through the ForEach into the pipeline and into the function. The ForEach works actually,  but fails <em>""Because an inner activity failed""</em>.</p>

<p>The Execute Pipeline task contains the following settings, where i'm trying to pass the parameter through which comes from the foreach loop. This part of the process also works, since it executes twice (as it should in this test phase): <a href=""https://i.stack.imgur.com/4cLwR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4cLwR.png"" alt=""enter image description here""></a></p>

<p>I dont know if it doesn't successfully pass the parameter through or it fails at adding it to the body of the azure function.</p>

<p>The child pipeline (FullLoad) contains the following parameters. I'm not sure if i should set a default value to be overwritten or how that actually works. The guides i've look at on the internet havent had a default value.
<a href=""https://i.stack.imgur.com/iAs0v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iAs0v.png"" alt=""enter image description here""></a></p>

<p>Finally there is the settings for the Azure function. I'm not sure what i need to write in order to correctly capture the parameter and/or what to fill in - if it's the header or the body regarding the error message. I know a post cannot be executed without a body.<br>
 <a href=""https://i.stack.imgur.com/pa3Vt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pa3Vt.png"" alt=""enter image description here""></a></p>

<p>If i run this specific funtion by hand (using the Function App part of portal.azure.com) it works fine, by using the following settings:</p>

<p><a href=""https://i.stack.imgur.com/Cz1Mo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Cz1Mo.png"" alt=""enter image description here""></a></p>
","<azure><azure-functions><azure-data-factory>","2019-11-18 15:27:22","6012","3","1","58949987","<p>I viewed all of your detailed question and I think the key of the issue is the format of Azure Function Request Body.</p>

<p><a href=""https://i.stack.imgur.com/y5fM7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/y5fM7.png"" alt=""enter image description here""></a></p>

<p>I'm afraid this is incorrect. Please see my below steps based on your description:</p>

<p>Work Flow:</p>

<p><a href=""https://i.stack.imgur.com/mXQRn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mXQRn.png"" alt=""enter image description here""></a></p>

<p>Inside ForEach Activity, only one Azure Function Activity:</p>

<p><a href=""https://i.stack.imgur.com/hDKLH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hDKLH.png"" alt=""enter image description here""></a></p>

<p>The preview data of LookUp Activity:</p>

<p><a href=""https://i.stack.imgur.com/HSgFD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HSgFD.png"" alt=""enter image description here""></a></p>

<p>Then the configuration of ForEach Activity: <code>@activity('Lookup1').output.value</code></p>

<p><a href=""https://i.stack.imgur.com/IhHJ1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IhHJ1.png"" alt=""enter image description here""></a></p>

<p>The configuration of Azure Function Activity: <code>@json(concat('{""name"":""',item().name,'""}'))</code></p>

<p><a href=""https://i.stack.imgur.com/ip7v6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ip7v6.png"" alt=""enter image description here""></a></p>

<p>From the azure function, I only output the input data. Sample Output as below:</p>

<p><a href=""https://i.stack.imgur.com/8jAcq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8jAcq.png"" alt=""enter image description here""></a></p>

<p><strong><em>Tips:</em></strong> I saw your step is executing azure function in another pipeline and using Execute Pipeline Activity, (I don't know why you have to follow such steps), but I think it doesn't matter because you only need to focus on the <code>Body</code> format, if your acceptable format is <code>JSON</code>, you could use <code>@json(....)</code>,if the acceptable format is <code>String</code>, you could use <code>@cancat(....)</code>. Besides, you could check the sample from the ADF UI portal which uses <code>pipeline().parameters</code></p>

<p><a href=""https://i.stack.imgur.com/pfWxu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pfWxu.png"" alt=""enter image description here""></a></p>
"
"58911699","Azure Data Factory Mapping Dataflow add Rownumber","<p>I thought this would be fairly straight forward but i can't really find a simple way of doing it.  I want to add a unique rownumber to a source dataset in a ADF Mapping Dataflow. In SSIS i would have done this with a script component but there's no option for that as far as i can see in ADF.  I've looked for suitable functions in the derived columns expressions editor and also the aggregate component but there doesn't appear to be one.</p>

<p>Any ideas how this could be achieved?</p>

<p>Thanks</p>
","<azure-data-factory><row-number>","2019-11-18 09:43:03","2305","0","2","58922757","<p>Many options:</p>

<ol>
<li>Add a surrogate key transform</li>
<li>Hash row columns in Derived Column using SHA2</li>
<li>Use the rowNumber() function in a Window transformation </li>
</ol>

<p>Give those a shot and let us know what you think</p>
"
"58911699","Azure Data Factory Mapping Dataflow add Rownumber","<p>I thought this would be fairly straight forward but i can't really find a simple way of doing it.  I want to add a unique rownumber to a source dataset in a ADF Mapping Dataflow. In SSIS i would have done this with a script component but there's no option for that as far as i can see in ADF.  I've looked for suitable functions in the derived columns expressions editor and also the aggregate component but there doesn't appear to be one.</p>

<p>Any ideas how this could be achieved?</p>

<p>Thanks</p>
","<azure-data-factory><row-number>","2019-11-18 09:43:03","2305","0","2","64128166","<p>I did like this:</p>
<ol>
<li>Add a Column with the same value to all the rows (I've used an integer with value = 1);</li>
<li>Added a window, using the column create previously on step 1 (Over);</li>
<li>Add a column on step 4 to window (window columns) with any name and <code>rowNumber()</code> as expression;</li>
</ol>
"
"58896347","Reference of previous activity without naming it","<p>In ADFv2 the expression allows referencing the activity by <code>@activity('ActivityName')</code> which can be used to get the output or error. Is it possible to get a reference to the previous activity without knowing or coding the name in the expression?</p>
","<azure-data-factory>","2019-11-17 00:00:48","543","3","1","58909932","<blockquote>
  <p>Is it possible to get a reference to the previous activity without
  knowing or coding the name in the expression?</p>
</blockquote>

<p>No such feature in ADF as i know. I tried to provide a property here for your reference which you may utilize: <code>DependsOn</code>.This property describes the dependencies between components.If multiple activities are caused by previous activity with different statuses,it will be constructed into an array.</p>

<p><a href=""https://i.stack.imgur.com/No3Kp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/No3Kp.png"" alt=""enter image description here""></a></p>

<p>But it is not for usage in the expression,it could be get and set in the sdk. You could refer to some snippet of this case(<a href=""https://stackoverflow.com/questions/58417511/python-azure-data-factory-update-pipeline/58423873"">Python Azure Data Factory Update Pipeline</a>) ,especially for <code>ActivityDependency</code>.</p>
"
"58875709","Azure Data Factory - filter Mongodb source dataset by date","<p>This scenario is pretty straightforward, as described in ADFv2 docs and samples, I've created a copy pipeline to get the data from MongoDB collection, and write it to Azure SQL database. </p>

<p>Full collection data is successfully transfered and all the mappings are set correctly. The problem starts when I try to filter the source dataset to get only the last <em>n</em> days from MongoDB. I've tried several queries, and cross-checked with MongoDB Compass to see if they're actually executing Mongo side, which they are. 
It has come down to this filter query:</p>

<p><code>{""CacheTime"": {$gt: new Date(ISODate().getTime()-1000*60*60*24*5)}}</code></p>

<p>which executes successfully on MongoDB, but fails on ADF with error:</p>

<p><strong>The property value '{""CacheTime"": {$gt: new Date(ISODate().getTime()-1000*60*60*24*5)}}' is invalid. . Activity ID: xxxxx</strong></p>

<p>So I tried using ADF dynamic content to set the current date in ISODate format and pass the whole filter as a string:</p>

<p><code>@concat('{""CacheTime"": {$gt: ISODate(""',formatDateTime(utcnow(),'yyyy-MM-ddThh:mm:ssZ'),'"")}}')</code></p>

<p>This one fails with a similar error:</p>

<p><strong>The property value '{""CacheTime"": {$gt: new Date(ISODate(""2019-11-15T10:45:16Z"")-1000*60*60*24*2)}}' is invalid. . Activity ID: xxxxx</strong></p>

<p>The property value itself ({""CacheTime"": {$gt: new Date(ISODate(""2019-11-15T10:45:16Z"")-1000*60*60*24*2)}}) executes just fine on MongoDB. And I can't find the reason why it doesn't work on ADF. 
Checked the MongoDB documentation if this has something to do with the supported MongoDB version. My MongoDB version is 4.0, but the functions in my query is simple date functions so I believe filter query should not fail.</p>

<p>Has anyone passed a successful query filter with dates to MongoDB and survived?</p>
","<mongodb><filter><azure-data-factory>","2019-11-15 11:05:45","2104","2","1","59098564","<p>Try this.</p>

<p>{""CacheTime"":{$gt: ISODate(""@{adddays(utcnow(),-5)}"")}}</p>
"
"58874603","LastModified Date not getting updated in azure Meta DataActivity","<p>I have created a dataset which points to a Azure Blob Container.Every week one file will be saved in the container .I have created a metadata activity pointing to this data set inside a pipeline. But i am not getting the updated  lastModified date inside MetaData .I am using this date in order to check if any new files has coming in the container folder.Inputs please</p>
","<azure><azure-data-factory>","2019-11-15 10:03:32","89","-1","1","58909116","<p>If the file does comes into container every week once indeed,you could consider using <code>modifiedDatetimeStart</code> and <code>modifiedDatetimeEnd</code> configurations in the GetMetaData Activity.</p>

<p>For example:</p>

<p><a href=""https://i.stack.imgur.com/wnrfU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wnrfU.png"" alt=""enter image description here""></a></p>

<p>Based on the statement in the official document:</p>

<p><a href=""https://i.stack.imgur.com/ySkdG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ySkdG.png"" alt=""enter image description here""></a></p>

<p>you will get the match file names array:</p>

<p><a href=""https://i.stack.imgur.com/PU6T1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PU6T1.png"" alt=""enter image description here""></a></p>

<p>Then get the file name in the ForEach Activity by using <code>@item().name</code>:</p>

<p><a href=""https://i.stack.imgur.com/9g6Hu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9g6Hu.png"" alt=""enter image description here""></a></p>

<p>Please note that <code>modifiedDatetimeStart</code> and <code>modifiedDatetimeEnd</code> could be dynamic content so that you could configure them by using parameters.</p>
"
"58867230","Data Factory V2 copy Data Activities and Data flow ETL","<p>My question about the Data Factory V2 copy-data activity i have 5 questions.</p>

<p><strong>Questions 1</strong></p>

<p>Should I use parquet file or <strong>SQL server With 500 DTU</strong> I want to transfer data fast to staging table or <strong>staging</strong>  parquet file</p>

<p><strong>Questions 2</strong></p>

<p>Copy data activity data integration Unit should i use <strong>auto or 32 data integration Unit</strong></p>

<p><a href=""https://i.stack.imgur.com/AnRTS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AnRTS.png"" alt=""enter image description here""></a></p>

<p><strong>Questions 3</strong></p>

<p>What benefit of using <strong>degree of copy parallelism</strong> should I use <strong>Auto or use 32 again</strong> I want to transfer everything quick as possible I have around 50 million rows every day. </p>

<p><strong>Questions 4</strong></p>

<p><strong>Data Flow Integration</strong> run time so should I use General Purpose, Compute Optimized or Memory Optimized as I mention we have <strong>50 million  rows</strong> every day, so we want to process the data as quickly as possible and somehow cheap if we can in Data Flow </p>

<p><a href=""https://i.stack.imgur.com/bvWHL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bvWHL.png"" alt=""enter image description here""></a></p>

<p><strong>Questions 5</strong></p>

<p>A bulk insert is better in Data Factory and Data flow Sink </p>
","<azure><azure-data-factory>","2019-11-14 22:05:25","340","0","1","58882595","<p>I think you have too many questions about too many topics, the answers to which will depend entirely on your desired end result. Even so, I will do my best to briefly address your situation.</p>

<p>If you are dealing with large volume and/or frequency, Data Flow (ADFDF) would probably be better than Copy activity. ADFDF runs on Spark via Data Bricks and is built from the ground up to run parallel workloads. Parquet is also built to support parallel workloads. If your SQL is an Azure Synapse (SQLDW) instance, then ADFDF will use Polybase to manage the upload, which is very fast because it is also built for parallel workloads. I'm not sure how this differs for Azure SQL, and there is no way to tell you what DTU level will work best for your task.</p>

<p>If having Parquet as your end result is acceptable, then that would probably be the easiest and least expensive to configure since it is just blob storage. ADFDF works just fine with Parquet, as either Source or Sink. For ETL workloads, Compute is the most likely IR configuration. The good news is it is the least expensive of the three. The bad news is I have no way to know what the core count should be, you'll just have to find out through trial and error. 50 million rows may sound like a lot, but it really depends on the row size (byte count and column count), and frequency. If the process is running many times a day, then you can include a ""Time to live"" value in the IR configuration. This will keep the cluster warm while it waits for another job, thus potentially reducing startup time (but incurring more run time cost).</p>
"
"58866133","Delete CSV File from FTP server in Azure Data Factory","<p>I can't seem to figure out how to do this.</p>

<p>I'm trying to use the delete activity to remove the CSV file I just processed in a pipeline.</p>

<p>After setting up the delete activity, I see nothing that indicates that it'll delete the file from my FTP server. After I debug/run the pipeline, I get an error. Everything I've seen related to using this activity is in regard to some other storage type. </p>

<p>Here's the actual error:</p>

<pre><code>{
    ""errorCode"": ""3703"",
    ""message"": ""Invalid delete activity payload with 'folderPath' that is required and cannot be empty."",
    ""failureType"": ""UserError"",
    ""target"": ""DeleteCSVFromFTPServer""
}
</code></pre>

<p>But there's nowhere on the activity to specify the folder path.</p>

<p><a href=""https://i.stack.imgur.com/QSK5H.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QSK5H.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/DJxq1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DJxq1.png"" alt=""enter image description here""></a></p>

<p>Can anyone point me to and FTP specific example of how to use the delete activity?</p>
","<azure><delete-file><azure-data-factory>","2019-11-14 20:38:12","1034","0","1","58866188","<p>I figured I'd answer my own question since I figured this out about 5 minutes after I posted the question. </p>

<p>Hopefully this will help someone else out down the road.</p>

<p>The issue was that in the Dataset I had not supplied a value for the folder path. Leaving it empty worked on import, but would not work for the delete.</p>

<p>I supplied a <code>.</code> in the dataset's <code>file path</code> field as shown below.
<a href=""https://i.stack.imgur.com/Uf2kc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Uf2kc.png"" alt=""enter image description here""></a></p>

<p>Now the pipeline will run completely as I expect.</p>
"
"58863015","azure ADF - Get field list of .csv file from lookup activity","<p>context: azure - ADF  brief process description:</p>

<p>Get a list of the fields defined in the first row of a .csv(blobed) file. This is the first step, detect fields</p>

<p>then 2nd step would be a kind of compare with actual columns of an SQL table</p>

<p>3rd one a stored procedure execution to make the alter table task, finishing with a (customized) table containing all fields needed to successfully load the .csv file into the SQl table.</p>

<p>To begin my ADF pipeline, I set up a lookup activity that ""querys"" the first line of my blobed file, <code>""First row only"" flag = ON</code>.As a second pipeline activity, an ""Append Variable"" task, there I would like to get all .csv fields(first row) retrieved from the lookup activity, as a list. </p>

<p>Here is where a getting the nightmare. </p>

<p>As far I know, with dynamic content I can get an array with all values (w/ format like <code>{""field1_name"":""field1_value_1st_row"", ""field2_name"":""field2_value_1st_row"", etc }</code>) 
with something like <code>@activity('Lookup1').output.firstrow</code>. 
Or any array element with <code>@activity('Lookup1').output.firstrow.&lt;element_name&gt;</code>, 
but I can't figure out how to get a list of all field names (keys?) of the array.</p>

<p>I will appreciate any advice, many thanks!</p>
","<csv><azure-data-factory>","2019-11-14 17:53:55","2164","2","2","58869716","<p>I would save the part of LookUp Activity because it seems that you are familiar with it.</p>

<p>You could use Azure Function HttpTrigger to get the key list of firstrow JSON object. For example your json object like this as you mentioned in your question:</p>

<pre><code>{""field1_name"":""field1_value_1st_row"", ""field2_name"":""field2_value_1st_row""}
</code></pre>

<p>Azure Function code:</p>

<pre><code>module.exports = async function (context, req) {
    context.log('JavaScript HTTP trigger function processed a request.');
    var array = [];
    for(var key in req.body){
        array.push(key);       
    }
    context.res = {
        body: {""keyValue"":array}
    };
};
</code></pre>

<p>Test Output:</p>

<p><a href=""https://i.stack.imgur.com/7Goue.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7Goue.png"" alt=""enter image description here""></a></p>

<p>Then use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure Function Activity</a> to get the output:</p>

<pre><code>@activity('&lt;AzureFunctionActivityName&gt;').keyValue
</code></pre>

<p>Use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">Foreach Activity</a> to loop the keyValue array:</p>

<pre><code>@item()
</code></pre>

<hr>

<p>Still based on the above sample input data,please refer to my sample code:</p>

<pre><code>dct = {""field1_name"": ""field1_value_1st_row"", ""field2_name"": ""field2_value_1st_row""}
list = []
for key in dct.keys():
    list.append(key)

print(list)
dicOutput = {""keys"": list}
print(dicOutput)
</code></pre>

<p><a href=""https://i.stack.imgur.com/npgGs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/npgGs.png"" alt=""enter image description here""></a></p>
"
"58863015","azure ADF - Get field list of .csv file from lookup activity","<p>context: azure - ADF  brief process description:</p>

<p>Get a list of the fields defined in the first row of a .csv(blobed) file. This is the first step, detect fields</p>

<p>then 2nd step would be a kind of compare with actual columns of an SQL table</p>

<p>3rd one a stored procedure execution to make the alter table task, finishing with a (customized) table containing all fields needed to successfully load the .csv file into the SQl table.</p>

<p>To begin my ADF pipeline, I set up a lookup activity that ""querys"" the first line of my blobed file, <code>""First row only"" flag = ON</code>.As a second pipeline activity, an ""Append Variable"" task, there I would like to get all .csv fields(first row) retrieved from the lookup activity, as a list. </p>

<p>Here is where a getting the nightmare. </p>

<p>As far I know, with dynamic content I can get an array with all values (w/ format like <code>{""field1_name"":""field1_value_1st_row"", ""field2_name"":""field2_value_1st_row"", etc }</code>) 
with something like <code>@activity('Lookup1').output.firstrow</code>. 
Or any array element with <code>@activity('Lookup1').output.firstrow.&lt;element_name&gt;</code>, 
but I can't figure out how to get a list of all field names (keys?) of the array.</p>

<p>I will appreciate any advice, many thanks!</p>
","<csv><azure-data-factory>","2019-11-14 17:53:55","2164","2","2","58898444","<p>Have you considered doing this in ADF data flow? You would map the incoming fields to a SQL dataset without a target schema. Define a new table name in the dataset definition and then map the incoming fields from your CSV to a new target table schema definition. ADF will write the rows to a new table using that file's schema.</p>
"
"58853257","how to get Iteration Id for items in array using Azure Data factory","<p>I have a simple ADF pipeline which contains 1 lookup (which loads the name of tables to be migrated) and a ForEach activity (Which contains copy activity and a function App to loads data in BQ). I want to get the Iteration ID and want to send it to Azure function App.
Let say the  Lookup returns a JSON with three tables in it (A,B,C) I want to get the iteration id inside the foreach loop for example 1 for A and 2 for B and 3 for C.</p>

<p>Any help on this will be highly appreciated.</p>
","<azure-data-factory>","2019-11-14 09:21:50","2475","1","2","58869366","<p>I agree this is a common requirement,but it seems no direct way to get the array index inside the <code>for-each</code> activity. However,you could try my little trick with <code>AzureFunction</code> Activity.</p>

<p><strong><em>Step1:</em></strong> Create a text file (named as <code>index.txt</code>)in the some blob storage path and store <code>1</code> value in it(for using it as array index)</p>

<p><strong><em>Step2:</em></strong> Inside the For-each Activity, use LookUp Activity to read the value of <code>index.txt</code>. First time, it is <code>1</code>. </p>

<p><a href=""https://i.stack.imgur.com/dGRIq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dGRIq.png"" alt=""enter image description here""></a></p>

<p><strong><em>Step3:</em></strong> After that, execute an Azure Function Activity to change the value --plus 1.So that,next time it is <code>2</code>.</p>

<p><strong><em>Step4:</em></strong> When you finish For-each Activity,you could reset the value as <code>0</code> by Azure Function Activity.</p>

<p>No need to create 2 azure functions,just 1. You could pass a <code>boolean</code> parameter to distinct whether this invoke is for reset or plus. </p>
"
"58853257","how to get Iteration Id for items in array using Azure Data factory","<p>I have a simple ADF pipeline which contains 1 lookup (which loads the name of tables to be migrated) and a ForEach activity (Which contains copy activity and a function App to loads data in BQ). I want to get the Iteration ID and want to send it to Azure function App.
Let say the  Lookup returns a JSON with three tables in it (A,B,C) I want to get the iteration id inside the foreach loop for example 1 for A and 2 for B and 3 for C.</p>

<p>Any help on this will be highly appreciated.</p>
","<azure-data-factory>","2019-11-14 09:21:50","2475","1","2","58915173","<p>In the lookup table from which I was going to pick the Source and destination tables/databases. I added another column with the Iterator number like 1, 2,3,4 for each row in the Source table from which the lookup activities is retrieving the data.</p>

<p>Then inside Azure data factory, I read that column inside the Foreach loop. For each of the Source and Destination tables I have a self made Iterator and used that for my purpose. It worked perfectly fine for me.</p>
"
"58835810","Azure Data Factory V2 multiple environments like in SSIS","<p>I'm coming from a long SSIS background, we're looking to use Azure data factory v2 but I'm struggling to find any (clear) way of working with multiple environments. In SSIS we would have project parameters tied to the Visual Studio project configuration (e.g. development/test/production etc...) and say there were 2 parameters for SourceServerName and DestinationServerName, these would point to different servers if we were in development or test.</p>

<p>From my initial playing around I can't see any way to do this in data factory. I've searched google of course, but any information I've found seems to be around CI/CD then talks about Git 'branches' and is difficult to follow.</p>

<p>I'm basically looking for a very simple explanation and example of how this would be achieved in Azure data factory v2 (if it is even possible).</p>
","<azure><ssis><azure-data-factory>","2019-11-13 11:09:08","2660","4","3","58845153","<p>Yes, it's possible although not so simple as it was in VS for SSIS.<br>
1) First of all: there is no desktop application for developing ADF, only the browser.<br>
Therefore developers should make the changes in their DEV environment and from many reasons, the best way to do it is a way of working with GIT repository connected.<br>
2) Then, you need ""only"":<br>
a) publish the changes (it creates/updates adf_publish branch in git)<br>
b) With Azure DevOps deploy the code from adf_publish replacing required parameters for target environment.
I know that at the beginning it sounds horrible, but the sooner you set up an environment like this the more time you save while developing pipelines.</p>

<p>How to do these things step by step?<br>
I describe all the steps in the following posts:<br>
- <a href=""https://sqlplayer.net/2018/10/setting-up-code-repository-for-azure-data-factory-v2/"" rel=""nofollow noreferrer"">Setting up Code Repository for Azure Data Factory v2</a><br>
- <a href=""https://sqlplayer.net/2019/06/deployment-of-azure-data-factory-with-azure-devops/"" rel=""nofollow noreferrer"">Deployment of Azure Data Factory with Azure DevOps</a></p>

<p>I hope this helps.</p>
"
"58835810","Azure Data Factory V2 multiple environments like in SSIS","<p>I'm coming from a long SSIS background, we're looking to use Azure data factory v2 but I'm struggling to find any (clear) way of working with multiple environments. In SSIS we would have project parameters tied to the Visual Studio project configuration (e.g. development/test/production etc...) and say there were 2 parameters for SourceServerName and DestinationServerName, these would point to different servers if we were in development or test.</p>

<p>From my initial playing around I can't see any way to do this in data factory. I've searched google of course, but any information I've found seems to be around CI/CD then talks about Git 'branches' and is difficult to follow.</p>

<p>I'm basically looking for a very simple explanation and example of how this would be achieved in Azure data factory v2 (if it is even possible).</p>
","<azure><ssis><azure-data-factory>","2019-11-13 11:09:08","2660","4","3","58847452","<p>It works differently. You create an instance of data factory per environment and your environments are effectively embedded in each instance.</p>

<p>So here's one simple approach:</p>

<ol>
<li>Create three data factories: dev, test, prod</li>
<li>Create your linked services in the dev environment pointing at dev sources and targets</li>
<li>Create the same named linked services in test, but of course these point at your tst systems</li>
<li>Now when you ""migrate"" your pipelines from dev to test, they use the same logical name (just like a connection manager)</li>
</ol>

<p>So you don't designate an environment at execution time or map variables or anything... everything in test just runs against test because that's the way the linked servers have been defined.</p>

<p>That's the first step.</p>

<p>The next step is to connect <em>only</em> the dev ADF instance to Git. If you're a newcomer to Git it can be daunting but it's just a version control system. You save your code to it and it remembers every change you made.</p>

<p>Once your pipeline code is in git, the theory is that you migrate code out of git into higher environments in an automated fashion.</p>

<p>If you go through the links provided in the other answer, you'll see how you set it up.</p>

<p>I <em>do</em> have an issue with this approach though - you have to look up all of your environment values in keystore, which to me is silly because why do we need to designate the test servers hostname everytime we deploy to test?</p>

<p>One last thing is that if you a pipeline that doesn't use a linked service (say a REST pipeline), I haven't found a way to make that environment aware. I ended up building logic around the current data factories name to dynamically change endpoints.</p>

<p>This is a bit of a bran dump but feel free to ask questions.</p>
"
"58835810","Azure Data Factory V2 multiple environments like in SSIS","<p>I'm coming from a long SSIS background, we're looking to use Azure data factory v2 but I'm struggling to find any (clear) way of working with multiple environments. In SSIS we would have project parameters tied to the Visual Studio project configuration (e.g. development/test/production etc...) and say there were 2 parameters for SourceServerName and DestinationServerName, these would point to different servers if we were in development or test.</p>

<p>From my initial playing around I can't see any way to do this in data factory. I've searched google of course, but any information I've found seems to be around CI/CD then talks about Git 'branches' and is difficult to follow.</p>

<p>I'm basically looking for a very simple explanation and example of how this would be achieved in Azure data factory v2 (if it is even possible).</p>
","<azure><ssis><azure-data-factory>","2019-11-13 11:09:08","2660","4","3","58999807","<p>Although it's not recommended - yes, you can do it.<br>
Take a look at Linked Service - in this case, I have a connection to Azure SQL Database:<br>
<a href=""https://i.stack.imgur.com/pxvfZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pxvfZ.png"" alt=""enter image description here""></a>
You have possibilities to use dynamic content for either the server name and database name.
Just add a parameter to your pipeline, pass it to the Linked Service and use in the required field.<br>
Let me know whether I explained it clearly enough?</p>
"
"58829810","How to Check duplicate records in table using ADF?","<p>I am trying to send an alert if there is no records in destination table after copy activity is completed. right now I am try with lookup activity along with if activity but getting this below error</p>

<p>Operation on target Alert If no records in activity ts failed: The function 'int' was invoked with a parameter that is not valid. The value cannot be converted to the target type</p>

<p><a href=""https://i.stack.imgur.com/Ha6C5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ha6C5.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2019-11-13 03:17:50","258","0","1","58836563","<p>You can do this more simple.
Add an variable to you pipeline, varError.
In your lookup, add a select count(*) from your_table
Add an activity ""Set variable"" where you set the variable to </p>

<p><code>@string(div(100, int(activity('IsRecordExist').output.firstrow.yourColumn)))</code></p>

<p>This will do an device by 0 if no values are copied, and cause the pipeline to fail.
You can then sett up a monitor for pipeline failed that sends you an alert.</p>
"
"58828940","How do I dynamically map files in Copy Activity to load the data into destination","<p>Azure Data factory V2 - Copy Activity - Copy data from Changing Column names and number of columns to Destination. I have to copy data from a Flat File where number of Columns will change in each file and even the column names. How do I dynamically map them in Copy Activity to load the data into destination in Azure Data factory V2.</p>

<p>Suppose my destination has 20 columns, but source will come sometimes as 10 columns or 15 or sometimes 20. If the source columns are less than destination then remaining column values in destination should be passed as Null.</p>
","<azure-data-factory>","2019-11-13 01:11:53","427","0","1","58843033","<p>Use data flows in ADF. Data Flow sinks can generate the table schema on the fly if you wish. Or you can just ""auto-map"" any changing schema to your target. If your source schema changes often, just use ""schema drift"" with no schema defined in your dataset.</p>
"
"58822518","How to compare Get Metadata Structure output with known structure to validate file?","<p>I am loading a blob storage file to SQL based on an event trigger in ADF, and want to validate that the metadata for that file conforms to a known template before running subsequent activities. How would I write an expression in an If Condition to check that the 'structure' output object for the Get Metadata activity matches a known structure?</p>

<p>There are a set of collection functions such as 'contains' which may be applicable, but I don't know how to have the expression compare the output object, which I believe is a list, with the string that represents the column names and types. Below is my non-functional attempt...</p>

<pre><code>@equals(activity('Get Metadata').output.structure, '[{""name"": ""ID"",""type"": ""String""},{""name"": ""reg_number"",""type"": ""String""},...,{""name"":""final_column"",""type"":""String""}]')
</code></pre>

<p>Struggling to find any examples of metadata validation within Data Factory online that might help with this. The validation activity seems to simply be a traffic light for whether the blob file exists at all.</p>
","<azure-data-factory>","2019-11-12 16:09:09","1434","2","2","58823348","<p>I identified that the 'structure' output object for Get Metadata activity was an array of json objects, so I used the functions createArray() and json() to compare the objects, which seems to have worked. I'm sure there's a more elegant solution out there though.</p>

<pre><code>@equals(activity('Get Metadata').output.structure,createArray(
        json('{
            ""name"": ""ID"",
            ""type"": ""String""
        }'),...
        json('{
            ""name"": ""final_column"",
            ""type"": ""String""
        }')
))
</code></pre>
"
"58822518","How to compare Get Metadata Structure output with known structure to validate file?","<p>I am loading a blob storage file to SQL based on an event trigger in ADF, and want to validate that the metadata for that file conforms to a known template before running subsequent activities. How would I write an expression in an If Condition to check that the 'structure' output object for the Get Metadata activity matches a known structure?</p>

<p>There are a set of collection functions such as 'contains' which may be applicable, but I don't know how to have the expression compare the output object, which I believe is a list, with the string that represents the column names and types. Below is my non-functional attempt...</p>

<pre><code>@equals(activity('Get Metadata').output.structure, '[{""name"": ""ID"",""type"": ""String""},{""name"": ""reg_number"",""type"": ""String""},...,{""name"":""final_column"",""type"":""String""}]')
</code></pre>

<p>Struggling to find any examples of metadata validation within Data Factory online that might help with this. The validation activity seems to simply be a traffic light for whether the blob file exists at all.</p>
","<azure-data-factory>","2019-11-12 16:09:09","1434","2","2","61605290","<p>I came across this post after I implemented the code below to do similar. My idea was to use a control file with a known schema that could be updated if the schema changes without having to modify the pipeline.</p>

<p>@equals(
activity('Get Metadata Data').output.structure
,
activity('Get Metadata KnownFile').output.structure
)</p>

<p>I will add this technique Tom's solution to my templates but I will store the string in a setup table which can also be updated when required.</p>
"
"58820399","Azure Data Factory V2 Copy data issue - Error code: 2200","<p>I'm trying to pull data from ServiceNow tables using the <strong>REST API</strong> connector in <strong>Azure Data Factory V2</strong>. While pulling the data, I'm getting the following error sometimes and sometimes I'm not getting any error and the pipeline is successfully running.</p>

<pre><code>{
""errorCode"": ""2200"",
""message"": ""Failure happened on 'Source' side. ErrorCode=UserErrorInvalidJsonDataFormat,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Error occurred when deserializing source JSON data. Please check if the data is in valid JSON object format.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=Newtonsoft.Json.JsonReaderException,Message=Invalid character after parsing property name. Expected ':' but got: ,. Path 'result', line 1, position 15740073.,Source=Newtonsoft.Json,'"",
""failureType"": ""UserError"",
""target"": ""REST-API-ServiceNow"" 
}
</code></pre>

<p>Can someone please help me here?</p>

<p>Thanks in advance!</p>
","<json><azure><azure-data-factory><jsonparser>","2019-11-12 14:11:40","8014","1","1","58829360","<blockquote>
  <p>1.Error occurred when deserializing source JSON data. Please check if the data is in valid JSON object format.
  2.Message=Invalid character after parsing property name. Expected ':' but got: ,.</p>
</blockquote>

<p>I think the error details indicates that your source data can't be deserialized by ADF because it's not standard JSON format.Illegal JSON data can't pass through ADF copy activity.</p>

<p>I can't reproduce your issue on my side because i can't touch your source data.However, i suggest you using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">WEB Activity</a> to call your REST API before the execution of Copy Activity. And collect the output of Web Activity (Response from REST API) to store them in other residence. So that you could check whether your source data is legal every time.</p>

<hr>

<p>My idea is as below:</p>

<p>1.Configure Web Activity to call your REST API, then you could get the response from your source data.</p>

<p><a href=""https://i.stack.imgur.com/qAScK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qAScK.png"" alt=""enter image description here""></a></p>

<p>2.Configure a Function App Activity to log the output of above Web Activity. </p>

<p><a href=""https://i.stack.imgur.com/CwfXa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CwfXa.png"" alt=""enter image description here""></a></p>

<p><code>Body</code> should be set the output of Web Activity:<code>@activity('Web1').output</code>, then log it in the function app. Some sample function code as below:</p>

<pre><code>using System;
using System.IO;
using System.Threading.Tasks;
using Microsoft.AspNetCore.Mvc;
using Microsoft.Azure.WebJobs;
using Microsoft.Azure.WebJobs.Extensions.Http;
using Microsoft.AspNetCore.Http;
using Microsoft.Extensions.Logging;
using Newtonsoft.Json;

namespace FunctionAppStoreCSV
{
    public static class Function2
    {
        [FunctionName(""Function2"")]
        public static async Task&lt;IActionResult&gt; Run(
            [HttpTrigger(AuthorizationLevel.Anonymous, ""get"", ""post"", Route = null)] HttpRequest req,
            ILogger log)
        {
            log.LogInformation(""C# HTTP trigger function processed a request."");

            string requestBody = await new StreamReader(req.Body).ReadToEndAsync();


            dynamic data = JsonConvert.DeserializeObject(requestBody);

            log.LogInformation(requestBody);

            return requestBody != null
                ? (ActionResult)new OkObjectResult($""Log Successfully"")
                : new BadRequestObjectResult(""Please pass output in the request body"");
        }
    }
}
</code></pre>

<p>I did a test locally and you may see the log data something like :</p>

<p><a href=""https://i.stack.imgur.com/OMLd8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OMLd8.png"" alt=""enter image description here""></a></p>

<p>If it is in the portal, you could view the log on the KUDU url: <code>D:\home\LogFiles\Application\Functions\function\Function2&gt;</code></p>

<p><a href=""https://i.stack.imgur.com/GVsxh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GVsxh.png"" alt=""enter image description here""></a></p>

<p>3.Connect both of them in ADF before your Copy Activity.</p>

<p><a href=""https://i.stack.imgur.com/IIMEc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IIMEc.png"" alt=""enter image description here""></a></p>

<p>Tips: </p>

<p>My way could save the money for the azure storage,i just log the data so that you could check whether the data is followed strict format. Surely,you could store them in the blob storage. Just write code to store the <code>Body</code> data into Blob storage as you want inside Azure Function app.</p>
"
"58817597","ADF data flow concat expression with single quote","<p>I need to generate a SQL string using Azure data flow expression builder, but it won't allow me to add a single quote between my string using Concat function</p>
<p>I need to have a SQL string as below</p>
<pre><code>SELECT * FROM ABC WHERE myDate &lt;= '2019-10-10'
</code></pre>
<p>Here 2019-10-10 is coming from a parameter, and so expression which I build is like below</p>
<pre><code>concat('SELECT * FROM ABC WHERE myDate &gt;=','''',$ToDate,'''')
</code></pre>
<p>but above statement unable to parse expression.</p>
<p>The result will be executed as a SQL query. SQL query doesn't allow double quote. It has to be single quote.</p>
<p>This is easily achievable using data factory expression but not with azure data flow expression.</p>
","<sql><expression><azure-data-factory><google-cloud-dataflow><quote>","2019-11-12 11:18:58","16714","4","5","58866855","<p>Today, you can do it this way:</p>

<p>'select * from saleslt.product  where myDate >= \'' + $ToDate + '\''</p>

<p>Essentially, just use + with escaped single quotes.</p>

<p>We're releasing a new string interpolation feature next week to make this much easier. Any expression, column, or parameter can be used inside double-quotes.</p>

<p>Your example will look like this:</p>

<p>""SELECT * FROM ABC WHERE myDate >='{$ToDate}'""</p>

<p>Much easier. I'll send out an announcement once the feature is ready next week.</p>
"
"58817597","ADF data flow concat expression with single quote","<p>I need to generate a SQL string using Azure data flow expression builder, but it won't allow me to add a single quote between my string using Concat function</p>
<p>I need to have a SQL string as below</p>
<pre><code>SELECT * FROM ABC WHERE myDate &lt;= '2019-10-10'
</code></pre>
<p>Here 2019-10-10 is coming from a parameter, and so expression which I build is like below</p>
<pre><code>concat('SELECT * FROM ABC WHERE myDate &gt;=','''',$ToDate,'''')
</code></pre>
<p>but above statement unable to parse expression.</p>
<p>The result will be executed as a SQL query. SQL query doesn't allow double quote. It has to be single quote.</p>
<p>This is easily achievable using data factory expression but not with azure data flow expression.</p>
","<sql><expression><azure-data-factory><google-cloud-dataflow><quote>","2019-11-12 11:18:58","16714","4","5","58869265","<p>I called Azure Support and they told me a using bellow expression can help us add single quote to the Column or data :</p>

<pre><code>toString('\''+ toString(byName('col2'))+ '\'')
</code></pre>

<p>So your  expression should be:</p>

<p><code>concat('SELECT * FROM ABC WHERE myDate &gt;=',toString('\''+ toString(byName('$ToDate'))+ '\'') )</code></p>

<p>I tested in my date type column2 and it works well.
<a href=""https://i.stack.imgur.com/tousK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tousK.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"58817597","ADF data flow concat expression with single quote","<p>I need to generate a SQL string using Azure data flow expression builder, but it won't allow me to add a single quote between my string using Concat function</p>
<p>I need to have a SQL string as below</p>
<pre><code>SELECT * FROM ABC WHERE myDate &lt;= '2019-10-10'
</code></pre>
<p>Here 2019-10-10 is coming from a parameter, and so expression which I build is like below</p>
<pre><code>concat('SELECT * FROM ABC WHERE myDate &gt;=','''',$ToDate,'''')
</code></pre>
<p>but above statement unable to parse expression.</p>
<p>The result will be executed as a SQL query. SQL query doesn't allow double quote. It has to be single quote.</p>
<p>This is easily achievable using data factory expression but not with azure data flow expression.</p>
","<sql><expression><azure-data-factory><google-cloud-dataflow><quote>","2019-11-12 11:18:58","16714","4","5","63112395","<p>After doing a lot of trials and experiments,
I got a solution for this,Use ''(one extra quote) instead of '. it will work.
for example.</p>
<pre><code>@concat('Select batchid from execution_Log where Execution_status=''Success'' and Hierachy_ID = ', pipeline().parameters.Hierachy_ID)
</code></pre>
"
"58817597","ADF data flow concat expression with single quote","<p>I need to generate a SQL string using Azure data flow expression builder, but it won't allow me to add a single quote between my string using Concat function</p>
<p>I need to have a SQL string as below</p>
<pre><code>SELECT * FROM ABC WHERE myDate &lt;= '2019-10-10'
</code></pre>
<p>Here 2019-10-10 is coming from a parameter, and so expression which I build is like below</p>
<pre><code>concat('SELECT * FROM ABC WHERE myDate &gt;=','''',$ToDate,'''')
</code></pre>
<p>but above statement unable to parse expression.</p>
<p>The result will be executed as a SQL query. SQL query doesn't allow double quote. It has to be single quote.</p>
<p>This is easily achievable using data factory expression but not with azure data flow expression.</p>
","<sql><expression><azure-data-factory><google-cloud-dataflow><quote>","2019-11-12 11:18:58","16714","4","5","67496829","<p>I also had a lot of trouble with this, trying to pass 2 timestamp variables with quotes around them. Lots of trail and error and I was able to use string interpolation to get it going. Expression ended up being:</p>
<pre><code>&quot;SELECT c.data.user_id ... FROM c WHERE c.data.date &gt; '{toString($StartTime)}' AND c.data.date &lt;= '{toString($EndTime)}'&quot;
</code></pre>
"
"58817597","ADF data flow concat expression with single quote","<p>I need to generate a SQL string using Azure data flow expression builder, but it won't allow me to add a single quote between my string using Concat function</p>
<p>I need to have a SQL string as below</p>
<pre><code>SELECT * FROM ABC WHERE myDate &lt;= '2019-10-10'
</code></pre>
<p>Here 2019-10-10 is coming from a parameter, and so expression which I build is like below</p>
<pre><code>concat('SELECT * FROM ABC WHERE myDate &gt;=','''',$ToDate,'''')
</code></pre>
<p>but above statement unable to parse expression.</p>
<p>The result will be executed as a SQL query. SQL query doesn't allow double quote. It has to be single quote.</p>
<p>This is easily achievable using data factory expression but not with azure data flow expression.</p>
","<sql><expression><azure-data-factory><google-cloud-dataflow><quote>","2019-11-12 11:18:58","16714","4","5","72289284","<p>You can specify by adding 4 comma at both side to parameter. This will applicable when you passing SQL query and the parameter is in String format.</p>
<pre><code>@concat('SELECT * FROM DW_DEV.Schema_Name.DEMO_CONFIG WHERE SOURCESYSTEM = ','''',pipeline().parameters.SourceSystem,'''')
</code></pre>
"
"58815827","Changing path of ADFv2 Hive logs for HDInsight","<p>I have noticed that whenever I run any ADFv2 pipeline (HDInsightHive activity), a container 'adfjobs' is created in the default storage for the cluster if it's not already existing. Also, a folder (call it pseudo folder since it's blob based) HiveQueryJobs appears under that container which in turn seems to be storing logs of various run instances. Can we change this path in an ADF acivity? I haven't seen any such option yet. This would be a good option where lets say one would like to create a more logically meaningful path for log files of each run (e.g. pipelinename/date_id).</p>
","<azure-hdinsight><azure-data-factory>","2019-11-12 09:39:40","58","0","1","58833972","<p>Unfortunately, you cannot change location of Azure Data Factory job logs.</p>

<p>By default, Azure Data Factory create a folder name ""adfjobs"" to store the ADF job logs.</p>

<p>The ""adflogs"" folders saves the log files with meaningful path.</p>

<p><strong>Example:</strong> I have submitted a mapreduce job and the logs files related to this job is located in the path ""<code>adfjobs / MapReduceJobs / 04e3074c-b641-4a1d-b766-29bc48b41884 / 13_08_2019_05_13_43_619 / Status</code>"" which has meaningful path ""<code>logs folder / Jobtype / RunID / Timestamp / Status of job</code>"".</p>

<p><a href=""https://i.stack.imgur.com/Ego5P.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ego5P.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"58814191","Is it possible to source Hive script through Git for HDInsightHive activity type in ADFv2?","<p>This question is about ADF v2. We have activity of type HDInsightHive where script is sourced through Azure Storage. This is not very good way to source a code. It would be good if we could source this script from some existing Git repo. Is it possible? I didn't find any such option but just double checking.</p>
","<azure-data-factory>","2019-11-12 07:49:40","47","1","1","58833321","<p>According to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-hadoop-hive"" rel=""nofollow noreferrer"">official document</a>,git repo configuration is not supported for <code>FilePath</code> property in ADF Hive Activity directly.</p>

<p>I try to share a workaround with you that using <code>azure-storage-fuse</code> which provides a virtual filesystem backed by the Azure Blob storage.It could maps the file system directory(it could be your git repo) in your linux server to the Azure Blob Storage.Please see more details from this <a href=""https://github.com/Azure/azure-storage-fuse"" rel=""nofollow noreferrer"">github code</a>.</p>
"
"58793564","Azure Data Factory V2 - Input and Output","<p>I'm trying to reproduce the following architecture based on the following github repo: <a href=""https://github.com/Azure/cortana-intelligence-price-optimization"" rel=""nofollow noreferrer"">https://github.com/Azure/cortana-intelligence-price-optimization</a></p>

<p>The problem is the part linked to the ADF, since in the guide it uses the old version of ADF: I don't know how to map in ADF v2 the ""input"" and ""output"" properties of a single activity so that they point to a dataset.</p>

<p>The pipeline performs a spark activity that does nothing more than execute a python script, and then I think it should write data into the dataset I defined already.</p>

<p>Here is the json of the ADF V1 pipeline inside the guide, which I cannot replicate:</p>

<pre><code>    ""activities"": [
    {
      ""type"": ""HDInsightSpark"",
      ""typeProperties"": {
        ""rootPath"": ""adflibs"",
        ""entryFilePath"": ""Sales_Data_Aggregation_2.0_blob.py"",
        ""arguments"": [ ""modelsample"" ],
        ""getDebugInfo"": ""Always""
      },
      ""outputs"": [
        {
          ""name"": ""BlobStoreAggOutput""
        }
      ],
      ""policy"": {
        ""timeout"": ""00:30:00"",
        ""concurrency"": 1,
        ""retry"": 1
      },
      ""scheduler"": {
        ""frequency"": ""Hour"",
        ""interval"": 1
      },
      ""name"": ""AggDataSparkJob"",
      ""description"": ""Submits a Spark Job"",
      ""linkedServiceName"": ""HDInsightLinkedService""
    },
</code></pre>
","<azure><pipeline><azure-hdinsight><azure-data-factory>","2019-11-10 22:38:02","378","3","1","58931005","<p>The Spark activity in a Data Factory pipeline executes a Spark program on your own or on-demand HDInsight cluster. This <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-spark"" rel=""nofollow noreferrer"">article</a> builds on the data transformation activities article, which presents a general overview of data transformation and the supported transformation activities. When you use an on-demand Spark linked service, Data Factory automatically creates a Spark cluster for you just-in-time to process the data and then deletes the cluster once the processing is complete.</p>

<p>Upload ""Sales_Data_Aggregation_2.0_blob.py"" to storage account attached to the HDInsight cluster and the modify the sample definition of a spark activity and create a schedule trigger and run the code:</p>

<p>Here is the sample JSON definition of a Spark activity:</p>

<pre><code> {
    ""name"": ""Spark Activity"",
    ""description"": ""Description"",
    ""type"": ""HDInsightSpark"",
    ""linkedServiceName"": {
        ""referenceName"": ""MyHDInsightLinkedService"",
        ""type"": ""LinkedServiceReference""
    },
    ""typeProperties"": {
        ""sparkJobLinkedService"": {
            ""referenceName"": ""MyAzureStorageLinkedService"",
            ""type"": ""LinkedServiceReference""
        },
        ""rootPath"": ""adfspark"",
        ""entryFilePath"": ""test.py"",
        ""sparkConfig"": {
            ""ConfigItem1"": ""Value""
        },
        ""getDebugInfo"": ""Failure"",
        ""arguments"": [
            ""SampleHadoopJobArgument1""
        ]
    }
}
</code></pre>

<p>Hope this helps.</p>
"
"58770282","Fail azure data factory pipeline if notebook execution skipped","<p><a href=""https://i.stack.imgur.com/FAUGB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FAUGB.png"" alt=""enter image description here""></a>I have build an ADF pipline and i am executing multiple databricks notebooks in the pipeline. When one of the notebook fails the remianing command in the notebook skipped but the pipeline did not fail. I want to make sure that the pipeline execution stops if the notebook fails and next command skipped</p>

<p>I have tried trigering an error so that the pipeline fails</p>

<p>NA</p>

<p>I want to make sure that the pipeline execution stops if the notebook fails and next command skipped</p>
","<azure-data-factory><azure-databricks>","2019-11-08 16:19:40","1002","2","2","58781064","<p>If you connect the Azure Databricks acitivities as follows:</p>

<p><a href=""https://i.stack.imgur.com/ajwn8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ajwn8.png"" alt=""Pipeline Design""></a></p>

<p>Notebook2-Activity will only be executed if the first Notebook-Activity is successfull. </p>

<p>Here the first activity fails and the next is not executed (Monitoring view of Azure Data Factory):</p>

<p><a href=""https://i.stack.imgur.com/sbZ2u.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sbZ2u.png"" alt=""enter image description here""></a></p>
"
"58770282","Fail azure data factory pipeline if notebook execution skipped","<p><a href=""https://i.stack.imgur.com/FAUGB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FAUGB.png"" alt=""enter image description here""></a>I have build an ADF pipline and i am executing multiple databricks notebooks in the pipeline. When one of the notebook fails the remianing command in the notebook skipped but the pipeline did not fail. I want to make sure that the pipeline execution stops if the notebook fails and next command skipped</p>

<p>I have tried trigering an error so that the pipeline fails</p>

<p>NA</p>

<p>I want to make sure that the pipeline execution stops if the notebook fails and next command skipped</p>
","<azure-data-factory><azure-databricks>","2019-11-08 16:19:40","1002","2","2","69281672","<p>Use <code>assert</code>; here a the Scala code snippet:</p>
<pre><code>try {
  ... ... ...
}
catch {
  case e: Exception =&gt; {
    println(&quot;exception caught while ... ... : &quot; + e)
    assert(false)
  }
}
</code></pre>
<p>This will fail your Notebook Activity in Azure Data Factory.</p>
"
"58763752","What is ADF Mapping data flow expression language?","<p>I am using Azure Data factory Mapping data flow and i just noticed that in Mapping data flow Expression Builder the  Expression i build is actually in expression language.</p>

<p>My client ask about that expression language, ""Is it <code>json</code> or <code>SQL</code>?""
So what is the answer will be? 
It is just  expression language or anything else?</p>
","<azure><el><azure-data-factory>","2019-11-08 09:25:29","1836","1","2","58773374","<p>The expressions that you build in the Expression Builder are made up of the ""high order"" functions exposed to the transformations in the ADF Data Flow UI. The full functions list is here: <a href=""https://aka.ms/dataflowexpressions"" rel=""nofollow noreferrer"">https://aka.ms/dataflowexpressions</a>.</p>
"
"58763752","What is ADF Mapping data flow expression language?","<p>I am using Azure Data factory Mapping data flow and i just noticed that in Mapping data flow Expression Builder the  Expression i build is actually in expression language.</p>

<p>My client ask about that expression language, ""Is it <code>json</code> or <code>SQL</code>?""
So what is the answer will be? 
It is just  expression language or anything else?</p>
","<azure><el><azure-data-factory>","2019-11-08 09:25:29","1836","1","2","58814276","<p>It's just the &quot;expression language&quot;, it's not a kind of language like <code>json</code> or <code>SQL</code>.</p>
<p>Data Factory doesn't named for the expression, just call it &quot;expression language&quot;.</p>
<p><a href=""https://i.stack.imgur.com/odtom.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/odtom.png"" alt=""expression language"" /></a></p>
<p>Hope this helps.</p>
"
"58760510","Timezone option is not working in Azure data factory","<p>I use Google Chrome browser. When I click the Timezone filter option, it does nothing. It doesn't list various time zones as it used to be.</p>

<p>Can someone help in this regard?</p>
","<azure-data-factory>","2019-11-08 04:45:38","41","0","1","58761500","<p>It got resolved on its own after clearing the cookies. </p>
"
"58739184","Azure Data Factory - Copy Data from PostgreSQL DB on Ubuntu","<p>Trying to copy data from <strong>PostgreSQL</strong> DB on an Ubuntu box that needs IPs whitelisted to access it. With <strong>Azure Data Factory</strong> IPs changing all the time and since i cannot install <strong>Self-hosted integration runtime</strong> as its a <strong>Linux</strong> server, what other options are available to be able to copy data from this PostgreSQL DB into an Azure SQL DB without having to worry about the IP addresses. Any suggestions or known solutions for this please?</p>
","<postgresql><azure-data-factory>","2019-11-06 22:05:01","243","1","1","58798226","<p>Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">document</a>,ADF Self-hosted integration runtime can't be installed on the linux server,only could be used on the windows server.</p>

<p>BTW,this feature will also not be supported recently,please follow this feedback <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/35520787-create-self-hosted-integration-runtime-capable-of"" rel=""nofollow noreferrer"">link</a>.</p>

<blockquote>
  <p>the latest comment:Currently we don't have any plan on this yet. Could
  you share us your reasons why do you want Linux?</p>
</blockquote>

<p>As workaround,i suggest you get an idea of Azure DMS(Database Migration Service). Please see more detail about it from this <a href=""https://learn.microsoft.com/en-us/azure/dms/tutorial-postgresql-azure-postgresql-online"" rel=""nofollow noreferrer"">link</a> and this <a href=""https://azure.microsoft.com/en-us/resources/videos/how-to-migrate-postgresql-to-azure-postgresql-online-dms-and-cli/"" rel=""nofollow noreferrer"">video</a>.</p>
"
"58735874","Data Factory - Data Lake File Created Event Trigger fires twice","<p>I'm developing Pipeline in Azure Data Factory V2. It has very simple Copy activity. The Pipeline has to start when a file is added to Azure Data Lake Store Gen 2. In order to do that I have created a Event Trigger attached to ADLS_gen2 on Blob created. Then assigned trigger to pipeline and associate trigger data @triggerBody().fileName to pipeline parameter.</p>
<p>To test this I'm using Azure Storage Explorer and upload file to data lake. The problem is that the trigger in Data Factory is fired twice, resulting pipeline to be started twice. First pipeline run finish as expected and second one stays in processing.</p>
<p>Has anyone faced this issue? I have tried to delete the trigger in DF and create new one but the result was the same with new trigger.</p>
<p><img src=""https://i.stack.imgur.com/FXPsS.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.stack.imgur.com/nhA66.png"" alt=""enter image description here"" /></p>
","<azure-data-factory>","2019-11-06 17:47:42","2203","5","2","58741030","<p>I'm having the same issue myself.</p>

<p>When writing a file to ADLS v2 there is an initial a CreateFile operation and a FlushWithClose operation and they are both triggering a Microsoft.Storage.BlobCreated event type.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/event-grid/event-schema-blob-storage"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/event-grid/event-schema-blob-storage</a></p>

<p>If you want to ensure that the Microsoft.Storage.BlobCreated event is triggered only when a Block Blob is completely committed, filter the event for the FlushWithClose REST API call. This API call triggers the Microsoft.Storage.BlobCreated event only after data is fully committed to a Block Blob.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/event-grid/how-to-filter-events"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/event-grid/how-to-filter-events</a></p>

<p>You can filter out the CreateFile operation by navigating to Event Subscriptions in the Azure portal and choosing the correct topic type (Storage Accounts) and subscription and location. Once you've done that you should be able to see the trigger and update the filter settings on it. I removed CreateFile.</p>
"
"58735874","Data Factory - Data Lake File Created Event Trigger fires twice","<p>I'm developing Pipeline in Azure Data Factory V2. It has very simple Copy activity. The Pipeline has to start when a file is added to Azure Data Lake Store Gen 2. In order to do that I have created a Event Trigger attached to ADLS_gen2 on Blob created. Then assigned trigger to pipeline and associate trigger data @triggerBody().fileName to pipeline parameter.</p>
<p>To test this I'm using Azure Storage Explorer and upload file to data lake. The problem is that the trigger in Data Factory is fired twice, resulting pipeline to be started twice. First pipeline run finish as expected and second one stays in processing.</p>
<p>Has anyone faced this issue? I have tried to delete the trigger in DF and create new one but the result was the same with new trigger.</p>
<p><img src=""https://i.stack.imgur.com/FXPsS.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.stack.imgur.com/nhA66.png"" alt=""enter image description here"" /></p>
","<azure-data-factory>","2019-11-06 17:47:42","2203","5","2","58830240","<p>On your Trigger definition, set 'Ignore empty blobs' to Yes.</p>

<p>The comment from @dtape is probably what's happening underneath, and toggling this ignore empty setting on is effectively filtering the Create portion out (but not the data written part).</p>

<p>This fixed the problem for me.</p>
"
"58732540","Is it possible to read a parameter at runtime from inside Azure Function Linked Service in Data Factory?","<p>I need to dynamically call an Azure Function from inside my ADF pipeline.</p>

<p>Currently i'm able to parameterize the functionName through the Azure Function Activity, but i'm not able to parameterize nor the functionKey nor the URL.</p>

<p>The URL is no problem since I can store all the functions below the same URL but the functionKey is really a must for this.</p>

<p>Do you now any option to do that?</p>

<p>What I've tried</p>

<p>Parameter inside the json as with DataStoreLinkedServices:</p>

<pre><code>{
    ""properties"": {
        ""type"": ""AzureFunction"",
        ""annotations"": [],
        ""parameters"": {
            ""functionSecret"": {
                ""type"": ""String""
            }
        },
        ""typeProperties"": {
            ""functionAppUrl"": ""https://&lt;myurl&gt;.azurewebsites.net"",
            ""functionKey"": {
                ""type"": ""AzureKeyVaultSecret"",
                ""store"": {
                    ""referenceName"": ""KeyVaultLinkedService"",
                    ""type"": ""LinkedServiceReference""
                },
                ""secretName"": ""@{linkedService().functionSecret}""
            }
        }
    }
}
</code></pre>

<p>ErrorMsg:</p>

<pre><code>""code"":""BadRequest"",""message"":""No value provided for Parameter 'functionSecret'""
</code></pre>

<p>Is there a way to achieve this? It seems not obvious, and I didn't found anything surfing the web. The most similar was <a href=""https://stackoverflow.com/questions/54076004/setting-a-linked-service-parameter-at-runtime-in-a-data-factory-pipeline"">this</a></p>
","<azure><azure-functions><azure-data-factory>","2019-11-06 14:37:19","496","1","1","58853900","<p>I'll answer myself just in case someone have the same problem, what we do to manage this was parameterize the needed information from the pipeline itself. </p>

<p>So we have a pipeline that just call a generic Azure Function. In the caller pipeline, there is a process to obtain the desired parameters from the KeyVault and pass them to the AF pipeline.</p>

<p>The LS remain as follows:</p>

<pre><code>{
    ""properties"": {
        ""annotations"": [],
        ""type"": ""AzureFunction"",
        ""typeProperties"": {
            ""functionAppUrl"": ""https://@{linkedService().functionAppUrl}.azurewebsites.net"",
            ""functionKey"": {
                ""type"": ""AzureKeyVaultSecret"",
                ""store"": {
                    ""referenceName"": ""KeyVaultLinkedService"",
                    ""type"": ""LinkedServiceReference""
                },
                ""secretName"": ""@{linkedService().functionKey}""
            }
        },
        ""parameters"": {
            ""functionAppUrl"": {
                ""type"": ""String"",
                ""defaultValue"": ""@pipeline().parameters.functionAppUrl""
            },
            ""functionKey"": {
                ""type"": ""String"",
                ""defaultValue"": ""@pipeline().parameters.functionKey""
            }
        }
    }
}
</code></pre>
"
"58732510","What is the appropriate way to build JSON within a Data Factory Pipeline","<p>In my earlier post, <a href=""https://stackoverflow.com/questions/58716310/sql-server-complains-about-invalid-json"">SQL Server complains about invalid json</a>, I was advised to use an 'appropriate methods' for building a json string, which is to be inserted into a SQL Server table for logging purposes.  In the earlier post, I was using string concatenation to build a json string.</p>

<p>What is the appropriate tools/functions to build json within a Data Factory pipeline?  I've looked into the json() and string() functions, but they would still rely on concatenation.</p>

<p>Clarification:  I'm trying to generate a logging message that looks like this:  Right now I'm using string concatenation to generate the logging json.  Is there a better, more elegant (but lightweight) way to generate the json data?</p>

<pre><code>{   ""EventType"": ""DataFactoryPipelineRunActivity"",    
    ""DataFactoryName"":""fa603ea7-f1bd-48c0-a690-73b92d12176c"",   
    ""DataFactoryPipelineName"":""Import Blob Storage Account Key CSV file into generic SQL table using Data Flow Activity Logging to Target SQL Server"",   
    ""DataFactoryPipelineActivityName"":""Copy Generic CSV Source to Generic SQL Sink"",   
    ""DataFactoryPipelineActivityOutput"":""{runStatus:{computeAcquisitionDuration:316446,dsl: source() ~&gt; ReadFromCSVInBlobStorage  ReadFromCSVInBlobStorage derive() ~&gt; EnrichWithDataFactoryMetadata  EnrichWithDataFactoryMetadata sink() ~&gt; WriteToTargetSqlTable,profile:{ReadFromCSVInBlobStorage:{computed:[],lineage:{},dropped:0,drifted:1,newer:1,total:1,updated:0},EnrichWithDataFactoryMetadata:{computed:[],lineage:{},dropped:0,drifted:1,newer:6,total:7,updated:0},WriteToTargetSqlTable:{computed:[],lineage:{__DataFactoryPipelineName:{mapped:false,from:[{source:EnrichWithDataFactoryMetadata,columns:[__DataFactoryPipelineName]}]},__DataFactoryPipelineRunId:{mapped:false,from:[{source:EnrichWithDataFactoryMetadata,columns:[__DataFactoryPipelineRunId]}]},id:{mapped:true,from:[{source:ReadFromCSVInBlobStorage,columns:[id]}]},__InsertDateTimeUTC:{mapped:false,from:[{source:EnrichWithDataFactoryMetadata,columns:[__InsertDateTimeUTC]}]},__DataFactoryName:{mapped:false,from:[{source:EnrichWithDataFactoryMetadata,columns:[__DataFactoryName]}]},__FileName:{mapped:false,from:[{source:EnrichWithDataFactoryMetadata,columns:[__FileName]}]},__StorageAccountName:{mapped:false,from:[{source:EnrichWithDataFactoryMetadata,columns:[__StorageAccountName]}]}},dropped:0,drifted:1,newer:0,total:7,updated:7}},metrics:{WriteToTargetSqlTable:{rowsWritten:4,sinkProcessingTime:1436,sources:{ReadFromCSVInBlobStorage:{rowsRead:4}},stages:[{stage:3,partitionTimes:[621],bytesWritten:0,bytesRead:24,streams:{WriteToTargetSqlTable:{type:sink,count:4,partitionCounts:[4],cached:false},EnrichWithDataFactoryMetadata:{type:derive,count:4,partitionCounts:[4],cached:false},ReadFromCSVInBlobStorage:{type:source,count:4,partitionCounts:[4],cached:false}},target:WriteToTargetSqlTable,time:811}]}}},effectiveIntegrationRuntime:DefaultIntegrationRuntime (East US)}"",   
    ""DataFactoryPipelineRunID"":""63759585-4acb-48af-8536-ae953efdbbb0"",   
    ""DataFactoryPipelineTriggerName"":""Manual"",   
    ""DataFactoryPipelineTriggerType"":""Manual"",   
    ""DataFactoryPipelineTriggerTime"":""2019-11-05T15:27:44.1568581Z"",   
    ""Parameters"":{    
        ""StorageAccountName"":""fa603ea7"",     
        ""FileName"":""0030_SourceData1.csv"",    
        ""TargetSQLServerName"":""5a128a64-659d-4481-9440-4f377e30358c.database.windows.net"",     
        ""TargetSQLDatabaseName"":""TargetDatabase"",     
        ""TargetSQLUsername"":""demoadmin""   
    },    
    ""InterimValues"":{    
        ""SchemaName"":""utils"",     
        ""TableName"":""vw_0030_SourceData1.csv-2019-11-05T15:27:57.643""   
    }  
}
</code></pre>
","<azure-data-factory>","2019-11-06 14:36:07","2062","0","1","58741371","<p>You can using Data Flow, it help you build the JSON string within pipeline in Data Factory.</p>

<p>Here's the Data Flow tutorial: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-json"" rel=""nofollow noreferrer"">Mapping data flow JSON handling</a>.</p>

<p>It can help you:</p>

<ol>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-json#creating-json-structures-in-derived-column"" rel=""nofollow noreferrer"">Creating JSON structures in Derived Column</a>
<a href=""https://i.stack.imgur.com/jrhiQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jrhiQ.png"" alt=""enter image description here""></a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-json#source-format-options"" rel=""nofollow noreferrer"">Source format options</a></li>
</ol>

<p><a href=""https://i.stack.imgur.com/RKl6i.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RKl6i.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"58726459","MS Access data into Azure Blob","<p>Data is in MS Access and it's in one of the shared drive on the network. I need this data in azure blob storage as CSV files. Can anyone please suggest me how can this be possible? </p>
","<python><ms-access><azure-data-factory><azure-blob-storage><azure-data-lake-gen2>","2019-11-06 09:05:09","172","0","1","58739510","<p>You can move data to Azure Blob storage in several ways, You could use either Azcopy: located here: <a href=""https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10</a> , Or Storage Explorer(GUI): <a href=""https://azure.microsoft.com/en-us/features/storage-explorer/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/features/storage-explorer/</a> </p>

<p>OR using Python SDK: </p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>block_blob_service.create_blob_from_path(container, file, file)</code></pre>
</div>
</div>
</p>

<p>Python SDK can be found here: <a href=""https://github.com/Azure/azure-sdk-for-python"" rel=""nofollow noreferrer"">https://github.com/Azure/azure-sdk-for-python</a> </p>

<p>When it comes to changing the format from Access to CSV, it's something not related to Azure Storage, you can try existing libraries for that conversion, then upload to blob storage.</p>
"
"58725843","ADF - Odata - How to perform an optional select","<p>I've created an ADF flow that loops over URL's to fetch OData using the OData connection.</p>

<p>However, not all fields are available in all URL's, there are certain ones that are available in one URL, but not in the other. A $Select is used to select the fields that we need.</p>

<p>Is it possible to have an optional selection (as in, if the path is not available, do not fetch this field and return null instead for instance)? Would help us a great deal.</p>

<p>I've tried adding ? after the field, but that does not work. <code>$select=Field1,Field2,FieldOptional?</code></p>

<p>Thanks</p>
","<odata><azure-data-factory>","2019-11-06 08:26:08","144","0","1","58847101","<p>As I understand you are trying to loop through a bunch of a URL and the query on the ODATA URL will change and so will be the fields . I think you can use a lookup where you pass the unique url and its gives the fields and then concatenate the url with the fields and make a odata call . </p>
"
"58725356","how to add dynamic content for ADF datafactory","<p>I want to add dynamic parameter for my ADF pipeline. I have 2 parameter and i want to concat with '/'. 
 <code>1. My source folder-&gt; pipeline().parameters.sourcefolder</code>
 <code>2. My source file-&gt; pipeline().parameters.sourcefile</code></p>

<p>I have tried this dynamic parameter. but i am getting error.<br>
<code>@concat{(pipeline().parameters.sourcefolder,'/',(pipeline().parameters.sourcefile)}</code></p>

<p>but is giving me error. Can you please help me.</p>
","<scala><azure><apache-spark><azure-cloud-services><azure-data-factory>","2019-11-06 07:49:49","388","0","1","58774955","<p>Since you already have the two parameter , I suggest you define the variable and use the ""Set variable"" activity and concatenate the parameters there .I have the Fname &amp; Lname as the defined parameter and the below expression should work fine .</p>

<pre><code>@concat(pipeline().parameters.Fname,'/',pipeline().parameters.Lname)
</code></pre>
"
"58718361","How do I check for duplicates in the same source using ADF","<p>I have a single source CSV with many duplicate rows. I want to highlight or remove these duplicates. Does anyone know how?</p>
","<azure-data-factory>","2019-11-05 19:41:58","1688","0","2","58719448","<p>This is possible with Data Flows!</p>

<p>It is quite well explained here by Kromer the great: <a href=""https://kromerbigdata.com/2019/04/21/use-adf-mapping-data-flows-for-fuzzy-matching-and-dedupe/"" rel=""nofollow noreferrer"">https://kromerbigdata.com/2019/04/21/use-adf-mapping-data-flows-for-fuzzy-matching-and-dedupe/</a></p>

<p>Hope this helped!</p>
"
"58718361","How do I check for duplicates in the same source using ADF","<p>I have a single source CSV with many duplicate rows. I want to highlight or remove these duplicates. Does anyone know how?</p>
","<azure-data-factory>","2019-11-05 19:41:58","1688","0","2","58722086","<p>You'll also find those patterns in the ADF pipeline template gallery. Just go to New Pipeline From Template in the ADF UI in your browser and look for these 2 data flow templates:</p>

<p><a href=""https://i.stack.imgur.com/P8jwM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P8jwM.png"" alt=""Distrinct rows and dedupe""></a></p>
"
"58716310","SQL Server complains about invalid json","<p>I am writing an ETL tool using Azure Data Factory and Azure SQL Database.  The Data Factory captures the output of a Mapping Data Flow and inserts it into the StatusMessage column of a SQL Server table (Audit.OperationsEventLog) as a string.  The StatusMessage column is varchar(8000) and is intended to store data formatted as valid json.</p>

<pre><code>SELECT *
FROM Audit.OperationsEventLog lg
CROSS APPLY OPENJSON(lg.StatusMessage) dt
</code></pre>

<p>When I query the json string from the table using the query above, it complains </p>

<blockquote>
  <p>JSON text is not properly formatted.  Unexpected character '""' is
  found at position 382</p>
</blockquote>

<p>That's a double-quote surrounded by two single-quotes.</p>

<p>I have used JSONLint (<a href=""https://jsonlint.com"" rel=""nofollow noreferrer"">http://jsonlint.com</a>) to validate that the json string is encoded properly.</p>

<p>When I copy the json string from the column StatusMessage into a varchar(8000) variable, I'm able to parse the string using OPENJSON.</p>

<pre><code>DECLARE @testjson varchar(8000) = '
{   ""EventType"": ""DataFactoryPipelineRunActivity"",    
    ""DataFactoryName"":""fa603ea7-f1bd-48c0-a690-73b92d12176c"",   
    ""DataFactoryPipelineName"":""Import Blob Storage Account Key CSV file into generic SQL table using Data Flow Activity Logging to Target SQL Server"",   
    ""DataFactoryPipelineActivityName"":""Copy Generic CSV Source to Generic SQL Sink"",   
    ""DataFactoryPipelineActivityOutput"":""{runStatus:{computeAcquisitionDuration:316446,dsl: source() ~&gt; ReadFromCSVInBlobStorage  ReadFromCSVInBlobStorage derive() ~&gt; EnrichWithDataFactoryMetadata  EnrichWithDataFactoryMetadata sink() ~&gt; WriteToTargetSqlTable,profile:{ReadFromCSVInBlobStorage:{computed:[],lineage:{},dropped:0,drifted:1,newer:1,total:1,updated:0},EnrichWithDataFactoryMetadata:{computed:[],lineage:{},dropped:0,drifted:1,newer:6,total:7,updated:0},WriteToTargetSqlTable:{computed:[],lineage:{__DataFactoryPipelineName:{mapped:false,from:[{source:EnrichWithDataFactoryMetadata,columns:[__DataFactoryPipelineName]}]},__DataFactoryPipelineRunId:{mapped:false,from:[{source:EnrichWithDataFactoryMetadata,columns:[__DataFactoryPipelineRunId]}]},id:{mapped:true,from:[{source:ReadFromCSVInBlobStorage,columns:[id]}]},__InsertDateTimeUTC:{mapped:false,from:[{source:EnrichWithDataFactoryMetadata,columns:[__InsertDateTimeUTC]}]},__DataFactoryName:{mapped:false,from:[{source:EnrichWithDataFactoryMetadata,columns:[__DataFactoryName]}]},__FileName:{mapped:false,from:[{source:EnrichWithDataFactoryMetadata,columns:[__FileName]}]},__StorageAccountName:{mapped:false,from:[{source:EnrichWithDataFactoryMetadata,columns:[__StorageAccountName]}]}},dropped:0,drifted:1,newer:0,total:7,updated:7}},metrics:{WriteToTargetSqlTable:{rowsWritten:4,sinkProcessingTime:1436,sources:{ReadFromCSVInBlobStorage:{rowsRead:4}},stages:[{stage:3,partitionTimes:[621],bytesWritten:0,bytesRead:24,streams:{WriteToTargetSqlTable:{type:sink,count:4,partitionCounts:[4],cached:false},EnrichWithDataFactoryMetadata:{type:derive,count:4,partitionCounts:[4],cached:false},ReadFromCSVInBlobStorage:{type:source,count:4,partitionCounts:[4],cached:false}},target:WriteToTargetSqlTable,time:811}]}}},effectiveIntegrationRuntime:DefaultIntegrationRuntime (East US)}"",   
    ""DataFactoryPipelineRunID"":""63759585-4acb-48af-8536-ae953efdbbb0"",   
    ""DataFactoryPipelineTriggerName"":""Manual"",   
    ""DataFactoryPipelineTriggerType"":""Manual"",   
    ""DataFactoryPipelineTriggerTime"":""2019-11-05T15:27:44.1568581Z"",   
    ""Parameters"":{    
        ""StorageAccountName"":""fa603ea7"",     
        ""FileName"":""0030_SourceData1.csv"",    
        ""TargetSQLServerName"":""5a128a64-659d-4481-9440-4f377e30358c.database.windows.net"",     
        ""TargetSQLDatabaseName"":""TargetDatabase"",     
        ""TargetSQLUsername"":""demoadmin""   
    },    
    ""InterimValues"":{    
        ""SchemaName"":""utils"",     
        ""TableName"":""vw_0030_SourceData1.csv-2019-11-05T15:27:57.643""   
    }  
}'

SELECT      *
FROM        OPENJSON(@testjson)

SELECT      *
FROM        OPENJSON(@testjson) data
            CROSS APPLY OPENJSON(data.value) moredata
WHERE       data.type = 5
</code></pre>

<p>The problem is isolated to ""DataFactoryPipelineActivityOutput"".</p>

<p>The data factory builds the json string to be inserted into the StatusMessage column of the table.  I strip out any occurrences of double-quotes within the StatusMessage string.</p>

<pre><code>{
    ""EventDateTime"":""@{utcNow()}"",
    ""EventState"":""Success"",
    ""SourceName"":""@{concat(pipeline().DataFactory, '/', pipeline().Pipeline, '/Copy Generic CSV Source to Generic SQL Sink')}"",
    ""SourceType"":""DataFactoryPipelineRunActivity"",
    ""StatusMessage"":""{
        \""EventType\"": \""DataFactoryPipelineRunActivity\"", 
        \""DataFactoryName\"":\""@{pipeline().DataFactory}\"",
        \""DataFactoryPipelineName\"":\""@{pipeline().Pipeline}\"",
        \""DataFactoryPipelineActivityName\"":\""Copy Generic CSV Source to Generic SQL Sink\"",
        \""DataFactoryPipelineActivityOutput\"":\""@{replace(string(activity('Copy Generic CSV Source to Generic SQL Sink').output), '""', '')}\"",      \""DataFactoryPipelineRunID\"":\""@{pipeline().RunID}\"",
        \""DataFactoryPipelineTriggerName\"":\""@{pipeline().TriggerName}\"",
        \""DataFactoryPipelineTriggerType\"":\""@{pipeline().TriggerType}\"",
        \""DataFactoryPipelineTriggerTime\"":\""@{pipeline().TriggerTime}\"",
        \""Parameters\"":{
            \""StorageAccountName\"":\""@{pipeline().parameters.StorageAccountName}\"", 
            \""FileName\"":\""@{pipeline().parameters.FileName}\"",
            \""TargetSQLServerName\"":\""@{pipeline().parameters.TargetSQLServerName}\"", 
            \""TargetSQLDatabaseName\"":\""@{pipeline().parameters.TargetSQLDatabaseName}\"", 
            \""TargetSQLUsername\"":\""@{pipeline().parameters.TargetSQLUsername}\""
        }, 
        \""InterimValues\"":{
            \""SchemaName\"":\""@{activity('Get Target View Schema and Name').output.firstRow.SchemaName}\"", 
            \""TableName\"":\""@{activity('Get Target View Schema and Name').output.firstRow.ViewName}\""
        }
    }""
}
</code></pre>

<p>Can anyone see if I'm doing something wrong, or is this a bug in OPENJSON?  I'm hoping that I did something stupid and all I need is a second set of eyes</p>
","<json><sql-server><azure-data-factory>","2019-11-05 17:13:17","2284","3","1","58716448","<p>If the sequence </p>

<pre><code> '""'
</code></pre>

<p>is part of a string, shouldn't that be </p>

<pre><code>'\""'
</code></pre>

<p>otherwise it's interpreted as the end of string, and that would indeed be invalid JSON. </p>

<p>So, that means </p>

<pre><code>\""DataFactoryPipelineActivityOutput\"":\""@{replace(string(activity('Copy Generic CSV Source to Generic SQL Sink').output), '\""', '')}\"",      \""DataFactoryPipelineRunID\"":\""@{pipeline().RunID}\"",
</code></pre>

<p>instead of </p>

<pre><code>\""DataFactoryPipelineActivityOutput\"":\""@{replace(string(activity('Copy Generic CSV Source to Generic SQL Sink').output), '""', '')}\"",      \""DataFactoryPipelineRunID\"":\""@{pipeline().RunID}\"",
</code></pre>
"
"58711108","load different files from a container in azure blob storage to different tables using azure data factory copy activity","<p>We have a requirement to copy multiple files from a blob container to azure db and on successful completion archive them into another container and delete the files in source container.I used two copy activities and one delete activity(with recursive option) for this and was able to load files with same structure to azure data base and then archive them into another container in blob. </p>

<p>But now,we have requirement to load multiple files of different structure in a container to azure database tables based on file name,can you please help in loading multiples files to azure db dynamically using azure data factory.</p>

<p>Thanks</p>
","<azure-data-factory><azure-blob-storage>","2019-11-05 12:12:09","865","0","1","58721551","<p>You will have to parameterized both the source and sink linked service . This may help you .
<a href=""https://social.technet.microsoft.com/wiki/contents/articles/53367.azure-data-factory-v2-copy-content-of-multiple-blob-to-different-sql-tables.aspx"" rel=""nofollow noreferrer"">https://social.technet.microsoft.com/wiki/contents/articles/53367.azure-data-factory-v2-copy-content-of-multiple-blob-to-different-sql-tables.aspx</a></p>
"
"58710092","Set variable from activity response Azure Data Factory","<p><a href=""https://i.stack.imgur.com/QDU3X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QDU3X.png"" alt=""Pipeline variables""></a></p>

<p><a href=""https://i.stack.imgur.com/Io23U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Io23U.png"" alt=""Pipeline""></a></p>

<p>I have REST call in a Copy data activity which gives me a json response
My goal is to fetch the ""hasNextPage"" value and put it into the hasNext variable</p>

<p>I want to set it as a value in a ""Set variable"" activity that is connected to the ""Copy data"" activity, where I expected to acess the output in a way like this: @activity('Timesheets').output.data.timesheets.pageinfo.hasNext</p>

<p>I also want to be able to fetch the value of ""cursor"" from the last element in the ""edges"" array[]</p>

<p>I couldn't find any documentation on how to do this</p>

<p>Json response that I get from the Timesheets activity</p>

<pre><code>[
{
""data"": {
  ""timesheets"": {
    ""pageInfo"": {
      ""hasNextPage"": true
    },
    ""edges"": [
      {
        ""cursor"": ""81836000243260.81836000243275."",
        ""node"": {
          ""parameter1"": ""2019-11-04"",
          ""parameter2"": ""81836000243260""
        }
      },
      {
        ""cursor"": ""81836000243252.81836000243260.81836000243275"",
        ""node"": {
          ""parameter1"": ""2019-11-04"",
          ""parameter2"": ""81836000243260""
        }
      }
     ]
   }
  }
 }
]
</code></pre>
","<json><azure-data-factory>","2019-11-05 11:09:02","1160","0","1","58857138","<p>According to this, the output of an copy data activity don't have a data property you can access.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview</a></p>

<p>Copy Activity are made for copying large data, and it doesn't copy all rows in one go.
So it would not make sense to have an output dataset for a Copy Activity.</p>

<p>If your response from your REST service contains limited element, you can use an Web Activity to consume the REST service.
This have an output dataset you can access.
Followed by a foreach activity to iterate the data set. Remember to take into consideration parallel vs sequential iteration of you data set in the foreach activity.</p>

<p>Note in your service response, you get an array of ""data"" objects, so you need to address the first ""data"" element.  </p>
"
"58694739","ADF - Using parameter from loop in a copy data flow","<p>I've created a loop that loops over URL's to fetch ODATA. Then, a copy flow is created for every path in the oDATA service. However, I need to be able to pass the URL into these tables as well.</p>

<p>How can I add the URL (@pipeline().parameters.ProjectUrl) to my sink when I am unable to import schemas because I'm working with parameters? Note that my query is a select, like so:</p>

<pre><code>$select=Field1,Field2,Field3
</code></pre>

<p>I'd like to add my parameter here, so it gets added to the tables.</p>

<p>THanks!</p>
","<azure-data-factory>","2019-11-04 13:46:36","227","0","1","58718215","<p>By copy flow do you mean 'a mapping data flow, which is used just to copy' ?</p>

<p>If that is the case, go into the flow, and add a parameter.  Keep the type as string (not all of the flow parameter types have pipeline equivalents).  Go back to the pipeline, and look at the execute data flow activity.  The parameter will now be visible in the activity.  When you click on the 'value' field, you can choose between 'Pipeline expression' and 'Data flow expression'.  Choose 'Pipeline expression' and place the <code>@pipeline().parameters.ProjectUrl</code> here.</p>
"
"58690138","Implementation of webhook in Azure Data Factory","<p>I am having difficulties in getting the callback URI from azure data factory when using webhook activity also I am currently calling webjob within the webhook. Inside the webjob I have .Net code but I am not able to get the callback URI.</p>

<p>And I really need to use the the webhook activity because I have a long running job</p>
","<.net><azure><azure-data-factory>","2019-11-04 09:09:17","4158","1","1","58708524","<p>Based on this very detailed explanation from this <a href=""https://mrpaulandrew.com/2019/06/18/azure-data-factory-web-hook-vs-web-activity/"" rel=""nofollow noreferrer"">blog</a> and interesting example in this <a href=""https://vanishedgradient.com/2019/04/25/webhooks-with-azure-data-factory/"" rel=""nofollow noreferrer"">link</a>, at the time of invocation of webhook activity in a pipeline, Data Factory will add an additional field to the JSON body of the request, “callBackUri”, which will be automatically created.(If not,you could set it in the body by youself)</p>

<p>From there you can choose to continue executing in the pipeline, or use Data Factory’s control flows to gracefully handle a failure, or timeout.</p>

<p><a href=""https://i.stack.imgur.com/He8gR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/He8gR.png"" alt=""enter image description here""></a></p>

<hr>

<p>Update Answer:</p>

<p>Based on your latest comment,i assume that you want to pass a callbackuri as a parameter into webjob and use it in the webjob inside.</p>

<p>I searched the WebJob REST APi, and got this <a href=""https://github.com/projectkudu/kudu/wiki/WebJobs-API#invoke-a-triggered-job"" rel=""nofollow noreferrer"">api</a>:<code>/api/triggeredwebjobs/{job name}/run?arguments={arguments}</code></p>

<blockquote>
  <p>To run with arguments use the arguments parameters that will be added
  to the script when invoked. It also gets passed to the WebJob as the
  WEBJOBS_COMMAND_ARGUMENTS environment variable</p>
</blockquote>

<p>It seems that WebJob only accepts command arguments, so i did a test with a simple Console App.</p>

<pre><code>using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace JayWebJobConsoleApp
{
    class Program
    {
        static void Main(string[] args)
        {
            Console.Write(args[0]);
        }
    }
}
</code></pre>

<p>Then i invoke the above rest api by <code>https://***.scm.azurewebsites.net/api/triggeredwebjobs/WebJob1/run?arguments=jayuri</code>,it could be printed in the log:</p>

<p><a href=""https://i.stack.imgur.com/zgYph.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zgYph.png"" alt=""enter image description here""></a></p>

<p>I also did a test that passing the jayuri in the body but it can't be touched.</p>

<p><a href=""https://i.stack.imgur.com/1EPJY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1EPJY.png"" alt=""enter image description here""></a></p>

<p>So i'm afraid that you have to pass the <code>callbackuri</code> just behind the webhook uri like uri/arguments=callbackuri as same as my test.</p>
"
"58688355","Azure Data Factory v2 - Web Activity - Post Request to HTTPS self-signed Certificate","<p>I have a web server which set up a self-signed certificate. </p>

<p>When I use Web Activity from ADF v2 to send a post request to the HTTPS URL, I got the error message: </p>

<p>""Error calling the endpoint 'https://...'. Response status code: ''. More details:Exception message: 'An error occurred while sending the request.'.No response from the endpoint. Possible causes: network connectivity, DNS failure, server certificate validation or timeout.</p>

<p>Is there anyway that I can cancel the Web Activity server certificate validation or any workaround that makes the Web Activity works with Self-signed certificate? </p>

<p>I have been stuck with this problem for few days so your help is greatly appreciated. Thanks very much in advance. </p>
","<azure><post><https><azure-data-factory><self-signed>","2019-11-04 06:40:54","3927","6","1","58690712","<p>From <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-http#linked-service-properties"" rel=""nofollow noreferrer"">here</a>, should be exactly what you are looking for.
<a href=""https://i.stack.imgur.com/t0LBD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t0LBD.png"" alt=""enter image description here""></a></p>

<p>Also, in times of free certificates from Let's Encrypt, can't you get a proper cert?</p>
"
"58685278","Azure DataFactory Salesforce (data read != data written)","<p>I'm trying to figure out why the data read is different from the data written in my azure datafactory. The extract is from salesforce accounts into a table with the exact same schema.</p>

<pre><code>""dataRead"": 1094945,
""dataWritten"": 817359,
</code></pre>

<p>Where would I go to figure out the issue?</p>
","<azure><azure-data-factory>","2019-11-03 22:37:45","52","0","1","58689028","<p>Is there any pre-copy script you were using in Copy activity.</p>
"
"58678259","How to use wildcards in filename in AzureDataFactoryV2 to copy only specific files from a container?","<p>So I have a pipeline in AzureDataFactoryV2, in that pipeline I have defined a copyActivity to copy files from blob storage to Azure DataLake Store. But I want to copy all the files except the files that have ""-2192-"" string in them.</p>

<p>So If I have these files:</p>

<pre><code>213-1000-aasd.csv
343-2000-aasd.csv
343-3000-aasd.csv
213-2192-aasd.csv
</code></pre>

<p>I want to copy all using copyactivity but not 213-2192-aasd.csv. I have tried using different regular expression in wildcard option but no success.</p>

<p>According to my knowledge regular expression should be:
    <em>[^-2192-]</em>.csv</p>

<p>But it gives errors on this.
Thanks.</p>
","<copy><wildcard><pipeline><azure-data-factory>","2019-11-03 07:35:28","319","0","1","58700938","<p>I don't know whether the data factory expression language supports Regex. Assuming it does not, the Wildcard is probably positive matching only, so Wildcard to exclude specific patterns seems unlikely.</p>

<p>What you could do is use 1) Get Metadata to get the list of objects in the blob folder, then 2) a Filter where the item().type is 'File' and index of '-2192-' in the file name is &lt; 0 [the indexes are 0-based], and finally 3) a ForEach over the Filter that contains the Copy activity.</p>
"
"58667077","Update cosmos db data from sql azure","<p>I am able to successfully copy data from sql Azure to Azure cosmos DB using data factory. After first copy I want to keep it updated from the same sql source but Here is the problem:</p>

<p>Lets assume I have a SQL table like this:</p>

<pre><code>Col1   Col2
val11  val21
val12  val22
val13   val21
val11   val23
</code></pre>

<ol>
<li><p>In data factory copy activity I chose SQL as source and cosmos DB as destination. I chose upsert option for copying.</p></li>
<li><p>In the mapping section of azure data factory I setup the col1 mapped to /primarykey of my cosmosdb collection and col2 mapped to col2.</p></li>
<li><p>Now I executed the data factory pipeline. It successfully copied it and items look like following:</p></li>
</ol>

<pre>
   ID            PartitionKey         Document
   `SomeGuid1`      val11              { Col2: val21 }
   `SomeGuid2`      val12              { Col2: val22 }
   `SomeGuid3`      val13              { Col2: val21 }
   `SomeGuid4`      val11              { Col2: val23 }
</pre>

<ol start=""4"">
<li>Until this point it is fine. But let's say SQL data has changed and first row has become (val11  val25(Changed)). Now I want to copy this sql table again to cosmos DB.</li>
<li>When I run the data factory pipeline again then it copies all the rows again and duplicate the data.</li>
</ol>

<pre>
   ID            PartitionKey         Document
   `SomeGuid1`      val11              { Col2: val21 }
   `SomeGuid2`      val12              { Col2: val22 }
   `SomeGuid3`      val13              { Col2: val21 }
   `SomeGuid4`      val11              { Col2: val23 }
   `SomeGuid5`      val11              { Col2: val25 }  >-- changed value
   `SomeGuid6`      val12              { Col2: val22 }
   `SomeGuid7`      val13              { Col2: val21 }
   `SomeGuid8`      val11              { Col2: val23 }
</pre>

<p>But I don't want this to be duplicated. I want this to just copy and replace the old data. So I want data after 2nd copy task it to be: </p>

<pre>
   ID            PartitionKey         Document
   `SomeGuid5`      val11              { Col2: val25 }  >-- changed value
   `SomeGuid6`      val12              { Col2: val22 }
   `SomeGuid7`      val13              { Col2: val21 }
   `SomeGuid8`      val11              { Col2: val23 }
</pre>

<p>I think it is duplicating all the rows because upsert checks the Id property but since source doesn't have Id it generates one, and in 2nd copy also it generates new Ids and adds the new documents.</p>

<p>The way we do it in SQL is we copy in a temp table and then swap the table name with original table after full copy. But cosmos db doesn't support renaming the container: <a href=""https://feedback.azure.com/forums/263030-azure-cosmos-db/suggestions/19466575-add-ability-to-rename-databases-and-collections"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/263030-azure-cosmos-db/suggestions/19466575-add-ability-to-rename-databases-and-collections</a> </p>

<p>Any help will be highly appreciated. We like cosmos db and we will love to use it.</p>
","<azure-sql-database><azure-cosmosdb><azure-data-factory>","2019-11-01 23:51:50","1304","0","1","58673626","<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db#azure-cosmos-db-sql-api-as-sink"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db#azure-cosmos-db-sql-api-as-sink</a></p>

<blockquote>
  <p>Note: Data Factory automatically generates an ID for a document if an ID isn't specified either in the original document or by column mapping. This means that you must ensure that, for upsert to work as expected, your document has an ID.</p>
</blockquote>

<p>If ""Col1"" is your unique key by which you identify duplicates, you should map it to the <strong>id</strong> field. You don't need to use Guids for the <strong>id</strong> field.</p>

<p><a href=""http://More%20information%20about%20column%20mapping"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping</a></p>
"
"58663800","Using parameterized data sets within Azure Data Factory Mapping Data Flows","<p>I'm having trouble using parameterized data sets when referenced through a Data Factory Mapping Data Flow.</p>

<p>I'm trying to write a generic ELT pipeline that will read from a CSV file in blob storage and write it to a table in Azure SQL Database.</p>

<p>When using a Copy Activity, everything works well (refer to pipeline ""Import CSV file into generic SQL table using Copy Activity"").  </p>

<p>A CSV file (file name passed into the Data Factory Pipeline as a parameter) is read from blob storage.
The structure of the CSV file is determined using a Get Metadata activity.</p>

<p>The structure of the CSV file is passed to a SQL Server stored procedure, which locates a view with the same structure as the CSV file.  If a view with the same structure doesn't exist, a table and view are created.  In either case the schema name and view name are passed back to the pipeline.  The contents of the CSV file are then read and written to the SQL Server view.</p>

<p>This works well, but I don't have the opportunity to add metadata to each written row.  So I created a mapping Data Flow that will add metadata (Data Factory name, Data Factory Pipeline Name, Data Factory Pipeline Run ID) using a derived column transform.</p>

<p>When I attempt to debug the dataflow, I get an error <code>""An error occurred, please view notification for more details.""</code>  I'm unable to locate any notifications that give information on the error.</p>

<p>The data flow works when I use data sets with the filename hardcoded.  When I switch to the parameterized data flow (passing in parameters like pipeline().DataFactory, etc), that's when I get the error.</p>

<p>I've tried hard-coding the mapping data flow parameter values, still no joy.</p>

<p>The template passed validation, but if I try to run the pipeline ""Import CSV file into generic SQL table using Data Flow"" by triggering the pipeline, it fails very quickly and gives me a cryptic error message:</p>

<pre><code>ErrorCode=InvalidTemplate, ErrorMessage=Unable to parse expression 'body('Copy Generic CSV Source to Generic SQL SinkComposeRuntimeVariables')?.Copy Generic CSV Source to Generic SQL Sinkd7ea532482e64afc88501b46924214b3?.ReadFromSourceCSVFileInBlobStorage.FileName'
</code></pre>

<p>Any Azure Data Factory experts out there willing to lend a hand?</p>

<p>All source code is available at <a href=""https://github.com/marc-jellinek/AzureDataFactoryDemo_GenericSqlSink"" rel=""noreferrer"">https://github.com/marc-jellinek/AzureDataFactoryDemo_GenericSqlSink</a></p>

<p>Thanks in advance!</p>
","<azure-data-factory>","2019-11-01 18:01:11","3602","5","2","58666263","<p>You can pass those values directly into the data flow activity from the pipeline using expressions in data flow parameters, you don't need to use dataset parameters.</p>

<p>To debug, switch on the Debug switch and then in the Data Flow designer, go to the Data Preview tab on your transformations. You can test with default values in the data flow parameters or set it at debug time using the debug settings.</p>
"
"58663800","Using parameterized data sets within Azure Data Factory Mapping Data Flows","<p>I'm having trouble using parameterized data sets when referenced through a Data Factory Mapping Data Flow.</p>

<p>I'm trying to write a generic ELT pipeline that will read from a CSV file in blob storage and write it to a table in Azure SQL Database.</p>

<p>When using a Copy Activity, everything works well (refer to pipeline ""Import CSV file into generic SQL table using Copy Activity"").  </p>

<p>A CSV file (file name passed into the Data Factory Pipeline as a parameter) is read from blob storage.
The structure of the CSV file is determined using a Get Metadata activity.</p>

<p>The structure of the CSV file is passed to a SQL Server stored procedure, which locates a view with the same structure as the CSV file.  If a view with the same structure doesn't exist, a table and view are created.  In either case the schema name and view name are passed back to the pipeline.  The contents of the CSV file are then read and written to the SQL Server view.</p>

<p>This works well, but I don't have the opportunity to add metadata to each written row.  So I created a mapping Data Flow that will add metadata (Data Factory name, Data Factory Pipeline Name, Data Factory Pipeline Run ID) using a derived column transform.</p>

<p>When I attempt to debug the dataflow, I get an error <code>""An error occurred, please view notification for more details.""</code>  I'm unable to locate any notifications that give information on the error.</p>

<p>The data flow works when I use data sets with the filename hardcoded.  When I switch to the parameterized data flow (passing in parameters like pipeline().DataFactory, etc), that's when I get the error.</p>

<p>I've tried hard-coding the mapping data flow parameter values, still no joy.</p>

<p>The template passed validation, but if I try to run the pipeline ""Import CSV file into generic SQL table using Data Flow"" by triggering the pipeline, it fails very quickly and gives me a cryptic error message:</p>

<pre><code>ErrorCode=InvalidTemplate, ErrorMessage=Unable to parse expression 'body('Copy Generic CSV Source to Generic SQL SinkComposeRuntimeVariables')?.Copy Generic CSV Source to Generic SQL Sinkd7ea532482e64afc88501b46924214b3?.ReadFromSourceCSVFileInBlobStorage.FileName'
</code></pre>

<p>Any Azure Data Factory experts out there willing to lend a hand?</p>

<p>All source code is available at <a href=""https://github.com/marc-jellinek/AzureDataFactoryDemo_GenericSqlSink"" rel=""noreferrer"">https://github.com/marc-jellinek/AzureDataFactoryDemo_GenericSqlSink</a></p>

<p>Thanks in advance!</p>
","<azure-data-factory>","2019-11-01 18:01:11","3602","5","2","58674016","<p>The problem is there are spaces in the name of the data flow.</p>
<blockquote>
<p>The issue is parameter expression is not supported for data flow which
has whitespaces in the name. Could you try to rename your data flow
“Copy Generic CSV Source to Generic SQL SinkComposeRuntimeVariables”
and then preview? Meanwhile we will add more validation on UX side to
handle the naming restriction of data flow properly.</p>
</blockquote>
<p>I have removed the spaces from the name of the dataflow and successfully tested.</p>
"
"58662376","How to archive data from Azure Data lake by validating the size of file in source and destination","<p>We have different zones in Azure Data lake and i am trying to move data from one zone to another and then deleting data from the first zone. But before the delete the data i need to compare the size of source and destination file. </p>

<p>I have created a pipeline in ADF which copies data from one zone to another and delete data from first zone. But i dont know how to compare the size before deleting.</p>

<p>CopyData->Delete</p>

<p>Compare the size of source and destination files and then perform delete task</p>
","<azure><azure-data-factory>","2019-11-01 16:07:08","95","0","1","58664302","<p>Use the Get Metadata activity against your source and target, then compare in an If conditional</p>
"
"58656410","Connection between Azure Data Factory and Databricks","<p>I'm wondering what is the most appropriate way of accessing databricks from Azure data factory. </p>

<p>Currently I've got databricks as a linked service to which I gain access via a generated token.</p>
","<azure><azure-data-factory><azure-databricks>","2019-11-01 08:50:46","172","0","1","58665200","<p>What do you want to do?</p>

<ul>
<li>Do you want to trigger a Databricks notebook from ADF?   </li>
<li>Do you want to supply Databricks with data? (blob storage or Azure Data Lake store) </li>
<li>Do you want to retrieve data from Databricks? (blob storage or Azure Data Lake store)</li>
</ul>
"
"58635459","How to configure Webhook activity for runbooks execution in Azure Data Factory v2?","<p>I am working on running runbooks (powershell and graphical) from ADF. One of the ways I found to accomplish this task is to use webhooks. I will have runbooks running in parallel and in series (if dependency exists on previous runbook).</p>

<p>Overall,</p>

<ul>
<li><p>If a flat file is dropped in Azure Blob storage then it triggers the pipeline that contains respective runbook(s). This part is working.</p></li>
<li><p>The webhook of runbook(s) are used in ADF webhook activity. This is where I am facing the problem. I am unsure about what should be in the body of webhook activity?</p></li>
</ul>

<p>After some research I was able to find something about Callback uri that needs to be added (or somehow generated) in the body of the webhook. How can I get this Callback uri? If I don't add proper callback uri then the activity runs till timeout. I believe the functioning should be webhook activity completes when the runbook it's running is executed successfully so we can move on to next webhook activity in a pipeline. I have tried web activity as well but it's the same issue.</p>

<p>The body I am using right now is just below json.</p>

<p>{""body"":{""myMessage"":""Sample""}}</p>

<p>I have referenced:</p>

<p><a href=""https://vanishedgradient.com/2019/04/25/webhooks-with-azure-data-factory/"" rel=""nofollow noreferrer"">https://vanishedgradient.com/2019/04/25/webhooks-with-azure-data-factory/</a></p>

<p><a href=""https://mrpaulandrew.com/2019/06/18/azure-data-factory-web-hook-vs-web-activity/"" rel=""nofollow noreferrer"">https://mrpaulandrew.com/2019/06/18/azure-data-factory-web-hook-vs-web-activity/</a></p>

<p><a href=""https://social.msdn.microsoft.com/Forums/en-US/2effcefb-e65b-4d5c-8b01-138c95126b79/in-azure-data-factory-v2-how-to-process-azure-analysis-service-cube?forum=AzureDataFactory"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/en-US/2effcefb-e65b-4d5c-8b01-138c95126b79/in-azure-data-factory-v2-how-to-process-azure-analysis-service-cube?forum=AzureDataFactory</a></p>
","<webhooks><azure-data-factory><azure-runbook>","2019-10-31 00:37:12","3458","1","3","58867344","<p>I'm not sure if this is best practice but I have something that is working in a Powershell Workflow Runbook. </p>

<p>If the runbook has a webhook defined then you use the webhookdata parameter. Your request body needs to be in JSON format and the $WebhookData param picks it up. For example supposed the Body in your webhook activity looks like this: </p>

<p>{""MyParam"":1, ""MyOtherParam"":""Hello""}</p>

<p>In your runbook you pick up the parameters this way:</p>

<pre><code>Param([object]$WebhookData)

if($WebhookData){
    $parameters=(ConvertFrom-Json -InputObject $WebhookData.RequestBody)
    if($parameters.MyParam) {$ParamOne = $parameters.MyParam} 
    if($parameters.MyOtherParam) {$ParamTwo = $parameters.MyOtherParam} 
}
</code></pre>

<p>The variables in your runbook $ParamOne and $ParamTwo are populated from the parsed JSON Body string. The data factory automatically appends the callBackUri to the Body string. You don't need to create it.</p>

<p>You have to use the $WebhookData name. It's a defined property. </p>

<p>I hope this helps. </p>
"
"58635459","How to configure Webhook activity for runbooks execution in Azure Data Factory v2?","<p>I am working on running runbooks (powershell and graphical) from ADF. One of the ways I found to accomplish this task is to use webhooks. I will have runbooks running in parallel and in series (if dependency exists on previous runbook).</p>

<p>Overall,</p>

<ul>
<li><p>If a flat file is dropped in Azure Blob storage then it triggers the pipeline that contains respective runbook(s). This part is working.</p></li>
<li><p>The webhook of runbook(s) are used in ADF webhook activity. This is where I am facing the problem. I am unsure about what should be in the body of webhook activity?</p></li>
</ul>

<p>After some research I was able to find something about Callback uri that needs to be added (or somehow generated) in the body of the webhook. How can I get this Callback uri? If I don't add proper callback uri then the activity runs till timeout. I believe the functioning should be webhook activity completes when the runbook it's running is executed successfully so we can move on to next webhook activity in a pipeline. I have tried web activity as well but it's the same issue.</p>

<p>The body I am using right now is just below json.</p>

<p>{""body"":{""myMessage"":""Sample""}}</p>

<p>I have referenced:</p>

<p><a href=""https://vanishedgradient.com/2019/04/25/webhooks-with-azure-data-factory/"" rel=""nofollow noreferrer"">https://vanishedgradient.com/2019/04/25/webhooks-with-azure-data-factory/</a></p>

<p><a href=""https://mrpaulandrew.com/2019/06/18/azure-data-factory-web-hook-vs-web-activity/"" rel=""nofollow noreferrer"">https://mrpaulandrew.com/2019/06/18/azure-data-factory-web-hook-vs-web-activity/</a></p>

<p><a href=""https://social.msdn.microsoft.com/Forums/en-US/2effcefb-e65b-4d5c-8b01-138c95126b79/in-azure-data-factory-v2-how-to-process-azure-analysis-service-cube?forum=AzureDataFactory"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/en-US/2effcefb-e65b-4d5c-8b01-138c95126b79/in-azure-data-factory-v2-how-to-process-azure-analysis-service-cube?forum=AzureDataFactory</a></p>
","<webhooks><azure-data-factory><azure-runbook>","2019-10-31 00:37:12","3458","1","3","60536680","<p>Thanks for the links, they are useful sources. I've managed to get this working for a pipeline that calls a runbook to resize azure analysis services.  Having the runbook return failure and success information was not well documented.</p>

<p>Here's some code to assist a little, which i've taken from several places but a lot from the open issue (<a href=""https://github.com/MicrosoftDocs/azure-docs/issues/43897"" rel=""nofollow noreferrer"">https://github.com/MicrosoftDocs/azure-docs/issues/43897</a>) on this Microsoft page: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-webhook-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-webhook-activity</a></p>

<p>The datafactory Webhook activity passes in some ""Headers"", SourceHost which is @pipeline().DataFactory and SourceProcess which is @pipeline().Pipeline. This was so we can do some checking to confirm that the runbook is being run by acceptable processes.</p>

<p>The Body of the call is then other variables we required:</p>

<pre><code>@json(concat('{""AnalysisServer"":""', pipeline().parameters.AASName, '"", ""MinimumSKU"":""', pipeline().parameters.SKU,'""}') )
</code></pre>

<p>Your runbook needs the WebhookData parameter</p>

<pre><code>param
(
    [Parameter (Mandatory=$false)]
    [object] $WebhookData
)
</code></pre>

<p>You can then grab all the bits you need, including checking if a callbackuri was provided:</p>

<pre><code>if ($WebhookData)
{

    # Split apart the WebhookData
    $WebhookName     =     $WebhookData.WebhookName
    $WebhookHeaders  =     $WebhookData.RequestHeader
    $WebhookBody     =     $WebhookData.RequestBody | Convertfrom-Json
    $WebhookADF      =     $WebhookHeaders.SourceHost
    $WebhookPipeline =     $WebhookHeaders.SourceProcess

Write-Output -InputObject ('Runbook started through webhook {0} called by {1} on {2}.' -f $WebhookName, $WebhookPipeline, $WebhookADF)

# if there's a callBackURI then we've been called by something that is waiting for a response
If ($WebhookBody.callBackUri)
{
    $WebhookCallbackURI =  $WebhookBody.callBackUri
}

...
}

</code></pre>

<p>The variable $WebHookHeaders: <code>@{Connection=Keep-Alive; Expect=100-continue; Host=sXXevents.azure-automation.net; SourceHost=**MYDATAFACTORYNAME**; SourceProcess=**MYPIPELINENAME**; x-ms-request-id=**UNIQUEIDENTIFIER**}</code></p>

<p>You can then grab information out of your json body: <code>$AzureAnalysisServerName = $WebHookBody.AnalysisServer</code></p>

<p>Passing an error/failure back to your runbook is relatively easy, note that I put my success/update message in to $Message and only have content in $ErrorMessage if there's been an error:</p>

<pre><code>$ErrorMessage = ""Failed to do stuff I wanted"" 

if ($ErrorMessage) 
{

    $Output = [ordered]@{ output= @{
        AzureAnalysisServerResize = ""Failed"" }
        error = @{
            ErrorCode = ""ResizeError""
            Message = $ErrorMessage
        }
        statusCode = ""500""
    }
} else {
    $Output = [ordered]@{ 
        output= @{ 
            ""AzureAnalysisServerResize"" = ""Success""
            ""message"" = $Outputmessage 
        }
        statusCode = ""200""
    }
}

$OutputJson = $Output | ConvertTo-Json -Depth 10

# if we have a callbackuri let the ADF Webhook activity know that the script is complete 
# Otherwise it waits until its timeout
If ($WebhookCallBackURI)
{
    $WebhookCallbackHeaders = @{
        ""Content-Type""=""application/json""
    }
    Invoke-WebRequest -UseBasicParsing -Uri $WebhookCallBackURI -Method Post -Body $OutputJson -Header $WebhookCallbackHeaders
}
</code></pre>

<p>I then end the if ($WebhookData) { call with an else to say the runbook shouldn't be running if not called from webhook:</p>

<pre><code>} else {
    Write-Error -Message 'Runbook was not started from Webhook' -ErrorAction stop
}
</code></pre>

<p>Passing back an error message was quiet easy, passing back a success message has been traumatic, but the above seems to work, and in my datafactory pipeline i can access the results.</p>

<pre><code>Output
{
    ""message"": ""Analysis Server MYSERVERNAME which is SKU XX is already at or above required SKU XX."",
    ""AzureAnalysisServerResize"": ""Success""
}
</code></pre>

<p>Note that with the Invoke-WebRequest, some examples online don't specify -UseBasicParsing but we had to as the runbook complained: Invoke-WebRequest : The response content cannot be parsed because the Internet Explorer engine is not available, or Internet Explorer's first-launch configuration is not complete. </p>
"
"58635459","How to configure Webhook activity for runbooks execution in Azure Data Factory v2?","<p>I am working on running runbooks (powershell and graphical) from ADF. One of the ways I found to accomplish this task is to use webhooks. I will have runbooks running in parallel and in series (if dependency exists on previous runbook).</p>

<p>Overall,</p>

<ul>
<li><p>If a flat file is dropped in Azure Blob storage then it triggers the pipeline that contains respective runbook(s). This part is working.</p></li>
<li><p>The webhook of runbook(s) are used in ADF webhook activity. This is where I am facing the problem. I am unsure about what should be in the body of webhook activity?</p></li>
</ul>

<p>After some research I was able to find something about Callback uri that needs to be added (or somehow generated) in the body of the webhook. How can I get this Callback uri? If I don't add proper callback uri then the activity runs till timeout. I believe the functioning should be webhook activity completes when the runbook it's running is executed successfully so we can move on to next webhook activity in a pipeline. I have tried web activity as well but it's the same issue.</p>

<p>The body I am using right now is just below json.</p>

<p>{""body"":{""myMessage"":""Sample""}}</p>

<p>I have referenced:</p>

<p><a href=""https://vanishedgradient.com/2019/04/25/webhooks-with-azure-data-factory/"" rel=""nofollow noreferrer"">https://vanishedgradient.com/2019/04/25/webhooks-with-azure-data-factory/</a></p>

<p><a href=""https://mrpaulandrew.com/2019/06/18/azure-data-factory-web-hook-vs-web-activity/"" rel=""nofollow noreferrer"">https://mrpaulandrew.com/2019/06/18/azure-data-factory-web-hook-vs-web-activity/</a></p>

<p><a href=""https://social.msdn.microsoft.com/Forums/en-US/2effcefb-e65b-4d5c-8b01-138c95126b79/in-azure-data-factory-v2-how-to-process-azure-analysis-service-cube?forum=AzureDataFactory"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/en-US/2effcefb-e65b-4d5c-8b01-138c95126b79/in-azure-data-factory-v2-how-to-process-azure-analysis-service-cube?forum=AzureDataFactory</a></p>
","<webhooks><azure-data-factory><azure-runbook>","2019-10-31 00:37:12","3458","1","3","60789026","<p>Apologies for delay. I had found the complete solution few months back. Thanks to Nick and Sara for adding the pieces. I used similar code as return code. We were using graphical runbooks with limited changes allowed so I just added return code (Powershell) at the end of the runbook to have little to no impact. I plugged in below code:</p>

<pre><code>if ($WebhookData)
{
    Write-Output $WebhookData
    $parameters = (ConvertFrom-Json -InputObject $WebhookData.RequestBody)
    if ($parameters.callBackUri)
    {
        $callbackuri = $parameters.callBackUri
    }
}


if ($callbackuri)
{
    Invoke-WebRequest -Uri $callbackuri -UseBasicParsing -Method POST
}

Write-Output $callbackuri
</code></pre>

<p>After this I added an input parameter using ""Input and Output"" button available in the runbook. I named the input parameter as ""WebhookData"" and type as ""Object"". The name of input parameter is case-sensitive and should match the parameter used in Powershell code.</p>

<p>This resolved my issue. The runbook started when called from ADF pipeline and moved to next pipeline only when the underlying runbook called by the webhook was completed.</p>
"
"58632744","ADF expression to convert array to comma separated string","<p>This appears to be pretty basic but I am unable to find a suitable pipeline expression function to achieve this.</p>

<p>I have set an array variable VAR1 with the following value, which is an output from a SQL Lookup activity in an ADF pipeline:</p>

<pre><code>[
    {
        ""Code1"": ""1312312""
    },
    {
        ""Code1"": ""3524355""
    }
]
</code></pre>

<p>Now, I need to convert this into a comma separated string so I can pass it to a SQL query in the next activity - something like:</p>

<p><code>""'1312312','3524355'""</code></p>

<p>I am unable to find an expression function to iterate over the array elements, nor convert an array to a string. The only pipeline expression functions I see are to convert string to array and not the other way around.</p>

<p>Am I missing something basic? How can this be achieved?</p>
","<azure><azure-data-factory>","2019-10-30 19:55:10","14958","3","4","58633621","<p>In case, you just have two elements in the array, then you can do something like:</p>

<pre><code>@concat(variables('variable_name')[0].Code1, ',', variables('variable_name')[1].Code1)
</code></pre>
"
"58632744","ADF expression to convert array to comma separated string","<p>This appears to be pretty basic but I am unable to find a suitable pipeline expression function to achieve this.</p>

<p>I have set an array variable VAR1 with the following value, which is an output from a SQL Lookup activity in an ADF pipeline:</p>

<pre><code>[
    {
        ""Code1"": ""1312312""
    },
    {
        ""Code1"": ""3524355""
    }
]
</code></pre>

<p>Now, I need to convert this into a comma separated string so I can pass it to a SQL query in the next activity - something like:</p>

<p><code>""'1312312','3524355'""</code></p>

<p>I am unable to find an expression function to iterate over the array elements, nor convert an array to a string. The only pipeline expression functions I see are to convert string to array and not the other way around.</p>

<p>Am I missing something basic? How can this be achieved?</p>
","<azure><azure-data-factory>","2019-10-30 19:55:10","14958","3","4","58644390","<p>I got it working using a ForEach loop activity to iterate over my array and use a Set Variable task with a concat expression function to create my comma separated string.</p>

<p>Wish they had an iterator function in the expression language itself, that would have made it much easier.</p>
"
"58632744","ADF expression to convert array to comma separated string","<p>This appears to be pretty basic but I am unable to find a suitable pipeline expression function to achieve this.</p>

<p>I have set an array variable VAR1 with the following value, which is an output from a SQL Lookup activity in an ADF pipeline:</p>

<pre><code>[
    {
        ""Code1"": ""1312312""
    },
    {
        ""Code1"": ""3524355""
    }
]
</code></pre>

<p>Now, I need to convert this into a comma separated string so I can pass it to a SQL query in the next activity - something like:</p>

<p><code>""'1312312','3524355'""</code></p>

<p>I am unable to find an expression function to iterate over the array elements, nor convert an array to a string. The only pipeline expression functions I see are to convert string to array and not the other way around.</p>

<p>Am I missing something basic? How can this be achieved?</p>
","<azure><azure-data-factory>","2019-10-30 19:55:10","14958","3","4","64646611","<p>Use 'join' function present in 'collection' functions in 'Add dynamic content'. For example:</p>
<pre><code>join(variables('ARRAY_VARIABLE'), ',')
</code></pre>
"
"58632744","ADF expression to convert array to comma separated string","<p>This appears to be pretty basic but I am unable to find a suitable pipeline expression function to achieve this.</p>

<p>I have set an array variable VAR1 with the following value, which is an output from a SQL Lookup activity in an ADF pipeline:</p>

<pre><code>[
    {
        ""Code1"": ""1312312""
    },
    {
        ""Code1"": ""3524355""
    }
]
</code></pre>

<p>Now, I need to convert this into a comma separated string so I can pass it to a SQL query in the next activity - something like:</p>

<p><code>""'1312312','3524355'""</code></p>

<p>I am unable to find an expression function to iterate over the array elements, nor convert an array to a string. The only pipeline expression functions I see are to convert string to array and not the other way around.</p>

<p>Am I missing something basic? How can this be achieved?</p>
","<azure><azure-data-factory>","2019-10-30 19:55:10","14958","3","4","68551954","<p>I had this same issue and was not totally satisfied just using the join function because it keeps the keys from the json object. Also, using an iterator approach can work but is needlessly expensive and slow if you have a long list. Here was my approach, using join and replace:</p>
<pre><code>replace(replace(join(variables('VAR1'), ','), '{&quot;Code1&quot;:', ''), '}', ''))
</code></pre>
<p>This will give you exactly the output you are looking for.</p>
"
"58631018","Calling a PowerShell script from Azure batch custom activity using PowerShell and application environment variable","<p>I've been slowly working out how to call a PowerShell script to transform IIS logs using LogParser 2.2.  I've settled on using Azure Data Factory Batch Service Custom Activity to run the PowerShell script.  I've been able to figure out how to address many of the file path issues that arise in running PowerShell from within Azure Custom Batch Activity, but I can't figure this one out.</p>

<p>Currently I'm just trying to print via Write-Host the environment variable AZ_BATCH_APP_PACKAGE_powershellscripts#1.0 I've been able to print other environment variables, but I believe the #1.0 at the end of this one is causing all my grief.  BTW the 1.0 is the version of the application loaded into the batch framework in Azure.</p>

<p>All of the following attempts have failed:</p>

<pre><code>powershell powershell Write-Host ""$AZ_BATCH_APP_PACKAGE_powershellscripts#1.0""
powershell Write-Host ""$AZ_BATCH_APP_PACKAGE_powershellscripts#1.0""
powershell Write-Host ""$env:AZ_BATCH_APP_PACKAGE_powershellscripts#1.0""
powershell powershell Write-Host ""$env:AZ_BATCH_APP_PACKAGE_powershellscripts\#1.0""
powershell powershell Write-Host ""$env:AZ_BATCH_APP_PACKAGE_powershellscripts/#1.0""
powershell powershell Write-Host ""$env:AZ_BATCH_APP_PACKAGE_powershellscripts""
powershell powershell Write-Host ""$env:AZ_BATCH_APP_PACKAGE_powershellscripts`#1.0""
powershell powershell Write-Host ""$env:AZ_BATCH_APP_PACKAGE_powershellscripts`#1`.0""
powershell powershell Write-Host ""$env:AZ_BATCH_APP_PACKAGE_powershellscripts\`#1.0""
powershell powershell Write-Host ""$AZ_BATCH_APP_PACKAGE_powershellscripts`#1.0""
</code></pre>

<p>These works, but are either cmd window or not the variable I want:</p>

<pre><code>powershell powershell Write-Host ""$env:AZ_BATCH_TASK_DIR""
powershell powershell Write-Host ""$env:AZ_BATCH_ACCOUNT_URL""
cmd /c echo %AZ_BATCH_APP_PACKAGE_powershellscripts#1.0%
</code></pre>

<p>So what is the secret syntax sugar to getting this to work in Azure?</p>
","<azure><powershell><azure-data-factory>","2019-10-30 17:43:18","859","0","2","58631395","<p>Sure, you can do this:</p>

<pre><code>powershell powershell Write-Host ""$((Get-Variable -Name 'AZ_BATCH_APP_PACKAGE_powershellscripts#1.0').Value)""
</code></pre>

<p>Or this:</p>

<pre><code>powershell powershell Write-Host (Get-Variable -Name ""AZ_BATCH_APP_PACKAGE_powershellscripts#1.0"").Value
</code></pre>
"
"58631018","Calling a PowerShell script from Azure batch custom activity using PowerShell and application environment variable","<p>I've been slowly working out how to call a PowerShell script to transform IIS logs using LogParser 2.2.  I've settled on using Azure Data Factory Batch Service Custom Activity to run the PowerShell script.  I've been able to figure out how to address many of the file path issues that arise in running PowerShell from within Azure Custom Batch Activity, but I can't figure this one out.</p>

<p>Currently I'm just trying to print via Write-Host the environment variable AZ_BATCH_APP_PACKAGE_powershellscripts#1.0 I've been able to print other environment variables, but I believe the #1.0 at the end of this one is causing all my grief.  BTW the 1.0 is the version of the application loaded into the batch framework in Azure.</p>

<p>All of the following attempts have failed:</p>

<pre><code>powershell powershell Write-Host ""$AZ_BATCH_APP_PACKAGE_powershellscripts#1.0""
powershell Write-Host ""$AZ_BATCH_APP_PACKAGE_powershellscripts#1.0""
powershell Write-Host ""$env:AZ_BATCH_APP_PACKAGE_powershellscripts#1.0""
powershell powershell Write-Host ""$env:AZ_BATCH_APP_PACKAGE_powershellscripts\#1.0""
powershell powershell Write-Host ""$env:AZ_BATCH_APP_PACKAGE_powershellscripts/#1.0""
powershell powershell Write-Host ""$env:AZ_BATCH_APP_PACKAGE_powershellscripts""
powershell powershell Write-Host ""$env:AZ_BATCH_APP_PACKAGE_powershellscripts`#1.0""
powershell powershell Write-Host ""$env:AZ_BATCH_APP_PACKAGE_powershellscripts`#1`.0""
powershell powershell Write-Host ""$env:AZ_BATCH_APP_PACKAGE_powershellscripts\`#1.0""
powershell powershell Write-Host ""$AZ_BATCH_APP_PACKAGE_powershellscripts`#1.0""
</code></pre>

<p>These works, but are either cmd window or not the variable I want:</p>

<pre><code>powershell powershell Write-Host ""$env:AZ_BATCH_TASK_DIR""
powershell powershell Write-Host ""$env:AZ_BATCH_ACCOUNT_URL""
cmd /c echo %AZ_BATCH_APP_PACKAGE_powershellscripts#1.0%
</code></pre>

<p>So what is the secret syntax sugar to getting this to work in Azure?</p>
","<azure><powershell><azure-data-factory>","2019-10-30 17:43:18","859","0","2","58660791","<p>I went through close to 50 tries before getting this to work like so:</p>

<pre><code>powershell powershell Write-Host (Get-ChildItem Env:AZ_BATCH_TASK_DIR).Value
powershell powershell Write-Host (Get-ChildItem Env:AZ_BATCH_APP_PACKAGE_powershellscripts#1.0).Value
</code></pre>

<p>Now this was just a stepping stone to running a PowerShell script stored in an attached application to the Azure Batch module.  I'm hopeful Microsoft will add a Databrick or better way to run a PowerShell script in Azure Data Factory, but until then this is the only method I found to run a powershell script:</p>

<pre><code>powershell powershell -command (""(Get-ChildItem Env:AZ_BATCH_APP_PACKAGE_powershellscripts#1.0).Value"" + '\Powershell\processWebLogsFromAzure.ps1')
</code></pre>

<p>This should work for anyone just trying to run from the Batch Task Dir:</p>

<pre><code>powershell powershell -command (""$env:AZ_BATCH_TASK_DIR"" + '\wd\processWebLogsFromAzure.ps1')
</code></pre>

<p>Hope it helps someone!</p>
"
"58627161","Delete Azure Data Factory pipeline after successful run","<p>Right now I have a WebAPI application that after receiving a request dynamically creates a specific pipeline in C# to do a specific task. </p>

<p>However, because the <a href=""https://github.com/MicrosoftDocs/azure-docs/blob/master/includes/azure-data-factory-limits.md"" rel=""nofollow noreferrer"">number of pipelines and datasets is limited to 5000</a>, the application requests will eventually cause to reach this limit. I'm thinking about a way to automatically delete the pipeline and its datasets, but I'm not sure how. Manual deletion is out of the question, unfortunately. </p>

<p>Is there maybe a way for executing a ""self-destruction"" of a pipeline after completion? Or maybe trigger of removing old pipelines periodically?</p>
","<c#><azure><pipeline><azure-data-factory>","2019-10-30 14:05:12","909","0","1","58637540","<p>No such mechanism to cleaning all the resources directly in ADF,however you could use <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-timer"" rel=""nofollow noreferrer"">Azure Function Time Trigger</a> to implement it in the schedule.Please refer to my thoughts:</p>

<p>1.Create time trigger azure function(for example triggered every day) to query pipeline runs with <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipelineruns/querybyfactory"" rel=""nofollow noreferrer"">REST API</a> or SDK.</p>

<p>2.Loop the results and filter the <code>Status==Succeeded</code> and <code>runEnd&lt; today</code> to get the pipeline name list</p>

<p><a href=""https://i.stack.imgur.com/r3HTy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/r3HTy.png"" alt=""enter image description here""></a></p>

<p>3.Delete them one by one by name list by using Delete API.(REST API:<a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/delete"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/delete</a>)</p>

<p>4.Deleting datasets is a little bit of trouble. Although you can get the pipeline name, the activities in the pipeline are not necessarily the same, resulting in different datasets.For example,if it is copy activity,you could get <code>referenceName</code> in <code>inputs</code> and <code>outputs</code> array.If it is feasible to clear all datasets and they will be re-created, you can easily use the <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/datasets/listbyfactory"" rel=""nofollow noreferrer"">LIST DATASETS API</a> and kill them all.</p>

<p><a href=""https://i.stack.imgur.com/5UH0U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5UH0U.png"" alt=""enter image description here""></a></p>
"
"58625927","Can md5 function be passed a dynamic list of columns via a Parameter","<p>In Azure Data Factory v2 i am calling a Mapping Data Flow from a pipeline.  The source and sink connections in the dataflow are parameterized, so i can reuse the pipeline/data flow for multiple source/target combinations. However i want to create a derived column in the Dataflow which is a md5 hash of the appropriate columns for that particular source/target combination. Therefor i want to pass a list of columns into the md5() function which can be evaluated as the dataflow runs and an appropriate hash value can be generated for each row of data.  At the moment I can't find a way of getting it to work.</p>

<p>I have a dataflow parameter (pColumnList) of type string (there is no type option for array). The pipeline passes a string comma separated list of columnnames eg Col1,Col2,Col3
and the derived column (ROW_HASH) has the code/expression <code>md5($pColumnList)</code> the problem at the moment is that the function is hashing the string value of 'Col1,Col2,Col3' and therefore every row has the same hash value rather that the hash being calculated per row.  I've also tried splitting string into an array to see if the function will interpret the separate items as the columns ie  <code>md5(split($pColumnList,','))</code> but this results in an error as i don't think the md5 function accepts an array.</p>

<p>The problem is that as the source is dynamic, there's no defined schema in the dataflow that the expression can explicitly refer to at design time. I'm just wondering whether this is just simply not possible to do in adf mapping data flows and in fact we will either need a separate data flow creating for each specific source to target process with explicit source and target connections creating with their appropriate schema visible. (or alternatively the hash created as calculated column in the target table).</p>
","<expression><md5><azure-data-factory>","2019-10-30 12:57:42","1971","2","2","58654046","<p>We don't allow array values in those functions today.</p>

<p>Are you looking to generate a hash/fingerprint for the entire row?</p>

<p>If that's your goal, you'll need to use one of these methods:</p>

<ol>
<li><p>Use toString(byPosition(1))+toString(byPositiong(2))+ ... in a derived column and concat each column into a new column, then hash that new column.</p></li>
<li><p>If the source is a file, use a source dataset without a delimiter, that just reads the entire row in as a single string. This way, you'll have the entire row as a single string (i.e. col_1) that you can hash.</p></li>
</ol>
"
"58625927","Can md5 function be passed a dynamic list of columns via a Parameter","<p>In Azure Data Factory v2 i am calling a Mapping Data Flow from a pipeline.  The source and sink connections in the dataflow are parameterized, so i can reuse the pipeline/data flow for multiple source/target combinations. However i want to create a derived column in the Dataflow which is a md5 hash of the appropriate columns for that particular source/target combination. Therefor i want to pass a list of columns into the md5() function which can be evaluated as the dataflow runs and an appropriate hash value can be generated for each row of data.  At the moment I can't find a way of getting it to work.</p>

<p>I have a dataflow parameter (pColumnList) of type string (there is no type option for array). The pipeline passes a string comma separated list of columnnames eg Col1,Col2,Col3
and the derived column (ROW_HASH) has the code/expression <code>md5($pColumnList)</code> the problem at the moment is that the function is hashing the string value of 'Col1,Col2,Col3' and therefore every row has the same hash value rather that the hash being calculated per row.  I've also tried splitting string into an array to see if the function will interpret the separate items as the columns ie  <code>md5(split($pColumnList,','))</code> but this results in an error as i don't think the md5 function accepts an array.</p>

<p>The problem is that as the source is dynamic, there's no defined schema in the dataflow that the expression can explicitly refer to at design time. I'm just wondering whether this is just simply not possible to do in adf mapping data flows and in fact we will either need a separate data flow creating for each specific source to target process with explicit source and target connections creating with their appropriate schema visible. (or alternatively the hash created as calculated column in the target table).</p>
","<expression><md5><azure-data-factory>","2019-10-30 12:57:42","1971","2","2","64937386","<p><a href=""https://www.youtube.com/watch?v=tc283k8CWh8"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=tc283k8CWh8</a></p>
<p>This demo shows how to get the values in a column array.</p>
<pre><code>md5(byName($PrimaryKey))
</code></pre>
<p>for single value or</p>
<pre><code>md5(byNames(split($Columns, ',')))
</code></pre>
<p>for an array generated by <code>split()</code></p>
"
"58616162","Is there a way in Azure Datafactory to pass @item() vaules of For Each activity to activies that are deeper in the context?","<p>Is there a way in Azure Datafactory to pass @item() vaule of for each activity through if activity to execute pipeline inside true of if activity?</p>

<p><a href=""https://i.stack.imgur.com/QJsum.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QJsum.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2019-10-29 22:18:25","467","0","2","58619598","<p>The way I understand you want to compare the @item() from some value and if its true you want to go with Execute Pipeline , otherwise nothing .</p>

<p>Yes this should be possible . 
I did added the expression in the If activity and did some quick test . </p>

<pre><code>@equals(string(item()),variables('value'))
</code></pre>

<p>Please do let me know if this works .</p>
"
"58616162","Is there a way in Azure Datafactory to pass @item() vaules of For Each activity to activies that are deeper in the context?","<p>Is there a way in Azure Datafactory to pass @item() vaule of for each activity through if activity to execute pipeline inside true of if activity?</p>

<p><a href=""https://i.stack.imgur.com/QJsum.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QJsum.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2019-10-29 22:18:25","467","0","2","58627630","<p>Passing value down to activities that are far removed from for each loop can only be accomplished with setting ForEach to <strong>Sequential</strong> and than inside for each setting a variable of a pipeline to value of @item(). After that you can set the dynamic property of what ever is under ForEach activity.</p>
"
"58615386","What variable expression to use to fetch name of dataset in ADF V2","<p>I have a dataset defined in ADF V2 of type Azure Storage. In the Connection we need to specify container and in my case the container is going to be the same as the name of the dataset itself. Is there any way I can access name of the dataset dynamically? Because I would then like to clone this dataset and create many datasets with different names and in all the cases container name is going to be the same as their dataset names. I have tried accessing dataset name using @dataset().name and @dataset() also but it doesn't work.</p>

<p><a href=""https://i.stack.imgur.com/ZX0Qv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZX0Qv.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2019-10-29 21:02:14","543","2","1","58635967","<p>Dhiraj, based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables"" rel=""nofollow noreferrer"">System variables in ADF</a> and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#introduction"" rel=""nofollow noreferrer"">Expressions and functions</a> , there is no such expression like <code>@dataset.name()</code> or <code>@dataset()</code>.</p>

<p>However,you could try to use parameters with dataset to implement your needs.When you create your datasets,you know the name definitely. Please add that name as a default value parameter of dataset like this:</p>

<p><a href=""https://i.stack.imgur.com/j9h59.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j9h59.png"" alt=""enter image description here""></a> </p>

<p>Then you could refer to the value by using <code>@dataset().selfName</code>.(You could view this link:<a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#a-dataset-with-a-parameter"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#a-dataset-with-a-parameter</a>)</p>

<p><a href=""https://i.stack.imgur.com/0p8O2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0p8O2.png"" alt=""enter image description here""></a></p>

<hr>

<p>My solution is also a hardcoding way because dataset name can't be referred in dataset inside so far as i found.</p>
"
"58611734","ADF Mapping Data Flow - Sink transform dynamic Number of partitions","<p>I have following expression to calculate the ""Number of partitions"" in Sink transform as dynamic content,</p>

<pre><code>toInteger (round( iif(toDecimal('5671478512', 38, 2) &lt;= 104857600, toDecimal(1.00) , toDecimal('5671478512', 38, 2)/104857600) ) )
</code></pre>

<p>This expression must result in integer 54, but for some reason it throws error when debugging in ADF portal.</p>

<p><a href=""https://i.stack.imgur.com/LOpAI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LOpAI.png"" alt=""enter image description here""></a></p>

<p>When I try the exact expression in a derived column transformation, I get expected value of 54. </p>

<p><a href=""https://i.stack.imgur.com/ZU1P8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZU1P8.png"" alt=""enter image description here""></a></p>

<p>Any ideas why it is failing in ""Number of partitions""? but works when tested in derived column</p>

<p>Following is the error I get when I add the expression in ""Number of partitions"" dynamic content</p>

<pre><code>collectPreviewData failure on job=e97f7e77-abae-41f2-95dd-7d2d0e03800b, jobState=Failed com.microsoft.dataflow.Issues: DF-SYS-01 - requirement failed: Number of partitions (0) must be positive. - Nonejava.lang.IllegalArgumentException: requirement failed: Number of partitions (0) must be positive.
    at scala.Predef$.require(Predef.scala:224)
    at org.apache.spark.sql.catalyst.plans.logical.RepartitionByExpression.&lt;init&gt;(basicLogicalOperators.scala:1123)
    at com.microsoft.dataflow.TransformPlanner$$anonfun$physicalPartitionPlan$1.apply(Transformer.scala:299)
    at com.microsoft.dataflow.TransformPlanner$$anonfun$physicalPartitionPlan$1.apply(Transformer.scala:283)
    at scala.collection.immutable.Stream.map(Stream.scala:418)
    at com.microsoft.dataflow.TransformPlanner$class.physicalPartitionPlan(Transformer.scala:283)
    at com.microsoft.dataflow.transformers.ExternalCodeGenerator.physicalPartitionPlan(External.scala:126)
    at com.microsoft.dataflow.FlowRunner$$anonfun$16.apply(FlowRunner.scala:237)
    at com.microsoft.dataflow.FlowRunner$$anonfun$16.apply(FlowRunner.scala:216)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
    at scala.collection.TraversableOnce$class.collectFirst(TraversableOnce.scala:145)
    at scala.collection.SeqViewLike$AbstractTransformed.collectFirst(SeqViewLike.scala:37)
    at com.microsoft.dataflow.FlowRunner$.com$microsoft$dataflow$FlowRunner$$runner(FlowRunner.scala:309)
    at com.microsoft.dataflow.FlowRunner$$anonfun$runner$2.apply(FlowRunner.scala:178)
    at com.microsoft.dataflow.FlowRunner$$anonfun$runner$2.apply(FlowRunner.scala:173)
    at scala.util.Success.flatMap(Try.scala:231)
    at com.microsoft.dataflow.FlowRunner$.runner(FlowRunner.scala:173)
    at com.microsoft.dataflow.DataflowExecutor$$anonfun$6$$anonfun$apply$3$$anonfun$apply$4$$anonfun$apply$5$$anonfun$apply$6$$anonfun$apply$9$$anonfun$apply$10$$anonfun$apply$11$$anonfun$7.apply(DataflowExecutor.scala:119)
    at com.microsoft.dataflow.DataflowExecutor$$anonfun$6$$anonfun$apply$3$$anonfun$apply$4$$anonfun$apply$5$$anonfun$apply$6$$anonfun$apply$9$$anonfun$apply$10$$anonfun$apply$11$$anonfun$7.apply(DataflowExecutor.scala:106)
    at com.microsoft.dataflow.DataflowJobFuture$$anonfun$flowCode$1.apply(DataflowJobFuture.scala:66)
    at com.microsoft.dataflow.DataflowJobFuture$$anonfun$flowCode$1.apply(DataflowJobFuture.scala:66)
    at scala.Option.map(Option.scala:146)
    at com.microsoft.dataflow.DataflowJobFuture.flowCode$lzycompute(DataflowJobFuture.scala:66)
    at com.microsoft.dataflow.DataflowJobFuture.flowCode(DataflowJobFuture.scala:66)
    at com.microsoft.dataflow.DataflowJobFuture$$anonfun$start$1.apply$mcV$sp(DataflowJobFuture.scala:290)
    at com.microsoft.dataflow.DataflowJobFuture$$anonfun$start$1.apply(DataflowJobFuture.scala:287)
    at com.microsoft.dataflow.DataflowJobFuture$$anonfun$start$1.apply(DataflowJobFuture.scala:287)
    at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
    at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
    at com.microsoft.dataflow.DataflowJobFuture$$anonfun$start$1.apply$mcV$sp(DataflowJobFuture.scala:315)
    at com.microsoft.dataflow.DataflowJobFuture$$anonfun$start$1.apply(DataflowJobFuture.scala:287)
    at com.microsoft.dataflow.DataflowJobFuture$$anonfun$start$1.apply(DataflowJobFuture.scala:287)
    at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
    at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
</code></pre>
","<azure-data-factory>","2019-10-29 16:32:43","649","1","1","58655452","<p>Sharing the answer as per the comment by @Mark Kromer.
Unfortunately, “We're not evaluating the dynamic content in the Number of Partitions property”.and this is confirmed as bug and product team is actively working on the fix.</p>
"
"58611183","Is there any way to convert the encoding of json files in Azure Blob Storage?","<p>I have copied the files from remote server to Azure Blob Storage using Azure Data Factory Copy Activity (Binary file copy). Those files are json files and txt files. I would like to change the encoding of the files to UTF-16.</p>

<p>I know its possible to change the encoding while copying the text files from remote server by just mentioning the encoding as UTF-16 in sink side in Copy Activity.I have implemented the copy activity which takes every files as txt file and it was working file. Sometimes, i get some error related to row delimiter and i changed the implementation to binary copy.Now, i would like to change the encoding of those files from UTF-8 to UTF-16. I couldn't find any way to do it.</p>

<p>Any help/suggestions would be appreciated.</p>
","<azure><azure-data-factory><azure-blob-storage>","2019-10-29 16:02:27","893","0","1","58637440","<p>If a file is stored in blob storage, you cannot directly change it's content-encoding, even if you set the blob property of content-encoding.</p>

<p>The way(via code or manually) to do this is that you should download it -> encode it with UTF-16 -> then upload it again.</p>
"
"58607300","Exception while calling Azure Function from Azure Data Factory (ADF)","<p>I have set up an Azure Powershell function with version 2.0. While executing the function from Azure Data Factory (ADF) getting below exception.</p>

<pre><code>""errorCode"": ""2011"",
    ""message"": ""Could not load file or assembly 'CodeGenerator, Version=1.1.0.0, Culture=neutral, PublicKeyToken=XXXXXXXXXXXXXXX' or one of its dependencies. The system cannot find the file specified."",
    ""failureType"": ""UserError"",
    ""target"": ""Azure Function1""
</code></pre>

<p>Function App Code</p>

<pre><code>using namespace System.Net

# Input bindings are passed in via param block.
param($Request, $TriggerMetadata)

# Write to the Azure Functions log stream.
Write-Host ""PowerShell HTTP trigger function processed a request.""

# Interact with query parameters or the body of the request.
$name = $Request.Query.Name
if (-not $name) {
    $name = $Request.Body.Name
}

if ($name) {
    $status = [HttpStatusCode]::OK
    $body = ""Hello $name""
}
else {
    $status = [HttpStatusCode]::BadRequest
    $body = ""Please pass a name on the query string or in the request body.""
}

# Associate values to output bindings by calling 'Push-OutputBinding'.
Push-OutputBinding -Name Response -Value ([HttpResponseContext]@{
    StatusCode = $status
    Body = $body
})
</code></pre>

<p>I am a beginner to Azure Functions. Please help.</p>
","<azure><azure-functions><azure-data-factory>","2019-10-29 12:15:11","601","0","1","58654685","<p>This issue has been solved , copy OP's comment , it will be helpful for others have similar issue : </p>

<blockquote>
  <p>This issue was with integration runtime. I changed the runtime from
  self hosted to auto resolved and its working fine.</p>
</blockquote>
"
"58602945","store json object to azure table storage entity using azure data factory pipeline","<p>I've created one azure data factory pipeline. By which, I need to store my JSON file content as string from my blob storage JSON file to azure table storage entity.</p>

<p>I tried with copy activity where the source is my blob storage JSON file and destination is my azure table storage's table where I need to store that.</p>

<p>Finally, I need the content of my JSON file to an entity of table storage as below.
<a href=""https://i.stack.imgur.com/OllNu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OllNu.png"" alt=""enter image description here""></a></p>
","<json><azure-table-storage><azure-data-factory><dataflow>","2019-10-29 07:26:16","843","0","1","58604227","<p>You can copy data from a blob storage JSON file to Azure table by having the source and the sink in the copy activity.</p>

<p>In the mapping tab of the copy activity, you can define the mapping between the attributes of the JSON file and the table as shown below :
<a href=""https://i.stack.imgur.com/M6OMq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M6OMq.png"" alt=""enter image description here""></a></p>
"
"58598106","Calling a PowerShell script from Azure batch custom activity","<p>I am trying to run a PowerShell script from within an Azure, Data Factory, Batch Service, Custom Activity.  The closest I've gotten to this working is the following:</p>

<pre><code>powershell powershell -command  '$env:AZ_BATCH_TASK_DIR\wd\processInAzure.ps1'
</code></pre>

<p>When I run this I get the following error message</p>

<pre><code>At line:1 char:23
+ $env:AZ_BATCH_TASK_DIR\wd\processInAzure.ps1
+                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Unexpected token '\wd\processInAzure.ps1' in expression or statement.
    + CategoryInfo          : ParserError: (:) [], ParentContainsErrorRecordException
    + FullyQualifiedErrorId : UnexpectedToken
</code></pre>

<p>I've been able to get a directory listing of $env:AZ_BATCH_TASK_DIR\wd and see that processInAzure.ps1 exists at this location.  I've been able to Write-Host ""Hello from Azure"" so I can see powershell is working.  What I'm not getting is how to reference the ps1 script file using an environment variable.  Would anyone know the syntax sugar to get this working?</p>

<p>With Architect Jamie's input (including his deleted edit!!! Put edit back it directly led to the solution) I was able to put a few things together to get this to work.  The double powershell at the beginning of the command line is not a typo BTW.  This is what ended up working:</p>

<pre><code>powershell powershell -command (""$env:AZ_BATCH_TASK_DIR"" + '\wd\processInAzure.ps1')
</code></pre>

<p>What didn't work is the following:</p>

<pre><code>powershell powershell -command (""$env:AZ_BATCH_TASK_DIR"" + ""\wd\processInAzure.ps1"")
powershell powershell -command (""$env:AZ_BATCH_TASK_DIR\wd\processInAzure.ps1"")
powershell -command (""$env:AZ_BATCH_TASK_DIR"" + '\wd\processInAzure.ps1')
</code></pre>
","<azure><powershell><azure-data-factory>","2019-10-28 20:41:05","177","0","1","58598161","<p>Try this:</p>

<pre><code>powershell -command ""$($env:AZ_BATCH_TASK_DIR)\ws\processInAzure.ps1""
</code></pre>

<p>It's not possible to access object member properties or methods inside single quote qualified strings.  Using the double quote in PowerShell allows you to expand variables at runtime.</p>

<p>Edit:</p>

<p>Though the above is true, in this instance the reason for the error is that PowerShell is treating the path being tacked on as part of the environment variable identifier.  Using $() variable expansion as above will work, and you should also be able to use <code>('$env:AZ_BATCH_TASK_DIR' + '\ws\processInAzure.ps1')</code></p>
"
"58581060","Having issues exporting from data lake to Azure SLQ Server DB","<p>I'm trying to use Sqoop to export data from my data lake to a table that I setup in Azure SQL Server.  This is all in Azure Data Factory.  The schema of the table matches the schema of the file name, with one exception...I have a column named 'file_name' in the table, which I want to populate from the file itself (this field is not in the raw data).  Here is the code that I tried.</p>

<pre><code>sqoop-export 
--connect ""jdbc:sqlserver://server_name.database.windows.net:1433;username=user_name;password='pass word';database=db_name"" 
--table REUTERS_CW 
--export-dir adl://rawdata/corp/file_name.gz 
--input-fields-terminated-by ""|""
</code></pre>

<p>When I run that, I get this error:</p>

<pre><code>&lt;console&gt;:10: error: identifier expected but string literal found.
--input-fields-terminated-by ""|""
</code></pre>

<p>There are three things that I'm not sure about.</p>

<pre><code>#1) My password actually has spaces in it; I think wrapping that in quotes will fix it.

#2) I'm running this in Scala in Azure Databricks; I'm guessing that's oen way to do it, but I don't know for sure.

#3) I'm not certain about how to copy the data from the file and simultaneously append the file name relative to the data that's supposed to be copied over.
</code></pre>

<p>I'm following the example from the link below.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-data-transfer-sql-sqoop"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-data-transfer-sql-sqoop</a></p>
","<scala><azure><sqoop><azure-data-factory><azure-data-lake>","2019-10-27 16:04:38","59","1","1","58600911","<p>What I get from <code>sqoop-export</code> document is that it seems not support compressed files.</p>

<p>You can reference <a href=""https://sqoop.apache.org/docs/1.4.4/SqoopUserGuide.html#_syntax_3"" rel=""nofollow noreferrer"">sqoop-export Syntax</a>.</p>

<p><strong>sqoop-export</strong>:</p>

<p><a href=""https://i.stack.imgur.com/nIdJH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nIdJH.png"" alt=""enter image description here""></a>
The <code>--export-dir</code> argument and one of <code>--table</code> or <code>--call</code> are required. These specify the table to populate in the database (or the stored procedure to call), and the directory in HDFS that contains the source data.</p>

<p><strong>sqoop-import</strong>:</p>

<p><a href=""https://i.stack.imgur.com/nIIY7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nIIY7.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"58564992","Calling Azure Function from ADF with Route Path Implementation Fails","<p>I am trying to call an Azure Function from ADF. Azure Function has Route implemented as shown below.</p>

<pre><code>[FunctionName(""UpdateStatus_V1"")]
public static async Task&lt;IActionResult&gt; Run([HttpTrigger(AuthorizationLevel.Function, ""post"", Route = ""V1/UpdateStatus"")] HttpRequest req, ILogger log)
</code></pre>

<p>Also the host file has a routePrefix in Azure Function</p>

<pre><code>""http"": {
  ""routePrefix"": ""DeltaAPI""
}
</code></pre>

<p>URI on the Function APP appears as below.</p>

<p><a href=""https://functionAPP.azurewebsites.net/DeltaAPI/V1/UpdateStatus?code=secret"" rel=""nofollow noreferrer"">https://functionAPP.azurewebsites.net/DeltaAPI/V1/UpdateStatus?code=secret</a></p>

<p>When i try to invoke this function from Azure Data Factory using a Function Activity - I keep getting this error</p>

<pre><code>{ 
   ""errorCode"":""3608"",
   ""message"":""Call to provided Azure function '' failed with status-'NotFound' and message - 'Invoking Azure function failed with HttpStatusCode - NotFound.'."",
   ""failureType"":""UserError"",
   ""target"":""Azure Function1""
}
</code></pre>

<p>I have followed the recommendation as per document <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity#routing-and-queries"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity#routing-and-queries</a> and the adf pipeline code is given below:</p>

<pre><code>{
  ""name"": ""FunctionTest"",
  ""properties"": {
    ""activities"": [
      {
        ""name"": ""Azure Function1"",
        ""type"": ""AzureFunctionActivity"",
        ""dependsOn"": [],
        ""policy"": {
          ""timeout"": ""7.00:00:00"",
          ""retry"": 0,
          ""retryIntervalInSeconds"": 30,
          ""secureOutput"": false,
          ""secureInput"": false
        },
        ""userProperties"": [],
        ""typeProperties"": {
          ""functionName"": ""V1/UpdateStatus"",
          ""method"": ""POST"",
          ""headers"": {
            ""Content-Type"": ""application/json""
          },
          ""body"": {
            ""value"": ""@json( '{\n \""transactionid\"": \""5d283e11e62943ceb2e6e0bb5fe4898f\"",\n \""status\"": \""Completed\"",\n \""Description\"": \""\""\n }')"",
            ""type"": ""Expression""
          }
        },
        ""linkedServiceName"": {
          ""referenceName"": ""AzureFunction"",
          ""type"": ""LinkedServiceReference""
        }
      }
    ],
    ""annotations"": []
  }
}
</code></pre>
","<azure-functions><azure-data-factory>","2019-10-25 19:59:11","1135","2","1","58604279","<p>The Azure Function Activity in the ADF pipeline expects the Azure Function to return a JSON object instead of an HttpResponseMessage. Here is how you can solve it: <a href=""https://microsoft-bitools.blogspot.com/2019/01/introducing-azure-function-activity-to.html"" rel=""nofollow noreferrer"">https://microsoft-bitools.blogspot.com/2019/01/introducing-azure-function-activity-to.html</a></p>

<p>Ref - <a href=""https://stackoverflow.com/questions/54441426/error-calling-the-azure-function-endpoint-from-azure-data-factory"">Error calling the azure function endpoint from azure data factory</a></p>
"
"58562291","Mapping a custom variable in Azure Data Factory","<p>How do I use a custom variable in a Json schema mapping? I am currently getting an error:</p>

<pre><code>JSON path @variables('myVariable01') is invalid.
</code></pre>
","<azure-data-factory>","2019-10-25 16:20:56","291","0","1","58854093","<p>You have to use something like</p>

<pre><code>@activity('GetConfigurations').output.value[0].clientId
</code></pre>

<p>Where  clientId is in your json</p>

<pre><code>{
  ""clientId"": ""abc""
}
</code></pre>

<p>And GetConfigurations is a lookup activity to read your settings file.</p>
"
"58561402","Enable Publishing in 'Data Factory' mode","<p>I have enabled Git on my Azure Data Factory. I have created my Git Repository also.
When I want to create a new Pipeline I have this message : </p>

<p>You have GIT enabled in your data factory. Publishing in 'Data Factory' mode is disabled. Please switch back to GIT mode to make further changes.
When I want to chose GitHub there is a popup ""You do not have access to the repository""</p>

<p>How can I give access to repository?</p>

<p><a href=""https://i.stack.imgur.com/ppNYf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ppNYf.png"" alt=""enter image description here""></a></p>

<p>Right now I have given below access to my user:</p>

<p><a href=""https://i.stack.imgur.com/UANn9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UANn9.png"" alt=""c""></a></p>
","<azure><github><repository><azure-data-factory>","2019-10-25 15:22:49","7939","3","1","59839775","<p>I had the same issue. What I did was to remove the GIT connection and then add it again. Then everything worked! </p>

<p>The git ""setup"" when creating the data factory seems not to work that well. Better to create the GIT connection later when the data factory is created.</p>

<p>To delete the git connection:</p>

<ol>
<li>Go to ""Data factory overview"" (house icon)</li>
<li>Press the ""git repo setting"" in the top right -> delete :)</li>
</ol>

<p><a href=""https://i.stack.imgur.com/IQb6j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IQb6j.png"" alt=""enter image description here""></a></p>
"
"58559980","How to delete the ADFPipeline which is having the references Forcefully","<p>I'm actually some automation for my ADF. As a part of that, I'm trying to delete all the ADF V2 pipelines. The problem is my pipelines having many references with different pipelines itself. </p>

<pre><code>$ADFPipeline = Get-AzDataFactoryV2Pipeline -DataFactoryName $(datafactory-name) -ResourceGroupName $(rg)



$ADFPipeline | ForEach-Object { Remove-AzDataFactoryV2Pipeline -ResourceGroupName $(rg) -DataFactoryName $(datafactory-name) -Name  $_.name -Force }
</code></pre>

<p>And most of the time I get the error like </p>

<blockquote>
  <p>The document cannot be deleted since it is referenced by ""blabla""</p>
</blockquote>

<p>I understand the error that it saying some references and cannot be deleted. However, when I tried the same deletion in the azure portal, irrespective of the reference I can able to delete. So I want to find a way that whether it possible to tell that Powershell even though it's having a reference delete it forcefully </p>

<p>Any other inputs much appreciated! </p>
","<azure><powershell><pipeline><azure-data-factory>","2019-10-25 13:56:59","1576","0","2","58635336","<p>Hello and thank you for the question.  According to <a href=""https://learn.microsoft.com/en-us/powershell/module/az.datafactory/remove-azdatafactoryv2pipeline?view=azps-28..0"" rel=""nofollow noreferrer"">the Remove-AzDataFactoryV2Pipeline doc</a>, the <code>-Force</code> flag simply skips the confirmation prompt.  It does not actually 'Force' the deletion in spite of errors.</p>

<p>Since you are already doing automation, might I suggest leveraging the error message to recursively attempt to delete the referencing pipeline.  <code>$error[0]</code> gets the most recent error.</p>

<p>(Pseudocode)</p>

<pre><code>try_recurse_delete( pipeline_name )
    do_delete(pipeline_name)
    if not $error[0].contains(""referenced by "" + pipeline_name)
        then return true
    else
        try_recurse_delete( get_refrencer_name($error[0]) )
</code></pre>

<p>Given that pipeline dependencies can be a many-to-many relationship, subsequent pipelines in your for-each loop might already be deleted by the recursion.  You will have to adapt your code to react to 'pipeline not found' type errors.</p>
"
"58559980","How to delete the ADFPipeline which is having the references Forcefully","<p>I'm actually some automation for my ADF. As a part of that, I'm trying to delete all the ADF V2 pipelines. The problem is my pipelines having many references with different pipelines itself. </p>

<pre><code>$ADFPipeline = Get-AzDataFactoryV2Pipeline -DataFactoryName $(datafactory-name) -ResourceGroupName $(rg)



$ADFPipeline | ForEach-Object { Remove-AzDataFactoryV2Pipeline -ResourceGroupName $(rg) -DataFactoryName $(datafactory-name) -Name  $_.name -Force }
</code></pre>

<p>And most of the time I get the error like </p>

<blockquote>
  <p>The document cannot be deleted since it is referenced by ""blabla""</p>
</blockquote>

<p>I understand the error that it saying some references and cannot be deleted. However, when I tried the same deletion in the azure portal, irrespective of the reference I can able to delete. So I want to find a way that whether it possible to tell that Powershell even though it's having a reference delete it forcefully </p>

<p>Any other inputs much appreciated! </p>
","<azure><powershell><pipeline><azure-data-factory>","2019-10-25 13:56:59","1576","0","2","64271412","<p>I run into the same issue, found out that it's rather complicated to build the whole dependency graph out of the pipeline's Activities property.</p>
<p>As a working solution (powershell):</p>
<pre><code>function Remove-Pipelines {
    param (
        [Parameter(Mandatory=$true)]
        [AllowEmptyCollection()]
        [AllowNull()]
        [System.Collections.ArrayList]$pipelines
    )
    if($pipelines.Count -gt 0) {
        [System.Collections.ArrayList]$plsToProcess = New-Object System.Collections.ArrayList($null)
        foreach ($pipeline in $pipelines) { 
            try {
                $removeAzDFCommand = &quot;Remove-AzDataFactoryV2Pipeline -dataFactoryName '$DataFactoryName' -resourceGroupName '$ResourceGroupName' -Name '$($pipeline.Name)' -Force -ErrorAction Stop&quot;
                Write-Host $removeAzDFCommand
                Invoke-Expression $removeAzDFCommand
            }
            catch {
                if ($_ -match '.*The document cannot be deleted since it is referenced by.*') {
                    Write-Host $_
                    $plsToProcess.Add($pipeline)
                } else {
                    throw $_
                }
            }
        }
        Remove-Pipelines $plsToProcess
    }
}
</code></pre>
<p>Here is the complete solution for clearing the whole DF: &quot;trigger&quot;,&quot;pipeline&quot;,&quot;dataflow&quot;,&quot;dataset&quot;,&quot;linkedService&quot;</p>
<pre><code>Param(
  [Parameter(Mandatory=$true)][string] $ResourceGroupName,
  [Parameter(Mandatory=$true)][string] $DataFactoryName
)

$artfTypes = &quot;trigger&quot;,&quot;pipeline&quot;,&quot;dataflow&quot;,&quot;dataset&quot;,&quot;linkedService&quot;

function Remove-Artifacts {
    param (
        [Parameter(Mandatory=$true)][AllowEmptyCollection()][AllowNull()][System.Collections.ArrayList]$artifacts,
        [Parameter(Mandatory=$true)][string]$artfType
    )
    if($artifacts.Count -gt 0) {
        [System.Collections.ArrayList]$artToProcess = New-Object System.Collections.ArrayList($null)
        foreach ($artifact in $artifacts) { 
            try {
                $removeAzDFCommand = &quot;Remove-AzDataFactoryV2$($artfType) -dataFactoryName '$DataFactoryName' -resourceGroupName '$ResourceGroupName' -Name '$($artifact.Name)' -Force -ErrorAction Stop&quot;
                Write-Host $removeAzDFCommand
                Invoke-Expression $removeAzDFCommand
            }
            catch {
                if ($_ -match '.*The document cannot be deleted since it is referenced by.*') {
                    Write-Host $_
                    $artToProcess.Add($artifact)
                } else {
                    throw $_
                }
            }
        }
        Remove-Artifacts $artToProcess $artfType
    }
}

foreach ($artfType in $artfTypes) {
  $getAzDFCommand = &quot;Get-AzDataFactoryV2$($artfType) -dataFactoryName '$DataFactoryName' -resourceGroupName '$ResourceGroupName'&quot;
  Write-Output $getAzDFCommand

  $artifacts = Invoke-Expression $getAzDFCommand
  Write-Output $artifacts.Name

  Remove-Artifacts $artifacts $artfType
}
</code></pre>
<p>The same approach can be adapted for <code>&quot;Set-AzDataFactoryV2Pipeline&quot;</code> command as well.</p>
<p>It worth to mention that along with dependencies tracking, Remove/Set artifact's sequence should be right (because of cross artifacts' dependencies).</p>
<p>For Set - <code>&quot;linkedService&quot;,&quot;dataset&quot;,&quot;dataflow&quot;,&quot;pipeline&quot;,&quot;trigger&quot;</code></p>
<p>For Remove - <code>&quot;trigger&quot;,&quot;pipeline&quot;,&quot;dataflow&quot;,&quot;dataset&quot;,&quot;linkedService&quot;</code></p>
"
"58559726","How to pass Session parameters to Oracle in azure datafactory copy activity.. In Oracle Linkedservice","<p>I'm copying data from an Oracle instance in aws, self hosted integration runtime service running on a VM in source network.</p>

<p>Issue is, while copying data from Oracle database using copy data activity in Azure, how do I pass session parameters like - NLS_DATE_FORMAT, NLS_TIMESTAMP_FORMAT to oracle session to make timestamp strings in certain format.</p>

<p>Copy activity sink is csv. Files written in csv format with timestamp precision till nanoseconds isn't parseable by spark's csv-reader. </p>

<p>Hence It seemed best idea to bring only seconds to azure from oracle by settings NLS_TIMESTAMP_FORMAT parameter to YYYY-MM-DD HH24:MI:SS</p>

<p>Please suggest how to do it?</p>

<p>My another question on this topic here -<a href=""https://stackoverflow.com/questions/58557480/parse-micro-nano-seconds-timestamp-in-spark-csv-dataframe-reader-inconsistent"">Parse Micro/Nano Seconds timestamp in spark-csv Dataframe reader : Inconsistent results</a></p>

<p>providing it under connection properties parameters availed in no help. See attached screenshot.</p>

<p><a href=""https://i.stack.imgur.com/FlE8l.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FlE8l.jpg"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2019-10-25 13:42:48","367","2","1","71123532","<p>You need to set the parameters (NLS_DATE_FORMAT etc) as System properties in the VM that is hosting the Self Hosted Ingration Runtime.</p>
"
"58556459","Azure Data Factory and GraphDb","<p>In the Azure environment, I have an Azure SQL Db and a CosmosDb Graph. Using an Azure Data Factory, I
need to insert/update data from the Sql db to the GraphDb. </p>

<p>My thinking is that I need to first transform the data to json and from there insert it into the GraphDb.
Is this the way to go? Are there any other ways?</p>

<p>Thank you.</p>
","<azure><azure-cosmosdb><azure-data-factory>","2019-10-25 10:08:27","861","3","1","58585236","<p>1.Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">ADF copy activity connector</a> and the thread: <a href=""https://stackoverflow.com/questions/44143557/how-can-we-create-azures-data-factory-pipeline-with-cosoms-db-with-graph-api"">How can we create Azure&#39;s Data Factory pipeline with Cosoms DB (with Graph API) as data sink ?</a> mentioned by @silent,Cosmos db graph api connector is not supported in ADF so far. You could vote up this feature in this feedback <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/37339906-we-do-a-lot-of-work-in-the-gremlin-api-and-would-l"" rel=""nofollow noreferrer"">link</a> which is updated at April 12, 2019.</p>

<p>2.Cosmos db migration tool isn't a supported import tool for Gremlin API accounts at this time. Please see this link:<a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/import-data"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/cosmos-db/import-data</a></p>

<p>3.You could get an idea of <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/bulk-executor-graph-dotnet"" rel=""nofollow noreferrer"">graph bulk executor .NET library</a> now.This is the sample application:git clone <a href=""https://github.com/Azure-Samples/azure-cosmosdb-graph-bulkexecutor-dotnet-getting-started.gi"" rel=""nofollow noreferrer"">https://github.com/Azure-Samples/azure-cosmosdb-graph-bulkexecutor-dotnet-getting-started.gi</a></p>
"
"58554735","Transfer Tables from One Azure Datawarehouse to Another of Different Subscription","<p>I want to transfer some tables from one azure datawarehouse to another azure data warehouse. But main problem is that both are under different azure subscription. Could anyone told me the source and target both are having different subscription in azure data factory will work or not?</p>
","<azure><azure-data-factory><azure-synapse><sql-data-warehouse>","2019-10-25 08:17:50","73","0","1","58554787","<p>Yes, you can do that in Data Factory. It works.</p>

<p>Choose the new sink Azure datawarehouse from different subscription:
<a href=""https://i.stack.imgur.com/5OESB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5OESB.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"58553528","how to create a text file and insert error message into it in ADFV2","<p>We have a copy activity in ADF and the requirement is to abort the operation on failure,send the mail with error message and log the error information in a text file in blob storage.</p>

<p>Till sending the mail with error message, the task is completed using logic apps.Since skip and log cannot be done in ADF fault tolerance in our case to store the error information in blob storage.Is there a way we can log the error information in a text file in blob storage using ADF when fault tolerance for copy activity is set as 'abort activity on first incompatible row'?</p>

<p>Please let me know if there is any way to log the error information in blob storage when fault tolerance is 'abort activity on first incompatible row'.</p>
","<azure-data-factory><azure-blob-storage>","2019-10-25 06:47:26","662","0","1","58586862","<p>vicky,based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-fault-tolerance#configuration"" rel=""nofollow noreferrer"">Fault tolerance document</a> and options in the ADF UI,there are only 3 options we could pick:</p>

<p><a href=""https://i.stack.imgur.com/airPd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/airPd.png"" alt=""enter image description here""></a></p>

<p>Only <code>""Skip and log incompatible rows""</code> could save the error log into Azure Blob Storage which is configured by us. No any direct way to save the error log into Azure Storage if you have to pick <code>""abort activity on first incompatible row""</code>.</p>

<p>So,my workaround is adding a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure Function Activity</a> behind the Copy Activity if the Copy Activity Status is Failure,like this:</p>

<p><a href=""https://i.stack.imgur.com/xNwEQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xNwEQ.png"" alt=""enter image description here""></a></p>

<p>I make a error situation on purpose for copy activity and you could view the output of Copy Activity which contains error message:</p>

<p><a href=""https://i.stack.imgur.com/DESFB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DESFB.png"" alt=""enter image description here""></a> </p>

<p>Then you could refer this message in the azure function body by <code>@activity('Copy data1').output</code></p>

<p><a href=""https://i.stack.imgur.com/GA3AV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GA3AV.png"" alt=""enter image description here""></a></p>

<p>Inside the azure function,you could store it into blob storage using sdk code.</p>

<hr>

<blockquote>
  <p>can you please share the code for azure function to store the error
  information inside the blob storage as i dont have much knowledge of
  c#.</p>
</blockquote>

<p>I followed this <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob?tabs=csharp#output---usage"" rel=""nofollow noreferrer"">tutorial</a> to test c# code for you:</p>

<pre><code>using System.IO;
using System.Linq;
using System.Net;
using System.Net.Http;
using System.Threading.Tasks;
using Microsoft.Azure.WebJobs;
using Microsoft.Azure.WebJobs.Extensions.Http;
using Microsoft.Azure.WebJobs.Host;
using Microsoft.WindowsAzure.Storage.Blob;

namespace FunctionApp2
{
    public static class Function1
    {
        [FunctionName(""Function1"")]
        public static async Task&lt;HttpResponseMessage&gt; Run([HttpTrigger(AuthorizationLevel.Anonymous, ""get"", ""post"", Route = null)]HttpRequestMessage req,
            [Blob(""test/error.txt"", FileAccess.ReadWrite)] CloudBlockBlob blob,
            TraceWriter log)
        {
            log.Info(""C# HTTP trigger function processed a request."");
            string name = req.GetQueryNameValuePairs()
                .FirstOrDefault(q =&gt; string.Compare(q.Key, ""name"", true) == 0)
                .Value;
            await blob.UploadTextAsync(name);
            if (name == null)
            {
                // Get request body
                dynamic data = await req.Content.ReadAsAsync&lt;object&gt;();
                name = data?.name;
            }
            return name == null
                ? req.CreateResponse(HttpStatusCode.BadRequest, ""Please pass a name on the query string or in the request body"")
                : req.CreateResponse(HttpStatusCode.OK, ""Hello "" + name);
        }
    }
}
</code></pre>

<p>The <code>name</code> parameter is the <code>error message</code> you need to pass.Then it will be saved into <code>test/error.txt</code> path.</p>
"
"58551475","Datafactory : forloop not running in parallel","<p>I am using data factory foreach activity . however the sub-sequence activity are running in sequence . Even issequence is not check . Let's have a look on the following screen. 
<a href=""https://i.stack.imgur.com/e7eqB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e7eqB.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/2Fk3Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2Fk3Q.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/CD8qR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CD8qR.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2019-10-25 02:38:11","2100","1","1","58552075","<p>What is that your sub activity doing ? I did tried a very simple test and I think it is working as expected . Please share more info and I will like to investigate further in this .</p>

<p>This is what i tried .</p>

<p>Created a pipeline with array as a variable , just add four elements to that and in the foreach loop printed the value using the Set Variable activity . I added a wait activity to make it more visible . I set the sequential as false and batch count as 2 . 
This is what i see the , the set variable is grouped together and the value for the SV activity is not in order , very much on the expected lines .</p>

<p><a href=""https://i.stack.imgur.com/iTyw6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iTyw6.png"" alt=""enter image description here""></a></p>

<p>Now when i set the seqiuential = true , i see that only one value is picked at a time .
<a href=""https://i.stack.imgur.com/PN2Vt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PN2Vt.png"" alt=""enter image description here""></a></p>
"
"58551010","Azure Data Flow How to get Max id and return only that row which has max id","<p>How to filer max id and return the last row in the Azure Data flow. Or use Filter in data flow and get only last max id.</p>

<p><a href=""https://i.stack.imgur.com/KMN4r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KMN4r.png"" alt=""enter image description here""></a>
In SQL we Get do it like this</p>

<pre><code>  SELECT Level1Id FROM  [D].[DepartmentGroupLevel1]
WHERE [Level1Id] = (SELECT MAX([Level1Id]) FROM [D].[DepartmentGroupLevel1])
</code></pre>
","<azure><azure-data-factory>","2019-10-25 01:23:48","1070","2","1","58554238","<p>It should just be an Aggregate without any group-by. Group by is optional in the Agg transformation.</p>
"
"58531170","Connecting to SAP Open Hub through Data Factory","<p>I'm working with a client to ETL data from SAP into an Azure storage account. Here are my current conditions:</p>

<p>1) The Open Hub destination has not been set up. </p>

<p>2) We're using Azure Data Factory to ETL data.</p>

<p>3) I can connect to SAP using the SAP Table connector, however, I'm getting an RFC error when I go to select the data source. The specific error is Failed to invoke function /SAPDS/RFC_READ_TABLE2. </p>

<p>Specifically focusing on #1 above, is it possible to successfully connect to SAP using the Azure Data Factory SAP Open Hub connector even if an Open Hub destination does not exist? i.e. is there a configuration setting that needs to be performed for access to connect? </p>
","<azure><sap><azure-data-factory>","2019-10-23 21:23:19","1415","1","1","58620921","<blockquote>
  <p>is it possible to successfully connect to SAP using the Azure Data
  Factory SAP Open Hub connector even if an Open Hub destination does
  not exist?</p>
</blockquote>

<p>The short answer is no. Based on the ADF for SAP <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sap-business-warehouse-open-hub#dataset-properties"" rel=""nofollow noreferrer"">document</a>,the open hub destination name is <code>required</code> property:</p>

<p><a href=""https://i.stack.imgur.com/FimWD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FimWD.png"" alt=""enter image description here""></a></p>

<p>More details about ADF's overall support on SAP data integration scenario,please refer to this link:<a href=""https://github.com/Azure/Azure-DataFactory/blob/master/whitepaper/SAP%20Data%20Integration%20using%20Azure%20Data%20Factory.pdf"" rel=""nofollow noreferrer"">https://github.com/Azure/Azure-DataFactory/blob/master/whitepaper/SAP%20Data%20Integration%20using%20Azure%20Data%20Factory.pdf</a></p>
"
"58531077","Azure Data Factory - CRM (OData) Connector","<p>I have an Azure Data Factory for Data extraction from OnPremise CRM. I am running into an issue with one of the Data entities where the Pipeline runs for close to 8 hours and throws this below exception. I know it's not an issue with authentication as I am able to get the other entities without any issues. I tried to change the parallelCopies to 18 and DIUs but when I trigger the Pipeline it sticks to Parallel Copies of '1', DIUs of 4 and eventually fails. Appreciate any inputs.</p>

<pre><code>Operation on target XXXX failed: Failure happened on 'Source' side. ErrorCode=UserErrorFailedFileOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Upload file failed at path XXXXXXX,Source=Microsoft.DataTransfer.Common,''Type=System.NotSupportedException,Message=The authentication endpoint Kerberos was not found on the configured Secure Token Service!,Source=Microsoft.Xrm.Sdk,'
</code></pre>
","<azure><azure-data-factory>","2019-10-23 21:14:13","272","0","2","58560681","<p>It turned out to be a time out setting on the ADFS, once the time out is increased the job ran successfully. </p>
"
"58531077","Azure Data Factory - CRM (OData) Connector","<p>I have an Azure Data Factory for Data extraction from OnPremise CRM. I am running into an issue with one of the Data entities where the Pipeline runs for close to 8 hours and throws this below exception. I know it's not an issue with authentication as I am able to get the other entities without any issues. I tried to change the parallelCopies to 18 and DIUs but when I trigger the Pipeline it sticks to Parallel Copies of '1', DIUs of 4 and eventually fails. Appreciate any inputs.</p>

<pre><code>Operation on target XXXX failed: Failure happened on 'Source' side. ErrorCode=UserErrorFailedFileOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Upload file failed at path XXXXXXX,Source=Microsoft.DataTransfer.Common,''Type=System.NotSupportedException,Message=The authentication endpoint Kerberos was not found on the configured Secure Token Service!,Source=Microsoft.Xrm.Sdk,'
</code></pre>
","<azure><azure-data-factory>","2019-10-23 21:14:13","272","0","2","74731407","<p>I ran into something similar when using CRM as a sink; any upsert activities would fail very near exactly 60 minutes. The error I observed in the Azure Data Factory activity was:</p>
<pre><code>'Type=System.NotSupportedException,Message=The authentication endpoint Kerberos was not found on the configured Secure Token Service!,Source=Microsoft.Xrm.Sdk,'
</code></pre>
<p><a href=""https://stackoverflow.com/questions/39529608/how-to-configure-session-timeout-for-each-relying-party-trust-in-adfs"">This post</a> helped me find what to change in ADFS. I ran <code>Get-ADFSRelyingPartyTrust</code> and reviewed the <code>TokenLifetime</code> property, which happened to be <code>0</code>. Apparently tokens last 60 minutes when the configuration is 0.</p>
<p>The following PowerShell increased the timeout, and I confirmed upsert activities no longer fail when exceeding 60 minutes.</p>
<pre><code>Set-ADFSRelyingPartyTrust –TargetName &quot;&lt;RelyingPartyTrust&gt;&quot; –TokenLifetime &lt;timeout in minutes&gt;
</code></pre>
"
"58525018","Azure Data Factory: Copy Data activity always returning error from REST or HTTP source datasets","<p>I'm trying to use the Azure Data Factory to create a simple example of extracting data from a REST API and save in a Azure SQL Database. </p>

<p>Linked Service (REST)</p>

<ul>
<li>Base URL: <code>http://dummy.restapiexample.com/</code></li>
<li>Authentication:</li>
<li>Anonymous Test Connection -> Connection Successful</li>
</ul>

<p>Dataset (REST)</p>

<ul>
<li>Linked Service: (the service created above)</li>
<li>Relative URL: <code>api/v1/employees</code></li>
<li>Test Connection -> Connection Successful</li>
</ul>

<p>Pipeline</p>

<ul>
<li>Copy Data activity</li>
<li>Source Dataset: (the dataset created above)</li>
<li>Request method: GET</li>
<li>Preview data -> Failed.</li>
</ul>

<p>Message:</p>

<pre><code>Failed to get response from server, please check network connection and retry.
Contact support or send feedback if the issue persists. Activity ID:d466c55d-d3f3-4216-b560-df84c33ff0b6
</code></pre>

<p>I have tried multiple different endpoints, from different providers, but no luck.</p>

<p>The above endpoint works perfectly on Postman or browser. </p>
","<azure><dataset><azure-data-factory>","2019-10-23 14:21:34","771","1","1","58532932","<p>Which region is the ADF in ? 
Mine is in EAST US and the setup which you mentioned is the same and it works for me.Are you on self hosted integration runtime(SHIR)? </p>

<p>Are you able to preview the data at the dataset level ? My understanding is error  which you mentioning in the question is at the Copy activity level .</p>
"
"58519383","Python Azure function used in Data Factory returns: ""Response Content is not a valid JObject""","<p>I have created a very basic HTTP Trigger in Python that I can validate and test with success. However when I plug it in Data Factory and no matter what I do I get the message: </p>

<blockquote>
  <p>Response Content is not a valid JObject</p>
</blockquote>

<p>From the Azure Python function I tried returning:</p>

<blockquote>
  <p>return func.HttpResponse(json.dumps(f""Hello Test!""), mimetype='application/json')</p>
</blockquote>

<p>and also:</p>

<blockquote>
  <p>return json.dumps(f""Hello Test!"")</p>
</blockquote>

<p>But in both cases I always get the not valid JObject error message. Any ideas what is going on?</p>
","<azure><azure-functions><azure-data-factory>","2019-10-23 09:13:19","845","0","1","58519611","<p>Did you write it with a mistake of ""f""</p>

<p>return json.dumps(<strong>f</strong>""Hello Test!"")</p>

<p>And could you please try to modify your code to </p>

<pre><code>a = {'name': 'wang'}
json.dumps(a)
</code></pre>
"
"58517440","Debugging values into variables or user properties","<p>How can I spy into my values when I'm on ADF debug mode ? </p>

<p>I want to build a simple pipeline that digs into a storage account table. For each row, enter the value of the second column, use it to create a URL and call a web service.</p>

<p>I saw the output of the Lookup command but how can I saw, for example, the content of each input() into the foreach activity. Can I used the user properties for debugging reason ?</p>
","<azure-data-factory>","2019-10-23 07:17:57","2580","3","1","58532282","<p>When debugging, I frequently make use of the 'Set Variable' activity.  Viewing the output of a 'Set Variable' activity is spying on the value.</p>

<p>You want to see the input to each iteration of your ForEach.  Prepend the inner activity with a Set Variable activity.  Dynamic content <code>@string(item())</code> should be enough.</p>
"
"58507717","How to decode html encoded text?","<p>I have a column in a file that contains html encoded text. I would like to decode this column to text before saving it to the database. What is the best method to do this in data factory?</p>
","<azure-data-factory>","2019-10-22 15:32:28","633","2","1","58517873","<p>It seems that <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#conversion-functions"" rel=""nofollow noreferrer"">decode function</a> in ADF can't decode html directly.</p>

<p><a href=""https://i.stack.imgur.com/BFBbR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BFBbR.png"" alt=""enter image description here""></a></p>

<p>ADF sql server connector supports execution of stored procedure:<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#invoke-a-stored-procedure-from-a-sql-sink"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#invoke-a-stored-procedure-from-a-sql-sink</a></p>

<p>You could refer to below threads to decode html with User-Defined Function in sql server:</p>

<p>1.<a href=""https://stackoverflow.com/questions/25432805/sql-server-html-decode-based-on-the-html-names-in-a-string-input/26356779"">SQL Server: HTML Decode based on the HTML names in a String input</a></p>

<p>2.<a href=""https://stackoverflow.com/questions/28253101/is-there-any-way-to-do-html-decode-in-sql-server"">Is there any way to do HTML decode in SQL Server?</a></p>

<p>Then execute function in stored procedure,please refer to this <a href=""https://www.c-sharpcorner.com/UploadFile/rohatash/calling-a-function-from-a-stored-procedure-in-sql-server-201/"" rel=""nofollow noreferrer"">link</a>.</p>
"
"58505987","Copy data from blob storage to sqlDatabase (into multiple tables)","<p>i am relatively new to azure and I found myself stuck! I am trying to read data from my blob storage into SQL database using Azure DataFactory. I got this process to work fine using copy activity, now I am trying to insert the data into multiple tables which relate to each other somehow (privateKey, foreignKey). <strong>For example, to updated the Table CAR I need to know if the owner exists in the Table Owner</strong>. And I am not able to find a detailed explanation on how to go about it! Anyone with some experience to give me some guidance? thanks</p>
","<azure><azure-data-factory>","2019-10-22 13:57:59","300","1","4","58515692","<p>You will need to perform a Join or a Lookup using Mapping Data Flows and then redirect rows to the appropriate database tables using the Conditional Split transformation.</p>
"
"58505987","Copy data from blob storage to sqlDatabase (into multiple tables)","<p>i am relatively new to azure and I found myself stuck! I am trying to read data from my blob storage into SQL database using Azure DataFactory. I got this process to work fine using copy activity, now I am trying to insert the data into multiple tables which relate to each other somehow (privateKey, foreignKey). <strong>For example, to updated the Table CAR I need to know if the owner exists in the Table Owner</strong>. And I am not able to find a detailed explanation on how to go about it! Anyone with some experience to give me some guidance? thanks</p>
","<azure><azure-data-factory>","2019-10-22 13:57:59","300","1","4","58519739","<p>As per my understanding you need to use lookup activity to get table names and then use forEach activity to traverse through the tables and then use Mapping data flow or Databricks to apply filters and joins </p>
"
"58505987","Copy data from blob storage to sqlDatabase (into multiple tables)","<p>i am relatively new to azure and I found myself stuck! I am trying to read data from my blob storage into SQL database using Azure DataFactory. I got this process to work fine using copy activity, now I am trying to insert the data into multiple tables which relate to each other somehow (privateKey, foreignKey). <strong>For example, to updated the Table CAR I need to know if the owner exists in the Table Owner</strong>. And I am not able to find a detailed explanation on how to go about it! Anyone with some experience to give me some guidance? thanks</p>
","<azure><azure-data-factory>","2019-10-22 13:57:59","300","1","4","58533407","<p>I could add to what Shivar had called out . We will have to use the lookup activity , but you should make sure the the table ""Owner"" is copied first before you work on the CAR table , to do this you should enable the sequential option in foreach activity so that the follow the order , otherwise you will a lot of FK violation error . </p>
"
"58505987","Copy data from blob storage to sqlDatabase (into multiple tables)","<p>i am relatively new to azure and I found myself stuck! I am trying to read data from my blob storage into SQL database using Azure DataFactory. I got this process to work fine using copy activity, now I am trying to insert the data into multiple tables which relate to each other somehow (privateKey, foreignKey). <strong>For example, to updated the Table CAR I need to know if the owner exists in the Table Owner</strong>. And I am not able to find a detailed explanation on how to go about it! Anyone with some experience to give me some guidance? thanks</p>
","<azure><azure-data-factory>","2019-10-22 13:57:59","300","1","4","58618252","<p>I would take a different approach to solving this problem.  Using the code below, we can merge data from multiple files, all with a similar name, into a data frame and push the whole thing into SQL Server.  This is Scala, so it needs to be run in your Azure Databricks environment.  </p>

<pre><code># merge files with similar names into a single dataframe
val DF = spark.read.format(""csv"")
   .option(""sep"",""|"")
   .option(""inferSchema"",""true"")
   .option(""header"",""false"")
   .load(""mnt/rawdata/corp/ABC*.gz"")


DF.count()


# rename headers in dataframe
val newNames = Seq(""ID"", ""FName"", ""LName"", ""Address"", ""ZipCode"", ""file_name"")
val dfRenamed = df.toDF(newNames: _*)

dfRenamed.printSchema


# push the dataframe to sql server
import com.microsoft.azure.sqldb.spark.config.Config
import com.microsoft.azure.sqldb.spark.connect._

// Aquire a DataFrame collection (val collection)

val config = Config(Map(
  ""url""            -&gt; ""my_sql_server.database.windows.net"",
  ""databaseName""   -&gt; ""my_db_name"",
  ""dbTable""        -&gt; ""dbo.my_table"",
  ""user""           -&gt; ""xxxxx"",
  ""password""       -&gt; ""xxxxx"",
  ""connectTimeout"" -&gt; ""5"", //seconds
  ""queryTimeout""   -&gt; ""5""  //seconds
))

import org.apache.spark.sql.SaveMode
DF.write.mode(SaveMode.Append).sqlDB(config)
</code></pre>

<p>The code above will read every line of every file.  If the headers are in the first line, this works great.  If the headers and NOT in the first line, use the code below to crate a specific schema, and again, read every line of every file.</p>

<pre><code>import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType};
import org.apache.spark.sql.functions.input_file_name

val customSchema = StructType(Array(
    StructField(""field1"", StringType, true),
    StructField(""field2"", StringType, true),
    StructField(""field3"", StringType, true),
    StructField(""field4"", StringType, true),
    StructField(""field5"", StringType, true),
    StructField(""field6"", StringType, true),
    StructField(""field7"", StringType, true)))

val df = sqlContext.read
    .format(""com.databricks.spark.csv"")
    .option(""header"", ""false"")
    .option(""sep"", ""|"")
    .schema(customSchema)
    .load(""mnt/rawdata/corp/ABC*.gz"")
    .withColumn(""file_name"", input_file_name())


import com.microsoft.azure.sqldb.spark.bulkcopy.BulkCopyMetadata
import com.microsoft.azure.sqldb.spark.config.Config
import com.microsoft.azure.sqldb.spark.connect._



val bulkCopyConfig = Config(Map(
  ""url""               -&gt; ""mysqlserver.database.windows.net"",
  ""databaseName""      -&gt; ""MyDatabase"",
  ""user""              -&gt; ""username"",
  ""password""          -&gt; ""*********"",
  ""databaseName""      -&gt; ""MyDatabase"",
  ""dbTable""           -&gt; ""dbo.Clients"",
  ""bulkCopyBatchSize"" -&gt; ""2500"",
  ""bulkCopyTableLock"" -&gt; ""true"",
  ""bulkCopyTimeout""   -&gt; ""600""
))

df.write.mode(SaveMode.Append).
//df.bulkCopyToSqlDB(bulkCopyConfig, bulkCopyMetadata)
//df.bulkCopyToSqlDB(bulkCopyConfig) if no metadata is specified.
</code></pre>
"
"58502205","Azure DataFactory Integrated HDInsight Spark Problem","<p>When i use Data Factory, created <code>HDInsight Spark activity</code>, there are two options in type(script and jar), when i choose <strong>jar to run</strong>( i use <strong>scala</strong> to develop jar file, it ran <strong>succeeded</strong> by <code>remote livy submit</code>) ,it <strong>failed</strong> and log shows: </p>

<pre><code>*stdout: 
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/2.6.5.3009-43/spark2/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/2.6.5.3009-43/spark_llap/spark-llap-assembly-1.0.0.2.6.5.3009-43.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Error: --py-files given but primary resource is not a Python script
Run with --help for usage help or --verbose for debug output

stderr: 

YARN Diagnostics: 
java.lang.Exception: No YARN application is found with tag livy-batch-4-tpqnpjdw in 120 seconds. Please check your cluster status, it is may be very busy.
org.apache.livy.utils.SparkYarnApp.org$apache$livy$utils$SparkYarnApp$$getAppIdFromTag(SparkYarnApp.scala:182) org.apache.livy.utils.SparkYarnApp$$anonfun$1$$anonfun$4.apply(SparkYarnApp.scala:239) org.apache.livy.utils.SparkYarnApp$$anonfun$1$$anonfun$4.apply(SparkYarnApp.scala:236) scala.Option.getOrElse(Option.scala:120) org.apache.livy.utils.SparkYarnApp$$anonfun$1.apply$mcV$sp(SparkYarnApp.scala:236) org.apache.livy.Utils$$anon$1.run(Utils.scala:97)
*
</code></pre>

<p>but when i choose script <strong>to run</strong>( using <code>python</code> to develop scrits) , it <strong>succeeded</strong>, can anyone solve this problem?</p>
","<apache-spark><azure-hdinsight><azure-data-factory>","2019-10-22 10:27:11","239","1","1","58519045","<p>This is expected behaviour in HDInsight Spark Activity.</p>

<p><strong>Reason:</strong> It's mandatory to specify at least a <strong>Python file</strong> or a <strong>.jar file</strong> while running HDInsight Spark activity. </p>

<p><a href=""https://i.stack.imgur.com/8YeMu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8YeMu.png"" alt=""enter image description here""></a></p>

<p>From the error message ""<code>Error: --py-files given but primary resource is not a Python script</code>"" clearly says it not a Python script.</p>

<p><strong>Reference:</strong> <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-spark"" rel=""nofollow noreferrer"">Transform data using Spark activity in Azure Data Factory</a> </p>

<p>Hope this helps.</p>
"
"58500603","debug and breakpoint print parameters as alert in ADF","<p><a href=""https://i.stack.imgur.com/dIla1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dIla1.png"" alt=""enter image description here""></a>I have an pipeline where i am calling other pipeline.
I want to see the values passed and the action happening when i debug.
In the below, during debug, i want to see what is the value sent with breakpoints for each flow of parameters and also want to see the PL_Purge Folder in action with debug</p>

<p><a href=""https://i.stack.imgur.com/dIla1.png"" rel=""nofollow noreferrer"">!PIPELINE PURGE</a>]<a href=""https://i.stack.imgur.com/dIla1.png"" rel=""nofollow noreferrer"">1</a></p>

<p>How to do it,. can you share some links</p>
","<azure><azure-data-factory>","2019-10-22 08:59:46","498","0","1","58533477","<p>I think you have the only option to use the set variable activity and assign the value <code>@item().XXXX</code> . When you do that &amp; run the pipeline you will be able to values at the pipeline output tab , nothing like a break point in a IDE , but since you have only 4 inputs which you are tracking this should be doable .
Please do let me know how it goes .</p>
"
"58498849","Update Existing Documents in Cosmos db using Azure Data Factory","<p>I have created a data factory to Update my existing Cosmos DB using data stored in Blob as csv file.
The csv file contains two fields column ""X"" and ""Y"" and their values.
I want to insert new key in  my existing Cosmos Db as Y ,filter criteria is column X which is unique</p>

<p>I already created the pipeline ,and upsert the documents in cosmosdb.
But in current Pipeline old data is getting removed and only column Y is getting inserted.</p>

<p>Data in current Document in cosmos db</p>

<pre><code>  {
    ""_id"" : ObjectId(""5dad5adbfa882146ea8e7a0e""),
    ""x"" : ""UUID"",
   ""old_key"" : true
  }
</code></pre>

<p>Data in csv file </p>

<pre><code>   x,new_key
   UUID, ""new_value""
</code></pre>

<p>Expected output</p>

<pre><code>   {
     ""_id"" : ObjectId(""5dad5adbfa882146ea8e7a0e""),
      ""x"" : ""UUID"",
     ""old_key"" : true,
    ""new_key"":""new_value""

    }
</code></pre>

<p>My output which i tried(old keys are removed)</p>

<pre><code>   {
     ""_id"" : ObjectId(""5dad5adbfa882146ea8e7a0e""),
     ""x"" : ""UUID""
     ""new_key"":""new_value""

   }
</code></pre>
","<azure-cosmosdb><azure-blob-storage><azure-data-factory><azure-cosmosdb-mongoapi>","2019-10-22 07:09:32","1796","2","1","58499466","<p>You want to implement incremental update or partial update for cosmos db.</p>

<pre><code>{
    ""_id"" : ObjectId(""5dad5adbfa882146ea8e7a0e""),
    ""x"" : ""UUID"",
   ""old_key"" : true
}
</code></pre>

<p>change to:</p>

<pre><code>{
     ""_id"" : ObjectId(""5dad5adbfa882146ea8e7a0e""),
      ""x"" : ""UUID"",
     ""old_key"" : true,
    ""new_key"":""new_value""
}
</code></pre>

<p>As i know, partial update is not supported in the cosmos db so far.Please see this <a href=""https://feedback.azure.com/forums/263030-azure-cosmos-db/suggestions/6693091-be-able-to-do-partial-updates-on-document"" rel=""nofollow noreferrer"">user voice</a> and this <a href=""https://buildazure.com/top-10-cosmos-db-features-coming-soon-march-2019/"" rel=""nofollow noreferrer"">feature statement</a>(i know this feature lack lasts so long already....). So, you have to contains all the properties in your new documents except <code>""_id""</code>.</p>

<p>I did a simple test:</p>

<p>1.load a csv file into cosmos db:</p>

<pre><code>_id,x,A
5daeb1eda34d640184a71d47,123,ValueA
</code></pre>

<p>2.load a new csv file into cosmos db:</p>

<pre><code>_id,x,B
5daeb1eda34d640184a71d47,123,ValueBBB
</code></pre>

<p>3.print two outputs:</p>

<p><a href=""https://i.stack.imgur.com/f0klG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f0klG.png"" alt=""enter image description here""></a></p>
"
"58494148","How to start modify all Azure Data Factory Triggers","<p>The following code will stop all Azure Data Factory triggers:</p>

<pre><code>$triggersADF = Get-AzDataFactoryV2Trigger -DataFactoryName $DataFactoryName -ResourceGroupName $ResourceGroupName

$triggersADF | ForEach-Object { Stop-AzDataFactoryV2Trigger -ResourceGroupName $ResourceGroupName -DataFactoryName $DataFactoryName -Name $_.name -Force }
</code></pre>

<p>However, what I thought would restart all the Triggers doesn't appear to work:</p>

<pre><code>$triggersADF | ForEach-Object { Start-AzDataFactoryV2Trigger -ResourceGroupName $ResourceGroupName -DataFactoryName $DataFactoryName -Name $_.name -Force }
</code></pre>

<p>I get the following error:</p>

<pre><code>$triggersADF | ForEach-Object { Start-AzDataFactoryV2Trigger -ResourceGroupName $ResourceGroupName -DataFactoryName $DataFactoryName -Name $_.name -Force }
True
Start-AzDataFactoryV2Trigger : HTTP Status Code: BadRequest
Error Code: BadRequest
Error Message: Missing or invalid pipeline references for trigger Trigger_az4
Request Id: 08b0e51a-9a3d-4662-ba8c-8ea62279c558
Timestamp (Utc):10/21/2019 21:05:10
At line:3 char:33
+ ... ch-Object { Start-AzDataFactoryV2Trigger -ResourceGroupName $Resource ...
+                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : CloseError: (:) [Start-AzDataFactoryV2Trigger], CloudException
    + FullyQualifiedErrorId : Microsoft.Azure.Commands.DataFactoryV2.StartAzureDataFactoryTriggerCommand

Start-AzDataFactoryV2Trigger : HTTP Status Code: BadRequest
Error Code: BadRequest
Error Message: Missing or invalid pipeline references for trigger Trigger_c2l
Request Id: 86a0199d-5399-4079-abd4-7fe6973df222
Timestamp (Utc):10/21/2019 21:05:11
At line:3 char:33
+ ... ch-Object { Start-AzDataFactoryV2Trigger -ResourceGroupName $Resource ...
+                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : CloseError: (:) [Start-AzDataFactoryV2Trigger], CloudException
    + FullyQualifiedErrorId : Microsoft.Azure.Commands.DataFactoryV2.StartAzureDataFactoryTriggerCommand
</code></pre>

<p>Any thoughts on where I might be going wrong?</p>
","<azure-devops><azure-data-factory>","2019-10-21 21:06:48","1707","1","2","58498020","<p>The issue seems to be in the ARM template, the payload of trigger has a property called pipeline. For scheduled trigger it should be 'pipelines'. Refer the json of the trigger from the UI, and you will see that it should be an array of pipelines, not a single pipeline.</p>
"
"58494148","How to start modify all Azure Data Factory Triggers","<p>The following code will stop all Azure Data Factory triggers:</p>

<pre><code>$triggersADF = Get-AzDataFactoryV2Trigger -DataFactoryName $DataFactoryName -ResourceGroupName $ResourceGroupName

$triggersADF | ForEach-Object { Stop-AzDataFactoryV2Trigger -ResourceGroupName $ResourceGroupName -DataFactoryName $DataFactoryName -Name $_.name -Force }
</code></pre>

<p>However, what I thought would restart all the Triggers doesn't appear to work:</p>

<pre><code>$triggersADF | ForEach-Object { Start-AzDataFactoryV2Trigger -ResourceGroupName $ResourceGroupName -DataFactoryName $DataFactoryName -Name $_.name -Force }
</code></pre>

<p>I get the following error:</p>

<pre><code>$triggersADF | ForEach-Object { Start-AzDataFactoryV2Trigger -ResourceGroupName $ResourceGroupName -DataFactoryName $DataFactoryName -Name $_.name -Force }
True
Start-AzDataFactoryV2Trigger : HTTP Status Code: BadRequest
Error Code: BadRequest
Error Message: Missing or invalid pipeline references for trigger Trigger_az4
Request Id: 08b0e51a-9a3d-4662-ba8c-8ea62279c558
Timestamp (Utc):10/21/2019 21:05:10
At line:3 char:33
+ ... ch-Object { Start-AzDataFactoryV2Trigger -ResourceGroupName $Resource ...
+                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : CloseError: (:) [Start-AzDataFactoryV2Trigger], CloudException
    + FullyQualifiedErrorId : Microsoft.Azure.Commands.DataFactoryV2.StartAzureDataFactoryTriggerCommand

Start-AzDataFactoryV2Trigger : HTTP Status Code: BadRequest
Error Code: BadRequest
Error Message: Missing or invalid pipeline references for trigger Trigger_c2l
Request Id: 86a0199d-5399-4079-abd4-7fe6973df222
Timestamp (Utc):10/21/2019 21:05:11
At line:3 char:33
+ ... ch-Object { Start-AzDataFactoryV2Trigger -ResourceGroupName $Resource ...
+                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : CloseError: (:) [Start-AzDataFactoryV2Trigger], CloudException
    + FullyQualifiedErrorId : Microsoft.Azure.Commands.DataFactoryV2.StartAzureDataFactoryTriggerCommand
</code></pre>

<p>Any thoughts on where I might be going wrong?</p>
","<azure-devops><azure-data-factory>","2019-10-21 21:06:48","1707","1","2","58509208","<p>In order to stop the triggers you will need to change the Azure Powershell Inline:script task to Preview 4, see image.
<a href=""https://i.stack.imgur.com/SqVz8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SqVz8.png"" alt=""enter image description here""></a></p>

<p>The following code will restart the Triggers;</p>

<pre><code>#Triggers 

    $triggersADF = Get-AzDataFactoryV2Trigger -DataFactoryName 

$DataFactoryName -ResourceGroupName $ResourceGroupName
        $triggersTemplate = $resources | Where-Object { $_.type -eq ""Microsoft.DataFactory/factories/triggers"" }
        $triggerNames = $triggersTemplate | ForEach-Object {$_.name.Substring(37, $_.name.Length-40)}
        $activeTriggerNames = $triggersTemplate | Where-Object { $_.properties.runtimeState -eq ""Started"" -and ($_.properties.pipelines.Count -gt 0 -or $_.properties.pipeline.pipelineReference -ne $null)} | ForEach-Object {$_.name.Substring(37, $_.name.Length-40)}
        $deletedtriggers = $triggersADF | Where-Object { $triggerNames -notcontains $_.Name }
        $triggerstostop = $triggerNames | where { ($triggersADF | Select-Object name).name -contains $_ }


    if ($predeployment -eq $true) {
        Write-Host ""Starting active triggers""
        $activeTriggerNames | ForEach-Object { 
            Write-host ""Enabling trigger "" $_
            Start-AzDataFactoryV2Trigger -ResourceGroupName $ResourceGroupName -DataFactoryName $DataFactoryName -Name $_ -Force 
        }
    }

The answers came directly from Microsoft. The code can be found at the official site for CI/CD for Azure Data Factory here:
https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#update-active-triggers
</code></pre>

<p>I hope this helps.</p>
"
"58493251","Reading metadata CSV from a datalake, too big for a lookup activity","<p>I need to create a pipeline to read CSVs from a folder, load from Row 8 into an Azure SQL table, Frist 5 rows will go into a different table ([tblMetadata]).</p>

<p>So far I have done it using Lookup Activity, works fine, but one of the files is bigger than 6 MB and it fails.</p>

<p>I checked all options in Lookup, read everything about Copy Activity (which I am using to load main data - skip 7 rows). The pipeline is created using GUI.</p>

<p>The output from the Lookup is used as parameters for a Stored Procedure to insert into tblMetadata</p>

<p>Can someone advise me how to deal with this? At the moment I am on the training, no one can help me on site.</p>
","<azure-sql-database><azure-data-factory>","2019-10-21 19:54:05","92","0","2","58497843","<p>You could probably do this with a single Data Flow activity that has a couple of transformations.</p>

<p>You would use a Source transformation that reads from a folder using folder paths and wildcards, then add a conditional split transformation to send different rows to different sinks.</p>
"
"58493251","Reading metadata CSV from a datalake, too big for a lookup activity","<p>I need to create a pipeline to read CSVs from a folder, load from Row 8 into an Azure SQL table, Frist 5 rows will go into a different table ([tblMetadata]).</p>

<p>So far I have done it using Lookup Activity, works fine, but one of the files is bigger than 6 MB and it fails.</p>

<p>I checked all options in Lookup, read everything about Copy Activity (which I am using to load main data - skip 7 rows). The pipeline is created using GUI.</p>

<p>The output from the Lookup is used as parameters for a Stored Procedure to insert into tblMetadata</p>

<p>Can someone advise me how to deal with this? At the moment I am on the training, no one can help me on site.</p>
","<azure-sql-database><azure-data-factory>","2019-10-21 19:54:05","92","0","2","58506449","<p>I did workaround in different way, modified CSVs that are bing imported to have whole Metadata in the first row (as this was part of my different project). Then used FirstRow only in Lookup.</p>
"
"58488225","Azure Data Factory V1 - Date for Deprecation/End of life?","<p>In the Azure portal I get a message about that dataset and pipeline monitoring for Azure Data Factory V1 will be disabled on 2019-11-30. It seems to me that Azure Data Factory V1 is getting more and more deprecated and that one should migrate to Azure Data Factory V2 as soon as possible. Is there an approximate date for total deprecation or end of life set according to some roadmap?</p>

<p>Would be really appreciated to get an answer because if there's a near shutdown planned we need to assign resources to do this migration quickly.</p>
","<azure><azure-data-factory>","2019-10-21 14:16:21","1407","1","2","58488872","<p>You can see any retirement notices here:</p>

<p><a href=""https://azure.microsoft.com/en-us/updates/?updatetype=retirements"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/updates/?updatetype=retirements</a></p>

<p>However this announcement seems to have been made in the blog:</p>

<p><a href=""https://azure.microsoft.com/en-us/blog/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/blog/</a></p>

<p>You can keep up to date with these announcements by subscribing to <a href=""https://azure.microsoft.com/en-us/blog/feed/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/blog/feed/</a> in Outlook or another RSS compatible application.</p>

<p>You can subscribe to the retirement announcements here <a href=""https://azure.microsoft.com/en-us/updates/feed/?product=data-factory&amp;updatetype=retirements"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/updates/feed/?product=data-factory&amp;updatetype=retirements</a></p>
"
"58488225","Azure Data Factory V1 - Date for Deprecation/End of life?","<p>In the Azure portal I get a message about that dataset and pipeline monitoring for Azure Data Factory V1 will be disabled on 2019-11-30. It seems to me that Azure Data Factory V1 is getting more and more deprecated and that one should migrate to Azure Data Factory V2 as soon as possible. Is there an approximate date for total deprecation or end of life set according to some roadmap?</p>

<p>Would be really appreciated to get an answer because if there's a near shutdown planned we need to assign resources to do this migration quickly.</p>
","<azure><azure-data-factory>","2019-10-21 14:16:21","1407","1","2","67126504","<p>UPDATE April 2021</p>
<p>In <a href=""https://azure.microsoft.com/en-us/updates/migrate-your-azure-data-factory-version-1-to-2-service/"" rel=""nofollow noreferrer"">&quot;Migrate your Azure Data Factory version 1 to 2 service&quot; update</a>, which was published on September 03, 2020, Azure announced that after 31 August 2023, Azure Data Factory version 1 will not be supported.</p>
<blockquote>
<p>With these enhanced functionalities, we are retiring Azure Data Factory version 1 on 31 August 2023. Please make the switch early to get the benefits of version 2 (current version).</p>
</blockquote>
<blockquote>
<p>After 31 August 2023, Azure Data Factory version 1 will not be supported.</p>
</blockquote>
"
"58481492","How to get Frequency and Interval information from AzureDataFactoryV2 TriggerResource object?","<p>So I pass triggerName to:
<code>client.Triggers.Get(""&lt;resourceGroupName&gt;"", ""&lt;dataFactoryName&gt;"", ""&lt;triggerName&gt;"")</code> function and it returns me triggerResource object. Now If I <strong>Debug</strong> that triggerResource object has these class attributes and functions:</p>

<pre><code>1) triggerResource.StartTime
2) triggerResource.EndTime
3) triggerResource.Delay
4) triggerResource.Frequency
5) triggerResource.Interval
</code></pre>

<p>Now I want frequency and Interval values but these attributes are only visible when I debug not during I refer. So doing triggerResource. gives no option for frequency and Interval. This seems that are they private? Or how can I get them?</p>
","<c#><triggers><azure-data-factory>","2019-10-21 07:19:04","27","0","1","60578130","<p>So I resolved the issue, I was actually having parent class instead of child class, So what i did is used dynamic type with explicit casting to get the right object. So that i can use its property.</p>
"
"58476393","Do we have DataSlice concept in AzureDataFactoryV2?","<p>Like in AzureDataFactory we can use Microsoft.Azure.Management.DataFactories.DataSlice to get DataSlices from given Dataset. Do we have similar functionality in AzureDataFactoryV2 under Microsoft.Azure.Management.DataFactory (Used for ADFV2) so that we can get previous DataSlices. I want to use ADFV2 .Net SDK to fetch DataSlices.</p>
","<dataset><azure-data-factory><azure-search-.net-sdk>","2019-10-20 19:11:22","95","0","1","58476749","<p>ADFv2 uses a different model than ADFv1.
The new model is based on triggers and not any more on a dataset slicing model as it was the case with ADFv1.</p>

<p>From <a href=""https://learn.microsoft.com/en-us/azure/data-factory/compare-versions"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/compare-versions</a> :</p>

<p>ADF v1</p>

<blockquote>
  <p>Availability defines the processing window slicing model for the dataset (for example, hourly, daily, and so on).</p>
</blockquote>

<p>ADF v2</p>

<blockquote>
  <p>You do not need to define availability schedules for datasets. You can define a trigger resource that can schedule pipelines from a clock scheduler paradigm. For more information, see Triggers and Datasets.</p>
</blockquote>
"
"58466728","Error while mapping .tsv file data with Azure data warehouse using data factory","<p>I wanted to copy data from form data lake present in <strong>.tsv</strong> file to Azure data warehouse but there are 2 columns that are in <strong>date</strong> datatype and because of that I am facing issues while mapping the columns. And because of that I am not able to load the data. I can use u-SQL activity only if needed.</p>
","<azure><azure-sql-database><azure-data-factory><azure-data-lake><u-sql>","2019-10-19 18:18:49","204","0","1","58483862","<p>I tried and tested, always get the error message:</p>

<pre><code>""ErrorCode=UserErrorInvalidDataValue,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,
Message=Column 'dd' contains an invalid value '20190101'.,Source=Microsoft.DataTransfer.ServiceLibrary,''Type=System.ArgumentException,Message=String was not recognized as a valid DateTime.Couldn't store &lt;20190101&gt; in dd Column.  Expected type is DateTime.,Source=System.Data,''Type=System.FormatException,Message=String was not recognized as a valid DateTime.,Source=mscorlib,'"",
        ""failureType"": ""UserError"",
        ""target"": ""Copy_b70""
</code></pre>

<p>If you don't want to change your tsv file,  I suggest you to using the Data Flow Derived Column. It can help covert the ""20100101"" data to a correct date format.</p>

<p><strong>Derived Column</strong>:</p>

<p><a href=""https://i.stack.imgur.com/KQHx1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KQHx1.png"" alt=""enter image description here""></a></p>

<p>Using this Expression for field Column ""20100101"":</p>

<pre><code>   add( (substring(Column_2, 1, 4)),'-')+
   add((substring(Column_2, 5, 2)), '-')+
   substring(Column_2, 7, 2)
</code></pre>

<p><a href=""https://i.stack.imgur.com/PBCsI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PBCsI.png"" alt=""enter image description here""></a></p>

<p>It works ok.</p>

<p><a href=""https://i.stack.imgur.com/dSWjG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dSWjG.png"" alt=""enter image description here""></a> </p>

<p>Hope this helps.</p>
"
"58462309","Delete nested date folder getdate() < 5 date","<p>Folders in DataLake have <strong>nested date folder structure</strong> </p>

<pre><code>2019 
    09
       29
       30
    10
       01
       02
       ..
       ..
       20

</code></pre>

<p>I have wrote an Datafactory using actives for-each , GetMetaData,IfCondition and Delete </p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>{
    ""name"": ""IterateEachADLSItem"",
    ""type"": ""ForEach"",
    ""dependsOn"": [
        {
            ""activity"": ""F_SAP"",
            ""dependencyConditions"": [
                ""Succeeded""
            ]
        }
    ],
    ""userProperties"": [],
    ""typeProperties"": {
        ""items"": {
            ""value"": ""@activity('F_SAP').output.value"",
            ""type"": ""Expression""
        },
        ""isSequential"": false,
        ""activities"": [
            {
                ""name"": ""GetPurgeFolderMetadata"",
                ""description"": ""GetPurgeFolderMetadata"",
                ""type"": ""GetMetadata"",
                ""dependsOn"": [],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""userProperties"": [],
                ""typeProperties"": {
                    ""dataset"": {
                        ""referenceName"": ""DS_ADLS_FolderMetadata"",
                        ""type"": ""DatasetReference"",
                        ""parameters"": {
                            ""FolderPath"": {
                                ""value"": ""@concat(item().DestinationPath,item().SinkTableName,'/',item().LoadTypeName,'/',formatDateTime(adddays(utcnow(),-30),item().LoadIntervalFormat))"",
                                ""type"": ""Expression""
                            }
                        }
                    },
                    ""fieldList"": [
                        ""exists""
                    ]
                }
            },
            {
                ""name"": ""CheckPurgeFolderExists"",
                ""type"": ""IfCondition"",
                ""dependsOn"": [
                    {
                        ""activity"": ""GetPurgeFolderMetadata"",
                        ""dependencyConditions"": [
                            ""Succeeded""
                        ]
                    }
                ],
                ""userProperties"": [],
                ""typeProperties"": {
                    ""expression"": {
                        ""value"": ""@activity('GetPurgeFolderMetadata').output.Exists"",
                        ""type"": ""Expression""
                    },
                    ""ifTrueActivities"": [
                        {
                            ""name"": ""DeletePurgeFiles"",
                            ""type"": ""Delete"",
                            ""dependsOn"": [],
                            ""policy"": {
                                ""timeout"": ""7.00:00:00"",
                                ""retry"": 0,
                                ""retryIntervalInSeconds"": 30,
                                ""secureOutput"": false,
                                ""secureInput"": false
                            },
                            ""userProperties"": [],
                            ""typeProperties"": {
                                ""dataset"": {
                                    ""referenceName"": ""DS_ADLS_CopyDataSinkFile"",
                                    ""type"": ""DatasetReference"",
                                    ""parameters"": {
                                        ""SinkFilePath"": {
                                            ""value"": ""@concat(item().DestinationPath,item().SinkTableName,'/',item().LoadTypeName,'/',formatDateTime(adddays(utcnow(),-5),item().LoadIntervalFormat))"",
                                            ""type"": ""Expression""
                                        }
                                    }
                                },
                                ""enableLogging"": false,
                                ""recursive"": true
                            }
                        }
                    ]
                }
            }
        ]
    }
}</code></pre>
</div>
</div>
</p>

<p>I am able to delete the exact 5 days before when it comes to date in the yyyy/mm/dd . if i execute on 2019/10/19, i am able to delete 2019/10/14</p>

<p>Delete is fine based on below syntax for exact 5 dates before</p>

<pre><code>''""value"": ""@concat(item().DestinationPath,item().SinkTableName,'/',item().LoadTypeName,'/',formatDateTime(adddays(utcnow(),-5),item().LoadIntervalFormat))"",
</code></pre>

<p>I am not able to loop and find for earlier days in the folder for each month or earlier year provided there is an folder existing.</p>

<p>ie :
<strong>how to delete for previous month and previous years when we are in say eg: 03/01/2020.</strong></p>
","<azure><azure-data-factory>","2019-10-19 09:37:43","204","0","1","58474671","<p>I think you could accomplish your goal by below approach:</p>

<ol>
<li>Have Get Metadata activity to list all folders within root folder, and include foreach activity within GetMetadata.This should produce list of years.</li>
<li>Include Condition activity to compare the current year against folder's name and include delete activity,if that number is less  than the current year.</li>
<li>If the above condition doesn't match, include another condition to check if it's equal and if yes, include another Get Metadata activity, </li>
<li>The above Get Metadata activity will list folders, which should produce months list and include foreach activity. Have similar logic for the months and include another metadata activity to list days for the current month.

<ol start=""3"">
<li>Finally, the last Get Metadata activity will list days and when day number is less than current day, will include Delete activity</li>
</ol></li>
</ol>

<p>Hope that helps,
Fikrat</p>
"
"58456846","Azure Function can write files in Data Lake, when it is bound to Event Grid, but not when it is called from Azure Data Factory","<p>I have an Azure Function that should process zip files and convert their contents to csv files and save them on a Data Lake gen 1.</p>
<p>I have enabled managed Identity of this Azure Functions. Then I have added this managed Identity as an OWNER on the Access control (IAM) of Data Lake.</p>
<p><strong>First Scenario:</strong></p>
<p>I call this Azure function from <strong>Azure Data Factory</strong>, and send the fileuri of zip files, which are persisted in a Storage Acccount from Azure Data Factory to Azure Functions, Azure Function process the files by saving csv files in data lake I get this error:</p>
<blockquote>
<p>Error in creating file root\folder1\folder2\folder3\folder4\test.csv.</p>
<p>Operation: CREATE failed with HttpStatus:Unauthorized Token Length: 1162</p>
<p>Unknown Error: Unexpected type of exception in JSON error output.
Expected: RemoteException Actual: error Source:
Microsoft.Azure.DataLake.Store StackTrace: at
Microsoft.Azure.DataLake.Store.WebTransport.ParseRemoteError(Byte[]
errorBytes, Int32 errorBytesLength, OperationResponse resp, String
contentType).</p>
<p>RemoteJsonErrorResponse: Content-Type of error response:
application/json; charset=utf-8.</p>
<p>Error:{&quot;error&quot;:{&quot;code&quot;:&quot;AuthenticationFailed&quot;,&quot;message&quot;:&quot;The access token in the 'Authorization' header is expired.</p>
</blockquote>
<p><strong>Second Scenario</strong></p>
<p>I have set an Event Grid for this Azure Functions. After dropping zip files in storage account, which is bound to Azure Functions, zip files are processed and csv files are successfully saved in the Data Lake.</p>
<p>good to know azure function and Data Lake are in the same vnet</p>
<p>Could someone explain to me, why my function works fine with the event grid but doesn't work if I call it from Azure Data Factory? (saving csv files in Data Lake Gen 1)</p>
","<azure><azure-functions><azure-data-factory><azure-data-lake><azure-managed-identity>","2019-10-18 19:08:21","864","0","1","58483564","<p>Have you show all of the error?</p>

<p>From the error seems it is related to Azure Active AD authentication(Data Lake Storage Gen1 uses Azure Active Directory for authentication). Have you use <strong>Bearer token</strong> in your Azure Function when you try to send something to Data Lake Storage Gen1?</p>

<p>please show the code of your azure function, otherwise it will be hard to find the cause of the error.</p>
"
"58456158","Azure Data Factory user parameter","<p>In Azure Databricks I want to get the user that trigger manually a Notebook in Data Factory pipeline. I think Data Factory doesn't have a dynamic parameter to pass the user to Databricks, only pipeline features and functions. Do you know any solution for this?</p>
","<azure><azure-pipelines><azure-data-factory><azure-databricks>","2019-10-18 18:11:05","1228","1","1","58458041","<p>It does have dynamic parameters for a databricks notebook!! Follow this tutorial and it will guide you to do just that :D</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-databricks-notebook"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-databricks-notebook</a></p>

<p>Hope this helped!</p>
"
"58453457","Multiple Data flows vs all Transformations in one","<p>Hi I am new to Azure data factory and not all familiar with the back-end processing that run behind the scenes. I am wondering if there is a performance impact to running couple of data flows in parallel when compared to having all the transformations in one data flow.</p>

<p>I am trying to stage some data with a not exists transformation. i have to do it for multiple tables. when i test ran two data flows in parallel the clusters were brought up together for both the data flows simultaneously. But I am not sure if this the best approach to distribute the loading of tables across couple of data flows or to have all the transformations in one data flow</p>
","<azure-data-factory>","2019-10-18 15:00:24","1184","0","1","58455769","<p>1: If you execute data flows in a pipeline in parallel, ADF will spin-up separate Spark clusters for each based on the settings in your Azure Integration Runtime attached to each activity.</p>

<p>2: If you put all of your logic inside a single data flow, then it will all execute in that same job execution context on a single Spark cluster instance.</p>

<p>3: Another option is to execute the activities in serial in the pipeline. If you have set a TTL on the Azure IR configuration, then ADF will reuse the compute resources (VMs) but you will still a brand-new Spark context for each execution.</p>

<p>All are valid practices and which one you choose should be driven by your requirements for your ETL process.</p>

<p>No. 3 will likely take the longest time to execute end-to-end. But it does provide a clean separation of operations in each data flow step.</p>

<p>No. 2 could be more difficult to follow logically and doesn't give you much re-usability.</p>

<p>No. 1 is really similar to #3, but you run them all in parallel. Of course, not every end-to-end process can run in parallel. You may require a data flow to finish before starting the next, in which case you're back in #3 serial mode.</p>
"
"58434951","Azure Data factory Copy Pipeline failing if more than 6 columns","<p>I'm trying to copy data from a view in either on-premise PostgeSQL or MSSQL to a table in Azure PostgreSQL. I can't seem to get this to successfully copy when I map more than 6 columns from source to sink.</p>

<p>I suspected that one of the source columns was the issue, so varied what columns were mapped but all columns will succeed if less than 6 are copied in total.
I then tried different source and sink tables, which produces the same results.</p>

<p>If I copy to an intermediary csv file I can import/export unlimited columns successfully.</p>

<p>Error output shown in the ADF console consistently is:
""Operation on target Copy_7dp failed: Type=Npgsql.PostgresException,Message=08P01: invalid message format,Source=Npgsql,'""</p>
","<postgresql><azure><azure-data-factory><npgsql><azure-postgresql>","2019-10-17 14:26:37","2943","2","2","58448342","<blockquote>
  <p>Error output shown in the ADF console consistently is: ""Operation on
  target Copy_7dp failed: Type=Npgsql.PostgresException,Message=08P01:
  invalid message format,Source=Npgsql,'""</p>
</blockquote>

<p>You may find some clues in this <a href=""https://github.com/npgsql/npgsql/issues/189"" rel=""nofollow noreferrer"">old case</a> which is similar to your error trace. Please see the solution in above link: <a href=""https://github.com/npgsql/npgsql/issues/189#issuecomment-67001166"" rel=""nofollow noreferrer"">https://github.com/npgsql/npgsql/issues/189#issuecomment-67001166</a></p>

<blockquote>
  <p>Basically, if the string value we try to persist using Npgsql was
  derived from MemoryStream.GetBuffer() it will explode, but if it came
  from MemoryStream.ToArray() it will work fine.</p>
</blockquote>

<p>It is explained in official <a href=""https://learn.microsoft.com/en-us/dotnet/api/system.io.memorystream.getbuffer?view=netframework-4.8#remarks"" rel=""nofollow noreferrer"">document</a>:</p>

<p>Note that the buffer contains allocated bytes which might be unused. For example, if the string ""test"" is written into the MemoryStream object, the length of the buffer returned from GetBuffer is 256, not 4, with 252 bytes unused. To obtain only the data in the buffer, use the ToArray method; however, ToArray creates a copy of the data in memory.</p>

<p>However,i suspect that this is the defect of ADF Postgresql connector and we could not adjust any transfer source code at the use level.(Surely,you could submit feedback to azure adf team to get official statement) </p>

<p>For now,as workaround, you could use csv file to be an intermediary. Transfer data from on-premise database to csv files in Azure Blob Storage. Then transfer data into destination postgresql database.</p>
"
"58434951","Azure Data factory Copy Pipeline failing if more than 6 columns","<p>I'm trying to copy data from a view in either on-premise PostgeSQL or MSSQL to a table in Azure PostgreSQL. I can't seem to get this to successfully copy when I map more than 6 columns from source to sink.</p>

<p>I suspected that one of the source columns was the issue, so varied what columns were mapped but all columns will succeed if less than 6 are copied in total.
I then tried different source and sink tables, which produces the same results.</p>

<p>If I copy to an intermediary csv file I can import/export unlimited columns successfully.</p>

<p>Error output shown in the ADF console consistently is:
""Operation on target Copy_7dp failed: Type=Npgsql.PostgresException,Message=08P01: invalid message format,Source=Npgsql,'""</p>
","<postgresql><azure><azure-data-factory><npgsql><azure-postgresql>","2019-10-17 14:26:37","2943","2","2","58558726","<p>Having had a discussion with the ADF team at Microsoft they enlightened me to the fact that Postgresql has a 16 bit limit to the number of elements that can be written at once. i.e. if (row size * column size) > 65536 it will raise this error.</p>

<p>The solution is to simply reduce the ""Write batch size"" in the Sink properties of the Copy Data activity <a href=""https://i.stack.imgur.com/xee5R.png"" rel=""nofollow noreferrer"">e.g this image</a></p>
"
"58432962","Reading data from lake","<p>I need to read data from azure data from azure data lake and apply some joins in sql and show in Web UI.
Data is around 300 gb and migrating data from azure data factory to azure sql database is happening at the speed of 4Mbps.
I have also tried to use sql server 2019 which has polybase support but that is also taking 12-13 hours to copy data.
Also tried cosmos db for storing data from lake but seems it is taking large amount of time.</p>

<p>Any other way we can read data from lake.
One way can be azure data warehouse,but that is too costly and support only 128 concurrent transactions.
Can databricks be used,but its a computation engine and we need it to be available 24*7 for UI Queries</p>
","<azure><azure-sql-database><azure-cosmosdb><azure-data-factory><azure-data-lake>","2019-10-17 12:43:16","144","0","2","58443059","<p>I still suggest you using Azure Data Factory. As you said, your data is around 300 gb.</p>

<p>Here's the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance#copy-performance-and-scalability-achievable-using-adf"" rel=""nofollow noreferrer"">Copy performance and scalability achievable using ADF</a>:</p>

<p><a href=""https://i.stack.imgur.com/SngnH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SngnH.png"" alt=""enter image description here""></a></p>

<p>I agree with David Makogon. The performance of your Data Factory is very slowly( 4Mbps). Please reference this document <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance"" rel=""nofollow noreferrer"">Copy activity performance and scalability guide</a>.</p>

<p>It will help you improve the Data Factory data copy performance, give more suggestions about Data Factory settings or Database settings.</p>

<p>Hope this helps.</p>
"
"58432962","Reading data from lake","<p>I need to read data from azure data from azure data lake and apply some joins in sql and show in Web UI.
Data is around 300 gb and migrating data from azure data factory to azure sql database is happening at the speed of 4Mbps.
I have also tried to use sql server 2019 which has polybase support but that is also taking 12-13 hours to copy data.
Also tried cosmos db for storing data from lake but seems it is taking large amount of time.</p>

<p>Any other way we can read data from lake.
One way can be azure data warehouse,but that is too costly and support only 128 concurrent transactions.
Can databricks be used,but its a computation engine and we need it to be available 24*7 for UI Queries</p>
","<azure><azure-sql-database><azure-cosmosdb><azure-data-factory><azure-data-lake>","2019-10-17 12:43:16","144","0","2","59359999","<p>I had a very similar situation, just more data +-900GB. 
If you need to show it in ui, you will still need to load data to Azure SQL, as DWH is not very good at handling parallel load and its costy.
We ended up using bulk insert from blob storage.
I created sp to call bulk insert with parameters (source file, target table) and ADF to orchestrate and run in parallel.
Could not find anything faster than that.
<a href=""https://learn.microsoft.com/en-us/sql/relational-databases/import-export/examples-of-bulk-access-to-data-in-azure-blob-storage?view=sql-server-ver15"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sql/relational-databases/import-export/examples-of-bulk-access-to-data-in-azure-blob-storage?view=sql-server-ver15</a></p>
"
"58432223","How to delete files based older than specified date in Azure Data lake","<p>I have data folders created on daily basis in datalake. Folder path is dynamic from JSON Format </p>

<p><a href=""https://i.stack.imgur.com/EvKE8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EvKE8.png"" alt=""JSON File with Parameters""></a></p>

<p><strong>Source Folder Structure</strong></p>

<p><code>SAPBW/Master/Text</code></p>

<p><strong>Destination Folder Structure</strong></p>

<pre><code>SAP_BW/Master/Text/2019/09/25
SAP_BW/Master/Text/2019/09/26
SAP_BW/Master/Text/2019/09/27
..
..
..
SAP_BW/Master/Text/2019/10/05
SAP_BW/Master/Text/2019/09/06
SAP_BW/Master/Text/2019/09/07
..
..
SAP_BW/Master/Text/2019/09/15
SAP_BW/Master/Text/2019/09/16
SAP_BW/Master/Text/2019/09/17

</code></pre>

<p>I want to delete the folders created before 5 days for each folder of sinkTableName</p>

<p>So, in DataFactory, i have  Called the folder path in a for each loop as</p>

<pre><code>@concat(item().DestinationPath,item().SinkTableName,'/',item().LoadTypeName,'/',formatDateTime(adddays(utcnow(),-5),item().LoadIntervalFormat),'/')""
</code></pre>

<p><a href=""https://i.stack.imgur.com/gPwKR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gPwKR.png"" alt=""Datafactory for each loop""></a></p>

<p>Need syntax to delete the files in each folder based on the JSON.</p>

<p>Unable to find the way to delete folder wise and setup the delete activity depending on the dates prior to five days from now</p>
","<azure><azure-data-factory>","2019-10-17 12:04:19","1405","0","1","58471222","<p>I see that you are doing a concatenation , which I think is the way to go . But I see that you are using the expression <code>formatDateTime(adddays(utcnow(),-5)</code> , which will give you something like 2019-10-15T08:23:18.9482579Z which i don't think is desired . I suggest to try with <code>@formatDateTime(adddays(utcnow(),-5) ,'yyyy/MM/dd')</code>. Let me know how it goes .</p>
"
"58420951","Getting Data from HubSpot REST api with Azure Data Factory - all contacts (Pagination)","<p>I'm trying to get ALL contacts from Hubspot, but hubspot has a limit of 100 contacts per call, so I need to set up pagination in Data Factory to get all 25,000 contacts</p>

<p>This is the GET url I used to get a portion of the contacts <code>https://api.hubapi.com/contacts/v1/lists/all/contacts/all?hapikey=[mykey]&amp;vidOffset=1</code></p>

<p>How do I set Data Factory up to retrieve all contacts?</p>
","<pagination><azure-data-factory>","2019-10-16 20:05:35","777","0","2","58479520","<p>I think as per the doc <a href=""https://developers.hubspot.com/docs/methods/contacts/get_contacts"" rel=""nofollow noreferrer"">https://developers.hubspot.com/docs/methods/contacts/get_contacts</a> 
they use the &amp;vidOffset=X to navigate to the next page . I think we can use that value  for pagination . The below article may help.
 <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-rest</a></p>
"
"58420951","Getting Data from HubSpot REST api with Azure Data Factory - all contacts (Pagination)","<p>I'm trying to get ALL contacts from Hubspot, but hubspot has a limit of 100 contacts per call, so I need to set up pagination in Data Factory to get all 25,000 contacts</p>

<p>This is the GET url I used to get a portion of the contacts <code>https://api.hubapi.com/contacts/v1/lists/all/contacts/all?hapikey=[mykey]&amp;vidOffset=1</code></p>

<p>How do I set Data Factory up to retrieve all contacts?</p>
","<pagination><azure-data-factory>","2019-10-16 20:05:35","777","0","2","59218038","<p>I have had a similar situation in Hubspot but mine was with companies and not in contacts. If you make a recursive function or put the function in a while loop while returning the vidOffset each time and use that value on the next function call, you should be good.  </p>

<p>In your case (azure data factory), try adding an ""Until"" activity. 
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-until-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-until-activity</a></p>

<p>Hopefully this helps out!</p>
"
"58417782","I get an error while reading multiple multiline JSON files in Databricks","<p>I get an  Py4JJavaError message when I'm reading multiple multiline JSON files from a folder. It seems like Spark is struggling while infering a schema from these files.  </p>

<p>I tried to reduce the number of files to read since it has to infer from thousands of JSON files but it doesn't seem to work.</p>

<pre class=""lang-py prettyprint-override""><code>def get_user_details_schema(url):
  df = sqlContext.read.json(url, multiLine=True)

  return df.schema
</code></pre>

<p>This is the message I get :</p>

<pre><code>Py4JJavaError                             Traceback (most recent call last)
&lt;command-2296498238051133&gt; in &lt;module&gt;()
     19 
     20 
---&gt; 21 main()

&lt;command-2296498238051133&gt; in main()
     15 
     16 
---&gt; 17   process_users(config.user_input_url, config.user_output_url)
     18 
     19 

&lt;command-2296498238051133&gt; in process_users(input_url, output_url)
      1 def process_users(input_url, output_url):
----&gt; 2   user_df = get_cleansed_users(input_url)
      3 
      4   if not user_df or user_df.rdd.isEmpty():
      5     print(""User input dataset does not exists or is empty. Nothing to do."")

&lt;command-2296498238051132&gt; in get_cleansed_users(input_url)
     16 
     17 def get_cleansed_users(input_url):
---&gt; 18   df = read_if_exists(input_url, get_user_details_schema(input_url))
     19 
     20   formater_date = udf(format_date)

&lt;command-2296498238051132&gt; in get_user_details_schema(url)
      1 def get_user_details_schema(url):
----&gt; 2   df = sqlContext.read.json(url, multiLine=True)
      3 
      4   return df.schema
      5 

/databricks/spark/python/pyspark/sql/readwriter.py in json(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding)
    272             path = [path]
    273         if type(path) == list:
--&gt; 274             return self._df(self._jreader.json(self._spark._sc._jvm.PythonUtils.toSeq(path)))
    275         elif isinstance(path, RDD):
    276             def func(iterator):

/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1255         answer = self.gateway_client.send_command(command)
   1256         return_value = get_return_value(
-&gt; 1257             answer, self.gateway_client, self.target_id, self.name)
   1258 
   1259         for temp_arg in temp_args:

/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
     61     def deco(*a, **kw):
     62         try:
---&gt; 63             return f(*a, **kw)
     64         except py4j.protocol.Py4JJavaError as e:
     65             s = e.java_exception.toString()

/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    326                 raise Py4JJavaError(
    327                     ""An error occurred while calling {0}{1}{2}.\n"".
--&gt; 328                     format(target_id, ""."", name), value)
    329             else:
    330                 raise Py4JError(

Py4JJavaError: An error occurred while calling o389.json.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 10.10.25.4, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2355)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2343)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2342)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2342)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1096)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1096)
    at scala.Option.foreach(Option.scala:257)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1096)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2574)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2510)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:893)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2243)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2341)
    at org.apache.spark.sql.catalyst.json.JsonInferSchema$.infer(JsonInferSchema.scala:83)
    at org.apache.spark.sql.execution.datasources.json.MultiLineJsonDataSource$$anonfun$infer$1.apply(JsonDataSource.scala:172)
    at org.apache.spark.sql.execution.datasources.json.MultiLineJsonDataSource$$anonfun$infer$1.apply(JsonDataSource.scala:172)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:240)
    at org.apache.spark.sql.execution.datasources.json.MultiLineJsonDataSource$.infer(JsonDataSource.scala:171)
    at org.apache.spark.sql.execution.datasources.json.JsonDataSource.inferSchema(JsonDataSource.scala:65)
    at org.apache.spark.sql.execution.datasources.json.JsonFileFormat.inferSchema(JsonFileFormat.scala:59)
    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:204)
    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:195)
    at scala.Option.orElse(Option.scala:289)
    at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:195)
    at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:412)
    at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:298)
    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:284)
    at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:467)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
    at py4j.Gateway.invoke(Gateway.java:295)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:251)
    at java.lang.Thread.run(Thread.java:748)
</code></pre>
","<apache-spark><pyspark><azure-data-factory><databricks>","2019-10-16 16:22:05","1177","0","1","58430639","<p>If your cluster is running Databricks Runtime 4.0 and above, you can read JSON files in single-line or multi-line mode. In single-line mode, a file can be split into many parts and read in parallel.</p>

<p><strong>Read JSON files in single-line:</strong></p>

<pre><code>val testJsonData = sqlContext.read.json(""/tmp/test.json"")
display(testJsonData)
</code></pre>

<p><a href=""https://i.stack.imgur.com/iuHZp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iuHZp.png"" alt=""enter image description here""></a></p>

<p><strong>Read JSON files in multi-line:</strong></p>

<pre><code>val testJsonData = sqlContext.read.option(""multiline"",""true"").json(""/tmp/test.json"")
display(testJsonData)
</code></pre>

<p><a href=""https://i.stack.imgur.com/vu5Xf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vu5Xf.png"" alt=""enter image description here""></a></p>

<p>Reference: <a href=""https://docs.databricks.com/data/data-sources/read-json.html"" rel=""nofollow noreferrer"">Azure Databricks - JSON Files</a></p>

<p>Hope this helps.</p>
"
"58417511","Python Azure Data Factory Update Pipeline","<p>I want to use Python to add an activity to a pipeline in Azure Data Factory. With the following code I am replacing the actual activity but not adding a new one:</p>

<pre class=""lang-py prettyprint-override""><code>p_name = 'test'
act_name = 'Wait4'

Wait_activity = WaitActivity(name=act_name,wait_time_in_seconds=5)


p_obj = PipelineResource(activities=[Wait_activity])
p = adf_client.pipelines.create_or_update(rg_name, df_name, p_name, p_obj)

</code></pre>

<p>This is the pipeline before running the code :
<a href=""https://i.stack.imgur.com/Mo2jg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Mo2jg.png"" alt=""enter image description here""></a></p>

<p>After running the code:</p>

<p><a href=""https://i.stack.imgur.com/L5Vwa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L5Vwa.png"" alt=""enter image description here""></a></p>

<p>Expected :</p>

<p><a href=""https://i.stack.imgur.com/sGUrm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sGUrm.png"" alt=""enter image description here""></a></p>
","<python><azure><azure-data-factory>","2019-10-16 16:06:30","797","1","1","58423873","<p>Researched the statements in source code:</p>

<p><a href=""https://i.stack.imgur.com/LKOeQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LKOeQ.png"" alt=""enter image description here""></a></p>

<p>So when you update the pipeline, the <code>activities</code> property should be the list of activities in pipeline, not single one.</p>

<p>For example:</p>

<pre><code>wait_activity = WaitActivity(name=""waittest"", type=""Wait"", wait_time_in_seconds=100, )
ActivityDependency = [{""activity"":""waittest"",""dependencyConditions"":[""Succeeded""]}]
wait_activity1 = WaitActivity(name=""waittest1"", type=""Wait"", wait_time_in_seconds=100,depends_on=ActivityDependency)


p_name = 'testforadf'
p_obj = PipelineResource(
        activities=[wait_activity, wait_activity1])
p = adf_client.pipelines.create_or_update(rg_name, df_name, p_name, p_obj)
</code></pre>

<p>Please note two lines:</p>

<pre><code>activities=[wait_activity, wait_activity1])
</code></pre>

<p>This property should contains all of your activities.</p>

<pre><code>ActivityDependency = [{""activity"":""waittest"",""dependencyConditions"":[""Succeeded""]}]
</code></pre>

<p>This is the dependency conditions between your activities.</p>

<p>My output:</p>

<p><a href=""https://i.stack.imgur.com/uTmgM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uTmgM.png"" alt=""enter image description here""></a></p>

<p>Any concern, please let me know.</p>

<hr>

<p>Well,please see my sample code: </p>

<p><strong>The premise is that I already have the above two wait activities</strong></p>

<pre><code>adftest = adf_client.pipelines.get(rg_name,df_name,p_name)
print(adftest)
for activity in adftest.activities :
    print(activity.name)
    print(activity.type)
</code></pre>

<p>Then output is :</p>

<pre><code>{'additional_properties': None, 'id': '/subscriptions/b83c1ed3-c5b6-44fb-b5ba-2b83a074c23f/resourceGroups/v-jugong-ChinaCXPTeam/providers/Microsoft.DataFactory/factories/jaygongadf/pipelines/testforadf', 'name': 'testforadf', 'type': 'Microsoft.DataFactory/factories/pipelines', 'etag': 'ed006cf3-0000-0800-0000-5da970600000', 'description': None, 'activities': [&lt;azure.mgmt.datafactory.models.wait_activity_py3.WaitActivity object at 0x000001C05FEDE0F0&gt;, &lt;azure.mgmt.datafactory.models.wait_activity_py3.WaitActivity object at 0x000001C05FED6DA0&gt;], 'parameters': None, 'variables': None, 'concurrency': None, 'annotations': None, 'folder': None}
waittest
Wait
waittest1
Wait
</code></pre>

<p>Then you could see the objects in above <code>activities</code> property. Besides,you could see their types:  <code>'activities': [&lt;azure.mgmt.datafactory.models.wait_activity_py3.WaitActivity object at 0x000001C05FEDE0F0&gt;, &lt;azure.mgmt.datafactory.models.wait_activity_py3.WaitActivity object at 0x000001C05FED6DA0&gt;]</code></p>

<p>They are WaitActivity type, so you could view their loop the activity to get every item in it using :</p>

<pre><code>for activity in adftest.activities :
        print(activity.name)
        print(activity.type)
</code></pre>

<p>You could view what properties the WaitActivity type contains, like<code>name</code>,<code>type</code> in source code statements.(For me, i used Pycharm to test code,the IDE could detect source code directly)</p>

<p><a href=""https://i.stack.imgur.com/ejXz4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ejXz4.png"" alt=""enter image description here""></a></p>

<p>Then if you want to add one more activity,for example, one more WaitActivity:</p>

<pre><code>wait_activity2 = WaitActivity(name=""waittest2"", type=""Wait"", wait_time_in_seconds=100, )
adftest.activities.append(wait_activity2)
p = adf_client.pipelines.create_or_update(rg_name, df_name, p_name, adftest)
</code></pre>

<p>Please see above code, i created a new WaitActivity named <code>wait_activity2</code> ,then append it into <code>activities</code> array. Then update the pipeline as normal, you will find the new activity :</p>

<p><a href=""https://i.stack.imgur.com/PrQ23.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PrQ23.png"" alt=""enter image description here""></a></p>
"
"58416258","Data Load from DB2 to Azure SQL DB","<p>I'm performing data load from on-Prem DB2 system to Azure SQL DB using ADF V2 through Self-hosted IR. SHIR is having direct connectivity/access to both source and target system.</p>

<p>For some of the table its failing with below error:</p>

<pre><code> Error Message:

 Failure happened on source 
 side.'Type=Microsoft.HostIntegration.DrdaClientException, Messgae=Non- 
 negative number required.\r\nParameter name: count SQLSTATE=HY0000 
  SQLCODE=-343, Source=Microsoft.HostIntegration.Connectors
</code></pre>

<p>And this error is happening at different record count. For e.g in the first it will fail at 100th record then in subsequent run it will fail at different row count. Like shown below:</p>

<p><a href=""https://i.stack.imgur.com/KnyuY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KnyuY.png"" alt=""enter image description here""></a></p>

<p>And since the error is happening at the source end i'm unable to capture the error log at destination. In my pipeline i have only copy activity, please help to find solution for this issue. </p>

<p>What are the things that i need to check/debug so that i can get the real cause for this issue.</p>
","<azure><db2><azure-data-factory><db2-400>","2019-10-16 14:55:50","1122","0","1","58479323","<p>If I understand this right you are using SHIR and it seems that the failures happens at different record number in subsequent runs ( even if nothing is fixed at the source ) .</p>

<p>I suggest you to look at the SHIR log and see if that has more info , the error ""343"" reminds me that I have seen this before on some other forum and I think the work around was to pull a small chunk of data , not sure if you can try that . </p>
"
"58409244","Tab Delimiter in Data Factory","<p>I am running into an issue when trying to parse the data from a config file in Data Factory. 
I am using a configuration file and the items are called in the copy activity. We have the option to parameterize the 'Column Delimiter' field from a data set, so I am using the value from the file (because in some cases is ';' and in others '\t'). 
When the delimiter is semicolon is working perfectly, but when it's \t , I get the following error :</p>

<blockquote>
  <p>Copy activity doesn't support multi-char or none column delimiter.</p>
</blockquote>

<p>When I'm checking the value that goes into the field, I see that the value is not the one from the file (<code>\t</code>), but <code>\\t</code>.
Do you have any idea why this behavior or if there is an escape character for this one. I also tried with ASCII code (<code>\0009</code>) and I get the same error - it doesn't know to transform it. Thanks a lot! </p>
","<azure><delimiter><azure-data-factory>","2019-10-16 08:45:02","2170","0","4","58409726","<p>Based on the statements in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-delimited-text#dataset-properties"" rel=""nofollow noreferrer"">official document</a>, Currently, multi-char delimiter is only supported for <code>mapping data flow</code> but not <code>Copy activity</code>.</p>

<p><a href=""https://i.stack.imgur.com/b0fJz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/b0fJz.png"" alt=""enter image description here""></a></p>

<p>You could try to use <code>mapping data flows</code> which is also designed data transformations in ADF. Please see more details here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-overview"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-overview</a></p>

<p>Any concern,please let me know.</p>
"
"58409244","Tab Delimiter in Data Factory","<p>I am running into an issue when trying to parse the data from a config file in Data Factory. 
I am using a configuration file and the items are called in the copy activity. We have the option to parameterize the 'Column Delimiter' field from a data set, so I am using the value from the file (because in some cases is ';' and in others '\t'). 
When the delimiter is semicolon is working perfectly, but when it's \t , I get the following error :</p>

<blockquote>
  <p>Copy activity doesn't support multi-char or none column delimiter.</p>
</blockquote>

<p>When I'm checking the value that goes into the field, I see that the value is not the one from the file (<code>\t</code>), but <code>\\t</code>.
Do you have any idea why this behavior or if there is an escape character for this one. I also tried with ASCII code (<code>\0009</code>) and I get the same error - it doesn't know to transform it. Thanks a lot! </p>
","<azure><delimiter><azure-data-factory>","2019-10-16 08:45:02","2170","0","4","58483588","<p>Can you try passing a real tab copied from a text editor, like - '  '.</p>

<p>This has been seen to work. 
Had there been no parameterization in the delimiter, you could have done it through the GUI or even the code.</p>
"
"58409244","Tab Delimiter in Data Factory","<p>I am running into an issue when trying to parse the data from a config file in Data Factory. 
I am using a configuration file and the items are called in the copy activity. We have the option to parameterize the 'Column Delimiter' field from a data set, so I am using the value from the file (because in some cases is ';' and in others '\t'). 
When the delimiter is semicolon is working perfectly, but when it's \t , I get the following error :</p>

<blockquote>
  <p>Copy activity doesn't support multi-char or none column delimiter.</p>
</blockquote>

<p>When I'm checking the value that goes into the field, I see that the value is not the one from the file (<code>\t</code>), but <code>\\t</code>.
Do you have any idea why this behavior or if there is an escape character for this one. I also tried with ASCII code (<code>\0009</code>) and I get the same error - it doesn't know to transform it. Thanks a lot! </p>
","<azure><delimiter><azure-data-factory>","2019-10-16 08:45:02","2170","0","4","60166268","<p>The short answer, is when entering a tab value in the UI, do not use <code>\t</code>, instead use <code>""   ""</code>.
Between the empty quotes, I pasted an actual tab character.</p>
"
"58409244","Tab Delimiter in Data Factory","<p>I am running into an issue when trying to parse the data from a config file in Data Factory. 
I am using a configuration file and the items are called in the copy activity. We have the option to parameterize the 'Column Delimiter' field from a data set, so I am using the value from the file (because in some cases is ';' and in others '\t'). 
When the delimiter is semicolon is working perfectly, but when it's \t , I get the following error :</p>

<blockquote>
  <p>Copy activity doesn't support multi-char or none column delimiter.</p>
</blockquote>

<p>When I'm checking the value that goes into the field, I see that the value is not the one from the file (<code>\t</code>), but <code>\\t</code>.
Do you have any idea why this behavior or if there is an escape character for this one. I also tried with ASCII code (<code>\0009</code>) and I get the same error - it doesn't know to transform it. Thanks a lot! </p>
","<azure><delimiter><azure-data-factory>","2019-10-16 08:45:02","2170","0","4","65324210","<p>You should use t instead of \t. Data Factory replaces t with \t itself. That is why \t ends up as \t</p>
"
"58399842","How to find the column causes the error on Azure Data Factory?","<blockquote>
  <p>{
      ""errorCode"": ""2200"",
      ""message"": ""Failure happened on 'Sink' side. 'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The
  given value of type String from the data source cannot be converted to
  type decimal of the specified target column.. Failed to convert
  parameter value from a String to a
  Decimal.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.InvalidOperationException,Message=The
  given value of type String from the data source cannot be converted to
  type decimal of the specified target
  column.,Source=System.Data,''Type=System.FormatException,Message=Failed
  to convert parameter value from a String to a
  Decimal.,Source=System.Data,''Type=System.FormatException,Message=Input
  string was not in a correct format.,Source=mscorlib,'"",
      ""failureType"": ""UserError"",
      ""target"": ""Copy Data1"" }</p>
</blockquote>

<p>Can anyone please let me know the main cause of this error? I am trying to migrate the data from Web API to the Azure SQL database.
I need to know what specific column causes the error.</p>
","<sql><database><azure><azure-data-factory>","2019-10-15 17:28:35","3648","-1","1","58404645","<p>The error message is very clear: </p>

<blockquote>
  <p>column.,Source=System.Data,''Type=System.FormatException,Message=Failed to convert parameter value from a String to a Decimal.,Source=System.Data,''Type=System.FormatException,Message=Input string was not in a correct format.,Source=mscorlib,'"", ""failureType"": ""UserError"", ""target"": ""Copy Data1"" }</p>
</blockquote>

<p>This error is caused by your input dataset.</p>

<p>Please check your source dataset from Web API, during the Copy Active Mapping settings, find which column(String) is mapping to decimal column. Then you can find the error data from your source dataset.</p>

<p><a href=""https://i.stack.imgur.com/DRDDP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DRDDP.png"" alt=""enter image description here""></a></p>

<p>The data from your source dataset is not a 'decimal' format String which caused the error.</p>

<p>Hope this helps.</p>
"
"58377788","Blob to azure sql db load","<p>blob contain Hive Partition table data partition created on Year, month and day.
Container look like Year=2016/ Months=1/Day-1 - 0000_1(File) to Day-31 - 0000_31(File) 
Like this we have 3 months inside year and each month contain days folder and ear day folder contain a file.</p>

<p>Now we want o put that data into a azure sql Db table which is not partitioned.</p>
","<azure-data-factory>","2019-10-14 13:24:12","20","0","1","58533121","<p>If I understand it right , you have blobs with the structure 
2016/03/01_001
2016/03/01_003 
and the intend is to copy the data to SQL Azure . I am assuming that the blob structure is the same on all the files . 
I suggest </p>

<p>1:Use the GetMetadata activity and get all the blob info </p>

<p>2:Use a foreach activity to read one blob at a time </p>

<p>3:Inside the foreach add a copy activity source being blob and sink SQL Azure .</p>
"
"58377124","I have about 20 files of type excel /pdf which can be dowloaded from an Http Server.I need to load this file into Azure Storage using Data Factory","<p>I have 20 files of type Excel/pdf  located in different https server. i need to validate these file and load into azure storage Using Data Factory.I need to do apply some  business logic on this data and load into azure SQL Database.I need to if we have to create a pipe line and store this data in azure blob storage and then load into Azure sql Database</p>

<p>I have tried creating copy data in data factory</p>
","<azure><azure-data-factory>","2019-10-14 12:44:29","967","1","1","58481529","<p>My idea as below:</p>

<p><strong>No.1</strong> </p>

<p>Step 1: Use Copy Activity to transfer data from http connector source into blob storage connector sink.</p>

<p>Step 2: Meanwhile, configure a blob storage trigger to execute your logic code so that the blob data will be processed as soon as it's collected into blob storage.</p>

<p>Step 3: Use Copy Activity to transfer data from blob storage connector source into SQL database connector sink.</p>

<p><strong>No.2:</strong></p>

<p>Step 1:Use Copy Activity to transfer data from http connector source into SQL database connector sink.</p>

<p>Step 2: Meanwhile, you could configure <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure"" rel=""nofollow noreferrer"">stored procedure</a> to add your logic steps. The data will be executed before inserted into table.</p>

<p>I think both methods are feasible. The No.1, the business logic is freer and more flexible. The No.2, it is more convenient, but it is limited by the syntax of stored procedures. You could pick the solution as you want.</p>

<hr>

<p>The excel and pdf are supported yet. Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#binary-format"" rel=""nofollow noreferrer"">link</a>,only below formats are supported by ADF diectly:</p>

<p><a href=""https://i.stack.imgur.com/P2U2U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P2U2U.png"" alt=""enter image description here""></a></p>

<p>i tested for csv file and get the below random characters:</p>

<p><a href=""https://i.stack.imgur.com/6X5XL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6X5XL.png"" alt=""enter image description here""></a></p>

<p>You could refer to this case to read excel files in ADF:<a href=""https://stackoverflow.com/questions/52514153/how-to-read-files-with-xlsx-and-xls-extension-in-azure-data-factory"">How to read files with .xlsx and .xls extension in Azure data factory?</a></p>
"
"58374715","How to filter data on last successful trigger run in ADF(Azure Data Factory) V2 in copy activity ( NoSQL to SQL )?","<p>I have a ADF pipenine having only copy activity from Cosmos Mongo DB and copying to SQL Server.
Currently I am truncating the table every time the pipeline is run then copy all the data.
But now I want to add a filtering on one field say(lastModifiedTime) and comparing with the last successful trigger run.</p>

<p>Can anyone suggest me the expression which can be used here to put as a filter in the source for incremental updates?</p>

<p>I totally want to avoid truncating tables in the precopy script.</p>
","<azure><azure-data-factory>","2019-10-14 10:20:39","1130","0","1","58480571","<p>Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-overview"" rel=""nofollow noreferrer"">Incrementally copy data</a> in ADF, MongoDB connector is not supported so far. My workaround is adding a outside static variable to store every end time of pipeline.</p>

<p><strong><em>Step1-</em></strong> Add an Azure function activity after the copy activity. If the copy activity executes successfully, pass the end of execution time into azure function as parameters. Such as <code>utcnow()</code>: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#date-functions"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#date-functions</a></p>

<p><strong><em>Step2-</em></strong> Inside Azure function method, store the <code>time</code> into some place which could be accessed later, such as Blob Storage.</p>

<p><strong><em>Step3-</em></strong> Next trigger run,get the last execute time with <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">Look Up activity</a> from blob storage before the copy activity and set it into <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-set-variable-activity"" rel=""nofollow noreferrer"">set variable activity</a> so that you could use it in the next step.</p>

<p><strong><em>Step4-</em></strong> Add <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-mongodb#copy-activity-properties"" rel=""nofollow noreferrer"">filter condition</a> in the MongoDB source dataset with the value of set variable activity. </p>
"
"58365201","Is it possible to subtract x number of days from todays date in Azure Data Factory","<p>I have the GetMetaData activity that returns a 'lastmodifieddate'. Is there anyway in Azure DF to take the current date (today) and then subtract 3 days from today to compare against the 'lastmodifidedate'?</p>
","<azure><azure-data-factory>","2019-10-13 15:47:36","3248","0","1","58365759","<blockquote>
  <p>Is there anyway in Azure DF to take the current date (today) and then
  subtract 3 days from today to compare against the 'lastmodifidedate'?</p>
</blockquote>

<pre><code> @equals(formatDateTime(addDays(utcnow(),-3),'yyyy-MM-dd'),formatDateTime(activity('Your Metadata Activity Name').output.lastmodifieddate,'yyyy-MM-dd'))
</code></pre>

<p>In order to compare the dates or any value, you have to make use of Logical Function. Here the <strong>equals</strong> method compares two values and return boolean value. Please find more details about <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#logical-functions"" rel=""nofollow noreferrer"">logical function on MSDN</a></p>

<p><strong>formatDateTime</strong> function helps us to modify the format of any datetime. Since you need to compare the date part we have made use of 'yyyy-MM-dd' format.</p>

<p><strong>addDays</strong> method helps us in adding any number of days to the datetime.</p>

<p><strong>utcNow()</strong> method returns the current UTC date time value.</p>

<p>Please find complete details about functions <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">here</a>.</p>
"
"58357714","How can I filter my source dataset to copy only specific vaules to my sink?","<p>I have a csv file with 2 columns (id and name). The csv file has over 1 million names. I'm struggling to workout how I can filter my results to only copy data where column 2 has the name 'mary' in it.</p>

<p>Can anyone advise?</p>
","<azure-data-factory>","2019-10-12 19:27:06","383","0","1","58358983","<p>Add a Data Flow activity to your ADF pipeline. In that pipeline, point the Source to your CSV dataset. Next, add a Filter transformation and write an expression such as name == 'mary'. Next, add a Sink. This will copy only rows that have 'Mary' for the value in the name column.</p>
"
"58357198","08p01: invalid message format","<p>In Azure ADF v2 pipeline, i'm trying to copy data from sql server to Azure postgresql database. My source is sql server and my sink is Azure postgresql database. I'm moving all records from my source table to destination table. However, on pipeline execution getting error as</p>

<blockquote>
  <p>""errorcode"": ""2200"", ""message"": ""'type=npgsql.postgresexception,message=08p01: invalid message format'"", ""failuretype"": ""usererror.</p>
</blockquote>

<p>What is the issue?</p>
","<postgresql><azure-pipelines><npgsql><azure-data-factory>","2019-10-12 18:26:08","10864","4","2","58448372","<blockquote>
  <p>""errorcode"": ""2200"", ""message"":
  ""'type=npgsql.postgresexception,message=08p01: invalid message
  format'"", ""failuretype"": ""usererror.</p>
</blockquote>

<p>You may find some clues in this <a href=""https://github.com/npgsql/npgsql/issues/189"" rel=""nofollow noreferrer"">old case</a> which is similar to your error trace. Please see the solution in above link: <a href=""https://github.com/npgsql/npgsql/issues/189#issuecomment-67001166"" rel=""nofollow noreferrer"">https://github.com/npgsql/npgsql/issues/189#issuecomment-67001166</a></p>

<blockquote>
  <p>Basically, if the string value we try to persist using Npgsql was
  derived from MemoryStream.GetBuffer() it will explode, but if it came
  from MemoryStream.ToArray() it will work fine.</p>
</blockquote>

<p>It is explained in official <a href=""https://learn.microsoft.com/en-us/dotnet/api/system.io.memorystream.getbuffer?view=netframework-4.8#remarks"" rel=""nofollow noreferrer"">document</a>:</p>

<p>Note that the buffer contains allocated bytes which might be unused. For example, if the string ""test"" is written into the MemoryStream object, the length of the buffer returned from GetBuffer is 256, not 4, with 252 bytes unused. To obtain only the data in the buffer, use the ToArray method; however, ToArray creates a copy of the data in memory.</p>

<p>For now,as workaround, you could use csv file to be an intermediary. Transfer data from sql server to csv files in Azure Blob Storage. Then transfer data into destination postgresql database.</p>
"
"58357198","08p01: invalid message format","<p>In Azure ADF v2 pipeline, i'm trying to copy data from sql server to Azure postgresql database. My source is sql server and my sink is Azure postgresql database. I'm moving all records from my source table to destination table. However, on pipeline execution getting error as</p>

<blockquote>
  <p>""errorcode"": ""2200"", ""message"": ""'type=npgsql.postgresexception,message=08p01: invalid message format'"", ""failuretype"": ""usererror.</p>
</blockquote>

<p>What is the issue?</p>
","<postgresql><azure-pipelines><npgsql><azure-data-factory>","2019-10-12 18:26:08","10864","4","2","58563994","<p>I encountered this issue myself. You need to change the ""Write batch size"" from 1000 to a smaller value (e.g. 1000) as per my answer here.
<a href=""https://stackoverflow.com/a/58558726/10393844"">https://stackoverflow.com/a/58558726/10393844</a></p>
"
"58347379","Get the processing time of cube in Azure Analysis Services","<p>Quick question - How do we capture the process runtime of azure analysis cube?  it gets triggered using web activity in Azure data factory. </p>

<p>Please advise!</p>
","<azure><azure-data-factory><azure-analysis-services>","2019-10-11 19:26:18","143","0","1","58626158","<p>Please refer to this <a href=""https://github.com/furmangg/automating-azure-analysis-services/blob/master/README.md#processazureas"" rel=""nofollow noreferrer"">sample</a> which starts an Azure AS model processing and then loops until the most recent processing batch is complete. </p>
"
"58328872","Variables in HiveQL","<p>Background: </p>

<p>As a part of the Modernization effort, I am trying to convert a big Stored Procedure into HiveQL script. HiveQL script, as a part of hive activity, runs on Azure HDInsight cluster whenever a Pipeline is triggered from Azure Data Factory.</p>

<p>The stored procedure that I am trying to transform has a lot of variables declared using 'DECLARE' statements. For eg:</p>

<pre><code>DECLARE @Variable1 INT;
</code></pre>

<p>Values in these variables are Set using SELECT statements. For eg:</p>

<pre><code>SELECT  @Variable1 = ColumnName1 FROM Table_Name;
</code></pre>

<p>and these variables are referenced throughout the stored procedure like this:</p>

<pre><code>SELECT * FROM Some_Table where ColumeName &lt; @Variable1
</code></pre>

<p>and many complex scenarios where subquerying is not possible. </p>

<p>How can I do the same in HiveQL? Is there any way to declare, modify and use variables in HiveQL script?</p>
","<variables><hive><hiveql><azure-data-factory><azure-hdinsight>","2019-10-10 18:15:52","197","1","1","58346228","<p><code>HiveQL</code> is not procedure language unfortunately. </p>

<p>You cannot use variables like this. </p>

<p>Variables in <code>HiveQL</code> are just text replacement, they are not calculated and being substituted as is literally. </p>

<p>Use some shell script or python, etc or try <a href=""http://hplsql.org/doc"" rel=""nofollow noreferrer"">HPL/SQL</a> </p>

<p>Related answer: <a href=""https://stackoverflow.com/a/37821218/2700344"">https://stackoverflow.com/a/37821218/2700344</a> and this one: <a href=""https://stackoverflow.com/a/51492264/2700344"">https://stackoverflow.com/a/51492264/2700344</a> - read for better understanding how variable substitution works in hiveql</p>
"
"58325867","Azure Data Factory Linked Service error- Failed to get the secret from key vault","<p>I am creating a linked service to a remote server in Azure Data Factory v2. The remote server uses username-password authentication mechanism. I have already created a linked service to the same server using username and password both in the linked service creation window and its working fine. I would like to store the password as a secret in Azure Key vault and access that secret from Azure Data Factory.</p>

<p>I have the secret in Azure Key vault and i have granted the access permission to Azure Data Factory to access Azure Key Vault by adding the Access policy in Key vault. I created linked service to azure key vault and it shows 'connection successful' when i tested the connection. However, when i try to create the linked service to a remote server using Azure key vault credentials instead of password, the connection failed. The error looks like this:</p>

<p>Failed to get the secret from key vault, secretName: *********, secretVersion: , vaultBaseUrl:*****************. The error message is: An error occurred while sending the request. The underlying connection was closed: Could not establish trust relationship for the SSL/TLS secure channel. The remote certificate is invalid according to the validation procedure.</p>

<p>I couldn't proceed further. Any help/suggestions would be appreciated. </p>
","<azure><azure-keyvault><azure-data-factory>","2019-10-10 14:59:23","3469","0","1","60460668","<p>Please follow the steps that is linked in the doc to create a managed identity for your azure data factory? <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-factory-service-identity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-factory-service-identity</a></p>

<p>Below powershell script can be used to associate the MSI. Using the new Azure Powershell Az Module : </p>

<p><a href=""https://learn.microsoft.com/en-us/powershell/azure/new-azureps-module-az?view=azps-2.8.0"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/powershell/azure/new-azureps-module-az?view=azps-2.8.0</a></p>

<pre><code>PS C:\WINDOWS\system32&gt; Set-AzDataFactoryV2 -ResourceGroupName &lt;resourceGroupName&gt; -Name &lt;dataFactoryName&gt; -Location &lt;region&gt;

DataFactoryName   : ADFV2DemoFactory
DataFactoryId     : /subscriptions/&lt;subsID&gt;/resourceGroups/&lt;resourceGroupName&gt;/providers/Microsoft.DataFactory/factories/ADFV2DemoFactory
ResourceGroupName : &lt;resourceGroupName&gt;
Location          : East US
Tags              : {}
Identity          : Microsoft.Azure.Management.DataFactory.Models.FactoryIdentity
ProvisioningState : Succeeded
</code></pre>
"
"58315848","How to open an ORC File in Azure data lake?","<p>I have an ORC File in azure data lake and i am unable to preview data.when i try to view it's just showing some irrelevant junk data.Can anyone help me which tool or how to preview contents of an ORC File</p>
","<azure><azure-data-factory><azure-data-lake>","2019-10-10 05:15:21","273","0","1","58317061","<p>According my experience, Azure Data Factory doesn't support preview the data of some specific format file, such as .xlsx file. ORC file also is not support data preview.</p>

<p><a href=""https://i.stack.imgur.com/DKhDV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DKhDV.png"" alt=""enter image description here""></a></p>

<p>But Azure Data Factory support copy ORC format file from Azure Data lake. To see: </p>

<ol>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#orc-format"" rel=""nofollow noreferrer"">Supported file formats and compression codecs in Azure Data
Factory</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage#other-format-dataset"" rel=""nofollow noreferrer"">Copy data to or from Azure Data Lake Storage Gen2 using Azure Data
Factory: Other format dataset</a>.</li>
</ol>

<p>Hope this helps.</p>
"
"58313896","Unable to Successfully Call Logic App From ADF Copy Activity","<p>I have a V.2 ADF Pipeline, within which I have one copy activity which calls an Logic App. The Logic App returns JSON via an HTTP response.</p>

<p>When debugging the copy activity I am getting the error message:</p>

<blockquote>
  <p>{ ""errorCode"": ""2200"", ""message"":
  ""ErrorCode=UserErrorFailedToReadHttpFile,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The
  remote server returned an error: (411) Length
  Required.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Net.WebException,Message=The
  remote server returned an error: (411) Length
  Required.,Source=System,'"", ""failureType"": ""UserError"", ""target"":
  ""Copy data1"" }</p>
</blockquote>

<p>Currently, in the ""Additional Headers"" box in the ADF Copy Activity settings I have the following:</p>

<blockquote>
  <p>Content-Type: application/json</p>
  
  <p>Content-Length: 0</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/Wa36z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Wa36z.png"" alt=""enter image description here""></a></p>

<p>I have tried every variation of entering the two above parameters but the error remains.</p>

<p>I can successfully call the logic app using postman, passing only the Content-Type.</p>

<p>How can I get past this error?</p>
","<azure-data-factory><azure-logic-apps>","2019-10-10 00:31:29","275","1","1","58318785","<p>I exactly reproduce your issue:</p>

<p><a href=""https://i.stack.imgur.com/JTZqS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JTZqS.png"" alt=""enter image description here""></a></p>

<p>The method is POST but you are sending nothing. You could write some fake data in the request body and it will be fine.</p>

<p><a href=""https://i.stack.imgur.com/tNM9Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tNM9Q.png"" alt=""enter image description here""></a></p>

<p>Success snapshot:</p>

<p><a href=""https://i.stack.imgur.com/vRlTu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vRlTu.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/yaYAD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yaYAD.png"" alt=""enter image description here""></a></p>
"
"58313119","No option to Detect or change Data Type/schema in Copy Activity","<p>I'm attempting to get data from a .json blob, detect column data types &amp; save to a table, Cosmosdb or another location. The json columns have no type, and are all imported as strings.</p>

<p>I don't see/have the ""detect data type"" button on the copy activity, the data flow, or the source data type. Additionally, I don't have the ability to manually change column types.</p>

<p>I'm uncertain if I took an action to trigger a restriction etc.</p>
","<azure-data-factory>","2019-10-09 22:32:11","1586","0","1","58318146","<blockquote>
  <p>Q:I don't see/have the ""detect data type"" button on the copy activity,
  the data flow, or the source data type. Additionally, I don't have the
  ability to manually change column types.</p>
</blockquote>

<p>Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#data-type-mapping"" rel=""nofollow noreferrer"">document statements</a>,ADF supports convert data type from source to sink:</p>

<blockquote>
  <p>Copy activity performs source types to sink types mapping with the
  following 2-step approach:</p>
  
  <p>1.Convert from native source types to Azure Data Factory interim data types
  2.Convert from Azure Data Factory interim data types to native sink type</p>
</blockquote>

<p>You could use Import Schemas in ADF UI to set your mapping columns:</p>

<p><a href=""https://i.stack.imgur.com/c3wWA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c3wWA.png"" alt=""enter image description here""></a> </p>
"
"58305285","Pass parameters in Copy activity for input file in Azure data factory","<p>I need to copy data from SFTP folder and need to dynamically pick only the current date minus 1 day file. I need to load this data to ADLS Gen -1. I'm using Copy activity and have parameterised the File path and File name in Dataset and pass these values from Pipeline.</p>

<p>File path - xyz/test
File name - @concat('auto-miles-per-gallon',formatDateTime(addDays(utcnow(),-1),'yyyyMMdd'),'.csv').</p>

<p>I'm able to run the copy activity but its not copying only the specified file (auto-miles-per-gallon20191008.csv). It copies all the files available in the SFTP folder. </p>

<p>Files available in the folder
auto-miles-per-gallon20191008.csv
auto-miles-per-gallon20191009.csv</p>

<p>Attached the parameters screenshot from Dataset,Pipeline.<a href=""https://i.stack.imgur.com/EaLzT.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EaLzT.jpg"" alt=""Dataset parameter[![][1]""></a>]<a href=""https://i.stack.imgur.com/1g5CJ.png"" rel=""nofollow noreferrer"">2</a></p>
","<azure><azure-data-factory>","2019-10-09 13:27:52","2220","1","1","58319157","<p>I checked <code>File name - @concat('auto-miles-per-gallon',formatDateTime(addDays(utcnow(),-1),'yyyyMMdd'),'.csv')</code>,it is right. I notice that the statements in the SFTP dataset <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sftp#dataset-properties"" rel=""nofollow noreferrer"">document</a>:</p>

<p><a href=""https://i.stack.imgur.com/3aGEK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3aGEK.png"" alt=""enter image description here""></a></p>

<p>I suggest you configuring the dynamic content for filePath and fileName in the copy activity and try again.</p>

<p><a href=""https://i.stack.imgur.com/8oF9i.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8oF9i.png"" alt=""enter image description here""></a></p>
"
"58304085","ARM Template deployment does not remove old pipelines","<p>I'm deploying my data factory using ARM Templates. When I delete a pipeline, the old version is not deleted from the target environment. The same happens with renaming, my target environment then contains a pipeline with the old and AND a pipeline with the new name.</p>

<p>How do I delete old pipelines with ARM Template deployments?</p>

<p>Note: I only want to delete the orphaned pipelines inside the data factory, not wipe out the entire data factory.</p>
","<azure><azure-data-factory><azure-rm-template>","2019-10-09 12:24:42","2966","2","3","58320675","<p>In order to delete pipelines inside the data factory, you need to run the script published by Microsoft <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#sample-prepostdeployment-script"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Note that this requires the use of the new Azure ""Az"" powershell modules, if you are using Azure DevOps requires the use of the preview V4 powershell tasks.</p>
"
"58304085","ARM Template deployment does not remove old pipelines","<p>I'm deploying my data factory using ARM Templates. When I delete a pipeline, the old version is not deleted from the target environment. The same happens with renaming, my target environment then contains a pipeline with the old and AND a pipeline with the new name.</p>

<p>How do I delete old pipelines with ARM Template deployments?</p>

<p>Note: I only want to delete the orphaned pipelines inside the data factory, not wipe out the entire data factory.</p>
","<azure><azure-data-factory><azure-rm-template>","2019-10-09 12:24:42","2966","2","3","60890835","<p>I ran into the same issue and followed Neil P's suggestion and looked at creating a post deployment script to remove the items. I was already using CI based on the arm template, so this extended my original CI. </p>

<p>As of this writing, Microsoft's power-shell script does work correctly for ADF V2. </p>

<p>Honestly, the approach right now is a little convoluted. I had to set up two branch artifacts. One looks at adf_publish and one that looks at master. adf_publish triggers the release pipeline, master handles the powershell script.</p>

<p><a href=""https://i.stack.imgur.com/48w2W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/48w2W.png"" alt=""enter image description here""></a></p>

<p>Then, I created a deployment powershell script in my repo on the master branch
<a href=""https://i.stack.imgur.com/PGdLR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PGdLR.png"" alt=""enter image description here""></a>.</p>

<p>After that, I updated the release pipeline to include the powershell script after the resource was updated. Note that I had to reference the ARMTemplateForFactory.json directly to get the script to work.</p>

<p><a href=""https://i.stack.imgur.com/yc5FQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yc5FQ.png"" alt=""enter image description here""></a></p>

<p>Everything worked as expected and the downstream QA release has updated properly and removed obsolete pipelines, etc.</p>
"
"58304085","ARM Template deployment does not remove old pipelines","<p>I'm deploying my data factory using ARM Templates. When I delete a pipeline, the old version is not deleted from the target environment. The same happens with renaming, my target environment then contains a pipeline with the old and AND a pipeline with the new name.</p>

<p>How do I delete old pipelines with ARM Template deployments?</p>

<p>Note: I only want to delete the orphaned pipelines inside the data factory, not wipe out the entire data factory.</p>
","<azure><azure-data-factory><azure-rm-template>","2019-10-09 12:24:42","2966","2","3","63143331","<p>I had the same issue and resolved it by leveraging an extension in the marketplace called <a href=""https://github.com/liprec/vsts-publish-adf"" rel=""nofollow noreferrer"">Azure Data Factory</a>. Within it are various tasks, one of which is called 'Azure Data Factory Delete Items'. I use that before executing the ARM template task from Microsoft and ultimetly I am left with a data factory pipeline that does not contain orphaned pipelines/datasets/linked_services/etc.
<a href=""https://i.stack.imgur.com/yiInW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yiInW.png"" alt=""Pipeline Tasks"" /></a></p>
"
"58299711","How to use the output of GetMetaData activity in copy activity","<p>I need to copy file from a folder to Azure BLOB Storage using copy activity and this copy activity should run only when the file exists in the specific folder. Im using GETMETADATA Activity to see if the file exists, but where i need to configure this output so that copy activity gets triggered when the file exists. I will use @activity(GetMetadata1).exists. But where i need to check this value is true in Copy activity. </p>
","<azure><azure-data-factory>","2019-10-09 08:10:43","898","1","1","58300430","<p>Configure <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity#sample-output"" rel=""nofollow noreferrer"">dynamic content</a>:<code>@{activity('MyGetMetadataActivity').output.exists}</code> in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-if-condition-activity"" rel=""nofollow noreferrer"">If-Condition Activity</a>.</p>

<p><a href=""https://i.stack.imgur.com/2N4UW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2N4UW.png"" alt=""enter image description here""></a></p>

<p>Output:</p>

<p><a href=""https://i.stack.imgur.com/2qDar.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2qDar.png"" alt=""enter image description here""></a></p>
"
"58297819","Execute Script after two separate ADFv2 Pipelines have completed","<p>I have two ADFv2 Pipelines that import data into two seperate srl tables within an Azure SQL database. Once both the pipelines have completed I would need to execute a script.</p>

<p>The source .csv files that initiates the execution of each individual pipeline will be created on a daily basis, but I can only execute the script when both Pipelines have completed... </p>

<p>each seperate pipeline is triggered via a Logic App by the creation of a seperate .csv file</p>

<p>I can use Logic Apps as well, but at the moment I can't find the best process to implement this.</p>

<p>Any help greatly appreciated.</p>
","<sql-server><azure><azure-data-factory>","2019-10-09 05:54:25","74","1","1","58298706","<p>2 situation:</p>

<p>1.If you don't mind the pipeline linear execution,you could use Execute Pipeline Activity. Execute function until the first two Execute Pipeline Activity executes successfully, like this process:</p>

<p><a href=""https://i.stack.imgur.com/6C4NF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6C4NF.png"" alt=""enter image description here""></a></p>

<p>2.If not, my idea is using queue trigger. After pipeline execution, send a message to <a href=""https://learn.microsoft.com/en-us/azure/storage/queues/storage-dotnet-how-to-use-queues"" rel=""nofollow noreferrer"">azure queue storage</a> by for example Web Activity(<a href=""https://learn.microsoft.com/en-us/rest/api/storageservices/Queue-Service-REST-API?redirectedfrom=MSDN"" rel=""nofollow noreferrer"">REST API</a>). Configure a function queue trigger, judge if it receive 2 successful messages,then do some jobs.</p>

<p>Of course, you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-programmatically"" rel=""nofollow noreferrer"">ADF monitor SDKs</a> to de-polling to check the execution status and results of two pipelines and do the next jobs. You could pick a suitable solution.</p>

<p>Besides, you could get an idea of Logic App as you mentioned in the answer.It supports run after for 2 connectors. Both of them are successful, then do the next job.</p>

<p><a href=""https://i.stack.imgur.com/fRaCs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fRaCs.png"" alt=""enter image description here""></a></p>
"
"58292941","Cant Drag and Drop Activities for Pipeline","<p>Hi I am new to Azure Data Factory, I was hoping to play around and create a pipeline to see how it works. I have a free trial setup with all linked services and connections. I am having trouble as I try to drag and drop any activities, I cant seem to to drag any activities. please let me know</p>
","<azure-data-factory>","2019-10-08 19:34:05","211","0","1","58390580","<blockquote>
  <p>Question: I am having trouble as I try to drag and drop any
  activities, I cant seem to to  drag any activities.</p>
</blockquote>

<p>Just for summary, actually it may be caused by the system or browser cache. The solution is clearing the cache and reboot system.</p>
"
"58279997","Enumerate all files in a container and copy each file in a foreach","<p>Need to load all .csv files in Azure Blob Container into SQL database.</p>

<p>Tried using a wild card <code>*.*</code> on the filename in the dataset which uses the linked service that connects to the blob and outputting the itemName in the Get Meta Data activity.</p>

<p>When executing in debug a list of filenames is not returned in the Output window. When referencing the parameter with an expression it is stated that the type is String not collection.</p>
","<azure-blob-storage><azure-data-factory>","2019-10-08 03:55:22","2446","1","3","58281067","<p>For you needs, you could get an idea of <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">LookUp Activity</a>.</p>

<blockquote>
  <p>Lookup activity can retrieve a dataset from any of the Azure Data
  Factory-supported data sources. Use it in the following scenario:</p>
  
  <p>Dynamically determine which objects to operate on in a subsequent
  activity, instead of hard coding the object name. Some object examples
  are files and tables.</p>
</blockquote>

<p>For example, my container has 2  csv files in <code>test</code> container:</p>

<p><a href=""https://i.stack.imgur.com/emjy4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/emjy4.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/bvphV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bvphV.png"" alt=""enter image description here""></a></p>

<p>Configure a blob storage dataset :</p>

<p><a href=""https://i.stack.imgur.com/1NTMI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1NTMI.png"" alt=""enter image description here""></a></p>

<p>Configure Lookup Activity:</p>

<p><a href=""https://i.stack.imgur.com/c7jrL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c7jrL.png"" alt=""enter image description here""></a></p>

<p>Output:</p>

<p><a href=""https://i.stack.imgur.com/xNbAl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xNbAl.png"" alt=""enter image description here""></a></p>
"
"58279997","Enumerate all files in a container and copy each file in a foreach","<p>Need to load all .csv files in Azure Blob Container into SQL database.</p>

<p>Tried using a wild card <code>*.*</code> on the filename in the dataset which uses the linked service that connects to the blob and outputting the itemName in the Get Meta Data activity.</p>

<p>When executing in debug a list of filenames is not returned in the Output window. When referencing the parameter with an expression it is stated that the type is String not collection.</p>
","<azure-blob-storage><azure-data-factory>","2019-10-08 03:55:22","2446","1","3","58288769","<p>For this kind of task, I use the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">Get Metadata</a> activity and process the results with a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">For Each</a> activity. Inside the For Each activity, you can have a simple Copy activity to copy the CSV files to SQL tables, or if the work is more complex you can use Data Flow.</p>

<p>Some useful tips:</p>

<ol>
<li>In the Get Metadata activity, under the DataSet tab > ""Field list"", select the ""Child Items"" option:</li>
</ol>

<p><a href=""https://i.stack.imgur.com/4Tmxz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4Tmxz.png"" alt=""enter image description here""></a></p>

<ol start=""2"">
<li>I recommend adding a ""Filter"" activity after the Get Metadata to ensure that you are only processing files, and optionally even expected extensions. You do this in the Settings tab like so:</li>
</ol>

<p><a href=""https://i.stack.imgur.com/HRekB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HRekB.png"" alt=""enter image description here""></a></p>

<ol start=""3"">
<li>In the For Each activity, on the Settings tab, set the Items based on the output of the Filter activity:</li>
</ol>

<p><a href=""https://i.stack.imgur.com/M7GGB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M7GGB.png"" alt=""enter image description here""></a></p>

<ol start=""4"">
<li>Inside the For Each, at the activity level, you reference the instance by ""@item().name"":</li>
</ol>

<p><a href=""https://i.stack.imgur.com/fsgn7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fsgn7.png"" alt=""enter image description here""></a></p>

<p>Here's what one of my production pipelines that implements this pattern looks like:</p>

<p><a href=""https://i.stack.imgur.com/6J0jq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6J0jq.png"" alt=""enter image description here""></a></p>
"
"58279997","Enumerate all files in a container and copy each file in a foreach","<p>Need to load all .csv files in Azure Blob Container into SQL database.</p>

<p>Tried using a wild card <code>*.*</code> on the filename in the dataset which uses the linked service that connects to the blob and outputting the itemName in the Get Meta Data activity.</p>

<p>When executing in debug a list of filenames is not returned in the Output window. When referencing the parameter with an expression it is stated that the type is String not collection.</p>
","<azure-blob-storage><azure-data-factory>","2019-10-08 03:55:22","2446","1","3","58295888","<p>Are you actually using <code>*.*</code>?  That may be too vague for the system to interpret.  Maybe you can try something a little more descriptive, like this, <code>???20190101.json</code>, or whatever matches the patters of your data-sets.</p>

<p>I encountered a weird problem a couple weeks ago, whereby I was using the wildcard character to iterate through a bunch of files.  I was always starting on row 3 and as luck would have it, some files didn't have a row 3.  A handful of files had metadata in row 1, field names in row 2, and no row 3.  So, I was getting some weird errors.  I changed the line start to be row 2 and everything worked fine after that.</p>

<p>Also, check out the link below.</p>

<p><a href=""https://azure.microsoft.com/en-us/updates/data-factory-supports-wildcard-file-filter-for-copy-activity/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/updates/data-factory-supports-wildcard-file-filter-for-copy-activity/</a></p>
"
"58276008","How to dynamically pass MS Dynamics token to REST API setup in Azure Data Factory?","<p>MS Dynamics 365 has exposed two REST APIs in order to get to the data.  The first API is an URL that returns a token.  I need to dynamically pass the token as header to the second REST API and pull the actual data. Can we use Logic Apps or Azure data factory to accomplish this?  If yes, can you please provide the steps to accomplish this?  Any help will be much appreciated.  Thanks in advance. </p>
","<azure><microsoft-dynamics><azure-data-factory><azure-logic-apps>","2019-10-07 19:31:19","104","0","1","58309243","<p>Logic Apps has a native connector to the D365 Common Data Service, and it can be triggered using a REST call, so it could in effect replace the need for the native REST API, but it would not be my first choice because it will add some overhead (delay) to your round-trip depending on whether the Logic App has been used lately. </p>
"
"58271511","Azure DataLake Analytics U-SQL Pipeline Activities Error","<p>Previously we have used Author deployment for authentication, but it get disabled after 14 days of inactive, So i tried service Principal authentication but is does't work and throw the below error while running activity in datafactory</p>

<p>Cannot resolve DataLakeAnalyticsUri '', Please change or remove DataLakeAnalyticsUri and have a try.</p>

<p>{<br>
    ""name"": ""AzureDataLakeAnalyticsLinkedService"",<br>
    ""properties"": {<br>
         ""type"": ""AzureDataLakeAnalytics"",<br>
         ""typeProperties"": { 
                    ""accountName"": ""accountName"",
                    ""dataLakeAnalyticsUri"":""azuredatalakeanalytics.net"",
                    ""subscriptionId"": ""subscription Id"",
                    ""resourceGroupName"": ""resource Group Name"",
                    ""servicePrincipalId"":""service Principal Id"",
                    ""servicePrincipalKey"":""service Principal Key"",
                    ""tenant"":""tenant id""
           }<br>
     }
}</p>
","<azure-data-factory><azure-data-lake><u-sql>","2019-10-07 14:21:01","70","0","1","58273136","<p><strong>Updated answer:</strong></p>

<p><strong>Note:</strong> User accounts that are managed by Azure AD expires 14 days after the last slice run. </p>

<p>To avoid or resolve this error, reauthorize by selecting the Authorize permissions when the token expires. Then, redeploy the linked service.</p>

<hr>

<p>To resolve the issue, make sure to pass the <code>""dataLakeAnalyticsUri"":""&lt;azure data lake analytics URI&gt;""</code>.</p>

<p><strong>Example:</strong> <code>""dataLakeAnalyticsUri"": ""azuredatalakeanalytics.net""</code></p>

<p>For more details, refer ""<a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-data-lake-analytics"" rel=""nofollow noreferrer"">Transform data by running U-SQL scripts on Azure Data Lake Analytics</a>"".</p>

<p>Hope this helps.</p>
"
"58267534","Passing parameters from master pipeline to a SP activity in child pipeline","<p>I'm trying to pass parameters from a Master pipeline to a stored proc activity in the Child pipeline using ExecutePipeline activity.</p>
<p>I've tried referencing the parameters as below but it throws an error-</p>
<p><img src=""https://i.stack.imgur.com/kMUTp.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.stack.imgur.com/80ZcX.png"" alt=""enter image description here"" /></p>
<p>Is there any way to achieve this?</p>
","<azure><azure-data-factory>","2019-10-07 10:15:14","83","0","1","58269980","<p>Pass it as a parameter while calling the execute pipeline activity for the child pipeline.</p>

<p><img src=""https://imgur.com/0Q7pwvY.jpg"" alt=""Pipeline""></p>

<p>Expand the advanced section header under settings in the Execute Pipeline Activity for the child and add the required inputs.</p>

<p>Don't forget to create the parameters in the child pipeline first.</p>

<p>After passing the paramters from the master, you should be able to reference it while calling the stored procedure activity.</p>

<pre><code>@pipeline().parameters.version
</code></pre>
"
"58266117","How to rename a blob storage file from Azure Data Factory?","<p>I have a Pipeline on Azure Data Factory that go through several folders from Blob Storage (with a ForEach Activities).
Inside thoses folders, I need to rename a file (files have the same name on every folders) I don't want to use copy activities to rename the file and I failed using Azure Function Activities to do it.
How Can I pass parameters from Azure Data Factory to Azure Function and what is the architecture of the Azure Function (or Logic Apps) to rename a blob file ?</p>
","<azure><azure-functions><azure-data-factory>","2019-10-07 08:44:46","4047","0","1","58267277","<p>The easiest way seems to use Blob folder change(file upload) trigger which can be used on both AF and LA.
When you get this trigger you have your file name(that you don't like) you just make a copy of this file(with desired name) to your desired folder and delete old file or move it to some archive folder.
Both tasks can be achieved using AF and LA however I would suggest using AF as it is easier and more robust than AF.. and much much cheaper.</p>

<p>ps.
You can't rename files that are in Blob.. the only way is to copy content of source blob to target blob with different name. Blob name in the folder is like ""index"".</p>
"
"58243460","Databricks Filestore = 0","<p>I just ran this:</p>

<pre><code>dbutils.fs.ls(""dbfs:/FileStore/"")
</code></pre>

<p>I see this result:</p>

<pre><code>[FileInfo(path='dbfs:/FileStore/import-stage/', name='import-stage/', size=0),
 FileInfo(path='dbfs:/FileStore/jars/', name='jars/', size=0),
 FileInfo(path='dbfs:/FileStore/job-jars/', name='job-jars/', size=0),
 FileInfo(path='dbfs:/FileStore/plots/', name='plots/', size=0),
 FileInfo(path='dbfs:/FileStore/tables/', name='tables/', size=0)]
</code></pre>

<p>Shouldn't there be something in filestore?  I have hundreds of GB of data in a lake.  I am having all kinds of problems getting Databricks to find these files.  When I use Azure Data Factory, everything works perfectly fine.  It's starting to drive me crazy!</p>

<p>For instance, when I run this:</p>

<pre><code>dbutils.fs.ls(""/mnt/rawdata/2019/06/28/parent/"")
</code></pre>

<p>I get this message:</p>

<pre><code>java.io.FileNotFoundException: File/6199764716474501/mnt/rawdata/2019/06/28/parent does not exist.
</code></pre>

<p>I have tens of thousands of files in my lake!  I can't understand why I can't get a list these files!!</p>
","<azure><azure-data-lake><azure-data-factory><azure-databricks>","2019-10-04 21:41:44","527","1","1","58624176","<p>In Azure Databricks, this is expected behaviour. </p>

<ul>
<li>For Files it displays the actual file size. </li>
<li>For Directories it displays the size=0</li>
</ul>

<p><strong>Example:</strong> In dbfs:/FileStore/ I have three files shown in white color and three folders shown in blue color. Checking the file size using databricks cli.</p>

<pre><code>dbfs ls -l dbfs:/FileStore/
</code></pre>

<p><a href=""https://i.stack.imgur.com/xjXsk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xjXsk.png"" alt=""enter image description here""></a></p>

<p>When you check out the result using dbutils as follows:</p>

<pre><code>dbutils.fs.ls(""dbfs:/FileStore/"")
</code></pre>

<p><a href=""https://i.stack.imgur.com/usL0s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/usL0s.png"" alt=""enter image description here""></a></p>

<p><strong>Important point to remember while reading the files larger than 2GB:</strong></p>

<ul>
<li>Support only files less than 2GB in size. If you use local file I/O APIs to read or write files larger than 2GB you might see corrupted files. Instead, access files larger than 2GB using the DBFS CLI, dbutils.fs, or Spark APIs or use the /dbfs/ml folder described in Local file APIs for deep learning.</li>
<li>If you write a file using the local file I/O APIs and then immediately try to access it using the DBFS CLI, dbutils.fs, or Spark APIs, <strong>you might encounter a FileNotFoundException, a file of size 0, or stale file contents</strong>. That is expected because the OS caches writes by default. To force those writes to be flushed to persistent storage (in our case DBFS), use the standard Unix system call sync. </li>
</ul>

<p>There are multiple way to solve this issue. You may checkout similar <a href=""https://stackoverflow.com/questions/57116963/databricks-error-to-copy-and-read-file-from-to-dbfs-that-is-2gb/57355371#57355371"">SO</a> thread answered by me.</p>

<p>Hope this helps.</p>
"
"58239186","Including time elapsed/duration in success notification email from logic apps in data factory","<p>I built out a pipeline in Azure DataFactory to run a Databricks process. Depending on many variables, the databricks process can take 10 hours to run or 18 hours to run. I have an email notification set up through logic apps/Web activity in Data Factory to send me an email when the process is completely finished. However, I can not figure out how I pull in the ""duration"" of the trigger run into that success email.</p>

<p>This snippet is included in the ""Body"" section of the Web activity in Data Factory:</p>

<pre><code>{""message"":""@{activity('NAME OF ACTIVITY').output.dataWritten}"",""dataFactoryName"":""@{pipeline().DataFactory}"",""pipelineName"":""@{pipeline().Pipeline}"",""receiver"":""@pipeline().parameters.receiver""}
</code></pre>

<p>This is the HTTP Request I have in Logic APP:</p>

<pre><code>{
    ""properties"": {
        ""DataFactoryName"": {
            ""type"": ""string""
        },
        ""EmailTo"": {
            ""type"": ""string""
        },
        ""ErrorMessage"": {
            ""type"": ""string""
        },
        ""PipelineName"": {
            ""type"": ""string""
        },
        ""Subject"": {
            ""type"": ""string""
        }
    },
    ""type"": ""object""
}
</code></pre>

<p>I expect a success email sent to my email with Your pipeline took xx hrs and xx minutes to run on mm/dd/yyyy</p>
","<azure-data-factory><azure-logic-apps>","2019-10-04 15:18:35","498","0","1","58265960","<p>Based on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables#pipeline-scope"" rel=""nofollow noreferrer"">System variables in ADF pipeline</a>, you can't get the duration time of pipeline execution directly. However, you could get the pipeline trigger time.</p>

<p>So,my idea is using azure function activity after the Databricks activity. The main framework is Databricks activity ---> Azure Function Activity ---> Web Activity.</p>

<p>Step1: Process databricks pipleine in a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-pipeline-activity"" rel=""nofollow noreferrer"">Execute Pipeline Activity</a>.</p>

<p>Step2: After that,execute <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure function activity</a> and pass trigger time into azure function Inside azure function method, calculate duration time and output the string as you want.</p>

<p>Step3: Then pass the params into Web Activity and send email.</p>
"
"58217722","Reading file names from an azure file_storage directory","<p>I have a file_storage within my azure portal which is roughly like : </p>

<pre><code>- 01_file.txt
- 02_file.txt
- 03_file.txt
</code></pre>

<p>In azure data studio I have a data set which is linked to this file storage.</p>

<p>If possible, I would like to loop through this directory and get a list of all the file names in my ETL Pipeline.</p>

<p>I've had a look at the <code>For Each</code> and <code>look up</code> but I can't figure out how to apply it to the directory.</p>

<p>the end result would be a list of file_names that I would then carry out some further procedures before ingesting the data into azure.</p>

<p>my current work around is to create a JSON file which lists the file_names when I load the data into the file-storage and parse that using <code>look up</code> and <code>For Each</code> but I'd like to know if there is a better solution using datafactory?</p>
","<azure-data-factory>","2019-10-03 10:58:39","123","1","1","58228899","<p>Please use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">GetMetadata-Activity</a>. You could get folder metadata then get file name lists by accessing childItem properties. More details,please refer to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity#get-a-folders-metadata"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity#get-a-folders-metadata</a></p>

<p>Pipeline configuration:</p>

<p><a href=""https://i.stack.imgur.com/MbAIj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MbAIj.png"" alt=""enter image description here""></a></p>

<p>Execution:</p>

<p><a href=""https://i.stack.imgur.com/z1gSS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z1gSS.png"" alt=""enter image description here""></a></p>
"
"58216379","How to unzip a zipped folder which contains TAR files which contain zipped files, each containing CSV file, in Data Factory?","<p>I am currently learning how to use Azure Data Factory. I try to do a data ingestion and transformation that I did in a Python script. Now I try to do the same on Data Factory as it is supposed to be easier.</p>

<p>I have a zipped folder. It contains Tar files. Each file contains zipped cvs file. By ingesting csv file directly to a blob it would be easy of course but if I have to automate the reception of such a zipped folder how would I be able to load the csv in a database if I even can't figure out how to unzip, detar and unzip again? In this cas I use climate data from a real case study... I you have any idea how to handle this problem I would appreciate! Thanks in advance!!!</p>
","<azure-blob-storage><azure-data-factory><unzip><blobstorage><data-ingestion>","2019-10-03 09:40:36","255","0","1","58266932","<p>Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#compression-support"" rel=""nofollow noreferrer"">official document</a>, adf blob storage dataset only supports below compression type:</p>

<p><a href=""https://i.stack.imgur.com/cuBCB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cuBCB.png"" alt=""enter image description here""></a></p>

<p>Tar format is not supported by adf naturally. However,you could follow the solution which is mentioned in the same document, using Azure Function to <a href=""https://github.com/Azure/Azure-DataFactory/tree/master/SamplesV2/UntarAzureFilesWithAzureFunction"" rel=""nofollow noreferrer"">extract the contents of a tar file</a> or build this functionality using a custom dotnet activity. Considering your slightly complicated situation, I suggest you using custom activity.</p>
"
"58216323","Import Simple Json into Sql table via Azure Datafactory V2","<p>I want to copy Daily JSON files from Azure Datalake V2 blob into Azure Sql database Table</p>

<p>--Simple JSON code structure</p>

<pre><code>{
    ""EmpRec"": [{
            ""PersNo"": 5553345,
            ""FirstName"": ""David"",
            ""LastName"": ""Bishop"",
            ""ContrType"": ""Permanent"",
            ""Age"": 45,
            ""Length"": 5,
            ""Wsr"": ""1001WK01 "",
            ""WorkSchedule"": ""Office Workers "",
            ""OctTHome"": ""35,12"",
            ""SeptTHome"": ""50,40"",
            ""AugTHome"": ""10,0"",
            ""Site"": ""Home Depot"",
            ""SDate"": ""2012-04-17 "",
            ""EDate"": ""2016-04-17 "",
            ""Status"": ""Active"",
            ""Group"": ""Citizen"",
            ""Subarea"": ""Citizen"",
            ""SubGroup"": ""Citizen"",
            ""Position"": ""Driver"",
            ""Organization"": ""Loading RD""
        }, {
            ""PersNo"": 5553346,
            ""FirstName"": ""Norris"",
            ""LastName"": ""OFdachuck"",
            ""ContrType"": ""Permanent"",
            ""Age"": 42,
            ""Length"": 2,
            ""Wsr"": ""1001WK012"",
            ""WorkSchedule"": ""Office Workers "",
            ""OctTHome"": ""66,35"",
            ""SeptTHome"": ""72,32"",
            ""AugTHome"": ""68,02"",
            ""Site"": ""Normandy"",
            ""SDate"": ""2012-04-17 "",
            ""EDate"": ""2016-04-17 "",
            ""Status"": ""Active"",
            ""Group"": ""Citizen"",
            ""Subarea"": ""Citizen"",
            ""SubGroup"": ""Citizen"",
            ""Position"": ""Driver"",
            ""Organization"": ""Loading RD""
        }, {
            ""PersNo"": 5553347,
            ""FirstName"": ""List"",
            ""LastName"": ""Forbes"",
            ""ContrType"": ""Permanent"",
            ""Age"": 30,
            ""Length"": 1,
            ""Wsr"": ""1001WK011"",
            ""WorkSchedule"": ""Office Workers "",
            ""OctTHome"": ""35,12"",
            ""SeptTHome"": ""68,02"",
            ""AugTHome"": ""66,31"",
            ""Site"": ""Normandy"",
            ""SDate"": ""2012-04-17 "",
            ""EDate"": ""2016-04-17 "",
            ""Status"": ""Active"",
            ""Group"": ""Citizen"",
            ""Subarea"": ""Citizen"",
            ""SubGroup"": ""Citizen"",
            ""Position"": ""Driver"",
            ""Organization"": ""Loading RD""
        }, {
            ""PersNo"": 5553348,
            ""FirstName"": ""Childs"",
            ""LastName"": ""Play"",
            ""ContrType"": ""Delivery"",
            ""Age"": 25,
            ""Length"": 3,
            ""Wsr"": ""1001WK10"",
            ""WorkSchedule"": ""Office Workers "",
            ""OctTHome"": ""Office Workers "",
            ""SeptTHome"": ""50,40"",
            ""AugTHome"": ""10,0"",
            ""Site"": ""Balamb Garden"",
            ""SDate"": ""2012-04-17 "",
            ""EDate"": ""2016-04-17 "",
            ""Status"": ""Active"",
            ""Group"": ""Citizen"",
            ""Subarea"": ""Citizen"",
            ""SubGroup"": ""Citizen"",
            ""Position"": ""Driver"",
            ""Organization"": ""Loading RD""
        }, {
            ""PersNo"": 5553349,
            ""FirstName"": ""Rob"",
            ""LastName"": ""Black"",
            ""ContrType"": ""Permanent"",
            ""Age"": 28,
            ""Length"": 10,
            ""Wsr"": ""1001WK05"",
            ""WorkSchedule"": ""Packing"",
            ""OctTHome"": ""66,35"",
            ""SeptTHome"": ""72,32"",
            ""AugTHome"": ""68,02"",
            ""Site"": ""Fisherman's Horizon"",
            ""SDate"": ""2012-04-17 "",
            ""EDate"": ""2016-04-17 "",
            ""Status"": ""Active"",
            ""Group"": ""Citizen"",
            ""Subarea"": ""Citizen"",
            ""SubGroup"": ""Citizen"",
            ""Position"": ""Driver"",
            ""Organization"": ""Loading RD""
        }
    ]
</code></pre>

<p>Expected the 5 records to be uploaded, but only the first record loads. Is there any good tutorials on loading JSON to SQL? Can ADFV2 handle this or should i just revert to OPENJSON ?</p>
","<json><database><azure><azure-data-factory>","2019-10-03 09:37:56","216","0","2","58219557","<p>so I managed to extract some Azure SQL data to JSON and here's the caveat. JSON like XML has multiple structures. Make sure your JSON is in it's flattest form for ADFV2 data import and exports.</p>

<p><a href=""https://i.stack.imgur.com/EPIjP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EPIjP.png"" alt=""JSON on the left is flat JSON on the right has table object as top level""></a></p>

<p>Assuming the JSON on the right could potentially load multiple tables, just not sure if ADFV2 is at this point able to Copy any JSON format to Azure SQL DB tables.</p>
"
"58216323","Import Simple Json into Sql table via Azure Datafactory V2","<p>I want to copy Daily JSON files from Azure Datalake V2 blob into Azure Sql database Table</p>

<p>--Simple JSON code structure</p>

<pre><code>{
    ""EmpRec"": [{
            ""PersNo"": 5553345,
            ""FirstName"": ""David"",
            ""LastName"": ""Bishop"",
            ""ContrType"": ""Permanent"",
            ""Age"": 45,
            ""Length"": 5,
            ""Wsr"": ""1001WK01 "",
            ""WorkSchedule"": ""Office Workers "",
            ""OctTHome"": ""35,12"",
            ""SeptTHome"": ""50,40"",
            ""AugTHome"": ""10,0"",
            ""Site"": ""Home Depot"",
            ""SDate"": ""2012-04-17 "",
            ""EDate"": ""2016-04-17 "",
            ""Status"": ""Active"",
            ""Group"": ""Citizen"",
            ""Subarea"": ""Citizen"",
            ""SubGroup"": ""Citizen"",
            ""Position"": ""Driver"",
            ""Organization"": ""Loading RD""
        }, {
            ""PersNo"": 5553346,
            ""FirstName"": ""Norris"",
            ""LastName"": ""OFdachuck"",
            ""ContrType"": ""Permanent"",
            ""Age"": 42,
            ""Length"": 2,
            ""Wsr"": ""1001WK012"",
            ""WorkSchedule"": ""Office Workers "",
            ""OctTHome"": ""66,35"",
            ""SeptTHome"": ""72,32"",
            ""AugTHome"": ""68,02"",
            ""Site"": ""Normandy"",
            ""SDate"": ""2012-04-17 "",
            ""EDate"": ""2016-04-17 "",
            ""Status"": ""Active"",
            ""Group"": ""Citizen"",
            ""Subarea"": ""Citizen"",
            ""SubGroup"": ""Citizen"",
            ""Position"": ""Driver"",
            ""Organization"": ""Loading RD""
        }, {
            ""PersNo"": 5553347,
            ""FirstName"": ""List"",
            ""LastName"": ""Forbes"",
            ""ContrType"": ""Permanent"",
            ""Age"": 30,
            ""Length"": 1,
            ""Wsr"": ""1001WK011"",
            ""WorkSchedule"": ""Office Workers "",
            ""OctTHome"": ""35,12"",
            ""SeptTHome"": ""68,02"",
            ""AugTHome"": ""66,31"",
            ""Site"": ""Normandy"",
            ""SDate"": ""2012-04-17 "",
            ""EDate"": ""2016-04-17 "",
            ""Status"": ""Active"",
            ""Group"": ""Citizen"",
            ""Subarea"": ""Citizen"",
            ""SubGroup"": ""Citizen"",
            ""Position"": ""Driver"",
            ""Organization"": ""Loading RD""
        }, {
            ""PersNo"": 5553348,
            ""FirstName"": ""Childs"",
            ""LastName"": ""Play"",
            ""ContrType"": ""Delivery"",
            ""Age"": 25,
            ""Length"": 3,
            ""Wsr"": ""1001WK10"",
            ""WorkSchedule"": ""Office Workers "",
            ""OctTHome"": ""Office Workers "",
            ""SeptTHome"": ""50,40"",
            ""AugTHome"": ""10,0"",
            ""Site"": ""Balamb Garden"",
            ""SDate"": ""2012-04-17 "",
            ""EDate"": ""2016-04-17 "",
            ""Status"": ""Active"",
            ""Group"": ""Citizen"",
            ""Subarea"": ""Citizen"",
            ""SubGroup"": ""Citizen"",
            ""Position"": ""Driver"",
            ""Organization"": ""Loading RD""
        }, {
            ""PersNo"": 5553349,
            ""FirstName"": ""Rob"",
            ""LastName"": ""Black"",
            ""ContrType"": ""Permanent"",
            ""Age"": 28,
            ""Length"": 10,
            ""Wsr"": ""1001WK05"",
            ""WorkSchedule"": ""Packing"",
            ""OctTHome"": ""66,35"",
            ""SeptTHome"": ""72,32"",
            ""AugTHome"": ""68,02"",
            ""Site"": ""Fisherman's Horizon"",
            ""SDate"": ""2012-04-17 "",
            ""EDate"": ""2016-04-17 "",
            ""Status"": ""Active"",
            ""Group"": ""Citizen"",
            ""Subarea"": ""Citizen"",
            ""SubGroup"": ""Citizen"",
            ""Position"": ""Driver"",
            ""Organization"": ""Loading RD""
        }
    ]
</code></pre>

<p>Expected the 5 records to be uploaded, but only the first record loads. Is there any good tutorials on loading JSON to SQL? Can ADFV2 handle this or should i just revert to OPENJSON ?</p>
","<json><database><azure><azure-data-factory>","2019-10-03 09:37:56","216","0","2","58220029","<p>Json is a supported file format, you have to create a dataset and configure it properly. Remember to use the ""preview data"" button to see if the configurations are working before publishing a pipeline and running it. </p>

<p>Link to the doc:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-json"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/format-json</a></p>

<p>Hope this helped!</p>
"
"58205188","How to add variables to the data factory ARM Template","<p>I've got a data factory with 3 variables in the main pipeline.</p>

<p>These variables contain environment settings, so I'd therefore like them to be exposed to the ARM template. When I import the ARM template to a new environment, they are not in the variables list, only ""factoryid"" is shown.</p>

<p><a href=""https://i.stack.imgur.com/grcyd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/grcyd.png"" alt=""enter image description here""></a></p>

<p>I tried to add <code>arm-template-parameters-definition.json</code> (shown below) to my master branch, but it did not make a difference. This was recommended by <a href=""https://stackoverflow.com/a/54422611/3932615"">this answer</a></p>

<pre><code>""Microsoft.DataFactory/factories/pipelines"": {
""properties"": {
    ""variables"":{
        ""*"":{
            ""defaultValue"":""=""
        }
    }
}
</code></pre>

<p>}</p>

<p>How do I get my variables recognized, so that I can set up environment configurations?</p>
","<azure-data-factory><azure-rm-template>","2019-10-02 15:59:53","1706","1","2","58235177","<p>You can add parameters to your Azure data factory by having a parameters JSON file and deploying the ADF with the parameters as follows :</p>

<p>New-AzResourceGroupDeployment -Name MyARMDeployment -ResourceGroupName ADFTutorialResourceGroup -TemplateFile C:\ADFTutorial\ADFTutorialARM.json -TemplateParameterFile C:\ADFTutorial\ADFTutorialARM-Parameters.json
For more information, please refer the following doc :</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-resource-manager-template#parameters-json"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-resource-manager-template#parameters-json</a></p>

<p>Hope this helps.</p>
"
"58205188","How to add variables to the data factory ARM Template","<p>I've got a data factory with 3 variables in the main pipeline.</p>

<p>These variables contain environment settings, so I'd therefore like them to be exposed to the ARM template. When I import the ARM template to a new environment, they are not in the variables list, only ""factoryid"" is shown.</p>

<p><a href=""https://i.stack.imgur.com/grcyd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/grcyd.png"" alt=""enter image description here""></a></p>

<p>I tried to add <code>arm-template-parameters-definition.json</code> (shown below) to my master branch, but it did not make a difference. This was recommended by <a href=""https://stackoverflow.com/a/54422611/3932615"">this answer</a></p>

<pre><code>""Microsoft.DataFactory/factories/pipelines"": {
""properties"": {
    ""variables"":{
        ""*"":{
            ""defaultValue"":""=""
        }
    }
}
</code></pre>

<p>}</p>

<p>How do I get my variables recognized, so that I can set up environment configurations?</p>
","<azure-data-factory><azure-rm-template>","2019-10-02 15:59:53","1706","1","2","58252856","<p>Your example is not so bad. Please double-check whether you don't make any mistake:<br>
1) The file must be located in the root folder and named exactly: <strong>arm-template-parameters-definition.json</strong><br>
<a href=""https://i.stack.imgur.com/XejKf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XejKf.png"" alt=""enter image description here""></a><br>
2) Verify if your file contains all necessary brackets and structure reflects what you're looking for parametrise:  </p>

<pre><code>{
    ""Microsoft.DataFactory/factories/pipelines"": {
        ""properties"": {
            ""variables"": {
                ""*"": {
                    ""defaultValue"": ""=""
                }
            }
        }
    }
}
</code></pre>

<p>3) Make sure that the file has been pushed to the repo and is present over there.<br>
4) Remember that you must click <strong>Publish</strong> in ADF Designer (Author) to regenerate ARM templates<br>
5) All changes go to <strong>adf-publish</strong> branch. Do you check the regenerated files exactly in <em>that branch</em>?  </p>

<hr>

<p>Let me show you my extremely simple example.<br>
a) Pipeline <em>PL_SimpleCopy</em> with variable <em>DbUser</em><br>
<a href=""https://i.stack.imgur.com/HvAhT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HvAhT.png"" alt=""Pipeline""></a><br>
b) File: <em>arm-template-parameters-definition.json</em>  </p>

<pre><code>{
    ""Microsoft.DataFactory/factories/pipelines"": {
        ""properties"": {
            ""variables"": {
                ""DbUser"": {
                    ""defaultValue"": ""=:-DbUserValue""
                }
            }
        }
    }
}
</code></pre>

<p>c) ARM Template in <em>ADF_Publish</em> branch. Selected section is a parameter that you need:<br>
<a href=""https://i.stack.imgur.com/Dh8Xx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dh8Xx.png"" alt=""enter image description here""></a></p>
"
"58200451","How to check status of pipeline B from Pipeline A","<p>I'm solving one problem with Azure Data Factory. I have two data factoy pipelines (for example A, B). Pipeline A is running every 5 minutes. Pipeline B is running every 24 hours (night load). I need to block the run of pipeline A if pipeline B is currently running. Is it possible to check the status of B pipeline, for example directly in the ""If Condition"" activity, in the A pipeline? Something like:</p>

<pre><code>@DataFactory.Pipeline(""B"").Status
</code></pre>
","<azure><azure-data-factory>","2019-10-02 11:20:55","1620","1","1","58212289","<p>Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables"" rel=""nofollow noreferrer"">System variables supported by Azure Data Factory</a>, status is not included in pipeline.</p>

<p>My idea is using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure Function activity</a> in your Pipeline B.</p>

<p>step 1: use Azure Function activity and follow this thread <a href=""https://stackoverflow.com/questions/50243988/disable-activity-in-azure-data-factory-pipeline-without-removing-it"">Disable activity in Azure Data factory pipeline without removing it</a> to disable pipeline A.</p>

<p>step 2: do pipeline B</p>

<p>step 3:use Azure Function activity and follow this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-programmatically#net"" rel=""nofollow noreferrer"">sdk</a> to start pipeline A.</p>

<hr>

<p>Just for summary here:</p>

<p>Finally,@milos did the pipeline concurrence run checking inside database (in etl scheme) which is working.</p>
"
"58198558","CSV to ORC conversation in Azure Error : found more columns than expected column count","<p>Am pushing csv files pipe(|) delimited from one storage account in azure to another storage account  using the ORC file format but it throws an error:</p>

<pre><code>Error found when processing 'Csv/Tsv Format Text' source 'time.csv' with row number 122277 found more columns than expected column count
</code></pre>

<p>how do I solve this error ?</p>
","<azure-data-factory>","2019-10-02 09:14:04","1744","1","1","58216224","<blockquote>
  <p>'Csv/Tsv Format Text' source 'time.csv' with row number 122277 found
  more columns than expected column count</p>
</blockquote>

<p>Based on the error, it indicates that your columns violate the below 3rd rule which is mentioned in this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#alternative-column-mapping"" rel=""nofollow noreferrer"">link</a>.</p>

<ul>
<li>Source data store query result does not have a column name that is
specified in the input dataset ""structure"" section.</li>
<li>Sink data store (if with pre-defined schema) does not have a column
name that is specified in the output dataset ""structure"" section.</li>
<li>Either fewer columns or more columns in the ""structure"" of sink
dataset than specified in the mapping.</li>
<li>Duplicate mapping.</li>
</ul>

<p>You need to check whether the row number <code>122277</code> source columns are divided into different constants with <code>|</code> delimitation so that it can't map to the sink columns.</p>
"
"58180813","Linked Service error : 'System.EnterpriseServices.Wrapper.dll' not found","<p>When trying to create linked service in azure data factory which uses the Microsoft integration run time (self-hosted), the connection test throws error ""Could not load file or assembly 'System.EnterpriseServices.Wrapper.dll' or one of its dependencies. The specified module could not be found.""
<a href=""https://i.stack.imgur.com/NAylo.png"" rel=""nofollow noreferrer"">screenshot with error</a>
This is for copy activity in azure data factory which will have source db as an on-premise SQL db, self-hosted integration runtime is set up and running successfully.</p>
","<azure-data-factory>","2019-10-01 08:26:15","782","0","1","58194672","<blockquote>
  <p>Could not load file or assembly 'System.EnterpriseServices.Wrapper.dll or one of its dependencies</p>
</blockquote>

<p>Since self-hosted integration runtime is set up and running successfully, this error may be caused by the environment of on-premise. You could try to re-install .net framework or follow below similar solutions:</p>

<p>1.<a href=""https://stackoverflow.com/questions/2890773/system-enterpriseservices-wrapper-dll-error"">System.EnterpriseServices.Wrapper.dll error</a></p>

<p>2.<a href=""https://stackoverflow.com/questions/10356058/could-not-load-file-or-assembly-system-enterpriseservices-wrapper-dll"">Could not load file or assembly &#39;System.EnterpriseServices.Wrapper.dll&#39;</a></p>
"
"58176992","How to set Analytic Units (AUs) to U-SQL Activity in Azure Data Factory","<p>How to set Analytic Units (AUs) in a U-SQL Activity in Azure Data Factory v2.
Simple as that.  I've crawled documentation and can't manage to find out what the parameter is....</p>
","<azure-data-factory><u-sql>","2019-10-01 01:17:54","152","0","1","58182350","<p>In your U-SQL activity in ADF you have script advanced settings ---> Degree of parallelism, you can define how many AUs you want. Be aware that 32 is the max.</p>

<p><a href=""https://i.stack.imgur.com/64yyr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/64yyr.png"" alt=""enter image description here""></a></p>
"
"58169695","Passing single record to Copy Activity","<p>How to pass record by record in a sequential manner to a Copy Activity (including stored proc) in ADF V2</p>
","<azure><azure-sql-database><azure-data-factory>","2019-09-30 14:11:44","67","0","1","58170452","<p>You can do this by doing a lookup activity calling your sp, then do a foreach where each item is a record. Inside the foreach, you can have a copy activity where you insert only that row.</p>

<p>This way of working will have a ton of overhead time!! Also it will use a ton of activities making your whole money consumption on Data Factory much more expensive than it needs to be. Only apply it when its super mandatory, and warn everyone about the cost so you dont get into trouble.</p>

<p>Maybe if you could share a bit more on your scenario we are able to suggest a better process.</p>

<p>Hope this helped!!</p>
"
"58162722","Azure data factory pipeline not update using arm","<p>I've already publish one of my azure data factory pipeline using arm template. Now in one of the sink blob storage file path which I've updated in my template and re-deploy  it in same pipeline using 'New-AzResourceGroupDeployment'.</p>

<p>But here my file path is not getting updated.
<a href=""https://i.stack.imgur.com/TaqIj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TaqIj.png"" alt=""enter image description here""></a></p>
","<pipeline><azure-data-factory><azure-resource-manager>","2019-09-30 06:46:14","205","-1","1","58587247","<p>Please use the incremental mode of <code>New-AzResourceGroupDeployment</code> to update the existing properties.</p>

<p><a href=""https://learn.microsoft.com/en-us/powershell/module/az.resources/new-azresourcegroupdeployment?view=azps-2.8.0"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/powershell/module/az.resources/new-azresourcegroupdeployment?view=azps-2.8.0</a></p>

<p><a href=""https://learn.microsoft.com/en-us/azure/azure-resource-manager/deployment-modes#incremental-mode"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-resource-manager/deployment-modes#incremental-mode</a></p>
"
"58152360","Using SSIS script task to connect to on-premise servers using Self-Hosted IR as a proxy for Azure-SSIS IR in ADF","<p>I am trying to connect to on-premise rest web-service using SSIS script task , and running it in Azure Data Factory on its SSIS-IR which has a proxy to Self-Hosted IR which eventually connects to on-premise servers.
Is it feasible, any settings is required?
I am following this article <a href=""https://learn.microsoft.com/en-us/azure/data-factory/self-hosted-integration-runtime-proxy-ssis#enable-ssis-packages-to-connect-by-proxy"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/self-hosted-integration-runtime-proxy-ssis#enable-ssis-packages-to-connect-by-proxy</a>
But it only talks about odbc connection managers, not about a script task , which i wish to persom some rest api calls to web services hosted on premises.
ADF's REST connectors or web activity is not an option here, as the API's are meant to download images, but these out-of-the-box ADF connectors doesn't support non-json responses.</p>
","<azure><ssis><azure-data-factory>","2019-09-29 05:00:32","354","1","1","69228550","<p>Only components that work for me have been: With the new <code>ConnectByProxy</code> or <code>ExecuteOnProxy</code> properties, I had to install the stand-alone SSDT on my Visual Studio PC before these properties appeared in only the following components:</p>
<ul>
<li>All types of connection manager objects.</li>
<li>Execute SQL task</li>
<li>Execute SQL process</li>
</ul>
<p>Also, a 'source' script component in a data flow task if you add the specific Connection Manager from the edit menu of the script component.</p>
<p>I have not been able to access the on-premise data source from a SQL task component.</p>
"
"58141391","Using ADF, how to get file names loaded into a SQL Server table?","<p>I am trying to use the GetMetadata activity and a CopyData activity together.  My setup looks like this.</p>
<p><a href=""https://i.stack.imgur.com/FUWxe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FUWxe.png"" alt=""enter image description here"" /></a></p>
<p>I am trying to get files names (using GetMetadata) and load these into a field in a SQL Server table (in conjunction with the CopyData).  The CopyData works perfectly fine, but I don't see any way to have the GetMetadata get file names and pass those into a field in a table.  In my example, I have 4 fields in the source data which match 4 fields in the destination table.  The 5th field, presumably, will be the file name.  Apparently, it doesn't really work like this.  I read through the documentation below and I still can't figure it out.</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity</a></p>
","<azure><azure-sql-database><azure-data-factory>","2019-09-27 21:12:48","3157","1","1","58155361","<p><strong>Update July 2020</strong></p>
<p>A new feature has been added to the Copy activity recently to allow you add columns, <code>$$FILEPATH</code> is currently the only supported variable.  See here for more detail:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#add-additional-columns-during-copy"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#add-additional-columns-during-copy</a></p>
<hr />
<p><strong>Original Answer</strong></p>
<p>Adding an extra column to a dataset might be considered Transform and the Azure Data Factory v2 (ADF v2) Copy Task does not lend itself easily to Transform.  It can do a couple of things like convert from one format (eg csv) to other formats (eg JSON) but it is limited.  Maybe at some point in the future they add something to the mapping which allows adding string literals or something similar to the SSIS Derived Column feature, but these type of features are getting added to Mapping Data Flows at the moment it seems.</p>
<p>One way to achieve this however, is to use a stored procedure target with a parameter for the filename and a table-type parameter for the main dataset.  It looks a bit like this:
<a href=""https://i.stack.imgur.com/SJF1Y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SJF1Y.png"" alt=""ADF Copy Task with Stored Proc target"" /></a></p>
<p>The downside is you now have to create a supporting table-type in your database (<code>CREATE TYPE</code>) and a stored proc to handle it, something like this:</p>
<pre><code>CREATE TYPE dbo.typ_multiFile AS TABLE (
    col1    CHAR(1) NOT NULL,
    col2    CHAR(1) NOT NULL,
    col3    CHAR(1) NOT NULL
)
GO


CREATE OR ALTER PROC dbo.usp_ins_myTable (
    @fileName       AS VARCHAR (100),
    @typ            AS dbo.typ_multiFile READONLY
    )
AS
SET NOCOUNT ON

INSERT INTO dbo.myTable ( [fileName], col1, col2, col3 )
SELECT @fileName, col1, col2, col3
FROM @typ 

RETURN
GO
</code></pre>
<p>Note the Copy Task is inside a ForEach task, as per this diagram:</p>
<p><a href=""https://i.stack.imgur.com/uK9rx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uK9rx.png"" alt=""ADF Logical Diagram"" /></a></p>
"
"58132771","Get XML Data from API URL in Azure","<p>I need to get XML Data from API and store the data in Azure Data lake store and finally have a table created for this in Azure SQL DB/DWH. I know ADF can't handle XML data. How do i need to pull XML Data into azure.  I have checking some links on using Logicapps.Any suggestions or way to handle it</p>
","<azure><azure-data-factory><azure-logic-apps>","2019-09-27 10:31:42","391","0","1","58161868","<p>As I'm not so clear about the details of your requirement, you asked ""how to pull XML data into azure"", so I post some suggestions for your reference.</p>

<p>If you want to get the xml data of your api, you can use ""HTTP"" action in your logic app.
<a href=""https://i.stack.imgur.com/b9c7y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/b9c7y.png"" alt=""enter image description here""></a></p>

<p>Then you can use the output of the api in the next steps of your logic app.
<a href=""https://i.stack.imgur.com/FZ8LX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FZ8LX.png"" alt=""enter image description here""></a></p>

<p>If you want to parse the xml data, you need to transform it to json, please refer to the screenshot below.(input ""json(xml(body('HTTP')))"" to the content box and provide a schema)
<a href=""https://i.stack.imgur.com/gwRCG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gwRCG.png"" alt=""enter image description here""></a></p>
"
"58123919","Downloading Binary content (images) from on-premises server using Azure Data Factory ""Rest"" Dataset","<p>We have this Rest based web-wervice hosted on-premises.</p>

<p>We are using Azure Data Factory with Integration-Runtime configure on-premises on a server.</p>

<p>We are getting urls of images (all different but hosted on same server).
Is it possible to  download these images from these rest-endpoints (hosted in on-premises server) using ADF?</p>
","<azure><azure-data-factory>","2019-09-26 19:56:54","371","1","1","58160898","<p>Since you already set up Self-hosted IR on your machine,i think you could use Copy Activity to transfer binary contents into Azure Service(e.g Azure Blob Storage)</p>

<p>Base on this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-binary"" rel=""nofollow noreferrer"">document</a>,Binary format is supported for the REST connectors and Blob Storage connectors:</p>

<p><a href=""https://i.stack.imgur.com/8Uobq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8Uobq.png"" alt=""enter image description here""></a></p>
"
"58120925","data factory loses permissions when copying from data lake (gen1) to blob storage","<p>Data factory gives me this error when attempting to copy from data lake gen1 to blob storage:</p>

<pre><code>""message"": ""Failure happened on 'Sink' side. ErrorCode=UserErrorFailedFileOperation,
'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Upload file failed at
 path myblobcontainer\\file_that_im_tryin_to_copy.xml.,Source=Microsoft.DataTransfer.Common,''Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Failed to read a 'AzureDataLakeStore' file. File path: 'SourceFolderInDataLake/2019/09/26/SomeOtherFile.usql'.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Net.WebException,Message=The remote server returned an error: (403) Forbidden.
</code></pre>

<p>I have a <code>U-SQL Script</code> activity that will execute <code>1-Patient.usql</code>:
<a href=""https://i.stack.imgur.com/unyM0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/unyM0.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/EuIoE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EuIoE.png"" alt=""enter image description here""></a></p>

<p>The next activity is a <code>copy data</code> step:</p>

<p><a href=""https://i.stack.imgur.com/EG9N8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EG9N8.png"" alt=""enter image description here""></a></p>

<p><strong>Source</strong></p>

<p><a href=""https://i.stack.imgur.com/6Od4m.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6Od4m.png"" alt=""enter image description here""></a></p>

<p><strong>Sink</strong></p>

<p><a href=""https://i.stack.imgur.com/7FlcU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7FlcU.png"" alt=""enter image description here""></a></p>

<p>I have configured roles/permissions using <a href=""https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-secure-data"" rel=""nofollow noreferrer"">this tutorial</a>. </p>

<p>I can solve this issue by going to <code>Data Explorer --&gt; Access</code>:</p>

<p><a href=""https://i.stack.imgur.com/uJ1Wk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uJ1Wk.png"" alt=""enter image description here""></a></p>

<p>I then click on <code>Advanced</code>:</p>

<p><a href=""https://i.stack.imgur.com/Qb7TJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qb7TJ.png"" alt=""enter image description here""></a></p>

<p>After clicking on <code>Apply to all children</code>, then the copying works fine!</p>

<p><a href=""https://i.stack.imgur.com/HRrQT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HRrQT.png"" alt=""enter image description here""></a></p>

<p>Please note that <em>prior</em> to the <code>Copy Data</code> activity, data factory is executing <code>usql</code> script inside of the <code>gen1</code>. The script is stored in gen1, and it generates files inside of data lake as well as folders. <strong>There is never any permissions issue running this script.</strong></p>

<p><strong>What am I doing wrong?</strong></p>
","<azure><azure-active-directory><azure-data-lake><azure-data-factory><azure-data-explorer>","2019-09-26 16:15:53","1510","0","1","58130539","<p>I can reproduce your issue. Actually the <code>Apply folder permissions to sub-folders</code> is not necessary. The issue should be caused by the access control of data lake gen1, the key to the problem is the order in which files are uploaded and permissions are set.</p>

<p>You could check the <a href=""https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-access-control"" rel=""nofollow noreferrer"">Access control in Azure Data Lake Storage Gen1</a> first and refer to the information below which was based my test.</p>

<p>I suppose you add the permissions like below to the root <code>/</code>. </p>

<p><a href=""https://i.stack.imgur.com/5j2zL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5j2zL.png"" alt=""enter image description here""></a></p>

<p>If your file is already existing before setting the permission, it will be affected by the operation, i.e. the access to the file will be set, you can access the file.</p>

<p><strong>But</strong> if you upload the file or create a new folder after setting the permission, the folder and file will not have the access, you will not be able to access them. You could select the file, click the <code>Access</code> to check directly.</p>

<p>After setting the permission above, then if you set the <code>A default permission entry</code>, it will not affect the existing folders and files, but if you create new folders and files, you will get the access of all of them. i.e. the old folder and file still have not access, the new ones will have access. If you want to get the access to the old ones, just add the permissions like the screenshot again, the <code>Apply folder permissions to sub-folders</code> is the same logic.</p>

<p>So in conclusion, if you want to your service principal/MSI access all the files in your data lake, you could add the third option <code>An access permission entry and a default permission entry</code>, then you will be able to access both the existing and new folders/files.</p>
"
"58119875","Is that possible to use control M to orchestrate Azure Data factory Jobs","<p>Is that possible to use control M to orchestrate Azure Data factory Jobs?
I found this agent that can be installed on an VM:
<a href=""https://azuremarketplace.microsoft.com/en-us/marketplace/apps/bmc-software.ctm-agent-linux-arm"" rel=""nofollow noreferrer"">https://azuremarketplace.microsoft.com/en-us/marketplace/apps/bmc-software.ctm-agent-linux-arm</a> </p>

<p>But I didn't find documentation about it.
Cal Control M call an REST API to run and monitor a Job? I could user Azure functions and Blobs to control it.</p>
","<azure><azure-data-factory><control-m>","2019-09-26 15:13:33","4902","0","3","58131023","<p>All Control-M components can be installed and operated on Azure (and most other cloud infrastructure). Either use the link you quote or alternatively deploy Agents using Control-M Automation API (AAPI) or a combination of the two.</p>

<p>So long as you are on a fairly recent version Control-M you can do most operational tasks, for example you can monitor a job like so -</p>

<blockquote>
  <p>ctm run jobs:status::get -s ""jobid=controlm:00001""</p>
</blockquote>

<p>The Control-M API is developing quickly, check out the documentation linked from here -</p>

<p><a href=""https://docs.bmc.com/docs/automation-api/9019100monthly/services-872868740.html#Control-MAutomationAPI-Services-ctmrunjob:status::get"" rel=""nofollow noreferrer"">https://docs.bmc.com/docs/automation-api/9019100monthly/services-872868740.html#Control-MAutomationAPI-Services-ctmrunjob:status::get</a></p>

<p>Also see -</p>

<p><a href=""https://github.com/controlm/automation-api-quickstart"" rel=""nofollow noreferrer"">https://github.com/controlm/automation-api-quickstart</a> <a href=""http://controlm.github.io"" rel=""nofollow noreferrer"">http://controlm.github.io</a> <a href=""https://docs.bmc.com/docs/display/public/workloadautomation/Control-M+Automation+API+-+Services"" rel=""nofollow noreferrer"">https://docs.bmc.com/docs/display/public/workloadautomation/Control-M+Automation+API+-+Services</a> <a href=""https://52.32.170.215:8443/automation-api/swagger-ui.html"" rel=""nofollow noreferrer"">https://52.32.170.215:8443/automation-api/swagger-ui.html</a></p>
"
"58119875","Is that possible to use control M to orchestrate Azure Data factory Jobs","<p>Is that possible to use control M to orchestrate Azure Data factory Jobs?
I found this agent that can be installed on an VM:
<a href=""https://azuremarketplace.microsoft.com/en-us/marketplace/apps/bmc-software.ctm-agent-linux-arm"" rel=""nofollow noreferrer"">https://azuremarketplace.microsoft.com/en-us/marketplace/apps/bmc-software.ctm-agent-linux-arm</a> </p>

<p>But I didn't find documentation about it.
Cal Control M call an REST API to run and monitor a Job? I could user Azure functions and Blobs to control it.</p>
","<azure><azure-data-factory><control-m>","2019-09-26 15:13:33","4902","0","3","59976304","<p><strong><em>At this time</em></strong>, I don't believe you will find any out of the box connectors for Control-M to Azure Data Factory integration. You do have some other options, though!</p>

<hr>

<h2>Proxy ADF Yourself</h2>

<p>You can write the glue code for this, essentially being the mediator between the two.</p>

<ol>
<li><p>Write a program that will invoke the ADF REST API to run a pipeline.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#rest-api"" rel=""nofollow noreferrer"">Details Here</a></p></li>
<li><p>After triggering the pipeline, then write the code for monitoring for status.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-rest-api#monitor-pipeline"" rel=""nofollow noreferrer"">Details Here</a></p></li>
<li><p>Have Control-M call your code via an Agent that has access to it.</p>

<ul>
<li>I've done this with a C# console app running on a local server, and a Control-M Agent that invokes the glue code.</li>
<li><a href=""http://documents.bmc.com/supportu/9.0.19/help/Plugins/en-US/Azure/Web_Help/index.htm#97591.htm"" rel=""nofollow noreferrer"">Control-M Documentation here</a> also allows a way for you to execute an Azure Function directly from Control-M. This means you could put your code in an Azure Function.
<a href=""http://documents.bmc.com/supportu/9.0.19/help/Plugins/en-US/Azure/Web_Help/index.htm#97591.htm"" rel=""nofollow noreferrer"">Details Here</a>'</li>
</ul></li>
</ol>

<hr>

<h2><strong>ALTERNATIVE METHOD</strong></h2>

<p>For a ""no code"" way, check out this <a href=""https://learn.microsoft.com/en-us/connectors/azuredatafactory"" rel=""nofollow noreferrer"">Logic App connector</a>.
Write a logic app to <a href=""https://learn.microsoft.com/en-us/connectors/azuredatafactory/#create-a-pipeline-run"" rel=""nofollow noreferrer"">run the pipeline</a> and <a href=""https://learn.microsoft.com/en-us/connectors/azuredatafactory/#get-a-pipeline-run"" rel=""nofollow noreferrer"">get the pipeline run</a> to monitor status in a loop.</p>

<p>Next, Control-M should be able to use <a href=""http://documents.bmc.com/supportu/9.0.19/help/Plugins/en-US/Azure/Web_Help/index.htm#97560.htm"" rel=""nofollow noreferrer"">a plugin</a> to invoke the logic app.</p>

<hr>

<h2>Notes</h2>

<p>**Note that Control-M required an <a href=""http://documents.bmc.com/supportu/9.0.19/help/Plugins/en-US/Azure/Web_Help/index.htm#97591.htm"" rel=""nofollow noreferrer"">HTTP Trigger</a> for Azure Functions and Logic Apps.
**You might also be able to take advantage of the <a href=""http://documents.bmc.com/supportu/9.0.19/help/Plugins/en-US/BPI/Web_Help/index.htm#39457.htm"" rel=""nofollow noreferrer"">Control-M Web Services plugin</a>. Though, in my experience, I wasn't impressed with the lack of support for different authentication methods.</p>

<p><strong><em>Hope this helps!</em></strong></p>
"
"58119875","Is that possible to use control M to orchestrate Azure Data factory Jobs","<p>Is that possible to use control M to orchestrate Azure Data factory Jobs?
I found this agent that can be installed on an VM:
<a href=""https://azuremarketplace.microsoft.com/en-us/marketplace/apps/bmc-software.ctm-agent-linux-arm"" rel=""nofollow noreferrer"">https://azuremarketplace.microsoft.com/en-us/marketplace/apps/bmc-software.ctm-agent-linux-arm</a> </p>

<p>But I didn't find documentation about it.
Cal Control M call an REST API to run and monitor a Job? I could user Azure functions and Blobs to control it.</p>
","<azure><azure-data-factory><control-m>","2019-09-26 15:13:33","4902","0","3","63636132","<p>I just came across this post so a bit late to the party.</p>
<p>Control-M includes Application Integrator which enables you to use integrations created by others and to either enhance them or build your own. You can use REST or cli to instruct Control-M what requests should be generated to an application when a job is started, during execution and monitoring and how to analyze results and collect output.</p>
<p>A public repository accessible from Application Integrator shows existing jobs and there is one for Data Factory. I have extended it a bit so that the the Data Factory is started and monitored to completion via REST but then a Powershell script is invoked to retrieve the pipeline run information for each activity within the pipeline.</p>
<p>I've posted that job and script in <a href=""https://github.com/JoeGoldberg/automation-api-community-solutions/tree/master/4-ai-job-type-examples/CTM4AzureDataFactory"" rel=""nofollow noreferrer"">https://github.com/JoeGoldberg/automation-api-community-solutions/tree/master/4-ai-job-type-examples/CTM4AzureDataFactory</a> but the README is coming later.</p>
"
"58118151","How to pass parameters to ADF pipeline using .NET SDK","<p>I'm trying to pass the pipeline parameter from my function, but it seems like it still uses just the default value. I'm curious what I'm missing here.</p>

<p>I have my ADF-2 pipeline in place. I have defined a pipeline parameter called <strong>schema</strong>. I'm using this pipeline parameter within my <em>Copy Data</em> activity in the query like:</p>

<pre><code>...
FROM @{pipeline().parameters.schema}.CUSTOMERS t2
...
</code></pre>

<p>Everything is working fine when I'm debugging the pipeline (when I'm filling in the correct value). But if I'm calling the pipeline by this piece of code</p>

<pre><code>var parameters = new Dictionary&lt;string, object&gt;()
{
   {""schema"", ""fi"" }
};

var runResponse = await client.Pipelines.CreateRunWithHttpMessagesAsync(
  remoteControlConfigurations.InitPipeConfigurations.ResourceGroup,
  remoteControlConfigurations.InitPipeConfigurations.DataFactoryName,                                                                                        
  remoteControlConfigurations.InitPipeConfigurations.PipelineName,                                                                          
  parameters: parameters);
</code></pre>

<p>it fails because the <strong>schema</strong> parameter is empty:</p>

<pre><code>... 'Invalid object name '.CUSTOMERS' ...
</code></pre>

<p>What I'm missing here?</p>
","<.net><azure-data-factory>","2019-09-26 13:39:51","655","2","1","58138361","<p>Well, the mystery has been solved. By mistake I didn't run the presented code, but the older version without the parameters. When the correct version has been deployed, everything is working fine.</p>
"
"58114459","Concatenate and hash all fields in flow","<p>I'm trying to create a flow that adds two columns to existing columns. The first is the MD5 hash from the key fields, the second is the MD5 hash from all the others. The number of fields (both key and all others) may vary. I think i can pass the list of the keys and data fields via flow parameters, for example, KeyFields will be store keys columns metadata and DataFields metadata of all other columns. I try to find a function for the DerivedColumn activity, something like this (in pseudo-code):</p>

<pre><code>md5(foreach(key:$KeyFields) { concat(concatKeys,key) })
</code></pre>

<p>May be there is another way to do it ?
My flow is </p>

<p>Azure Blob Storage (csv) -> DerivedColumn -> Azure Blob Storage (csv)</p>
","<azure-data-factory>","2019-09-26 10:09:07","393","0","1","58231969","<p>You could try below idea:</p>

<p>step1: Use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">LookUp Activity</a> to get the data from your Azure Blob Storage.</p>

<p>step2: Pass the data into <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure Function Activity</a>. In the Azure Function, you could do the md5 operation with .net code or other language.</p>

<p>step3:Pass the output of Azure Function into Copy Activity as source, and configure the Blob Storage as sink.</p>
"
"58112370","Calling azure machine learning service pipeline externally without python sdk","<p>I have published a pipeline created inside azure machine learning service workspace, and i have its rest endpoint with me.
I also have Service principal Id and secrets which have contributor access over the workspace.
I am trying to invoke this pipeline thorough its rest endpoint from adf using the SPn id and secret.
But i am getting forbidden error, is there a guideline how to schive it without using python SDKs.</p>
","<azure><azure-data-factory><azure-machine-learning-service>","2019-09-26 08:14:22","251","0","1","58121632","<p>Can you try first with bearer token produced by CLI:
   az account get-access-token --subscription </p>

<p>Does it work?</p>
"
"58107360","How to call a ADF Pipeline in other resource group from ADF pipeline?","<p>We have different resource group and each resource group work on their own domain and ADF pipeleine.
We needs to call all those ADF pipeline from a Since Pipeline . As of now I am not finding a option in Pipeline activity to provide the resource group.</p>

<p>I have tried calling the pipeline from API and use web Activity in the pipeline. But few pipelines run for hours. And many times the API connection getting lost and not able to retrieve the status</p>

<p>Please help me so that I am able to call all the pipelines from single ADF with out any issue.</p>
","<azure><azure-data-factory>","2019-09-25 22:32:36","1460","0","2","58118093","<p>This doesn't look like a good practice. Do you want to call a pipeline from another Data Factory instance?</p>

<p>If that's your case, you can create an Azure function that starts the pipeline you want, and call that functions from your Data Factory. </p>

<p>Hope this helped!</p>
"
"58107360","How to call a ADF Pipeline in other resource group from ADF pipeline?","<p>We have different resource group and each resource group work on their own domain and ADF pipeleine.
We needs to call all those ADF pipeline from a Since Pipeline . As of now I am not finding a option in Pipeline activity to provide the resource group.</p>

<p>I have tried calling the pipeline from API and use web Activity in the pipeline. But few pipelines run for hours. And many times the API connection getting lost and not able to retrieve the status</p>

<p>Please help me so that I am able to call all the pipelines from single ADF with out any issue.</p>
","<azure><azure-data-factory>","2019-09-25 22:32:36","1460","0","2","58149017","<p>You can use Web activity to call other pipeline using rest api.
<a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/createrun"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/createrun</a></p>
"
"58106869","Concatenate values from 2 or more columns in a csv to single column in Azure Data Factory","<p>In Azure Data Factory v2 (via the Azure Portal), I'm using a csv file as my source file, and an Azure SQL Server table as the destination. How in ADF can I concatenate the values of 3 columns from the csv file into one field in the database table?  It's only giving me options for 1-to-1 column mapping.  Is a Data Flow element needed for this, or can it all be done inside a Copy Data pipeline?</p>

<p>FYI: this is <em>not</em> the answer to my question: <a href=""https://stackoverflow.com/questions/51352725/azure-data-factory-mapping-2-columns-in-one-column"">Azure Data Factory mapping 2 columns in one column</a></p>
","<azure-data-factory>","2019-09-25 21:37:43","3868","1","1","58110855","<p>Azure <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-overview"" rel=""nofollow noreferrer"">Data Flow</a> Derived Column can help you concatenate the values of 3 columns from the csv file into one field in the database table.</p>

<p>You can reference my example.</p>

<p><strong>My CSV data</strong>:</p>

<p><a href=""https://i.stack.imgur.com/Pyyd0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Pyyd0.png"" alt=""enter image description here""></a></p>

<p><strong>Create a mapping Data Flow in pipeline</strong>: set the csv file as <code>Source</code>+ <code>DerivedColun</code>+ <code>Sink</code>:</p>

<p><a href=""https://i.stack.imgur.com/Cb9YA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Cb9YA.png"" alt=""enter image description here""></a></p>

<p><strong>Derived Column</strong>: create a Visual Expression to concatenate the values of 3 columns from the csv file.
<a href=""https://i.stack.imgur.com/iKXNo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iKXNo.png"" alt=""enter image description here""></a></p>

<p>I concatenate '<code>b</code>','<code>c</code>','<code>d</code>' columns to '<code>b</code>':
<a href=""https://i.stack.imgur.com/psGbg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/psGbg.png"" alt=""enter image description here""></a></p>

<p><strong>Sink Mapping settings:</strong> choose the Derived Column instead of the column <code>b</code>
<a href=""https://i.stack.imgur.com/6Zrwe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6Zrwe.png"" alt=""enter image description here""></a></p>

<p><strong>Data Preivew:</strong> check the data will insert to my SQL database, and run the pipeline.</p>

<p><a href=""https://i.stack.imgur.com/JtYhM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JtYhM.png"" alt=""enter image description here""></a> </p>

<p>Hope this helps.</p>
"
"58099377","Azure Data Factory. Lookup date in Azure SQL DWH, use in Oracle Query","<p>I have an Oracle database and I have to load dat from this database to Azure SQL DWH. This is done once every day. At the beginning of the pipeline I first do a lookup on  SQL DWH to look for the latest date. The result for that is something like '2015-10-25'.
I want to use this date to query the Oracle database. But I allready found out, by trying the query on Oracle that the following code does not work:</p>

<pre><code>Select * from Table 1 where day = '2015-10-25'
</code></pre>

<p>The date in the day column looks like 25-OCT-15 (DD-MON-YY).</p>

<p>I treid the following where clause:</p>

<pre><code>where day = TO_DATE('2015-10-25','DD-MON-YY')
</code></pre>

<p>But then I get the error:  ""literal does not match format string""</p>

<p>I realy don't know how to make Oracle understand this T-SQL date format. </p>
","<oracle><tsql><date><azure-data-factory>","2019-09-25 13:18:49","687","1","2","58111099","<p>You can try this query:</p>

<pre><code>Select * from Table 1 where day = to_char (to_date('2015-10-25','YYYY-MM-DD'), 'DD-Mon-YY')
</code></pre>

<p>Reference this blog: <a href=""https://stackoverflow.com/questions/29263586/how-to-convert-yyyymmdd-to-dd-mon-yyyy-in-oracle?rq=1"">how to convert YYYYMMDD to DD-Mon-YYYY in oracle?</a></p>

<p>Hope this helps.</p>
"
"58099377","Azure Data Factory. Lookup date in Azure SQL DWH, use in Oracle Query","<p>I have an Oracle database and I have to load dat from this database to Azure SQL DWH. This is done once every day. At the beginning of the pipeline I first do a lookup on  SQL DWH to look for the latest date. The result for that is something like '2015-10-25'.
I want to use this date to query the Oracle database. But I allready found out, by trying the query on Oracle that the following code does not work:</p>

<pre><code>Select * from Table 1 where day = '2015-10-25'
</code></pre>

<p>The date in the day column looks like 25-OCT-15 (DD-MON-YY).</p>

<p>I treid the following where clause:</p>

<pre><code>where day = TO_DATE('2015-10-25','DD-MON-YY')
</code></pre>

<p>But then I get the error:  ""literal does not match format string""</p>

<p>I realy don't know how to make Oracle understand this T-SQL date format. </p>
","<oracle><tsql><date><azure-data-factory>","2019-09-25 13:18:49","687","1","2","58111999","<p>Your Oracle column is of <code>date</code> datatype.  When you connect to an Oracle database and write a query against that date column, you will see its default format <code>DD-MON-YY</code> as per <a href=""https://docs.oracle.com/cd/B28359_01/server.111/b28318/datatype.htm#CNCPT413"" rel=""nofollow noreferrer"">this reference</a>.</p>

<p>You can override this setting by running an <code>ALTER SESSION</code> command, eg</p>

<pre><code>ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY MM DD';
</code></pre>

<p>but this is just in that local session.  The data is still stored in Oracle in the same manner and it's simply the way you view it that is changing.</p>

<p>In Azure Data Factory (ADF v2) and your example, you are dealing with strings.  So you must make sure any parameters you pass in are in the correct format or set to the correct datatype.  The Oracle function <code>TO_DATE</code> converts strings to the date datatype.  Therefore when <em>passing in</em> a string of format <code>YYYY-MM-DD</code> then that is the format you must use, to let the <code>TO_DATE</code> function know what you are passing in:</p>

<pre><code>TO_DATE('2015-10-25','YYYY-MM-DD')
</code></pre>

<p>The function then successfully converts your parameter to a <code>date</code> datetype for correct comparison against the main date column.</p>
"
"58096113","Author and deploy","<p>Recently I noticed that the Author and Deploy section has been removed from ADF v1. Is there any way that I can read the json script written for datasets and pipelines already deployed and make any changes?</p>
","<azure-data-factory>","2019-09-25 10:16:49","40","0","1","58107475","<p>There are two options remaining for reading your existing ADF V1 definitions.</p>

<ol>
<li>Use a REST API call (optionally via powershell)</li>
<li>Use Azure SDK Visual Studio 2015 Update 3 + <a href=""https://marketplace.visualstudio.com/items?itemName=AzureDataFactory.MicrosoftAzureDataFactoryToolsforVisualStudio2015"" rel=""nofollow noreferrer"">ADF Tools for Visual Studio 2015</a></li>
</ol>

<p>For the API call you will need to register an app to authenticate, like for any other REST API call.</p>

<pre><code>GET https://management.azure.com/subscriptions/$subsId/resourcegroups/$resourceGroup/providers/Microsoft.DataFactory/datafactories/$dataFactoryName/pipelines/$pipelineName?api-version=2015-10-01
</code></pre>
"
"58094196","Azure Data factory parameter","<p>I am trying to load the data from the on-premise sql server to Sql Server at VM. I need to do it every day. For the same, I have created a trigger. Trigger is inserting the data properly. But now, I need to insert triggerID in the destination columns for every run in a column.</p>

<p>I don't know what mistake i am doing. I found many blogs on the same but all have information when we are extracting the data from a blob not from sql server.</p>

<p>I was trying to insert the value of the same like this but it's giving error.
""Activity Copy Data1 failed: Please choose only one of the three property ""name"", ""path"" and ""ordinal"" to reference columns for ""source"" and ""sink"" under ""mappings"" property. ""
pipeline details. Please suggest</p>

<pre><code>{
    ""name"": ""pipeline1"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""Copy Data1"",
                ""type"": ""Copy"",
                ""dependsOn"": [],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""userProperties"": [],
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""AzureSqlSource""
                    },
                    ""sink"": {
                        ""type"": ""SqlServerSink""
                    },
                    ""enableStaging"": false,
                    ""translator"": {
                        ""type"": ""TabularTranslator"",
                        ""mappings"": [
                            {
                                ""source"": {
                                    ""name"": ""Name"",
                                    ""type"": ""String""
                                },
                                ""sink"": {
                                    ""name"": ""Name"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""type"": ""String"",
                                    ""name"": ""@pipeline().parameters.triggerIDVal""
                                },
                                ""sink"": {
                                    ""name"": ""TriggerID"",
                                    ""type"": ""String""
                                }
                            }
                        ]
                    }
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""AzureSqlTable1"",
                        ""type"": ""DatasetReference""
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""SqlServerSQLDEV02"",
                        ""type"": ""DatasetReference""
                    }
                ]
            }
        ],
        ""parameters"": {
            ""triggerIDVal"": {
                ""type"": ""string""
            }
        },
        ""annotations"": []
    }
}
</code></pre>

<p>I want that each time trigger is executed then the triggerID should be populating into the destination column TriggerID.</p>
","<azure><azure-data-factory>","2019-09-25 08:24:16","1313","0","1","58162718","<p>Firstly,please see the limitation in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#alternative-column-mapping"" rel=""nofollow noreferrer"">copy activity column mapping</a>:</p>

<ul>
<li>Source data store query result does not have a column name that is
specified in the input dataset ""structure"" section.</li>
<li>Sink data store (if with pre-defined schema) does not have a column
name that is specified in the output dataset ""structure"" section.</li>
<li>Either fewer columns or more columns in the ""structure"" of sink
dataset than specified in the mapping.</li>
<li>Duplicate mapping.</li>
</ul>

<p>So,i don't think you could do the data transfer plus trigger id which is not contained by the source columns.My idea is:</p>

<p>1.First use a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity#get-a-folders-metadata"" rel=""nofollow noreferrer"">Set Variable</a> activity to get the trigger id value.</p>

<p><a href=""https://i.stack.imgur.com/qIFFS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qIFFS.png"" alt=""enter image description here""></a></p>

<p>2.Then connect with copy activity and pass the value as parameter.</p>

<p>3.In the sink of copy activity, you could invoke stored procedure to combine the trigger id with other columns before the row is inserted into table. More details, please see this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#invoke-a-stored-procedure-from-a-sql-sink"" rel=""nofollow noreferrer"">document</a>.</p>
"
"58090889","how to flatten multiple child nodes using jsonNodeReference in azure data factory","<p>i have a json file with the below format.</p>

<pre><code>{
  ""results"": [
    {
      ""product"": {
        ""code"": ""104AB001"",
        ""names"": [
          {
            ""lang_code"": ""fr_CM"",
            ""name"": ""BANOLE""
          },
          {
            ""lang_code"": ""f_CM"",
            ""name"": ""BANOLE""
          }
        ]
      }
    },
    {
      ""product"": {
        ""code"": ""104AB002"",
        ""names"": [
          {
            ""lang_code"": ""fr_CM"",
            ""name"": ""BANOLE""
          },
          {
            ""lang_code"": ""f_CM"",
            ""name"": ""BANOLE""
          }
        ]
      }
    }
  ]
}
</code></pre>

<p>I am using a copy activity and</p>

<pre><code>""jsonNodeReference"": ""$.['results'][*].['product'].['names']"",
                ""jsonPathDefinition"": {
                    ""product__code"": ""$.['results'][*].['product'].['code']"",
                    ""product__names__lang_code"": ""['lang_code']"",
                    ""product__names__name"": ""['name']""
                }
</code></pre>

<p>The expected output is </p>

<pre><code>product__code   product__names__lang_code   product__names__name
104AB001        fr_CM                       BANOLE
104AB001        f_CM                        BANOLE
104AB002        fr_CM                       BANOLE
104AB002        f_CM                        BANOLE
</code></pre>

<p>But i am getting
<a href=""https://i.stack.imgur.com/P3aRY.png."" rel=""nofollow noreferrer"">Azure data factory output as</a>
When i did search in stack overflow and google, i got some info like it is not possible in azure data factory. below are the links</p>

<p><a href=""https://social.msdn.microsoft.com/Forums/en-US/5ebcef1f-5817-434c-9426-a83e9df35965/jsonnodereference-and-jsonpathdefinition-for-multiple-child-nodes?forum=AzureDataFactory"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/en-US/5ebcef1f-5817-434c-9426-a83e9df35965/jsonnodereference-and-jsonpathdefinition-for-multiple-child-nodes?forum=AzureDataFactory</a> </p>

<p><a href=""https://medium.com/@gary.strange/flattening-json-in-azure-data-factory-2f2130794258"" rel=""nofollow noreferrer"">https://medium.com/@gary.strange/flattening-json-in-azure-data-factory-2f2130794258</a> </p>

<p>My question is here, if it is not possible in azure data factory then what could be the other solution to achieve this.</p>
","<azure><azure-sql-database><azure-data-factory><azure-databricks>","2019-09-25 03:47:46","2121","0","1","58108419","<p>Only one array can be flattened in a schema. Multiple arrays can be referenced—returned as one row containing all of the elements in the array. However, only one array can have each of its elements returned as individual rows. This is the current limitation with jsonPath.</p>

<p>However you can first convert json file with nested objects into CSV file using Logic App and then you can use the CSV file as input for Azure Data factory. Please refer below URL to understand how Logic App can be used to convert nested objects in json file to CSV.</p>

<p>[link] ""<a href=""https://adatis.co.uk/converting-json-with-nested-arrays-into-csv-in-azure-logic-apps-by-using-array-variable/"" rel=""nofollow noreferrer"">https://adatis.co.uk/converting-json-with-nested-arrays-into-csv-in-azure-logic-apps-by-using-array-variable/</a>""</p>

<p>Thanks</p>
"
"58078734","Invalid Column Name error when trying to update a row using Data Flow sink","<p>My actual use case is more complicated (and based on the SCD Type 2 template pipeline), but in attempting to debug my error I've created a new simplified data flow of the following format:</p>

<p><b>Data Source</b> (data set is an Azure SQL Database table with a primary key column and multiple other NULLable columns) <br>
<b>Derived Column</b> (creating a new column called <code>""NewRowStatus""</code> set to the literal string 'Historic') <br>
<b>Alter Row</b> (single condition ""Update If"" set to true(), so all rows will update) <br>
<b>Sink</b> to the same dataset we started with, mapping <code>""NewRowStatus""</code> to the ""Row Status"" database column, and with my primary key as the Key Column, allowing update.</p>

<p>Data Preview runs successfully for each step in the data flow, but when I run the entire data flow through a debug pipeline I get the following error:</p>

<pre><code>""error"": {
        ""code"": 400,
        ""message"": ""Job '[jobID] failed due to reason: DF-SYS-01 at Sink 'sink1': java.sql.BatchUpdateException: Invalid column name '{[My Key Column]}'.\njava.sql.BatchUpdateException: Invalid column name '{[My Key Column]}'...
</code></pre>

<p>(square brackets are mine)</p>

<p>I've tried using different (non-key) columns to see if that made any difference, but so far I've not managed a single successful update. I'm not sure if I'm missing something really obvious, or if this is an issue with ADF still being in preview, but any thoughts would be appreciated.</p>
","<azure-data-factory>","2019-09-24 10:54:46","1272","0","1","58101591","<p>So in trying to get some non-sensitive screenshots, I created an even simpler new data flow, and <em>that</em> one worked fine. After a bit of digging, it seems like ADF doesn't like it when your key field in the sink has a space in it. Having amended my Dimension table's key column to not include a space, and re-run my original data flow, everything is now working as expected.</p>
"
"58077439","how to monitor adf pipeline from CI/CD pipeline","<p>I have a CD Pipeline which triggers an azure data factory(adf) pipeline but it(CD pipeline) doesn't wait to proceed with next tasks until the(adf) pipeline completes. I wanna know that is there any inbuilt capabilities in CD pipeline through which i can trigger as well as monitor adf pipeline. </p>

<p>So, my desired outcome is that CD pipeline should wait for adf pipeline to complete before proceeding with next tasks in CD pipeline.</p>
","<azure-devops><continuous-integration><azure-pipelines><azure-data-factory>","2019-09-24 09:43:27","658","3","1","58082957","<p>You can use the following Powershell script from Azure DevOps.</p>

<pre><code>$resourceGroupName  = ""yourresourcegroup""
$DataFactoryName    = ""yourdatafactory""
$pipelineName       = ""yourpipeline""
$pollFrequency      = 1

$executionId = Invoke-AzDataFactoryV2Pipeline -ResourceGroupName $resourceGroupName -DataFactoryName $DataFactoryName -PipelineName $pipelineName

$runStatus = (Get-AzDataFactoryV2PipelineRun -ResourceGroupName $resourceGroupName -DataFactoryName $DataFactoryName -PipelineRunId $executionId).Status
While ($runStatus -eq 'InProgress') {

    Write-Host (""Pipeline {0} in progress"" -f $pipelineName)
    Start-Sleep $pollFrequency

    $runStatus = (Get-AzDataFactoryV2PipelineRun -ResourceGroupName $resourceGroupName -DataFactoryName $DataFactoryName -PipelineRunId $executionId).Status
}

Write-Host (""Pipeline {0} finished with status {1}"" -f $pipelineName, $runStatus)
</code></pre>
"
"58073075","Using If-Condition ADF V2","<p>I have one Copy activity and two stored Proc Activity and i want to basically update the status of my pipeline as Failed in Logtable if any of these activities failed with error message details. Below is the flow of my pipeline</p>

<p><a href=""https://i.stack.imgur.com/0Nijj.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0Nijj.jpg"" alt=""enter image description here""></a></p>

<p>I wanted to use If-Condition activity and need help in setting the expression for it. For Copy activity i can use the below expression, but not sure about getting the status of stored Proc activity</p>

<pre><code>@or(equals(activity('Copy Data Source1').output.executionDetails[0].status, 'Failed'), &lt;expression to get the status of Stored Proc&gt;)
</code></pre>

<p>If the above expression is true then i want to have one common stored proc activity that i will set in Add If True Activity to log the error details</p>

<p>Let me know if this possible.</p>
","<azure><azure-data-factory>","2019-09-24 04:24:11","826","1","1","58118384","<p>I think you have overcomplicated this.<br>
A much easier way to do that is to leverage a Failure path for required activities. Furthermore, SP would not be executed when Copy Data fails, therefore checking the status of execution of SP doesn't really make sense.
My pipeline would look like this:
<a href=""https://i.stack.imgur.com/5aSpX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5aSpX.png"" alt=""enter image description here""></a></p>
"
"58072161","Is there any possibility to transfer data from Azure data lake gen2 to Azure event hub by using Azure data factory?","<p>Is there any possibility to transfer data from Azure data lake gen2 to Azure event hub by using Azure data factory? Is there any alternative ways to to preserve same folder structure in  Event hub once transfer to Event hub from Data Lake?  </p>
","<azure><azure-storage><azure-data-factory><azure-data-lake><azure-eventhub>","2019-09-24 02:10:21","1233","1","2","58072399","<p>Azure Data Factory support Azure data lake gen2 but doesn't support Azure event hut now.
Please see  <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-overview"" rel=""nofollow noreferrer"">Azure Data Factory connector overview</a>.
<a href=""https://i.stack.imgur.com/dA0xV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dA0xV.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"58072161","Is there any possibility to transfer data from Azure data lake gen2 to Azure event hub by using Azure data factory?","<p>Is there any possibility to transfer data from Azure data lake gen2 to Azure event hub by using Azure data factory? Is there any alternative ways to to preserve same folder structure in  Event hub once transfer to Event hub from Data Lake?  </p>
","<azure><azure-storage><azure-data-factory><azure-data-lake><azure-eventhub>","2019-09-24 02:10:21","1233","1","2","58072516","<p>There is no direct connection to Event Hub, but you can use <a href=""http://nnmer.github.io/azure-services-map/"" rel=""nofollow noreferrer"">this service</a> to see what IO direct endpoints are available and use the <em>IO tree</em> to see how you can connect multiple services</p>
"
"58067345","Azure Functions as Scheduler","<p>Is Azure functions a good alternative to Azure Data Factory to use as scheduler? It has blob trigger to monitor and can use C# to trigger databricks jobs using API. But is it a viable alternative.</p>

<p>Edited to add more information. Wanted to trigger a databricks job based on a trigger file but do not want to use Azure Data Factory or Data bricks job.</p>
","<azure><azure-functions><azure-data-factory>","2019-09-23 17:22:36","641","2","1","58069561","<p>I would probably use simple logic app with Event Grid trigger on blob storage event blob created event. Based on trigger data I would call Databricks Job REST API.</p>

<p>I did entire demo below working in under 10 minutes so its fast to set up.</p>

<p>With this demo I used</p>

<p><a href=""https://i.stack.imgur.com/Ch31U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ch31U.png"" alt=""enter image description here""></a></p>

<p>And logic app setup as trigger</p>

<p><a href=""https://i.stack.imgur.com/9blIM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9blIM.png"" alt=""enter image description here""></a></p>

<p>Where I strongly suggest to add prefix filter like </p>

<pre><code>/blobServices/default/containers/&lt;container_name&gt;
</code></pre>

<p>So you don't fire too many logic apps from different containers as event grid reacts to all events in entire storage account.</p>

<p>And HTTP call like so</p>

<p><a href=""https://i.stack.imgur.com/zTOTi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zTOTi.png"" alt=""enter image description here""></a></p>

<p>Of course at this point simply change clusters list to submitting job REST call.</p>

<p>And see execution like </p>

<p><a href=""https://i.stack.imgur.com/mQ42f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mQ42f.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/aZkzq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aZkzq.png"" alt=""enter image description here""></a></p>

<p>Just make sure that EventGrid resource provider is registered or logic app will never fire off.</p>

<p><a href=""https://i.stack.imgur.com/nyz8f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nyz8f.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/zLJCh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zLJCh.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/5Umxo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5Umxo.png"" alt=""enter image description here""></a></p>
"
"58062768","How to rename files, as they are copied to a new location, with adf","<p>I am performing copy activity from <code>cosmosDB</code> to <code>Blob storage</code>, collections will be copied to storage as files. I want those filenames to be renamed with ""collectionname-date"". The file should have <code>name</code>, followed by <code>date</code> and <code>time</code> as suffix to that. How can I achieve this? </p>
","<azure><azure-data-factory><azure-rm-template>","2019-09-23 12:45:06","3455","1","1","58163873","<p>I have to say i can't find any ways to get the collection name dynamically,but i implement other your requirements. Please see my configurations:</p>

<p>1.Cosmos db dataset:</p>

<p>as normal to set</p>

<p><a href=""https://i.stack.imgur.com/9JxK0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9JxK0.png"" alt=""enter image description here""></a></p>

<p>2.Blob Storage dataset:</p>

<p>configure a parameter for it:</p>

<p><a href=""https://i.stack.imgur.com/kJNwp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kJNwp.png"" alt=""enter image description here""></a></p>

<p>Then configure the dynamic file path:</p>

<p><a href=""https://i.stack.imgur.com/447Yo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/447Yo.png"" alt=""enter image description here""></a></p>

<p>Pass the collection static name(for me is coll) for the <code>fileName</code> param.</p>

<p><a href=""https://i.stack.imgur.com/E46pI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E46pI.png"" alt=""enter image description here""></a></p>

<p>3.Output in Blob Storage:</p>

<p><a href=""https://i.stack.imgur.com/FNdKu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FNdKu.png"" alt=""enter image description here""></a></p>
"
"58059864","ADF: Can't get the for each paramter to work in the query for the sink","<p>We have an Oracle database with a table and one of the tables holds dates. I want to itterate over this table by this date to copy dat from Oracle to Azure Datalake. But somehow I cannot get this to work.</p>

<p>The loopkup for the foreach works fine, but when I want to copy the data, using the one of the dates from the lookup, the copy activity task fails with the error: Message=ERROR [HY000] [Microsoft][ODBC Oracle Wire Protocol driver][Oracle]ORA-00936: missing expression</p>

<p>I suspect it has something to do with the dateformat that Oracle spits out en expects in the where clause. When I run the lookup-query in sql-developer, the dat format is like 29-DEC-14.</p>

<p>The query for the lookup looks like this:</p>

<pre><code>select distinct activity_day
from Table 1
where activity_day &lt; '01-JAN-15'
</code></pre>

<p>I restrict the data for testing so it only has to itterate everything before 01-01-2015 (which in this case is three rows)</p>

<p>In the foreach component items is stated as follows:</p>

<pre><code>@activity('LookupDates').output.value
</code></pre>

<p>In the Copy activity the sink is specified as an Oracle query (connection to the oracle database works fine)</p>

<pre><code>select column1, column2, coumn3,.......
from Table
where activity_day = @item().activity_day
</code></pre>

<p>The result should be that I get three files in my datalake with the data from three days. But as stated earlier, it fails in the copy activity on the source side. complet error below here:
""errorCode"": ""2200"",
    ""message"": ""Failure happened on 'Source' side. ErrorCode=UserErrorOdbcOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR [HY000] [Microsoft][ODBC Oracle Wire Protocol driver][Oracle]ORA-00936: missing expression,Source=Microsoft.DataTransfer.ClientLibrary.Odbc.OdbcConnector,''Type=System.Data.Odbc.OdbcException,Message=ERROR [HY000] [Microsoft][ODBC Oracle Wire Protocol driver][Oracle]ORA-00936: missing expression,Source=msora28.dll,'"",
    ""failureType"": ""UserError"",
    ""target"": ""Copy Data1""</p>
","<oracle><foreach><azure-data-factory>","2019-09-23 09:57:12","1123","0","1","58201926","<p>Answer was given on MSDN: in combination with another ttopic on stackoverflow:
<a href=""https://social.msdn.microsoft.com/Forums/en-US/4224338f-9511-4f80-9fbf-4bf4cbc1b596/cant-get-lookup-data-passed-to-oracle-database?forum=AzureDataFactory"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/en-US/4224338f-9511-4f80-9fbf-4bf4cbc1b596/cant-get-lookup-data-passed-to-oracle-database?forum=AzureDataFactory</a></p>
"
"58059104","adf v2 linked service connection pooling","<p>I am connecting azure sql server using adfv2 linked service.
I am getting below error</p>

<blockquote>
  <p>""Execution fail against sql server. Sql error number: 10928. Error
  Message: Resource ID : 1. The request limit for the database is 120
  and has been reached. See
  '<a href=""http://go.microsoft.com/fwlink/?LinkId=267637"" rel=""nofollow noreferrer"">http://go.microsoft.com/fwlink/?LinkId=267637</a>' for assistance</p>
</blockquote>

<p>Does linked service support the connection pooling and if yes then how to implement the same?
if you have better option to resolve the issue without any  sql server configuration changes Please guide me</p>
","<azure-sql-database><azure-data-factory>","2019-09-23 09:08:45","289","0","1","58072014","<p>The error message is very clear: ""1. The request limit for the database is 120 and has been reached."" 
<a href=""https://i.stack.imgur.com/HfG5R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HfG5R.png"" alt=""enter image description here""></a></p>

<p>For more details, please see :</p>

<ol>
<li><a href=""https://learn.microsoft.com/en-us/azure/sql-database/sql-database-dtu-resource-limits-single-databases"" rel=""nofollow noreferrer"">Resource limits for single databases using the DTU-based purchasing
model</a>.</li>
<li><a href=""https://learn.microsoft.com/en-us/azure/sql-database/sql-database-resource-limits-database-server"" rel=""nofollow noreferrer"">SQL Database resource limits for Azure SQL Database server</a></li>
</ol>

<p>To solve this error, you need to <a href=""https://learn.microsoft.com/en-us/azure/sql-database/sql-database-single-database-scale"" rel=""nofollow noreferrer"">Scale single database resources in Azure SQL Database</a>.</p>

<p>Here is another blog can help you: <a href=""https://blogs.technet.microsoft.com/latam/2015/06/01/how-to-deal-with-the-limits-of-azure-sql-database-maximum-logins/"" rel=""nofollow noreferrer"">How to deal with the limits of Azure SQL Database maximum logins</a>.</p>

<p>Hope this helps.</p>
"
"58054948","GET data from REST API using Azure Data Factory","<p>I am trying to get data from Pendo through REST API using Azure Data Factory. The PYTHON code for this activity is </p>

<pre><code>import requests
url = ""https://app.pendo.io/api/v1/aggregation""
data = ""{\""response\"":{\""mimeType\"":\""application/json\""},\""request\"":{\""pipeline\"":[{\""source\"":{\""guideEvents\"":null,\""timeSeries\"":{\""first\"":\""1506977216000\"",\""count\"":-10,\""period\"":\""dayRange\""}}}]}}""
headers = {
    'x-pendo-integration-key': ""[add_your_pendo_int_key_here]"",
    'content-type': ""application/json""
}
response = requests.post(url, data = data, headers = headers)
</code></pre>

<p>How do I define the data part of the code through Azure Data Factory?
I've put the content-type and x-pendo-integration-key in the additional headers.</p>
","<python><azure><azure-data-factory>","2019-09-23 02:23:14","3688","1","2","58058551","<p>You can use HTTP Connector to retrieve data from HTTP endpoint. This HTTP connector is supported for the following activities:</p>

<ul>
<li>Copy activity with supported source/sink matrix</li>
<li>Lookup activity
You can copy data from an HTTP source to any supported sink data store. For a list of data stores that Copy Activity supports as sources and sinks, <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats"" rel=""nofollow noreferrer"">see Supported data stores and formats</a>.</li>
</ul>

<p><strong>Note</strong>: To test an HTTP request for data retrieval before you configure the HTTP connector in Data Factory, learn about the API specification for header and body requirements. You can use tools like Postman or a web browser to validate.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-http"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-http</a></p>

<p>Here is a sample Custom activity for POST:</p>

<pre><code>{
    ""name"": ""HttpSourceDataInput"",
    ""properties"": {
        ""type"": ""HttpFile"",
        ""linkedServiceName"": {
            ""referenceName"": ""&lt;HTTP linked service name&gt;"",
            ""type"": ""LinkedServiceReference""
        },
        ""typeProperties"": {
            ""relativeUrl"": ""&lt;relative url&gt;"",
            ""requestMethod"": ""Post"",
            ""requestBody"": ""&lt;body for POST HTTP request&gt;""
        }
    }
}
</code></pre>

<p>You can check ADF related samples <a href=""https://github.com/Azure/Azure-DataFactory"" rel=""nofollow noreferrer"">here</a>.Also find <a href=""https://stackoverflow.com/questions/35551533/using-azure-data-factory-to-get-data-from-a-rest-api"">this</a> for additional reference.
Hope it helps.</p>
"
"58054948","GET data from REST API using Azure Data Factory","<p>I am trying to get data from Pendo through REST API using Azure Data Factory. The PYTHON code for this activity is </p>

<pre><code>import requests
url = ""https://app.pendo.io/api/v1/aggregation""
data = ""{\""response\"":{\""mimeType\"":\""application/json\""},\""request\"":{\""pipeline\"":[{\""source\"":{\""guideEvents\"":null,\""timeSeries\"":{\""first\"":\""1506977216000\"",\""count\"":-10,\""period\"":\""dayRange\""}}}]}}""
headers = {
    'x-pendo-integration-key': ""[add_your_pendo_int_key_here]"",
    'content-type': ""application/json""
}
response = requests.post(url, data = data, headers = headers)
</code></pre>

<p>How do I define the data part of the code through Azure Data Factory?
I've put the content-type and x-pendo-integration-key in the additional headers.</p>
","<python><azure><azure-data-factory>","2019-09-23 02:23:14","3688","1","2","58090287","<p>Problem solved. The data part of the PYTHON code(with out the <code>\</code>) is put in the Request Body of Copy Activity in Azure Data Factory and the Request Method is POST.</p>
"
"58054800","How does Get Metadata order/sort its output?","<p>I have set up a DataFactory pipeline that gets a list of files in Azure Data Lake Storage Gen2 then iterates over each files using a ForEach loop.</p>

<p>Im using a Get Metadata activity to produce the list of files and the argument its outputting is 'Child Items'.</p>

<p>I want to make sure the list (child items) is always sorted in name order. My question is what is the default sorting method for child items or can i sort this manually?</p>

<p>Thanks</p>

<pre><code>            ""name"": ""GetMetadata"",
            ""description"": """",
            ""type"": ""GetMetadata"",
            ""dependsOn"": [
                {
                    ""activity"": ""Execute Previous Activity"",
                    ""dependencyConditions"": [
                        ""Succeeded""
                    ]
                }
            ],
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false,
                ""secureInput"": false
            },
            ""userProperties"": [],
            ""typeProperties"": {
                ""dataset"": {
                    ""referenceName"": ""Folder"",
                    ""type"": ""DatasetReference""
                },
                ""fieldList"": [
                    ""childItems""
                ]
            }
        },
</code></pre>
","<azure-data-factory>","2019-09-23 01:54:25","5108","4","3","58062788","<p>Unfortunatelly, there is not way to sort the order of ChildItems. I find this requirement quite strange, what is the scenario that you need you files sorted?</p>
"
"58054800","How does Get Metadata order/sort its output?","<p>I have set up a DataFactory pipeline that gets a list of files in Azure Data Lake Storage Gen2 then iterates over each files using a ForEach loop.</p>

<p>Im using a Get Metadata activity to produce the list of files and the argument its outputting is 'Child Items'.</p>

<p>I want to make sure the list (child items) is always sorted in name order. My question is what is the default sorting method for child items or can i sort this manually?</p>

<p>Thanks</p>

<pre><code>            ""name"": ""GetMetadata"",
            ""description"": """",
            ""type"": ""GetMetadata"",
            ""dependsOn"": [
                {
                    ""activity"": ""Execute Previous Activity"",
                    ""dependencyConditions"": [
                        ""Succeeded""
                    ]
                }
            ],
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false,
                ""secureInput"": false
            },
            ""userProperties"": [],
            ""typeProperties"": {
                ""dataset"": {
                    ""referenceName"": ""Folder"",
                    ""type"": ""DatasetReference""
                },
                ""fieldList"": [
                    ""childItems""
                ]
            }
        },
</code></pre>
","<azure-data-factory>","2019-09-23 01:54:25","5108","4","3","58070635","<p>You should probably try and refactor your process to take advantage of one of the real strengths of Azure Data Factory (ADF) which is the ability to process things in parallel.  What if you did <code>DELETE</code> based on the file / date / period instead of <code>TRUNCATE</code>?</p>

<p>I did get a sequential process to work using a Lookup to a database, a query with an <code>ORDER BY</code> clause to sort the output, and a <code>For Each</code> loop running in sequential mode, but this is counter to the strengths of ADF:</p>

<p><a href=""https://i.stack.imgur.com/mOk99.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mOk99.png"" alt=""ADF pattern""></a></p>
"
"58054800","How does Get Metadata order/sort its output?","<p>I have set up a DataFactory pipeline that gets a list of files in Azure Data Lake Storage Gen2 then iterates over each files using a ForEach loop.</p>

<p>Im using a Get Metadata activity to produce the list of files and the argument its outputting is 'Child Items'.</p>

<p>I want to make sure the list (child items) is always sorted in name order. My question is what is the default sorting method for child items or can i sort this manually?</p>

<p>Thanks</p>

<pre><code>            ""name"": ""GetMetadata"",
            ""description"": """",
            ""type"": ""GetMetadata"",
            ""dependsOn"": [
                {
                    ""activity"": ""Execute Previous Activity"",
                    ""dependencyConditions"": [
                        ""Succeeded""
                    ]
                }
            ],
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false,
                ""secureInput"": false
            },
            ""userProperties"": [],
            ""typeProperties"": {
                ""dataset"": {
                    ""referenceName"": ""Folder"",
                    ""type"": ""DatasetReference""
                },
                ""fieldList"": [
                    ""childItems""
                ]
            }
        },
</code></pre>
","<azure-data-factory>","2019-09-23 01:54:25","5108","4","3","64063010","<p>I've implemented the following solution to overcome the problem with get metadata default sorting order without a use of Azure Functions:</p>
<p><a href=""https://i.stack.imgur.com/Buk05.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Buk05.png"" alt=""ADF pipeline design"" /></a></p>
<ol>
<li>Get a list of items from the BLOB storage</li>
<li>Apply custom filtering (out-of-scope in your question context - just skip)</li>
<li>Apply a lookup activity, that basically receives the JSON representation of 1. Get Metadata activity, parses it using T-SQL Stored Procedure and returns batk as a sorted table representation of the input JSON (sorted in a descending manner)</li>
<li>For each activity start to iterate through the list from the top to down, starting with the most recented date folders and moving to the oldest ones</li>
</ol>
<p>Below you can find a:</p>
<p><strong>Configuration of the lookup activity from 3.</strong></p>
<p><a href=""https://i.stack.imgur.com/y0kso.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/y0kso.png"" alt=""Configuration of the lookup activity from 3."" /></a></p>
<p><strong>T-SQL Stored Procedure that transforms the output of Get Metadata activity to an input of ForEach activity.</strong></p>
<pre><code>ALTER PROCEDURE Tech.spSortBlobMetadata 
     @Json     NVARCHAR(MAX)
   , @SortOder VARCHAR(5)    = 'DESC'
   , @Debug    INT           = 0
AS

/***************************************************************************
     EXEC Tech.spSortBlobMetadata 
       '[{&quot;name&quot;:&quot;dt=2020-06-17&quot;,&quot;type&quot;:&quot;Folder&quot;},{&quot;name&quot;:&quot;dt=2020-06-18&quot;}]'
       , 'DESC'
       , 1
***************************************************************************/



    BEGIN

        DECLARE 
             @sqlTransform NVARCHAR(MAX) = 'SELECT *
FROM OPENJSON(@Json) WITH(name NVARCHAR(200) ''$.name'', type NVARCHAR(50) ''$.type'')
ORDER BY name ' + @SortOder



        IF @Debug = 0
            BEGIN


                EXEC sp_executesql 
                   @sqlTransform
                 , N'@Json nvarchar(max)'
                 , @json = @json
        END
            ELSE
            BEGIN
                SELECT @sqlTransform
        END


    END
</code></pre>
"
"58047662","enabling diagnostics in an Azure datafactory using arm template","<p>I would like to enable diagnostics for Azure Datafactory using ARM-Template.</p>

<p>I have found this <a href=""https://stackoverflow.com/questions/52975611/how-to-enable-diagnostics-in-an-azure-datafactory-after-creation"">post</a> and then I try to test my code:</p>

<pre><code>{
  ""$schema"": ""https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#"",
  ""contentVersion"": ""1.0.0.0"",
  ""parameters"": { },
  ""variables"": {},
  ""resources"": [
 {
        ""name"": ""test-adf2-we"",
        ""type"": ""Microsoft.DataFactory/factories"",
         ""apiVersion"": ""2018-06-01"",
         ""location"": ""West Europe"",
         ""identity"": {
           ""type"": ""SystemAssigned""
         }
 },
{
      ""name"": ""test-ala-we"",
      ""type"": ""Microsoft.OperationalInsights/workspaces"",
      ""apiVersion"": ""2015-11-01-preview"",
      ""location"": ""West Europe""
    },
    {
      ""type"": ""microsoft.datafactory/factories/providers/diagnosticsettings"",
      ""name"": ""[concat('test-adf2-we','/Microsoft.Insights/diagnostics')]"",
      ""location"": ""West Europe"",
      ""apiVersion"": ""2017-05-01-preview"",
      ""properties"": {
        ""name"": ""diagnostics"",
        ""storageAccountId"": null,
        ""eventHubAuthorizationRuleId"": null,
        ""eventHubName"": null,
        ""workspaceId"": ""/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/test-arm/providers/Microsoft.OperationalInsights/test-ala-we""
        ""logs"": [
          {
            ""category"": ""PipelineRuns"",
            ""enabled"": true,
            ""retentionPolicy"": {
              ""enabled"": false,
              ""days"": 0
            }
          },
          {
            ""category"": ""TriggerRuns"",
            ""enabled"": true,
            ""retentionPolicy"": {
              ""enabled"": false,
              ""days"": 0
            }
          },
          {
            ""category"": ""ActivityRuns"",
            ""enabled"": true,
            ""retentionPolicy"": {
              ""enabled"": false,
              ""days"": 0
            }
          }
        ],
        ""metrics"": [
          {
            ""category"": ""AllMetrics"",
            ""timeGrain"": ""PT1M"",
            ""enabled"": true,
            ""retentionPolicy"": {
              ""enabled"": false,
              ""days"": 0
            }
          }
        ]
      }
   }
}
</code></pre>

<p>After running this code. I get this error:</p>

<pre><code>Template deployment returned the following errors:
 Resource microsoft.datafactory/factories/providers/diagnosticsettings 'test-adf2-we/Microsoft.Insights/diagnostics' failed with message '{
  ""code"": ""BadRequest"",
   ""message"": ""\""Resource type 'microsoft.operationalinsights/test-ala-we' is invalid for property 'properties.workspaceId'. Expected types are 'microsoft.operationalinsights/workspaces'\""""
 }'
</code></pre>

<p>could you help me to fix this problem?</p>
","<azure><azure-data-factory><azure-rm-template><azure-diagnostics>","2019-09-22 09:01:31","1069","0","2","58048672","<p>your resourceId is wrong, fix it like so:</p>

<pre><code>""/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/test-arm/providers/Microsoft.OperationalInsights/workspaces/test-ala-we""
</code></pre>

<p>you left out <code>workspaces</code> (and this is what the error is telling you).</p>
"
"58047662","enabling diagnostics in an Azure datafactory using arm template","<p>I would like to enable diagnostics for Azure Datafactory using ARM-Template.</p>

<p>I have found this <a href=""https://stackoverflow.com/questions/52975611/how-to-enable-diagnostics-in-an-azure-datafactory-after-creation"">post</a> and then I try to test my code:</p>

<pre><code>{
  ""$schema"": ""https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#"",
  ""contentVersion"": ""1.0.0.0"",
  ""parameters"": { },
  ""variables"": {},
  ""resources"": [
 {
        ""name"": ""test-adf2-we"",
        ""type"": ""Microsoft.DataFactory/factories"",
         ""apiVersion"": ""2018-06-01"",
         ""location"": ""West Europe"",
         ""identity"": {
           ""type"": ""SystemAssigned""
         }
 },
{
      ""name"": ""test-ala-we"",
      ""type"": ""Microsoft.OperationalInsights/workspaces"",
      ""apiVersion"": ""2015-11-01-preview"",
      ""location"": ""West Europe""
    },
    {
      ""type"": ""microsoft.datafactory/factories/providers/diagnosticsettings"",
      ""name"": ""[concat('test-adf2-we','/Microsoft.Insights/diagnostics')]"",
      ""location"": ""West Europe"",
      ""apiVersion"": ""2017-05-01-preview"",
      ""properties"": {
        ""name"": ""diagnostics"",
        ""storageAccountId"": null,
        ""eventHubAuthorizationRuleId"": null,
        ""eventHubName"": null,
        ""workspaceId"": ""/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/test-arm/providers/Microsoft.OperationalInsights/test-ala-we""
        ""logs"": [
          {
            ""category"": ""PipelineRuns"",
            ""enabled"": true,
            ""retentionPolicy"": {
              ""enabled"": false,
              ""days"": 0
            }
          },
          {
            ""category"": ""TriggerRuns"",
            ""enabled"": true,
            ""retentionPolicy"": {
              ""enabled"": false,
              ""days"": 0
            }
          },
          {
            ""category"": ""ActivityRuns"",
            ""enabled"": true,
            ""retentionPolicy"": {
              ""enabled"": false,
              ""days"": 0
            }
          }
        ],
        ""metrics"": [
          {
            ""category"": ""AllMetrics"",
            ""timeGrain"": ""PT1M"",
            ""enabled"": true,
            ""retentionPolicy"": {
              ""enabled"": false,
              ""days"": 0
            }
          }
        ]
      }
   }
}
</code></pre>

<p>After running this code. I get this error:</p>

<pre><code>Template deployment returned the following errors:
 Resource microsoft.datafactory/factories/providers/diagnosticsettings 'test-adf2-we/Microsoft.Insights/diagnostics' failed with message '{
  ""code"": ""BadRequest"",
   ""message"": ""\""Resource type 'microsoft.operationalinsights/test-ala-we' is invalid for property 'properties.workspaceId'. Expected types are 'microsoft.operationalinsights/workspaces'\""""
 }'
</code></pre>

<p>could you help me to fix this problem?</p>
","<azure><azure-data-factory><azure-rm-template><azure-diagnostics>","2019-09-22 09:01:31","1069","0","2","59378741","<p>Be sure to add</p>

<pre><code>""logAnalyticsDestinationType"": ""Dedicated""
</code></pre>

<p>Otherwise Log Analytics will write to the Default AzureDiagnostic Tables.  This is <a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/platform/resource-logs-collect-workspace"" rel=""nofollow noreferrer"">Microsoft Best Practice</a> when using Log Analytics since there is a hard stop at 500 columns in the AzureDiagnostics table.  Once the 500 column threshold is met records won't be inserted into the table.</p>
"
"58027104","Can not use expression language functions from Events Trigger body","<p>I am using events trigger in ADF v2. My trigger parameter values look like this:-</p>

<pre><code>            ""parameters"": {
                ""FolderPath"": ""@triggerBody().folderPath"",
                ""FileName"": ""@triggerBody().fileName"",
                ""Year"": ""@{split(@triggerBody().folderPath,'/')[2]}"",
                ""Month"": ""@{split(@triggerBody().folderPath,'/')[3]}"",
                ""Day"": ""@{split(@triggerBody().folderPath,'/')[4]}"",
            }
</code></pre>

<p>I initially ran it with just 2 initial parameters and the sample values for FolderPath look like this as seen from ADF monitor after a run is succeeded:-</p>

<pre><code>test/poc/2019/09/20/00
</code></pre>

<p>But when I add other parameters to it, like Year, Month, Day as shown here, while publishing the changes, I get an error 'Trigger activation failed':- </p>

<p><a href=""https://i.stack.imgur.com/fAnpz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fAnpz.png"" alt=""enter image description here""></a></p>

<p>When I hard coded Year, Month, Day in the trigger, it again started working. I am guessing some issues in calling expression language functions from event based trigger. I am basically trying to dynamically calculate individual elements from FolderPath such as 2019 as Year, 09 as Month and so on. How to achieve this?</p>
","<azure-data-factory>","2019-09-20 11:00:35","237","0","1","58037118","<p>Pasting DraganB's comment as an answer here:-</p>

<p>Try </p>

<pre><code>""Year"": ""@{split(triggerBody().folderPath,'/')[2]}"", ""Month"": ""@{split(triggerBody().folderPath,'/')[3]}"", ""Day"": ""@{split(triggerBody().folderPath,'/')[4]}"" 
</code></pre>

<p>Basically delete @ before triggerBody</p>
"
"58026776","How to resolve no access in Azure Data Factory 2 to Azure DevOps Repository","<p>I am facing an issue on Azure Data Factory when I try to access the Azure DevOps Git Repository that I configured. The error message is: ""Invalid GIT configuration. You need to gain access to the repository before you can publish any changes. Details: Authentication error - you do not have access to the provided Azure DevOps account.""</p>

<p>I am using the same account on both DevOps and Data Factory. My Azure portal access is ""Contributor"" at the subscription level and my DevOps role is Project Administrator on the project. </p>

<p>Regards,
Tania</p>

<p>I've tried various combinations of creating the repository as new from Data Factory as well as using an existing one created (by myself) in Azure DevOps.</p>

<p>From <a href=""https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles</a> in the Open Feedback items a user mentions that ""the Build-in role Data Factory Contributor this is not in Allow part:
Action:Microsoft.DataFactory/locations/configureFactoryRepo/action"" 
I investigated the the contributor role and it appears to have the Action allowed as ""Read"" ?</p>

<p><a href=""https://i.stack.imgur.com/Tgsk8.png"" rel=""nofollow noreferrer"">ADF Repo Settings</a></p>

<p><a href=""https://i.stack.imgur.com/4NC3T.png"" rel=""nofollow noreferrer"">ADF Repo Error</a></p>
","<azure><azure-devops><azure-data-factory>","2019-09-20 10:40:02","11971","4","3","58096227","<p>After the detailed message shared from @taniaw, this issue caused by the account confused.</p>

<p>Here has 2 accounts, <code>A@abc.com</code>, and <code>A@efg.com</code>.</p>

<p>When login to azure portal with <code>A@abc.com</code>, and configure the <strong>Azure Devops Git</strong>. It's all correct. Now, the issue is when go <strong>Azure Devops Git</strong> tab after the repos configured successfully, its access account used in the backend changed into <code>A@efg.com</code>. But this account does not has permission to access the organization and repos. That's why when click the Azure Devops Git tab, it is grey and receiving the error message ""You do not access to the repository"". </p>

<p>These detailed message can all be viewed by <a href=""https://www.telerik.com/download/fiddler"" rel=""nofollow noreferrer"">Fiddler</a>. Everyone can use fiddler trace to know the exactly error caused then analyze it.</p>

<p>At last, the solution is this is the account mapping confused issue which need Azure Account Team support.</p>
"
"58026776","How to resolve no access in Azure Data Factory 2 to Azure DevOps Repository","<p>I am facing an issue on Azure Data Factory when I try to access the Azure DevOps Git Repository that I configured. The error message is: ""Invalid GIT configuration. You need to gain access to the repository before you can publish any changes. Details: Authentication error - you do not have access to the provided Azure DevOps account.""</p>

<p>I am using the same account on both DevOps and Data Factory. My Azure portal access is ""Contributor"" at the subscription level and my DevOps role is Project Administrator on the project. </p>

<p>Regards,
Tania</p>

<p>I've tried various combinations of creating the repository as new from Data Factory as well as using an existing one created (by myself) in Azure DevOps.</p>

<p>From <a href=""https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles</a> in the Open Feedback items a user mentions that ""the Build-in role Data Factory Contributor this is not in Allow part:
Action:Microsoft.DataFactory/locations/configureFactoryRepo/action"" 
I investigated the the contributor role and it appears to have the Action allowed as ""Read"" ?</p>

<p><a href=""https://i.stack.imgur.com/Tgsk8.png"" rel=""nofollow noreferrer"">ADF Repo Settings</a></p>

<p><a href=""https://i.stack.imgur.com/4NC3T.png"" rel=""nofollow noreferrer"">ADF Repo Error</a></p>
","<azure><azure-devops><azure-data-factory>","2019-09-20 10:40:02","11971","4","3","58606309","<p>This issue was resolved today. The cause was a different Azure DevOps tenant where my account had been added as a guest had used an email account instead of my Azure AD account and this caused the confusion when passing credentials from Azure Data Factory to Azure DevOps. The issue was resolved by leaving the organization that had my user mis-configured as I'm no longer actively working on that project. A better solution may have been to have their Azure AD administrator update my guest account to the correct details.</p>
"
"58026776","How to resolve no access in Azure Data Factory 2 to Azure DevOps Repository","<p>I am facing an issue on Azure Data Factory when I try to access the Azure DevOps Git Repository that I configured. The error message is: ""Invalid GIT configuration. You need to gain access to the repository before you can publish any changes. Details: Authentication error - you do not have access to the provided Azure DevOps account.""</p>

<p>I am using the same account on both DevOps and Data Factory. My Azure portal access is ""Contributor"" at the subscription level and my DevOps role is Project Administrator on the project. </p>

<p>Regards,
Tania</p>

<p>I've tried various combinations of creating the repository as new from Data Factory as well as using an existing one created (by myself) in Azure DevOps.</p>

<p>From <a href=""https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles</a> in the Open Feedback items a user mentions that ""the Build-in role Data Factory Contributor this is not in Allow part:
Action:Microsoft.DataFactory/locations/configureFactoryRepo/action"" 
I investigated the the contributor role and it appears to have the Action allowed as ""Read"" ?</p>

<p><a href=""https://i.stack.imgur.com/Tgsk8.png"" rel=""nofollow noreferrer"">ADF Repo Settings</a></p>

<p><a href=""https://i.stack.imgur.com/4NC3T.png"" rel=""nofollow noreferrer"">ADF Repo Error</a></p>
","<azure><azure-devops><azure-data-factory>","2019-09-20 10:40:02","11971","4","3","62087009","<p>Verify the User configuration at Organization level: 
The default is ""Stakeholder"" - it should be ""Basic"" actually.
Hope it is useful.</p>
"
"58024798","Which is the best way to use a Upsert method on Azure Data Factory?","<p>I have some csv files stored in a Blob Storage. Each csv gets updated every day. That update consist in the insertion of some new rows and the modification of some old rows. I'm using Azure Data Factory (v2) to get that data from the Blob storage and sink it on a SQL database.</p>
<p>The problem is that my process takes around 15 minutes to finish, so I suspect that I'm not following the BEST PRACTICES.</p>
<p>I don't know how exactly works the &quot;Upsert&quot; sink method. But I think this method needs a boolean condition that indicates if you want to Update that row (if true) or insert that row (if false).</p>
<p>I get that condition using a column that I get by making a join of the csv (origin) with the ddbb (destiny). Making it this way you will get a &quot;null&quot; if the row is a new one, and a &quot;not null&quot; if the row exists on the ddbb already. So I insert the rows with that &quot;null&quot; value and the other ones I just update them.</p>
<p>This is the best/correct way to do this kind of upsert methods? Could I do something better to improve my times?</p>
","<insert><insert-update><upsert><azure-data-factory>","2019-09-20 08:37:20","9326","1","1","58032622","<p>Are you using Data Flows? If so, you can update your SQL DB using upsert or separate insert/update paths. Set the policy for which values you wish to update in an Alter Row transformation, then set the Sink for Upsert, Update, and/or Insert. You will need to identify the key column on your sink that we will use as the update key on your database.</p>
"
"58023383","How to trigger a Azure data factory pipeline with multiple parameter from postman","<p>I have created a Azure Data Factory pipeline which have multiple pipeline parameter,which I need to enter all the time when pipeline trigger.Now I want to trigger this pipeline from  postman in my local system and i need to pass  parameters to pipeline from post.</p>
","<azure><rest><postman><azure-api-management><azure-data-factory>","2019-09-20 07:07:52","3408","4","3","58028582","<p>Do you really need to use postman? I've posted examples of doing this with Powershell and with Python.</p>

<p>Powershell: <a href=""https://stackoverflow.com/questions/53756570/how-to-pass-arguments-to-adf-pipeline-using-powershell"">How to pass arguments to ADF pipeline using powershell</a></p>

<p>Python: <a href=""https://gist.github.com/Gorgoras/1fe534fd9b454412f81c8203c773c483"" rel=""nofollow noreferrer"">https://gist.github.com/Gorgoras/1fe534fd9b454412f81c8203c773c483</a></p>

<p>If your only option is to use the rest api, you can read about it and get some examples here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-rest-api"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-rest-api</a></p>

<p>Hope this helped!!</p>
"
"58023383","How to trigger a Azure data factory pipeline with multiple parameter from postman","<p>I have created a Azure Data Factory pipeline which have multiple pipeline parameter,which I need to enter all the time when pipeline trigger.Now I want to trigger this pipeline from  postman in my local system and i need to pass  parameters to pipeline from post.</p>
","<azure><rest><postman><azure-api-management><azure-data-factory>","2019-09-20 07:07:52","3408","4","3","58390331","<p>You can trigger Azure Data Factory via a policy in API Management.</p>

<p>I've added a sample here: <a href=""https://github.com/Azure/api-management-policy-snippets/blob/master/examples/Trigger%20Azure%20Data%20Factory%20Pipeline%20With%20Parameters.policy.xml"" rel=""nofollow noreferrer"">https://github.com/Azure/api-management-policy-snippets/blob/master/examples/Trigger%20Azure%20Data%20Factory%20Pipeline%20With%20Parameters.policy.xml</a></p>
"
"58023383","How to trigger a Azure data factory pipeline with multiple parameter from postman","<p>I have created a Azure Data Factory pipeline which have multiple pipeline parameter,which I need to enter all the time when pipeline trigger.Now I want to trigger this pipeline from  postman in my local system and i need to pass  parameters to pipeline from post.</p>
","<azure><rest><postman><azure-api-management><azure-data-factory>","2019-09-20 07:07:52","3408","4","3","73470206","<p>Azure Docs doesn't provide examples on how to pass a parameter which I find weird also nowhere else on the internet have I found an example of how to pass multiple parameters via REST API, I guess most people use ADF shell to trigger it or python script.</p>
<p>Anyway, if someone else stumbles on the same question then here's the solution (which is quite simple).</p>
<p>Firstly, Create an Azure App Registration and generate client ID and client secret value.</p>
<p>Authenticate via REST API to get the Bearer Token</p>
<pre><code>curl --location --request POST 'https://login.microsoftonline.com/${TENANT_ID}/oauth2/token' \
--form 'grant_type=&quot;client_credentials&quot;' \
--form 'client_id=&quot;${CLIENT_ID}&quot;' \
--form 'client_secret=&quot;${CLIENT_SECRET_VALUE}&quot;' \
--form 'resource=&quot;https://management.azure.com/&quot;'
</code></pre>
<p>The response will contain a Bearer token, use it to trigger the pipeline. Replace subscription id, resource group name, and adf name.</p>
<pre><code>curl --location --request POST 'https://management.azure.com/subscriptions/${SUBSCRIPTION_ID}/resourceGroups/${RESOURCE_GROUP_NAME}/providers/Microsoft.DataFactory/factories/${ADF_NAME}/pipelines/trigger-pipeline-from-rest/createRun?api-version=2018-06-01' \
--header 'Authorization: Bearer ${BEARER_TOKEN}' \
--header 'Content-Type: application/json' \
--data-raw '{
    &quot;date&quot;:&quot;2022-08-22&quot;,
    &quot;param1&quot;:&quot;param1 value&quot;,
    &quot;param2&quot;:&quot;some-value&quot;
}'
</code></pre>
<p>Note: The app should have contributor access to ADF to trigger the pipeline.</p>
"
"58006613","unable to use pagination rules in Azure data factory","<p>I am trying to use the Pagination rules in the ADF rest source to download all my pages data. My API looks something like this: www.xyz.com?page=0 and i get the response like this : 
{content:[],</p>

<p>totalPages:""150"",</p>

<p>page:0}
Here is the pagination rule that i am trying to write to fetch the next page data</p>

<p>QueryParameters.page = @add(int($.page),1) but this is reflecting an error saying UnsupportedConcept and showing invalid.</p>

<p>Please tell if there is any way to increment the value of a response and hit the api again</p>
","<azure><azure-data-factory>","2019-09-19 08:15:24","1687","0","1","58032187","<p>You can create an until loop.</p>

<p>Create a pipeline that contains a variable that will be evaluated by the until activity and a call to your api to get the total number of pages (or the 1st batch of data). 
Then, inside the Until activity, you run it until your @equals(pipeline.variable, activity('1stcall').output.totalPages).</p>

<p>The activities inside the until should be a 
-Another web call to your appy
-A set variable activity increasing the value of the page variable.</p>
"
"58005873","Azure data factory- Azure function to connect sql database does not work","<p>I have a copy activity in ADF. On completion I wanted to update a table. 
Stored proc is not an option for us. 
Would like to know all other options to run the update query post copy.
Tried Azure function to connect to sql database and update the table. 
Works fine separately with time trigger . 
What trigger to be used for the Azure function for this scenario in ADF?</p>

<p>Thanks in advance,
Aruna</p>
","<c#><azure><azure-sql-database><azure-functions><azure-data-factory>","2019-09-19 07:29:29","699","0","2","58007279","<p>Per my understanding, you want to execute a function to update a table after a copy activity and you don't want to use time trigger. Currently Azure function supported as a step in Azure Data Factory pipelines. So we just need to drag the azure function to pipeline in Azure Data Factory and provide the info of your function(such as the name, url...).</p>

<p><a href=""https://i.stack.imgur.com/802fo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/802fo.png"" alt=""enter image description here""></a></p>

<p>For further information, you can refer to this tutorial: <a href=""https://azure.microsoft.com/en-us/blog/azure-functions-now-supported-as-a-step-in-azure-data-factory-pipelines/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/blog/azure-functions-now-supported-as-a-step-in-azure-data-factory-pipelines/</a></p>
"
"58005873","Azure data factory- Azure function to connect sql database does not work","<p>I have a copy activity in ADF. On completion I wanted to update a table. 
Stored proc is not an option for us. 
Would like to know all other options to run the update query post copy.
Tried Azure function to connect to sql database and update the table. 
Works fine separately with time trigger . 
What trigger to be used for the Azure function for this scenario in ADF?</p>

<p>Thanks in advance,
Aruna</p>
","<c#><azure><azure-sql-database><azure-functions><azure-data-factory>","2019-09-19 07:29:29","699","0","2","58022869","<p>As a work-a-round: within lookup you can define a sql query and execute quiete a lot of logic. There is a limit of ~2MB of data returned on the lookup (not listed as a limitation of ADF).</p>
"
"57996929","Filename as Column using Data Factory V2","<p>I have a lot of JSON files in Blob Storage and what I would like to do is to load the JSON files via Data factoryV2 into SQL Data Warehouse. I would like the filename in a column for each JSON file. I know how to 
do this in SSIS but I am not sure how to replicate this in Data Factory.</p>

<p>e.g File Name: CornerShop.csv as CornerShop in the filename column in SQL Data Warehouse</p>
","<azure-data-factory>","2019-09-18 16:12:56","3389","2","1","58006021","<p>Firstly,please see the limitation in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#alternative-column-mapping"" rel=""nofollow noreferrer"">copy activity column mapping</a>:</p>

<ul>
<li>Source data store query result does not have a column name that is
specified in the input dataset ""structure"" section.</li>
<li>Sink data store (if with pre-defined schema) does not have a column
name that is specified in the output dataset ""structure"" section.</li>
<li>Either fewer columns or more columns in the ""structure"" of sink
dataset than specified in the mapping.</li>
<li>Duplicate mapping.</li>
</ul>

<p>So,i don't think you could do the data transfer plus file name at one time.My idea is:</p>

<p>1.First use a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity#get-a-folders-metadata"" rel=""nofollow noreferrer"">GetMetadata</a> activity.  It should get the filepaths of each file you want to copy.  Use the ""Child Items"" in the Field list.</p>

<p>2.On success of GetMetaData activity, do <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">ForEach activity</a>.  For the ForEach activity's Items, pass the list of filepaths.</p>

<p>3.Inside the ForEach activity's Activities, place the Copy activity. Reference the iterated item by <code>@item()</code> or <code>@item().name</code>  on the blob storage source file name.</p>

<p>4.Meanwhile,configure the filename as a parameter into stored procedure. In the stored procedure, merge the filename into <code>fileName</code> column.</p>
"
"57991361","How to reference a linked service in an expression","<p>I want to reference a property of a linked service inside an expression.</p>

<p>How do I do something like:</p>

<pre><code>@linkedservice(""service_name"").properties
</code></pre>
","<azure-data-factory>","2019-09-18 11:08:51","209","0","1","57994922","<p>What do you want to achieve doing this? </p>

<p>If you want to parameterize connections to data stores, try something like this: <a href=""https://azure.microsoft.com/en-us/blog/parameterize-connections-to-your-data-stores-in-azure-data-factory/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/blog/parameterize-connections-to-your-data-stores-in-azure-data-factory/</a> This uses expressions inside the linked services.</p>

<p>Hope this helped!!</p>
"
"57988005","Azure Data Factory: Start of SSIS-Integration Runtime takes very long time","<p>Our problem is that Start / Stop of the Integration Runtime takes 4 to 6 minutes!
When checking the screenshot in the docs post below the start process takes 3 seconds.</p>

<p>What can be the cause of the poor performance?</p>

<p>We are evaluating the SSIS runtime in Azure datafactory. The integration runtime (IR) is quite costly and we want to keep costs down by starting and stopping the IR.
There are several blogs on how to achieve this including the Microsoft docs one:
<a href=""https://learn.microsoft.com/sv-se/azure/data-factory/how-to-schedule-azure-ssis-integration-runtime#test-run-your-pipelines"" rel=""nofollow noreferrer"">https://learn.microsoft.com/sv-se/azure/data-factory/how-to-schedule-azure-ssis-integration-runtime#test-run-your-pipelines</a></p>

<pre class=""lang-bsh prettyprint-override""><code># Example in powershell

Write-Output (Get-Date)
Stop-AzDataFactoryV2IntegrationRuntime -Name XXXXXXIntegrationRuntime -ResourceGroupName XXXXXX -DataFactoryName XXXXXXDataFactory
Write-Output (Get-Date)

#result
Wednesday, 18 September 2019 08:20:33
Wednesday, 18 September 2019 08:24:54
</code></pre>
","<ssis><azure-data-factory>","2019-09-18 08:11:17","1305","1","1","57995136","<p>That command just sends the order to stop the IR, but it doesn't wait until it is actually stopped. Try using Get-AzDataFactoryV2IntegrationRuntime and check the status. </p>

<p>You are not doing anything wrong with the IR, it takes like 5-10 mins to start and stop (stop is usually faster in my experience). As far as I know, there is no way to make it start or stop faster, so you should build your scripts to check the status periodically and wait for the IR to be up and running.</p>

<p>Hope this helped!</p>
"
"57986999","Automate Azure Data Factory triggers using release pipeline by using powershell task","<p>I am new on DevOps, i am trying to change Azure Data Factory triggers starttime and endtime properties in azure data factory at deployment time using powershell.</p>

<p>I found this <a href=""https://learn.microsoft.com/en-us/powershell/module/az.datafactory/set-azdatafactoryv2trigger?view=azps-2.6.0"" rel=""nofollow noreferrer"">link</a> and tried to follow in PowerShell. I am getting following error when i am running below command.</p>

<pre><code>PS C:\&gt; Set-AzDataFactoryV2Trigger -ResourceGroupName ""ADF"" -DataFactoryName ""WikiADF"" -Name ""ScheduledTrigger"" -DefinitionFile "".\scheduledTrigger.json""

Set-AzureRmDataFactoryV2 : The term 'Set-AzureRmDataFactoryV2' is not recognized as the name of a cmdlet, function, script file, or operable program.
Check the spelling of the name, or if a path was included, verify that the path is correct and try again.

At line:1 char:16
+ $DataFactory = Set-AzureRmDataFactoryV2 -ResourceGroupName $ResGrp.Re ...
+                ~~~~~~~~~~~~~~~~~~~~~~~~
+ CategoryInfo          : ObjectNotFound: (Set-AzureRmDataFactoryV2:String) [], CommandNotFoundException
+ FullyQualifiedErrorId : CommandNotFoundException
</code></pre>

<p>I searched about this issue and found this issue as a bug in <a href=""https://github.com/MicrosoftDocs/azure-docs/issues/30971"" rel=""nofollow noreferrer"">Microsoft official github account</a></p>

<p>If anyone able to resolved this issue or have already resolved, please help me.</p>

<p><strong><em>Note: Azure DataFactory Triggers tag is not available in StackOverflow, so i am not able to add it.</em></strong></p>
","<powershell><azure-pipelines-release-pipeline><release-management><azure-data-factory>","2019-09-18 07:11:30","679","0","2","58003981","<p>Very agreed with Nick's opinion:</p>

<blockquote>
  <p>Any time youe see The term 'xyz' is not recognized as the name of a
  cmdlet in powershell, it means you haven't installed the module.</p>
</blockquote>

<p>Actually, in your issue, the error also caused by this reason. I think you should haven't installed the <code>Az</code> module for your Powershell.</p>

<p>The <code>Az</code> module is a rollup module for the Azure PowerShell cmdlets. Installing it can download all of the available Azure Resource Manager modules, include the <code>Set-AzDataFactoryV2Trigger</code> you are using, and makes their cmdlets available for use.</p>

<p>Try this command to install the <code>Az</code> module for your Powershell:</p>

<pre><code>Install-Module -Name Az -AllowClobber -Scope CurrentUser
</code></pre>

<p><a href=""https://i.stack.imgur.com/kUw01.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kUw01.png"" alt=""enter image description here""></a></p>

<p>Since as default, the <code>PowerShell</code> gallery isn't configured as a trusted repository for <code>PowerShellGet</code>, the first time you use the <code>PSGallery</code> you would see the prompt which need you ensure if you want to get the <code>Az</code> module from PSGallery. Just answer Yes or Yes to All is ok.</p>

<p>Then, you can execute the following command to check the Az module: </p>

<pre><code> Get-InstalledModule -Name Az
</code></pre>

<p>When you see the following message, it means the <code>Az</code> module has exists in the Powershell:</p>

<p><a href=""https://i.stack.imgur.com/pPBOJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pPBOJ.png"" alt=""enter image description here""></a></p>

<p>Now, try this <a href=""https://learn.microsoft.com/en-us/powershell/module/az.datafactory/set-azdatafactoryv2trigger?view=azps-2.6.0"" rel=""nofollow noreferrer"">link</a> again, it would be succeed.</p>
"
"57986999","Automate Azure Data Factory triggers using release pipeline by using powershell task","<p>I am new on DevOps, i am trying to change Azure Data Factory triggers starttime and endtime properties in azure data factory at deployment time using powershell.</p>

<p>I found this <a href=""https://learn.microsoft.com/en-us/powershell/module/az.datafactory/set-azdatafactoryv2trigger?view=azps-2.6.0"" rel=""nofollow noreferrer"">link</a> and tried to follow in PowerShell. I am getting following error when i am running below command.</p>

<pre><code>PS C:\&gt; Set-AzDataFactoryV2Trigger -ResourceGroupName ""ADF"" -DataFactoryName ""WikiADF"" -Name ""ScheduledTrigger"" -DefinitionFile "".\scheduledTrigger.json""

Set-AzureRmDataFactoryV2 : The term 'Set-AzureRmDataFactoryV2' is not recognized as the name of a cmdlet, function, script file, or operable program.
Check the spelling of the name, or if a path was included, verify that the path is correct and try again.

At line:1 char:16
+ $DataFactory = Set-AzureRmDataFactoryV2 -ResourceGroupName $ResGrp.Re ...
+                ~~~~~~~~~~~~~~~~~~~~~~~~
+ CategoryInfo          : ObjectNotFound: (Set-AzureRmDataFactoryV2:String) [], CommandNotFoundException
+ FullyQualifiedErrorId : CommandNotFoundException
</code></pre>

<p>I searched about this issue and found this issue as a bug in <a href=""https://github.com/MicrosoftDocs/azure-docs/issues/30971"" rel=""nofollow noreferrer"">Microsoft official github account</a></p>

<p>If anyone able to resolved this issue or have already resolved, please help me.</p>

<p><strong><em>Note: Azure DataFactory Triggers tag is not available in StackOverflow, so i am not able to add it.</em></strong></p>
","<powershell><azure-pipelines-release-pipeline><release-management><azure-data-factory>","2019-09-18 07:11:30","679","0","2","64473093","<p>You seemed to use PowerShell task instead of Azure PowerShell task (<a href=""https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-powershell?view=azure-devops"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-powershell?view=azure-devops</a>) in your release pipeline.</p>
<p>You won't have any issue if using Azure PowerShell task.</p>
"
"57982516","Azure Data Factory: Reading doubles with a comma as decimal separator instead of a dot","<p>I'm currently trying to read from a CSV files ( separated with semicolon ';') with decimal numbers formatted with a comma(,) as a decimal separator instead of a dot (.).</p>

<p>i.e: the number 12356.12 is stored as 12356,12.</p>

<p>In the source's projection, what would be the correct format to read the value correctly?</p>

<p>The format should in Java Decimal Format</p>

<p><a href=""https://i.stack.imgur.com/BQvbf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BQvbf.png"" alt=""enter image description here""></a></p>
","<formatting><azure-data-factory>","2019-09-17 21:51:32","3516","1","2","58008218","<p>If your CSV file's columnDelimiter is a comma (','), your first concern is how to avoid your number data won't be treated as different columns. Since your number data is stored as <code>12356,12</code>, so my suggests as below :</p>

<ol>
<li>Change the columnDelimiter as <code>|</code> or other special characters. </li>
</ol>

<p>2.Set escape char. Please see this description:</p>

<p><a href=""https://i.stack.imgur.com/UDYYR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UDYYR.png"" alt=""enter image description here""></a> </p>

<p>In addition, 12356,12 can't be identified as Decimal format in ADF automatically. And no such mechanism o turn <code>,</code> into <code>.</code>. So I think you need to transfer data as <code>string</code> temporary. Then convert it into Decimal in your destination with java code.</p>
"
"57982516","Azure Data Factory: Reading doubles with a comma as decimal separator instead of a dot","<p>I'm currently trying to read from a CSV files ( separated with semicolon ';') with decimal numbers formatted with a comma(,) as a decimal separator instead of a dot (.).</p>

<p>i.e: the number 12356.12 is stored as 12356,12.</p>

<p>In the source's projection, what would be the correct format to read the value correctly?</p>

<p>The format should in Java Decimal Format</p>

<p><a href=""https://i.stack.imgur.com/BQvbf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BQvbf.png"" alt=""enter image description here""></a></p>
","<formatting><azure-data-factory>","2019-09-17 21:51:32","3516","1","2","72828238","<p>True answer is in the comments: In the copy job the culture can be defined, which influences the decimal separator. Go to &quot;mapping&quot; &gt; &quot;Type conversion settings&quot; &gt; &quot;culture&quot; and chose en-us, de-de or whatever works for you. Be aware that this will also influence other types like dates.</p>
"
"57979705","Using ADF for REST API","<p>We want to use ADF to pull data from a REST API service. However the service uses OAuth and needs an access token to be passed. </p>

<p>Would you know if this is possible using ADFv2 ? If yes how ?</p>
","<azure><azure-data-factory>","2019-09-17 17:58:58","925","0","1","57986530","<p>I suppose you want to call the <a href=""https://learn.microsoft.com/en-us/rest/api/azure/"" rel=""nofollow noreferrer""><code>Azure REST API</code></a> in the ADFv2, if so, you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer""><code>Web activity in Azure Data Factory</code></a>. </p>

<p>You have not mentioned which REST API you want to use, here is a sample for you to refer.</p>

<p>In my sample, we call the api <a href=""https://learn.microsoft.com/en-us/rest/api/compute/virtualmachines/list"" rel=""nofollow noreferrer""><code>Virtual Machines - List</code></a> to list all the VMs in the specified resource group.</p>

<p>1.The web activity does not support bearer token auth, we need to use MSI auth in this case. Navigate to your subscription in the portal -> <code>Access control (IAM)</code> -> <code>Add</code> -> <code>Add role assignment</code> -> search for the name of your ADFv2 and add it as an <code>Owner/Contributor</code> role in the subscription. More details see this <a href=""https://learn.microsoft.com/en-us/azure/role-based-access-control/quickstart-assign-role-user-portal"" rel=""nofollow noreferrer"">link</a>. <strong>Note</strong> your account need to be an <code>Onwer</code> of the subscription to do this step.</p>

<p>2.In the pipline of your ADFv2, create a <code>web</code> activity. Set the settings as below.</p>

<p>The <code>URL</code> is <code>https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Compute/virtualMachines?api-version=2019-03-01</code>, change the parameters to yours. Select the <code>Authentication</code> with <code>MSI</code>, the <code>Resource</code> is <code>https://management.azure.com/</code>. <strong>Note</strong> the resource depends on the specific api you want to call.</p>

<p><a href=""https://i.stack.imgur.com/f5ncO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f5ncO.png"" alt=""enter image description here""></a></p>

<p>3.Publish it and test it, it works fine on my side.</p>

<p><a href=""https://i.stack.imgur.com/2rKx5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2rKx5.png"" alt=""enter image description here""></a></p>
"
"57971647","Passing secrets to web activity in Azure Data Factory","<p>I am using web activity to get a bearer token from an authentication endpoint. I need to pass secrets such as user id and password in the header as below.</p>

<p><a href=""https://i.stack.imgur.com/cafNs.png"" rel=""nofollow noreferrer"">Headers in web activity</a></p>

<p>I want to fetch user id and password from Azure key vault and pass it to web activity. Is there a way I can access the key vault in the expression? </p>

<p>Currently, I am fetching the secrets from an azure function and passing them as parameters to the pipeline. </p>
","<azure-keyvault><azure-data-factory>","2019-09-17 09:48:07","544","3","1","58853897","<p>Currently you can´t access directly to Azure Key vault.
You can create a json file like:</p>

<pre><code>{
  ""IC-USER-ID"": ""abc"",
  ""IC-USER-PASSWORD"": ""abc123"",
}
</code></pre>

<p>And read from it. In release process you can read the values from key vault and update json file. You have to ensure that only the right person can access the file.</p>
"
"57963514","Azure Data Factory pipeline into compressed Parquet file: “java.lang.OutOfMemoryError:Java heap space”","<p>I have a pipeline that reads data from an MS SQL Server and stores them into a file in a <strong>BLOB</strong> container in Azure Storage. The file has Parquet (or Apache Parquet, as it is also called) format. </p>

<p>So, when the “sink” (output) file is stored in a compressed way (snappy, or gzip – does not matter) AND the file is large enough (more than 50 Mb), the pipeline failed. The message was the following:</p>

<pre><code>""errorCode"": ""2200"",
    ""message"": ""Failure happened on 'Sink' side. 
ErrorCode=UserErrorJavaInvocationException,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=An error occurred when invoking java, message: java.lang.OutOfMemoryError:Java heap space\ntotal entry:11\r\njava.util.ArrayDeque.doubleCapacity(Unknown Source)\r\njava.util.ArrayDeque.addFirst(Unknown Source)\r\njava.util.ArrayDeque.push(Unknown Source)\r\norg.apache.parquet.io.ValidatingRecordConsumer.endField(ValidatingRecordConsumer.java:108)\r\norg.apache.parquet.example.data.GroupWriter.writeGroup(GroupWriter.java:58)\r\norg.apache.parquet.example.data.GroupWriter.write(GroupWriter.java:37)\r\norg.apache.parquet.hadoop.example.GroupWriteSupport.write(GroupWriteSupport.java:87)\r\norg.apache.parquet.hadoop.example.GroupWriteSupport.write(GroupWriteSupport.java:37)\r\norg.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:123)\r\norg.apache.parquet.hadoop.ParquetWriter.write(ParquetWriter.java:292)\r\ncom.microsoft.datatransfer.bridge.parquet.ParquetBatchWriter.addRows(ParquetBatchWriter.java:60)\r\n,Source=Microsoft.DataTransfer.Common,''Type=Microsoft.DataTransfer.Richfile.JniExt.JavaBridgeException,Message=,Source=Microsoft.DataTransfer.Richfile.HiveOrcBridge,'"",
    ""failureType"": ""UserError"",
    ""target"": ""Work_Work""
}
</code></pre>

<p>The <code>""Work_Work""</code> is the name of a Copy Data activity in the pipeline.
If I turn the compression off (the generated BLOB file is uncompressed), the error does not happen.</p>

<p>Is this the error described in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-parquet"" rel=""nofollow noreferrer"">link</a> the </p>

<blockquote>
  <p>“…If you copy data to/from Parquet format using Self-hosted
  Integration Runtime and hit error saying ""An error occurred when
  invoking java, message: java.lang.OutOfMemoryError:Java heap space"",
  you can add an environment variable _JAVA_OPTIONS in the machine that
  hosts the Self-hosted IR to adjust the min/max heap size for JVM to
  empower such copy, then rerun the pipeline….”?</p>
</blockquote>

<p>If it is, have I understood correctly that I have to do the following:
To go to a server where the “Self-hosted Integration Runtime” (still have no idea what it is) and increase the max heap size for JVM. Is this correct?</p>

<p>If it is, my next question is: how large the max heap size should be? My pipeline can generate a file whose size will be 30 GB.<br>
What “max heap size” can guarantee that such a file will not cause the fail?</p>
","<azure><azure-blob-storage><azure-pipelines><parquet><azure-data-factory>","2019-09-16 19:48:02","1932","1","1","58163984","<p>If you copy data to/from Parquet format using Self-hosted Integration Runtime and hit error saying ""An error occurred when invoking java, message: java.lang.OutOfMemoryError:Java heap space"", you can add an environment variable _JAVA_OPTIONS in the machine that hosts the Self-hosted IR to adjust the min/max heap size for JVM to empower such copy, then rerun the pipeline.</p>

<p><a href=""https://i.stack.imgur.com/glPyE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/glPyE.png"" alt=""enter image description here""></a></p>

<p><strong>Example:</strong> set variable _JAVA_OPTIONS with value -Xms256m -Xmx16g. The flag Xms specifies the initial memory allocation pool for a Java Virtual Machine (JVM), while Xmx specifies the maximum memory allocation pool. This means that JVM will be started with Xms amount of memory and will be able to use a maximum of Xmx amount of memory. By default, ADF use min 64MB and max 1G.</p>

<p>Reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-parquet"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/format-parquet</a></p>

<p>Hope this helps.</p>
"
"57962795","Azure Data Flow not deleting the row in AlterRow","<p>I am not sure what wrong data-flow is not deleting the rows it giving this error "" Activity dataflow38 failed: ""</p>

<p>In Preview Tab it is showing the rows which i want to delete but it not there is no relationship with the table</p>

<p><a href=""https://i.stack.imgur.com/4M3VY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4M3VY.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/14fyR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/14fyR.png"" alt=""enter image description here""></a></p>

<p><strong>error</strong></p>

<pre><code> ""message"": ""Job 'c688a5bd-34dd-44e2-8292-724f0ea5f98a failed due to reason: DF-EXEC-1 Conversion failed when converting date and/or time from character string.\ncom.microsoft.sqlserver.jdbc.SQLServerException: Conversion failed when converting date and/or time from character string.\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:258)\n\tat com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:256)\n\tat com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:108)\n\tat com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:28)\n\tat com.microsoft.sqlserver.jdbc.SQLServerBulkCopy.doInsertBulk(SQLServerBulkCopy.java:1611)\n\tat com.microsoft.sqlserver.jdbc.SQLServerBulkCopy.access$200(SQLServerBulkCopy.java:58)\n\tat com.microsoft.sqlserver.jdbc.SQLServerBulkCopy$1InsertBulk.doExecute(SQLServerBulkCopy.java:709)\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7151)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:2478)\n\tat com.microsoft.sqlserver.jdbc.SQLServerBulkCopy.sendBulkLoadBCP(SQLServerBulkCopy.java:739)\n\tat com.microsoft.sqlserver.jdbc.SQLServerBulkCopy.writeToServer(SQLServerBulkCopy.java:1684)\n\tat com.microsoft.sqlserver.jdbc.SQLServerBulkCopy.writeToServer(SQLServerBulkCopy.java:669)\n\tat com.microsoft.azure.sqldb.spark.connect.DataFrameFunctions.com$microsoft$azure$sqldb$spark$connect$DataFrameFunctions$$bulkCopy(DataFrameFunctions.scala:127)\n\tat com.microsoft.azure.sqldb.spark.connect.DataFrameFunctions$$anonfun$bulkCopyToSqlDB$1.apply(DataFrameFunctions.scala:72)\n\tat com.microsoft.azure.sqldb.spark.connect.DataFrameFunctions$$anonfun$bulkCopyToSqlDB$1.apply(DataFrameFunctions.scala:72)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:951)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:951)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2284)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:139)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:112)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1526)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:503)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"",
    ""failureType"": ""UserError"",
    ""target"": ""dataflow38""
}
</code></pre>
","<azure-sql-database><azure-data-factory>","2019-09-16 18:49:59","1203","0","1","57964990","<p>This Alter Row delete policy will delete all rows because you are using true() in your expression. Are you sure this is what you want?</p>

<p>This error is likely coming from your Sink field mapping. If you just want to delete rows, then don't set any sink mapping.</p>

<p>Just map your key column. And make sure the data type matches for your key mapping. If it doesn't cast it in a Derived Column.</p>

<p>To make this better, I don't think we should default to auto-mapping if all you're doing is deleting.</p>
"
"57959428","Azure Data Factory: Access web service residing in virtual network","<p>My Data Factory (v2) pipeline needs to invoke a REST service that resides in a virtual network deployed to Azure. This is a private network and is not public facing.</p>

<p>There appears to be 2 options:</p>

<ol>
<li>Web Activity: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">documentation</a> mentions it works for public URLs only</li>
<li>Web Hook: documentation doesn't mention any such limitations so presumably it does support private access (?)</li>
</ol>

<p>However, when I attempt to invoke my URL from a Web Hook it fails with Bad Request and no other details. </p>

<p>My strong suspicion is that this is related to accessing a private network from Data Factory, although I can't determine this for sure. The request definitely does not reach my REST service, so the 403 is not coming from there.</p>

<p>Therefore, I have 2 questions:</p>

<ol>
<li>Is it possible for Data Factory Web Hooks to access a private virtual network</li>
<li>Is there any way to get more logging on what a Web Hook is doing?</li>
</ol>
","<azure><azure-data-factory>","2019-09-16 14:53:23","963","4","1","57959612","<blockquote>
<p>Is it possible for Data Factory Web Hooks to access a private virtual network?</p>
</blockquote>
<p>Unfortunately out of the box no. The way data factory works is by invoking pipelie activities on something called <strong>Auto-resolve integration runtime</strong> which is, simply said, a small VM which is provided by MS for you for time required to run the task. This VM is not connect to any VNet and as such can't reach your internal resources.</p>
<p>You would need to use <strong>Self-hosted integration runtime</strong>. Which in essence in a data factory agent running on a Virtual Machine. This was it can be part of your VNet. Data factory will then proxy all requests through that VM.</p>
<p><a href=""https://i.stack.imgur.com/b0bgn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/b0bgn.png"" alt=""enter image description here"" /></a></p>
<p>While it isn't possible to call Web action over HTTP you can trick it via Lookup action over HTTP dataset with HTTP linked service hosted on that Self hosted integration runtime. I did quick demo for you and tested this using BLOB protected by firewall accepting only VNet traffic.</p>
<p><strong>linked service</strong></p>
<p><a href=""https://i.stack.imgur.com/6uKbw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6uKbw.png"" alt=""enter image description here"" /></a></p>
<p><strong>http dataset</strong></p>
<p><a href=""https://i.stack.imgur.com/rPWAt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rPWAt.png"" alt=""enter image description here"" /></a></p>
<p><strong>Lookup setting</strong></p>
<p><a href=""https://i.stack.imgur.com/KhMyW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KhMyW.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/rWNHo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rWNHo.png"" alt=""enter image description here"" /></a></p>
<blockquote>
<p>Is there any way to get more logging on what a Web Hook is doing?</p>
</blockquote>
<p>You could try enabling diagnostic settings but I never checked what kind of logs you will get.</p>
<p><a href=""https://i.stack.imgur.com/lci5n.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lci5n.png"" alt=""enter image description here"" /></a></p>
"
"57951999","Azure Data Factory V2 bug in Set-AzureRmDataFactoryV2Dataset when dataset type json","<p>When I try to use Set-AzureRmDataFactoryV2Dataset and dataset type is ""Json"" in incoming file for some reason in ADFV2 code changed so that type = ""Dataset"" and ADFV2 says files is corrupted.</p>

<p>This is the end result in ADFV2:</p>

<pre><code>""properties"": {
    ""type"": ""Dataset"",
    ""typeProperties"": {
        ""location"": {
            ""type"": ""AzureBlobFSLocation"",
            ""fileName"": ""somefile"",
            ""folderPath"": ""folder/files"",
            ""fileSystem"": ""source-data""
        },
</code></pre>

<p>If I modify file in ADFV2 value ""Dataset"" back to ""Json"" its not corrupted anymore. See above.</p>

<pre><code>""properties"": {
    ""type"": ""Json"",
    ""typeProperties"": {
        ""location"": {
            ""type"": ""AzureBlobFSLocation"",
            ""fileName"": ""somefile"",
            ""folderPath"": ""folder/files"",
            ""fileSystem"": ""source-data""
        },
</code></pre>

<p>Is there bug in parser?</p>
","<azure><powershell-2.0><azure-powershell><azure-data-factory>","2019-09-16 07:15:07","218","0","2","57974409","<p>The type in the dataset json tells data factory which kind of storage you are accessing. It looks like you are trying to access a blob storage or a data lake v2. In those cases, the type attribute must be set to:</p>

<p>AzureBlobStorageLocation if its a blob storage as seen here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#dataset-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#dataset-properties</a></p>

<p>AzureBlobFSLocation if its a lake v2 as seen here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage#dataset-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage#dataset-properties</a></p>

<p>You can also check out an example json if you scroll down a bit in those links.</p>

<p>Hope this helped!</p>
"
"57951999","Azure Data Factory V2 bug in Set-AzureRmDataFactoryV2Dataset when dataset type json","<p>When I try to use Set-AzureRmDataFactoryV2Dataset and dataset type is ""Json"" in incoming file for some reason in ADFV2 code changed so that type = ""Dataset"" and ADFV2 says files is corrupted.</p>

<p>This is the end result in ADFV2:</p>

<pre><code>""properties"": {
    ""type"": ""Dataset"",
    ""typeProperties"": {
        ""location"": {
            ""type"": ""AzureBlobFSLocation"",
            ""fileName"": ""somefile"",
            ""folderPath"": ""folder/files"",
            ""fileSystem"": ""source-data""
        },
</code></pre>

<p>If I modify file in ADFV2 value ""Dataset"" back to ""Json"" its not corrupted anymore. See above.</p>

<pre><code>""properties"": {
    ""type"": ""Json"",
    ""typeProperties"": {
        ""location"": {
            ""type"": ""AzureBlobFSLocation"",
            ""fileName"": ""somefile"",
            ""folderPath"": ""folder/files"",
            ""fileSystem"": ""source-data""
        },
</code></pre>

<p>Is there bug in parser?</p>
","<azure><powershell-2.0><azure-powershell><azure-data-factory>","2019-09-16 07:15:07","218","0","2","58084434","<p>In case anyone else runs into this issue, I can confirm this is a bug in the Az/AzureRM PowerShell module, which appears to have been fixed as of Az version 2.5.0 (at least, possibly earlier).</p>

<p>I hit the same problem when trying to define a dataset with a type 'Binary'. The JSON I provided had it correct, but when it was actually created the type was 'Dataset'. When playing around I realized it was only happening on my Windows VM, running the Az module version 2.1.0. Running from my Linux OS with Az version 2.5.0 worked fine. On updating my Windows VM to 2.6.0 the problem went away.</p>
"
"57944904","Copy data with Azure Data Factory from Excel or CSV File and perform transformation on the input file","<p>I need do copy data from Excel file with Azure Data factory (v2) and perform transformations (like splitting or adding columns) on the input Excel file before importing into my database. Is this possible?</p>

<p>If not, is it possible to import the data in .csv format e perform the same transformations I need with Azure Data Factory on the .csv?</p>
","<azure><azure-data-factory>","2019-09-15 14:10:34","1690","1","1","57948628","<p>Use the Mapping Data Flows feature in ADF to do this. Use Conditional Split for splitting logic, partitioned file output for file splitting, and Derived Column to add columns.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-create"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-create</a></p>

<p><a href=""https://aka.ms/dflinks"" rel=""nofollow noreferrer"">https://aka.ms/dflinks</a> </p>
"
"57940314","Extract data from Http rest api, How to configure http request header for cookie authentication","<p>I need to extract data from Http web API. However it requires cookie authentication. I can get the cookie information when I inspect the chrome browser. But I don't know how to configure the headers and authentication type on Azure Data Factory V2. Can you please provide some details ? (for the token based authentication, I have successfully extracted the data. But cookie one, I don't know how to do it)</p>
","<restful-authentication><azure-data-factory>","2019-09-15 00:45:27","739","0","2","57952565","<p>Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest#supported-capabilities"" rel=""nofollow noreferrer"">official document</a>, ADF REST Linked Service only supports below authentications:</p>

<ul>
<li>Anonymous,</li>
<li>Basic,</li>
<li>AAD service principal,</li>
<li>managed identities for Azure resources.</li>
</ul>

<p><a href=""https://i.stack.imgur.com/aggwx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aggwx.png"" alt=""enter image description here""></a></p>

<p>cookie authentication is not supported by ADF.</p>
"
"57940314","Extract data from Http rest api, How to configure http request header for cookie authentication","<p>I need to extract data from Http web API. However it requires cookie authentication. I can get the cookie information when I inspect the chrome browser. But I don't know how to configure the headers and authentication type on Azure Data Factory V2. Can you please provide some details ? (for the token based authentication, I have successfully extracted the data. But cookie one, I don't know how to do it)</p>
","<restful-authentication><azure-data-factory>","2019-09-15 00:45:27","739","0","2","58032089","<p>So far, the only way Id found to do such thing is to create an Azure Function and do the API call within using HttpClient, and then just add the cookie to the request object.</p>

<p>You will need to optimize how the call to your api is made and also what type of data you will return from the Azure Function Activity. </p>
"
"57938549","Loading JSON files via Azure Data Factory","<p>I have over 100 JSON files which is nested and I am trying to Load the JSON files via Data FactoryV2 into SQL Data Warehouse. I have created the Data FactoryV2 and everything seems fine the connection below seems fine and the Data Preview seems fine also. </p>

<p><a href=""https://i.stack.imgur.com/GkxFu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GkxFu.png"" alt=""enter image description here""></a></p>

<p>When I run the Data Factory I get this error:</p>

<p><a href=""https://i.stack.imgur.com/diR9o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/diR9o.png"" alt=""enter image description here""></a></p>

<p>I am not sure what the issue is. I have tried to re-create the Data Factory several times.</p>
","<azure-data-factory>","2019-09-14 19:19:30","460","0","1","57944919","<p>The error message is clear enough when it says ""All columns of the table must be specified..."". This means that the table in the data warehouse has more columns than what you are seeing in the preview of the json file. You will need to create a table in the data warehouse with the same columns that are shown in the preview of the json files.</p>

<p>If you need to insert them in a table with more fields, create a ""staging"" table with the same columns as the json file, and then call a stored procedure to insert the content of this staging table in the corresponding table.</p>

<p>Hope this helped!</p>
"
"57918485","Not Loading all columns from Azure Table Storage Source in Azure Data Factory","<p>I am trying to copy data from Azure Table Storage to Csv file using ""Copy Activity"" in Azure Data Factory. But few columns are not loading. </p>

<p>In Azure Table Source Dataset Preview I'm not able to see all columns. Those columns have Null data for first 400 rows. If i have data for all fields in first 11 rows then i am able to see and load all fields data. 
But in my case for few fields we have null data for few rows so how to load all columns data?</p>
","<azure><azure-table-storage><azure-data-factory>","2019-09-13 06:40:23","775","0","2","57950327","<p>Couple of points </p>

<ol>
<li><p>In  preview we always show a few of records  and not all .</p></li>
<li><p>The table storage is not a schema based storage and the null are treated differently here . I think this  \
<a href=""https://stackoverflow.com/questions/4228460/querying-azure-table-storage-for-null-values"">Querying azure table storage for null values</a></p>

<p>will help you understand more . </p></li>
</ol>

<p>I am pretty confident that when you run the copy activity it will copy all the records to the sink , even if you do see few in the preview .</p>
"
"57918485","Not Loading all columns from Azure Table Storage Source in Azure Data Factory","<p>I am trying to copy data from Azure Table Storage to Csv file using ""Copy Activity"" in Azure Data Factory. But few columns are not loading. </p>

<p>In Azure Table Source Dataset Preview I'm not able to see all columns. Those columns have Null data for first 400 rows. If i have data for all fields in first 11 rows then i am able to see and load all fields data. 
But in my case for few fields we have null data for few rows so how to load all columns data?</p>
","<azure><azure-table-storage><azure-data-factory>","2019-09-13 06:40:23","775","0","2","58362939","<p>I meet the same problem ""Not Loading all columns from Azure Table Storage Source in Azure Data Factory"". I think it may be a bug of Azure Data Factory.</p>
"
"57917908","Is it possible to connect to databricks deltalake tables from adf","<p>I'm looking for a way to be able to connect to Databricks deltalake tables from ADF and other Azure Services(like Data Catalog).  I don't see databricks data store listed in ADF data sources. </p>

<p>On a similar question  - <a href=""https://stackoverflow.com/questions/54185630/is-possible-to-read-an-azure-databricks-table-from-azure-data-factory"">Is possible to read an Azure Databricks table from Azure Data Factory?</a></p>

<p>@simon_dmorias seems to have suggested using ODBC connection to connect to databricks tables.</p>

<p>I tried to set up the ODBC connection but it requires IR to be setup. There are 2 options I see when creating the IR. Self-hosted and linked Self-hosted. I tried to create the Self-hosted IR but it requires installation on my local desktop and probably is more meant for an on-premise odbc connection. I couldn't use the IR on my linked Services.</p>

<p>I have been able to connect powerbi with databricks deltalake tables and plan to use the same creds here. Here is the reference link -</p>

<p><a href=""https://docs.azuredatabricks.net/user-guide/bi/power-bi.html"" rel=""nofollow noreferrer"">https://docs.azuredatabricks.net/user-guide/bi/power-bi.html</a></p>

<p>Any guidance will be helpful</p>
","<azure-data-factory><azure-databricks><delta-lake>","2019-09-13 05:46:33","2694","1","3","57919006","<p>Please refer to the section <a href=""https://docs.azuredatabricks.net/user-guide/dev-tools/data-pipelines.html#azure-data-factory"" rel=""nofollow noreferrer""><code>Azure Data Factory</code></a> of Azure Databricks offical document  <a href=""https://docs.azuredatabricks.net/user-guide/dev-tools/data-pipelines.html"" rel=""nofollow noreferrer""><code>User Guide &gt; Developer Tools &gt; Managing Dependencies in Data Pipelines</code></a>. And you will see there are two Azure documents list in the topic about how to create a Databricks notebook with the Databricks Notebook Activity and run it to do the transfer data task in Azure Data Factory, as below. I think it will help you to realize your needs.</p>

<ol>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-databricks-notebook"" rel=""nofollow noreferrer"">Run a Databricks notebook with the Databricks Notebook Activity in Azure Data Factory</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-databricks-notebook"" rel=""nofollow noreferrer"">Transform data by running a Databricks notebook</a></li>
</ol>
"
"57917908","Is it possible to connect to databricks deltalake tables from adf","<p>I'm looking for a way to be able to connect to Databricks deltalake tables from ADF and other Azure Services(like Data Catalog).  I don't see databricks data store listed in ADF data sources. </p>

<p>On a similar question  - <a href=""https://stackoverflow.com/questions/54185630/is-possible-to-read-an-azure-databricks-table-from-azure-data-factory"">Is possible to read an Azure Databricks table from Azure Data Factory?</a></p>

<p>@simon_dmorias seems to have suggested using ODBC connection to connect to databricks tables.</p>

<p>I tried to set up the ODBC connection but it requires IR to be setup. There are 2 options I see when creating the IR. Self-hosted and linked Self-hosted. I tried to create the Self-hosted IR but it requires installation on my local desktop and probably is more meant for an on-premise odbc connection. I couldn't use the IR on my linked Services.</p>

<p>I have been able to connect powerbi with databricks deltalake tables and plan to use the same creds here. Here is the reference link -</p>

<p><a href=""https://docs.azuredatabricks.net/user-guide/bi/power-bi.html"" rel=""nofollow noreferrer"">https://docs.azuredatabricks.net/user-guide/bi/power-bi.html</a></p>

<p>Any guidance will be helpful</p>
","<azure-data-factory><azure-databricks><delta-lake>","2019-09-13 05:46:33","2694","1","3","57938091","<p>You can but it is quite complex. You need to use the ODBC connector in Azure Data Factory with a self hosted runtime. </p>

<p>ADF can connect using ODBC (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-odbc"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-odbc</a>). It does require a self hosted IR. Assuming you have the right drivers installed you can configure the ODBC connection to a Databricks cluster.</p>

<p>The connections details for the ODBC settings can be found in cluster settings screen in the Databricks workspace (<a href=""https://learn.microsoft.com/en-us/azure/azure-databricks/connect-databricks-excel-python-r"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-databricks/connect-databricks-excel-python-r</a>).</p>

<p>The process is very similar to what you posted for PowerBI.</p>
"
"57917908","Is it possible to connect to databricks deltalake tables from adf","<p>I'm looking for a way to be able to connect to Databricks deltalake tables from ADF and other Azure Services(like Data Catalog).  I don't see databricks data store listed in ADF data sources. </p>

<p>On a similar question  - <a href=""https://stackoverflow.com/questions/54185630/is-possible-to-read-an-azure-databricks-table-from-azure-data-factory"">Is possible to read an Azure Databricks table from Azure Data Factory?</a></p>

<p>@simon_dmorias seems to have suggested using ODBC connection to connect to databricks tables.</p>

<p>I tried to set up the ODBC connection but it requires IR to be setup. There are 2 options I see when creating the IR. Self-hosted and linked Self-hosted. I tried to create the Self-hosted IR but it requires installation on my local desktop and probably is more meant for an on-premise odbc connection. I couldn't use the IR on my linked Services.</p>

<p>I have been able to connect powerbi with databricks deltalake tables and plan to use the same creds here. Here is the reference link -</p>

<p><a href=""https://docs.azuredatabricks.net/user-guide/bi/power-bi.html"" rel=""nofollow noreferrer"">https://docs.azuredatabricks.net/user-guide/bi/power-bi.html</a></p>

<p>Any guidance will be helpful</p>
","<azure-data-factory><azure-databricks><delta-lake>","2019-09-13 05:46:33","2694","1","3","62820700","<p>Actually, I figured it is possible to get metadata from any tables inside a Databricks workspace directly, by using ODBC connection available on current version of Azure Data Catalog, it would be much better a native connector, but for now if you wanna give it a try just fill up the info bellow (on the Azure Data Catalog publishing app):</p>
<p><strong>Driver:</strong> Microsoft Spark ODBC Driver (it must be intalled on your system)</p>
<p><strong>Connection String:</strong> host=eastus.azuredatabricks.net;port=443;SSL=1;HTTPPath=sql/protocolv1/o/XXXXXXXXXXXXXXX/XXXX-XXXXXX-XXXXXX;transportMode=http;AuthMech=8</p>
<p><strong>User:</strong> token</p>
<p><strong>Password:</strong> dapiXXXXXXXXXXXXXXXXXXXXX</p>
<p>And let <strong>Database</strong> field blank</p>
"
"57908140","Azure data factory. Use many different git branches","<p>My coworker and I are working in two different git branches: <code>mybranch</code> and <code>hisbranch</code>. Each of us is developing a databrick's notebook for data processing. This notebooks are then added as an activity in two different <code>ADF</code>'s pipeline.</p>

<p>For testing the pipeline It's needed to be publish, but when I push it to the repo an error message pop's up: <em>Publish is only allowed from collaboration ('develop') branch</em>. </p>

<p>That forces us to work in the very same git branch (the collaboration one) which is unconvinient because we may conflict a lot during the development and testing. </p>

<p>Is there any way to test pipelines in independent branches and then merge?</p>
","<git><azure><azure-data-factory>","2019-09-12 13:46:20","3421","4","1","57921654","<p><strong>Note:</strong> Your Azure Repos collaboration branch that is used for publishing. By default, its <code>master</code>. Change this setting in case you want to publish resources from another branch.</p>

<p>To develop multiple features in parallel, you need to just use ""Save"". Save will commit your changes to the branch you are actually working on. Other branches will do the same. Whenever you want to publish, you need to first make a pull request from your branch to master, then publish. Any merge conflict should be solved when merging everything in the master branch. Then just publish and there shouldn't be any conflicts, and aDF_publish will get generated after that.</p>

<p>For more details, refer ""<a href=""https://learn.microsoft.com/en-us/azure/data-factory/source-control"" rel=""nofollow noreferrer"">Source control in Azure Data Factory</a>"".</p>

<p>Hope this helps.</p>
"
"57904740","how to create a view in Azure data factory","<p>I am creating a new a Azure data factory pipeline. In that I need to to copy one table to Azure Blob and delete the data after copy success. before deleting the data i need to create a view of copied data and compare the data in source database which is going to delete.I need to delete the data from source table only the data in view and source table match.</p>
","<sql-server><azure><view><pipeline><azure-data-factory>","2019-09-12 10:26:02","956","0","1","57950673","<p>As I know about Azure Data Factory, it doesn't support you create the view, so you can not do that.</p>

<p>Hope this helps.</p>
"
"57896240","Azure Data flow new branch how data flow","<p>I have a <strong>source1</strong> in the image below, i am creating a branch from the <strong>source1</strong> and then in that I am doing join and then sink to <strong>skin1</strong>. my question is how the flow will work first it will completer the flow from <strong>sources1</strong> to <strong>sink1</strong> then it jumps to <strong>NewBrach""Source1""</strong> then join and skin, or it will run parallel </p>

<p>Run <strong>Sequential</strong></p>

<p>Will it flow like 1->2>3->4->5>6
or </p>

<p>Run <strong>Parallel</strong></p>

<p><a href=""https://i.stack.imgur.com/8lCwh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8lCwh.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/EbwjA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EbwjA.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2019-09-11 20:17:47","392","0","2","57965072","<p>It depends :)</p>

<p>It is non-deterministic and depends on whether ADF needs to block, waiting for any computation to complete in the other streams.</p>

<p>We are also working on adding UI gestures to control ordering.</p>

<p>For now, execute the data flow in a pipeline debug, then look at the execution plan using the eyeglasses icon to get a better idea of your parallelism.</p>
"
"57896240","Azure Data flow new branch how data flow","<p>I have a <strong>source1</strong> in the image below, i am creating a branch from the <strong>source1</strong> and then in that I am doing join and then sink to <strong>skin1</strong>. my question is how the flow will work first it will completer the flow from <strong>sources1</strong> to <strong>sink1</strong> then it jumps to <strong>NewBrach""Source1""</strong> then join and skin, or it will run parallel </p>

<p>Run <strong>Sequential</strong></p>

<p>Will it flow like 1->2>3->4->5>6
or </p>

<p>Run <strong>Parallel</strong></p>

<p><a href=""https://i.stack.imgur.com/8lCwh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8lCwh.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/EbwjA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EbwjA.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2019-09-11 20:17:47","392","0","2","60404603","<p>Just an Update if someone following this post-Microsoft has released the custom Sink feature.</p>

<p>Thanks @Mark</p>

<p>please check the Link
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-sink"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-sink</a></p>
"
"57894768","How to update ADF Pipeline level parameters during CICD","<p>Being novice to ADF CICD i am currently exploring how we can update the pipeline scoped parameters when we deploy the pipeline from one enviornment to another.
Here is the detailed scenario - <br/>
 I have a simple ADF pipeline with a copy activity moving files from one blob container to another <br/>
 Example - Below there is copy activity and pipeline has two parameters named :<br/>
              1- SourceBlobContainer<br/>
              2- SinkBlobContainer<br/>
with their default values.<br/><br/></p>

<p><a href=""https://i.stack.imgur.com/2a6RC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2a6RC.png"" alt=""enter image description here""></a>
<br/><br/>
Here is how the dataset is configured to consume these Pipeline scoped parameters.
<br/><br/>
<a href=""https://i.stack.imgur.com/U08Gf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/U08Gf.png"" alt=""enter image description here""></a>
<br/><br/>
Since this is development environment its OK with the default values. But the Test environment will have the containers present with altogether different name (like ""TestSourceBlob"" &amp; ""TestSinkBlob"").<br/>
Having said that, when CICD will happen it should handle this via CICD process by updating the default values of these parameters.<br/></p>

<p>When read the documents, no where i found to handle such use-case.<br/>
Here are some links which i referred - <br/></p>

<ul>
<li><a href=""http://datanrg.blogspot.com/2019/02/continuous-integration-and-delivery.html"" rel=""nofollow noreferrer"">http://datanrg.blogspot.com/2019/02/continuous-integration-and-delivery.html</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a>
<br/><br/>
Thoughts on how to handle this will be much appreciated. :-) <br/></li>
</ul>
","<azure><continuous-integration><continuous-deployment><azure-data-factory>","2019-09-11 18:19:41","1296","0","3","61679917","<p>There is another approach in opposite to ARM templates located in 'ADF_Publish' branch.
Many companies leverage that workaround and it works great.<br>
I have spent several days and built a brand new PowerShell module to publish the whole Azure Data Factory code from your master branch or directly from your local machine. The module resolves all pains existed so far in any other solution, including:  </p>

<ul>
<li>replacing any property in JSON file (ADF object), </li>
<li>deploying objects in an appropriate order,</li>
<li>deployment part of objects, </li>
<li>deleting objects not existing in the source any longer, </li>
<li>stop/start triggers, etc. </li>
</ul>

<p>The module is publicly available in PS Gallery: <a href=""https://www.powershellgallery.com/packages/azure.datafactory.tools/"" rel=""nofollow noreferrer"">azure.datafactory.tools</a><br>
Source code and full documentation are in <a href=""https://github.com/SQLPlayer/azure.datafactory.tools"" rel=""nofollow noreferrer"">GitHub here</a>.<br>
Let me know if you have any question or concerns.</p>
"
"57894768","How to update ADF Pipeline level parameters during CICD","<p>Being novice to ADF CICD i am currently exploring how we can update the pipeline scoped parameters when we deploy the pipeline from one enviornment to another.
Here is the detailed scenario - <br/>
 I have a simple ADF pipeline with a copy activity moving files from one blob container to another <br/>
 Example - Below there is copy activity and pipeline has two parameters named :<br/>
              1- SourceBlobContainer<br/>
              2- SinkBlobContainer<br/>
with their default values.<br/><br/></p>

<p><a href=""https://i.stack.imgur.com/2a6RC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2a6RC.png"" alt=""enter image description here""></a>
<br/><br/>
Here is how the dataset is configured to consume these Pipeline scoped parameters.
<br/><br/>
<a href=""https://i.stack.imgur.com/U08Gf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/U08Gf.png"" alt=""enter image description here""></a>
<br/><br/>
Since this is development environment its OK with the default values. But the Test environment will have the containers present with altogether different name (like ""TestSourceBlob"" &amp; ""TestSinkBlob"").<br/>
Having said that, when CICD will happen it should handle this via CICD process by updating the default values of these parameters.<br/></p>

<p>When read the documents, no where i found to handle such use-case.<br/>
Here are some links which i referred - <br/></p>

<ul>
<li><a href=""http://datanrg.blogspot.com/2019/02/continuous-integration-and-delivery.html"" rel=""nofollow noreferrer"">http://datanrg.blogspot.com/2019/02/continuous-integration-and-delivery.html</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a>
<br/><br/>
Thoughts on how to handle this will be much appreciated. :-) <br/></li>
</ul>
","<azure><continuous-integration><continuous-deployment><azure-data-factory>","2019-09-11 18:19:41","1296","0","3","72546924","<p>There is a &quot;new&quot; way to do ci/cd for ADF that should handle this exact use case.  What I typically do is add global parameters and then reference those everywhere (in your case from the pipeline parameters).  Then in your build you can override the global parameters with the values that you want.  Here are some links to references that I used to get this working.</p>
<p>The &quot;new&quot; ci/cd method following something like what is outlined here <a href=""https://towardsdatascience.com/azure-data-factory-ci-cd-made-simple-building-and-deploying-your-arm-templates-with-azure-devops-30c30595afa5"" rel=""nofollow noreferrer"">Azure Data Factory CI-CD made simple: Building and deploying ARM templates with Azure DevOps YAML Pipelines</a>. If you have followed this, something like this should work in your yaml:</p>
<pre><code>overrideParameters: '-dataFactory_properties_globalParameters_environment_value &quot;new value here&quot;'
</code></pre>
<p>Here is an article that goes into more detail on the overrideParameters:  <a href=""https://microsoft-bitools.blogspot.com/2021/11/adf-snack-set-global-params-during.html"" rel=""nofollow noreferrer"">ADF Release - Set global params during deployment</a></p>
<p>Here is a reference on global parameters and how to get them exposed to your ci/cd pipeline: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/author-global-parameters"" rel=""nofollow noreferrer"">Global parameters in Azure Data Factory</a></p>
"
"57894768","How to update ADF Pipeline level parameters during CICD","<p>Being novice to ADF CICD i am currently exploring how we can update the pipeline scoped parameters when we deploy the pipeline from one enviornment to another.
Here is the detailed scenario - <br/>
 I have a simple ADF pipeline with a copy activity moving files from one blob container to another <br/>
 Example - Below there is copy activity and pipeline has two parameters named :<br/>
              1- SourceBlobContainer<br/>
              2- SinkBlobContainer<br/>
with their default values.<br/><br/></p>

<p><a href=""https://i.stack.imgur.com/2a6RC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2a6RC.png"" alt=""enter image description here""></a>
<br/><br/>
Here is how the dataset is configured to consume these Pipeline scoped parameters.
<br/><br/>
<a href=""https://i.stack.imgur.com/U08Gf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/U08Gf.png"" alt=""enter image description here""></a>
<br/><br/>
Since this is development environment its OK with the default values. But the Test environment will have the containers present with altogether different name (like ""TestSourceBlob"" &amp; ""TestSinkBlob"").<br/>
Having said that, when CICD will happen it should handle this via CICD process by updating the default values of these parameters.<br/></p>

<p>When read the documents, no where i found to handle such use-case.<br/>
Here are some links which i referred - <br/></p>

<ul>
<li><a href=""http://datanrg.blogspot.com/2019/02/continuous-integration-and-delivery.html"" rel=""nofollow noreferrer"">http://datanrg.blogspot.com/2019/02/continuous-integration-and-delivery.html</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a>
<br/><br/>
Thoughts on how to handle this will be much appreciated. :-) <br/></li>
</ul>
","<azure><continuous-integration><continuous-deployment><azure-data-factory>","2019-09-11 18:19:41","1296","0","3","75534432","<p>You would have to use the <code>arm-template-parameters-defintion.json</code> file. You can edit the file directly in ADF.</p>
<p><a href=""https://i.stack.imgur.com/n5VIv.png"" rel=""nofollow noreferrer"">Edit Parameters File</a></p>
<p>Example to include all pipeline parameters in your ARM template:</p>
<pre><code>&quot;Microsoft.DataFactory/factories/pipelines&quot;: {
    &quot;properties&quot;: {
        &quot;parameters&quot;: {
            &quot;*&quot;: {
                &quot;defaultValue&quot;: &quot;=&quot;
            }
        }
    }
}
</code></pre>
<p>Source: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-delivery-resource-manager-custom-parameters"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-delivery-resource-manager-custom-parameters</a></p>
"
"57893031","Azure Data lake port connection","<p>We are working on a POC to load data from various datasources to Azure Data lake Gen2 using Azure Data Factory. Is there any default port that we can use in ADF to connect to ADL? Please let me know that.</p>
","<azure-blob-storage><azure-data-lake><azure-data-factory>","2019-09-11 16:07:01","2141","0","1","57893787","<p>The default port to connect to ADL or blob storage is 443. If your destination/sink is sql server, then the default port is 1433.</p>

<p>Doc here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#ports-and-firewall"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#ports-and-firewall</a></p>

<p>Hope it helps!</p>
"
"57892542","Where can I find ""authenticationKey"" value in azure application for creating ""ClientCredential"" object","<p>I would like to transfer data from Azure Blob to Azure SQL Database using Azure Data Factory. 
I found this link : <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-copy-data-dot-net"" rel=""nofollow noreferrer"">Copy data from Azure Blob to Azure SQL Database using Azure Data Factory</a> </p>

<p><strong>EDIT :</strong>
I need to connect to my Azure Subscription to create a Data Factory and a Pipe line. </p>

<p>My problem is to find value for ""authenticationKey"".</p>

<pre><code>string authenticationKey = ""&lt;your authentication key for the application&gt;"";
</code></pre>

<p>I need to create a ""ClientCredential"" like below code :</p>

<pre><code>// Authenticate and create a data factory management client
var context = new AuthenticationContext(""https://login.windows.net/"" + tenantID);
ClientCredential cc = new ClientCredential(applicationId, authenticationKey);
AuthenticationResult result = 
context.AcquireTokenAsync(""https://management.azure.com/"", cc).Result;
ServiceClientCredentials cred = new TokenCredentials(result.AccessToken);
var client = new DataFactoryManagementClient(cred) { SubscriptionId = subscriptionId };
</code></pre>

<p>you can see in below image my application registration in azure.</p>

<p><a href=""https://i.stack.imgur.com/t1pc0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t1pc0.png"" alt=""enter image description here""></a></p>
","<c#><azure><azure-data-factory><clientcredential>","2019-09-11 15:37:42","1486","1","1","57896586","<p>Basically your authentication key is the secret key of your application registered in Active directory. </p>

<p>Please find the details here how to retrieve your secret/authentication key here.<a href=""https://stackoverflow.com/a/57849682/4111181"">https://stackoverflow.com/a/57849682/4111181</a></p>
"
"57886397","send email with attachment in Azure Data factory","<p>I need to send an email with attachment according to a condition from table. For e.g I have a table XYZ which has column ""Flag"" and if the column flag value is ""N"" then it has to pick the values with value as ""N from the table and write into a file. This file has to be sent as attachment in an email in Azure data factory. f you can help with the process for this in Azure data factory it should be great.</p>
","<azure><azure-data-factory>","2019-09-11 09:46:15","5313","2","1","57901788","<p>Per my researching, there is no such activity could send email with attachment,you could vote up this <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/18404794-add-a-new-email-activity-with-the-ability-to-send"" rel=""nofollow noreferrer"">feedback</a> to improve this feature.</p>

<p>As workaround, you could adopt @Raunak Jhawar's suggestion that using <a href=""https://flow.microsoft.com/en-us/blog/introducing-triggers-in-the-sql-connector/"" rel=""nofollow noreferrer"">SQL Connector Trigger</a>. </p>

<p><a href=""https://i.stack.imgur.com/p9wtK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p9wtK.png"" alt=""enter image description here""></a></p>

<p>Then you could get the value you want into <a href=""https://learn.microsoft.com/en-us/connectors/azurefile/"" rel=""nofollow noreferrer"">Azure File Storage</a> or <a href=""https://learn.microsoft.com/en-us/azure/connectors/connectors-create-api-azureblobstorage"" rel=""nofollow noreferrer"">Azure Blob Storage.</a></p>

<p>After that,send email with attachments following this <a href=""https://learn.microsoft.com/en-us/azure/logic-apps/tutorial-process-email-attachments-workflow"" rel=""nofollow noreferrer"">tutorial</a>. Here is s similar case for your reference:<a href=""https://stackoverflow.com/questions/51473878/azure-logic-app-how-to-send-an-email-with-one-or-more-attachments-after-gettin"">Azure Logic app : How to Send an Email with one or more attachments after getting the content from Blob storage?</a></p>
"
"57853422","ADF create REST Dataset BUG","<p>Is there some issue or known bug for creating a REST dataset? From last week I cant create, the screen just freeze, and it does nothing. </p>

<p><a href=""https://i.stack.imgur.com/AA7A0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AA7A0.png"" alt=""enter image description here""></a></p>

<p>I have already created in the past few REST datasets and they work fine, so this issue is something new.</p>
","<azure-data-factory>","2019-09-09 11:46:37","80","1","2","57853676","<p>I just tried and it works fine. As a workaround you can try creating a json file and upload it via powershell, here is the json for the dataset I've just created:</p>

<pre><code>{
""name"": ""RestDataset"",
""properties"": {
    ""linkedServiceName"": {
        ""referenceName"": ""RestService1"",
        ""type"": ""LinkedServiceReference""
    },
    ""annotations"": [],
    ""type"": ""RestResource"",
    ""typeProperties"": {
        ""relativeUrl"": ""RelatUrl""
    },
    ""schema"": []
}
}
</code></pre>

<p>Modify it to your needs, save it into a .json file and upload it with Set-AzDataFactoryV2Dataset. You can also use Set-AzureRMDataFactoryV2Dataset if you are still using AzureRM.</p>

<p>Hope this helped!</p>
"
"57853422","ADF create REST Dataset BUG","<p>Is there some issue or known bug for creating a REST dataset? From last week I cant create, the screen just freeze, and it does nothing. </p>

<p><a href=""https://i.stack.imgur.com/AA7A0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AA7A0.png"" alt=""enter image description here""></a></p>

<p>I have already created in the past few REST datasets and they work fine, so this issue is something new.</p>
","<azure-data-factory>","2019-09-09 11:46:37","80","1","2","57867204","<p>FYI: the issue was confirmed by ADF team and the bug is resolved. </p>
"
"57852062","How to run Azure CLI command from azure ADF pipeline?","<p>I have set of Azure CLI commands ready which append data into an existing azure data lake file. </p>

<p>We need to run all these commands from an ADF (Azure Data Factory) pipeline. Does anyone have any idea on how we can run Azure CLI commands from ADF pipeline?</p>
","<azure><azure-data-lake><azure-data-factory><azure-cli>","2019-09-09 10:20:34","3786","1","1","57857137","<p>You can create an Azure function and call it from ADF with the Azure Function Activity: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity</a></p>

<p>Here is a tutorial to run azure-cli commands in Azure Functions: <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/scripts/functions-cli-create-serverless"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-functions/scripts/functions-cli-create-serverless</a></p>

<p>Hope this helped!!</p>
"
"57825913","Is there any alternative to connect MySQL server to Azure Data Factory for MAC","<p>I am trying to connect the SQL server to Azure Data Factory for coping the data from server to Blob containers.
SO I am able to create MySQL server on MySQL Workbench but in pipeline setup when I am trying to declare the Source I am unable to connect the server due to unavailability of Integration Runtime file for MAC</p>

<p>So can anyone help me in finding any alternative for Integration Runtime or getting the .dmg file for Integration Runtime</p>

<p>I am new to these technologies so please don't mind if its a very basic question!
Thanks in advance</p>
","<mysql><azure><integration><azure-data-factory>","2019-09-06 16:48:47","392","1","1","57847193","<p>First question, you said you are trying to connect the SQL server to Azure Data Factory for coping the data from server to Blob containers.</p>

<p>Azure data fata factory has provide the tutorial for us, please reference:</p>

<ol>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-hybrid-copy-data-tool"" rel=""nofollow noreferrer"">Copy data from an on-premises SQL Server database to Azure Blob
storage by using the Copy Data tool</a>.</li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-hybrid-copy-portal"" rel=""nofollow noreferrer"">Copy data from an on-premises SQL Server database to Azure Blob
storage(UI)</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-hybrid-copy-powershell"" rel=""nofollow noreferrer"">Tutorial: Copy data from an on-premises SQL Server database to Azure
Blob storage(PowerShell)</a></li>
</ol>

<p>Second question,  create MySQL server on MySQL Workbench but in pipeline setup when I am trying to declare the Source I am unable to connect the server due to unavailability of Integration Runtime file for MAC.</p>

<p>SQL server and MySQL server are different, I guess your question is copy data from on-premise MySQL server to Azure Blob storage container.</p>

<p>No matter which way to copy data from on-premise source, it all need <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime"" rel=""nofollow noreferrer"">Integration Runtime</a>. But the limit is: </p>

<blockquote>
  <p>'Currently, we only support running the self-hosted IR on a Windows operating system'.</p>
</blockquote>

<p>You can reference <a href=""http://Copy%20data%20from%20MySQL%20using%20Azure%20Data%20Factory"" rel=""nofollow noreferrer"">Copy data from MySQL using Azure Data Factory</a> to get more details.</p>

<p>I don't think anyone can help you find the .dmg file for Integration Runtime.</p>

<p>Hope this helps.</p>
"
"57823289","How to invoke a pipeline with parameters","<p>I've created several pipelines with parameters, I'd like to test their behavior with different sets of parameters, how can I do this in Python? I've found an answer to do this in Powershell but not on Python.</p>

<p>I tried browsing the Python Data Factory sdk but I dont know if the issue is I'm not familiar with Python or if the sdk is not very intuitive.</p>

<p>Any help or pointers would be appreciated.</p>
","<python><python-3.x><azure><azure-data-factory><azure-blob-storage>","2019-09-06 13:47:51","142","1","1","57823667","<p>Instead of posting the code here, I've created a GitHub Gist to show 2 different ways to login and call pipelines with whatever parameters you need. </p>

<p>Here you go: <a href=""https://gist.github.com/Gorgoras/1fe534fd9b454412f81c8203c773c483"" rel=""nofollow noreferrer"">https://gist.github.com/Gorgoras/1fe534fd9b454412f81c8203c773c483</a></p>

<p>Hope this helped!!</p>
"
"57820938","How to execute a SQL query in Azure Data Factory","<p>I create a pipeline in ADF for performing copy activity. My source database is Azure SQL database and Sink is Azure Blob .I want to execute an SQL Query in ADF to delete data from source once data is copied to blob. I am not allowed to use copy or lookup to execute query.Is their any custom way to do this.I need to create a view and have to do some activity.Please help</p>
","<sql-server><azure><cloud><azure-sql-database><azure-data-factory>","2019-09-06 11:14:05","18132","4","4","57821076","<p>If you are using data mapping flows, there is a new activity to execute custom SQL scripts:</p>

<ul>
<li><a href=""https://azure.microsoft.com/en-us/updates/azure-data-factory-mapping-data-flows-adds-sql-scripts-to-sink-transformation/"" rel=""nofollow noreferrer"">Azure Data Factory mapping data flows adds SQL scripts to sink transformation</a></li>
</ul>

<p>In a regular pipeline, you probably have to resort to using the Stored Procedure activity:</p>

<ul>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure"" rel=""nofollow noreferrer"">Transform data by using the SQL Server Stored Procedure activity in Azure Data Factory</a></li>
</ul>

<p>You would have to write the delete logic in the SP, and then invoke the SP from Data Factory.</p>
"
"57820938","How to execute a SQL query in Azure Data Factory","<p>I create a pipeline in ADF for performing copy activity. My source database is Azure SQL database and Sink is Azure Blob .I want to execute an SQL Query in ADF to delete data from source once data is copied to blob. I am not allowed to use copy or lookup to execute query.Is their any custom way to do this.I need to create a view and have to do some activity.Please help</p>
","<sql-server><azure><cloud><azure-sql-database><azure-data-factory>","2019-09-06 11:14:05","18132","4","4","57825055","<p>You can write a stored procedure for deleting the data from source table and call that stored procedure in ""Stored procedure"" activity after copy activity.</p>

<p>Your data flow will look like:</p>

<pre><code>COPY ACTIVITY -----&gt; STORED PROCEDURE ACTIVITY
</code></pre>
"
"57820938","How to execute a SQL query in Azure Data Factory","<p>I create a pipeline in ADF for performing copy activity. My source database is Azure SQL database and Sink is Azure Blob .I want to execute an SQL Query in ADF to delete data from source once data is copied to blob. I am not allowed to use copy or lookup to execute query.Is their any custom way to do this.I need to create a view and have to do some activity.Please help</p>
","<sql-server><azure><cloud><azure-sql-database><azure-data-factory>","2019-09-06 11:14:05","18132","4","4","62933664","<p>You can also use the built-in stored procedure <code>sp_executesql</code>, which allows you to provide a random SQL statement as parameter.
That way you don't have to implement your own stored procedure.</p>
<p>See more information about this stored procedure on <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-executesql-transact-sql?view=sql-server-ver15"" rel=""noreferrer"">sp_executesql (Transact-SQL)</a>.</p>
"
"57820938","How to execute a SQL query in Azure Data Factory","<p>I create a pipeline in ADF for performing copy activity. My source database is Azure SQL database and Sink is Azure Blob .I want to execute an SQL Query in ADF to delete data from source once data is copied to blob. I am not allowed to use copy or lookup to execute query.Is their any custom way to do this.I need to create a view and have to do some activity.Please help</p>
","<sql-server><azure><cloud><azure-sql-database><azure-data-factory>","2019-09-06 11:14:05","18132","4","4","71390404","<p>They have rolled out the <strong>script activity</strong></p>
<p>The script task can be used for the following purposes:</p>
<p>Truncate a table or view in preparation for inserting data.
Create, alter, and drop database objects such as tables and views.
Re-create fact and dimension tables before loading data into them.
Run stored procedures. If the SQL statement invokes a stored procedure that returns results from a temporary table, use the WITH RESULT SETS option to define metadata for the result set.
Save the rowset returned from a query as activity output for downstream consumption.</p>
<p><strong>Script task is present under General tab of Activities.</strong></p>
<p>Ref 1
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-script"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-script</a></p>
<p>Ref 2
<a href=""https://techcommunity.microsoft.com/t5/azure-data-factory-blog/execute-sql-statements-using-the-new-script-activity-in-azure/ba-p/3239969"" rel=""nofollow noreferrer"">https://techcommunity.microsoft.com/t5/azure-data-factory-blog/execute-sql-statements-using-the-new-script-activity-in-azure/ba-p/3239969</a></p>
"
"57816064","How to perform Archiving and purging in Azure SQL database","<p>I'm using Azure SQL database as my source database.Which include 10 tables.
I need to do archiving and purging in the table in a period gap of one year.Currently I am using Azure data factory to move data from source database to data warehouse.Can I do archiving and purging using data factory.If not please suggest me a right way to perform this Activity.</p>
","<azure><archive><azure-data-factory><purge><azure-database-mysql>","2019-09-06 05:11:24","2763","0","2","57825175","<p>If I understood your question correctly, you need to empty your table based on certain set of logics and criteria in a interval of one year.</p>

<p>What you can do is create your own SQL purge query and create a new pipeline to call it in stored procedure activity. Also, create a scheduled trigger which will run the pipeline once a year automatically say Jan 01, every year.</p>

<p>Every year on Jan 01 your pipeline will be triggered and call that stored procedure and execute it to purge the data.</p>
"
"57816064","How to perform Archiving and purging in Azure SQL database","<p>I'm using Azure SQL database as my source database.Which include 10 tables.
I need to do archiving and purging in the table in a period gap of one year.Currently I am using Azure data factory to move data from source database to data warehouse.Can I do archiving and purging using data factory.If not please suggest me a right way to perform this Activity.</p>
","<azure><archive><azure-data-factory><purge><azure-database-mysql>","2019-09-06 05:11:24","2763","0","2","57867479","<blockquote>
  <p>Can I do archiving and purging using data factory?</p>
</blockquote>

<p>Yes, you can. Azure Data Factory support Azure data warehouse. You can followed this tutorial: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure"" rel=""nofollow noreferrer"">Transform data by using the SQL Server stored procedure activity in Azure Data Factory</a>.</p>

<p>Firstly, you need to create a stored procedure in your database.</p>

<p>For example, below is the procedure to delete all the table data in database:</p>

<pre><code>IF EXISTS (SELECT * FROM sysobjects WHERE type='P' and name=N'P_DeleteAllData' ) 
    DROP PROCEDURE P_DeleteAllData 
GO 

CREATE PROCEDURE P_DeleteAllData 
AS
    EXEC sp_MSForEachTable 'ALTER TABLE ? NOCHECK CONSTRAINT ALL'  
    EXEC sp_MSForEachTable 'ALTER TABLE ? DISABLE TRIGGER ALL'  
    EXEC sp_MSForEachTable 'DELETE FROM ?'  
    EXEC sp_MSForEachTable 'ALTER TABLE ? CHECK CONSTRAINT ALL' 
    EXEC sp_MSForEachTable 'ALTER TABLE ? ENABLE TRIGGER ALL' 
GO 
</code></pre>

<p>Create a stored procedure activity, choose the Azure data warehouse as the linked service.</p>

<p><a href=""https://i.stack.imgur.com/vd8M7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vd8M7.png"" alt=""enter image description here""></a></p>

<p>Choose the stored procedure in you data warehouse:</p>

<p><a href=""https://i.stack.imgur.com/UlDmR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UlDmR.png"" alt=""enter image description here""></a></p>

<p>Add a trigger to schedule the pipeline execute:</p>

<p><a href=""https://i.stack.imgur.com/Uzok8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Uzok8.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"57811363","Azure Data Factory Copy Activity Rest API JSON response date format incorrect","<p>I am trying to copy data from rest api source using the azure copy activity. I have used Rest Api as source and csv as target. But the json response I am receiving is having the date as below format:
{Createddate: /date(345667999)/}</p>

<p>But when I hit preview data its giving the correct date format.(yyyy-mm-dd).</p>

<p>Can anyone suggest why is it like this? Is there any workaround to get the date value with correct format in the copy activity itself?</p>
","<azure-data-factory>","2019-09-05 18:43:34","352","2","1","57820980","<p>In the mappings tab of the Copy activity, you will need to set the target column type to either date or string. Please refer the below screenshot :</p>

<p><a href=""https://i.stack.imgur.com/BTfh2.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BTfh2.jpg"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"57811174","Azure Data Factory v2 intermittent error while calling stored procedure","<p>Running Azure Data Factory v2 with a for each loop with a batch count of 4-8. Calling several stored procedures and 1 copy activity. Targets are all the same Azure SQL Database. Running this setup for ~8 months in production.</p>

<p>Suddenly this week acceptance started to fail intermittently on calling the stored procedures. Production since last night (2019-09-05). All with the same error:</p>

<pre><code>{
    ""errorCode"": ""2011"",
    ""message"": ""An error occurred while sending the request."",
    ""failureType"": ""UserError"",
    ""target"": ""USP_End_Batch_Successful""
}
</code></pre>

<p>There is no pattern. Rerunning the pipeline results in failing other parts of the for-each loop. Setting the batch count lower, no improvement. Load on the database is not high. Log analytics on the databases show no blocks, deadlocks, dropped connections etc. Even the most stripped and basic stored procedures fail. No data on the database is changed at all.</p>

<p>The retry option will not work: the option is set to 1, and the stored procedure is not re-run.</p>

<p>Any clue how to dig further into this problem or any solution?</p>

<p>Example activity run id: 033ca5ab-c396-407f-8362-794459e4d0c4</p>
","<azure-sql-database><azure-data-factory>","2019-09-05 18:28:40","186","0","1","63066955","<p>Found the cause a few days later: we had a job running that was scaling the database during our ETL window. Hence running queries got killed at some point, resulting in the error above.</p>
"
"57796666","Merging dataset results in an Azure Data Factory pipeline","<p>I am reading a JSON-formatted blob from Azure Storage. I am then using one of the values in that JSON to query a database to get more information. What I need to do is take the JSON from the blob, add the fields from the database to it, then write that combined JSON to another Azure Storage. I cannot, however, figure out how to combine the two pieces of information.</p>

<p>I have tried custom mapping in the copy activity for the pipeline. I have tried parameterized datasets, etc. Nothing seems to provide the results I'm looking for.</p>

<p>Is there a way to accomplish this using native activities and parameters (i.e. not by writing a simple utility and executing it as a custom activity)?</p>
","<json><azure><dataset><pipeline><azure-data-factory>","2019-09-04 23:23:31","335","0","1","57802083","<p>For this I would recommend create a custom U-SQL job to do what you want. So first lookup for both the data you want. Do the job in the U-SQL job and copy the results to the Azure Storage. See this example for your pipeline:</p>

<p><a href=""https://i.stack.imgur.com/WH1R8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WH1R8.png"" alt=""enter image description here""></a></p>

<p>If you are not familiar to U-SQL this can help you:</p>

<ul>
<li><a href=""https://saveenr.gitbooks.io/usql-tutorial/content/"" rel=""nofollow noreferrer"">https://saveenr.gitbooks.io/usql-tutorial/content/</a></li>
</ul>

<p>Also this will help you working with Json in your job:</p>

<ul>
<li><p><a href=""https://www.taygan.co/blog/2018/01/09/azure-data-lake-series-working-with-json-part-2"" rel=""nofollow noreferrer"">https://www.taygan.co/blog/2018/01/09/azure-data-lake-series-working-with-json-part-2</a></p></li>
<li><p><a href=""https://www.taygan.co/blog/2018/03/02/azure-data-lake-series-working-with-json-part-3"" rel=""nofollow noreferrer"">https://www.taygan.co/blog/2018/03/02/azure-data-lake-series-working-with-json-part-3</a></p></li>
</ul>
"
"57796314","ADF json to sql copy empty value not inserted as null","<p>I am attempting to copy json data to a sql table and noticed that any empty value is not being inserted as a null, even though the column is nullable. It seems to be inserting an empty string.</p>

<p>I have tried to add nullValue and treatEmptyAsNull parameters like the code below, but that made no difference:</p>

<pre><code>""source"": {
        ""type"": ""BlobSource"",
        ""recursive"": true,
        ""nullValue"": """",
        ""treatEmptyAsNull"": true
    },
</code></pre>

<p>I am expecting a null to be inserted.
Is this standard behavior for ADF copy using json as a source to not insert empty values as null? Is there other properties I need to add to the json?</p>
","<azure-data-factory>","2019-09-04 22:25:15","2308","0","1","57799301","<p>The value inserted into SQL db can't be null directly because your source data is empty string <code>""""</code>,not null value. ADF copy activity can't convert empty string to null automatically for you.</p>

<p>However, you could <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#invoke-a-stored-procedure-from-a-sql-sink"" rel=""nofollow noreferrer"">invoke a stored procedure</a> in sql server dataset. In that SP, you could convert the <code>""""</code> to <code>null</code> value as you want before the columns inserted into table. Please follow the detail steps in above link or some example in my previous case:<a href=""https://stackoverflow.com/questions/51412683/azure-data-factory-copy-activity-failed-mapping-strings-from-csv-to-azure-sql"">Azure Data factory copy activity failed mapping strings (from csv) to Azure SQL table sink uniqueidentifier field</a>.</p>
"
"57794567","How Azure Data flow Actives run is run in sequence or parallel","<p>Is data flow actives run in parallel or sequence. In more detail, the image we have two sources  named as Source1 and Source2 so when I run this pipeline will it run both sources together, or it executed source1, and when it all done to sink1 then it executes the sources2 </p>

<p><a href=""https://i.stack.imgur.com/i3Fb5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i3Fb5.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2019-09-04 19:14:23","603","0","1","57801204","<p>The Data Flow actives run  in sequence. When we run the Data flow, it executed source1, and when it all done to sink1 then it executes the sources2. You can find this in pipeline details.</p>

<p>For example, I create two actives in my pipeline Data Flow.</p>

<p><a href=""https://i.stack.imgur.com/ZYhJ6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZYhJ6.png"" alt=""enter image description here""></a></p>

<p>When we run the Data Flow, we can see the details from here.
<a href=""https://i.stack.imgur.com/7vZgZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7vZgZ.png"" alt=""enter image description here""></a></p>

<p>Before the pipeline Data Flow actives running.
<a href=""https://i.stack.imgur.com/SdxNN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SdxNN.png"" alt=""enter image description here""></a></p>

<p>After the pipeline Data Flow actives running succeed:</p>

<p><a href=""https://i.stack.imgur.com/mkVtO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mkVtO.png"" alt=""enter image description here""></a></p>

<p>The details become this: 
<a href=""https://i.stack.imgur.com/GNXjT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GNXjT.png"" alt=""enter image description here""></a></p>

<p><strong>My source1 data preview:</strong>
<a href=""https://i.stack.imgur.com/KTt9Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KTt9Q.png"" alt=""enter image description here""></a></p>

<p><strong>My source2 data preview:</strong>
<a href=""https://i.stack.imgur.com/9aPJO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9aPJO.png"" alt=""enter image description here""></a></p>

<p><strong>Result in my table:</strong></p>

<p><a href=""https://i.stack.imgur.com/FmnbX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FmnbX.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"57794556","Replace more than value in Azure Data Factory Data Flow","<p>I want to replace some values in one column using expression language in Data Factory Data Flow. I tried to combine multiple replace statements and if conditions in Derived Columns but it didn’t work.</p>

<p>For example:</p>

<pre><code>case

when column = ’SA’ then ‘SB’

when column = ‘PA’ then ‘PO’

when column = ‘KL’ then ‘KLL’

when column = ‘AAB’ then ‘A’

when column = ‘WWE’ then ‘A’

else ‘n’

end
</code></pre>

<p>Does anybody know how to handle this? I am new with Data Factory and need some help please.</p>
","<azure><etl><data-warehouse><azure-data-factory>","2019-09-04 19:13:45","3771","2","2","57802091","<p>This is because your expression is wrong.</p>

<p>About how to build a correctly expressions, please reference:</p>

<ol>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-expression-builder"" rel=""nofollow noreferrer"">Mapping Data Flow Expression Builder</a>.</li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions"" rel=""nofollow noreferrer"">Data transformation expressions in Mapping Data Flow</a>.</li>
</ol>

<p>For example, replace my <code>ename</code> columne with 'zzz' using this expression in Visual Expression Builder:</p>

<pre><code>case(ename=='aaa','zzz',ename)
</code></pre>

<p><a href=""https://i.stack.imgur.com/gLj91.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gLj91.png"" alt=""enter image description here""></a></p>

<p>Another test, add more conditions:
<a href=""https://i.stack.imgur.com/wgPln.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wgPln.png"" alt=""enter image description here""></a></p>

<p>So your expression should like this:</p>

<pre><code>case(column=='SA','SB',
     column=='PA','PO',
     column== 'KL','KLL',
     column== 'AAB','A',
     column== 'WWE','A',
     'n'
    )
</code></pre>

<p>Hope this helps.</p>
"
"57794556","Replace more than value in Azure Data Factory Data Flow","<p>I want to replace some values in one column using expression language in Data Factory Data Flow. I tried to combine multiple replace statements and if conditions in Derived Columns but it didn’t work.</p>

<p>For example:</p>

<pre><code>case

when column = ’SA’ then ‘SB’

when column = ‘PA’ then ‘PO’

when column = ‘KL’ then ‘KLL’

when column = ‘AAB’ then ‘A’

when column = ‘WWE’ then ‘A’

else ‘n’

end
</code></pre>

<p>Does anybody know how to handle this? I am new with Data Factory and need some help please.</p>
","<azure><etl><data-warehouse><azure-data-factory>","2019-09-04 19:13:45","3771","2","2","57810048","<p>This should work </p>

<pre><code>case(fname=='SA','SB', fname=='PA','PO',fname=='KL','KLL', fname=='AAB','A',fname=='WWE','A','n' )
</code></pre>
"
"57792857","Azure data factory data lake analytics linked service - failed because the connected party did not properly respond","<p>I have created a azure data factory to perform some USQL activities. while creating a New Linked Service (Azure Data Lake Analytics) getting following error.</p>

<p>while searching on this issue, found that developer facing different kind of issues with ADF.</p>

<p>Is anything am I missing?</p>

<p>Error: Failed to connect to ADLA account 'ad-cxp-analytics-c11' with error 'An error occurred while sending the request.'. An error occurred while sending the request. Unable to connect to the remote server A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond 40.90.138.193:443 Activity ID: c218e103-f0a8-4b07-811d-014d39607dcc.</p>
","<azure><azure-data-factory><azure-data-lake>","2019-09-04 16:57:03","817","0","1","57849997","<p>I would suggest you having looked into the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-integration-runtime"" rel=""nofollow noreferrer"">IR logs</a>.As its appears its seems the IR is having issues and you should have more info on the logs. Also capturing the performance metrics on the IR server will help.</p>

<p>Also as a short term solution ,you could try to enable the retry option on the activity.</p>

<p><a href=""https://i.stack.imgur.com/hG6zq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hG6zq.png"" alt=""enter image description here""></a></p>
"
"57792395","Azure Data flow taking mins to trigger next pipeline","<p>Azure Data factory transferring data in Db in 10 millisecond but the issue I am having is it is waiting for few mins to trigger next pipeline and that ends up with 40 mins all pipelines are taking less than 20 ms to transfer data. But somehow it is waiting a few mins to trigger the next one.</p>

<p><a href=""https://i.stack.imgur.com/3V1uR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3V1uR.png"" alt=""enter image description here""></a>
I used debug mode as well trigger the ADF using Logic App without debugging mood. Is there any way I can optimize it we want to move from SSIS to Data Flow but having a time issue 40 mins are so much in next step we have millions of records </p>

<p><strong>so it took 7 seconds to transfer data to dataBase but it waited for 6 mins :( check the image below</strong></p>

<p><a href=""https://i.stack.imgur.com/XX8Jj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XX8Jj.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/QnC83.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QnC83.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-lake><azure-data-factory>","2019-09-04 16:21:02","1553","1","2","57815081","<p>This document <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-performance#monitor-data-flow-performance"" rel=""nofollow noreferrer"">Monitor data flow performance</a> mentioned that:</p>

<blockquote>
  <p>Note that you can assume 1 minute of cluster job execution set-up time in your overall performance calculations and if you are using the default Azure Integration Runtime, you may need to add 5 minutes of cluster spin-up time as well.</p>
</blockquote>

<p>That's maybe a reason. You can first follow this tutorial <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-performance#monitor-data-flow-performance"" rel=""nofollow noreferrer"">Mapping data flows performance and tuning guide</a>.</p>

<p>This document <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-data-flow-activity"" rel=""nofollow noreferrer"">Execute data flow activity in Azure Data Factory</a> also can help us improve the performance.</p>

<p>Choose the compute environment for this execution of your data flow. The default is the Azure Auto-Resolve Default Integration Runtime. This choice will execute the data flow on the Spark environment in the same region as your data factory. The compute type will be a job cluster, which means the compute environment will take several minutes to start-up.</p>

<p>You have control over the Spark execution environment for your Data Flow activities. In the Azure integration runtime are settings to set the compute type (general purpose, memory optimized, and compute optimized), number of worker cores, and time-to-live to match the execution engine with your Data Flow compute requirements. Also, setting TTL will allow you to maintain a warm cluster that is immediately available for job executions.</p>

<p>Note:</p>

<p>The Integration Runtime selection in the Data Flow activity only applies to triggered executions of your pipeline. Debugging your pipeline with Data Flows with Debug will execute against the 8-core default Spark cluster.</p>

<p>Hope this helps.</p>
"
"57792395","Azure Data flow taking mins to trigger next pipeline","<p>Azure Data factory transferring data in Db in 10 millisecond but the issue I am having is it is waiting for few mins to trigger next pipeline and that ends up with 40 mins all pipelines are taking less than 20 ms to transfer data. But somehow it is waiting a few mins to trigger the next one.</p>

<p><a href=""https://i.stack.imgur.com/3V1uR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3V1uR.png"" alt=""enter image description here""></a>
I used debug mode as well trigger the ADF using Logic App without debugging mood. Is there any way I can optimize it we want to move from SSIS to Data Flow but having a time issue 40 mins are so much in next step we have millions of records </p>

<p><strong>so it took 7 seconds to transfer data to dataBase but it waited for 6 mins :( check the image below</strong></p>

<p><a href=""https://i.stack.imgur.com/XX8Jj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XX8Jj.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/QnC83.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QnC83.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-lake><azure-data-factory>","2019-09-04 16:21:02","1553","1","2","57828235","<p>You will hit the Databricks cluster spin-up time during job (triggered) execution.</p>

<p>As long as you are in Debug mode, you'll always hit a warmed cluster while the debug session is still green.</p>

<p>We've added TTL to the Azure IR in the Data Flow configuration section so that you can keep a cluster alive for your next data flow activity and you won't incur the start-up penalty on your next execution.</p>

<p>Note that option is greyed out at this time, but will enable it soon.</p>
"
"57791771","Azure Data Factory: How to pass triggered blob container name to the invoked Function Activity","<p>I've configured a trigger on blob container change which should invoke a Function activity(HTTP POST), and I want to use this container name in the function itself(trigger container could be changed frequently, and I don't want to change function code). How can I pass it?</p>

<p>I found only an example for the copy activity
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger</a></p>
","<azure-data-factory>","2019-09-04 15:40:05","1142","0","1","57797864","<p>Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger"" rel=""nofollow noreferrer"">link</a> you posted in your question,you could pass the value of <code>folder path</code> and <code>file name</code> to pipeline as parameters. <code>@triggerBody().folderPath</code> and <code>@triggerBody().fileName</code> could be configured in the parameters of pipeline.</p>

<p><a href=""https://i.stack.imgur.com/R37rg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/R37rg.png"" alt=""enter image description here""></a></p>

<p>For example:</p>

<p><a href=""https://i.stack.imgur.com/tkiAF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tkiAF.png"" alt=""enter image description here""></a></p>

<p>Then if you want to get the container name ,you just need to split the folder path with <code>/</code> so that you could get the root path which is container name exactly.</p>
"
"57787815","Is there any option in Azure Data factory to retain same date format while copying data from source systems","<p>I am copying data from Oracle Table to Azure Data Lake using Azure data factory copy activity.</p>

<p>Where ever my date format is <code>DD-MM-YYYY</code>, azure converting them into DATE_TIME format. 
Is there any way to retain the same format as the source</p>

<p>I am able to hard code the format for a specific column if copy activity is for a single table.
In my scenario, I am trying to copy for multiple tables using the same copy activity, so It is not possible to update the schema</p>
","<azure-data-factory>","2019-09-04 11:51:12","80","0","1","57788073","<p>There is no support for <code>DATE</code> as a data type while on ADLS (data lake store). The only supported data type is <code>DateTime and DateTimeOffset</code></p>

<p>In order for you to continue working with the <code>DATE</code> data type, the ETL which gets created on top of this data should address this i.e. typecast in the ETL build phase.</p>
"
"57780509","How to force to set Pipelines' status to failed","<p>I'm using Copy Data.
When there is some data error. I would export them to a blob.
But in this case, the Pipelines's status is still Succeeded. I want to set it to false. Is it possible?</p>
","<azure-data-factory>","2019-09-04 01:47:57","1371","1","1","57780690","<blockquote>
  <p>When there is some data error.</p>
</blockquote>

<p>It depends on what error you mentioned here.</p>

<p>1.If you mean it's common incompatibility or mismatch error, ADF supports built-in feature named <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-fault-tolerance#supported-scenarios"" rel=""nofollow noreferrer"">Fault tolerance</a> in Copy Activity which supports below 3 scenarios:</p>

<ul>
<li>Incompatibility between the source data type and the sink native
type.</li>
<li>Mismatch in the number of columns between the source and the sink.</li>
<li>Primary key violation when writing to SQL Server/Azure SQL
Database/Azure Cosmos DB.</li>
</ul>

<p>If you configure to log the incompatible rows, you can find the log file at this path: <code>https://[your-blob-account].blob.core.windows.net/[path-if-configured]/[copy-activity-run-id]/[auto-generated-GUID].csv</code>.</p>

<p>If you want to abort the job as soon as any error occurs,you could set as below:</p>

<p><a href=""https://i.stack.imgur.com/HlfWl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HlfWl.png"" alt=""enter image description here""></a></p>

<p>Please see this case: <a href=""https://stackoverflow.com/questions/48088308/fault-tolerance-and-log-the-incompatible-rows-in-azure-blob-storage"">Fault tolerance and log the incompatible rows in Azure Blob storage</a></p>

<p>2.If you are talking about your own logic for the data error,may some business logic. I'm afraid that ADF can't detect that for you, though it's also a common requirement I think. However,you could follow this case (<a href=""https://stackoverflow.com/questions/43535339/how-to-control-data-failures-in-azure-data-factory-pipelines"">How to control data failures in Azure Data Factory Pipelines?</a>) to do a workaround. The main idea is using custom activity to divert the bad rows before the execution of copy activity. In custom activity, you could upload the bad rows into Azure Blob Storage with .net SDK as you want.</p>

<hr>

<p><strong><em>Update:</em></strong></p>

<p>Since you want to log all incompatible rows and enforce the job failed at the same time, I'm afraid that it can not be implemented in the copy activity directly.</p>

<p>However, I came up with an idea that you could use If Condition activity after Copy Activity to judge if the output contains <code>rowsSkipped</code>. If so, output False,then you will know there are some skip data so that you could check them in the blob storage.</p>

<p><a href=""https://i.stack.imgur.com/KwBFS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KwBFS.png"" alt=""enter image description here""></a></p>
"
"57776810","Azure Data Factory Dataset Dynamic Folder Path","<p>I have a data set that resides under a folder path where the date is dynamic (e.g. rootfolder/subfolder/yyyy/mm/dd/subfolder/subfolder), and I am trying to pull it with a copy activity. So far I cannot get Data Factory to recognize that my date is dynamic...</p>

<p>This is the code that I have tried so far:</p>

<pre><code>[""rootfolder/subfolder/subfolder/subfolder/subfoler/@{formatDateTime(utcnow(),'yyyy')}/@{formatDateTime(utcnow(),'MM')}/@{formatDateTime(utcnow(),'dd')}/subfolder/file""]
</code></pre>
","<azure><azure-data-factory>","2019-09-03 18:12:04","5954","1","2","57776963","<p>You need to make use of concat function provided by data factory.</p>

<pre><code>@concat('rootfolder/subfolder/subfolder/subfolder/subfolder/',formatDateTime(utcnow(),'yyyy'),'/',formatDateTime(utcnow(),'MM'),'/',formatDateTime(utcnow(),'dd'),'/subfolder/file')
</code></pre>

<p>The concat function is similar as in programming languages which concats the strings.</p>

<p>More details: <a href=""https://stackoverflow.com/a/57535449/4111181"">Azure Data Factory Loop Through Files</a></p>
"
"57776810","Azure Data Factory Dataset Dynamic Folder Path","<p>I have a data set that resides under a folder path where the date is dynamic (e.g. rootfolder/subfolder/yyyy/mm/dd/subfolder/subfolder), and I am trying to pull it with a copy activity. So far I cannot get Data Factory to recognize that my date is dynamic...</p>

<p>This is the code that I have tried so far:</p>

<pre><code>[""rootfolder/subfolder/subfolder/subfolder/subfoler/@{formatDateTime(utcnow(),'yyyy')}/@{formatDateTime(utcnow(),'MM')}/@{formatDateTime(utcnow(),'dd')}/subfolder/file""]
</code></pre>
","<azure><azure-data-factory>","2019-09-03 18:12:04","5954","1","2","57779351","<p>Just to build pm Anish K answer you can also shorten this a bit by using formatting </p>

<pre><code>formatDateTime(utcnow(),'yyyy/MM/dd')
</code></pre>

<p>So final answer would be</p>

<pre><code>@concat('rootfolder/subfolder/subfolder/subfolder/subfolder/',formatDateTime(utcnow(),'yyyy//MM/dd'),'/subfolder/file')
</code></pre>

<p>In case you want to learn a bit more on parametrization on ADF feel free to check out this video <a href=""https://youtu.be/pISBgwrdxPM"" rel=""nofollow noreferrer"">https://youtu.be/pISBgwrdxPM</a></p>
"
"57773847","How to pass table name a parameter in Azure data factory","<p>I'm a new user in the Azure data factory. I tried to perform copy activity in between Azure database as the source and Azure data warehouse as a target . In that, I am not able to perform an update into target using data factory.  In which Source as Azure SQL server and target as the Azure data warehouse.  And how to update the field in data warehouse table in Azure using ADF when the data warehouse is a sink</p>
","<database><azure><updates><data-warehouse><azure-data-factory>","2019-09-03 14:32:37","2256","1","1","57819185","<p>From your pasted link I assume what expected is incrementally copy data from Azure sql database to Azure data warehouse.</p>

<p>In this case no need to modify the sink database, instead finish the modification before loading into sink table.</p>

<p>I followed the steps in this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-portal"" rel=""nofollow noreferrer"">article</a>, in which the target is: Azure Blob storage, I changed it to: Azure data warehouse. And the test result is successfully implemented data copying from Azure database to Azure data warehouse incrementally.</p>
"
"57772608","Azure datafactory deployment automation from multiple branches","<p>I want to create automated deployment pipeline for azure datafactory.
For one stream of development we can configure it using doc 
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a></p>

<p>But when it comes to deploying to two diff test datafactories for parrallel features development (in two different branches), it is not working because the adb_publish which gets generated is only specific to the one datafactory. </p>

<p>Currently we are doing deployement using powershell scripts and passing object list which needs to be deployed.
Our repo is in Azure devops.</p>

<p>I tried</p>

<ol>
<li>linking the repo to multiple df but then it is causing issue, perhaps when finding deltas to publish.</li>
<li>Creating forks of repo instead of branches so that adb_publish can be seperate for the every datafactory - but this approach will not work when there is a conflict, which needs manual merge, so the testing will be required again instead of moving to prod.</li>
</ol>
","<azure-devops><azure-data-factory>","2019-09-03 13:18:52","554","1","2","57773188","<p>Adf_publish get generated whenever you publish. Publishing takes whatever you have in your repo and updates data factory with it.</p>

<p>To develop multiple features in parallel, you need to just use ""Save"". Save will commit your changes to the branch you are actually working on. Other branches will do the same. Whenever you want to publish, you need to first make a pull request from your branch to master, then publish. Any merge conflict should be solved when merging everything in the master branch. Then just publish and there shouldn't be any conflicts, and adf_publish will get generated after that.</p>

<p>Hope this helped!</p>
"
"57772608","Azure datafactory deployment automation from multiple branches","<p>I want to create automated deployment pipeline for azure datafactory.
For one stream of development we can configure it using doc 
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a></p>

<p>But when it comes to deploying to two diff test datafactories for parrallel features development (in two different branches), it is not working because the adb_publish which gets generated is only specific to the one datafactory. </p>

<p>Currently we are doing deployement using powershell scripts and passing object list which needs to be deployed.
Our repo is in Azure devops.</p>

<p>I tried</p>

<ol>
<li>linking the repo to multiple df but then it is causing issue, perhaps when finding deltas to publish.</li>
<li>Creating forks of repo instead of branches so that adb_publish can be seperate for the every datafactory - but this approach will not work when there is a conflict, which needs manual merge, so the testing will be required again instead of moving to prod.</li>
</ol>
","<azure-devops><azure-data-factory>","2019-09-03 13:18:52","554","1","2","57785589","<p>Since a GitHub repository can be associated with only one data factory. And you are only allowed to publish to the Data Factory service from your collaboration branch. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/source-control"" rel=""nofollow noreferrer"">Check this</a></p>

<p>It seems there is not a direct and easy way to accomplish this. If forking repo as workaround, you may have to solve the conflicts before merging as @Martin suggested.</p>
"
"57771503","ADF pipelines not able to run on self hosted integration runtime","<p>We have Azure Data Factory V2 pipeline, consuming data from on-prem SQL Server, Pipelines was running perfectly fine until 2 weeks. 2 weeks back it started running very slow (15min vs 2h:15min).
So today we tried to restart the machine which installed IR. After that pipeline started giving an error: </p>

<blockquote>
  <p>Unable to load DLL 'jvm.dll': The specified module could not be found.</p>
</blockquote>

<p>We have verify everything mentioned in <a href=""https://support.microsoft.com/en-in/help/4497239/hresult-0x8007007e-error-when-you-run-a-copy-activity-to-azure"" rel=""nofollow noreferrer"">post</a>.</p>

<p>Then we have reinstalled the Integration Runtime on the machine and now pipelines keep running without transferring data. All the pipelines lies in the queue. There is no activity we can see on IR Monitor. Pipelines are not sending any request to IR.</p>
","<azure-storage><azure-data-factory>","2019-09-03 12:10:44","783","0","1","58022768","<p>Sharing the answer as per the comment by the original poster:</p>

<p>Able to resolve the issue by reinstalling JRE and integration runtime. </p>

<p>Try a clean install. Uninstall the IR and the JRE, then install JRE, then IR.</p>
"
"57768712","In ADF, can we use one Integration Runtime for two different on-premise data sources?","<p>IN Azure Data Factory, is it possible to use one Integration Run time to connect two different On-Premise data sources?</p>
<p>Scenario:
I have created one self hosted Integration Runtime installed in Virtual Machine for DB2 Database which is On-Premise DB.</p>
<p>I wanted to add one more On-Premises DB which is SQL Server.
Is it possible to use the existing Self Hosted Integration Runtime for SQL Server On Prem DB?</p>
<p>I have tried connecting to existing Self Hosted Integration Runtime in Linked Service. The test connection is getting failed.</p>
<p>I know, some where access privileges required for SQL Server DB either from VM or from the SQL Server to make the connectivity possible via existing Integration Runtime.</p>
<p>Connectivity to SQL Server DB is getting failed, while I use the existing IR, which is already used for DB2.</p>
","<azure><azure-sql-database><azure-data-factory>","2019-09-03 09:23:12","5092","0","3","57769041","<p>Yes you can reuse self-hosted IR.</p>

<p>Probably issue with connectivity lies somewhere else.</p>

<p>You can test this by logging into that VM via RDP and running tests either with SSMS to test connectivity or run simple PowerShell command to test network</p>

<pre><code>Test-NetConnection ""&lt;server_address&gt;"" -port 1433
</code></pre>
"
"57768712","In ADF, can we use one Integration Runtime for two different on-premise data sources?","<p>IN Azure Data Factory, is it possible to use one Integration Run time to connect two different On-Premise data sources?</p>
<p>Scenario:
I have created one self hosted Integration Runtime installed in Virtual Machine for DB2 Database which is On-Premise DB.</p>
<p>I wanted to add one more On-Premises DB which is SQL Server.
Is it possible to use the existing Self Hosted Integration Runtime for SQL Server On Prem DB?</p>
<p>I have tried connecting to existing Self Hosted Integration Runtime in Linked Service. The test connection is getting failed.</p>
<p>I know, some where access privileges required for SQL Server DB either from VM or from the SQL Server to make the connectivity possible via existing Integration Runtime.</p>
<p>Connectivity to SQL Server DB is getting failed, while I use the existing IR, which is already used for DB2.</p>
","<azure><azure-sql-database><azure-data-factory>","2019-09-03 09:23:12","5092","0","3","57770329","<p>Yes, you can. Note that adding more nodes as part of self-hosted IR (integration runtime) is part of highly available and making sure that there is no SPOF (single point of failure) with one on-premise data gateway.</p>

<p>This is no relation with the number of on-premise data sources which can be connected from services launched in Azure.</p>
"
"57768712","In ADF, can we use one Integration Runtime for two different on-premise data sources?","<p>IN Azure Data Factory, is it possible to use one Integration Run time to connect two different On-Premise data sources?</p>
<p>Scenario:
I have created one self hosted Integration Runtime installed in Virtual Machine for DB2 Database which is On-Premise DB.</p>
<p>I wanted to add one more On-Premises DB which is SQL Server.
Is it possible to use the existing Self Hosted Integration Runtime for SQL Server On Prem DB?</p>
<p>I have tried connecting to existing Self Hosted Integration Runtime in Linked Service. The test connection is getting failed.</p>
<p>I know, some where access privileges required for SQL Server DB either from VM or from the SQL Server to make the connectivity possible via existing Integration Runtime.</p>
<p>Connectivity to SQL Server DB is getting failed, while I use the existing IR, which is already used for DB2.</p>
","<azure><azure-sql-database><azure-data-factory>","2019-09-03 09:23:12","5092","0","3","57797387","<p>Yes, you can.</p>

<p>You can find this in this document <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#considerations-for-using-a-self-hosted-ir"" rel=""nofollow noreferrer"">Considerations for using a self-hosted IR</a>:</p>

<p>A single self-hosted integration runtime can be used for multiple on-premises data sources. A single self-hosted integration runtime can be shared with another data factory within the same Azure Active Directory tenant. For more information, see <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#sharing-the-self-hosted-integration-runtime-with-multiple-data-factories"" rel=""nofollow noreferrer"">Sharing a self-hosted integration runtime</a>.</p>

<p>When you want add a another on premise DB, you can try like this:</p>

<p>New link service:
<a href=""https://i.stack.imgur.com/bbWdm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bbWdm.png"" alt=""enter image description here""></a></p>

<p>Add another on premise DB:</p>

<p><a href=""https://i.stack.imgur.com/Ly03r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ly03r.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"57765991","How to run python egg (present in azure databricks) from Azure data factory?","<p>So I created a small pyspark application and converted it to an egg. Uploaded it to dbfs:/FileStore/jar/xyz.egg. In ADF I used jar activity. But in Main Class Name textbox i am confused what to provide.<a href=""https://i.stack.imgur.com/UFBrQ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/UFBrQ.png"" alt=""enter image description here""></a></p>

<p>My Pycharm application has three files, two of them are basically Utility files that contains utility functions that I call and the content of main file is: </p>

<p>Main.py</p>

<pre><code>from CommonUtils import appendZeros
from sampleProgram import writedf


def main():
    appendZeros('zzz')
    writedf()


if __name__ == ""__main__"":
    main()
</code></pre>

<p>Now what to specify in 'Main class name' textbox?</p>
","<pyspark><azure-data-lake><azure-data-factory><egg>","2019-09-03 06:19:16","538","5","1","58057198","<p><strong>Note:</strong> Main Class Name is ""The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library.""</p>

<p>The following table describes the JSON properties used in the JSON definition:</p>

<p><a href=""https://i.stack.imgur.com/9BwZ4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9BwZ4.png"" alt=""enter image description here""></a></p>

<p>Reference:  ""<a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-databricks-jar"" rel=""nofollow noreferrer"">Transform data by running a Jar activity in Azure Databricks</a>"".</p>

<p>Hope this helps.</p>

<hr>

<p>If this answers your query, do click “Mark as Answer” and ""Up-Vote"" for the same. And, if you have any further query do let us know.</p>
"
"57765525","How to execute a PowerShell Command from within Azure Data Factory custom activity?","<p>I have a custom activity in Azure Data Factory, which attempts to execute the following command:</p>

<p><code>PowerShell.exe -Command ""Write-Host 'Hello, world!'""</code></p>

<p>However, when I debug (run) this command from within Azure Data Factory, it runs for a long time, and finally fails.</p>

<p>I guess it fails because perhaps it could not locate ""PowerShell.exe"". How can I ensure that the ADF Custom Activity has access to PowerShell.exe?</p>

<p>Some sites say about specifying a package (.zip file) that contains everything needed for the exe to execute. However, since PowerShell is from Microsoft, I think it would be inappropriate to ZIP the PowerShell directory, and specify it as a package to the Custom Activity.</p>

<p>Please suggest as to how I can execute PowerShell command from Custom Activity of an Azure Data Factory. Thanks!</p>

<p>Whenever I search ""Execute PowerShell from Custom Activity in Azure Data Factory"", the search results are talking more about which Az PowerShell command to use to trigger start an ADF pipeline.</p>

<p>I saw two threads in Stackoverflow.com, where the answer just specifies to use a Custom Activity, and the answer is not specific to PowerShell command call from ADF</p>

<p>Here is the JSON for the task:</p>

<p><code>{
    ""name"": ""ExecutePs1CustomActivity"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""ExecutePSScriptCustomActivity"",
                ""type"": ""Custom"",
                ""dependsOn"": [],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""userProperties"": [],
                ""typeProperties"": {
                    ""command"": ""PowerShell.exe -Command \""Write-Host 'Hello, world!'\"""",
                    ""referenceObjects"": {
                        ""linkedServices"": [],
                        ""datasets"": []
                    }
                },
                ""linkedServiceName"": {
                    ""referenceName"": ""Ps1CustomActivityAzureBatch"",
                    ""type"": ""LinkedServiceReference""
                }
            }
        ],
        ""annotations"": []
    }
}</code></p>

<p>I see ""In Progress"" for 3 minutes (180 seconds), and then it shows as ""Failed.""</p>
","<powershell><azure-data-factory><custom-activity>","2019-09-03 05:28:08","1544","0","1","57866641","<p>I would suggest you to move all you scripting task in a powershell file and copy it to a storage account linked with your custom activity. . Once done try to call it like below:</p>

<pre><code>powershell .\script.ps1
</code></pre>

<p>You can also provide the path of the script in json like below:</p>

<pre><code>{
  ""name"": ""MyCustomActivityPipeline"",
  ""properties"": {
    ""description"": ""Custom activity sample"",
    ""activities"": [{
      ""type"": ""Custom"",
      ""name"": ""MyCustomActivity"",
      ""linkedServiceName"": {
        ""referenceName"": ""AzureBatchLinkedService"",
        ""type"": ""LinkedServiceReference""
      },
      ""typeProperties"": {
        ""command"": ""helloworld.exe"",
        ""folderPath"": ""customactv2/helloworld"",
        ""resourceLinkedService"": {
          ""referenceName"": ""StorageLinkedService"",
          ""type"": ""LinkedServiceReference""
        }
      }
    }]
  }
}
</code></pre>

<p>Please try it and see if it helps. Also i would suggest you to troubleshoot the pipeline steps to look for detailed error.</p>

<p>Also to your second point ""<strong>Some sites say about specifying a package (.zip file) that contains everything needed for the exe to execute.""</strong> This is required when you are building a custom activity using dot net then it is must copy all the Dll's and Exe's for execution.</p>

<p>Hope it helps.</p>
"
"57763549","Self-Hosted IR security concerns","<p>..Hi all, in my team we have to install an Integration Runtime and they are having some security concerns about what ports are opened. Which ones are needed? Also, if they have a VPN, do they have to include the Azure subscription into it?</p>
","<azure-data-factory>","2019-09-02 23:39:19","1605","0","1","57763639","<p>This is the documentation I use everytime when a client has security questions about the self hosted IR:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime</a></p>

<p>About the VPN issue, there is no need to include anything on the VPN as far as the required ports are opened (443 for blob storage, 1433 if using a SQL Server in the cloud).</p>

<p>Hope this helped!</p>
"
"57758402","Access API hosted in Azure VM on Private IP via Azure Data Factory","<p>I have Apache server running on Azure VM that exposes certain APIs. I am triggering those APIs using web activity of Azure Data Factory. My question is How can I acess the API over private network rather than using public API. that way I can insulate my VM from outside access and hence making it secure</p>
","<azure><networking><azure-data-factory><azure-virtual-network><azure-vm-scale-set>","2019-09-02 14:09:49","204","0","1","57764651","<p>You could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">Self-hosted integration runtime</a> to transfer between a cloud data stores and a data store in private network.</p>

<p>If you want to perform data integration securely in a private network environment, which does not have a direct line-of-sight from the public cloud environment, you can install a self-hosted IR on premises environment behind your corporate firewall, or inside a virtual private network. The self-hosted integration runtime only makes outbound HTTP-based connections to open internet.</p>

<p>Note:Self-Hosted IR only supports Windows OS System so far. More detailed steps,please refer to this <a href=""https://www.mssqltips.com/sqlservertip/5812/connect-to-onpremises-data-in-azure-data-factory-with-the-selfhosted-integration-runtime--part-1/"" rel=""nofollow noreferrer"">document</a>.</p>
"
"57757155","How to access windowStartTime?","<p>I want to use the windowStartTime parameter to access files that use a date based folder structure.</p>

<p>I have the below trigger:</p>

<pre><code>{
    ""name"": ""trigger1"",
    ""properties"": {
        ""annotations"": [],
        ""runtimeState"": ""Started"",
        ""pipeline"": {
            ""pipelineReference"": {
                ""referenceName"": ""Neil_Test"",
                ""type"": ""PipelineReference""
            },
            ""parameters"": {
                ""Window1"": {
                ""type"": ""Expression"",
                ""value"": ""@{trigger().outputs.windowStartTime}""
                }
            }
        },
        ""type"": ""TumblingWindowTrigger"",
        ""typeProperties"": {
            ""frequency"": ""Hour"",
            ""interval"": 24,
            ""startTime"": ""2019-08-30T00:00:00.000Z"",
            ""delay"": ""00:00:00"",
            ""maxConcurrency"": 1,
            ""retryPolicy"": {
                ""intervalInSeconds"": 30
            },
            ""dependsOn"": []
        }
    }
}
</code></pre>

<p>This sets the <code>Window1</code> parameter on the pipeline object to <code>NULL</code>.
How do I get actual the value of the windowStartTime parameter into my pipeline?</p>
","<azure-data-factory>","2019-09-02 12:43:27","265","1","1","57763750","<p>You issue here are the {}. This one is working fine for me:</p>

<pre><code>{
""name"": ""triggerwin"",
""properties"": {
    ""annotations"": [],
    ""runtimeState"": ""Started"",
    ""pipeline"": {
        ""pipelineReference"": {
            ""referenceName"": ""setVar"",
            ""type"": ""PipelineReference""
        },
        ""parameters"": {
            ""FileName"": ""@trigger().outputs.windowStartTime""
        }
    },
    ""type"": ""TumblingWindowTrigger"",
    ""typeProperties"": {
        ""frequency"": ""Minute"",
        ""interval"": 15,
        ""startTime"": ""2019-09-02T00:00:00.000Z"",
        ""delay"": ""00:00:00"",
        ""maxConcurrency"": 50,
        ""retryPolicy"": {
            ""intervalInSeconds"": 30
        },
        ""dependsOn"": []
    }
}
}
</code></pre>

<p><a href=""https://i.stack.imgur.com/jCt9P.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jCt9P.png"" alt=""Parameter shown""></a></p>

<p>Hope this helped!!</p>
"
"57753824","Parameterize connections in Azure data factory (ARM templates)","<p>I am trying to setup an Azure Data Factory in a CI/CD setup. I followed Microsoft's best practices (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a>). </p>

<p>I have 4 environments (dev, test, UAT, PRD)</p>

<p>In short what I have done:</p>

<ul>
<li><p>Create an Azure data factory and link it to my Azure DevOps repo on DEV environment</p></li>
<li><p>Create an Azure data factory on the other environments (test, UAT and PRD), but do NOT link it to DevOps. Instead, pipelines are released on these data factories using ARM templates and release pipelines in Azure DevOps.</p></li>
<li><p>I have parameterized all necessary parts to be able to overwrite the settings in each of my environments.</p></li>
</ul>

<p>At this moment, I am able to succesfully deploy to my other environments, however, the linkedservice to my database on azure is not working. I have parameterized everything, like Microsoft suggests, but when I export my linkedservice to an ARM template, it uses a connection string instead of my parameterized settings.</p>

<p>Below is a picture of my settings in the Azure Data Factory portal:
<img src=""https://i.ibb.co/Vvvy5wp/daf-setup.png"" alt=""daf-setup""></p>

<p>When I try to export this to ARM templates, I get the following:</p>

<pre><code>{
            ""name"": ""[concat(parameters('factoryName'), '/database')]"",
            ""type"": ""Microsoft.DataFactory/factories/linkedServices"",
            ""apiVersion"": ""2018-06-01"",
            ""properties"": {
                ""parameters"": {
                    ""sqlServerUrl"": {
                        ""type"": ""string""
                    },
                    ""databaseName"": {
                        ""type"": ""string""
                    },
                    ""sqlPwd"": {
                        ""type"": ""string""
                    },
                    ""sqlAdminUsername"": {
                        ""type"": ""string""
                    }
                },
                ""annotations"": [],
                ""type"": ""AzureSqlDatabase"",
                ""typeProperties"": {
                    ""connectionString"": {
                        ""type"": ""SecureString"",
                        ""value"": ""[parameters('database_connectionString')]""
                    }
                }
            },
            ""dependsOn"": []
        },
</code></pre>

<p>The problem with this ARM templates is that it does not use the parameters to create the connection string, but it uses the connection string parameter database_connectionString (the connection string is by default always parameterized by Azure, so I cannot remove this parameter). </p>

<p>When the release pipeline uses this template, the connectionstring is not filled in (only the parameters are filled in) and hence, the connection to the database fails. How should you setup the connection so that you can automatically (without human interaction) deploy to all environments by only changing the parameters? </p>

<p>I do not want to change the ARM templates coming from Azure Data Factory, because this requires human interaction</p>
","<azure><azure-resource-manager><azure-data-factory>","2019-09-02 08:39:16","4692","7","3","59499594","<p>I do this by using the <code>Azure resource group deployment</code> task in the Release. One of the options is <code>Override template parameters</code> which will allow you to do exactly what you need. You can create a space separated list of parameters to override your ARM Template and pass in a Variable</p>

<p>In your case it would be</p>

<pre><code>-database_connectionstring $(VariableHere)
</code></pre>

<p>I would store the connection string in Azure KeyVault and link it to a Secure Variable Group. You can also just hit the padlock on a standard variable to secure it.</p>

<p>Then bind your custom variable to each Stage in your Release</p>

<p><strong>Task</strong>
<a href=""https://i.stack.imgur.com/K6v80.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K6v80.png"" alt=""enter image description here""></a></p>

<p><strong>Override</strong>
<a href=""https://i.stack.imgur.com/Yuq7e.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Yuq7e.png"" alt=""enter image description here""></a></p>
"
"57753824","Parameterize connections in Azure data factory (ARM templates)","<p>I am trying to setup an Azure Data Factory in a CI/CD setup. I followed Microsoft's best practices (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a>). </p>

<p>I have 4 environments (dev, test, UAT, PRD)</p>

<p>In short what I have done:</p>

<ul>
<li><p>Create an Azure data factory and link it to my Azure DevOps repo on DEV environment</p></li>
<li><p>Create an Azure data factory on the other environments (test, UAT and PRD), but do NOT link it to DevOps. Instead, pipelines are released on these data factories using ARM templates and release pipelines in Azure DevOps.</p></li>
<li><p>I have parameterized all necessary parts to be able to overwrite the settings in each of my environments.</p></li>
</ul>

<p>At this moment, I am able to succesfully deploy to my other environments, however, the linkedservice to my database on azure is not working. I have parameterized everything, like Microsoft suggests, but when I export my linkedservice to an ARM template, it uses a connection string instead of my parameterized settings.</p>

<p>Below is a picture of my settings in the Azure Data Factory portal:
<img src=""https://i.ibb.co/Vvvy5wp/daf-setup.png"" alt=""daf-setup""></p>

<p>When I try to export this to ARM templates, I get the following:</p>

<pre><code>{
            ""name"": ""[concat(parameters('factoryName'), '/database')]"",
            ""type"": ""Microsoft.DataFactory/factories/linkedServices"",
            ""apiVersion"": ""2018-06-01"",
            ""properties"": {
                ""parameters"": {
                    ""sqlServerUrl"": {
                        ""type"": ""string""
                    },
                    ""databaseName"": {
                        ""type"": ""string""
                    },
                    ""sqlPwd"": {
                        ""type"": ""string""
                    },
                    ""sqlAdminUsername"": {
                        ""type"": ""string""
                    }
                },
                ""annotations"": [],
                ""type"": ""AzureSqlDatabase"",
                ""typeProperties"": {
                    ""connectionString"": {
                        ""type"": ""SecureString"",
                        ""value"": ""[parameters('database_connectionString')]""
                    }
                }
            },
            ""dependsOn"": []
        },
</code></pre>

<p>The problem with this ARM templates is that it does not use the parameters to create the connection string, but it uses the connection string parameter database_connectionString (the connection string is by default always parameterized by Azure, so I cannot remove this parameter). </p>

<p>When the release pipeline uses this template, the connectionstring is not filled in (only the parameters are filled in) and hence, the connection to the database fails. How should you setup the connection so that you can automatically (without human interaction) deploy to all environments by only changing the parameters? </p>

<p>I do not want to change the ARM templates coming from Azure Data Factory, because this requires human interaction</p>
","<azure><azure-resource-manager><azure-data-factory>","2019-09-02 08:39:16","4692","7","3","59503805","<p>One of the other answers provided is a valid way using override parameters. This answer will provide a different answer as well as some more context on how to define the SQL connections and how to utilize and implement some of the updates made with Key Vault and Data Factory Integration.</p>

<p>If using an on prem connection to SQL the conenction string will look like:</p>

<pre><code>""Server={serverName};Database={databaseName};User ID={domain}\{userName};Password={password};Integrated Security=True;"" 
</code></pre>

<p>The quotes are required and can be passed in as an override parameter.</p>

<p>If using an Azure Database or even using Key Vault look to add <a href=""https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview"" rel=""nofollow noreferrer"">Managed Identity</a> which <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-factory-service-identity"" rel=""nofollow noreferrer"">Data Factory Supports</a> by including this in your ARM template</p>

<pre><code> ""identity"": {
        ""type"": ""SystemAssigned""
    }
</code></pre>

<p>Once this is added then the Azure SQL Database will need to have the <a href=""https://learn.microsoft.com/en-us/azure/app-service/app-service-web-tutorial-connect-msi"" rel=""nofollow noreferrer"">managed identity added</a>. This can be done via a reusable SQL Script like:</p>

<pre><code>    DECLARE @rolename AS NVARCHAR(100) = 'DataFactory_User'
    DECLARE @username AS NVARCHAR(100) -- This will be the DataFactory name
    SET @username = 'DataFacotryName'

if exists(select * from sys.database_principals where name = @username and Type = 'X' or Type='E')
    BEGIN
        DECLARE @dropUserSql varchar(1000)
        SET @dropUserSql='DROP USER [' + @username + ']'
        PRINT 'Executing ' + @dropUserSql
        EXEC (@dropUserSql)
        PRINT 'Completed ' + @dropUserSql
    END

DECLARE @createUserSql varchar(1000)
SET @createUserSql='CREATE USER [' + @username + '] FROM EXTERNAL PROVIDER'
PRINT 'Executing ' + @createUserSql
EXEC (@createUserSql)
PRINT 'Completed ' + @createUserSql
</code></pre>

<p>I recommend dropping and recreating this user. SQL recognizes the thumbprint of the Managed Identity and everytime the DataFactory is dropped and recreated a new thumbprint is created.</p>

<p>In terms of leveraging Key Vault there is a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault"" rel=""nofollow noreferrer"">LinkedService type of Key Vault</a> that relies on the Managed Identity described above to retrieve secrets.</p>

<p>The Key Vault if deployed via ARM will need to have the access policy updated to something similar to this:</p>

<pre><code>""accessPolicies"": [
          {
            ""tenantID"": ""[subscription().tenantId]"",
            ""objectId"": ""[reference(resourceId('Microsoft.DataFactory/factories/', parameters('DataFactoryName')), '2018-02-01', 'Full').identity.principalId]"",
            ""permissions"": {
              ""secrets"": [
                ""get""
              ],
              ""keys"": [
                ""get""
              ],
              ""certificates"": [
                ""import""
              ]
            },
            ""dependsOn"": [
              ""[resourceId('Microsoft.DataFactory/factories/', parameters('DataFactoryName'))]""
            ]
          }
        ]
</code></pre>

<p>This snippet assumes the Key Vault and Data Factory are in the same ARM template.  If they are not the access policy can still be accomplished via ARM by <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-factory-service-identity#retrieve-managed-identity"" rel=""nofollow noreferrer"">obtaining the ObjectId of the Data Factory defined Managed Identity</a> and passing it in as the ObjectId and removing the dependsOn statement.</p>
"
"57753824","Parameterize connections in Azure data factory (ARM templates)","<p>I am trying to setup an Azure Data Factory in a CI/CD setup. I followed Microsoft's best practices (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a>). </p>

<p>I have 4 environments (dev, test, UAT, PRD)</p>

<p>In short what I have done:</p>

<ul>
<li><p>Create an Azure data factory and link it to my Azure DevOps repo on DEV environment</p></li>
<li><p>Create an Azure data factory on the other environments (test, UAT and PRD), but do NOT link it to DevOps. Instead, pipelines are released on these data factories using ARM templates and release pipelines in Azure DevOps.</p></li>
<li><p>I have parameterized all necessary parts to be able to overwrite the settings in each of my environments.</p></li>
</ul>

<p>At this moment, I am able to succesfully deploy to my other environments, however, the linkedservice to my database on azure is not working. I have parameterized everything, like Microsoft suggests, but when I export my linkedservice to an ARM template, it uses a connection string instead of my parameterized settings.</p>

<p>Below is a picture of my settings in the Azure Data Factory portal:
<img src=""https://i.ibb.co/Vvvy5wp/daf-setup.png"" alt=""daf-setup""></p>

<p>When I try to export this to ARM templates, I get the following:</p>

<pre><code>{
            ""name"": ""[concat(parameters('factoryName'), '/database')]"",
            ""type"": ""Microsoft.DataFactory/factories/linkedServices"",
            ""apiVersion"": ""2018-06-01"",
            ""properties"": {
                ""parameters"": {
                    ""sqlServerUrl"": {
                        ""type"": ""string""
                    },
                    ""databaseName"": {
                        ""type"": ""string""
                    },
                    ""sqlPwd"": {
                        ""type"": ""string""
                    },
                    ""sqlAdminUsername"": {
                        ""type"": ""string""
                    }
                },
                ""annotations"": [],
                ""type"": ""AzureSqlDatabase"",
                ""typeProperties"": {
                    ""connectionString"": {
                        ""type"": ""SecureString"",
                        ""value"": ""[parameters('database_connectionString')]""
                    }
                }
            },
            ""dependsOn"": []
        },
</code></pre>

<p>The problem with this ARM templates is that it does not use the parameters to create the connection string, but it uses the connection string parameter database_connectionString (the connection string is by default always parameterized by Azure, so I cannot remove this parameter). </p>

<p>When the release pipeline uses this template, the connectionstring is not filled in (only the parameters are filled in) and hence, the connection to the database fails. How should you setup the connection so that you can automatically (without human interaction) deploy to all environments by only changing the parameters? </p>

<p>I do not want to change the ARM templates coming from Azure Data Factory, because this requires human interaction</p>
","<azure><azure-resource-manager><azure-data-factory>","2019-09-02 08:39:16","4692","7","3","64979450","<p>Just in case you haven't found an answer or solved this:</p>
<p>I was in a very similar situation, I had several Azure SQL databases that were parameterized (database name), and needed to change the SQL server name in my ADF Azure Release Pipeline to swap between environments.</p>
<p>I found that by inspecting the code definition of the Linked Service ({} next to it), you can get a connection string that includes any parameters that you have defined:</p>
<p><img src=""https://i.stack.imgur.com/5AIzC.png"" alt=""Linked Service Code View"" /></p>
<p><img src=""https://i.stack.imgur.com/AzS08.png"" alt=""Connection String Parameter"" /></p>
<p>I copied the value of &quot;connectionString&quot; and modified what I needed to (server address) and left the parameters in place, and added it to the &quot;connectionString&quot; override parameter in my Azure Release Pipeline, and it worked! (include the quotes!)</p>
<p>I hope that this helps you or anyone in future suffering the same frustration/confusion.</p>
"
"57752881","Edit pipeline query in azure data factory V1 manually","<p>How to edit pipeline and data sets query in azure data factory v1 manually</p>

<p>With the recent changes in ui of azure portal.. we are not able to edit pipeline query manually. Is there any alternate way to edit pipeline query manually  ??</p>
","<azure><azure-data-factory>","2019-09-02 07:24:27","808","0","1","57763993","<p>I tried it and pipeline still could be edit now.</p>

<p>I created a copy active in Data Factory v1.</p>

<p>Choose the Monitor&amp;Manage action in Data Factory Overview.</p>

<p><a href=""https://i.stack.imgur.com/JzgJ3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JzgJ3.png"" alt=""enter image description here""></a></p>

<p>Edit the pipeline manually: AUTHOR--->Resource exploer---> right click pipeline/dataset--->edit
<a href=""https://i.stack.imgur.com/E52Aw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E52Aw.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"57749509","How to get custom output from an executed pipeline?","<p>I would like to be able to get custom output from an ""Execute Pipeline Activity"". During the execution of the invoked pipeline, I capture some information in a variable using the ""Set Variable"" activity. I would like to be able to use that value in the master pipeline.</p>

<p>I know that the master pipeline can read the invoked pipeline's name and runId using ""@activity('InvokedPipeline').output,"" but those are the only properties available.</p>

<p>I have the invokable pipeline because it's configurable to be used by multiple other pipelines, assuming we can get the output from it. It currently consists of 8 activities; I would hate to have to duplicate them all across multiple pipelines just because we can't get the output from an invoked pipeline.</p>

<p>Reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-pipeline-activity"" rel=""noreferrer"">Execute Pipeline Activity</a></p>

<pre><code>[
  {
    ""name"": ""MasterPipeline"",
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
    ""properties"": {
        ""description"": ""Uses the results of the invoked pipeline to do some further processing"",
        ""activities"": [
            {
                ""name"": ""ExecuteChildPipeline"",
                ""description"": ""Executes the child pipeline to get some value."",
                ""type"": ""ExecutePipeline"",
                ""dependsOn"": [],
                ""userProperties"": [],
                ""typeProperties"": {
                    ""pipeline"": {
                        ""referenceName"": ""InvokedPipeline"",
                        ""type"": ""PipelineReference""
                    },
                    ""waitOnCompletion"": true
                }
            },
            {
                ""name"": ""UseVariableFromInvokedPipeline"",
                ""description"": ""Uses the variable returned from the invoked pipeline."",
                ""type"": ""Copy"",
                ""dependsOn"": [
                    {
                        ""activity"": ""ExecuteChildPipeline"",
                        ""dependencyConditions"": [
                            ""Succeeded""
                        ]
                    }
                ]
            }
        ],
        ""parameters"": {},
        ""variables"": {}
    }
  },
  {
    ""name"": ""InvokedPipeline"",
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
    ""properties"": {
        ""description"": ""The child pipeline that makes some HTTP calls, gets some metadata, and sets a variable."",
        ""activities"": [
            {
                ""name"": ""SetMyVariable"",
                ""description"": ""Sets a variable after some processing from other activities."",
                ""type"": ""SetVariable"",
                ""dependsOn"": [
                    {
                        ""activity"": ""ProcessingActivity"",
                        ""dependencyConditions"": [
                            ""Succeeded""
                        ]
                    }
                ],
                ""userProperties"": [],
                ""typeProperties"": {
                    ""variableName"": ""MyVariable"",
                    ""value"": {
                        ""value"": ""@activity('ProcessingActivity').output"",
                        ""type"": ""Expression""
                    }
                }
            }
        ],
        ""parameters"": {},
        ""variables"": {
            ""MyVariable"": {
                ""type"": ""String""
            }
        }
    }
  }
]
</code></pre>
","<azure><azure-data-factory>","2019-09-01 21:06:45","9045","14","2","57814749","<p>Hello Heather and thank you for your inquiry.  Custom outputs are not an inbuilt feature at this time.  You can request/upvote for the feature in the <a href=""https://feedback.azure.com/d365community/forum/1219ec2d-6c26-ec11-b6e6-000d3a4f032c"" rel=""nofollow noreferrer"">Azure feedback forum</a>.  For now, I do have two workarounds.</p>
<p>Utilizing the invoked pipeline's <code>runID</code>, we can query the REST API (using Web Activity) for the activity run logs, and from there, the activity outputs.  However, before making the query, it is necessary to authenticate.
<a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/activityruns/querybypipelinerun"" rel=""nofollow noreferrer"">REST call to get the activities of a pipeline</a></p>
<p>For authentication I reccomend using the Web Activity to get an oauth2 token.  The URL would be <code>https://login.microsoftonline.com/tenantid/oauth2/token</code>.  Headers <code>&quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;</code> and body <code>&quot;grant_type=client_credentials&amp;client_id=xxxx&amp;client_secret=xxxx&amp;resource=https://management.azure.com/&quot;</code>. Since this request is to get credentials, the Authentication setting for this request is type 'None'. These credentials correspond to an App you create via Azure Active Directory&gt;App Registrations. Do not forget to assign the app RBAC in Data FActory Access Control (IAM).</p>
<p>Another workaround, has the child pipeline write its output.  It can write to a database table, or it can write to a blob (I passed the Data Factory variable to a Logic App which wrote to blob storage), or to something else of your choice.  Since you are planning to use the child pipeline for many different parent pipelines, I would recommend passing the child pipeline a parameter that it uses to identify the output to the parent.  That could mean a blob name, or writing the parent runID to a SQL table.  This way the parent pipeline knows where to look to get the output.</p>
"
"57749509","How to get custom output from an executed pipeline?","<p>I would like to be able to get custom output from an ""Execute Pipeline Activity"". During the execution of the invoked pipeline, I capture some information in a variable using the ""Set Variable"" activity. I would like to be able to use that value in the master pipeline.</p>

<p>I know that the master pipeline can read the invoked pipeline's name and runId using ""@activity('InvokedPipeline').output,"" but those are the only properties available.</p>

<p>I have the invokable pipeline because it's configurable to be used by multiple other pipelines, assuming we can get the output from it. It currently consists of 8 activities; I would hate to have to duplicate them all across multiple pipelines just because we can't get the output from an invoked pipeline.</p>

<p>Reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-pipeline-activity"" rel=""noreferrer"">Execute Pipeline Activity</a></p>

<pre><code>[
  {
    ""name"": ""MasterPipeline"",
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
    ""properties"": {
        ""description"": ""Uses the results of the invoked pipeline to do some further processing"",
        ""activities"": [
            {
                ""name"": ""ExecuteChildPipeline"",
                ""description"": ""Executes the child pipeline to get some value."",
                ""type"": ""ExecutePipeline"",
                ""dependsOn"": [],
                ""userProperties"": [],
                ""typeProperties"": {
                    ""pipeline"": {
                        ""referenceName"": ""InvokedPipeline"",
                        ""type"": ""PipelineReference""
                    },
                    ""waitOnCompletion"": true
                }
            },
            {
                ""name"": ""UseVariableFromInvokedPipeline"",
                ""description"": ""Uses the variable returned from the invoked pipeline."",
                ""type"": ""Copy"",
                ""dependsOn"": [
                    {
                        ""activity"": ""ExecuteChildPipeline"",
                        ""dependencyConditions"": [
                            ""Succeeded""
                        ]
                    }
                ]
            }
        ],
        ""parameters"": {},
        ""variables"": {}
    }
  },
  {
    ""name"": ""InvokedPipeline"",
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
    ""properties"": {
        ""description"": ""The child pipeline that makes some HTTP calls, gets some metadata, and sets a variable."",
        ""activities"": [
            {
                ""name"": ""SetMyVariable"",
                ""description"": ""Sets a variable after some processing from other activities."",
                ""type"": ""SetVariable"",
                ""dependsOn"": [
                    {
                        ""activity"": ""ProcessingActivity"",
                        ""dependencyConditions"": [
                            ""Succeeded""
                        ]
                    }
                ],
                ""userProperties"": [],
                ""typeProperties"": {
                    ""variableName"": ""MyVariable"",
                    ""value"": {
                        ""value"": ""@activity('ProcessingActivity').output"",
                        ""type"": ""Expression""
                    }
                }
            }
        ],
        ""parameters"": {},
        ""variables"": {
            ""MyVariable"": {
                ""type"": ""String""
            }
        }
    }
  }
]
</code></pre>
","<azure><azure-data-factory>","2019-09-01 21:06:45","9045","14","2","73055338","<p>just had a chat with ADF team, and the response</p>
<p>[10:11 PM] Mark Kromer
Brajesh Jaishwal: any plans on custom output from execute pipeline activity?
Yes, this work is on the engineering work plan</p>
"
"57729234","What is the best way to import Google Analytics data into Azure Blob/Data Lake?","<p>I am trying to import Google Analytics data into Azure Blob or Data Lake storage for analysis or reporting. But I don't see a Google Analytics connector in Azure Data Factory.</p>

<p>I see some third party connectors such as CData, Xplenty, Stitchdata, etc, but they all require payments. I also tried the Google Analytics API but I am not sure how to use it to bring data into Azure, with my limited knowledge. Is there a way to bring in Google Analytics data into Azure for free?</p>
","<azure><google-analytics><azure-blob-storage><azure-data-factory>","2019-08-30 15:05:18","9848","3","6","57752507","<p>Unfortunately, Azure Data Factory don’t support Google Analytics connector.</p>
<p>Reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-overview"" rel=""nofollow noreferrer"">Azure Data Factory supported connectors</a>.</p>
<p>I would suggest you to vote up an idea submitted by another Azure customer.</p>
<p><a href=""https://feedback.azure.com/d365community/idea/4ca9dce8-6d26-ec11-b6e6-000d3a4f032c"" rel=""nofollow noreferrer"">https://feedback.azure.com/d365community/idea/4ca9dce8-6d26-ec11-b6e6-000d3a4f032c</a></p>
<p>All of the feedback you share in these forums will be monitored and reviewed by the Microsoft engineering teams responsible for building Azure.</p>
"
"57729234","What is the best way to import Google Analytics data into Azure Blob/Data Lake?","<p>I am trying to import Google Analytics data into Azure Blob or Data Lake storage for analysis or reporting. But I don't see a Google Analytics connector in Azure Data Factory.</p>

<p>I see some third party connectors such as CData, Xplenty, Stitchdata, etc, but they all require payments. I also tried the Google Analytics API but I am not sure how to use it to bring data into Azure, with my limited knowledge. Is there a way to bring in Google Analytics data into Azure for free?</p>
","<azure><google-analytics><azure-blob-storage><azure-data-factory>","2019-08-30 15:05:18","9848","3","6","57752535","<p>Based on my researching,you may follow 2 ways to transfer data from Google Analytics data into Azure Blob.</p>

<p>1.In this case <a href=""https://stackoverflow.com/questions/43702851/how-could-i-import-google-analytics-data-to-google-cloud-platform,you"">How could I import google analytics data to Google Cloud Platform?</a> could transfer data from Google Analytics into Google BigQuery. ADF supports <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">Google BigQuery connector</a>.</p>

<p><a href=""https://i.stack.imgur.com/0JgKr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0JgKr.png"" alt=""enter image description here""></a></p>

<p>2.ADF supports REST connector. You could refer to this api document:<a href=""https://developers.google.com/analytics/devguides/reporting/core/v3/reference"" rel=""nofollow noreferrer"">https://developers.google.com/analytics/devguides/reporting/core/v3/reference</a></p>
"
"57729234","What is the best way to import Google Analytics data into Azure Blob/Data Lake?","<p>I am trying to import Google Analytics data into Azure Blob or Data Lake storage for analysis or reporting. But I don't see a Google Analytics connector in Azure Data Factory.</p>

<p>I see some third party connectors such as CData, Xplenty, Stitchdata, etc, but they all require payments. I also tried the Google Analytics API but I am not sure how to use it to bring data into Azure, with my limited knowledge. Is there a way to bring in Google Analytics data into Azure for free?</p>
","<azure><google-analytics><azure-blob-storage><azure-data-factory>","2019-08-30 15:05:18","9848","3","6","58880992","<p>I hope I am not too late for this question.</p>

<p>I have been looking into this as well, and narrowed down my options to the following:</p>

<ol>
<li><p>Granular/Hit level data with Google BigQuery: There is a connector available as of November 2019 in Azure data factory. In order to utilize it you must connect Analytics 360 to BigQuery. Analytics 360 costs about 150k / year which I don't think is the most reasonable option.</p></li>
<li><p>Sampled data : You can write a service worker to fetch data (live or otherwise) in azure using the Reporting v4 api or Streaming analytics api. Again, this is the sampled data, and I don't think it is going to bring value significantly.</p></li>
<li><p>Granular/Hit level raw data using event routing: You can modify the google analytics javascript code by injecting custom javascript to route hit level data to your server. This post describes that in more detail. <a href=""http://www.yuzongbao.com/2018/10/18/google-analytics-raw-data-ingest/"" rel=""nofollow noreferrer"">Ingesting raw google analytics data</a>  </p></li>
</ol>

<p>I am going to work on this one and capture everything on azure using azure workers and SQL Server, next week. Let me know if I can be of any help there. </p>
"
"57729234","What is the best way to import Google Analytics data into Azure Blob/Data Lake?","<p>I am trying to import Google Analytics data into Azure Blob or Data Lake storage for analysis or reporting. But I don't see a Google Analytics connector in Azure Data Factory.</p>

<p>I see some third party connectors such as CData, Xplenty, Stitchdata, etc, but they all require payments. I also tried the Google Analytics API but I am not sure how to use it to bring data into Azure, with my limited knowledge. Is there a way to bring in Google Analytics data into Azure for free?</p>
","<azure><google-analytics><azure-blob-storage><azure-data-factory>","2019-08-30 15:05:18","9848","3","6","67375521","<p>Refer Core reporting API allows you to get quite a lot of dimensions and metrics. The data factory has a rest connector that works for pagination also OOB.</p>
<p>Another good option is to use a big query in between and utilize a data factory big query connector.</p>
<p>The third option is to use GTM call back method to push your data layer to API that can be listened by one of the functions app in azure.</p>
"
"57729234","What is the best way to import Google Analytics data into Azure Blob/Data Lake?","<p>I am trying to import Google Analytics data into Azure Blob or Data Lake storage for analysis or reporting. But I don't see a Google Analytics connector in Azure Data Factory.</p>

<p>I see some third party connectors such as CData, Xplenty, Stitchdata, etc, but they all require payments. I also tried the Google Analytics API but I am not sure how to use it to bring data into Azure, with my limited knowledge. Is there a way to bring in Google Analytics data into Azure for free?</p>
","<azure><google-analytics><azure-blob-storage><azure-data-factory>","2019-08-30 15:05:18","9848","3","6","67618096","<p>I have implemented a reasonably low cost solution without the need for G360. This as done using Google Tag Manager which duplicates hits and pushes to Azure Event Hub where you can then consume that however you want. One option is saving files to blob storage for later use (using Event Hub Capture) or with an Azure Stream Analytics job or even perhaps Azure Functions.</p>
<p>This was inspired by this <a href=""https://www.simoahava.com/analytics/automatically-fork-google-analytics-hits-snowplow/"" rel=""nofollow noreferrer"">blog</a> which pushes to snowplow.</p>
"
"57729234","What is the best way to import Google Analytics data into Azure Blob/Data Lake?","<p>I am trying to import Google Analytics data into Azure Blob or Data Lake storage for analysis or reporting. But I don't see a Google Analytics connector in Azure Data Factory.</p>

<p>I see some third party connectors such as CData, Xplenty, Stitchdata, etc, but they all require payments. I also tried the Google Analytics API but I am not sure how to use it to bring data into Azure, with my limited knowledge. Is there a way to bring in Google Analytics data into Azure for free?</p>
","<azure><google-analytics><azure-blob-storage><azure-data-factory>","2019-08-30 15:05:18","9848","3","6","67680081","<p>This question is very broad and plenty of good solutions has been shared. However just for the update, the new version of Google Analytics (GA4) offer a free data integration with BigQuery. From there it should be easy to move it to Azure data factory.</p>
"
"57727759","Azure Key Vault Linked service not working, debug failed, trigger success","<p>I have created a Linked Service using key vault and then used that Linked service in Data Linked Service (Azure SQL database). Both Linked services independently tested successfully. I have used that in a very simple pipeline, while I am debugging the pipeline, it gets failed with an error:</p>

<blockquote>
  <p>'Invalid linked service reference. Name: '. </p>
</blockquote>

<p>This is referring to Key Vault linked service.</p>

<p>When I trigger the pipeline, it works fine. I have published my changes so many time but no success.</p>

<p>So my basic query is - My pipeline is not working on Debug, however it is working fine with Trigger now.</p>
","<pipeline><azure-keyvault><azure-data-factory>","2019-08-30 13:33:15","1172","0","3","57731677","<p>When a pipeline is working by trigger, but not by debug, that suggests either:  there is a difference between the published version and the version in the UI, or, you have parameters that depend upon the trigger.</p>
"
"57727759","Azure Key Vault Linked service not working, debug failed, trigger success","<p>I have created a Linked Service using key vault and then used that Linked service in Data Linked Service (Azure SQL database). Both Linked services independently tested successfully. I have used that in a very simple pipeline, while I am debugging the pipeline, it gets failed with an error:</p>

<blockquote>
  <p>'Invalid linked service reference. Name: '. </p>
</blockquote>

<p>This is referring to Key Vault linked service.</p>

<p>When I trigger the pipeline, it works fine. I have published my changes so many time but no success.</p>

<p>So my basic query is - My pipeline is not working on Debug, however it is working fine with Trigger now.</p>
","<pipeline><azure-keyvault><azure-data-factory>","2019-08-30 13:33:15","1172","0","3","57800988","<p>It's very strange thing I have noticed in Linked Service in ADF. I have selected Azure Key Vault near to password and just passed AKV linked service name there and it worked. </p>

<p>That suggests that JSON is not properly working with Azure key vault services in Linked Services. Well, my issue has been resolved however logically I am still unclear.</p>

<p>If any one looking for resolution of same, please refer below. Thank you.</p>

<p><a href=""https://i.stack.imgur.com/BmNRx.png"" rel=""nofollow noreferrer"">Key Vault Linked Service</a></p>
"
"57727759","Azure Key Vault Linked service not working, debug failed, trigger success","<p>I have created a Linked Service using key vault and then used that Linked service in Data Linked Service (Azure SQL database). Both Linked services independently tested successfully. I have used that in a very simple pipeline, while I am debugging the pipeline, it gets failed with an error:</p>

<blockquote>
  <p>'Invalid linked service reference. Name: '. </p>
</blockquote>

<p>This is referring to Key Vault linked service.</p>

<p>When I trigger the pipeline, it works fine. I have published my changes so many time but no success.</p>

<p>So my basic query is - My pipeline is not working on Debug, however it is working fine with Trigger now.</p>
","<pipeline><azure-keyvault><azure-data-factory>","2019-08-30 13:33:15","1172","0","3","68740362","<p>I had faced exactly the same problem, I performed the following actions:</p>
<ol>
<li>Save all existing pipelines</li>
<li>Validated all</li>
<li>Publish all</li>
<li>Closed the datafactory browser window/tab</li>
<li>logged back into datafactory</li>
<li>Opened the pipeline again and the debug worked fine. I didn't have to touch the Azure Vault configuration. Its most likely to do with cached vault configuration (or a sync issue with the cached vault config)</li>
</ol>
"
"57721242","How to upload a file from azure blob storage to Linux VM created on azure","<p>I have one large file on my azure blob storage container. I want to move my file from blob storage to Linux VM created on azure> How can I do that using data factory? or any Powershell Command?</p>
","<azure><azure-data-factory><azure-blob-storage><azcopy>","2019-08-30 06:01:22","6546","1","4","57721475","<p>You have many options to copy content from the blob store to the disk on the VM:</p>

<pre><code> 1. Use AzCopy 
 2. Use Azure Pipelines - File copy task
 3. Use Powershell cmdlets
</code></pre>

<p>A lot of content is available on these approaches on SO!</p>
"
"57721242","How to upload a file from azure blob storage to Linux VM created on azure","<p>I have one large file on my azure blob storage container. I want to move my file from blob storage to Linux VM created on azure> How can I do that using data factory? or any Powershell Command?</p>
","<azure><azure-data-factory><azure-blob-storage><azcopy>","2019-08-30 06:01:22","6546","1","4","57722523","<p>The easiest and without any tools is to generate SAS token for the blob and run CURL.</p>

<p>Generate SAS</p>

<p><a href=""https://i.stack.imgur.com/c1eTI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c1eTI.png"" alt=""enter image description here""></a></p>

<p>And then CURL</p>

<pre><code>curl &lt;blob_sas_url&gt; -o output.txt
</code></pre>

<p>If you need this automated every time you can generate SAS URL from the script or just use AzCopy.</p>
"
"57721242","How to upload a file from azure blob storage to Linux VM created on azure","<p>I have one large file on my azure blob storage container. I want to move my file from blob storage to Linux VM created on azure> How can I do that using data factory? or any Powershell Command?</p>
","<azure><azure-data-factory><azure-blob-storage><azcopy>","2019-08-30 06:01:22","6546","1","4","57722571","<p>Please reference this blog：<a href=""https://stackoverflow.com/questions/40250220/how-to-copy-data-to-vm-from-blob-storage"">How to copy data to VM from blob storage</a>, it gives you a way to solve the problem with <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#linked-service-properties"" rel=""nofollow noreferrer"">Data Factory</a>:</p>

<p>""To anyone who might get into same problem in future, I solved my problem by using 'copy wizard' present in ADF.</p>

<p>We need to install Data Management Gateway on VM and register it before we use 'copy wizard'.</p>

<p>We need to specify blob storage as source and in destination we need to choose 'File Server Share' option. In 'File Server Share' option we need to specify user credentials which I suppose pipeline uses to login to VM, folder on VM where pipeline will copy the data.""</p>

<p>From the Azure Blog Storage document, there is another way can help you <a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/storage-how-to-mount-container-linux"" rel=""nofollow noreferrer"">Mount Blob storage as a file system with blobfuse on Linux</a>.</p>

<p>Blobfuse is a virtual file system driver for Azure Blob storage. Blobfuse allows you to access your existing block blob data in your storage account through the Linux file system. Blobfuse uses the virtual directory scheme with the forward-slash '/' as a delimiter.</p>

<p>This guide shows you how to use blobfuse, and mount a Blob storage container on Linux and access data. To learn more about blobfuse, read the details in the <a href=""https://github.com/Azure/azure-storage-fuse"" rel=""nofollow noreferrer"">blobfuse repository</a>.</p>

<p>If you want to use AzCopy, you can reference this document <a href=""https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-blobs?toc=%2Fazure%2Fstorage%2Fblobs%2Ftoc.json"" rel=""nofollow noreferrer"">Transfer data with AzCopy and Blob storage</a>. You can download the <a href=""https://aka.ms/downloadazcopy-v10-linux"" rel=""nofollow noreferrer"">AzCopy for Linux</a>. It provided the command for upload and download files.</p>

<p>For example, upload file:</p>

<pre><code>azcopy copy ""&lt;local-file-path&gt;"" ""https://&lt;storage-account-name&gt;.&lt;blob or dfs&gt;.core.windows.net/&lt;container-name&gt;/&lt;blob-name&gt;""
</code></pre>

<p>For PowerShell, you need to use PowerShell Core 6.x and later on all platforms. It works with Windows and Linux virtual machines using Windows PowerShell 5.1 (Windows only) or PowerShell 6 (Windows and Linux).</p>

<p>You can find the PowerShell commands in this document:<a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-powershell#upload-blobs-to-the-container"" rel=""nofollow noreferrer"">Quickstart: Upload, download, and list blobs by using Azure PowerShell</a></p>

<p>Here is another link talked about <a href=""https://www.thomasmaurer.ch/2019/07/copy-files-to-azure-vm-using-powershell-remoting/"" rel=""nofollow noreferrer"">Copy Files to Azure VM using PowerShell Remoting 6 (Windows and Linux)</a>.</p>

<p>Hope this helps.</p>
"
"57721242","How to upload a file from azure blob storage to Linux VM created on azure","<p>I have one large file on my azure blob storage container. I want to move my file from blob storage to Linux VM created on azure> How can I do that using data factory? or any Powershell Command?</p>
","<azure><azure-data-factory><azure-blob-storage><azcopy>","2019-08-30 06:01:22","6546","1","4","58303364","<p>It seems this is not properly documented anywhere so I am sharing the most basic approach which is to use the azcopy tool that is available for both windows/linux OS. This approach doens't need the complexity of creating the credentials/tokens.</p>

<ol>
<li><p>Download <a href=""https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10"" rel=""nofollow noreferrer"">azcopy</a>
Its simple executable which can be run directly after extraction</p></li>
<li><p>Create a managed identity(system-assigned identity) for your Virtual machine. Navigate to VM-> Identity -> Turn the Status to 'ON' -> Save</p></li>
<li><p>Now the VM can be assigned permission at the following levels: </p>

<ul>
<li>Storage account</li>
<li>Container (file system)</li>
<li>Resource group</li>
<li>Subscription</li>
</ul></li>
</ol>

<p>For this case, navigate to storage account -> IAM -> Add role assignment -> Select role 'Storage Blob Data Contributor' -> Assign access to  'Virtual machine' -> Select the desired VM -> SAVE</p>

<p>NOTE: If you give access to the VM on IAM properties of a Resource Group, the VM will be able to access all the storage accounts of the RG.</p>

<ol start=""4"">
<li><p>Login to VM and assume the identity (run the command from the same location where the azcopy is located)
For windows : azcopy login --identity 
For linux   : ./azcopy login --identity           </p></li>
<li><p>Upload or download the files now:
azcopy cp ""source-file"" ""storageUri/blob-container/"" --recursive=true
Example: azcopy cp ""C:\test.txt"" ""<a href=""https://mystorageaccount.blob.core.windows.net/backup/"" rel=""nofollow noreferrer"">https://mystorageaccount.blob.core.windows.net/backup/</a>"" --recursive=true</p></li>
</ol>

<p>IAM permission can take few minutes to propagate. If you change/add the permissions/access level anywhere, run the azcopy login --identity command again to get the updated identity.</p>

<p>More info on Azcopy is available <a href=""https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10#run-azcopy"" rel=""nofollow noreferrer"">here</a></p>
"
"57717575","Getting Out of memory error in ADF when copying from On-premise to Blob in parquet file format","<p>I have around 10M records in an On-premise SQL that I am trying to transfer using my Integration Runtime to my Azure Blob Storage in parquet file format. For 2M records it is transferring with no issue, but when I try to transfer above 2M records I'm getting the following error. I did some research, and I read on a post that it might help changing the environment variable value for java </p>

<pre><code>**_JAVA_OPTIONS with value -Xms256m -Xmx16g.** 
</code></pre>

<p>or </p>

<pre><code>**_JAVA_OPTIONS with value -Xms256m -Xmx32g.** 
</code></pre>

<p>I tried both and I'm still getting the same error </p>

<pre><code>""errorCode"": ""2200"",
""message"": ""Failure happened on 'Sink' side. ErrorCode=UserErrorJavaInvocationException,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=An error occurred when invoking java, message: java.lang.OutOfMemoryError:Java heap space\ntotal entry:11\r\njava.util.ArrayDeque.doubleCapacity(Unknown Source)\r\njava.util.ArrayDeque.addFirst(Unknown Source)\r\njava.util.ArrayDeque.push(Unknown Source)\r\norg.apache.parquet.io.ValidatingRecordConsumer.endField(ValidatingRecordConsumer.java:108)\r\norg.apache.parquet.example.data.GroupWriter.writeGroup(GroupWriter.java:58)\r\norg.apache.parquet.example.data.GroupWriter.write(GroupWriter.java:37)\r\norg.apache.parquet.hadoop.example.GroupWriteSupport.write(GroupWriteSupport.java:87)\r\norg.apache.parquet.hadoop.example.GroupWriteSupport.write(GroupWriteSupport.java:37)\r\norg.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:123)\r\norg.apache.parquet.hadoop.ParquetWriter.write(ParquetWriter.java:292)\r\ncom.microsoft.datatransfer.bridge.parquet.ParquetBatchWriter.addRows(ParquetBatchWriter.java:60)\r\n,Source=Microsoft.DataTransfer.Common,''Type=Microsoft.DataTransfer.Richfile.JniExt.JavaBridgeException,Message=,Source=Microsoft.DataTransfer.Richfile.HiveOrcBridge,'"",
""failureType"": ""UserError"",
""target"": ""Copy Data1""
</code></pre>

<p>}</p>
","<azure><jvm><azure-blob-storage><parquet><azure-data-factory>","2019-08-29 20:49:19","1807","1","2","57717727","<p>How about doing a ""staging"" upload first, as a compressed csv, then taking that compressed csv and move it to destiny but taking blob storage as source. This way you will discard the issue being the self hosted IR. This workaround has worked for me up to 5M rows, so you might as well give it a try.</p>

<p>Hope this helped!</p>
"
"57717575","Getting Out of memory error in ADF when copying from On-premise to Blob in parquet file format","<p>I have around 10M records in an On-premise SQL that I am trying to transfer using my Integration Runtime to my Azure Blob Storage in parquet file format. For 2M records it is transferring with no issue, but when I try to transfer above 2M records I'm getting the following error. I did some research, and I read on a post that it might help changing the environment variable value for java </p>

<pre><code>**_JAVA_OPTIONS with value -Xms256m -Xmx16g.** 
</code></pre>

<p>or </p>

<pre><code>**_JAVA_OPTIONS with value -Xms256m -Xmx32g.** 
</code></pre>

<p>I tried both and I'm still getting the same error </p>

<pre><code>""errorCode"": ""2200"",
""message"": ""Failure happened on 'Sink' side. ErrorCode=UserErrorJavaInvocationException,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=An error occurred when invoking java, message: java.lang.OutOfMemoryError:Java heap space\ntotal entry:11\r\njava.util.ArrayDeque.doubleCapacity(Unknown Source)\r\njava.util.ArrayDeque.addFirst(Unknown Source)\r\njava.util.ArrayDeque.push(Unknown Source)\r\norg.apache.parquet.io.ValidatingRecordConsumer.endField(ValidatingRecordConsumer.java:108)\r\norg.apache.parquet.example.data.GroupWriter.writeGroup(GroupWriter.java:58)\r\norg.apache.parquet.example.data.GroupWriter.write(GroupWriter.java:37)\r\norg.apache.parquet.hadoop.example.GroupWriteSupport.write(GroupWriteSupport.java:87)\r\norg.apache.parquet.hadoop.example.GroupWriteSupport.write(GroupWriteSupport.java:37)\r\norg.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:123)\r\norg.apache.parquet.hadoop.ParquetWriter.write(ParquetWriter.java:292)\r\ncom.microsoft.datatransfer.bridge.parquet.ParquetBatchWriter.addRows(ParquetBatchWriter.java:60)\r\n,Source=Microsoft.DataTransfer.Common,''Type=Microsoft.DataTransfer.Richfile.JniExt.JavaBridgeException,Message=,Source=Microsoft.DataTransfer.Richfile.HiveOrcBridge,'"",
""failureType"": ""UserError"",
""target"": ""Copy Data1""
</code></pre>

<p>}</p>
","<azure><jvm><azure-blob-storage><parquet><azure-data-factory>","2019-08-29 20:49:19","1807","1","2","58243038","<p>I was getting similar error slightly different error message  ""Unable to retrieve Java exception"" (no heap size ), but mine was resolved by changing the compression type to <strong>'none'</strong>. by default the snappy compression on the parquet sink file was causing the issue</p>
"
"57714497","In ADF V2 - how to append date (""yyyyMMdd"")) to filename dynamically for S3 dataset","<p>I'm currently working to automate a pipeline in ADFv2 where the source data sits in S3. A new file is created daily and is structured ""data_20180829.csv""</p>

<p>I have tried to instrument dynamic content to accomplish this in the fileName field of Copy Data Activity. However even when I try something as simple as @{concat('data_','20180829.csv')} (that should resolve to the correct value) the source fails.</p>

<p>Is there any way to see what the dynamic content will resolve to?</p>
","<azure-pipelines><azure-data-factory>","2019-08-29 16:38:09","7435","4","2","57745690","<p>This should just be a matter of setting the filename expression in the dataset, eg</p>

<p><a href=""https://i.stack.imgur.com/wnrRZ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/wnrRZ.png"" alt=""Azure Data Factory dataset settings""></a></p>

<p>Note, the setting is done on the dataset not at the Copy activity level.  Also note you can make your expression more dynamic by combining the <code>utcnow</code> function with formatDateTime, eg something like this:</p>

<pre><code>@concat('data_',formatDateTime(utcnow(),'yyyMMdd_HHmmss'),'.csv')
</code></pre>

<p>Note carefully the case of the formatting strings.  Capital <code>MM</code> is for month in a two-digit format.  <code>HH</code> is for the hour in 24-hour format.  The full list of formatting strings is <a href=""https://learn.microsoft.com/en-us/dotnet/standard/base-types/custom-date-and-time-format-strings"" rel=""noreferrer"">here</a>.</p>

<p>The json for the dataset looks like this:</p>

<pre><code>{
    ""name"": ""DelimitedText3"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""linkedService2"",
            ""type"": ""LinkedServiceReference""
        },
        ""annotations"": [],
        ""type"": ""DelimitedText"",
        ""typeProperties"": {
            ""location"": {
                ""type"": ""AzureBlobStorageLocation"",
                ""fileName"": {
                    ""value"": ""@concat('data_',formatDateTime(utcnow(),'yyyMMdd_HHmmss'),'.csv')"",
                    ""type"": ""Expression""
                },
                ""container"": ""ang""
            },
            ""columnDelimiter"": "","",
            ""escapeChar"": ""\\"",
            ""quoteChar"": ""\""""
        },
        ""schema"": [
            {
                ""type"": ""String""
            },
            {
                ""type"": ""String""
            },
            {
                ""type"": ""String""
            },
            {
                ""type"": ""String""
            }
        ]
    }
}
</code></pre>
"
"57714497","In ADF V2 - how to append date (""yyyyMMdd"")) to filename dynamically for S3 dataset","<p>I'm currently working to automate a pipeline in ADFv2 where the source data sits in S3. A new file is created daily and is structured ""data_20180829.csv""</p>

<p>I have tried to instrument dynamic content to accomplish this in the fileName field of Copy Data Activity. However even when I try something as simple as @{concat('data_','20180829.csv')} (that should resolve to the correct value) the source fails.</p>

<p>Is there any way to see what the dynamic content will resolve to?</p>
","<azure-pipelines><azure-data-factory>","2019-08-29 16:38:09","7435","4","2","57779268","<p>Just to extend what wBob called out . If you want the structure like foo_YYYY\MM\DD\ , you can always use this </p>

<pre><code>@concat('foo-name','/',formatDateTime(utcnow(),'yyyy'),'/',formatDateTime(utcnow(),'MM'),'/',formatDateTime(utcnow(),'dd'))
</code></pre>
"
"57714344","How to run a pipeline for a specific windowStartTime","<p>Is there an easy way in the portal to re-run a pipeline for a defined window?</p>

<p>Data Factory V1 had the calendar that allowed a slice to be selected and re-ran. What is the equvilent in V2?</p>

<p>I can see the run history page shows all the runs for a window trigger, however you have to open each of the properties pages to find out the parameter value for each run. If I needed to find a specific day to re-run this would be really cumbersome having to open every iten in the to check the value, before re-running the row once I've found it.</p>
","<azure><azure-data-factory>","2019-08-29 16:27:59","65","0","1","57719526","<p>Azure Data Factory support you <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-visually#rerun-activities-inside-a-pipeline"" rel=""nofollow noreferrer"">Rerun activities inside a pipeline</a>.</p>

<p>You can get all the pipeline run history in Data Factory monitor.</p>

<p>You can now rerun activities inside a pipeline. Select View activity runs, and then select the activity in your pipeline from which point you want to rerun your pipeline.</p>

<p><a href=""https://i.stack.imgur.com/YIwxc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YIwxc.png"" alt=""enter image description here""></a></p>

<p>Using filter or Custom Range to find out  which pipeline you want to rerun.</p>

<p>Here's an another document you can reference <a href=""https://erwindekreuk.com/2019/03/rerun-pipeline-activities-in-azure-data-factory/"" rel=""nofollow noreferrer"">Rerun Pipeline activities in Azure Data Factory</a>.</p>

<p>Hope this helps.</p>
"
"57710493","How can I decompress my .zip file and store in ADL/Blob storage?","<p>I have a ftp as a source connection where some I have zip file and others are not in compress form. I want to copy the files from ftp,decompress zip files and put all files into azure data lake or azure blob storage wherever it's possible to get decompressed.</p>

<p>I'm using copy data activity where I have a source as ftp and properties is zipDeflate,fastest and binary copy and the sink side, I'm just defining the destination ADL path. The files are getting copied to ADL but they're copying in compress form only.
Please let me know if it's possible to achieve the above objective by using copy activity process?</p>
","<azure><zip><azure-data-lake><azure-data-factory>","2019-08-29 12:42:57","220","0","1","57712001","<p>Using binary copy is your issue here, data factory wont understand the data it is moving to uncompress it. Try the same setup without binary copy!</p>

<p>Hope this helped!</p>
"
"57703299","Passing the user name from ADF to notebooks","<p>I want to pass the logged in user name from Azure Data Factory to azure notebook.</p>

<p>I tried dbutils functionality but no luck.</p>

<p>x=str(dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user'))
print (x)</p>

<p>Tried above code in notebook. When I am running the notebook directly it is giving the expected result. But from ADF it is not working</p>
","<azure><azure-active-directory><azure-data-factory>","2019-08-29 05:06:05","429","0","1","57708524","<p>In short, you <strong>can't</strong>.</p>

<p>In DataFactory when you create linked service to Databricks you specify personal access token to ADF. This means all requests made from ADF are executed as that user, but there is currently no user logged in the databricks platform and as such no context is initialized.</p>

<p><a href=""https://i.stack.imgur.com/JXCMp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JXCMp.png"" alt=""enter image description here""></a></p>

<p>In ADF there isn't even user variable because pipelines are designed (expected) to be executed via schedule or event triggers or external systems. Not by users manually.</p>

<p>Here is the list of available system variables
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables</a></p>
"
"57700044","Error copying on-Premise SQL Data as Parquet","<p>At the moment unable to copy data from an on-premise SQL Server through Integration Runtime in parquet format to Azure Blob Storage using ADF V2 Copy Activity. Latest JRE installed on IR machine. Getting this error:</p>

<pre><code>{ 
""errorCode"": ""2200"", 
""message"": ""Failure happened on 'Sink' side. ErrorCode=UserErrorJavaInvocationException,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=An error occurred when invoking java, message: java.lang.UnsatisfiedLinkError:no snappyjava in java.library.path\ntotal entry:18\r\njava.lang.ClassLoader.loadLibrary(Unknown Source)\r\njava.lang.Runtime.loadLibrary0(Unknown Source)\r\njava.lang.System.loadLibrary(Unknown Source)\r\norg.xerial.snappy.SnappyLoader.loadNativeLibrary(SnappyLoader.java:170)\r\norg.xerial.snappy.SnappyLoader.load(SnappyLoader.java:145)\r\norg.xerial.snappy.Snappy.&lt;clinit&gt;(Snappy.java:47)\r\norg.apache.parquet.hadoop.codec.SnappyCompressor.compress(SnappyCompressor.java:67)\r\norg.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:81)\r\norg.apache.hadoop.io.compress.CompressorStream.finish(CompressorStream.java:92)\r\norg.apache.parquet.hadoop.CodecFactory$BytesCompressor.compress(CodecFactory.java:112)\r\norg.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:89)\r\norg.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:152)\r\norg.apache.parquet.column.impl.ColumnWriterV1.flush(ColumnWriterV1.java:240)\r\norg.apache.parquet.column.impl.ColumnWriteStoreV1.flush(ColumnWriteStoreV1.java:126)\r\norg.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:164)\r\norg.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:113)\r\norg.apache.parquet.hadoop.ParquetWriter.close(ParquetWriter.java:297)\r\ncom.microsoft.datatransfer.bridge.parquet.ParquetWriterBridge.close(ParquetWriterBridge.java:29)\r\n,Source=Microsoft.DataTransfer.Common,''Type=Microsoft.DataTransfer.Richfile.JniExt.JavaBridgeException,Message=,Source=Microsoft.DataTransfer.Richfile.HiveOrcBridge,'"", 
""failureType"": ""UserError"", 
""target"": ""CopyMetDBTableToBlob"" 
}
</code></pre>

<p>Have tested copying data from on-premise Oracle and Informix to Azure Blob Storage in Parquet format using ASDF V2 Copy activity and it works. Just having issue with in-premise SQL Server</p>
","<azure-data-factory>","2019-08-28 21:07:43","1294","0","2","57700162","<p>I dont know if you have already checked this out, but there is a section about using parquet file format with an on premise IR. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-parquet#using-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/format-parquet#using-self-hosted-integration-runtime</a></p>

<p>I wouldn't recommend using parquet with data factory, as it doesn't split the dataset into different values of a column (like Python does, for example). I've also had problems when uploading a big dataset (30 gb or more) in this format, it always seemed kind of buggy to me.</p>

<p>I'd always go for compressed csv unless I have no choice.</p>

<p>Hope this helped!</p>
"
"57700044","Error copying on-Premise SQL Data as Parquet","<p>At the moment unable to copy data from an on-premise SQL Server through Integration Runtime in parquet format to Azure Blob Storage using ADF V2 Copy Activity. Latest JRE installed on IR machine. Getting this error:</p>

<pre><code>{ 
""errorCode"": ""2200"", 
""message"": ""Failure happened on 'Sink' side. ErrorCode=UserErrorJavaInvocationException,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=An error occurred when invoking java, message: java.lang.UnsatisfiedLinkError:no snappyjava in java.library.path\ntotal entry:18\r\njava.lang.ClassLoader.loadLibrary(Unknown Source)\r\njava.lang.Runtime.loadLibrary0(Unknown Source)\r\njava.lang.System.loadLibrary(Unknown Source)\r\norg.xerial.snappy.SnappyLoader.loadNativeLibrary(SnappyLoader.java:170)\r\norg.xerial.snappy.SnappyLoader.load(SnappyLoader.java:145)\r\norg.xerial.snappy.Snappy.&lt;clinit&gt;(Snappy.java:47)\r\norg.apache.parquet.hadoop.codec.SnappyCompressor.compress(SnappyCompressor.java:67)\r\norg.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:81)\r\norg.apache.hadoop.io.compress.CompressorStream.finish(CompressorStream.java:92)\r\norg.apache.parquet.hadoop.CodecFactory$BytesCompressor.compress(CodecFactory.java:112)\r\norg.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:89)\r\norg.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:152)\r\norg.apache.parquet.column.impl.ColumnWriterV1.flush(ColumnWriterV1.java:240)\r\norg.apache.parquet.column.impl.ColumnWriteStoreV1.flush(ColumnWriteStoreV1.java:126)\r\norg.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:164)\r\norg.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:113)\r\norg.apache.parquet.hadoop.ParquetWriter.close(ParquetWriter.java:297)\r\ncom.microsoft.datatransfer.bridge.parquet.ParquetWriterBridge.close(ParquetWriterBridge.java:29)\r\n,Source=Microsoft.DataTransfer.Common,''Type=Microsoft.DataTransfer.Richfile.JniExt.JavaBridgeException,Message=,Source=Microsoft.DataTransfer.Richfile.HiveOrcBridge,'"", 
""failureType"": ""UserError"", 
""target"": ""CopyMetDBTableToBlob"" 
}
</code></pre>

<p>Have tested copying data from on-premise Oracle and Informix to Azure Blob Storage in Parquet format using ASDF V2 Copy activity and it works. Just having issue with in-premise SQL Server</p>
","<azure-data-factory>","2019-08-28 21:07:43","1294","0","2","60638580","<p>I ran into similar issue. Turns out, copy to parquet fails if source table has any column names with spaces. </p>

<p>So before the copy operation, i check to see if any of column names have spaces. If yes, instead of specifying table name in dataset, use a query and write select statement with aliases for column names without any spaces.</p>
"
"57694846","GET from API in Azure Data Factory without duplicating URLs","<p>I am quite new to Azure and I am struggling with the following workflow:</p>

<ul>
<li>Do a GET request to <code>/some/base/url/endpoint_credentials</code> obtaining ephemeral credentials in JSON format. <code>{ID: PASS}</code></li>
<li>Do a GET request to <code>/some/base/url/endpoint_data</code> (there are many data endpoints actually) using the previous credential as part of the header and get some JSON data <code>{filed1: value1, field2: value2, etc..}</code></li>
<li>Write te data in the file system.</li>
</ul>

<p>My approach is </p>

<ul>
<li>create a <code>HTTP</code> linked service pointing to <code>/some/base/url</code>. </li>
<li>create two DataSets pointing to each endpoint. </li>
<li>use the two <code>web</code> activities to retrive the desired data (one for credentials, one for data)</li>
<li>write it in the filesystem.</li>
</ul>

<p>The problem I've got is that <code>Web</code> activity force you to write the URL instead of using those provided by the linked service + dataset, which seems to be not optimal since you are force to write the whole URL in each pipeline instead of make use of a reusable components as linked services or datasets. </p>

<p>I've worked with <code>SSIS</code> in the past and this kind of operations were pretty easy to develop. I guess my approach is not correct. </p>
","<azure><azure-data-factory>","2019-08-28 14:34:38","176","0","1","57698790","<p>Your approach is correct. 
Make use of <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-set-variable-activity"" rel=""nofollow noreferrer"">Set Variable Activity</a>. Retrieve the credentials from the first web activity and store the credentials using the set variable activity. Make use of those variables in setting up the URL as <a href=""https://www.blue-granite.com/blog/using-azure-data-factory-v2-activities-dynamic-content-to-direct-your-files"" rel=""nofollow noreferrer"">dynamic content</a> of web activity. </p>

<p>Below is the way to set up the URL in web activity with dynamic content. Make sure you are setting up those variable in the first web activity. In case, you need to make use of multiple pipelines, pass those variables to the calling pipeline parameters and make use of those parameters in the URL.</p>

<p><a href=""https://i.stack.imgur.com/ax9h3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ax9h3.png"" alt=""Setting Up URL at run time using variables set in other activity""></a></p>

<p>Basically dynamic content let's you decide the values at runtime.</p>
"
"57685760","What is the valid value for destinationTable for the Data Factory Diagnostic settings in the ARM JSON Tempate?","<p>I want to use the recommended Diagnostic Settings for Azure Data Factory using ""Resource Specific"" destination table. I'm using ARM Templates to deploy the change, but none of the values I put in seems to work:</p>

<p>What is the correct value to use ""resource specific"" destination table?</p>

<p>Thanks!</p>

<p>I have tried these different values in the ARM Template: resourceSpecific, ResourceSpecific, Resource-Specific</p>

<pre><code> ""properties"": {
            ""name"": ""[variables('LogAnalyticsSettingName')]"",
            ""storageAccountId"": null,
            ""eventHubAuthorizationRuleId"": null,
            ""eventHubName"": null,
            ""workspaceId"": ""[resourceId('microsoft.operationalinsights/workspaces',parameters('OMSWorkspaceName'))]"",
            ""destinationTable"": ""resourceSpecific"",
            ""logs"": [
              {
                ""category"": ""PipelineRuns"",
                ""enabled"": true,
                ""retentionPolicy"": {
                  ""enabled"": false,
                  ""days"": 0
                }
              }
</code></pre>

<p>After deploying the ARM Template...
Expected Result: Destination table is Resource Specific in the ADF Diagnostics Settings</p>

<p>Actual Result: Destination table remains to be Azure diagnostics</p>
","<azure-data-factory>","2019-08-28 05:45:22","212","0","1","57702011","<p>I was able to find the solution by reviewing the Activity Log in the Data Factory when I change the diagnostic settings manually from the portal.</p>

<p>In the ""Create or update resource diagnostic setting"" JSON request body, I saw a property called ""logAnalyticsDestinationType"" with a value of ""Dedicated"".</p>

<p>I removed the destinationTable property in the ARM JSON template and replaced it with ""logAnalyticsDestinationType"":""Dedicated"" instead and redeployed the ARM template. It worked as expected.</p>
"
"57677288","On dataflow inside data factory pricing","<p>We are analyzing the option to use dataflow inside Data Factory but we are having some questions that cannot find the answer.</p>

<p>How much does it costs?</p>

<p>We are on a tight budget and not sure if this tool is the right one for our needs.</p>
","<azure><azure-data-factory><dataflow>","2019-08-27 14:49:06","1186","1","2","57677445","<p>I always use the Azure Pricing Calculator, its a really practical tool to check how much each product costs. You can configure everything from any resource you can create and check the monthly cost. Simply type Data Factory in the search bar, click and scroll down to configure the data flow options.</p>

<p><a href=""https://azure.microsoft.com/en-in/pricing/calculator/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-in/pricing/calculator/</a></p>

<p>Note that you pay per use, so you will never be using 730 hours a month!! It may look expensive when you see it that way. 10 hours per month is what we use internally to calculate when we are not sure how much it will take.</p>

<p>If its the right tool for your needs or not, will depend on your specific needs. Data factory on its own just moves data around, whereas with dataflow it can modify it on the fly.</p>
"
"57677288","On dataflow inside data factory pricing","<p>We are analyzing the option to use dataflow inside Data Factory but we are having some questions that cannot find the answer.</p>

<p>How much does it costs?</p>

<p>We are on a tight budget and not sure if this tool is the right one for our needs.</p>
","<azure><azure-data-factory><dataflow>","2019-08-27 14:49:06","1186","1","2","57677594","<p>Data Factory data flows are visually-designed components inside of Data Factory that enable data transformations at scale. You pay for the data flow cluster execution and debugging time per vCore-hour. The minimum cluster size to run a data flow is 8 vCores. Execution and debugging charges are prorated by the minute and rounded up. While in preview, data flow is offered with a preview discount.</p>

<ul>
<li><p>Type: <strong>Compute Optimized</strong> </p>

<ul>
<li>Price: $0.072 per vCore-hour </li>
<li>Description: Data flow built on Compute Optimized compute</li>
</ul></li>
<li><p>Type: <strong>General Purpose</strong> </p>

<ul>
<li>Price: $0.104 per vCore-hour </li>
<li>Description: Data flow built on General Purpose compute</li>
</ul></li>
<li><p>Type: <strong>Memory Optimized</strong> </p>

<ul>
<li>Price: $0.138 per vCore-hour </li>
<li>Description: Data flow built on Memory Optimized compute</li>
</ul></li>
</ul>

<p>More available at <a href=""https://azure.microsoft.com/en-us/pricing/details/data-factory/data-pipeline/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/pricing/details/data-factory/data-pipeline/</a></p>

<p>Note that this is in <strong>PREVIEW</strong> so final price (when released to GA) might change.</p>
"
"57675182","How to convert txt to csv files in Azure Data Factory","<p>I'm trying to move files in between multiple gen1 Azure data lake storage instances without having to resort to writing an Azure App Function or directly calling the Azure storage SDK.</p>

<p>The situation is that I've got a few text files in data lake 1 called <code>test1.txt</code> and <code>test2.txt</code>. They're both files with tabs as delimiters. When I try to use the copy activity with <code>.csv</code> as extension, the files do get moved over to data lake 2, but stay <code>.txt</code> files.</p>

<p>The copy activity source and sink looks as follows:</p>

<pre><code>{
    ""typeProperties"": {
        ""source"": {
            ""type"": ""DelimitedTextSource"",
            ""storeSettings"": {
                ""type"": ""AzureDataLakeStoreReadSettings"",
                ""recursive"": true,
                ""wildcardFileName"": ""*.*"",
                ""enablePartitionDiscovery"": false
            },
            ""formatSettings"": {
                ""type"": ""DelimitedTextReadSettings""
            }
        },
        ""sink"": {
            ""type"": ""DelimitedTextSink"",
            ""storeSettings"": {
                ""type"": ""AzureDataLakeStoreWriteSettings""
            },
            ""formatSettings"": {
                ""type"": ""DelimitedTextWriteSettings"",
                ""quoteAllText"": true,
                ""fileExtension"": "".csv""
            }
        }
    }
}
</code></pre>

<p>I've tried copying to a blob container as well, but the same issue remains: the files keep their <code>.txt</code> extension.</p>

<p>Is it possible to change the file extension after a copy?</p>
","<azure><azure-data-lake><azure-data-factory>","2019-08-27 12:55:08","1775","1","2","57675848","<p>You can define the end result filename in the sink dataset, if the files are always going to be called the same then you can hardcore it. If not, it might get a bit more complex because you will want to use a GetMetadata to get every filename, then a Foreach to iterate over them and copy each one with a modified name.</p>

<p>Hope this helped!</p>
"
"57675182","How to convert txt to csv files in Azure Data Factory","<p>I'm trying to move files in between multiple gen1 Azure data lake storage instances without having to resort to writing an Azure App Function or directly calling the Azure storage SDK.</p>

<p>The situation is that I've got a few text files in data lake 1 called <code>test1.txt</code> and <code>test2.txt</code>. They're both files with tabs as delimiters. When I try to use the copy activity with <code>.csv</code> as extension, the files do get moved over to data lake 2, but stay <code>.txt</code> files.</p>

<p>The copy activity source and sink looks as follows:</p>

<pre><code>{
    ""typeProperties"": {
        ""source"": {
            ""type"": ""DelimitedTextSource"",
            ""storeSettings"": {
                ""type"": ""AzureDataLakeStoreReadSettings"",
                ""recursive"": true,
                ""wildcardFileName"": ""*.*"",
                ""enablePartitionDiscovery"": false
            },
            ""formatSettings"": {
                ""type"": ""DelimitedTextReadSettings""
            }
        },
        ""sink"": {
            ""type"": ""DelimitedTextSink"",
            ""storeSettings"": {
                ""type"": ""AzureDataLakeStoreWriteSettings""
            },
            ""formatSettings"": {
                ""type"": ""DelimitedTextWriteSettings"",
                ""quoteAllText"": true,
                ""fileExtension"": "".csv""
            }
        }
    }
}
</code></pre>

<p>I've tried copying to a blob container as well, but the same issue remains: the files keep their <code>.txt</code> extension.</p>

<p>Is it possible to change the file extension after a copy?</p>
","<azure><azure-data-lake><azure-data-factory>","2019-08-27 12:55:08","1775","1","2","72309896","<p>I had the same issue but then I noticed that in the copy data sink settings there is an  option to set file extension, which was set to .txt. I changed it to .csv and it solved the issue.
Hope this helps.</p>
"
"57665096","Fail to create dataset using azure sdk python for azure data factory","<p>I am trying to create dataset in ADF using azure sdk for python, unfortunately I am running into this error message. I am not sure what is wrong with my code below.</p>

<pre><code>dsOut_name = 'POC_DatasetName'
ds_ls =""AzureBlobStorage""
output_blobpath = '/tempdir'
df_name = 'pipeline1'
dsOut_azure_blob = AzureBlobDataset(linked_service_name=ds_ls, folder_path=output_blobpath)
dsOut = adf_client.datasets.create_or_update(rg_name, df_name, dsOut_name, dsOut_azure_blob)
print_item(dsOut)
</code></pre>

<pre><code>Error Message: SerializationError: Unable to build a model: Unable to deserialize to object: type, AttributeError: 'str' object has no attribute 'get', DeserializationError: Unable to deserialize to object: type, AttributeError: 'str' object has no attribute 'get'
</code></pre>

<p>Help Please</p>
","<azure><dataset><azure-data-factory><azure-sdk-python>","2019-08-26 21:22:03","582","1","1","57666890","<p>I can reproduce your issue, this line <code>ds_ls =""AzureBlobStorage""</code> is wrong, it should be <code>ds_ls = LinkedServiceReference(reference_name=ls_name)</code>.</p>

<p><a href=""https://i.stack.imgur.com/D83OH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D83OH.png"" alt=""enter image description here""></a></p>

<p>You could refer to my complete working sample.</p>

<p>Make sure your service principal has an RBAC role(e.g <code>Owner</code>,<code>Contributor</code>) in the <code>Access control (IAM)</code> of your data factory and you have done all the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-python#prerequisites"" rel=""nofollow noreferrer""><strong>Prerequisites</strong></a>.</p>

<p>My package version:</p>

<pre><code>azure-mgmt-datafactory  0.6.0
azure-mgmt-resource  3.1.0
azure-common  1.1.23
</code></pre>

<hr>

<p>Code:</p>

<pre><code>from azure.common.credentials import ServicePrincipalCredentials
from azure.mgmt.resource import ResourceManagementClient
from azure.mgmt.datafactory import DataFactoryManagementClient
from azure.mgmt.datafactory.models import *


subscription_id = '&lt;subscription-id&gt;'
ls_name = 'storageLinkedService'
rg_name = '&lt;group-name&gt;'
df_name = '&lt;datafactory-name&gt;'

credentials = ServicePrincipalCredentials(client_id='&lt;client id of the service principal&gt;',
                                          secret='&lt;secret of the service principal&gt;', tenant='&lt;tenant-id&gt;')
resource_client = ResourceManagementClient(credentials, subscription_id)
adf_client = DataFactoryManagementClient(credentials, subscription_id)


storage_string = SecureString('DefaultEndpointsProtocol=https;AccountName=&lt;storage account name&gt;;AccountKey=&lt;storage account key&gt;')

ls_azure_storage = AzureStorageLinkedService(connection_string=storage_string)
ls = adf_client.linked_services.create_or_update(rg_name, df_name, ls_name, ls_azure_storage)

ds_ls = LinkedServiceReference(reference_name=ls_name)


# Create an Azure blob dataset (output)
dsOut_name = 'ds_out'
output_blobpath = '&lt;container name&gt;/&lt;folder name&gt;'
dsOut_azure_blob = AzureBlobDataset(linked_service_name=ds_ls, folder_path=output_blobpath)
dsOut = adf_client.datasets.create_or_update(rg_name, df_name, dsOut_name, dsOut_azure_blob)
print(dsOut)
</code></pre>

<p><a href=""https://i.stack.imgur.com/RcKuZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RcKuZ.png"" alt=""enter image description here""></a></p>
"
"57664561","Loop over of table names ADFv2","<p>I have several DB tables and would like to loop over them and then pass table name to next activity. </p>

<p>I tried using ADFv2. But im confused, how to loop over names. </p>

<p>I have tried putting names as Arrays in Pipeline Parameters. But it didn't work. </p>

<p>Can you provide some info. how to achieve this?</p>

<p>Thank you.</p>
","<azure-data-factory>","2019-08-26 20:28:32","621","1","1","57664822","<p>What you are looking for is the lookup activity. What you should do is a Lookup Activity that brings all the table names, and then a foreach activity that uses each table name as the item. Something like this:</p>

<p><a href=""https://i.stack.imgur.com/5vUle.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5vUle.png"" alt=""enter image description here""></a></p>

<p>In the Lookup Activity you will use a query that brings you every table you want to loop over, for example (Sql Server):</p>

<pre><code>SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE='BASE TABLE'
</code></pre>

<p>Then, in the ForEach activity you will go to settings, and in the Item box use something like:</p>

<pre><code>@activity('Lookup1').output.TABLE_NAME
</code></pre>

<p>This way, each iteration of the ForEach activity will the outputs of the lookup. Then, just double click on the foreach activity and do whatever you need to do. To reference the item inside of the foreach simply use @item().</p>

<p>Hope this helped! Here is the documentation on these kind of activities</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity</a></p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity</a></p>
"
"57663746","ADF data flow how to handle empty string when converting to integer","<p>How does one handle the conversion of string to integer when copying data from one sql table to another using ADF data flow?
I have one field that is empty and using the toInteger() function is causing an error.
how can I tell the expression to not convert if the value is null?</p>

<p>I see there is an if expression but i am unsure how to get it to work correctly. I tried using the iif, but the copy still fails as the toInteger() function cannot convert an empty string to integer</p>

<p><code>iif(DivisionNumber == ' ',0, toInteger(DivisionNumber))</code></p>
","<azure-data-factory>","2019-08-26 19:21:14","6221","1","2","57663974","<p>An empty string is <code>''</code> and is different from <code>' '</code>. Have you tried without space between your quotes ? </p>

<p>Hope it helps.</p>
"
"57663746","ADF data flow how to handle empty string when converting to integer","<p>How does one handle the conversion of string to integer when copying data from one sql table to another using ADF data flow?
I have one field that is empty and using the toInteger() function is causing an error.
how can I tell the expression to not convert if the value is null?</p>

<p>I see there is an if expression but i am unsure how to get it to work correctly. I tried using the iif, but the copy still fails as the toInteger() function cannot convert an empty string to integer</p>

<p><code>iif(DivisionNumber == ' ',0, toInteger(DivisionNumber))</code></p>
","<azure-data-factory>","2019-08-26 19:21:14","6221","1","2","57670825","<p>Seems we may have many situations here, in this case to use <code>case()</code> function may be a better choice, have a try like below:</p>

<pre><code>case(DivisionNumber == ' ',0,DivisionNumber == '',0,isNull(DivisionNumber),0,toInteger(DivisionNumber))
</code></pre>

<p>The value will be replaced by 0 while including these: <code>' '</code> or <code>''</code> or <code>null</code>， Other than this will use <code>toInteger()</code>.</p>

<p>If having any other cases need to be transformed you can also add it into this script.</p>
"
"57652693","I am not able to load the string value consists of "" , / from .csv into SQL using ADF v2","<p>I have a .CSV file wanted to load into SQL using Azure data factory V2. One of the column string value as (Hi there"" how,are /you) but i am not able to load this record with string when it has "" , / in to sql. </p>

<p>Escape Character /(Backslash)
Quote Character  ""(double quotes)
Column delimiter .(coma)</p>

<p>Want to pass the record when the string like below into sql table.
Column1
Hi there"" how,are /you</p>
","<sql><json><azure><csv><azure-data-factory>","2019-08-26 06:20:36","1507","0","1","57653834","<p>As far as I know, the only way is that you need to edit the csv file, then you can load this data into the sql table in Data Factory.</p>

<p>You need change your csv data follow this document: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#text-format"" rel=""nofollow noreferrer"">Text format</a>:
<a href=""https://i.stack.imgur.com/d4uSo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/d4uSo.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p><strong>Note:</strong> <strong>You cannot specify both escapeChar and quoteChar for a table.</strong></p>
</blockquote>

<p>So you should change the column1 data like this, add Escape Character ""\"" before "",are/you"":</p>

<pre><code>Hi there""how\,are/you
</code></pre>

<p>And don't set the Quote Character.</p>

<p><a href=""https://i.stack.imgur.com/utMp7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/utMp7.png"" alt=""enter image description here""></a></p>

<p><strong>Update:</strong></p>

<p>This the test2.csv file in my Blob Storage:
<a href=""https://i.stack.imgur.com/Ug7dg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ug7dg.png"" alt=""enter image description here""></a></p>

<p>Source dataset setting in pipline:</p>

<ol>
<li><strong>Column delimeiter</strong>: comma(,)</li>
<li><strong>Row delimiter</strong>: Auto detect (\r,\n, or \r\n)</li>
<li><strong>Escape character</strong>: Backslash ()</li>
<li><strong>Quote Character</strong>: No quote character</li>
</ol>

<p>Source dataset json code:</p>

<pre><code>{
    ""name"": ""DelimitedText1"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureBlobStorage1"",
            ""type"": ""LinkedServiceReference""
        },
        ""annotations"": [],
        ""type"": ""DelimitedText"",
        ""typeProperties"": {
            ""location"": {
                ""type"": ""AzureBlobStorageLocation"",
                ""fileName"": ""test2.csv"",
                ""container"": ""containerleon""
            },
            ""columnDelimiter"": "","",
            ""escapeChar"": ""\\"",
            ""firstRowAsHeader"": false,
            ""quoteChar"": """"
        },
        ""schema"": [
            {
                ""type"": ""String""
            },
            {
                ""type"": ""String""
            },
            {
                ""type"": ""String""
            }
        ]
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p>Hope this helps.</p>
"
"57632187","apply trim before copy using azure data factory","<p>I have a pipeline that copies files from azure datalake to azure SQL </p>

<ul>
<li>Gets Meta Data of files in a specific path in Datalake</li>
<li><p>Loops through the output and copies the file into a database table.</p>

<ul>
<li>file <code>A.csv</code> is copied to <code>dbo.[A]</code></li>
<li>file <code>B.csv</code> is copied to <code>dbo.[B]</code></li>
</ul></li>
</ul>

<p><a href=""https://i.stack.imgur.com/0kAkJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0kAkJ.png"" alt=""Get Meta Data and Copy in a For Each Loop""></a></p>

<h1>Question</h1>

<ul>
<li>Is there a generic way to define a <code>trim</code> on the column values being copied ?</li>
</ul>
","<azure-data-factory>","2019-08-23 19:43:50","6483","1","2","57654416","<p><code>trim</code> feature is supported by ADF expressions.(<a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#trim"" rel=""nofollow noreferrer"">link</a>)</p>

<p>e.g.:</p>

<pre><code>trim('!--!wor!ld!', '-!') -&gt; 'wor!ld'
</code></pre>

<p>Trims a string of leading and trailing characters. If second parameter is unspecified, it trims whitespace. Else it trims any character specified in the second parameter</p>
"
"57632187","apply trim before copy using azure data factory","<p>I have a pipeline that copies files from azure datalake to azure SQL </p>

<ul>
<li>Gets Meta Data of files in a specific path in Datalake</li>
<li><p>Loops through the output and copies the file into a database table.</p>

<ul>
<li>file <code>A.csv</code> is copied to <code>dbo.[A]</code></li>
<li>file <code>B.csv</code> is copied to <code>dbo.[B]</code></li>
</ul></li>
</ul>

<p><a href=""https://i.stack.imgur.com/0kAkJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0kAkJ.png"" alt=""Get Meta Data and Copy in a For Each Loop""></a></p>

<h1>Question</h1>

<ul>
<li>Is there a generic way to define a <code>trim</code> on the column values being copied ?</li>
</ul>
","<azure-data-factory>","2019-08-23 19:43:50","6483","1","2","57699212","<p>To apply a trim rule across ALL string columns in a dataset in ADF, use Mapping Data Flows with a column pattern.</p>

<p>In a Derived Column, set your matching rule to ""type == 'string'"".</p>

<p>In the Column Name, keep it the same using $$.</p>

<p>In the Value, set trim ($$,'...')</p>

<p>Reference docs:</p>

<p>Matching Patterns: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-column-pattern"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-column-pattern</a></p>

<p>Derived Column: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column</a></p>
"
"57631625","Azure Data Flow or Data factory v2 how to Merge two tables like we use to use Merger in SSIS","<p>What I am trying to achieve is to merge two tables sources and destination using merge in Azure data Flow or Data Factory, so it Deletes or Update or Insert data in respect to the Sources.</p>

<p>Preferring using Data flow</p>
","<azure-data-factory>","2019-08-23 18:52:25","963","0","2","57657722","<p>You can use the join transformation: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-join"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-join</a></p>

<p>It even compares this transformation to the merge in SSIS.</p>

<p>Hope this helped!</p>
"
"57631625","Azure Data Flow or Data factory v2 how to Merge two tables like we use to use Merger in SSIS","<p>What I am trying to achieve is to merge two tables sources and destination using merge in Azure data Flow or Data Factory, so it Deletes or Update or Insert data in respect to the Sources.</p>

<p>Preferring using Data flow</p>
","<azure-data-factory>","2019-08-23 18:52:25","963","0","2","58194771","<p>It's going to be a lot easier to convert objects to dataframes and work with those.  Something like this, perhaps.</p>

<pre><code>result = DataFrame.join([other DataFrame], how='inner', on=None)
</code></pre>
"
"57629817","Azure Data Factory: Response Content is not a valid JObject","<p>My ADF pipeline invokes an Azure Function (written in Java). </p>

<p>However, the pipeline fails with this message:</p>

<pre><code>Response Content is not a valid JObject.
</code></pre>

<p>I've looked high and low but can't seem to find any examples of a JObject in Java.</p>

<p>Can anyone enlighten me as to how this may be done?</p>
","<azure><azure-functions><azure-data-factory>","2019-08-23 16:13:17","3998","2","3","57629959","<p>You simply need an JSON object returned from functions. </p>

<pre><code>{
    ""name"": ""adam""
}
</code></pre>

<p>Unfotunately if you return array of objects, while it's proper JSON notation it will fail with <code>Response Content is not a valid JObject.</code> error.</p>

<p>So instead of </p>

<pre><code>[{
    ""name"": ""adam""
}, {
    ""name"": ""tom""
}]
</code></pre>

<p>Return something like</p>

<pre><code>{
    ""items"": [{
        ""name"": ""adam""
    }, {
        ""name"": ""tom""
    }]
}
</code></pre>

<p>Depending on API you might need to add</p>

<pre><code>Accept: application/json 
</code></pre>

<p>header to the request.</p>

<p>In java there is few options to do this one of which might be</p>

<pre><code>package com.demo;

import javax.ws.rs.GET;
import javax.ws.rs.Path;
import javax.ws.rs.PathParam;
import javax.ws.rs.Produces;
import javax.ws.rs.core.MediaType;

@Path(""/users"")
public class JsonFromRestful 
{
    @GET
    @Path(""/"")
    @Produces(MediaType.APPLICATION_JSON)
    public Customer produceCustomerDetailsinJSON() {
            Customer cust = new Customer();        
            cust.setCustNo(1);
            cust.setCustName(""demo"");
            cust.setCustCountry(""poland"");
            return cust;
    }

}
</code></pre>
"
"57629817","Azure Data Factory: Response Content is not a valid JObject","<p>My ADF pipeline invokes an Azure Function (written in Java). </p>

<p>However, the pipeline fails with this message:</p>

<pre><code>Response Content is not a valid JObject.
</code></pre>

<p>I've looked high and low but can't seem to find any examples of a JObject in Java.</p>

<p>Can anyone enlighten me as to how this may be done?</p>
","<azure><azure-functions><azure-data-factory>","2019-08-23 16:13:17","3998","2","3","60854621","<p>I got around this by using the ""Web"" activity rather than the ""Azure Function"" activity.</p>

<p>In the web activity, you need to enter the full URL, so therefore manually need to parameterise the Azure Function base URL and the access code, but otherwise the experience is very similar and potentially more flexible.</p>

<p>The content of the API response is contained in the JSON property ""Response"". i.e.</p>

<pre><code>{
    ""Response"": ""[{\""campaign_name\"":\""Interactive Support\"",\""campaign_end\"":\""26/01/2020\""}]"",
    ""ADFWebActivityResponseHeaders"": {
        ""Request-Context"": ""appId=cid-v1:691579a9-cebe-4c49-9301-99463f150d13"",
        ""Date"": ""Wed, 25 Mar 2020 17:45:27 GMT""
    },
    ...
}
</code></pre>

<p>so can then be extracted using <code>@json(activity('YourWebActivity').output.Response)</code></p>
"
"57629817","Azure Data Factory: Response Content is not a valid JObject","<p>My ADF pipeline invokes an Azure Function (written in Java). </p>

<p>However, the pipeline fails with this message:</p>

<pre><code>Response Content is not a valid JObject.
</code></pre>

<p>I've looked high and low but can't seem to find any examples of a JObject in Java.</p>

<p>Can anyone enlighten me as to how this may be done?</p>
","<azure><azure-functions><azure-data-factory>","2019-08-23 16:13:17","3998","2","3","62773377","<p>Please check your returning value. Because sometimes needed additional cast (next example on the C#):</p>
<pre><code>return (ActionResult)new OkObjectResult( new {Result = response.StatusCode.ToString()});
</code></pre>
<p>It is not working if we return simple string or numerical value.
Hope it will help you or anybody else.</p>
"
"57622042","How can I get the last day of a month in dynamic content in ADF2?","<p>I want to get the last day of a month based on the utcnow() timestamp.</p>

<p>Instead of ""dd"" in the expression bellow there should be automatically the last day of the month (28, 30 or 31):</p>

<pre><code>@{formatDateTime(adddays(utcnow(),-2), 'yyyy-MM-ddT23:59:59.999')}
</code></pre>

<p>Thinking that it´s actually august I expect the following result out of the expression:
<strong>""2019-08-31T23:59:59.999""</strong></p>
","<azure-data-factory>","2019-08-23 07:59:39","8810","8","2","57629928","<p>I would recommend the simplest way to do this is store the dates and their respective end of month dates in a table or file (eg in Data Lake or Blob Store) and then just use a Lookup task to get the date.  This would be the easiest to maintain and debug.</p>
<p>If you have some compute in your pipeline, why not pass it off to that where it will be undoubtedly easier.  eg if you have a SQL Database, store a calendar table with a pre-calculated end-of-month date column.  Then it's as simple as doing a lookup.  SQL Server and Azure SQL DB even have a function built in so you don't have to write your own:</p>
<pre><code>SELECT EOMONTH ( GETDATE() )
</code></pre>
<p>How easy is that?  If you're using Databricks there is a simple example <a href=""https://stackoverflow.com/questions/44686700/convert-date-to-end-of-month-in-spark"">here</a>.</p>
<p>If you absolutely have to do it inline in Data Factory using built-in functions and expression, then it's technically possible, but I'd do some through testing  first:</p>
<pre><code>@addDays(formatDateTime(addDays(formatDateTime(variables('inputDate'), 'yyyy-MM-28'),5), 'yyyy-MM-01T23:59:59.999'),-1)
</code></pre>
<p>This simpler approach (see edits for older more complex versions ) artificially sets the day to 28, then adds 5 days (which always lands you in the next month ), then resets the day to 1 and then takes 1 day off.</p>
<p>This has been tested successfully for dates 1 Jan 2000 to 31 Dec 2040.</p>
<p>I'd advise using a variable ( eg <code>variables('inputDate')</code> ), so you can test it but you can replace <code>variables('inputDate')</code> with <code>utcnow()</code> when you're finished testing.</p>
"
"57622042","How can I get the last day of a month in dynamic content in ADF2?","<p>I want to get the last day of a month based on the utcnow() timestamp.</p>

<p>Instead of ""dd"" in the expression bellow there should be automatically the last day of the month (28, 30 or 31):</p>

<pre><code>@{formatDateTime(adddays(utcnow(),-2), 'yyyy-MM-ddT23:59:59.999')}
</code></pre>

<p>Thinking that it´s actually august I expect the following result out of the expression:
<strong>""2019-08-31T23:59:59.999""</strong></p>
","<azure-data-factory>","2019-08-23 07:59:39","8810","8","2","59459661","<pre><code>    @adddays(startOfMonth(addToTime(utcnow(), 1, 'Month')), - 1, 'yyyy-MM-dd')
</code></pre>

<p>More here:</p>

<p><a href=""https://social.technet.microsoft.com/wiki/contents/articles/53497.azure-data-factory-expressions-for-strings-dates-and-more.aspx"" rel=""noreferrer"">Azure Data Factory - Expressions for Strings, Dates and More</a></p>
"
"57599942","Getting error in sql query ""The column 'nextid' was specified multiple times""","<p>I am getting an error when executing this SQL query:</p>

<blockquote>
  <p>The column 'nextid' is specified multiple times</p>
</blockquote>

<p>It is working fine in SQL Server but when I try to run the query on Azure Data flow as source query, I get the error.</p>

<pre><code>SELECT 
    CASE 
       WHEN bp.nextid IS NULL 
          THEN 
             CASE 
                WHEN nextid = '100000' THEN '100000' 
                WHEN nextid = '300000' THEN '300000' 
                WHEN nextid = '400000' THEN '400000' 
                   THEN '500000' 
                ELSE '500000' 
             END 

       WHEN bp.nextid IS NOT NULl 
          THEN bp.nextid    
    END 'LastBudgetPoolId',  
    * 
FROM 
    staging.nextid bp 
LEFT JOIN 
    staging.gca ga ON bp.next id = ga.nextid; 
</code></pre>
","<sql-server><azure-data-factory>","2019-08-21 22:33:32","85","0","1","57698520","<p>As @larnu said that don't use <code>*</code> so I change the query and added column names after that working fine as</p>

<pre><code>SELECT 
    CASE 
       WHEN bp.nextid IS NULL 
          THEN 
             CASE 
                WHEN nextid = '100000' THEN '100000' 
                WHEN nextid = '300000' THEN '300000' 
                WHEN nextid = '400000' THEN '400000' 
                   THEN '500000' 
                ELSE '500000' 
             END 

       WHEN bp.nextid IS NOT NULl 
          THEN bp.nextid    
    END 'LastBudgetPoolId',  
    id,name,username ,nextid
FROM 
    staging.nextid bp 
LEFT JOIN 
    staging.gca ga ON bp.next id = ga.nextid;
</code></pre>
"
"57586864","ADF Pipeline Bulk copy activity to Log files","<p>I have a bulk copy template to azure blob storage data transfer set up in ADF. This activity will dynamically produce 'n' number of files. 
I need to write log file (txt format) after pipeline activity completed finished. 
The log file should have pipeline start &amp; completion  datetime  and also number of files outputted, status etc.</p>

<p>What is the best way or to choose the activity to do this?</p>
","<pipeline><bulk><azure-data-factory>","2019-08-21 07:50:50","465","0","1","57588130","<p>Firstly,i have to say that ADF won't generate log files about the execution information automatically. You could see <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-visually"" rel=""nofollow noreferrer"">Visually monitor</a> and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-programmatically"" rel=""nofollow noreferrer"">Programmatically monitor</a> for activities in ADF.</p>

<p>In above link, you could get the <code>start time</code> of pipeline: <code>Run Start</code>.Even though it does not have any Run End, you could calculate by yourself: Run End = Run Start + Duration.</p>

<p><a href=""https://i.stack.imgur.com/e1Wq3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e1Wq3.png"" alt=""enter image description here""></a></p>

<p>As for the number of files, please refer to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#monitor-programmatically"" rel=""nofollow noreferrer"">link</a>.</p>

<p><a href=""https://i.stack.imgur.com/9KmpD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9KmpD.png"" alt=""enter image description here""></a></p>

<p>Anyway,all these metrics need to be got programatically i think,you could choose the language you are good at.</p>
"
"57583716","DTU utilization is not going above 56%","<p>I'm doing a data load on azure sql server using azure data factory v2. I started the data load &amp; the DB was set to Standard Pricing Tier with 800 DTUs. It was slow, so I increased the DTUs to 1600. (My pipeline is still running since 7 hrs).</p>

<p>I decided to change the pricing tier. I changed the pricing tier to Premium, DTUs set to 1000. (I didnt make any additional changes).</p>

<p>The pipeline failed as it lost connection. I rerun the pipeline.</p>

<p>Now, when I monitor the pipeline, it is working fine. When I monitor the database. The DTU usage on average is not going above 56%.</p>

<p>I am dealing with tremendous data. How can I speed up the process? </p>

<p>I expect the DTUs must max out. But the average utilization is around 56%.</p>
","<azure><azure-sql-database><azure-data-factory>","2019-08-21 02:31:23","836","0","2","57583935","<p>Please follow this document <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance#performance-tuning-steps"" rel=""nofollow noreferrer"">Copy activity performance and scalability guide</a>.</p>

<p>This tutorial gives us the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance#performance-tuning-steps"" rel=""nofollow noreferrer"">Performance tuning steps</a>. </p>

<p>One of ways is increase the Azure SQL Database tier with more DTUs. You have increased the Azure SQL Database tier with more 1000 DTUs, but the average utilization is around 56%. I think You don't need so higher price tier.</p>

<p>You need to think about other ways to improve the performance. Such as set more <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance#data-integration-units"" rel=""nofollow noreferrer"">Data Integration Units(DIU)</a>.</p>

<p>A Data Integration Unit is a measure that represents the power (a combination of CPU, memory, and network resource allocation) of a single unit in Azure Data Factory. Data Integration Unit only applies to Azure integration runtime, but not self-hosted integration runtime.</p>

<p>Hope this helps.</p>
"
"57583716","DTU utilization is not going above 56%","<p>I'm doing a data load on azure sql server using azure data factory v2. I started the data load &amp; the DB was set to Standard Pricing Tier with 800 DTUs. It was slow, so I increased the DTUs to 1600. (My pipeline is still running since 7 hrs).</p>

<p>I decided to change the pricing tier. I changed the pricing tier to Premium, DTUs set to 1000. (I didnt make any additional changes).</p>

<p>The pipeline failed as it lost connection. I rerun the pipeline.</p>

<p>Now, when I monitor the pipeline, it is working fine. When I monitor the database. The DTU usage on average is not going above 56%.</p>

<p>I am dealing with tremendous data. How can I speed up the process? </p>

<p>I expect the DTUs must max out. But the average utilization is around 56%.</p>
","<azure><azure-sql-database><azure-data-factory>","2019-08-21 02:31:23","836","0","2","57668153","<p>The standard answer from Microsoft seems to be that you need to tune the target database or scale up to a higher tier. This suggests that Azure Data Factory is not a limiting factor in the copy performance.</p>

<p>However we've done some testing on a single table, single copy activity, ~15 GB of data. The table did not contain varchar(max), high precision, just simple and plain data.</p>

<p>Conclusion: it does barely matter what kind of tier you choose (not too low ofcourse), roughly above S7 / 800 DTU, 8 vcores, the performance of the copy activity is ~10 MB/s and does not go up. The load on the target database is 50%-75%.</p>

<p>Our assumption is that since we could keep throwing higher database tiers against this problem, but did not see any improvement in the copy activity performance, this is Azure Data Factory related.</p>

<p>Our solution is, since we are loading a lot of separate tables, to scale out instead of scale up via a for each loop and a batch count set to at least 4.</p>

<p>The approach to increase the DIU is only applicable in some cases:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance#data-integration-units"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance#data-integration-units</a></p>

<blockquote>
  <p>Setting of DIUs larger than four currently applies only when you copy
  multiple files from Azure Storage, Azure Data Lake Storage, Amazon S3,
  Google Cloud Storage, cloud FTP, or cloud SFTP to any other cloud data
  stores.</p>
</blockquote>

<p>In our case we are copying data from relational databases.</p>
"
"57583009","In visual expression builder in data flow how do i find median","<p>how do i find median using expression builder in Azure Data Factory - Data Flow</p>

<p>There is ""avg"", ""min"", ""max"" etc but nothing for median.</p>
","<azure-data-factory>","2019-08-21 00:27:41","354","-1","1","57583451","<p>Median is not available as an aggregate function, so there's no Median function in  Data Factory Data.</p>

<p>May you can use the Expression Builder to create a new expression to get the median by yourself.</p>

<p>Reference these document:</p>

<ol>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-expression-builder"" rel=""nofollow noreferrer"">Mapping Data Flow Expression Builder</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions"" rel=""nofollow noreferrer"">Data transformation expressions in Mapping Data Flow</a></li>
</ol>

<p>Hope this helps.</p>
"
"57576958","How can I map each json file to individual collection in comsosDB's database?","<p>I am using copy activity to perform backup and restore activities for cosmosDB. I was able to backup multiple collections from database as individual json files. But when I restore I was not able to map those multiple files to individual collections back in comosDB databases.</p>
","<azure><azure-cosmosdb><azure-blob-storage><azure-data-factory>","2019-08-20 15:27:43","54","0","1","57583402","<p>The collection name need to be defined when you use cosmos db sql api connector as source dataset.</p>

<p><a href=""https://i.stack.imgur.com/a117Y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/a117Y.png"" alt=""enter image description here""></a></p>

<p>So you could define the file name in the blob storage output destination as the name of the collection.</p>

<p><a href=""https://i.stack.imgur.com/WFu85.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WFu85.png"" alt=""enter image description here""></a></p>

<p>When you restore the data back to collection,you could know which collection that this json file maps to. Then execute multiple copy activities one by one to restored data individually.</p>

<p>BTW,another cosmos db <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/import-data"" rel=""nofollow noreferrer"">migration tool</a> could implement same requirements,you could get know about it.</p>
"
"57566291","Azure Custom Connector for 3rd Party API with Oauth 2.0 access token","<p>I am trying to get data from a 3rd party API into an Azure SQL DB using Azure Data Factory with out using SSIS.</p>

<p>This lead me down a rabbit hole and I have been searching for 3 days now and cannot find a solution.I must be missing something.</p>

<p>I have tried using Azure Data Factory and the copy data controls.
I then tried using power apps and cant find anything that helps.
I then tried custom connector, from scrach, from postman and from OpenAPI</p>

<p>I cannot get any of it to work!</p>

<p>I really thought this would be easier than this?</p>

<p>I have read almost all of the standard MicroSoft documentation and none of it helps with my specific scenario.</p>

<p>I have a third party web site that I get an authorisation token from using a username and password with grant-type=password
Using this token I then get JSON data from the site.</p>

<p>I want to get this data into my SQL DB in Azure.</p>

<p>Thats it!</p>

<p>Any help would be greatly appreciated.</p>

<p>Thanks...</p>

<p>PS:Next step is the same thing but API returns XML, rabbit hole 2...</p>
","<rest><azure-data-factory>","2019-08-20 03:19:07","936","0","1","57653374","<p>Here is a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-copy-data-portal"" rel=""nofollow noreferrer"">official guide</a> that import data from Azure Storage to Azure SQL Database using pipeline.</p>

<p>I think this guide will be helpful for you. Before you follow this guide to meet your requirement, you should import your JSON data from 3rd party API first. So you can use <a href=""https://learn.microsoft.com/en-us/azure/logic-apps/"" rel=""nofollow noreferrer"">Azure logic App</a> to finish this process quickly. </p>

<p>I have finished this process , it is easy and just few steps : 
<a href=""https://i.stack.imgur.com/RlY56.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RlY56.png"" alt=""enter image description here""></a>
 1. Getting access token from 3rd party identity provider(here I use Azure AD as identity provider, just for demo) using http request action:
<a href=""https://i.stack.imgur.com/LKNY8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LKNY8.png"" alt=""enter image description here""></a>
 2. Generally , we can get an access token from step1 , use this auth info to call your 3rd API to fetch json data using http request action directly:
<a href=""https://i.stack.imgur.com/gIo60.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gIo60.png"" alt=""enter image description here""></a> 
The API I am using is a sample demo one, just reply a json as : </p>

<pre><code>{
    ""name"": ""stan"",
    ""id"": ""123""
}
</code></pre>

<ol start=""3"">
<li>Using create blob action to create a blob file with the json content from previous step (you should create a storage account with a blob container first , so that you can store files, here I name the json file with guid)
<a href=""https://i.stack.imgur.com/rabDD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rabDD.png"" alt=""enter image description here""></a></li>
</ol>

<p>Run this logic App you can see that a json file has been created in Azure storage account with the json data from API :
<a href=""https://i.stack.imgur.com/sPCSb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sPCSb.png"" alt=""enter image description here""></a></p>

<p>So the steps left , you can just refer to the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-copy-data-portal"" rel=""nofollow noreferrer"">official guide</a> to import data to your SQL, just selecting json as resource format in Configure source section.
If you have anything unclear , pls feel free to let me know. Hope it helps : )  </p>
"
"57553533","Loosing data from Source to Sink in Copy Data","<p>In my MS Azure datafactory, I have a rest API connection to a nested JSON dataset. 
The Source ""Preview data"" shows all data. (7 orders from the online store)</p>

<p>In the ""Activity Copy Data"", is the menu tab ""Mapping"" where I map JSON fields with the sink SQL table columns. If I under ""Collection Reference"" I select None, all 7 orders are copied over. 
But if I want the nested metadata, I select the meta field in ""Collection Reference"", then I get my nested data, in multiple order lines, each with a one metadata point, but I only get data from 1 order, not 7</p>

<p>I think I have a reason for my problem. One of the fields in the nested meta data, is both a string and array. But I still don't have a solution</p>

<p><a href=""https://imgur.com/wHKvFqS"" rel=""nofollow noreferrer"">sceen shot of meta data</a></p>
","<json><rest><azure-data-factory>","2019-08-19 09:04:47","3316","3","1","57686714","<p>Your sense is right,it caused by your nested structure meta data. Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#alternative-schema-mapping"" rel=""nofollow noreferrer"">statements</a> of Collection Reference property:</p>

<blockquote>
  <p>If you want to iterate and extract data from the objects inside an
  array field with the same pattern and convert to per row per object,
  specify the JSON path of that array to do cross-apply. This property
  is supported only when hierarchical data is source.</p>
</blockquote>

<p>same pattern is key point here, I think. However, your data inside metadata array are not same as your screenshot.</p>

<p><a href=""https://i.stack.imgur.com/OhEbm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OhEbm.png"" alt=""enter image description here""></a></p>

<p>My workaround is using Azure Blob Storage to make a transition, <code>REST API ---&gt; Azure Blob Storage---&gt;Your sink dataset</code>. Inside Blob Storage Dataset, you could flatten the incoming JSON data with <code>Cross-apply nested JSON array</code> setting:</p>

<p><a href=""https://i.stack.imgur.com/GsbeD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GsbeD.png"" alt=""enter image description here""></a></p>

<p>You could refer to this <a href=""https://medium.com/@gary.strange/flattening-json-in-azure-data-factory-2f2130794258"" rel=""nofollow noreferrer"">blog</a> to learn about this feature. Then you could copy the flatten data into your destination.</p>
"
"57549619","How to add Dynamic GroupBy Column in Data Flow Aggregate Activity in Azure Data Factory","<p>I am using the Data Flow(preview). My ""Aggregate"" activity requires a GroupBy column which is not dynamic. Hence, I cant group by that column. I just want to map the column by name.</p>

<p>For example:
These are the two schemas:
1) Columns: M Id, Date/Time, Data Type, Values
2) Columns: MID, Date, DataType, Units</p>

<p>Both have actually the same data type and structure. I want to GroupBy DataType and avg(units).</p>

<p>Because, the name of the one field is ""Data Type"" and other ""DataType"". How do I map it together.</p>

<p>I have created a ""Derived"" activity with this </p>

<p>Column: DataType
Expression: case(startsWith(toString(byPosition(7)), 'D'), toString(byName('Data Type')),toString(byName('DataType')))</p>

<p>But it doesnt work. Any help is highly appreciated.</p>

<p>I just want to know how do I map the column by name.</p>
","<azure><azure-data-factory>","2019-08-19 01:39:58","1359","0","2","57561375","<p>You can write a dynamic expression directly in the Group by field in the Aggregate transformation. Hover over the Group By field and select ""Computed Column"" to enter into the Expression Builder.</p>

<p><a href=""https://i.stack.imgur.com/Dt4RZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dt4RZ.png"" alt=""enter image description here""></a></p>
"
"57549619","How to add Dynamic GroupBy Column in Data Flow Aggregate Activity in Azure Data Factory","<p>I am using the Data Flow(preview). My ""Aggregate"" activity requires a GroupBy column which is not dynamic. Hence, I cant group by that column. I just want to map the column by name.</p>

<p>For example:
These are the two schemas:
1) Columns: M Id, Date/Time, Data Type, Values
2) Columns: MID, Date, DataType, Units</p>

<p>Both have actually the same data type and structure. I want to GroupBy DataType and avg(units).</p>

<p>Because, the name of the one field is ""Data Type"" and other ""DataType"". How do I map it together.</p>

<p>I have created a ""Derived"" activity with this </p>

<p>Column: DataType
Expression: case(startsWith(toString(byPosition(7)), 'D'), toString(byName('Data Type')),toString(byName('DataType')))</p>

<p>But it doesnt work. Any help is highly appreciated.</p>

<p>I just want to know how do I map the column by name.</p>
","<azure><azure-data-factory>","2019-08-19 01:39:58","1359","0","2","57561404","<p>Are you trying to determine whether to use the column ""Data Type"" or the column called ""DataType""? If so, just enter your conditional expression directly into the expression builder on the Aggregate group by. Note that in your expression above, you are using byPosition() which is a numeric value for the number representing the incoming columns left to right, starting at position 1. Is that what you intended?</p>
"
"57541745","Not able to connect the server while creating linked service for Postgresql(source), in data factory in azure","<p>I have created database and tables in postgres in linux virtual machine using shell scripts(vm created using azure).Now,I have to transfer those tables to Azure blob storage.For this,I created pipe line in data factory,but while creating linked service for the source (for which I mention IR as Auto resolve,server name as the name I used to ssh into the vm,port number as 5432,database name as the name of the database I created in postgres and username and password as the one which is used for the linux virtual machine).But when I did  Testconnection it gives error.
Can you tell where is the mistake? </p>

<p>Connection failed
ERROR [08001] [DataDirect][ODBC PostgreSQL Wire Protocol driver]Unexpected Network Error. ErrNum = 11003
 ERROR [HY000] [DataDirect][ODBC PostgreSQL Wire Protocol driver]Can't connect to server.</p>
","<azure><azure-data-factory>","2019-08-18 04:53:52","638","0","1","57554114","<p>For transfer data from private network into azure blob storage, you need to use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#self-hosted-ir-network-environment"" rel=""nofollow noreferrer"">Self-Hosted Integration Runtime.</a></p>

<blockquote>
  <p>If you want to perform data integration securely in a private network
  environment, which does not have a direct line-of-sight from the
  public cloud environment, you can install a self-hosted IR on premises
  environment behind your corporate firewall, or inside a virtual
  private network. The self-hosted integration runtime only makes
  outbound HTTP-based connections to open internet.</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/9hper.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9hper.png"" alt=""enter image description here""></a></p>
"
"57531466","How to get Azure Data Factory to Loop Through Files in a Folder","<p>I am looking at the link below.</p>

<p><a href=""https://azure.microsoft.com/en-us/updates/data-factory-supports-wildcard-file-filter-for-copy-activity/"" rel=""noreferrer"">https://azure.microsoft.com/en-us/updates/data-factory-supports-wildcard-file-filter-for-copy-activity/</a></p>

<p>We are supposed to have the ability to use wildcard characters in folder paths and file names.  If we click on the 'Activity' and click 'Source', we see this view.</p>

<p><a href=""https://i.stack.imgur.com/GJfOM.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/GJfOM.png"" alt=""""></a></p>

<p>I would like to loop through months any days, so it should be something like this view.</p>

<p><a href=""https://i.stack.imgur.com/JdX4p.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/JdX4p.png"" alt=""enter image description here""></a></p>

<p>Of course that doesn't actually work.  I'm getting errors that read: ErrorCode: 'PathNotFound'. Message: 'The specified path does not exist.'.  How can I get the tool to recursively iterate through all files in all folders, given a specific pattern of strings in a file path and file name?  Thanks.</p>
","<azure><azure-data-factory>","2019-08-16 21:41:03","23762","5","1","57535449","<blockquote>
  <p>I would like to loop through months any days</p>
</blockquote>

<ul>
<li>In order to do this you can pass two parameters to the activity from your pipeline so that the path can be build dynamically based on those parameters. ADF V2 allows you to pass parameters.</li>
</ul>

<p>Let's start the process one by one:</p>

<h3>1. Create a pipeline and pass two parameters in it for your month and day.<br/></h3>

<p><strong>Note</strong>: This parameters can be passed from the output of other activities as well if needed. Reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/parameters-data-flow#create-parameters-in-mapping-data-flow"" rel=""noreferrer"">Parameters in ADF</a> <br/>
<a href=""https://i.stack.imgur.com/nniAI.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/nniAI.png"" alt=""Passing Params to the copy activity through pipeline""></a>
<br/>
<strong>2. Create two datasets.</strong><br/><br/>
<strong>2.1 Sink Dataset</strong> - Blob Storage here. Link it with your Linked Service and provide the container name (make sure it is existing). Again if needed, it can be passed as parameters.
<a href=""https://i.stack.imgur.com/1h5g2.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1h5g2.png"" alt=""enter image description here""></a><br/><br/>
<strong>2.2 Source Dataset</strong> - Blob Storage here again or depends as per your need. Link it with your Linked Service and provide the container name (make sure it is existing). Again if needed, it can be passed as parameters.
<a href=""https://i.stack.imgur.com/BEsiL.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/BEsiL.png"" alt=""enter image description here""></a>
<br/><strong>Note:</strong> 
1. The folder path decides the path to copy the data. If the container does not exists, the activity will create for you and if the file already exists the file will get overwritten by default.<br/><br/>
2. Pass the parameters in the dataset if you want to build the output path dynamically. Here i have created two parameters for dataset named monthcopy and datacopy.<br/><br/>
<strong>3. Create Copy Activity in the pipeline.</strong><br/><br/>
<strong>Wildcard Folder Path:</strong></p>

<pre><code>    @{concat(formatDateTime(adddays(utcnow(),-1),'yyyy'),'/',string(pipeline().parameters.month),'/',string(pipeline().parameters.day),'/*')}

where:
    The path will become as: current-yyyy/month-passed/day-passed/* (the * will take any folder on one level)
</code></pre>

<p><a href=""https://i.stack.imgur.com/0aMMj.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/0aMMj.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/XQ9av.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/XQ9av.png"" alt=""enter image description here""></a></p>

<h2>Test Result:</h2>

<p><a href=""https://i.stack.imgur.com/JSGfe.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/JSGfe.png"" alt=""enter image description here""></a></p>

<h3>JSON Template for the pipeline:</h3>

<pre><code>{
    ""name"": ""pipeline2"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""Copy Data1"",
                ""type"": ""Copy"",
                ""dependsOn"": [],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""userProperties"": [],
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""DelimitedTextSource"",
                        ""storeSettings"": {
                            ""type"": ""AzureBlobStorageReadSettings"",
                            ""recursive"": true,
                            ""wildcardFolderPath"": {
                                ""value"": ""@{concat(formatDateTime(adddays(utcnow(),-1),'yyyy'),'/',string(pipeline().parameters.month),'/',string(pipeline().parameters.day),'/*')}"",
                                ""type"": ""Expression""
                            },
                            ""wildcardFileName"": ""*.csv"",
                            ""enablePartitionDiscovery"": false
                        },
                        ""formatSettings"": {
                            ""type"": ""DelimitedTextReadSettings""
                        }
                    },
                    ""sink"": {
                        ""type"": ""DelimitedTextSink"",
                        ""storeSettings"": {
                            ""type"": ""AzureBlobStorageWriteSettings""
                        },
                        ""formatSettings"": {
                            ""type"": ""DelimitedTextWriteSettings"",
                            ""quoteAllText"": true,
                            ""fileExtension"": "".csv""
                        }
                    },
                    ""enableStaging"": false
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""DelimitedText1"",
                        ""type"": ""DatasetReference""
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""DelimitedText2"",
                        ""type"": ""DatasetReference"",
                        ""parameters"": {
                            ""monthcopy"": {
                                ""value"": ""@pipeline().parameters.month"",
                                ""type"": ""Expression""
                            },
                            ""datacopy"": {
                                ""value"": ""@pipeline().parameters.day"",
                                ""type"": ""Expression""
                            }
                        }
                    }
                ]
            }
        ],
        ""parameters"": {
            ""month"": {
                ""type"": ""string""
            },
            ""day"": {
                ""type"": ""string""
            }
        },
        ""annotations"": []
    }
}
</code></pre>

<h3>JSON Template for the SINK dataset:</h3>

<pre><code>{
    ""name"": ""DelimitedText1"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureBlobStorage1"",
            ""type"": ""LinkedServiceReference""
        },
        ""annotations"": [],
        ""type"": ""DelimitedText"",
        ""typeProperties"": {
            ""location"": {
                ""type"": ""AzureBlobStorageLocation"",
                ""container"": ""corpdata""
            },
            ""columnDelimiter"": "","",
            ""escapeChar"": ""\\"",
            ""quoteChar"": ""\""""
        },
        ""schema"": []
    }
}
</code></pre>

<h3>JSON Template for the Source Dataset:</h3>

<pre><code>{
    ""name"": ""DelimitedText2"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureBlobStorage1"",
            ""type"": ""LinkedServiceReference""
        },
        ""parameters"": {
            ""monthcopy"": {
                ""type"": ""string""
            },
            ""datacopy"": {
                ""type"": ""string""
            }
        },
        ""annotations"": [],
        ""type"": ""DelimitedText"",
        ""typeProperties"": {
            ""location"": {
                ""type"": ""AzureBlobStorageLocation"",
                ""folderPath"": {
                    ""value"": ""@concat(formatDateTime(adddays(utcnow(),-1),'yyyy'),dataset().monthcopy,'/',dataset().datacopy)"",
                    ""type"": ""Expression""
                },
                ""container"": ""copycorpdata""
            },
            ""columnDelimiter"": "","",
            ""escapeChar"": ""\\"",
            ""quoteChar"": ""\""""
        },
        ""schema"": []
    }
}
</code></pre>
"
"57529810","How to set the ADF V1 pipeline to run every Tuesday","<p>I am using ADF V1 in Azure.</p>

<p>I want my pipeline to run every Tuesday at 10:00AM. <strong>I know how to set the time but how to set particular day of the week in dataset and pipeline?</strong>.</p>

<p>I want my pipeline to run every Tuesday 10:00 AM.</p>

<p>my sample data set</p>

<pre><code>{
  ""$schema"": ""http://datafactories.schema.management.azure.com/internalschemas/2015-09-01/Microsoft.DataFactory.table.json"",
  ""name"": ""SQL-My-Table-DS"",
  ""properties"": {
    ""structure"": [
      {
        ""name"": ""ServiceName"",
        ""type"": ""String""
      }
    ],
    ""published"": false,
    ""type"": ""SqlServerTable"",
    ""linkedServiceName"": ""MyLinkedService"",
    ""typeProperties"": {
      ""tableName"": ""[common].[MyTable_Staging]""
    },
    ""availability"": {
      ""frequency"": ""Week"",
      ""interval"": 1,
      ""offset"": ""00:00:10""
    },
    ""external"": false,
    ""policy"": {}
  }
}
</code></pre>
","<azure><azure-data-factory>","2019-08-16 18:56:59","109","0","1","57534424","<p>If you are using data factory version 1, you can achieve this by setting the availability with frequency month, interval 1, and set the offset with the number of the day you want the pipeline to run.</p>

<p>For example if you want it to run the 9th of each month as you said, you will have something like this:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code> ""availability"": {
""frequency"": ""Month"",
""interval"": 1,
""offset"": ""9.00:00:00"", 
""style"": ""StartOfInterval""
}</code></pre>
</div>
</div>
</p>

<p><strong>Editing the answer for week also</strong>, below code snippet will make pipeline to run every Tuesday.</p>

<pre><code>""availability"": {
""frequency"": ""Week"",
""interval"": 1,
""offset"": ""2.00:00:00"", 
""style"": ""StartOfInterval""
}
</code></pre>
"
"57525388","Data Factory Self-Hosted IntegrationRuntime Inactive Node","<p>I tries to setup multi nodes on Data Factory v2 self hosted IR, After setting up nodes dispatcher node is up and running but worker node shows inactive status, after restart dmgsvc its active for minute and goes inactive state.Did anyone experience this or know how to fix please advise.</p>

<p>Error: Unable to connect to other integration runtime (self-hosted) nodes. for details please refer blah blah, and the url points to datafactory v1 data management gateway, which is useless.</p>
","<azure><azure-data-factory>","2019-08-16 13:18:42","1002","1","1","57528237","<p>Make sure there are no configurations to hibernate or sleep the vm where you installed the IR. Even if the windows configuration shows that its not going to hibernate or sleep, the company may have group policies, or some configuration on the virtualization HyperV that puts the vm to sleep if its not actively being used.</p>

<p>Hope this helped!</p>
"
"57522174","Can I create SQL tables using Azure Data Factory with a dynamic schema","<p>I'm trying to use Azure Data Factory to take csv's and turn them into SQL tables in the DW. </p>

<p>The columns will change often so it need's to be dynamically taking the csv's schema.</p>

<p>I've tried using get metadata to get the structure and data type, but I'm unable to parse it into the relevant format to create the sql table.</p>

<p>has anyone done anything like this on ADF? Is it possible?</p>
","<azure><azure-data-factory>","2019-08-16 09:30:12","8879","4","1","57527764","<p>Yes - it takes a bit of configuration, but you can accomplish this with Azure Data Factory Data Flow (ADFDF). </p>

<ol>
<li><p>Create a DataSet pointing to your CSV location (I'm assuming Azure Blob Storage).</p>

<ul>
<li>Initially, select a specific CSV file.</li>
<li>On the Schema tab, click ""Import schema"". It is OK that this will change later, but the DataSet must have a schema at design time.</li>
<li>On the Parameters tab, create a parameter for the blobName.</li>
<li>On the Connection tab, reference that parameter in the ""File"" box. You will set its value in the pipeline at runtime. [This overrides the initial value used to define the schema].</li>
</ul></li>
<li><p>Create a DataSet for the SQLDW table.</p>

<ul>
<li>Select ""Create new table""</li>
<li>Add the schema and table names [this should be configurable/overrideable later via DataSet parameters if needed]</li>
<li>The Schema tab will show no Schema.</li>
</ul></li>
<li><p>Create a DataFlow to move the data from CSV to SQLDW.</p>

<ul>
<li>SOURCE: select the DataSet created in step 1.

<ul>
<li>On the Source Settings tab: Make sure ""Allow schema drift"" is checked and ""Validate schema"" is unchecked [These are the default settings]. </li>
<li>CHECK ""Infer drifted column types"", which is NOT the default.</li>
</ul></li>
<li>SINK: select the DataSet created in step 2.

<ul>
<li>On the Sink tab: Make sure ""Allow schema drift"" is checked and ""Validate schema"" is unchecked [These are the default settings].</li>
<li>On the Settings tab, change ""Table action"" to ""Recreate table"". This should infer the new schema and drop and create the columns based on what it finds.</li>
<li>On the Mappings tab: make sure ""Auto Mapping"" is enabled [should be by default]</li>
</ul></li>
</ul></li>
<li><p>In the Pipeline:</p>

<ul>
<li>Create a parameter for ""blobName""</li>
<li>Select the Data Flow activity: 

<ul>
<li>On the Settings tab: set the source parameter for blobName to the pipeline parameter you just created.</li>
<li><strong>SQLDW</strong> specific: you will need to provide a Blob Storage Linked Service and location for Polybase.</li>
</ul></li>
</ul></li>
<li><p>CAVEATS</p>

<ul>
<li>From what I've seen, every column in the SQLDW table is created as NVARCHAR(MAX). I thought the ""Infer drifted column types"" would address this, but apparently not. </li>
<li>This configuration assumes that the first row of the CSV is a header row.</li>
<li>The Sink operation will fail if the incoming column names in the header row contain spaces or special characters. To combat this in a production scenario, you should add a SELECT in between the Source and Sink activities in the Data Flow, then use the new Rule-based mapping and expressions to strip out any invalid characters.</li>
<li>My example uses the same SQLDW schema and table name every time, but as mentioned in step 2 above, you should be able to create DataSet parameters to override those at runtime if needed.</li>
</ul></li>
</ol>
"
"57521049","U-SQL-- Read latest modified file from a folder","<p>How can we read latest modified file from two different folders in U-SQL ??
NOTE: There will be many files in folder.But we want only latest file(single file)</p>

<p>first folder : E:\mysystem\dailyfiles\daily
second folder : E:\mysystem\weeklyfiles\weekly</p>

<p>DECLARE @file1 string = ""dailyfiles/daily/LATESTMODIFIEDFILENAME.csv"";
DECLARE @file2 string = ""weeklyfiles/weekly/LATESTMODIFIEDFILENAME.csv"";</p>

<p>DECLARE @out string = ""/output/result.csv"";  </p>

<p>@data =
    EXTRACT col1 string,
            col2 string,
            col3 string,
            col4 string
    FROM @file1,@file2
    USING Extractors.Csv();</p>
","<azure-sql-database><azure-data-lake><u-sql><azure-data-factory>","2019-08-16 08:09:27","138","1","1","57524698","<p>So I guess you want from two different folders with many files (and I suppose files have the same format) to take a file that was modified recently (latest modified file). You should use file function and virtual columns for dynamic path</p>

<pre><code>@allData =
    EXTRACT col1 string,
            col2 string,
            col3 string,
            DateModified = FILE.MODIFIED(),
            folder1 string, //virtualcolumn
            folder2 string //virtualcolumn
    FROM ""mysystem/{folder1}/{folder2}/{*}.csv""
    USING Extractors.Csv();


OUTPUT
(
    SELECT col1,
           col2,
           col3
    FROM @allData AS a
         SEMIJOIN
         (
         SELECT MAX(DateModified) AS MaxFileDate
         FROM @allData
         WHERE (folder1 == ""dailyfiles"" AND folder2 == ""daily"") OR (folder1 == ""weeklyfiles"" AND folder2 == ""weekly"")
         GROUP BY DateModified
ORDER BY DateModified DESC
FETCH 1 ROWS
         ) AS b
         ON a.DateModified == b.MaxFileDate
    WHERE (folder1 == ""dailyfiles"" AND folder2 == ""daily"") OR (folder1 == ""weeklyfiles"" AND folder2 == ""weekly"")
)
</code></pre>
"
"57515578","How to update a constant value into sql table field by using copy activity in azure data factory","<p>I have a SQL table which contains different fields along with Load_date. 
I have data in CSV format and stored it in blob storage. Now the job is to copy data from CSV to SQL Table through azure data factory using copy activity. while performing this activity i want to dynamically populate the Load_date field because this fields is not available in CSV.</p>
","<azure><azure-sql-database><azure-data-factory>","2019-08-15 20:06:55","1181","0","2","57515746","<p>You cannot do that unless you use Data Flows: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column</a></p>

<p>If data flows are too expensive, you can first insert the data in the table and then with a Stored Procedure fill all the null values in this Load_date field with getdate() or whatever data you want.</p>

<p>I'll leave an example for your SP's update logic:</p>

<pre><code>update [dbo].[YourTable] set [Load_date] = getdate() where [Load_date] is null
</code></pre>

<p>Then add a Stored Procedure activity in data factory, that will run after the copy activity finishes.</p>

<p>Hope this helped!</p>
"
"57515578","How to update a constant value into sql table field by using copy activity in azure data factory","<p>I have a SQL table which contains different fields along with Load_date. 
I have data in CSV format and stored it in blob storage. Now the job is to copy data from CSV to SQL Table through azure data factory using copy activity. while performing this activity i want to dynamically populate the Load_date field because this fields is not available in CSV.</p>
","<azure><azure-sql-database><azure-data-factory>","2019-08-15 20:06:55","1181","0","2","57518355","<p>If the 'Load_date' field means current time, you can try to set a default value by using: 'getdate()' for this field in target table like below:
<a href=""https://i.stack.imgur.com/leSNA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/leSNA.png"" alt=""enter image description here""></a></p>

<p>Thus you don't need to consider if source CSV file having this field or not, while rows inserting to target table, this field will filling with default value automatically.</p>
"
"57514587","Azure Update Module - Data factory missing?","<p><a href=""https://github.com/microsoft/AzureAutomation-Account-Modules-Update"" rel=""nofollow noreferrer"">https://github.com/microsoft/AzureAutomation-Account-Modules-Update</a></p>

<p>Trying to update Azure Modules but one of the modules does not get updated. Not sure if this module is part of the script provided by Microsoft</p>

<pre><code>AzurmeRm.DataFactoryv2 
</code></pre>

<p>Datafactory  can't start without running the module above.  So you have to upload manually and run the runbook.</p>

<p>Is there a way to incorporate the module above with the automation powershell provided by Microsoft ?</p>

<p>I appreciate any help and tips</p>
","<azure><azure-web-app-service><azure-data-factory>","2019-08-15 18:41:21","46","0","1","57514716","<p>If its missing, you can import it going here:</p>

<p><a href=""https://www.powershellgallery.com/packages/AzureRM.DataFactoryV2/0.5.8"" rel=""nofollow noreferrer"">https://www.powershellgallery.com/packages/AzureRM.DataFactoryV2/0.5.8</a></p>

<p>And click on the tab 'Azure Automation', then the blue button 'Deploy to Azure Automation'.</p>

<p>Hope this helped!</p>
"
"57511294","Case-insensitive column names breaks the Data Preview mode in Data Flow of Data Factory","<p>I have a csv file in my ADLS:</p>

<pre><code>a,A,b
1,1,1
2,2,2
3,3,3
</code></pre>

<p>When I load this data into a delimited text Dataset in ADF with first row as header the data preview seems correct (see picture below). The schema has the names a, A and b for columns.  </p>

<p><a href=""https://i.stack.imgur.com/8YK4y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8YK4y.png"" alt=""enter image description here""></a>  </p>

<p>However, now I want to use this dataset in Mapping Data Flow and here does the Data Preview mode break. The second column name (A) is seen as duplicate and no preview can be loaded. 
<a href=""https://i.stack.imgur.com/TFqSd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TFqSd.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/KMZS0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KMZS0.png"" alt=""enter image description here""></a></p>

<p>All other functionality in Data Flow keeps on working fine, it is only the Data Preview tab that gives an error. All consequent transformation nodes also gives this error in the Data Preview. </p>

<p>Moreover, if the data contains two ""exact"" same column names (e.g. <code>a, a, b</code>), then the Dataset recognizes the columns as duplicates and puts a ""1"" and ""2"" after each name. It is only when they are case-sensitive unequal and case-insensitive equal that the Dataset doesn't get an error and Data Flow does. </p>

<p>Is this a known error? Is it possible to change a specific column name in the dataset before loading into Data Flow? Or is there just something I'am missing?</p>
","<azure-data-factory>","2019-08-15 14:35:06","2383","0","1","57520821","<p>I testes it and get the error in source Data Preview:
<a href=""https://i.stack.imgur.com/EpHFT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EpHFT.png"" alt=""enter image description here"" /></a></p>
<p>I ask Azure support for help and they are testing now. Please wait my update.</p>
<p><strong>Update:</strong></p>
<p>I sent Azure Support the test.csv file. They tested and replied me. If you insist to use &quot; first row as header&quot;, Data Factory can not solve the error. The solution is that re-edit the csv file. Even in Azure SQL database, it doesn't support we create a table with same column name. Column names are case-insensitive.</p>
<p>For example, this code is not supported:
<a href=""https://i.stack.imgur.com/1ZKMv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1ZKMv.png"" alt=""enter image description here"" /></a></p>
<p>Here's the full email message:</p>
<p>Hi Leon,</p>
<p>Good morning! Thanks for your information.</p>
<p>I have tested the sample file you share with me and reproduce the issue. The data preview is alright by default when I connect to your sample file.
<a href=""https://i.stack.imgur.com/igkcn.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/igkcn.jpg"" alt=""enter image description here"" /></a>
But I noticed when we do the trouble shooting session – a, A, b are column name, so you have checked the first row as header your source connection. Please confirm it’s alright and you want to use a, A, b as column headers. If so, it should be a error because there’s no text- transform for “A” in schema.
<a href=""https://i.stack.imgur.com/kNBVx.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kNBVx.jpg"" alt=""enter image description here"" /></a>
<strong>Hope you can understand the column name doesn’t influence the data transform and it’s okay to change it to make sure no errors block the data flow.</strong>
<strong>There’re two tips for you to remove block, one is change the column name from your source csv directly or you can click the import schema button ( in the blow screen shot) in schema tab, and you can choose a sample file to redefine the schema which also allows you to change column name.</strong>
<a href=""https://i.stack.imgur.com/6hSkX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6hSkX.jpg"" alt=""enter image description here"" /></a></p>
<p>Hope this helps.</p>
"
"57508318","How to use dynamic date in mapping of a REST dataset to SQL dataset","<p>I get some data from Elasticsearch (via REST Dataset) and add it to a SQL Server Table. I want to add the acutal date via utcnow() to the datasets written to the SQL DB.</p>

<p>I´ve tried to add with @formatDateTime(adddays(utcnow(),-2), 'yyyy-MM-dd') in the field but this leads to ""null"" in the target database.</p>
","<azure-data-factory>","2019-08-15 10:30:56","39","1","1","57554286","<p>Have to say that your syntax for <code>formatDateTime</code> is correct. Per my understanding of your requirements,you want to add one more additional column (named nowDate)which is not involved by source dataset.I'm afraid that it is error condition which is listed <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping"" rel=""nofollow noreferrer"">here</a>:</p>

<ul>
<li>Source data store query result does not have a column name that is
specified in the input dataset ""structure"" section.</li>
<li>Sink data store (if with pre-defined schema) does not have a column
name that is specified in the output dataset ""structure"" section.</li>
<li>Either fewer columns or more columns in the ""structure"" of sink
dataset than specified in the mapping.</li>
<li>Duplicate mapping.</li>
</ul>

<p>However, since your sink is sql server,you could add column in stored procedure which could be executed in the copy activity.Please refer to this guide:<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#invoke-a-stored-procedure-from-a-sql-sink"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#invoke-a-stored-procedure-from-a-sql-sink</a></p>
"
"57501939","xp_xmdshell azure data factory","<p>I am wondering if I use SQL Server stored procedure inside ""Execute SQL Task"" in SSIS and this stored procedure uses <code>xp_cmdshell</code>. Now if I want to create Azure Data Factory pipeline for the same task, how do I achieve that?</p>

<p>Also I have some C# readonly and readwrite variables inside my SSIS, How do I convert them in ADF?</p>
","<azure><ssis><azure-data-factory>","2019-08-14 20:56:40","106","1","1","57704858","<blockquote>
  <p>Microsoft says you can use Custom tasks and achieve anything but how??</p>
</blockquote>

<p>For you situation,maybe you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure"" rel=""nofollow noreferrer"">Stored Procedure Activity</a> to run your SSIS package.Please see this link: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-invoke-ssis-package-stored-procedure-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-invoke-ssis-package-stored-procedure-activity</a></p>

<p>For that,you need to configure <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-invoke-ssis-package-stored-procedure-activity#create-an-azure-ssis-integration-runtime"" rel=""nofollow noreferrer"">Azure-SSIS integration runtime</a>,not default Azure IR.</p>
"
"57490447","Move Files from one directory to another using SSIS Package in Azure Data Lake Store Gen1","<p>I am creating SSIS package and planning to use ADF to run it. </p>

<p>I am using Azure Data Lake Gen1 as File Store.</p>

<p>And as per our process once a file load completed we will move the file from one directory to another into Data Lake.</p>

<p>But not able to find anything in SSIS to do it. Anyone have any idea about it.</p>

<p>Your help is highly appreciated.</p>
","<sql-server><azure><ssis><azure-data-factory><azure-data-lake>","2019-08-14 07:55:51","295","0","1","57503972","<p>As you said in comment, you will deploy the SSIS package in ADF using (Configure SSIS Integration).</p>

<p>You can reference this document to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-deploy-ssis-packages-azure#provision-an-azure-ssis-integration-runtime"" rel=""nofollow noreferrer"">Provision the Azure-SSIS Integration Runtime in Azure Data Factory</a>.</p>

<p>This tutorial provides steps for using the Azure portal to provision an Azure-SQL Server Integration Services (SSIS) Integration Runtime (IR) in Azure Data Factory (ADF). Azure-SSIS IR supports running packages deployed into SSIS catalog (SSISDB) hosted by Azure SQL Database server/Managed Instance (Project Deployment Model) and those deployed into file systems/file shares/Azure Files (Package Deployment Model). Once Azure-SSIS IR is provisioned, you can then use familiar tools, such as SQL Server Data Tools (SSDT)/SQL Server Management Studio (SSMS), and command line utilities, such as dtinstall/dtutil/dtexec, to deploy and run your packages in Azure.</p>

<ol>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-deploy-ssis-packages-azure#create-an-azure-ssis-integration-runtime"" rel=""nofollow noreferrer"">Create an Azure-SSIS integration runtime</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-deploy-ssis-packages-azure#provision-an-azure-ssis-integration-runtime"" rel=""nofollow noreferrer"">Provision an Azure-SSIS integration runtime</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-deploy-ssis-packages-azure#deploy-ssis-packages"" rel=""nofollow noreferrer"">Deploy SSIS packages</a></li>
</ol>

<p>After you have created and configured the Azure-SSIS integration runtime, about how to run your SSIS package in Data Factory, Data Factory also give us so many ways:</p>

<ol>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-invoke-ssis-package-ssdt"" rel=""nofollow noreferrer"">Execute SSIS packages in Azure from SSDT</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-invoke-ssis-package-ssis-activity"" rel=""nofollow noreferrer"">Run an SSIS package with the Execute SSIS Package activity in Azure
Data Factory</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-invoke-ssis-package-stored-procedure-activity"" rel=""nofollow noreferrer"">Run an SSIS package with the Stored Procedure activity in Azure Data
Factory</a></li>
</ol>

<p>Just choose the one which you like.</p>

<p>Hope this helps.</p>
"
"57489799","How to pull Email Attachments using Azure data factory copy activity?","<p>We have one scenario where in we would like to pull email attachments to Azure storage. Thinking Azure Data Factory (V2) as means how we can achieve this? Is there any connector available with which we can directly connect to Outlook and fetch the email attachments?</p>

<p>Thanks.</p>
","<azure><outlook><azure-data-factory>","2019-08-14 07:13:30","4639","1","1","57504435","<p>ADF supports <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-office-365#approving-new-data-access-requests"" rel=""nofollow noreferrer"">office 365 connector</a>.</p>

<p><a href=""https://i.stack.imgur.com/IPMhl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IPMhl.png"" alt=""enter image description here""></a></p>

<p>You could follow above link to create linkedService and DateSet.</p>

<p><a href=""https://i.stack.imgur.com/9YNI7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9YNI7.png"" alt=""enter image description here""></a></p>

<p>When you create Linkedservice,you need to grant Mail Read permission to sp id and secret.</p>

<p><a href=""https://i.stack.imgur.com/YQJbX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YQJbX.png"" alt=""enter image description here""></a></p>

<p>In dataset,you could define below properties in the structure json.</p>

<p><a href=""https://i.stack.imgur.com/qE9Sl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qE9Sl.png"" alt=""enter image description here""></a></p>
"
"57489453","ADFV2, VNET, and Linked Services","<p>I have an ADFv2, ADLS Gen2 and SQL Database all configured on a VNET. I'd like to connect from the data factory to the storage account and SQL database however I get a permission error on the data factory (see below). </p>

<p>I know I can stand up an integration runtime to solve the connectivity however, is there any way to configure the linked service to avoid the Virtual machine and IR config yet?</p>

<p>Thanks </p>

<p><a href=""https://i.stack.imgur.com/d2yqH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/d2yqH.png"" alt=""ADF Error""></a></p>
","<azure-sql-database><azure-data-lake><azure-data-factory>","2019-08-14 06:47:35","236","0","1","57521305","<p>I'm afraid that the error is not related to the IR,to the Permission instead.</p>

<p>You could refer to this link <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage#service-principal-authentication"" rel=""nofollow noreferrer"">Service principal authentication</a> to grant permission for your ADF to accessing ADLS. If ADLS is source, the READ permission is needed at least,i think.</p>

<p><a href=""https://i.stack.imgur.com/WqUVR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WqUVR.png"" alt=""enter image description here""></a></p>
"
"57482432","How to avoid line breaks if content contains \n using ADF?","<p>I have a ADF job which copies data from sql to csv but there is a column which contains free text and can contain tabs, new lines etc. When I run a job it breaks the column values at \n.</p>

<p>Is there any way to fix it?</p>

<p><a href=""https://i.stack.imgur.com/CI2T5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CI2T5.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2019-08-13 17:11:48","1331","0","1","57515597","<p>I think you can use a SQL Server function in your query to eliminate all the \n in this field. Try a query like this for example, considering that [Field3] is the one troubling you:</p>

<pre><code>select [Field1], [Field2], replace([Field3], '\n', ' ') as [Field3] from yourTable
</code></pre>

<p>This way, you are replacing all the \n with spaces and it won't break your csv data. Of course the replace is just an example, and your solution will depend on you specific use case.</p>

<p>Hope this helped!</p>
"
"57463310","Sink stored procedure table name is required","<p>I am copying data from a SQL table to another one. The sink has a stored procedure which in which a SQL Merge is done. The stored procedure has been tested directly and I can confirm that it works. I also have several different copy-activities with the same logic. </p>

<p>However, for 2 of them, I keep getting the following <code>Factory Validation Output</code> error.</p>

<blockquote>
  <p>Sink stored procedure table name is required</p>
</blockquote>

<p>Any idea where this might come from? </p>

<p>Here the type and procedure:</p>

<pre class=""lang-sql prettyprint-override""><code>-- create type
create type [sta].[my_type] as table (
    [column1] [nvarchar](255) null,
    [column2] [nvarchar](255) null,
    [column3] [nvarchar](255) null,
    [column4] [nvarchar](255) null,
    [column5] [nvarchar](255) null,
    [column6] [nvarchar](255) null
)
GO

-- create procedure
create procedure [cdw].[sp_load_table] 
    @mytable [sta].[my_type] readonly
as
begin
    merge [cdw].[mytable] as target
    -- handle duplicates
    using (select distinct * from @mytable) as source
    on (target.[column1] = source.[column1]
        and target.[column2] = source.[column2]
        and target.[column3] = source.[column3]
        and target.[column4] = source.[column4]
        and target.[column5] = source.[column5]
        and target.[column6] = source.[column6])

    when matched and (
           COALESCE(target.[column1], 1) &lt;&gt; COALESCE(source.[column1], 1)
        or COALESCE(target.[column2], 1) &lt;&gt; COALESCE(source.[column2], 1)
        or COALESCE(target.[column3], 1) &lt;&gt; COALESCE(source.[column3], 1)
        or COALESCE(target.[column4], 1) &lt;&gt; COALESCE(source.[column4], 1)
        or COALESCE(target.[column5], 1) &lt;&gt; COALESCE(source.[column5], 1)
        or COALESCE(target.[column6], 1) &lt;&gt; COALESCE(source.[column6], 1)) then

        update 
            set target.[column1] = source.[column1],
                target.[column2] = source.[column2],
                target.[column3] = source.[column3],
                target.[column4] = source.[column4],
                target.[column5] = source.[column5],
                target.[column6] = source.[column6]

    when not matched by target
       then 
          insert ([column1], [column2], [column3], [column4], [column5], [column6])
          values (source.[column1], source.[column2], source.[column3],
                  source.[column4], source.[column5], source.[column6])

    when not matched by source
        then delete;     
end
</code></pre>

<p>And here the ADF activity:</p>

<p><img src=""https://user-images.githubusercontent.com/12760630/62873306-740aa000-bd1f-11e9-9c09-709af6367411.png"" alt=""image""></p>
","<azure-sql-database><azure-data-factory>","2019-08-12 14:39:58","929","0","2","57471910","<p>Maybe you could try to adjust the value of Table type, replace <code>[sta].[my_type]</code> with <code>my_type</code>. </p>

<p>Please see my previous case:<a href=""https://stackoverflow.com/questions/51352725/azure-data-factory-mapping-2-columns-in-one-column/51354266#51354266,please"">Azure Data Factory mapping 2 columns in one column</a> or <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#invoke-a-stored-procedure-from-a-sql-sink"" rel=""nofollow noreferrer"">official example</a>. </p>

<p><a href=""https://i.stack.imgur.com/ZkvZa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZkvZa.png"" alt=""enter image description here""></a></p>
"
"57463310","Sink stored procedure table name is required","<p>I am copying data from a SQL table to another one. The sink has a stored procedure which in which a SQL Merge is done. The stored procedure has been tested directly and I can confirm that it works. I also have several different copy-activities with the same logic. </p>

<p>However, for 2 of them, I keep getting the following <code>Factory Validation Output</code> error.</p>

<blockquote>
  <p>Sink stored procedure table name is required</p>
</blockquote>

<p>Any idea where this might come from? </p>

<p>Here the type and procedure:</p>

<pre class=""lang-sql prettyprint-override""><code>-- create type
create type [sta].[my_type] as table (
    [column1] [nvarchar](255) null,
    [column2] [nvarchar](255) null,
    [column3] [nvarchar](255) null,
    [column4] [nvarchar](255) null,
    [column5] [nvarchar](255) null,
    [column6] [nvarchar](255) null
)
GO

-- create procedure
create procedure [cdw].[sp_load_table] 
    @mytable [sta].[my_type] readonly
as
begin
    merge [cdw].[mytable] as target
    -- handle duplicates
    using (select distinct * from @mytable) as source
    on (target.[column1] = source.[column1]
        and target.[column2] = source.[column2]
        and target.[column3] = source.[column3]
        and target.[column4] = source.[column4]
        and target.[column5] = source.[column5]
        and target.[column6] = source.[column6])

    when matched and (
           COALESCE(target.[column1], 1) &lt;&gt; COALESCE(source.[column1], 1)
        or COALESCE(target.[column2], 1) &lt;&gt; COALESCE(source.[column2], 1)
        or COALESCE(target.[column3], 1) &lt;&gt; COALESCE(source.[column3], 1)
        or COALESCE(target.[column4], 1) &lt;&gt; COALESCE(source.[column4], 1)
        or COALESCE(target.[column5], 1) &lt;&gt; COALESCE(source.[column5], 1)
        or COALESCE(target.[column6], 1) &lt;&gt; COALESCE(source.[column6], 1)) then

        update 
            set target.[column1] = source.[column1],
                target.[column2] = source.[column2],
                target.[column3] = source.[column3],
                target.[column4] = source.[column4],
                target.[column5] = source.[column5],
                target.[column6] = source.[column6]

    when not matched by target
       then 
          insert ([column1], [column2], [column3], [column4], [column5], [column6])
          values (source.[column1], source.[column2], source.[column3],
                  source.[column4], source.[column5], source.[column6])

    when not matched by source
        then delete;     
end
</code></pre>

<p>And here the ADF activity:</p>

<p><img src=""https://user-images.githubusercontent.com/12760630/62873306-740aa000-bd1f-11e9-9c09-709af6367411.png"" alt=""image""></p>
","<azure-sql-database><azure-data-factory>","2019-08-12 14:39:58","929","0","2","57473050","<p>I have started a discussion on github regarding the same problem as well. It seems to be a bug at the moment. Following workaround worked for me.</p>

<p>Follow the discussion on Github for further info: </p>

<p>Github link: github.com/MicrosoftDocs/azure-docs/issues/36916</p>

<p><a href=""https://i.stack.imgur.com/h7eAy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/h7eAy.png"" alt=""enter image description here""></a></p>

<p>The problem has now been fixed by the Microsoft Product team. It was a bug.</p>
"
"57462093","Getting Datas from API A, B, C and Pushing them to API X","<p>I've been tinkering a bit with Azure Data bricks and Azure data factory, and I was wondering if these were overkill when you just want to take datas from a few Services by calling these services API (Rest), and then pushing these datas to a Service X using a Rest API as well.</p>

<p>I've seen that the Data factory can call a rest API (but does not seem to support Oauth2), but I have not found anything about pushing the datas to another rest Api (And it doesnt really makes sens to consolidate everything in an Azure DB at this point)</p>

<p>So, should I use the data factory, or writing a python script and calling it with Azure Functions would be more suited to solve this problem ?</p>

<p>Regards,</p>
","<azure-functions><azure-data-factory>","2019-08-12 13:24:44","63","3","1","57462932","<p>I prefer to use DataFactory only when the data can be moved quite straightforward from one DB (or file) to another DB. When you need more logic to interact with an API such as in your case there is little additional value in using DataFactory since you already need some compute (such as Azure Functions).</p>

<p>If your ingestion of data is some sort of workflow (e.g. you need A, B and C in a specific order before you can push to X) then I can recommend using <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-overview"" rel=""nofollow noreferrer"">Durable Functions</a> with function chaining and fan-out/fan-in patterns. I also have some <a href=""https://www.youtube.com/watch?v=ARhgG7OeoX0&amp;t=586s"" rel=""nofollow noreferrer"">video content</a> where I explain this.</p>

<p>Here a small pseudo code example of an orchestration using function chaining:</p>

<pre><code>var id = input.GetInput&lt;string&gt;();
var a = await context.CallActivityAsync&lt;DataA&gt;(""CallApiAActivity"", id);
var b = await context.CallActivityAsync&lt;DataB&gt;(""CallApiBActivity"", id);
var c = await context.CallActivityAsync&lt;DataC&gt;(""CallApiCActivity"", id);
var combined = Builder.CombineABC(a, b, c);

return  await context.CallActivityAsync&lt;bool&gt;(""PushToApiXActivity"", combined);
</code></pre>

<p>If the calls to A, B and C are unrelated then I suggest you do a fan-out/fan-in so these functions can be called in parallel.</p>

<p>The benefit of Durable Functions is that the orchestration is stateful and does checkpointing to storage for each activity function call. This enables automatic retries by using the <code>CallActivityWithRetryAsync</code> method. This is especially useful when dealing with services out of your control and makes your solution more resilient.</p>
"
"57460346","Azure Databricks terminate the cluster from DAta factory","<p>I have a use case of running a spark job everyday. I am using databricks to run the job. Since it is a daily job, I would like to create a cluster, run the notebook and destroy the cluster. I am using data factory to do that. But I am not seeing any option to customise the ""Inactivity period"" in data factory when creating the linked service of databricks on creating the cluster. </p>

<p>How can I destroy the cluster once my job is completed?</p>
","<azure><azure-data-factory><azure-databricks>","2019-08-12 11:28:33","1326","2","1","57467064","<p>Just choose ""new job cluster"". Job clusters are only active during the job lifetime.</p>
"
"57447907","Continuous running job in Azure","<p>I am have created a simple Python job to watch a remote SFTP directory for any new files dropped in there by an external system. The task is to basically read the new files dropped into the directory in near real-time basis. The job is currently scheduled in Task Scheduler in a Windows Virtual Machine but I would like to move it to any PaaS Service in Azure to schedule this job to run this continuously but not on batch mode. Can someone let me know if any services in Azure that supports this use case?</p>
","<python-3.x><azure><azure-data-factory><azure-databricks>","2019-08-11 06:46:57","551","1","1","57470475","<p>You could get an idea of <a href=""https://learn.microsoft.com/en-us/azure/connectors/connectors-create-api-sftp"" rel=""nofollow noreferrer"">Azure Logic App and SFTP connector</a> which supports monitor, create, send, and receive files on a SFTP server.</p>

<p>SFTP triggers work by polling the SFTP file system and looking for any file that was changed since the last poll.More details,please see this <a href=""https://learn.microsoft.com/en-us/azure/connectors/connectors-create-api-sftp#how-sftp-triggers-work"" rel=""nofollow noreferrer"">link</a>.</p>

<p>BTW,the SFTP connector has limitation that it handles only files that are 50 MB or smaller and doesn't support message chunking.For larger files, you have to use the <a href=""https://learn.microsoft.com/en-us/azure/connectors/connectors-sftp-ssh"" rel=""nofollow noreferrer"">SFTP-SSH connector</a>.</p>
"
"57437259","Microsoft BotFramework Transcript Logging Timestamp Format","<p>I'm using Microsoft BotFramework and logging all the transcripts from the chatbot into an Azure blob storage container.</p>

<p>The timestamps on each message is of a format I can't recognise so I'm having to store them as strings for now. I'd like to transform and store them as a proper DateTime format using Azure Data Factory so I can improve my chatbot's analytics.</p>

<p>The timestamps look as follows:</p>

<pre><code>2019-08-07T09:19:45.342Z
2019-08-07T09:19:39.582Z
2019-08-07T09:19:57.637Z
</code></pre>

<p>What DateTime format are they?</p>
","<timestamp><botframework><azure-blob-storage><azure-data-factory>","2019-08-09 21:22:46","128","0","1","57437749","<p>The format is UTC also called Coordinated Universal Time and follows the International Standard for the representation of dates and times as specified in ISO 8601. You can read more about it from the W3 website <a href=""https://www.w3.org/TR/NOTE-datetime"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Hope of help!</p>
"
"57435705","Cannot Write to Access database from azure datafactory","<p>I'm trying to export data from an Azure SQL Database to an MS Access database using Data Factory, ODBC, and ADF's Integration Runtime.  I’ve been able to register the destination Access database in ADF using the documented process of setting up a “self-hosted” Integration Runtime.  The actual connection from ADF to the Access database works as expected – i.e., I can successfully “Test Connection” and query tables that reside in the local Access database.  I can even move data FROM the Access db to the Azure SQL Database.  However, our requirement is to move data OUT of our ASQLDB and into the Access db. Which is where the problem occurs.  I cannot write data from our ASQLDB to the Access database, and receive the following error:</p>

<pre><code>{ ""errorCode"": ""2200"", ""message"": ""Failure happened on 'Sink' side. ErrorCode=UserErrorOdbcOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR [IM001] [Microsoft][ODBC Driver Manager] Driver does not support this function,Source=Microsoft.DataTransfer.ClientLibrary.Odbc.OdbcConnector,''Type=Microsoft.DataTransfer.ClientLibrary.Odbc.Exceptions.OdbcException,Message=ERROR [IM001] [Microsoft][ODBC Driver Manager] Driver does not support this function,Source=ACEODBC.DLL,'"", ""failureType"": ""UserError"", ""target"": ""Assemblies_ASQL_to_MSA_VM"" }
</code></pre>

<p>I’ve scoured Google, but have not been able to find someone who’s experiencing the same/similar issue trying to get data from ASQLDB to Access.  I’ve also tried tweaking settings (i.e., connection strings, folders, shares, etc.) as many ways as I could possibly see to do, but to no avail.  Bottom line, the connectivity to the Access database works as expected, but I absolutely cannot write data to the Access database from ADF.  Assistance would be greatly appreciated.</p>
","<ms-access><odbc><azure-sql-database><azure-data-factory>","2019-08-09 18:51:15","308","2","1","57458401","<p>jjones64. I have to say that Access DB is not supported as a sink dataset in the ADF,please see this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats"" rel=""nofollow noreferrer"">support list</a>:</p>

<p><a href=""https://i.stack.imgur.com/Enqq3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Enqq3.png"" alt=""enter image description here""></a></p>

<p>My advice is you could transfer to sql db data into on-prem csv file with copy activity, then load csv file into Access DB following this tutorial:<a href=""https://blog.ip2location.com/knowledge-base/how-to-import-csv-into-microsoft-access-database/"" rel=""nofollow noreferrer"">https://blog.ip2location.com/knowledge-base/how-to-import-csv-into-microsoft-access-database/</a></p>
"
"57435025","How to get/set parameterized connection string from Key Vault in Azure Data Factory?","<p>I  have a parameterized connection string in Azure Data Factory Linked Services as below:</p>

<blockquote>
  <p>Integrated
  Security=False;Encrypt=True;Connection Timeout=30;Data
  Source=xyz;User ID=admin;Password=password;Initial
  Catalog=<strong>@{linkedService().LSDBName}</strong>;</p>
</blockquote>

<p>The value for database is passed from a pipeline variable at runtime.</p>

<p>I want to save this connection string to Azure Key Vault, but the issue is that after the value is read from the key vault, the linked service parameter ""LSDBName"" is not dynamically replaced by the actual value and it tries to connect to ""@{linkedService().LSDBName}"" as the database name. </p>

<p>Is there any way to secure a dynamically parameterized connection string in key vault? Or a workaround to achieve this?</p>

<p>Thanks!</p>
","<azure><azure-keyvault><azure-data-factory>","2019-08-09 17:52:20","1910","2","2","57455310","<p>My idea is using Set Variable Activity+Azure Function Activity.</p>

<p>First step is using Set Variable Activity to get the LinkedService Connection String.</p>

<p>Second Step is passing the variable as parameter into Azure Function Activity. Then use AKV sdk to store the connection string value in the azure function inside.</p>

<p>Incidentally, I think your connection string has been parameterized already, security issues have been avoided. You don't have to store it into AKV again because mostly we read private information from AKV, rather than write store information into AKV in ADF. Just my own opinion.</p>

<p><a href=""https://i.stack.imgur.com/5P4Kz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5P4Kz.png"" alt=""enter image description here""></a></p>
"
"57435025","How to get/set parameterized connection string from Key Vault in Azure Data Factory?","<p>I  have a parameterized connection string in Azure Data Factory Linked Services as below:</p>

<blockquote>
  <p>Integrated
  Security=False;Encrypt=True;Connection Timeout=30;Data
  Source=xyz;User ID=admin;Password=password;Initial
  Catalog=<strong>@{linkedService().LSDBName}</strong>;</p>
</blockquote>

<p>The value for database is passed from a pipeline variable at runtime.</p>

<p>I want to save this connection string to Azure Key Vault, but the issue is that after the value is read from the key vault, the linked service parameter ""LSDBName"" is not dynamically replaced by the actual value and it tries to connect to ""@{linkedService().LSDBName}"" as the database name. </p>

<p>Is there any way to secure a dynamically parameterized connection string in key vault? Or a workaround to achieve this?</p>

<p>Thanks!</p>
","<azure><azure-keyvault><azure-data-factory>","2019-08-09 17:52:20","1910","2","2","64139216","<p>If you want to store the entire connection string in key vault then you have to pass the connection string in &quot;Server=myServerAddress;Database=myDataBase;User Id=myUsername;Password=myPassword;&quot; format. Create separate connection string for each database and store it in key vault with different secrets and then create parameterized linked service in ADF, giving these secrets as parameter.</p>
"
"57428487","Azure Data Factory foreach activity step size support","<p>I have a pipeline that contains a list of IDs as input and I need to iterate through these IDs and call a REST API using batches of 10 IDs per time (these IDs will be passed as a parameter into JSON request).</p>

<p>1) Is there any approach using forEach activity in Data Factory passing the step size? </p>

<p>2) Do you have any other suggestions of how to accomplish this?</p>

<p>I have tried using ""forEach"" loop and also thinking in a way to use ""setVariable"" and ""appendVariable"" activities to store the current index during the loop, but also couldn't find a way to get the current index during the ""forEach"".</p>
","<azure><azure-data-factory>","2019-08-09 10:48:41","559","1","1","57428692","<p>You should use a <code>LookupActivity</code>. With that you can get information from database, files or whatever and them pass it to a <code>ForEach</code> Loop.</p>

<p>Consider I have the following information in my txt file:</p>

<pre><code>name|age
orochiBrabo|25
NarutoBoy|98
</code></pre>

<p>You can recover it using <code>LookupActivity</code> which I will call <code>MyLookUp</code> and then connect it box with a <code>ForEach</code> Box. </p>

<p>In <code>ForEach</code> Activity setting tab you write <code>@activity('MyLookUp').output.value</code> and now you can iterate over all rows in the file. Inside your ForEach you can refer results like <code>item().age</code> , <code>item().name</code> or <code>item().myColumnName</code>.</p>
"
"57421431","How can I create a one time pipeline run with a long delay in c# data factory","<p>I need to trigger a pipeline I have built inside of my azure data factory with certain parameters based off of a file I have stored in a database. My problem is that I need to schedule this pipeline to trigger ONCE after a certain amount of time( will usually be hours). This is needed for scheduling and I can't do it event driven. I am using the .NET SDK</p>

<p>I have already created a connection to my data factory and created a schedule trigger. My problem is that a schedule trigger doesn't allow me to trigger one time and then stopping. It requires intervals and a stop date, I tried to set the stop date the same as the start date but it gives me the error of ""interval cannot exceed end date"". </p>

<pre><code> for (int x = 0; x &lt; intervals.Count; x++)
            {
                // Create a schedule trigger
                string triggerName = location + deliveryDate+x;
                ScheduleTrigger myTrigger = new ScheduleTrigger()
                {

                    Pipelines = new List&lt;TriggerPipelineReference&gt;()
                    {
                        // Associate the Adfv2QuickStartPipeline pipeline with the trigger
                        new TriggerPipelineReference()
                        {
                            PipelineReference = new PipelineReference(pipelineName),
                            Parameters = pipelineParameters,
                        }
                    },
                    Recurrence = new ScheduleTriggerRecurrence()
                    {
                        StartTime = intervals[x],
                        TimeZone = ""UTC"",
                        EndTime = intervals[x],
                        Frequency = RecurrenceFrequency.Day

                    }





                };

                // Now, create the trigger by invoking the CreateOrUpdate method
                triggerResources.Add(triggerName,new TriggerResource()
                {
                    Properties = myTrigger
                });
            }
</code></pre>

<p>I cannot do a pipeline run because there is not way for me to do a run after a certain delay (like 2 hours) if this was possible I would just create a delayed pipeline run...I have tried everything like leaving the frequency blank, changing it to every possibility, and even using different trigger classes like tumbling and event.</p>
","<c#><azure><azure-data-factory>","2019-08-08 22:31:52","660","0","1","57437291","<p>There is a simple, crude solution.  Create a new pipeline with a parameter of integer type.  The first activity in the pipeline will be a Wait Activity.  Use the parameter to set how long the Wait Activity should last.  The Second activity in the pipeline will be an Execute Pipeline Activity, which depends on the Wait Activity, and will trigger the pipeline you really want to run.</p>

<p>This solution lets you choose how long to wait, then execute the real pipeline you want to run.  The Wait Activity is in seconds, I think, so you will need to do some arithmetic.  However since you can trigger manually, it shouldn't be a problem.</p>
"
"57408157","read duplicate metrics in extract schema in U-SQL","<pre><code>` @input =EXTRACT firstname string,name string,name string FROM ""/table1.csv""
USING Extractors.Csv(quoting : false, silent : true);
@output =SELECT * FROM @input;

OUTPUT @output TO ""/data_output.csv"" USING Outputters.Csv(quoting : false); `
</code></pre>

<p>extract schema contains duplicate metrics (NAME)</p>

<p>How can we read duplicate metrics ?</p>
","<analytics><azure-data-lake><u-sql><azure-data-factory>","2019-08-08 08:18:02","62","1","1","57425971","<p>U-SQL reads columns by position in EXTRACT statement and not by name, so you can call your columns, for example, Name1 and Name2 (or something more logical to your business domain).</p>

<pre><code>@input =EXTRACT firstname string,name1 string,name2 string FROM ""/table1.csv"" 
USING Extractors.Csv(quoting : false);
</code></pre>
"
"57405121","PARAMETERIS Parameterise Linked Service in Azure Data Factory for varied Source","<p>I want to create a parametrised Linked Service in Azure Data Factory. 
Source system includes the following :
a) SFTP Server
b) SAP Hana
c) SFDC
d) Oracle
Sink includes the following:
a) ADLS (gen2)
b) Azure SQL DB/DWH
c) Azure Analysis service.</p>

<p>I want to store the configuration details like Server Name, Port, User/Pwd in a text file and this will be read for fetching the details of each source and sink.</p>

<p>Can you please help me with this. If this can be achieved or we need to have this manually each time.</p>

<p>I want to create a parametrised Linked Service in Azure Data Factory. 
Source system includes the following :
a) SFTP Server
b) SAP Hana
c) SFDC
d) Oracle
Sink includes the following:
a) ADLS (gen2)
b) Azure SQL DB/DWH
c) Azure Analysis service.</p>

<p>I want to store the configuration details like Server Name, Port, User/Pwd in a text file and this will be read for fetching the details of each source and sink.</p>

<p>Can you please help me with this. If this can be achieved or we need to have this manually each time.</p>
","<azure><azure-data-factory>","2019-08-08 04:15:17","85","0","1","57422134","<p>Keeping your credentials in a text file is a <em>very risky</em> idea in terms of security.  May I suggest using Azure Key Vault?  This is a <a href=""https://learn.microsoft.com/azure/data-factory/store-credentials-in-key-vault"" rel=""nofollow noreferrer"">recommended way to store and reference your credentials for use in Azure Data Factory V2</a>.  Even if the UI does not display Azure Key Vault as an option, you can still <a href=""https://learn.microsoft.com/azure/data-factory/parameterize-linked-services#json"" rel=""nofollow noreferrer"">utilize it by altering the JSON</a>.</p>

<p>If you still prefer to use a text file, it is possible, but unwieldy.  You would use a Lookup activity on your text file, and then place the output into an array type variable.  I reccomend you write your text file in JSON format, in order to leverage the string-to-JSON-object function in a dynamic expression to transform the contents into a dictionary.</p>
"
"57402746","What is the easiest way to pull data from a blob and load it into a table in SQL Server?","<p>I have hundreds of zipped files sitting in different folders, which I can access using MS Storage Explorer.  I just setup a SQL Server DB in Azure.  Now I am trying to figure out how I can pull data from each file in each folder, unzip it, parse it, and load it into tables.  The data is coming in daily, so the folders are named '1', '2', '3', etc. '31', for the days of the month.  Also, I have monthly folders '1' through '12', for the 12 months of the year.  Finally, I have folders named '2017', '2018', and '2019'.  I could ask some people on my team to write Python code to do the work, but it seems like there should be an easier way.  I would like to avoid writing thousands of lines of Python code, if there is a simpler way to do this kind of thing.  TIA.</p>
","<python><azure><azure-data-factory><databricks>","2019-08-07 21:54:06","936","1","1","57403063","<p>I would create Azure Logic App that would</p>

<ol>
<li>HTTP trigger (this will be manually ran)</li>
<li>Iterate through all blobs present on storage account</li>
<li>For each element use blob connector which has action called Extract archive to extract zipped files into blob</li>
<li>Use Azure SQL BCP to pull this, if this is different format than CSV then use Azure Data Factory</li>
</ol>

<p>Later if you expect new blobs do similar flow except use New or modified blob trigger. </p>

<p>For Azure SQL BCP from BLOB example with Logic Apps check this article <a href=""https://marczak.io/posts/azure-loading-csv-to-sql/"" rel=""nofollow noreferrer"">https://marczak.io/posts/azure-loading-csv-to-sql/</a></p>

<p>And if you want general intro into Azure Logic Apps check this video <a href=""https://youtu.be/ZvsOzji_8ow"" rel=""nofollow noreferrer"">https://youtu.be/ZvsOzji_8ow</a> In here there is also new blob trigger demo.</p>

<p>In case you need data factory feel free to check this video. It has blob to sql demo too. Also you can trigger data factory from logic apps too.
<a href=""https://youtu.be/EpDkxTHAhOs"" rel=""nofollow noreferrer"">https://youtu.be/EpDkxTHAhOs</a></p>
"
"57402245","Tabulator - Fetching JSON file with ajax","<p>I'm trying to ingest a json file that is stored in an Azure blob container, having been run through the Azure Data Factory.</p>

<p>I've been struggling with getting the ajax request to work, despite following the documentation and reading lots of Stack Overflow questions about it. </p>

<p>I have already worked through various CORS issues to get access to the json file. Now that I've fixed the CORS issues I can access the json file but I now get a Data Loading Error:</p>

<pre><code>Data Loading Error - Unable to process data due to invalid data type
Expecting: array 
Received:  undefined 
Data:      undefined
</code></pre>

<p>I originally thought that the issue was with a BOM that was appended to the start of the json file, just before the json array, as I was seeing it appended before the array in the preview of the file in Chrome when it was loaded.</p>

<p>However, I've now manually removed the BOM and re-uploaded the file to my blob storage and I'm still getting the same error.</p>

<p>Below is my Tabulator code that is being loaded in an html file:</p>

<pre><code>var table = new Tabulator(""#example-table"", {
    ajaxConfig:{
        method: 'GET',
        mode: 'cors'
    },
    ajaxURL:""https://mihndbotblob.blob.core.windows.net/mihndbot-transcripts/finalTranscripts/2019-08-07.json"", //ajax URL
    index:""MessageID"",
    autoResize:true,
    layout:""fitData"", //layout options
    placeholder:""Awaiting Data..."",
    columns:[ //Define Table Columns
        {title:""Type"", field:""Type"", visible:false},
        {title:""Message ID"", field:""MessageID""},
        {title:""Message Time"", field:""MessageTime""},
        {title:""Channel"", field:""MessageChannel""},
        {title:""Sender ID"", field:""MessageSenderID""},
        {title:""Sender Name"", field:""MessageSenderName""},
        {title:""Conversation ID"", field:""ConversationID""},
        {title:""Message"", field:""MessageText""},
        {title:""Quick Reply Display Text"", field:""QuickReplyDisplayText""},
        {title:""Quick Reply Payload"", field:""QuickReplyPayload""},
        {title:""QnA Questions"", field:""Question""},
        {title:""Answer"", field:""Answer"", width:400, formatter:""textarea"", variableHeight:true},
        {title:""Prompts"", field:""FollowUpPrompts"", mutator:promptsMutator, width:500, formatter:""textarea"", variableHeight:true},
    ],

    ajaxResponse:function(url, params, response){
        //url - the URL of the request
        //params - the parameters passed with the request
        //response - the JSON object returned in the body of the response.

        return response.data; //pass the data array into Tabulator
    },

    rowClick:function(e, row){ //trigger an alert message when the row is clicked
    alert(""Row "" + row.getData().id + "" Clicked!!!!"");
    },
});
</code></pre>

<p>Tabulator works absolutely fine when I load the json data in manually:</p>

<pre><code>var tableData = [
    {""Type"":""trace"",""MessageID"":""aHEeHB0t0hHr7175E07zF8kfVjGLBRMdzD_oYUEHNE71Va3wN0yJv9aBAzkOk6JMZvTBRMzBNCUjCvTRuxrAVA"",""MessageTime"":""2019-08-07T09:19:45.342Z"",""MessageChannel"":""facebook"",""MessageSenderID"":""2826538590752448"",""MessageSenderName"":""Sean Betts"",""ConversationID"":""2826538590752448-530667464425015"",""MessageText"":""Symptoms"",""MessageRecipientID"":""530667464425015"",""QuickReplyDisplayText"":null,""QuickReplyPayload"":null,""Question"":""[\""What are the symptoms of schizophrenia?\""]"",""Answer"":""🙊\nThe symptoms of schizophrenia are usually classified into:\n\n* Positive symptoms – any change in behaviour or thoughts, such as hallucinations or delusions\n* Negative symptoms – a withdrawal or lack of function that you would not usually expect to see in a healthy person; for example, people with schizophrenia often appear emotionless and flat"",""FollowUpPrompts"":""[{\""displayOrder\"":0,\""qnaId\"":150,\""qna\"":null,\""displayText\"":\""Related Issues\""}]""}
    ,{""Type"":""trace"",""MessageID"":""hLgPSyBP-cAPdKYJ-bFHUckfVjGLBRMdzD_oYUEHNE4bJsq-K4GZvHL9m6guIMtShXFRY-XoxTMdZVBLLZBGDQ"",""MessageTime"":""2019-08-07T09:19:39.582Z"",""MessageChannel"":""facebook"",""MessageSenderID"":""2826538590752448"",""MessageSenderName"":""Sean Betts"",""ConversationID"":""2826538590752448-530667464425015"",""MessageText"":""Schizophrenia"",""MessageRecipientID"":""530667464425015"",""QuickReplyDisplayText"":null,""QuickReplyPayload"":null,""Question"":""[\""What is schizophrenia?\""]"",""Answer"":""🤔\nViews on schizophrenia have changed over the years. Lots of people question whether it's really a distinct condition, or actually a few different conditions that overlap. But you may still be given this diagnosis if you experience symptoms such as:\n\n* psychosis (such as hallucinations or delusions)\n\n* disorganised thinking and speech\n\n* feeling disconnected from your feelings\n\n* difficulty concentrating\n\n* wanting to avoid people\n\n* a lack of interest in things\n\n* not wanting to look after yourself.\n\nBecause psychiatric experts disagree about what schizophrenia is, some people argue that this term shouldn't be used at all. Others think the name of the condition doesn't matter, and prefer to just focus on helping you manage your symptoms and meeting your individual needs."",""FollowUpPrompts"":""[{\""displayOrder\"":0,\""qnaId\"":147,\""qna\"":null,\""displayText\"":\""Prevalence\""},{\""displayOrder\"":0,\""qnaId\"":148,\""qna\"":null,\""displayText\"":\""Causes\""},{\""displayOrder\"":0,\""qnaId\"":149,\""qna\"":null,\""displayText\"":\""Symptoms\""},{\""displayOrder\"":0,\""qnaId\"":150,\""qna\"":null,\""displayText\"":\""Related Issues\""}]""}
    ,{""Type"":""trace"",""MessageID"":""893qgqa3cR9"",""MessageTime"":""2019-08-07T09:19:27.853Z"",""MessageChannel"":""facebook"",""MessageSenderID"":""2826538590752448"",""MessageSenderName"":""Sean Betts"",""ConversationID"":""2826538590752448-530667464425015"",""MessageText"":""WHAT_IS_MENTAL_HEALTH"",""MessageRecipientID"":""530667464425015"",""QuickReplyDisplayText"":""Mental Health"",""QuickReplyPayload"":""WHAT_IS_MENTAL_HEALTH"",""Question"":""[\""What is mental health?\""]"",""Answer"":""🤔\nIn many ways, Mental Health is just like Physical Health: Everybody has it and we need to take care of it.\n\nGood Mental Health means being generally able to think, feel and react in the ways that you need and want to live your life. But if you go through a period of poor Mental Health you might find the ways you're frequently thinking, feeling or reacting become difficult, or even impossible, to cope with. This can feel just as bad as a physical illness, or even worse.\n\nMental Health problems affect around one in four people in any given year. They range from common problems, such as depression and anxiety, to rarer problems such as schizophrenia and bipolar disorder."",""FollowUpPrompts"":""[{\""displayOrder\"":0,\""qnaId\"":123,\""qna\"":null,\""displayText\"":\""What Types Are There?\""}]""}
    ,{""Type"":""trace"",""MessageID"":""V2VFDEjUb_w3uoJwMwdHYckfVjGLBRMdzD_oYUEHNE7ufPEJdVsZzedQSKsLniMtd9-f_Ngy-mk-tNd1w3X33w"",""MessageTime"":""2019-08-07T09:19:57.637Z"",""MessageChannel"":""facebook"",""MessageSenderID"":""2826538590752448"",""MessageSenderName"":""Sean Betts"",""ConversationID"":""2826538590752448-530667464425015"",""MessageText"":""Suicide"",""MessageRecipientID"":""530667464425015"",""QuickReplyDisplayText"":null,""QuickReplyPayload"":null,""Question"":""[\""How common is suicide?\""]"",""Answer"":""🤓\nApproximately 6.7% of people have attempted suicide during their lifetime in the UK"",""FollowUpPrompts"":""[{\""displayOrder\"":0,\""qnaId\"":195,\""qna\"":null,\""displayText\"":\""Get Help\""}]""}
    ,{""Type"":""trace"",""MessageID"":""V2VFDEjUb_w3uoJwMwdHYckfVjGLBRMdzD_oYUEHNE7ufPEJdVsZzedQSKsLniMtd9-f_Ngy-mk-tNd1w3X33w"",""MessageTime"":""2019-08-07T09:19:57.637Z"",""MessageChannel"":""facebook"",""MessageSenderID"":""2826538590752448"",""MessageSenderName"":""Sean Betts"",""ConversationID"":""2826538590752448-530667464425015"",""MessageText"":""Suicide"",""MessageRecipientID"":""530667464425015"",""QuickReplyDisplayText"":null,""QuickReplyPayload"":null,""Question"":""[\""How common is suicide?\""]"",""Answer"":""🤓\nApproximately 6.7% of people have attempted suicide during their lifetime in the UK"",""FollowUpPrompts"":""[{\""displayOrder\"":0,\""qnaId\"":195,\""qna\"":null,\""displayText\"":\""Get Help\""}]""}
    ,{""Type"":""trace"",""MessageID"":""RLOUlqwIcO4zSY4zHgWG78kfVjGLBRMdzD_oYUEHNE7gRJSePak9st7_nIIgw3OWXLvAMm4hMXeJ5kFnlIavpQ"",""MessageTime"":""2019-08-07T09:20:04.294Z"",""MessageChannel"":""facebook"",""MessageSenderID"":""2826538590752448"",""MessageSenderName"":""Sean Betts"",""ConversationID"":""2826538590752448-530667464425015"",""MessageText"":""Get Help"",""MessageRecipientID"":""530667464425015"",""QuickReplyDisplayText"":null,""QuickReplyPayload"":null,""Question"":""[\""Help me\""]"",""Answer"":""🆘\nBeing in a crisis can be very frightening. If you are worried about your safety, thinking of self-harm or feeling suicidal please get some help and support ASAP.\n\nWhen you're in a criss due to Mental Health problems or struggling to deal with your NeuroDiversity it can be difficult to know what to do. Everyone is deserving of support and it's vitally important that you get some when in crisis.\n\nPlease follow this link for details of where you can get help:\n\nhttps://www.mihnd.co.uk/crisis"",""FollowUpPrompts"":""[{\""displayOrder\"":0,\""qnaId\"":196,\""qna\"":null,\""displayText\"":\""Help Yourself\""}]""}
    ,{""Type"":""trace"",""MessageID"":""G2tRRm8FFZk"",""MessageTime"":""2019-08-07T09:19:14.723Z"",""MessageChannel"":""facebook"",""MessageSenderID"":""2826538590752448"",""MessageSenderName"":""Sean Betts"",""ConversationID"":""2826538590752448-530667464425015"",""MessageText"":""WHAT_IS_NEURODIVERSITY"",""MessageRecipientID"":""530667464425015"",""QuickReplyDisplayText"":""NeuroDiversity"",""QuickReplyPayload"":""WHAT_IS_NEURODIVERSITY"",""Question"":""[\""What is NeuroDiversity?\""]"",""Answer"":""🤔\nAt its core, NeuroDiversity advocates neurological differences to be recognised and respected in the same way other human variations are. These neurological differences are viewed as a variation of how our the human brain is wired. As such, it rejects the idea that these conditions can be cured and instead celebrates them for bringing diversity to human society and culture"",""FollowUpPrompts"":""[{\""displayOrder\"":0,\""qnaId\"":241,\""qna\"":null,\""displayText\"":\""More Info\""}]""}
    ,{""Type"":""trace"",""MessageID"":""hLgPSyBP-cAPdKYJ-bFHUckfVjGLBRMdzD_oYUEHNE4bJsq-K4GZvHL9m6guIMtShXFRY-XoxTMdZVBLLZBGDQ"",""MessageTime"":""2019-08-07T09:19:39.582Z"",""MessageChannel"":""facebook"",""MessageSenderID"":""2826538590752448"",""MessageSenderName"":""Sean Betts"",""ConversationID"":""2826538590752448-530667464425015"",""MessageText"":""Schizophrenia"",""MessageRecipientID"":""530667464425015"",""QuickReplyDisplayText"":null,""QuickReplyPayload"":null,""Question"":""[\""What is schizophrenia?\""]"",""Answer"":""🤔\nViews on schizophrenia have changed over the years. Lots of people question whether it's really a distinct condition, or actually a few different conditions that overlap. But you may still be given this diagnosis if you experience symptoms such as:\n\n* psychosis (such as hallucinations or delusions)\n\n* disorganised thinking and speech\n\n* feeling disconnected from your feelings\n\n* difficulty concentrating\n\n* wanting to avoid people\n\n* a lack of interest in things\n\n* not wanting to look after yourself.\n\nBecause psychiatric experts disagree about what schizophrenia is, some people argue that this term shouldn't be used at all. Others think the name of the condition doesn't matter, and prefer to just focus on helping you manage your symptoms and meeting your individual needs."",""FollowUpPrompts"":""[{\""displayOrder\"":0,\""qnaId\"":147,\""qna\"":null,\""displayText\"":\""Prevalence\""},{\""displayOrder\"":0,\""qnaId\"":148,\""qna\"":null,\""displayText\"":\""Causes\""},{\""displayOrder\"":0,\""qnaId\"":149,\""qna\"":null,\""displayText\"":\""Symptoms\""},{\""displayOrder\"":0,\""qnaId\"":150,\""qna\"":null,\""displayText\"":\""Related Issues\""}]""}
    ,{""Type"":""trace"",""MessageID"":""G2tRRm8FFZk"",""MessageTime"":""2019-08-07T09:19:14.723Z"",""MessageChannel"":""facebook"",""MessageSenderID"":""2826538590752448"",""MessageSenderName"":""Sean Betts"",""ConversationID"":""2826538590752448-530667464425015"",""MessageText"":""WHAT_IS_NEURODIVERSITY"",""MessageRecipientID"":""530667464425015"",""QuickReplyDisplayText"":""NeuroDiversity"",""QuickReplyPayload"":""WHAT_IS_NEURODIVERSITY"",""Question"":""[\""What is NeuroDiversity?\""]"",""Answer"":""🤔\nAt its core, NeuroDiversity advocates neurological differences to be recognised and respected in the same way other human variations are. These neurological differences are viewed as a variation of how our the human brain is wired. As such, it rejects the idea that these conditions can be cured and instead celebrates them for bringing diversity to human society and culture"",""FollowUpPrompts"":""[{\""displayOrder\"":0,\""qnaId\"":241,\""qna\"":null,\""displayText\"":\""More Info\""}]""}
    ,{""Type"":""trace"",""MessageID"":""jQ3W8nCoefM16arb5pAgQ8kfVjGLBRMdzD_oYUEHNE4L1csZIZpXf1CKnx7YFgCGg9X-fFegS4emSdid1as21g"",""MessageTime"":""2019-08-07T09:19:33.470Z"",""MessageChannel"":""facebook"",""MessageSenderID"":""2826538590752448"",""MessageSenderName"":""Sean Betts"",""ConversationID"":""2826538590752448-530667464425015"",""MessageText"":""What Types Are There?"",""MessageRecipientID"":""530667464425015"",""QuickReplyDisplayText"":null,""QuickReplyPayload"":null,""Question"":""[\""What types of mental health are there?\""]"",""Answer"":""🙉\nThere are many different mental health problems. Some of them have similar symptoms, so you may experience the symptoms of more than one mental health problem, or be given several diagnoses at once. Or you might not have any particular diagnosis, but still be finding things very difficult. Everyone’s experience is different and can change at different times. Click below to find out more about specific types of mental health problems:"",""FollowUpPrompts"":""[{\""displayOrder\"":0,\""qnaId\"":124,\""qna\"":null,\""displayText\"":\""Depression\""},{\""displayOrder\"":0,\""qnaId\"":130,\""qna\"":null,\""displayText\"":\""Anxiety\""},{\""displayOrder\"":0,\""qnaId\"":135,\""qna\"":null,\""displayText\"":\""Phobias\""},{\""displayOrder\"":0,\""qnaId\"":140,\""qna\"":null,\""displayText\"":\""Eating Disorders\""},{\""displayOrder\"":0,\""qnaId\"":146,\""qna\"":null,\""displayText\"":\""Schizophrenia\""},{\""displayOrder\"":0,\""qnaId\"":151,\""qna\"":null,\""displayText\"":\""OCD\""},{\""displayOrder\"":0,\""qnaId\"":166,\""qna\"":null,\""displayText\"":\""Personality Disorders\""},{\""displayOrder\"":0,\""qnaId\"":172,\""qna\"":null,\""displayText\"":\""Bipolar\""},{\""displayOrder\"":0,\""qnaId\"":156,\""qna\"":null,\""displayText\"":\""Panic Disorder\""},{\""displayOrder\"":0,\""qnaId\"":184,\""qna\"":null,\""displayText\"":\""Psychosis\""}]""}
    ,{""Type"":""trace"",""MessageID"":""RLOUlqwIcO4zSY4zHgWG78kfVjGLBRMdzD_oYUEHNE7gRJSePak9st7_nIIgw3OWXLvAMm4hMXeJ5kFnlIavpQ"",""MessageTime"":""2019-08-07T09:20:04.294Z"",""MessageChannel"":""facebook"",""MessageSenderID"":""2826538590752448"",""MessageSenderName"":""Sean Betts"",""ConversationID"":""2826538590752448-530667464425015"",""MessageText"":""Get Help"",""MessageRecipientID"":""530667464425015"",""QuickReplyDisplayText"":null,""QuickReplyPayload"":null,""Question"":""[\""Help me\""]"",""Answer"":""🆘\nBeing in a crisis can be very frightening. If you are worried about your safety, thinking of self-harm or feeling suicidal please get some help and support ASAP.\n\nWhen you're in a criss due to Mental Health problems or struggling to deal with your NeuroDiversity it can be difficult to know what to do. Everyone is deserving of support and it's vitally important that you get some when in crisis.\n\nPlease follow this link for details of where you can get help:\n\nhttps://www.mihnd.co.uk/crisis"",""FollowUpPrompts"":""[{\""displayOrder\"":0,\""qnaId\"":196,\""qna\"":null,\""displayText\"":\""Help Yourself\""}]""}
    ,{""Type"":""trace"",""MessageID"":""45tDLlo7yKuzjKgGHAOYfskfVjGLBRMdzD_oYUEHNE4iCVqhprpw8f0-sbuDSIo7IcuNITny2S9mRiVxgefD6A"",""MessageTime"":""2019-08-07T09:19:51.300Z"",""MessageChannel"":""facebook"",""MessageSenderID"":""2826538590752448"",""MessageSenderName"":""Sean Betts"",""ConversationID"":""2826538590752448-530667464425015"",""MessageText"":""Related Issues"",""MessageRecipientID"":""530667464425015"",""QuickReplyDisplayText"":null,""QuickReplyPayload"":null,""Question"":""[\""What are related issues of depression?\""]"",""Answer"":""🧐\nRelated issues of depression include:"",""FollowUpPrompts"":""[{\""displayOrder\"":0,\""qnaId\"":130,\""qna\"":null,\""displayText\"":\""Anxiety\""},{\""displayOrder\"":0,\""qnaId\"":179,\""qna\"":null,\""displayText\"":\""Self-Harm\""},{\""displayOrder\"":0,\""qnaId\"":181,\""qna\"":null,\""displayText\"":\""Suicide\""},{\""displayOrder\"":0,\""qnaId\"":172,\""qna\"":null,\""displayText\"":\""Bipolar\""},{\""displayOrder\"":0,\""qnaId\"":166,\""qna\"":null,\""displayText\"":\""Personality Disorder\""}]""}
    ,{""Type"":""trace"",""MessageID"":""aHEeHB0t0hHr7175E07zF8kfVjGLBRMdzD_oYUEHNE71Va3wN0yJv9aBAzkOk6JMZvTBRMzBNCUjCvTRuxrAVA"",""MessageTime"":""2019-08-07T09:19:45.342Z"",""MessageChannel"":""facebook"",""MessageSenderID"":""2826538590752448"",""MessageSenderName"":""Sean Betts"",""ConversationID"":""2826538590752448-530667464425015"",""MessageText"":""Symptoms"",""MessageRecipientID"":""530667464425015"",""QuickReplyDisplayText"":null,""QuickReplyPayload"":null,""Question"":""[\""What are the symptoms of schizophrenia?\""]"",""Answer"":""🙊\nThe symptoms of schizophrenia are usually classified into:\n\n* Positive symptoms – any change in behaviour or thoughts, such as hallucinations or delusions\n* Negative symptoms – a withdrawal or lack of function that you would not usually expect to see in a healthy person; for example, people with schizophrenia often appear emotionless and flat"",""FollowUpPrompts"":""[{\""displayOrder\"":0,\""qnaId\"":150,\""qna\"":null,\""displayText\"":\""Related Issues\""}]""}
    ,{""Type"":""trace"",""MessageID"":""jQ3W8nCoefM16arb5pAgQ8kfVjGLBRMdzD_oYUEHNE4L1csZIZpXf1CKnx7YFgCGg9X-fFegS4emSdid1as21g"",""MessageTime"":""2019-08-07T09:19:33.470Z"",""MessageChannel"":""facebook"",""MessageSenderID"":""2826538590752448"",""MessageSenderName"":""Sean Betts"",""ConversationID"":""2826538590752448-530667464425015"",""MessageText"":""What Types Are There?"",""MessageRecipientID"":""530667464425015"",""QuickReplyDisplayText"":null,""QuickReplyPayload"":null,""Question"":""[\""What types of mental health are there?\""]"",""Answer"":""🙉\nThere are many different mental health problems. Some of them have similar symptoms, so you may experience the symptoms of more than one mental health problem, or be given several diagnoses at once. Or you might not have any particular diagnosis, but still be finding things very difficult. Everyone’s experience is different and can change at different times. Click below to find out more about specific types of mental health problems:"",""FollowUpPrompts"":""[{\""displayOrder\"":0,\""qnaId\"":124,\""qna\"":null,\""displayText\"":\""Depression\""},{\""displayOrder\"":0,\""qnaId\"":130,\""qna\"":null,\""displayText\"":\""Anxiety\""},{\""displayOrder\"":0,\""qnaId\"":135,\""qna\"":null,\""displayText\"":\""Phobias\""},{\""displayOrder\"":0,\""qnaId\"":140,\""qna\"":null,\""displayText\"":\""Eating Disorders\""},{\""displayOrder\"":0,\""qnaId\"":146,\""qna\"":null,\""displayText\"":\""Schizophrenia\""},{\""displayOrder\"":0,\""qnaId\"":151,\""qna\"":null,\""displayText\"":\""OCD\""},{\""displayOrder\"":0,\""qnaId\"":166,\""qna\"":null,\""displayText\"":\""Personality Disorders\""},{\""displayOrder\"":0,\""qnaId\"":172,\""qna\"":null,\""displayText\"":\""Bipolar\""},{\""displayOrder\"":0,\""qnaId\"":156,\""qna\"":null,\""displayText\"":\""Panic Disorder\""},{\""displayOrder\"":0,\""qnaId\"":184,\""qna\"":null,\""displayText\"":\""Psychosis\""}]""}
    ,{""Type"":""trace"",""MessageID"":""45tDLlo7yKuzjKgGHAOYfskfVjGLBRMdzD_oYUEHNE4iCVqhprpw8f0-sbuDSIo7IcuNITny2S9mRiVxgefD6A"",""MessageTime"":""2019-08-07T09:19:51.300Z"",""MessageChannel"":""facebook"",""MessageSenderID"":""2826538590752448"",""MessageSenderName"":""Sean Betts"",""ConversationID"":""2826538590752448-530667464425015"",""MessageText"":""Related Issues"",""MessageRecipientID"":""530667464425015"",""QuickReplyDisplayText"":null,""QuickReplyPayload"":null,""Question"":""[\""What are related issues of depression?\""]"",""Answer"":""🧐\nRelated issues of depression include:"",""FollowUpPrompts"":""[{\""displayOrder\"":0,\""qnaId\"":130,\""qna\"":null,\""displayText\"":\""Anxiety\""},{\""displayOrder\"":0,\""qnaId\"":179,\""qna\"":null,\""displayText\"":\""Self-Harm\""},{\""displayOrder\"":0,\""qnaId\"":181,\""qna\"":null,\""displayText\"":\""Suicide\""},{\""displayOrder\"":0,\""qnaId\"":172,\""qna\"":null,\""displayText\"":\""Bipolar\""},{\""displayOrder\"":0,\""qnaId\"":166,\""qna\"":null,\""displayText\"":\""Personality Disorder\""}]""}
    ,{""Type"":""trace"",""MessageID"":""893qgqa3cR9"",""MessageTime"":""2019-08-07T09:19:27.853Z"",""MessageChannel"":""facebook"",""MessageSenderID"":""2826538590752448"",""MessageSenderName"":""Sean Betts"",""ConversationID"":""2826538590752448-530667464425015"",""MessageText"":""WHAT_IS_MENTAL_HEALTH"",""MessageRecipientID"":""530667464425015"",""QuickReplyDisplayText"":""Mental Health"",""QuickReplyPayload"":""WHAT_IS_MENTAL_HEALTH"",""Question"":""[\""What is mental health?\""]"",""Answer"":""🤔\nIn many ways, Mental Health is just like Physical Health: Everybody has it and we need to take care of it.\n\nGood Mental Health means being generally able to think, feel and react in the ways that you need and want to live your life. But if you go through a period of poor Mental Health you might find the ways you're frequently thinking, feeling or reacting become difficult, or even impossible, to cope with. This can feel just as bad as a physical illness, or even worse.\n\nMental Health problems affect around one in four people in any given year. They range from common problems, such as depression and anxiety, to rarer problems such as schizophrenia and bipolar disorder."",""FollowUpPrompts"":""[{\""displayOrder\"":0,\""qnaId\"":123,\""qna\"":null,\""displayText\"":\""What Types Are There?\""}]""}
    ,{""Type"":""trace"",""MessageID"":""L4QV73hzRzsKljnv6hDaZ8-5|0000011"",""MessageTime"":""2019-08-07T09:18:08.634Z"",""MessageChannel"":""webchat"",""MessageSenderID"":""r_u00jwzqzyu"",""MessageSenderName"":null,""ConversationID"":""L4QV73hzRzsKljnv6hDaZ8-5"",""MessageText"":""Causes"",""MessageRecipientID"":""r_u00jwzqzyu"",""QuickReplyDisplayText"":null,""QuickReplyPayload"":null,""Question"":""[\""What causes depression?\""]"",""Answer"":""🙈\nThere are several ideas about what causes depression. It can vary a lot between different people, and for some people a combination of different factors may cause their depression. Some find that they become depressed without any obvious reason.\n\nPossible causes of depression are:\n\n* Childhood experiences\n* Life events\n* Other mental health problems\n* Physical health problems\n* Genetic inheritance\n* Medication, recreational drugs and alcohol\n* Sleep, diet and exercise."",""FollowUpPrompts"":""[{\""displayOrder\"":0,\""qnaId\"":127,\""qna\"":null,\""displayText\"":\""Types\""}]""}
    ,{""Type"":""trace"",""MessageID"":""L4QV73hzRzsKljnv6hDaZ8-5|0000009"",""MessageTime"":""2019-08-07T09:18:03.664Z"",""MessageChannel"":""webchat"",""MessageSenderID"":""r_u00jwzqzyu"",""MessageSenderName"":null,""ConversationID"":""L4QV73hzRzsKljnv6hDaZ8-5"",""MessageText"":""Psychosis"",""MessageRecipientID"":""r_u00jwzqzyu"",""QuickReplyDisplayText"":null,""QuickReplyPayload"":null,""Question"":""[\""What is psychosis?\""]"",""Answer"":""🤔\nPsychosis (also called a psychotic experience or psychotic episode) is when you perceive or interpret reality in a very different way from people around you. The most common types of psychosis are:\n\n* hallucinations, such as hearing voices or having visions\n\n* delusions, such as paranoia or delusions of grandeur.\n\nPsychosis affects people in different ways. You might experience it once, have short episodes throughout your life, or live with it most of the time. It's also possible to have a psychotic experience without ever being diagnosed with a particular mental health problem.\n\nSome people have a positive experience of psychosis. You may find it comforting, or feel that it helps you understand the world or makes you more creative."",""FollowUpPrompts"":""[{\""displayOrder\"":0,\""qnaId\"":240,\""qna\"":null,\""displayText\"":\""Prevalence\""},{\""displayOrder\"":0,\""qnaId\"":239,\""qna\"":null,\""displayText\"":\""Causes\""},{\""displayOrder\"":0,\""qnaId\"":238,\""qna\"":null,\""displayText\"":\""Symptoms\""},{\""displayOrder\"":0,\""qnaId\"":237,\""qna\"":null,\""displayText\"":\""Related Issues\""}]""}
    ,{""Type"":""trace"",""MessageID"":""L4QV73hzRzsKljnv6hDaZ8-5|0000003"",""MessageTime"":""2019-08-07T09:17:49.494Z"",""MessageChannel"":""webchat"",""MessageSenderID"":""r_u00jwzqzyu"",""MessageSenderName"":null,""ConversationID"":""L4QV73hzRzsKljnv6hDaZ8-5"",""MessageText"":""What Is Mental Health?"",""MessageRecipientID"":""r_u00jwzqzyu"",""QuickReplyDisplayText"":null,""QuickReplyPayload"":null,""Question"":""[\""What is mental health?\""]"",""Answer"":""🤔\nIn many ways, Mental Health is just like Physical Health: Everybody has it and we need to take care of it.\n\nGood Mental Health means being generally able to think, feel and react in the ways that you need and want to live your life. But if you go through a period of poor Mental Health you might find the ways you're frequently thinking, feeling or reacting become difficult, or even impossible, to cope with. This can feel just as bad as a physical illness, or even worse.\n\nMental Health problems affect around one in four people in any given year. They range from common problems, such as depression and anxiety, to rarer problems such as schizophrenia and bipolar disorder."",""FollowUpPrompts"":""[{\""displayOrder\"":0,\""qnaId\"":123,\""qna\"":null,\""displayText\"":\""What Types Are There?\""}]""}
    ,{""Type"":""trace"",""MessageID"":""L4QV73hzRzsKljnv6hDaZ8-5|0000005"",""MessageTime"":""2019-08-07T09:17:54.321Z"",""MessageChannel"":""webchat"",""MessageSenderID"":""r_u00jwzqzyu"",""MessageSenderName"":null,""ConversationID"":""L4QV73hzRzsKljnv6hDaZ8-5"",""MessageText"":""What Types Are There?"",""MessageRecipientID"":""r_u00jwzqzyu"",""QuickReplyDisplayText"":null,""QuickReplyPayload"":null,""Question"":""[\""What types of mental health are there?\""]"",""Answer"":""🙉\nThere are many different mental health problems. Some of them have similar symptoms, so you may experience the symptoms of more than one mental health problem, or be given several diagnoses at once. Or you might not have any particular diagnosis, but still be finding things very difficult. Everyone’s experience is different and can change at different times. Click below to find out more about specific types of mental health problems:"",""FollowUpPrompts"":""[{\""displayOrder\"":0,\""qnaId\"":124,\""qna\"":null,\""displayText\"":\""Depression\""},{\""displayOrder\"":0,\""qnaId\"":130,\""qna\"":null,\""displayText\"":\""Anxiety\""},{\""displayOrder\"":0,\""qnaId\"":135,\""qna\"":null,\""displayText\"":\""Phobias\""},{\""displayOrder\"":0,\""qnaId\"":140,\""qna\"":null,\""displayText\"":\""Eating Disorders\""},{\""displayOrder\"":0,\""qnaId\"":146,\""qna\"":null,\""displayText\"":\""Schizophrenia\""},{\""displayOrder\"":0,\""qnaId\"":151,\""qna\"":null,\""displayText\"":\""OCD\""},{\""displayOrder\"":0,\""qnaId\"":166,\""qna\"":null,\""displayText\"":\""Personality Disorders\""},{\""displayOrder\"":0,\""qnaId\"":172,\""qna\"":null,\""displayText\"":\""Bipolar\""},{\""displayOrder\"":0,\""qnaId\"":156,\""qna\"":null,\""displayText\"":\""Panic Disorder\""},{\""displayOrder\"":0,\""qnaId\"":184,\""qna\"":null,\""displayText\"":\""Psychosis\""}]""}
    ,{""Type"":""trace"",""MessageID"":""L4QV73hzRzsKljnv6hDaZ8-5|0000007"",""MessageTime"":""2019-08-07T09:17:58.780Z"",""MessageChannel"":""webchat"",""MessageSenderID"":""r_u00jwzqzyu"",""MessageSenderName"":null,""ConversationID"":""L4QV73hzRzsKljnv6hDaZ8-5"",""MessageText"":""Panic Disorder"",""MessageRecipientID"":""r_u00jwzqzyu"",""QuickReplyDisplayText"":null,""QuickReplyPayload"":null,""Question"":""[\""What is Panic Disorder?\""]"",""Answer"":""🤔\nPanic disorder is an anxiety disorder where you regularly have sudden attacks of panic or fear."",""FollowUpPrompts"":""[{\""displayOrder\"":0,\""qnaId\"":157,\""qna\"":null,\""displayText\"":\""Prevalence\""},{\""displayOrder\"":0,\""qnaId\"":235,\""qna\"":null,\""displayText\"":\""Causes\""},{\""displayOrder\"":0,\""qnaId\"":158,\""qna\"":null,\""displayText\"":\""Symptoms\""},{\""displayOrder\"":0,\""qnaId\"":159,\""qna\"":null,\""displayText\"":\""Panic Attacks\""},{\""displayOrder\"":0,\""qnaId\"":160,\""qna\"":null,\""displayText\"":\""Related Issues\""}]""}
    ,{""Type"":""trace"",""MessageID"":""L4QV73hzRzsKljnv6hDaZ8-5|0000001"",""MessageTime"":""2019-08-07T09:17:42.363Z"",""MessageChannel"":""webchat"",""MessageSenderID"":""r_u00jwzqzyu"",""MessageSenderName"":null,""ConversationID"":""L4QV73hzRzsKljnv6hDaZ8-5"",""MessageText"":""hi"",""MessageRecipientID"":""r_u00jwzqzyu"",""QuickReplyDisplayText"":null,""QuickReplyPayload"":null,""Question"":""[\""Hi\"",\""Hello\"",\""Howdy\"",\""Sup\"",\""Whassup\"",\""Yo\""]"",""Answer"":""👋\nHi! How are you today?"",""FollowUpPrompts"":""[]""}
];
</code></pre>

<p>The json file that is being returned by the ajax call is structured exactly the same as the above, as per the preview in Chrome:</p>

<p><a href=""https://i.stack.imgur.com/6Agfy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6Agfy.png"" alt=""enter image description here""></a></p>

<p>I've run out of ideas of how to fix the issue I'm seeing.</p>
","<javascript><json><ajax><azure-data-factory><tabulator>","2019-08-07 21:01:02","1315","0","1","61740757","<p>Ok, I think you need to do something like this in your ajaxResponse function</p>

<pre><code> ajaxResponse:function(url, params, response){
        //url - the URL of the request
        //params - the parameters passed with the request
        //response - the JSON object returned in the body of the response.

        return response.data.d; //pass the data array into Tabulator
    },
</code></pre>
"
"57402210","Azure services: SFTP .xlsb multi tabs file loading, ETL and sending to Azure Function App","<p>We need to load a file with <code>.xlsb</code> file extension from sftp, extract all rows from the file and certain columns, then send the extracted data to Function app.</p>

<p>The <code>*.xlsb</code> file contains two tabs, where all data is required for processing.</p>

<p>Qustions:</p>

<p>What Azure service can be used for this process of data loading, ETL and sending?</p>
","<azure><azure-blob-storage><azure-data-factory><azure-logic-apps>","2019-08-07 20:57:55","164","0","1","57668893","<blockquote>
  <p>Qustions:</p>
  
  <p>What Azure service can be used for this process of data loading, ETL
  and sending?</p>
</blockquote>

<p>My idea is consists of two parts:</p>

<p>1.Use Azure Data Factory to transfer data from <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sftp"" rel=""nofollow noreferrer"">SFTP</a> source to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage"" rel=""nofollow noreferrer"">Blob Storage</a> sink with Copy Activity.</p>

<p>2.Use <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-triggered-function"" rel=""nofollow noreferrer"">Blob Storage Azure Function Trigger</a> to monitor any data into your blob storage account to trigger the function.</p>
"
"57396638","How to drop duplicates in source data set (JSON) and load data into azure SQL DB in azure data factory","<p>I have a table in SQL DB with primary key fields. Now i am using a copy activity in azure data factory with source dataset(JSON).</p>

<p>We are writing this data into sink dataset(SQL DB) but the pipeline is failing with the below error</p>

<pre><code>""message"": ""'Type=System.Data.SqlClient.SqlException,Message=Violation of 
 PRIMARY KEY constraint &amp;apos;PK__field__399771B9251AD6D4&amp;apos;. Cannot 
 insert duplicate key in object &amp;apos;dbo.crop_original_new&amp;apos;. The 
 duplicate key value is (9161&amp;#44; en).\r\nThe statement has been 
 terminated.,Source=.Net SqlClient Data Provider,SqlErrorNumber=2627,Class=14,ErrorCode=-2146232060,State=1,Errors= 
[{Class=14,Number=2627,State=1,Message=Violation of PRIMARY KEY 
constraint &amp;apos;PK__field__399771B9251AD6D4&amp;apos;. Cannot insert 
duplicate key in object &amp;apos;Table&amp;apos;. The duplicate key value is 
(9161&amp;#44; en).,},{Class=0,Number=3621,State=0,Message=The statement has 
been terminated.,},],'"",
</code></pre>
","<azure><azure-sql-database><azure-data-factory>","2019-08-07 14:24:16","1201","1","2","57396776","<p>Well, the finest solution would be:</p>

<ul>
<li>Create a staging table in your SQL environment stg_table (this table should have a different key policy)</li>
<li>Load data from JSON source to stg_table</li>
<li>Write a stored procedure to clean data from duplicates and to load into your destination table</li>
</ul>

<p>Or if you are familiar with Mapping Data Flows in ADF you can check <a href=""https://kromerbigdata.com/2019/04/21/use-adf-mapping-data-flows-for-fuzzy-matching-and-dedupe/"" rel=""nofollow noreferrer"">this</a> article by Mark Kromer</p>
"
"57396638","How to drop duplicates in source data set (JSON) and load data into azure SQL DB in azure data factory","<p>I have a table in SQL DB with primary key fields. Now i am using a copy activity in azure data factory with source dataset(JSON).</p>

<p>We are writing this data into sink dataset(SQL DB) but the pipeline is failing with the below error</p>

<pre><code>""message"": ""'Type=System.Data.SqlClient.SqlException,Message=Violation of 
 PRIMARY KEY constraint &amp;apos;PK__field__399771B9251AD6D4&amp;apos;. Cannot 
 insert duplicate key in object &amp;apos;dbo.crop_original_new&amp;apos;. The 
 duplicate key value is (9161&amp;#44; en).\r\nThe statement has been 
 terminated.,Source=.Net SqlClient Data Provider,SqlErrorNumber=2627,Class=14,ErrorCode=-2146232060,State=1,Errors= 
[{Class=14,Number=2627,State=1,Message=Violation of PRIMARY KEY 
constraint &amp;apos;PK__field__399771B9251AD6D4&amp;apos;. Cannot insert 
duplicate key in object &amp;apos;Table&amp;apos;. The duplicate key value is 
(9161&amp;#44; en).,},{Class=0,Number=3621,State=0,Message=The statement has 
been terminated.,},],'"",
</code></pre>
","<azure><azure-sql-database><azure-data-factory>","2019-08-07 14:24:16","1201","1","2","57515563","<p>You can use Fault tolerance setting provided in copy activity to skip incompatible rows.</p>

<p><a href=""https://i.stack.imgur.com/actVX.png"" rel=""nofollow noreferrer"">Setting image</a></p>
"
"57395922","Upload json (or csv) document to Azure Data Factory and create graph in Cosmos DB","<p>I am trying to use the Azure Data Factory to move a json or csv file from my blob storage to my Cosmos DB graph database.</p>

<p>By formatting the json file correctly, I am able to upload vertices, but have no idea how to create edges. Hard coding the edges into the json file does not work. This is one of the vertices in my json file: </p>

<pre><code>{
""id"": ""o0001"",
""label"": ""Order"",
""type"": ""vertex"",
""Product2"": 1.0,
""Product3"": 1.0,
""Product4"": 1.0,
""Product5"": 1.0,
""Product6"": 1.0,
""Product7"": 1.0,
""Product8"": 1.0,
""Product24"": 1.0,
""Product25"": 1.0,
""Product26"": 1.0,
""Product27"": 1.0
}
</code></pre>

<p>And this is an edge:</p>

<pre><code>{
""label"": ""purchased"",
""type"": ""edge"",
""inVLabel"": ""Product"",
""outVLabel"": ""Order"",
""inV"": ""Product2"",
""outV"": ""o0001""
}
</code></pre>

<p>Everything gets imported as a vertex. Does anybody have an idea how to upload both vertices and edges?</p>
","<json><azure-cosmosdb><azure-data-factory>","2019-08-07 13:45:09","413","0","1","57424565","<p>You can convert you json to dataframe and then execute the below steps to add records in Cosmos DB with one row in the DataFrame equating to 1 vertex in Cosmos.</p>

<ul>
<li><p>Click on the below link and download option and select the <code>uber.jar https://search.maven.org/artifact/com.microsoft.azure/azure-cosmosdb-spark_2.3.0_2.11/1.2.2/jar</code> then add in your dependency.</p>

<p>spark-shell --master yarn --executor-cores 5 --executor-memory 10g --num-executors 10 --driver-memory 10g --jars ""path/to/jar/dependency/azure-cosmosdb-spark_2.3.0_2.11-1.2.2-uber.jar"" --packages ""com.google.guava:guava:18.0,com.google.code.gson:gson:2.3.1,com.microsoft.azure:azure-documentdb:1.16.1""</p></li>
<li><p>here is the code for the same:</p>

<pre><code>import org.apache.spark.sql.types._
import org.apache.spark.sql.Row

val data = Seq(
Row(2, ""Abb""),
Row(4, ""Bcc""),
Row(6, ""Cdd"")
)

val schema = List(
StructField(""partitionKey"", IntegerType, true),
StructField(""name"", StringType, true)
)

val DF = spark.createDataFrame(
spark.sparkContext.parallelize(data),
StructType(schema)
)

val writeConfig = Map(""Endpoint"" -&gt; ""https://*******.documents.azure.com:443/"",
""Masterkey"" -&gt; ""**************"",
""Database"" -&gt; ""db_name"",
""Collection"" -&gt; ""collection_name"",
""Upsert"" -&gt; ""true"",
""query_pagesize"" -&gt; ""100000"",
""bulkimport""-&gt; ""true"",
""WritingBatchSize""-&gt; ""1000"",
""ConnectionMaxPoolSize""-&gt; ""100"",
""partitionkeydefinition""-&gt; ""/partitionKey"")

DF.write.format(""com.microsoft.azure.cosmosdb.spark"").mode(""overwrite"").options(writeConfig).save()
</code></pre></li>
</ul>

<p>Hope it helps.</p>
"
"57380501","Azure Data Flow filter distinct rows","<p>What I want to achieve is that I have sources which sending me some data, but before saving that data in sink I want to filter that distinct with respect to columns I am not able to find Distinct function in expression functions. Can anyone tell me how to achieve this </p>
","<azure-data-factory>","2019-08-06 16:35:58","7989","0","2","57703362","<p>Not sure if you still have this problem, I suggest to use the 'Aggregate' component in dataflow, I did a test like below:</p>

<p><a href=""https://i.stack.imgur.com/4988j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4988j.png"" alt=""enter image description here""></a></p>

<p>in 'Aggregate Settings' we define all the 'Group by' columns and 'Aggregates' columns, the source table have 9 columns in total, and 900 rows in total containing 450 distinct rows plus 450 duplicated rows.</p>

<p><a href=""https://i.stack.imgur.com/xCzLQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xCzLQ.png"" alt=""enter image description here""></a></p>

<p>I use max to aggregate the 'ModifiedDate' column, and in sink table there's only 450 distinct rows.</p>
"
"57380501","Azure Data Flow filter distinct rows","<p>What I want to achieve is that I have sources which sending me some data, but before saving that data in sink I want to filter that distinct with respect to columns I am not able to find Distinct function in expression functions. Can anyone tell me how to achieve this </p>
","<azure-data-factory>","2019-08-06 16:35:58","7989","0","2","69465679","<p>This can be done by manually editing the script (and then linking it together on the UI).
The following snippet does a distinct filtering using all columns:</p>
<pre><code>aggregate(groupBy(mycols = sha2(256,columns())),
    each(match(true()), $$ = first($$))) ~&gt; DistinctRows
</code></pre>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-script#distinct-row-using-all-columns"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-script#distinct-row-using-all-columns</a></p>
"
"57377455","How to process the telemetry json messages in Azure data lake Gen2?","<p>I have simulated devices which is sending messages to IoT Hub blob storage and from there I am copying data(encoded in JSON format) to Azure Data Lake Gen2 by creating a pipeline using Azure Data Factory.</p>

<p>How to convert these json output file to CSV file to be processed by data lake engine? Can't I process all the incoming json telemetry directly in azure data lake?</p>
","<azure><azure-data-factory><azure-data-lake><azure-iot-hub>","2019-08-06 13:42:09","399","0","1","57388587","<p>There are 3 official <a href=""https://learn.microsoft.com/en-us/u-sql/functions/operators/built-in-extractors"" rel=""nofollow noreferrer"">built-in extractors</a> that allows you to analyze data contained in CSV, TSV or Text files.</p>

<p>But MSFT also released some additional sample extractors on their Azure GitHub <a href=""https://github.com/Azure/usql/tree/master/Examples/DataFormats/Microsoft.Analytics.Samples.Formats"" rel=""nofollow noreferrer"">repo</a> that deal with Xml, Json and Avro files. I have used the Json extractor in production as it is really stable and useful.</p>

<blockquote>
  <p>The JSON Extractor treats the entire input file as a single JSON document. If you have a JSON document per line, see the next section. The columns that you try to extract will be extracted from the document. In this case, I'm extracting out the _id and Revision properties. Note, it's possible that one of these is a further nested object, in which case you can use the JSON UDF's for subsequent processing.</p>
</blockquote>

<pre><code>REFERENCE ASSEMBLY [Newtonsoft.Json];
REFERENCE ASSEMBLY [Microsoft.Analytics.Samples.Formats]; 

//Define schema of file, must map all columns
 @myRecords =
    EXTRACT
        _id string,
    Revision string     
    FROM @""sampledata/json/{*}.json""
    USING new Microsoft.Analytics.Samples.Formats.Json.JsonExtractor();
</code></pre>
"
"57376236","Extract element from output array in a Copy Data activity","<p>I have a copy data activity that dynamically adds a datetime suffix to the sink file name, which is based on <code>utcnow()</code>.  This corresponds to the <code>start</code> datetime in the copy data activity. I am looking to extract the 'start' element from the <code>executionDetails</code> array in the output:</p>

<pre><code>{
    ""dataRead"": 0,
    ""dataWritten"": 86,
    ""filesWritten"": 1,
    ""sourcePeakConnections"": 1,
    ""sinkPeakConnections"": 1,
    ""rowsRead"": 0,
    ""rowsCopied"": 0,
    ""copyDuration"": 4,
    ""throughput"": 0,
    ""errors"": [],
    ""effectiveIntegrationRuntime"": ""FXL"",
    ""usedParallelCopies"": 1,
    ""executionDetails"": [
        {
            ""source"": {
                ""type"": ""SqlServer""
            },
            ""sink"": {
                ""type"": ""AzureBlobFS""
            },
            ""status"": ""Succeeded"",
            ""start"": ""2019-08-06T12:29:20.477586Z"",
            ""duration"": 4,
            ""usedParallelCopies"": 1,
            ""detailedDurations"": {
                ""queuingDuration"": 3,
                ""transferDuration"": 1
            }
        }
    ]
}
</code></pre>

<p>Assuming the activity is called <code>CopyData</code>, I want to set the value of <code>start</code> to a variable.  I am struggling to get this, a simple <code>@activity('CopyData').output.executionDetails.start</code> does not work, telling me to assign an integer value of the <code>executionDetails</code> array.  However trying <code>@activity('CopyData').output.executionDetails[3]</code> errors telling me the range is (0,0).  I am looking for a method to extract the datetimestamp into a string variable.</p>

<p>I can store <code>executionDetails</code> in an array variable, but still unable thereafter to extract the <code>start</code> value.</p>
","<azure-data-factory>","2019-08-06 12:35:58","1113","2","1","57376579","<p>Already worked it out, there range is 0,0 because there is only 1 array in <code>executionDetails</code> containing various values.  So, I just need to call the array with <code>[0]</code> and then  call the <code>start</code> value, so:</p>

<p><code>@activity('CopyData').output.executionDetails[0].start</code></p>
"
"57374617","Copy new and changed files by LastModifiedDate with Azure Data Factory V2","<p>Using the copy activity, I need to fetch only the latest file within a folder.</p>
<p>Example:
Folder</p>
<p>File1</p>
<p>File2
where File2 is the latest one.</p>
<p>However, it is taking both files.</p>
<p>I followed:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/solution-template-copy-new-files-lastmodifieddate"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/solution-template-copy-new-files-lastmodifieddate</a></p>
<p>Also tried in the source dataset&gt;Connection&gt;Filter by last modified</p>
<blockquote>
<p>Start Date: @adddays(utcnow(),-31)</p>
<p>End Date: utcnow()</p>
</blockquote>
<p>I expect only the last modified file to be taken within the range date specified.</p>
","<azure><azure-data-factory>","2019-08-06 11:06:47","3835","2","3","57390283","<p>My rough idea is using such flow:Get Metadata activity-->Azure Function Activity-->Copy Activity.</p>

<p>Step1:using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity#metadata-options"" rel=""nofollow noreferrer"">Metadata Activity</a> to get the last modified datetime of the files in the specific folder within the range date specified.</p>

<p>Step2:pass the output array to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">azure function activity</a>,use bubble sort(or other compare algorithm) to get the latest modified file name and filter others.Output that file name.</p>

<p>Step3:pass the file name to copy activity to do the transmission.</p>

<p>You could get some similar clues from this case:<a href=""https://social.msdn.microsoft.com/Forums/en-US/affdcb44-6a8c-4b20-8e2e-6da20ab86c32/default-sorting-of-get-metadata-activity?forum=AzureDataFactory"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/en-US/affdcb44-6a8c-4b20-8e2e-6da20ab86c32/default-sorting-of-get-metadata-activity?forum=AzureDataFactory</a></p>
"
"57374617","Copy new and changed files by LastModifiedDate with Azure Data Factory V2","<p>Using the copy activity, I need to fetch only the latest file within a folder.</p>
<p>Example:
Folder</p>
<p>File1</p>
<p>File2
where File2 is the latest one.</p>
<p>However, it is taking both files.</p>
<p>I followed:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/solution-template-copy-new-files-lastmodifieddate"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/solution-template-copy-new-files-lastmodifieddate</a></p>
<p>Also tried in the source dataset&gt;Connection&gt;Filter by last modified</p>
<blockquote>
<p>Start Date: @adddays(utcnow(),-31)</p>
<p>End Date: utcnow()</p>
</blockquote>
<p>I expect only the last modified file to be taken within the range date specified.</p>
","<azure><azure-data-factory>","2019-08-06 11:06:47","3835","2","3","72881948","<p>I tried a different approach for a similar problem. I noticed that getmetadata activity returns the file names in ascending order. so in the child items array file1 will come first, followed by file2, file3 etc.</p>
<p>I used the following steps.</p>
<ol>
<li>get child items using a getmetadata activity</li>
<li>define a sting variable v_filename</li>
<li>iterate child items through foreach loop in sequential mode</li>
<li>inside the foreach loop use a setvariable activity which will overwrite the v_filename in each iteration</li>
<li>After the final iteration the variable will have the latest file name</li>
<li>use your copy activty after the foreach and pass v_filename as filename.</li>
</ol>
<p>it works</p>
"
"57374617","Copy new and changed files by LastModifiedDate with Azure Data Factory V2","<p>Using the copy activity, I need to fetch only the latest file within a folder.</p>
<p>Example:
Folder</p>
<p>File1</p>
<p>File2
where File2 is the latest one.</p>
<p>However, it is taking both files.</p>
<p>I followed:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/solution-template-copy-new-files-lastmodifieddate"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/solution-template-copy-new-files-lastmodifieddate</a></p>
<p>Also tried in the source dataset&gt;Connection&gt;Filter by last modified</p>
<blockquote>
<p>Start Date: @adddays(utcnow(),-31)</p>
<p>End Date: utcnow()</p>
</blockquote>
<p>I expect only the last modified file to be taken within the range date specified.</p>
","<azure><azure-data-factory>","2019-08-06 11:06:47","3835","2","3","72943480","<p>I suspect the OP did not press the &quot;Publish&quot; button after making changes to the &quot;Filter by last modified&quot; fields (i.e. Start Time and End Time). The trigger only will act upon the state of the ADF as it sits in your Publish branch. So even if you hit the Save or Save All button which saves changes to the Collaboration branch, that isn't enough.</p>
"
"57373612","Duplicate Blob Created Events When Writing to Azure Blob Storage from Azure Databricks","<p>We are using an Azure Storage Account (Blob, StorageV2) with a single container in it. We are also using Azure Data Factory to trigger data copy pipelines from blobs (.tar.gz) created in the container. The trigger works fine when creating the blobs from an Azure App Service or by manually uploading via the Azure Storage Explorer. But when creating the blob from a Notebook on Azure Databricks, we get two (2) events for every blob created (same parameters for both events). The code for creating the blob from the notebook resembles:</p>

<pre><code>dbutils.fs.cp(
  ""/mnt/data/tmp/file.tar.gz"", 
  ""/mnt/data/out/file.tar.gz""
)
</code></pre>

<p>The <code>tmp</code> folder is just used to assemble the package, and the event trigger is attached to the <code>out</code> folder. We also tried with <code>dbutils.fs.mv</code>, but same result. The trigger rules in Azure Data Factory are:</p>

<pre><code>Blob path begins with: out/

Blob path ends with: .tar.gz
</code></pre>

<p>The container name is <code>data</code>.</p>

<p>We did find some similar posts relating to zero-length files, but at least we can't see them anywhere (if some kind of by-product to <code>dbutils</code>).</p>

<p>As mentioned, just manually uploading <code>file.tar.gz</code> works fine - a single event is triggered.</p>
","<azure><azure-data-factory><azure-blob-storage><azure-databricks>","2019-08-06 10:04:47","598","1","1","57689320","<p>We had to revert to uploading the files from Databricks to the Blob Storage using the <code>azure-storage-blob</code> library. Kind of a bummer, but it works now as expected. Just in case anyone else runs into this.</p>

<p>More information:</p>

<p><a href=""https://learn.microsoft.com/en-gb/azure/storage/blobs/storage-quickstart-blobs-python"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-gb/azure/storage/blobs/storage-quickstart-blobs-python</a></p>
"
"57373286","Serializing SDO_GEOMETRY type to text really slow","<p>I am trying for a couple of days now to extract SDO_GEOMETRY records from an Oracle table into a CSV file via Microsoft Azure Data Factory (gen2). My select statement looks like this:</p>

<pre><code>select t.MY_GEOM.get_WKT() from my_table t
</code></pre>

<p>where MY_GEOM column is of type SDO_GEOMETRY. It works but it's really, really slow. About 2 hours to pull 74000 records via this method. 
Without that conversion (so, plain select without <code>.get_wkt()</code> takes about 32 seconds, but of course the result is rubbish and unusable. </p>

<p>Is there some way to speed up the process? My guess it's that the problem is on the server side, but I'm not a DBA and don't have direct access to it. I can connect to it via SQL Developer or from Data Factory. </p>

<p>The data contained there is just some LINESTRING(x1 y1, x2 y2, ...)</p>

<p>I also tried running <code>SDO_UTIL.TO_WKTGEOMETRY</code> to convert it, but it's equally slow.</p>

<p>If you have any suggestions, please let me know.</p>

<p>Kind regards, 
Tudor</p>
","<oracle><azure><azure-data-factory>","2019-08-06 09:47:29","474","2","2","57427331","<p>As i know,no additional burden will be imposed on data sources or sinks in ADF,so looks like that is a performance bottleneck at the db side with <code>get_WKT()</code> method.</p>

<p>Of course,you could refer to the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance"" rel=""nofollow noreferrer"">tuning guides</a> in this link to improve your transfer performance.Especially for <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance#parallel-copy"" rel=""nofollow noreferrer"">Parallel copy</a>. For each copy activity run, Azure Data Factory determines the number of parallel copies to use to copy data from the source data store and to the destination data store.That's based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance#data-integration-units"" rel=""nofollow noreferrer"">DIU</a>.</p>
"
"57373286","Serializing SDO_GEOMETRY type to text really slow","<p>I am trying for a couple of days now to extract SDO_GEOMETRY records from an Oracle table into a CSV file via Microsoft Azure Data Factory (gen2). My select statement looks like this:</p>

<pre><code>select t.MY_GEOM.get_WKT() from my_table t
</code></pre>

<p>where MY_GEOM column is of type SDO_GEOMETRY. It works but it's really, really slow. About 2 hours to pull 74000 records via this method. 
Without that conversion (so, plain select without <code>.get_wkt()</code> takes about 32 seconds, but of course the result is rubbish and unusable. </p>

<p>Is there some way to speed up the process? My guess it's that the problem is on the server side, but I'm not a DBA and don't have direct access to it. I can connect to it via SQL Developer or from Data Factory. </p>

<p>The data contained there is just some LINESTRING(x1 y1, x2 y2, ...)</p>

<p>I also tried running <code>SDO_UTIL.TO_WKTGEOMETRY</code> to convert it, but it's equally slow.</p>

<p>If you have any suggestions, please let me know.</p>

<p>Kind regards, 
Tudor</p>
","<oracle><azure><azure-data-factory>","2019-08-06 09:47:29","474","2","2","57606274","<p>I found a nice solution while searching for different approaches. As stated in some comments above, this solution that works for me consists of two steps:</p>

<ol>
<li>Split the SDO_GEOMETRY LINESTRING entry into its coordinates via the following select statement </li>
</ol>

<pre><code>SELECT t.id, nt.COLUMN_VALUE AS coordinates, rownum FROM my_table t, TABLE(t.SDO_GEOMETRY.SDO_ORDINATES) nt 
</code></pre>

<p>I just use it in a plain Copy Activity in Azure Data Factory to save my raw files as CSVs into a Data Lake. The files are quite large, about 4 times bigger than the final version created by the next step</p>

<ol start=""2"">
<li>Aggregate the coordinates back into a string via some Databricks Scala Spark code</li>
</ol>

<pre><code>val mergeList = udf { strings: Seq[String] =&gt; strings.mkString("", "") } 

val result = df.withColumn(""collected"", 
     collect_list($""coordinates"").over(Window.partitionBy(""id"").orderBy(""rownum"")) 
  ) 
  .groupBy(""id"") 
  .agg(max($""collected"").as(""collected"")) 
  .withColumn(""final_coordinates"", mergeList($""collected"")) 
  .select(""id"", ""final_coordinates"")

val outputFilePrefix = s""$dataLakeFolderPath/$tableName""
val tmpOutputFolder = s""$outputFilePrefix.tmp""

result
  .coalesce(1)
  .write
  .option(""header"", ""true"")
  .csv(tmpOutputFolder)

dbutils.fs.cp(partition_path, s""$outputFilePrefix.csv"")
dbutils.fs.rm(tmpOutputFolder, recurse = true)
</code></pre>

<p>The final_coordinates column contains my coordinates in the proper order (I had some issues with this). And I can plainly save the file back into my storage account. In the end, I only keep the proper CSV file that I am interested in. </p>

<p>As I said, it's quite fast. It takes about 2.5 minutes for my first step and a couple of seconds for the second one compared to 2 hours, so, I'm quite happy with this solution.</p>
"
"57358274","ADF V2 - SQL source dataset - column structure mapping issue","<p>In copy activity (SQL data set to azure blob), i'm using dynamic content for source data set, sink data set &amp; Mapping of source and sink.</p>

<p>in SQL source i used SP output having 3 columns named (col1,col2,col3) in same order, but in source data set structure i used dynamic content with same name but different order (col2, col1, col3), because of that values are swapped between col1 &amp; col2 in the source data set itself</p>

<p>My question is why name based mapping is not taking in ADF V2 data set.
in the same way for another Source (SP Output) returns 7 columns, if i want to use only 3 columns it picking first 3 columns only, there is no leverage of columns to choose using dynamic content.</p>
","<dataset><schema><azure-data-factory>","2019-08-05 12:05:33","181","0","1","57358414","<p>The dynamic schema mapping is really useful and saves a ton of work, specially when you dont have a fixed schema. In your case it seems that your schemas are always the same, so why not do the mapping yourself?</p>

<p>Just go to your copy activity, select the tab Mapping and click the button ""New Mapping"". It will pop 2 textbox with a line, indicating a column from source, being mapped to a column in the sink.</p>

<p><a href=""https://i.stack.imgur.com/43UYN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/43UYN.png"" alt=""enter image description here""></a></p>

<p>Just fill it with the corresponding names, and you should be good to go.</p>

<p>Hope this helped!</p>
"
"57350956","Resource on Load, extract data and send data via Azure Data Facory V2 to Azure Function App","<p>I am seeking resources on how to implement this scenario below.</p>
<p>I found some documentation, but I cannot find steps 2 and 3 below</p>
<p>Using Azure Data Facory V2 to</p>
<ol>
<li><p>Load a *.csv file from SFP server</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-portal"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-portal</a>
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sftp"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-sftp</a></p>
</li>
<li><p>extract rows from the *.csv file based on condition in column</p>
<p>There might be three separate extracted data, thus three sepearte Azure Aunction App.</p>
</li>
<li><p>send each extracted data to each Azure Function App</p>
</li>
</ol>
","<azure><azure-data-factory>","2019-08-04 23:49:52","38","0","1","57365161","<p>One way to structure a pipeline for this is:</p>

<ol>
<li>Lookup Activity</li>
<li>Filter Activity</li>
<li>Azure Function Activity</li>
</ol>

<p>(connected by success dependencies)</p>

<p>For each of your three cases, you can branch out from the single Lookup activity to three Filter Activities, each leading into their own Azure Function activity.</p>

<p>The details of each activity depend upon your situation, but here are some examples.</p>

<p>In the Filter activity,Items can look like <code>@activity('Lookup1').output.value</code>
Condition can look like <code>@greater(int(item()['My Number']),0)</code>.</p>
"
"57346597","How to create Event Trigger in Azure Data Factory when two files created in Azure Blob Container?","<p>I need to create an event trigger in the Azure Data Factory, which will execute when two different files created in an Azure Blob Storage Container. The trigger should be executed once both files are created in the blob container.</p>
","<azure><azure-data-factory><azure-blob-storage>","2019-08-04 12:35:26","246","1","1","57365005","<p>I don't think that it is possible as is . But I am very confident that using a GetMetadata activity with foreach activity we can achieve this , with this workaround look for the file(s)( using the GetMetaData activity)  and once you have the condition met , copy a dummy file to a different container and point the event trigger to that new container . </p>
"
"57330098","Is the new functionality called ""Microsoft Graph data connect"" available for the Canada Central region?","<p>I want to know if the functionality described in this link is available for Microsoft Canada Central region <a href=""https://developer.microsoft.com/en-us/graph/blogs/build-2019-microsoft-graph-powers-the-microsoft-365-platform/"" rel=""nofollow noreferrer"">https://developer.microsoft.com/en-us/graph/blogs/build-2019-microsoft-graph-powers-the-microsoft-365-platform/</a>.
How can I check if it is? I would like to use it in Datafactory.</p>
","<azure><office365><azure-data-factory>","2019-08-02 16:09:53","43","1","1","57330497","<p>For me Microsoft Graph as the Azure AD are global services, so once their features are Generally Available (GA) it should be available everywhere. </p>

<p>For available regional services in Azure, you can check this page : <a href=""https://azure.microsoft.com/en-us/global-infrastructure/services/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/global-infrastructure/services/</a></p>

<p>For data location of your Office 365 tenant : 
<a href=""https://products.office.com/en-us/where-is-your-data-located"" rel=""nofollow noreferrer"">https://products.office.com/en-us/where-is-your-data-located</a></p>
"
"57327825","Data factory polybase - zero rows copied to sink","<p>Data factory pipeline using the copy activity from a source data warehouse -> staging blob storage -> sink data warehouse.</p>

<p>The copy from source to blob works as expected (rows are copied). The copy from staging to sink fails - 0 rows copied</p>

<p>Disabling Polybase , and using bulk insert works.</p>

<pre><code>{
    ""name"": ""PI_TEST"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""MaterializedEventIdFilter_Copy"",
                ""type"": ""Copy"",
                ""dependsOn"": [],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""userProperties"": [
                    {
                        ""name"": ""Destination"",
                        ""value"": ""[formigration].[MaterializedEventIdFilter]""
                    }
                ],
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""SqlDWSource"",
                        ""sqlReaderStoredProcedureName"": ""[formigration].[proc_GetStgMaterializedEventIdFilter]""
                    },
                    ""sink"": {
                        ""type"": ""SqlDWSink"",
                        ""allowPolyBase"": true,
                        ""writeBatchSize"": 100000,
                        ""polyBaseSettings"": {
                            ""rejectValue"": 0,
                            ""rejectType"": ""value"",
                            ""useTypeDefault"": false
                        }
                    },
                    ""enableStaging"": true,
                    ""stagingSettings"": {
                        ""linkedServiceName"": {
                            ""referenceName"": ""riskstoreprd"",
                            ""type"": ""LinkedServiceReference""
                        },
                        ""enableCompression"": true
                    }
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""ioPrePrdMaterializedEventIdFilter"",
                        ""type"": ""DatasetReference""
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""CloudPrdMaterializedEventIdFilter"",
                        ""type"": ""DatasetReference""
                    }
                ]
            },
            {
                ""name"": ""MaterialisedEvent"",
                ""type"": ""SqlServerStoredProcedure"",
                ""dependsOn"": [
                    {
                        ""activity"": ""MaterializedEventIdFilter_Copy"",
                        ""dependencyConditions"": [
                            ""Succeeded""
                        ]
                    }
                ],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 2,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""userProperties"": [],
                ""typeProperties"": {
                    ""storedProcedureName"": ""[formigration].[proc_SetStgMaterializedEventIdFilter]""
                },
                ""linkedServiceName"": {
                    ""referenceName"": ""cloud_prd"",
                    ""type"": ""LinkedServiceReference""
                }
            }
        ],
        ""annotations"": []
    },
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>

<p>I expected the data from the blob to make it into the sink but no rows are copied.</p>

<p>Edit 1:
Checked the data warehouse (sink) a connection is made where I can observe the external tables etc created form the blob storage all within a second, yet no data is copied in.</p>

<pre class=""lang-sql prettyprint-override""><code>INSERT INTO [formigration].[MaterializedEventIdFilter] SELECT * FROM [ADFCopyGeneratedExternalTable_307e2c7f-a56f-4b75-86fb-10ab0cb94548]
</code></pre>
","<sql><azure-data-factory><polybase><azure-synapse>","2019-08-02 13:49:49","199","0","1","57361489","<p>In polybase, external tables are just a reference to a blob storage folder/file and they dont have any rows. If you want to actually copy data into your warehouse, create a regular table and use it as a sink in your copy activity!!</p>

<p>Hope this helped!</p>
"
"57322576","Linked Service to MySQL. Error converting invalid input with source encoding UTF-8 using ICU","<p>In <code>Azure Data Factory</code> I created a <code>MySQL</code> linked service. But during data preview I get error:</p>

<blockquote>
  <p>ERROR [HY000] [Microsoft][Support] (50311) Error converting invalid input with source encoding UTF-8 using ICU. Rejected bytes began with: CFC2.. Activity ID:e3b7fc32-8fcd-4981-b0e3-f377c6b17150 </p>
</blockquote>

<p>Most likely a coding error. But I did not find how to configure it</p>

<p>Here is <code>JSON</code> of linked sevice</p>

<pre><code>{
    ""name"": ""n_trans_connection"",
    ""type"": ""Microsoft.DataFactory/factories/linkedservices"",
    ""properties"": {
        ""annotations"": [],
        ""type"": ""MySql"",
        ""typeProperties"": {
            ""connectionString"": ""server=some-server;port=some-port;database=u5256_mti_spr;user=some-user;sslmode=1;usesystemtruststore=0"",
            ""encryptedCredential"": ""some-hash""
        }
    }
}
</code></pre>

<p>How to set up encoding?</p>
","<mysql><azure-data-factory>","2019-08-02 08:25:54","199","1","1","57470915","<p>Since the error is when you preview data , I don't think that is coding issue . Can you please check if you select a different table and if you face the same issue .</p>
"
"57305860","Azure pipeline run status queued","<p>I am new to Azure environment. i have written some code using .net core that starts Azure pipeline using Azure Data factory. The status of the pipeline run status when trying from my local is always success. I deployed the code in the azure environment. When try to start the pipe line from azure server always the status is queued. what is queued status and what i have to do with it. can some one please help. do i need to change any settings in azure so that the pipeline run will be success</p>

<pre><code>            AuthenticationContext context = new AuthenticationContext(""https://login.windows.net/"" + tenantID);
            ClientCredential cc = new ClientCredential(applicationId, authenticationKey);
            AuthenticationResult result = context.AcquireTokenAsync(""https://management.azure.com/"", cc).Result;
            ServiceClientCredentials cred = new TokenCredentials(result.AccessToken);
            var client = new Microsoft.Azure.Management.DataFactory.DataFactoryManagementClient(cred) { SubscriptionId = subscriptionId };

            CreateRunResponse runResponse = client.Pipelines.CreateRunWithHttpMessagesAsync(resourceGroup, dataFactoryName, pipelineName).Result.Body;
            string RunId = runResponse.RunId;
            PipelineRun pipelineRun;
            while (true)
            {
                pipelineRun = client.PipelineRuns.Get(resourceGroup, dataFactoryName, runResponse.RunId);
                if (pipelineRun.Status == ""InProgress"")
                    System.Threading.Thread.Sleep(15000);
                else
                    break;
            }
</code></pre>
","<azure-data-factory>","2019-08-01 09:27:16","2414","0","1","57682774","<p>You should try to look into the diagnotics logs of the IR , it does have valuable info which should help .</p>

<p>Navigate to IR -> diagnotics log or open event viewer -> Application and service logs -></p>
"
"57293674","Mapping Data Flow in Data Factory doesn't ignore line-breaks in text-values of CSV files","<p>I have the following problem in Azure Data Factory:</p>

<p>In a ADLS I have a CSV file with a linebreak in a value:</p>

<pre><code>A, B, C
a, b, c
a, ""b
b"", c
</code></pre>

<p>This CSV is loaded in a (CSV) Dataset (in ADF) with the following settings; first row is header, quote character double quote (""), column delimiter Comma (,), row delimiter (\r,\n, or \r\n) and escape char backslash ().      </p>

<p>The ""Preview Data"" of the dataset seems to work correct and outputs a table with 2 rows. This is also the output I expect, as the overal structure of the data is preserved.
<a href=""https://i.stack.imgur.com/nQwPf.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/nQwPf.png"" alt=""enter image description here""></a></p>

<p>However, when I try to use this dataset in Mapping Data Flow and select ""Data Preview"" (in the source node directly), I get the following output:
<a href=""https://i.stack.imgur.com/H3p7E.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/H3p7E.png"" alt=""enter image description here""></a></p>

<p>The linebreak isn't ignored, even as the whole value is between double quotes. The overal structure of the data is now broken as one row is splitted into two.</p>

<p>I get this kind of data when I save an Excel file with linebreaks in some cells as CSV. How should I work around this problem? Should I save the Excel differently, should I try to delete all linebreaks before saving as CSV, or is there a way to let Data Factory solve this problem? Als why does the Preview Data function in a Dataset seems to work correct, where the Data Preview function in Mapping Data Flow does not?</p>
","<azure><csv><azure-data-factory>","2019-07-31 14:56:30","5840","5","3","57301000","<p>I tried this and there is nothing different between Copy Active and Data Flow source settings.</p>

<p>I uploaded an csv file and changed the data to the same with you in my Blob storage. </p>

<p>Test.csv:</p>

<p><a href=""https://i.stack.imgur.com/r89bi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/r89bi.png"" alt=""enter image description here""></a></p>

<p>The result in <strong>Copy active settings and data preview:</strong>
<a href=""https://i.stack.imgur.com/Zf6yj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Zf6yj.png"" alt=""enter image description here""></a></p>

<p>The result in <strong>Data flow source data settings and data preview:</strong>
<a href=""https://i.stack.imgur.com/RY3GV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RY3GV.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/FcBjD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FcBjD.png"" alt=""enter image description here""></a></p>

<p>Please check your settings in your data flow.</p>

<p>Reference tutorial: <a href=""https://www.sqlservercurry.com/2011/01/sql-server-export-table-to-csv.html"" rel=""nofollow noreferrer"">Export Table to CSV</a>, it also works for Azrue SQL database. </p>

<p><strong>Update:</strong></p>

<p>I asked Azure Support for help, they tested and have the same error.</p>

<p>Azure Support replied me and give the reason:
<a href=""https://i.stack.imgur.com/5Oepg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5Oepg.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/WQEnp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WQEnp.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"57293674","Mapping Data Flow in Data Factory doesn't ignore line-breaks in text-values of CSV files","<p>I have the following problem in Azure Data Factory:</p>

<p>In a ADLS I have a CSV file with a linebreak in a value:</p>

<pre><code>A, B, C
a, b, c
a, ""b
b"", c
</code></pre>

<p>This CSV is loaded in a (CSV) Dataset (in ADF) with the following settings; first row is header, quote character double quote (""), column delimiter Comma (,), row delimiter (\r,\n, or \r\n) and escape char backslash ().      </p>

<p>The ""Preview Data"" of the dataset seems to work correct and outputs a table with 2 rows. This is also the output I expect, as the overal structure of the data is preserved.
<a href=""https://i.stack.imgur.com/nQwPf.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/nQwPf.png"" alt=""enter image description here""></a></p>

<p>However, when I try to use this dataset in Mapping Data Flow and select ""Data Preview"" (in the source node directly), I get the following output:
<a href=""https://i.stack.imgur.com/H3p7E.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/H3p7E.png"" alt=""enter image description here""></a></p>

<p>The linebreak isn't ignored, even as the whole value is between double quotes. The overal structure of the data is now broken as one row is splitted into two.</p>

<p>I get this kind of data when I save an Excel file with linebreaks in some cells as CSV. How should I work around this problem? Should I save the Excel differently, should I try to delete all linebreaks before saving as CSV, or is there a way to let Data Factory solve this problem? Als why does the Preview Data function in a Dataset seems to work correct, where the Data Preview function in Mapping Data Flow does not?</p>
","<azure><csv><azure-data-factory>","2019-07-31 14:56:30","5840","5","3","57382147","<p>We are adding multi-line string handling to ADF Data Flows for text-delimited datasets. We'll have that feature enabled within a few iterations.</p>

<p>The workaround until then is to copy the data from your CSV to Parquet and then consume that Parquet dataset through your data flow source transformation.</p>
"
"57293674","Mapping Data Flow in Data Factory doesn't ignore line-breaks in text-values of CSV files","<p>I have the following problem in Azure Data Factory:</p>

<p>In a ADLS I have a CSV file with a linebreak in a value:</p>

<pre><code>A, B, C
a, b, c
a, ""b
b"", c
</code></pre>

<p>This CSV is loaded in a (CSV) Dataset (in ADF) with the following settings; first row is header, quote character double quote (""), column delimiter Comma (,), row delimiter (\r,\n, or \r\n) and escape char backslash ().      </p>

<p>The ""Preview Data"" of the dataset seems to work correct and outputs a table with 2 rows. This is also the output I expect, as the overal structure of the data is preserved.
<a href=""https://i.stack.imgur.com/nQwPf.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/nQwPf.png"" alt=""enter image description here""></a></p>

<p>However, when I try to use this dataset in Mapping Data Flow and select ""Data Preview"" (in the source node directly), I get the following output:
<a href=""https://i.stack.imgur.com/H3p7E.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/H3p7E.png"" alt=""enter image description here""></a></p>

<p>The linebreak isn't ignored, even as the whole value is between double quotes. The overal structure of the data is now broken as one row is splitted into two.</p>

<p>I get this kind of data when I save an Excel file with linebreaks in some cells as CSV. How should I work around this problem? Should I save the Excel differently, should I try to delete all linebreaks before saving as CSV, or is there a way to let Data Factory solve this problem? Als why does the Preview Data function in a Dataset seems to work correct, where the Data Preview function in Mapping Data Flow does not?</p>
","<azure><csv><azure-data-factory>","2019-07-31 14:56:30","5840","5","3","66020300","<p>Anyone facing the same issue, now Microsoft has enabled the multiline feature in data flow. Now we can use csv files with multiline values</p>
<p><a href=""https://i.stack.imgur.com/BL6jV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BL6jV.png"" alt=""enter image description here"" /></a></p>
<p>And if you still having issues check this one</p>
<p><a href=""https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/data-factory/data-flow-troubleshoot-guide.md#error-code-df-executor-systeminvalidjson-1"" rel=""nofollow noreferrer"">https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/data-factory/data-flow-troubleshoot-guide.md#error-code-df-executor-systeminvalidjson-1</a></p>
"
"57291170","Problem to pass variable parent to child in SSIS 2016 on Datafactory Azure","<p>The pipeline works on DataFactory Azure (SSIS 2016) with no call child package.
When my parent package called child package, the variable child doesn't replace by a variable parent. Why?</p>
","<azure><ssis><azure-data-factory><ssis-2016>","2019-07-31 12:51:14","259","0","1","57364493","<p>If I understand this right , I think what you are suggesting is that you are using ADF , and you are trying to call a child pipeline from a parent pipeline . </p>

<p>What you need to do is on the parent pipeline , please pass the values  as an parameter to the child pipeline and you should be able to access that from the child pipeline . </p>

<p><a href=""https://i.stack.imgur.com/e6522.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e6522.png"" alt=""enter image description here""></a></p>
"
"57285552","Data archival from cosmos to ADLS","<p>I am working on IOT data where I am getting data from every device every 1 second. This is coming through IOT hub then it is processed in Azure functions and finally stored in Cosmos DB. But this data is growing to massive scales so I want to set TTL in cosmos for 5 days and archive the data in ADLS that can be done periodically, either twice a day or once a day. But I am not able to figure out whether I should write a spark job to do this transfer or use any other robust and optimized approach? I want some suggestions on different approaches I can use for this use case.</p>
","<apache-spark><bigdata><azure-cosmosdb><data-migration><azure-data-factory>","2019-07-31 07:36:12","299","0","1","57305098","<p>Azure Data factory have a way to copy data from Cosmos DB to many data stores. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db</a></p>
"
"57283593","Start-AzureRmDataFactoryV2Trigger Trigger XXX has an unexpected resource provider: , an unexpected resource type , or both","<p>I have a trigger of type Microsoft.DataFactory/factories/triggers/BlobEventsTrigger</p>

<pre><code>{
    ""name"": ""FolderTrigger"",
    ""properties"": {
        ""annotations"": [],
        ""runtimeState"": ""Stopped"",
        ""pipelines"": [
            {
                ""pipelineReference"": {
                    ""referenceName"": ""FileTrigger Import Accounts Orders"",
                    ""type"": ""PipelineReference""
                },
                ""parameters"": {
                    ""TriggerFolderPath"": ""@triggerBody().folderPath"",
                    ""TriggerFilePath"": ""@triggerBody().fileName""
                }
            }
        ],
        ""type"": ""BlobEventsTrigger"",
        ""typeProperties"": {
            ""blobPathBeginsWith"": ""/triggers/blobs/orders/trigger"",
            ""blobPathEndsWith"": "".json"",
                 ""scope"": ""/subscriptions/12345678-1234-1234-1234-123456789012/resourceGroups/europe-resource-integrations/providers/Microsoft.Storage/storageAccounts/DefaultEndpointsProtocol=https;AccountName=deveuwintgr"",
            ""events"": [
                ""Microsoft.Storage.BlobCreated""
            ]
        }
    }
}
</code></pre>

<p>When I am trying to start it (PowerShell IDE or CI/DI) as here:</p>

<pre><code>$GetAllTriggers = Get-AzureRmDataFactoryV2Trigger -DataFactoryName $datafatoryName -ResourceGroupName $resourceGroup
$GetAllTriggers | ForEach-Object { Start-AzureRmDataFactoryV2Trigger -ResourceGroupName $resourceGroup -DataFactoryName $datafatoryName -Name $_.name -Force }
Get-AzureRmDataFactoryV2Trigger -DataFactoryName $datafatoryName -ResourceGroupName $resourceGroup
</code></pre>

<p>I am receiving the error:</p>

<p>Start-AzureRmDataFactoryV2Trigger : HTTP Status Code: BadRequest
Error Code: BadRequest
Error Message: Trigger FolderTrigger has an unexpected resource provider: , an unexpected resource type , or both.</p>

<p>I can start a trigger of type Microsoft.DataFactory/factories/triggers/ScheduleTrigger.</p>
","<powershell><azure-data-factory>","2019-07-31 05:10:46","122","0","1","58501275","<p>I received the same error.
The problem with me was that I tried to start a trigger with no pipelines connected to it. I found this error by starting the trigger inside the portal and this doesn't seem to work.
by adding the following code around the start Trigger I check first if the trigger has any pipelines before actually starting the trigger.</p>

<pre><code> if ($triggers.properties.Pipelines){}
</code></pre>

<p>PS: triggers. comes from my foreach loop where I loop over every trigger as followed</p>

<pre><code>$triggersADF = Get-AzureRmDataFactoryV2Trigger -DataFactoryName $DataFactoryName -ResourceGroupName $ResourceGroupName


foreach ( $triggers in $triggersADF) {
</code></pre>
"
"57282500","Copy JSON Array data from REST data factory to Azure Blob as is","<p>I have used REST to get data from API and the format of JSON output that contains arrays. When I am trying to copy the JSON as it is using copy activity to BLOB, I am only getting first object data and the rest is ignored.</p>

<p>In the documentation is says we can copy JSON as is by skipping schema section on both dataset and copy activity. I followed the same and I am the getting the output as below.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest#export-json-response-as-is"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-rest#export-json-response-as-is</a></p>

<p>Tried copy activity without schema, using the header as first row and output files to BLOB as .json and .txt</p>

<p>Sample REST output:</p>

<pre><code>{
  ""totalPages"": 500,
  ""firstPage"": true,
  ""lastPage"": false,
  ""numberOfElements"": 50,
  ""number"": 0,
  ""totalElements"": 636,
  ""columns"": {
    ""dimension"": {
      ""id"": ""variables/page"",
      ""type"": ""string""
    },
    ""columnIds"": [
      ""0""
    ]
  },
  ""rows"": [
    {
      ""itemId"": ""1234"",
      ""value"": ""home"",
      ""data"": [
        65
      ]
    },
    {
      ""itemId"": ""1235"",
      ""value"": ""category"",
      ""data"": [
        92
      ]
    },
    ],
  ""summaryData"": {
    ""totals"": [
      157
    ],
    ""col-max"": [
      123
    ],
    ""col-min"": [
      1
    ]
  }
}
</code></pre>

<p>BLOB Output as the text is below: which is only first object data</p>

<pre><code>totalPages,firstPage,lastPage,numberOfElements,number,totalElements
500,True,False,50,0,636
</code></pre>
","<json><azure-data-factory>","2019-07-31 02:51:59","2037","1","1","57303566","<p>If you want to write the JSON response as is, you can use an <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-http"" rel=""nofollow noreferrer"">HTTP connector</a>. However, please note that the HTTP connector doesn't support pagination.</p>

<p>If you want to keep using the REST connector and to write a csv file as output, can you please specify how you want the nested objects and arrays to be written ?</p>

<p>In csv files, we can not write arrays. You could always use a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">custom activity</a> or an <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">azure function activity</a> to call the REST API, parse it the way you want and write to a csv file.</p>

<p>Hope this helps.</p>
"
"57271103","How to scale Azure SQL DW up/down using a Logic app","<p>I know that logic apps can be created to resume/pause Azure SQL data warehouse and the same can be used within Azure Data Factory in a pipeline. I wanted to know if there is any way we can similarly create a logic app and use it in ADF to scale DW units up or down. Reason being, some components of my pipeline require the DW to be run at higher units whereas it can be kept at minimum for certain sections of the pipeline. Any help will be appreciated !</p>
","<azure><azure-sql-database><azure-data-factory><azure-logic-apps>","2019-07-30 11:57:01","1813","2","2","57277914","<p>Azure SQL Data Warehouse can be scaled up/down <a href=""https://learn.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-manage-compute-overview"" rel=""nofollow noreferrer"">via PowerShell, T-SQL, or Rest API</a>. A Logic App can execute a SQL statement, call Azure Functions that scale the DW up/down, or call the API directly. </p>

<p><strong>Option 1: T-SQL.</strong> 
Add a <a href=""https://learn.microsoft.com/en-us/azure/connectors/connectors-create-api-sqlazure#add-sql-action"" rel=""nofollow noreferrer"">t-SQL Action</a>. There is a separate connector for Azure SQL DW (as opposed to DB). Make sure you connect to the <strong>Master</strong> database rather than the user database (this may be a deal breaker depending on your security requirements). Have it execute a query that is something like </p>

<pre><code>ALTER DATABASE mySampleDataWarehouse
MODIFY (SERVICE_OBJECTIVE = 'DW300c');
</code></pre>

<p>You may have to add a wait or a loop for polling because it takes several seconds for the scale to happen and you need to wait for that to finish.</p>

<p><strong>Option 2: Azure Function.</strong> 
Create an <a href=""https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-azure-functions"" rel=""nofollow noreferrer"">Azure Function</a> and use the Logic App to call that. There is a <a href=""https://learn.microsoft.com/en-us/azure/sql-data-warehouse/manage-compute-with-azure-functions#add-a-new-trigger-function"" rel=""nofollow noreferrer"">template available</a> for the Function App to scale the DW up/down that you could potentially copy from. It uses a timer trigger, but you want an HTTP trigger to call from the Logic App. For this to work, you'll need a a Service Principal Account with contributor access under the same subscription as your data warehouse instance.</p>

<p>Edit: I'm not sure what you Data Factory control flow is like, but you could potentially just scale up/down in there rather than using a Logic App. </p>
"
"57271103","How to scale Azure SQL DW up/down using a Logic app","<p>I know that logic apps can be created to resume/pause Azure SQL data warehouse and the same can be used within Azure Data Factory in a pipeline. I wanted to know if there is any way we can similarly create a logic app and use it in ADF to scale DW units up or down. Reason being, some components of my pipeline require the DW to be run at higher units whereas it can be kept at minimum for certain sections of the pipeline. Any help will be appreciated !</p>
","<azure><azure-sql-database><azure-data-factory><azure-logic-apps>","2019-07-30 11:57:01","1813","2","2","57328547","<p>If you do need to achieve this from a Logic App (eg you prefer the easy scheduling from Logic Apps rather than the CRON form in Azure Functions) or Logic Apps are your chosen integration solution, then it is possible.  Simply use a HTTP task with PATCH call and the Azure SQL Data Warehouse REST API which has <code>pause</code> and <code>resume</code> methods and also allows you to pass in a body with the actual service level objective, eg</p>

<p><a href=""https://i.stack.imgur.com/5R3WQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5R3WQ.png"" alt=""Logic App HTTP activity""></a></p>

<p>In my example I used the following settings with Service Principal authentication:</p>

<pre><code>Method:         PATCH
URI:            https://management.azure.com/subscriptions/{subscription-id}/resourceGroups/{resource-group-name}/providers/Microsoft.Sql/servers/{server-name}/databases/{database-name}/pause?api-version=2014-04-01-preview
Headers:        (leave empty)
Queries:        (leave empty)
Body:           
{
""properties"": {
    ""requestedServiceObjectiveName"": ""DW100c""
    }
}
Authentication: Active Directory OAuth
Tenant:         (your tenant)
Audience:       https://management.core.windows.net/
Client ID:      (your client id)
Credential Type: Secret
Secret:         (your secret)
</code></pre>

<p>Set the service objective to whatever you want - my example uses <code>DW100c</code> which is actually part of a scale down operation: 100 is the lowest you can go.  Note the example JSON in the REST API activity is incorrect / not valid JSON.  Simply add quotes for a Gen 2 warehouse you will need to specify ""c"" after the service level objective, eg <code>""DW3000c""</code></p>

<p><a href=""https://i.stack.imgur.com/FhGvB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FhGvB.png"" alt=""Diagram""></a></p>
"
"57267706","Azure data factory pipeline for-each activity not working sequential","<p>I've azure data factory pipeline through which I need to pull my all CSV files from blob storage container and store that files to azure data lake container. Before storing those files to data lake I need to apply some data manipulation on that file's data.</p>

<p>Now I need to do this process sequentially and not parallel. So I use ForEach Activity->Settings->Sequential.</p>

<p>But it not working sequentially and works as a parallel process.</p>

<p><a href=""https://i.stack.imgur.com/RLZqb.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RLZqb.jpg"" alt=""Pipeline main activity panel""></a></p>

<p><a href=""https://i.stack.imgur.com/75ch4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/75ch4.jpg"" alt=""Pipeline foreach activity panel""></a>
Below is the pipeline code</p>

<pre><code>
{
    ""name"":""PN_obfuscate_and_move"",
    ""properties"":{
        ""description"":""move PN blob csv to adlgen2(obfuscated)"",
        ""activities"":[
            {
                ""name"":""GetBlobFileName"",
                ""type"":""GetMetadata"",
                ""dependsOn"":[

                ],
                ""policy"":{
                    ""timeout"":""7.00:00:00"",
                    ""retry"":0,
                    ""retryIntervalInSeconds"":30,
                    ""secureOutput"":false,
                    ""secureInput"":false
                },
                ""userProperties"":[

                ],
                ""typeProperties"":{
                    ""dataset"":{
                        ""referenceName"":""PN_Getblobfilename_Dataset"",
                        ""type"":""DatasetReference""
                    },
                    ""fieldList"":[
                        ""childItems""
                    ],
                    ""storeSettings"":{
                        ""type"":""AzureBlobStorageReadSetting"",
                        ""recursive"":true
                    },
                    ""formatSettings"":{
                        ""type"":""DelimitedTextReadSetting""
                    }
                }
            },
            {
                ""name"":""ForEachBlobFile"",
                ""type"":""ForEach"",
                ""dependsOn"":[
                    {
                        ""activity"":""GetBlobFileName"",
                        ""dependencyConditions"":[
                            ""Succeeded""
                        ]
                    }
                ],
                ""userProperties"":[

                ],
                ""typeProperties"":{
                    ""items"":{
                        ""value"":""@activity('GetBlobFileName').output.childItems"",
                        ""type"":""Expression""
                    },
                    ""isSequential"":true,
                    ""activities"":[
                        {
                            ""name"":""Blob_to_SQLServer"",
                            ""description"":""Copy PN blob files to sql server table"",
                            ""type"":""Copy"",
                            ""dependsOn"":[

                            ],
                            ""policy"":{
                                ""timeout"":""7.00:00:00"",
                                ""retry"":0,
                                ""retryIntervalInSeconds"":30,
                                ""secureOutput"":false,
                                ""secureInput"":false
                            },
                            ""userProperties"":[
                                {
                                    ""name"":""Source"",
                                    ""value"":""PNemailattachment//""
                                },
                                {
                                    ""name"":""Destination"",
                                    ""value"":""[dbo].[PN]""
                                }
                            ],
                            ""typeProperties"":{
                                ""source"":{
                                    ""type"":""DelimitedTextSource"",
                                    ""storeSettings"":{
                                        ""type"":""AzureBlobStorageReadSetting"",
                                        ""recursive"":false,
                                        ""wildcardFileName"":""*.*"",
                                        ""enablePartitionDiscovery"":false
                                    },
                                    ""formatSettings"":{
                                        ""type"":""DelimitedTextReadSetting""
                                    }
                                },
                                ""sink"":{
                                    ""type"":""AzureSqlSink""
                                },
                                ""enableStaging"":false
                            },
                            ""inputs"":[
                                {
                                    ""referenceName"":""PNBlob"",
                                    ""type"":""DatasetReference""
                                }
                            ],
                            ""outputs"":[
                                {
                                    ""referenceName"":""PN_SQLServer"",
                                    ""type"":""DatasetReference""
                                }
                            ]
                        },
                        {
                            ""name"":""Obfuscate_PN_SQLData"",
                            ""description"":""mask specific columns"",
                            ""type"":""SqlServerStoredProcedure"",
                            ""dependsOn"":[
                                {
                                    ""activity"":""Blob_to_SQLServer"",
                                    ""dependencyConditions"":[
                                        ""Succeeded""
                                    ]
                                }
                            ],
                            ""policy"":{
                                ""timeout"":""7.00:00:00"",
                                ""retry"":0,
                                ""retryIntervalInSeconds"":30,
                                ""secureOutput"":false,
                                ""secureInput"":false
                            },
                            ""userProperties"":[

                            ],
                            ""typeProperties"":{
                                ""storedProcedureName"":""[dbo].[Obfuscate_PN_Data]""
                            },
                            ""linkedServiceName"":{
                                ""referenceName"":""PN_SQLServer"",
                                ""type"":""LinkedServiceReference""
                            }
                        },
                        {
                            ""name"":""SQLServer_to_ADLSGen2"",
                            ""description"":""move PN obfuscated data to azure data lake gen2"",
                            ""type"":""Copy"",
                            ""dependsOn"":[
                                {
                                    ""activity"":""Obfuscate_PN_SQLData"",
                                    ""dependencyConditions"":[
                                        ""Succeeded""
                                    ]
                                }
                            ],
                            ""policy"":{
                                ""timeout"":""7.00:00:00"",
                                ""retry"":0,
                                ""retryIntervalInSeconds"":30,
                                ""secureOutput"":false,
                                ""secureInput"":false
                            },
                            ""userProperties"":[

                            ],
                            ""typeProperties"":{
                                ""source"":{
                                    ""type"":""AzureSqlSource""
                                },
                                ""sink"":{
                                    ""type"":""DelimitedTextSink"",
                                    ""storeSettings"":{
                                        ""type"":""AzureBlobFSWriteSetting""
                                    },
                                    ""formatSettings"":{
                                        ""type"":""DelimitedTextWriteSetting"",
                                        ""quoteAllText"":true,
                                        ""fileExtension"":"".csv""
                                    }
                                },
                                ""enableStaging"":false
                            },
                            ""inputs"":[
                                {
                                    ""referenceName"":""PN_SQLServer"",
                                    ""type"":""DatasetReference""
                                }
                            ],
                            ""outputs"":[
                                {
                                    ""referenceName"":""PNADLSGen2"",
                                    ""type"":""DatasetReference""
                                }
                            ]
                        },
                        {
                            ""name"":""Delete_PN_SQLData"",
                            ""description"":""delete all data from table"",
                            ""type"":""SqlServerStoredProcedure"",
                            ""dependsOn"":[
                                {
                                    ""activity"":""SQLServer_to_ADLSGen2"",
                                    ""dependencyConditions"":[
                                        ""Succeeded""
                                    ]
                                }
                            ],
                            ""policy"":{
                                ""timeout"":""7.00:00:00"",
                                ""retry"":0,
                                ""retryIntervalInSeconds"":30,
                                ""secureOutput"":false,
                                ""secureInput"":false
                            },
                            ""userProperties"":[

                            ],
                            ""typeProperties"":{
                                ""storedProcedureName"":""[dbo].[Delete_PN_Data]""
                            },
                            ""linkedServiceName"":{
                                ""referenceName"":""PN_SQLServer"",
                                ""type"":""LinkedServiceReference""
                            }
                        }
                    ]
                }
            }
        ],
        ""folder"":{
            ""name"":""PN""
        },
        ""annotations"":[

        ]
    },
    ""type"":""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>
","<foreach><pipeline><azure-data-factory><sequential>","2019-07-30 08:46:39","3154","1","2","57291565","<p>The <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">ForEach activity</a> in Azure Data Factory (ADF) by default runs up to 20 tasks in parallel.  You can make it run up to 50.  If you want to <strong>force</strong> it to run sequentially, ie one after the other, then you can either set the Sequential checkbox on the Settings section of the ForEach UI (see below) or set the <code>isSequential</code> property of the ForEach activity in the JSON to true, eg</p>

<p><a href=""https://i.stack.imgur.com/4JJzv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4JJzv.png"" alt=""Data Factory UI""></a></p>

<pre><code>{
    ""name"": ""&lt;MyForEachPipeline&gt;"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""&lt;MyForEachActivity&gt;"",
                ""type"": ""ForEach"",
                ""typeProperties"": {
                    ""isSequential"": ""true"",
                    ""items"": {
...
</code></pre>

<p>I would caution the use of this setting though.  Running things in serial, ie one after the other will slow things down.  Is there another way you can design your workflow to take advantage of this really powerful feature of Azure Data Factory?  Then your job will only take as long as the longest task(s), rather than the cumulative total of all tasks together.</p>

<p>Let's say I have a job to run which has 10 tasks each taking 1 second.  If I run this job in serial it will take 10 seconds, but if I run it in parallel it will take 1 second.</p>

<p>SSIS never really had this - you could either manually create multiple paths or maybe use third-party components but it wasn't built in.  It's really a superb feature of ADF you should try and take advantage of.  There may of course be occasions where you really do need to run in serial which is why this option is available.</p>
"
"57267706","Azure data factory pipeline for-each activity not working sequential","<p>I've azure data factory pipeline through which I need to pull my all CSV files from blob storage container and store that files to azure data lake container. Before storing those files to data lake I need to apply some data manipulation on that file's data.</p>

<p>Now I need to do this process sequentially and not parallel. So I use ForEach Activity->Settings->Sequential.</p>

<p>But it not working sequentially and works as a parallel process.</p>

<p><a href=""https://i.stack.imgur.com/RLZqb.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RLZqb.jpg"" alt=""Pipeline main activity panel""></a></p>

<p><a href=""https://i.stack.imgur.com/75ch4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/75ch4.jpg"" alt=""Pipeline foreach activity panel""></a>
Below is the pipeline code</p>

<pre><code>
{
    ""name"":""PN_obfuscate_and_move"",
    ""properties"":{
        ""description"":""move PN blob csv to adlgen2(obfuscated)"",
        ""activities"":[
            {
                ""name"":""GetBlobFileName"",
                ""type"":""GetMetadata"",
                ""dependsOn"":[

                ],
                ""policy"":{
                    ""timeout"":""7.00:00:00"",
                    ""retry"":0,
                    ""retryIntervalInSeconds"":30,
                    ""secureOutput"":false,
                    ""secureInput"":false
                },
                ""userProperties"":[

                ],
                ""typeProperties"":{
                    ""dataset"":{
                        ""referenceName"":""PN_Getblobfilename_Dataset"",
                        ""type"":""DatasetReference""
                    },
                    ""fieldList"":[
                        ""childItems""
                    ],
                    ""storeSettings"":{
                        ""type"":""AzureBlobStorageReadSetting"",
                        ""recursive"":true
                    },
                    ""formatSettings"":{
                        ""type"":""DelimitedTextReadSetting""
                    }
                }
            },
            {
                ""name"":""ForEachBlobFile"",
                ""type"":""ForEach"",
                ""dependsOn"":[
                    {
                        ""activity"":""GetBlobFileName"",
                        ""dependencyConditions"":[
                            ""Succeeded""
                        ]
                    }
                ],
                ""userProperties"":[

                ],
                ""typeProperties"":{
                    ""items"":{
                        ""value"":""@activity('GetBlobFileName').output.childItems"",
                        ""type"":""Expression""
                    },
                    ""isSequential"":true,
                    ""activities"":[
                        {
                            ""name"":""Blob_to_SQLServer"",
                            ""description"":""Copy PN blob files to sql server table"",
                            ""type"":""Copy"",
                            ""dependsOn"":[

                            ],
                            ""policy"":{
                                ""timeout"":""7.00:00:00"",
                                ""retry"":0,
                                ""retryIntervalInSeconds"":30,
                                ""secureOutput"":false,
                                ""secureInput"":false
                            },
                            ""userProperties"":[
                                {
                                    ""name"":""Source"",
                                    ""value"":""PNemailattachment//""
                                },
                                {
                                    ""name"":""Destination"",
                                    ""value"":""[dbo].[PN]""
                                }
                            ],
                            ""typeProperties"":{
                                ""source"":{
                                    ""type"":""DelimitedTextSource"",
                                    ""storeSettings"":{
                                        ""type"":""AzureBlobStorageReadSetting"",
                                        ""recursive"":false,
                                        ""wildcardFileName"":""*.*"",
                                        ""enablePartitionDiscovery"":false
                                    },
                                    ""formatSettings"":{
                                        ""type"":""DelimitedTextReadSetting""
                                    }
                                },
                                ""sink"":{
                                    ""type"":""AzureSqlSink""
                                },
                                ""enableStaging"":false
                            },
                            ""inputs"":[
                                {
                                    ""referenceName"":""PNBlob"",
                                    ""type"":""DatasetReference""
                                }
                            ],
                            ""outputs"":[
                                {
                                    ""referenceName"":""PN_SQLServer"",
                                    ""type"":""DatasetReference""
                                }
                            ]
                        },
                        {
                            ""name"":""Obfuscate_PN_SQLData"",
                            ""description"":""mask specific columns"",
                            ""type"":""SqlServerStoredProcedure"",
                            ""dependsOn"":[
                                {
                                    ""activity"":""Blob_to_SQLServer"",
                                    ""dependencyConditions"":[
                                        ""Succeeded""
                                    ]
                                }
                            ],
                            ""policy"":{
                                ""timeout"":""7.00:00:00"",
                                ""retry"":0,
                                ""retryIntervalInSeconds"":30,
                                ""secureOutput"":false,
                                ""secureInput"":false
                            },
                            ""userProperties"":[

                            ],
                            ""typeProperties"":{
                                ""storedProcedureName"":""[dbo].[Obfuscate_PN_Data]""
                            },
                            ""linkedServiceName"":{
                                ""referenceName"":""PN_SQLServer"",
                                ""type"":""LinkedServiceReference""
                            }
                        },
                        {
                            ""name"":""SQLServer_to_ADLSGen2"",
                            ""description"":""move PN obfuscated data to azure data lake gen2"",
                            ""type"":""Copy"",
                            ""dependsOn"":[
                                {
                                    ""activity"":""Obfuscate_PN_SQLData"",
                                    ""dependencyConditions"":[
                                        ""Succeeded""
                                    ]
                                }
                            ],
                            ""policy"":{
                                ""timeout"":""7.00:00:00"",
                                ""retry"":0,
                                ""retryIntervalInSeconds"":30,
                                ""secureOutput"":false,
                                ""secureInput"":false
                            },
                            ""userProperties"":[

                            ],
                            ""typeProperties"":{
                                ""source"":{
                                    ""type"":""AzureSqlSource""
                                },
                                ""sink"":{
                                    ""type"":""DelimitedTextSink"",
                                    ""storeSettings"":{
                                        ""type"":""AzureBlobFSWriteSetting""
                                    },
                                    ""formatSettings"":{
                                        ""type"":""DelimitedTextWriteSetting"",
                                        ""quoteAllText"":true,
                                        ""fileExtension"":"".csv""
                                    }
                                },
                                ""enableStaging"":false
                            },
                            ""inputs"":[
                                {
                                    ""referenceName"":""PN_SQLServer"",
                                    ""type"":""DatasetReference""
                                }
                            ],
                            ""outputs"":[
                                {
                                    ""referenceName"":""PNADLSGen2"",
                                    ""type"":""DatasetReference""
                                }
                            ]
                        },
                        {
                            ""name"":""Delete_PN_SQLData"",
                            ""description"":""delete all data from table"",
                            ""type"":""SqlServerStoredProcedure"",
                            ""dependsOn"":[
                                {
                                    ""activity"":""SQLServer_to_ADLSGen2"",
                                    ""dependencyConditions"":[
                                        ""Succeeded""
                                    ]
                                }
                            ],
                            ""policy"":{
                                ""timeout"":""7.00:00:00"",
                                ""retry"":0,
                                ""retryIntervalInSeconds"":30,
                                ""secureOutput"":false,
                                ""secureInput"":false
                            },
                            ""userProperties"":[

                            ],
                            ""typeProperties"":{
                                ""storedProcedureName"":""[dbo].[Delete_PN_Data]""
                            },
                            ""linkedServiceName"":{
                                ""referenceName"":""PN_SQLServer"",
                                ""type"":""LinkedServiceReference""
                            }
                        }
                    ]
                }
            }
        ],
        ""folder"":{
            ""name"":""PN""
        },
        ""annotations"":[

        ]
    },
    ""type"":""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>
","<foreach><pipeline><azure-data-factory><sequential>","2019-07-30 08:46:39","3154","1","2","70258556","<p>I've had something similar.</p>
<p>I've only just started with ADF so could be wrong but I noticed that by default, there is a batch size set on the for each activity. I made sure to set this to 1 <strong>as well as</strong> setting the sequential checkbox and now the inner activities are running in the order I expected.</p>
<p>For anyone else facing this issue:</p>
<ol>
<li>Unchecking the sequential checkbox then type 1 for the batch limit</li>
<li>Recheck the box. Note that I'm using the designer and can't see anything in the code view relating to batch size so maybe it's hidden.</li>
</ol>
"
"57264676","Azure Data Factory UI - Author and Monitor does not load","<p>Clicking on Author and Monitor for Azure Data Factory V2 does not load the UI. It keeps saying Loading..... and nothing happens.</p>

<p>I tried clearing browser cache, history.
Tried different browsers.
Tried directly logging into ADF</p>
","<azure><azure-data-factory>","2019-07-30 05:10:57","817","1","1","57266417","<p>Firstly,please see this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-portal#create-a-data-factory"" rel=""nofollow noreferrer"">Link</a>: Currently, Data Factory UI is supported only in Microsoft Edge and Google Chrome web browsers.</p>

<p>I often experience this issue but not always.The azure data factory UI keep loading or  keep asking me to re-enter my account information. I fixed it by using <code>incognito</code> mode in the browser.</p>

<p>You could find many clues from this <a href=""https://social.msdn.microsoft.com/Forums/en-US/26dd2244-efa2-4225-a32f-2f68d3a60139/data-factory-ui-is-not-loading?forum=AzureDataFactory"" rel=""nofollow noreferrer"">MSDN case</a>. </p>

<hr>

<p>Updates at 30.DEC.2019:</p>

<p>The UI started working after couple of days without any changes.</p>
"
"57261025","How to decompress a zip file in Azure Data Factory v2","<p>I'm trying to decompress a zip file (with multiple files inside) using Azure Data Factory v2. The zip file is located in Azure File Storage. The ADF Copy task just copies the original zip file without decompressing it. Any suggestion on how to make this work?</p>

<p>This is the current configuration:</p>

<ol>
<li>The zip file source was setup as a binary dataset with Compression Type = ZipDeflate. </li>
<li>The target folder was also setup as a binary dataset but with Compression Type = None.</li>
<li>A pipeline with a single Copy task was created to move files from zip file to target folder.</li>
</ol>
","<zip><unzip><azure-data-factory>","2019-07-29 20:34:55","21971","8","2","60460555","<p>This can be achieved by having a setting ""ZipDeflate"" compression type in your source data set and in the sink data set of Copy activity you don't need to specify any compression configuration (Compression type is ""none"").</p>

<p><a href=""https://i.stack.imgur.com/jIL2q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jIL2q.png"" alt=""enter image description here""></a></p>

<p>In the Copy activity sink settings, please set the copy behavior to ""Flatten Hierarchy"" to unzip and write the individual files.  </p>

<p><a href=""https://i.stack.imgur.com/dt7lK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dt7lK.png"" alt=""enter image description here""></a></p>

<p>When the Copy behavior is set to ""Flatten Hierarchy"", all the files from zipped source file are extracted and written to destination folder mentioned in the sink dataset as individual files by renaming the files to data_SomeGUID.csv.</p>

<p>In case if you do not specify the copy behavior (set to ""none"") in copy activity, then it decompress ZipDeflate file(s) and write to file-based sink data store, files will be extracted to the folder: //. </p>

<p>Please refer to this doc to know about the Compression support in Azure data factory: <a href=""https://learn.microsoft.com/azure/data-factory/supported-file-formats-and-compression-codecs-legacy#compression-support"" rel=""nofollow noreferrer"">https://learn.microsoft.com/azure/data-factory/supported-file-formats-and-compression-codecs-legacy#compression-support</a></p>
"
"57261025","How to decompress a zip file in Azure Data Factory v2","<p>I'm trying to decompress a zip file (with multiple files inside) using Azure Data Factory v2. The zip file is located in Azure File Storage. The ADF Copy task just copies the original zip file without decompressing it. Any suggestion on how to make this work?</p>

<p>This is the current configuration:</p>

<ol>
<li>The zip file source was setup as a binary dataset with Compression Type = ZipDeflate. </li>
<li>The target folder was also setup as a binary dataset but with Compression Type = None.</li>
<li>A pipeline with a single Copy task was created to move files from zip file to target folder.</li>
</ol>
","<zip><unzip><azure-data-factory>","2019-07-29 20:34:55","21971","8","2","65729238","<p>If you don't want to lose the names of the files within your zip, use the Copy activity but set the Copy Behavior to &quot;Preserve hierarchy&quot;. This will create a folder with the name of your zip file, and the files will be inside with their original names.</p>
<p><a href=""https://i.stack.imgur.com/vT9ug.png"" rel=""nofollow noreferrer"">Zip Copy Behavior</a></p>
"
"57260192","Cannot transfer a large 30 GB SQL table from a client SQL Server machine to my Azure Data Lake Gen2 as a 530 MB Parquet File","<p>I cannot copy a 30 GB SQL Server table from this machine to my Azure Data Lake Gen 2 storage account as a 530 MB Parquet file using Azure Data Factory. The compression type is gzip. The Throughput is 11.8 MB/s</p>

<p>The copy detail:
<a href=""https://i.stack.imgur.com/0MTkN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0MTkN.png"" alt=""enter image description here""></a></p>

<p>The failed ADF copy error message is:</p>

<pre><code>{ ""errorCode"": ""2200"", ""message"": ""Failure happened on 'Sink' side. ErrorCode=UserErrorFailedBlobFSOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=BlobFS operation failed for: A task was canceled.. Account: &amp;apos;datalake&amp;apos;. FileSystem: &amp;aposcontainer-dl&amp;apos;. Path: &amp;apos;ImportLayer/F61ILBarAcct_Txns.parquet&amp;apos;.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Threading.Tasks.TaskCanceledException,Message=A task was canceled.,Source=mscorlib,'"", ""failureType"": ""UserError"", ""target"": ""Copy Latest Source Data"" }
</code></pre>

<p>It is the same on the client integration runtime log</p>

<pre><code>DEBUG:
TraceComponentId: TransferClientLibrary
TraceMessageId: BlobFSOperationRetry
@logId: Warning
jobId: c063e070-cc12-4cae-895f-f8ada2bfa3ff
activityId: ecfa652d-8471-4297-be2a-4ecc0ebc89c5
eventId: BlobFSOperationRetry
message: 'Type=System.Threading.Tasks.TaskCanceledException,Message=A task was canceled.,Source=mscorlib,StackTrace=   at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task)
   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
   at Microsoft.Azure.Storage.Data.AzureDfsClient.&amp;lt;UpdatePathWithHttpMessagesAsync&amp;gt;d__41.MoveNext()
--- End of stack trace from previous location where exception was thrown ---
   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()
   at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task)
   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
   at Microsoft.Azure.Storage.Data.AzureDfsClientExtensions.&amp;lt;UpdatePathAsync&amp;gt;d__24.MoveNext()
--- End of stack trace from previous location where exception was thrown ---
   at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()
   at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task)
   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
   at Microsoft.Azure.Storage.Data.AzureDfsClientExtensions.UpdatePath(IAzureDfsClient operations&amp;#44; String action&amp;#44; String filesystem&amp;#44; String path&amp;#44; Nullable`1 position&amp;#44; Nullable`1 retainUncommittedData&amp;#44; String contentLength&amp;#44; String xMsLeaseAction&amp;#44; String xMsLeaseId&amp;#44; String xMsCacheControl&amp;#44; String xMsContentType&amp;#44; String xMsContentDisposition&amp;#44; String xMsContentEncoding&amp;#44; String xMsContentLanguage&amp;#44; String xMsProperties&amp;#44; String ifMatch&amp;#44; String ifNoneMatch&amp;#44; String ifModifiedSince&amp;#44; String ifUnmodifiedSince&amp;#44; Stream requestBody&amp;#44; String xMsClientRequestId&amp;#44; Nullable`1 timeout&amp;#44; String xMsDate)
   at Microsoft.Azure.Storage.Data.BlobFSClient.&amp;lt;&amp;gt;c__DisplayClass37_0.&amp;lt;AppendFile&amp;gt;b__1()
   at Microsoft.Rest.TransientFaultHandling.RetryPolicy.&amp;lt;&amp;gt;c__DisplayClass16_0.&amp;lt;ExecuteAction&amp;gt;b__0()
   at Microsoft.Rest.TransientFaultHandling.RetryPolicy.ExecuteAction[TResult](Func`1 func),'
</code></pre>

<p>On the client machine, the cpu is a Intel Xeon E7-2830 @2.13Ghz, 64bit OS. It has 16.0 GB of Ram. It has a 40 GB Hard drive with 10 GB free space. I increase the virtual memory max to 10 GB so it can use the free space. For the Java pption, I set -Xmx, Java max heap memory, to 26 GB to take advantage of that. I can only use this client machine which has the Integration Runtime installed on it. </p>

<p>What could be the problem?</p>
","<sql-server-2008><bigdata><parquet><azure-data-factory><azure-data-lake>","2019-07-29 19:19:53","316","1","1","57528139","<p>I manage  to solve it by using the compression type snappy instead of gzip. It uses less processing power.</p>

<p>In addition, i ran the copy one at a time instead of many at once. It is slower but safer</p>
"
"57256158","Azure Data Factory Copy Data Activity SQL Sink stored procedure and table-typed parameter in ARM template","<p>I am deploying an Azure Data Factory pipeline that contains a Copy Data activity, where the sink is a SQL Server stored procedure.  I supply the stored procedure name, which maps to the sqlWriterStoredProcedureName in the ARM Template for the data factory.  Recently the interface for setting up the sink in a Copy Data activity changed to include a Table Type Parameter Name, which should map to the TableType (sqlWriterTableType).</p>

<p><a href=""https://i.stack.imgur.com/NHPbN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NHPbN.png"" alt=""enter image description here""></a></p>

<p>What property should I use to set the Table Type Parameter Name?  I'm guessing it would be something like sqlWriterTableTypeParameter or sqlWriterTableTypeParameterName.</p>

<p>Can anyone point me in the right direction?  </p>
","<azure-data-factory>","2019-07-29 14:42:49","4749","2","3","57258898","<p>Have you tried if its the same as the dataset type? SqlServerTable. As seen here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#dataset-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#dataset-properties</a> </p>

<p>Scrolling down a bit in the documentation appeared this: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#invoke-a-stored-procedure-from-a-sql-sink"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#invoke-a-stored-procedure-from-a-sql-sink</a> so I would also try with sqlWriterTableType.</p>

<p>Hope this helped!!</p>
"
"57256158","Azure Data Factory Copy Data Activity SQL Sink stored procedure and table-typed parameter in ARM template","<p>I am deploying an Azure Data Factory pipeline that contains a Copy Data activity, where the sink is a SQL Server stored procedure.  I supply the stored procedure name, which maps to the sqlWriterStoredProcedureName in the ARM Template for the data factory.  Recently the interface for setting up the sink in a Copy Data activity changed to include a Table Type Parameter Name, which should map to the TableType (sqlWriterTableType).</p>

<p><a href=""https://i.stack.imgur.com/NHPbN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NHPbN.png"" alt=""enter image description here""></a></p>

<p>What property should I use to set the Table Type Parameter Name?  I'm guessing it would be something like sqlWriterTableTypeParameter or sqlWriterTableTypeParameterName.</p>

<p>Can anyone point me in the right direction?  </p>
","<azure-data-factory>","2019-07-29 14:42:49","4749","2","3","57335411","<p>I figured this out by leaving the column blank and seeing what ADF gave as the error.</p>

<p>If you have a stored procedure like this:</p>

<pre><code>              CREATE PROCEDURE [dbo].[spFoo]
                    @FooType [dbo].[FooType] READONLY
                AS ...
</code></pre>

<p>Then your variables are:</p>

<pre><code>Table Type: ""FooType""
Table type parameter name: ""FooType"" (without the @ symbol)
</code></pre>

<p>ADF uses the @ symbol as a reserved word, so just remove that from your input variables and it works as expected.</p>
"
"57256158","Azure Data Factory Copy Data Activity SQL Sink stored procedure and table-typed parameter in ARM template","<p>I am deploying an Azure Data Factory pipeline that contains a Copy Data activity, where the sink is a SQL Server stored procedure.  I supply the stored procedure name, which maps to the sqlWriterStoredProcedureName in the ARM Template for the data factory.  Recently the interface for setting up the sink in a Copy Data activity changed to include a Table Type Parameter Name, which should map to the TableType (sqlWriterTableType).</p>

<p><a href=""https://i.stack.imgur.com/NHPbN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NHPbN.png"" alt=""enter image description here""></a></p>

<p>What property should I use to set the Table Type Parameter Name?  I'm guessing it would be something like sqlWriterTableTypeParameter or sqlWriterTableTypeParameterName.</p>

<p>Can anyone point me in the right direction?  </p>
","<azure-data-factory>","2019-07-29 14:42:49","4749","2","3","57359345","<p>MSFT support finally got back to me.  The property to be set in the ARM template that defines the data factory is ""storedProcedureTableTypeParameterName""</p>

<p>Using kirikintha's foo example:</p>

<pre><code>CREATE PROCEDURE [dbo].[spFoo]
   @FooTypeVariable [dbo].[FooType] READONLY
AS
...
</code></pre>

<p>When calling a SQL stored procedure as a data factory copy activity sink, the ARM template would look like this:</p>

<pre><code>...

    ""sink"": {
     ""type"": ""AzureSqlSink"",
     ""sqlWriterStoredProcedureName"": ""[[dbo].[spFoo]"",
     ""sqlWriterTableType"": ""[[dbo].[FooType]"",
     ""storedProcedureTableTypeParameterName"": ""FooTypeVariable""
     }

...
</code></pre>
"
"57239876","Include dynamic property as source column in ADFv2 copy activity within foreach","<p>I have a Copy activity with an OData endpoint as the source, and an Azure SQL Database table as the sink.</p>

<p>This copy activity is within a Foreach activity, with the items coming from another table. I am using the items iterated through the <code>foreach</code>, within the URL as a query parameter with <code>@item().Name</code> (where Name is the column of the item that I need).</p>

<p>Now, the sink of the copy activity has an extra column <code>Name</code> in which I want to write the current <code>@item().Name</code> in each record being copied. So for all records retrieved for endpoint with <code>Name=X</code> I want to populate the <code>Name</code> with <code>X</code>, etc.</p>

<p>However, when I am in the Copy Activity mapping, I cannot find any way to include the current item along with the other columns coming from the OData feed.</p>

<p>Is there a way to populate one column of the sink from an activity variable and the rest of the columns from the source data? </p>
","<azure-data-factory>","2019-07-28 10:11:56","669","1","1","57258824","<p>The easy way will be inserting the data first into a database, then you could use something as explained here: <a href=""https://stackoverflow.com/questions/57007733/is-it-possible-to-add-a-column-with-specific-value-to-data-in-azure-data-factory/57008208"">Is it possible to add a column with specific value to data in azure data factory pipeline</a> where you end up using a select statement to add the new column.</p>

<p>If you dont, you may try using one of the functions within data flow. Then you can add @item().name as a variable and join/merge the original data with this new column.</p>

<p>Hope this helped!!</p>
"
"57222099","how to merge two csv files in azure data factory","<p>I want to update the Target csv file (Located in Azure Data Lake Store) with delta records updated every day (delta file sit in blob). If existed record updated, then I want to update the same in Target file or if the delta records is new one, then want to append that records to Target CSV file in azure data lake store. I want to implement this using Azure Data Factory, preferably using ADF Data flow.  </p>

<p>I am trying to do this using Azure Data Factory Data Flow Task, but I observed it is possible to create new target file post the merge but couldn't able to update the existed file. </p>

<p>Please let me know if any powershell or any other way if we can update the target file</p>
","<azure-data-lake><azure-data-factory>","2019-07-26 14:27:40","3345","3","1","57225119","<p>We have a sample template that shows you how to update an existing file from a new file using ADF Data Flows. The file type is Parquet, but will work for CSV as well.</p>

<p>Go to New > Pipeline from Template and look for ""Parquet CRUD Operations"". You can open up that Data Flow to see how it's done.</p>
"
"57220308","Set-AzDataFactoryV2Pipeline cmdlet deploys new pipeline with modified JSON tag values","<p>I am using Powershell to deploy a pipeline to an Azure Data Factory V2. I am passing a file with the JSON of the pipeline to be deployed. </p>

<p>But, the new pipeline created in the target ADF has a slightly modified JSON compared to the one passed as input. The source type tag under Lookup Activity has the value 'CopySink' instead of  the value 'AzureSqlSink' specified in the input JSON file. Thus, Data Factory finds the deployed pipeline invalid. </p>

<p>The pipeline runs alright when I manually correct the tag values using the GUI though.  </p>

<p>I have tried the below cmdlets. Both of them seem to have the same outcome. </p>

<pre><code>Set-AzDataFactoryV2Pipeline -ResourceGroupName $DataFactoryResourceGroup -Name $svc.name -DataFactoryName $DataFactoryName -File ""$currentPipelinePath"" -Force      

New-AzDataFactoryV2Pipeline -ResourceGroupName $DataFactoryResourceGroup -Name $svc.name -DataFactoryName $DataFactoryName -File ""$currentPipelinePath"" -Force
</code></pre>

<p>Appreciate any help on this issue. My intention is to automate deployment of ADF pipelines using Powershell. </p>
","<powershell><azure-data-factory>","2019-07-26 12:41:09","191","1","1","57785982","<p>This worked when I executed the command from the Admin Powershell console after upgrading all Az modules to their latest versions.</p>
"
"57214984","Back End Of Azure Data Factory?","<p>A lot of information is saved internally when we create pipelines and publish in Azure Data Factory. When we create pipelines and Trigger them, the history is saved somewhere. Where exactly and how can we access it. I am not referring to the Monitor page but where that page gets the data ?
May be I missed it in the documentation and am unable to get hold of it.</p>

<p>Thanks
- Akshay</p>
","<azure-data-factory>","2019-07-26 07:19:14","1197","0","3","57216766","<p>All the information you could get is what <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-visually"" rel=""nofollow noreferrer"">monitor</a> feature shows for you in ADF. That's the built-in feature.</p>

<p><a href=""https://i.stack.imgur.com/5N4kG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5N4kG.png"" alt=""enter image description here""></a></p>

<p>If you do want to know what place these data reside exactly,you need to persist them. Please refer to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor#persist-data-factory-data"" rel=""nofollow noreferrer"">statement</a> in this document:</p>

<blockquote>
  <p>Data Factory only stores pipeline run data for 45 days. If you want to
  persist pipeline run data for more than 45 days, using Azure Monitor,
  you cannot only route diagnostic logs for analysis, you can persist
  them into a storage account so you have factory information for the
  duration of your choosing.</p>
</blockquote>

<p>You could store the log information with Diagnostic Log.Please see this <a href=""https://azure.microsoft.com/en-us/updates/adf-logs-in-dedicated-tables/"" rel=""nofollow noreferrer"">link</a> or this third-party <a href=""https://www.datadoghq.com/dg/logs/azure-data-factory-logs/?utm_source=Advertisement&amp;utm_medium=GoogleAdsNon1stTierLogs&amp;utm_campaign=GoogleAdsNon1stTierLogs-AzureData&amp;utm_content=Logs&amp;utm_keyword=%2Bdata%20%2Bfactory%20%2Blog&amp;utm_matchtype=b&amp;gclid=Cj0KCQjwyerpBRD9ARIsAH-ITn_Hd4x1CCZyH8-6bfl9N6eCRGAmu-zjKurvtC_OtH9uzlpp_NldIbIaAvs9EALw_wcB"" rel=""nofollow noreferrer"">management tool</a>.</p>
"
"57214984","Back End Of Azure Data Factory?","<p>A lot of information is saved internally when we create pipelines and publish in Azure Data Factory. When we create pipelines and Trigger them, the history is saved somewhere. Where exactly and how can we access it. I am not referring to the Monitor page but where that page gets the data ?
May be I missed it in the documentation and am unable to get hold of it.</p>

<p>Thanks
- Akshay</p>
","<azure-data-factory>","2019-07-26 07:19:14","1197","0","3","57217337","<p>We can see the all the operations log from the Azure Data Factory Active log:
<a href=""https://i.stack.imgur.com/sa2v0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sa2v0.png"" alt=""enter image description here""></a></p>

<p>I'm not sure where the log stored, but you can download ADF logs with PowerShell.</p>

<p>You can run <a href=""https://learn.microsoft.com/en-us/powershell/module/azurerm.datafactories/save-azurermdatafactorylog?view=azurermps-6.13.0"" rel=""nofollow noreferrer""><code>Save-AzureRmDataFactoryLog</code></a> cmdlet with Id value you see from the output and download the log files using the -DownloadLogsoption for the cmdlet.</p>

<p>For example:</p>

<pre><code> Save-AzureRmDataFactoryLog -ResourceGroupName ""ADF"" -DataFactoryName ""LogProcessingFactory"" -Id ""841b77c9-d56c-48d1-99a3-8c16c3e77d39"" -DownloadLogs -Output ""C:\Test""
</code></pre>

<p>Hope this helps.</p>
"
"57214984","Back End Of Azure Data Factory?","<p>A lot of information is saved internally when we create pipelines and publish in Azure Data Factory. When we create pipelines and Trigger them, the history is saved somewhere. Where exactly and how can we access it. I am not referring to the Monitor page but where that page gets the data ?
May be I missed it in the documentation and am unable to get hold of it.</p>

<p>Thanks
- Akshay</p>
","<azure-data-factory>","2019-07-26 07:19:14","1197","0","3","57378825","<p>The Azure Data Factory pipeline run metadata is stored at Azure Data Factory web server database, which is accessible via Azure SDKs. For example, if you can use Python, you can create a data factory Python client and extract pipeline runs/activity runs metadata. You can then parse the response metedata into a pandas dataframe and write to a database table.</p>

<p>The below function can be used to do so.</p>

<pre><code>def adf_logging():
    # Azure subscription ID
    subscription_id = ''
    # Azure resource group
    rg_name = ''
    # The data factory name. It must be globally unique.
    df_name = ''
    credentials = ServicePrincipalCredentials(
                                client_id='',
                                secret='',
                                tenant='')
    # Create a data factory client.
    adf_client = DataFactoryManagementClient(credentials, subscription_id)
    filter_parameters = {""lastUpdatedAfter"": ""2019-08-06T00:36:44.3345758Z"",
                         ""lastUpdatedBefore"": ""2019-08-07T00:49:48.3686473Z""}
    # Extract last 24 hours pipeline run metadata.
    pipeline_runs = adf_client.pipeline_runs.query_by_factory(rg_name, df_name, filter_parameters)
    # Iterate and parse metadata, get log stats.
    cols = ['Pipeline_Name', 'Pipeline_Run_Id', 'Pipeline_Run_Status', 
            'Activity_Name', 'Activity_Start_Time', 'Activity_End_Time', 
            'Activity_Run_Status', 'Data_Read',
            'Data_Written', 'Rows_Read', 'Rows_Written', 'Copy_Duration', 
             'Error_Message']
    df_stats = pd.DataFrame(columns=cols)
    for i in pipeline_runs.as_dict()['value']:
        pipeline_name = i['pipeline_name']
        run_id = i['run_id']
        pipeline_run_status = i['status']
        activity_runs = list(adf_client.activity_runs.list_by_pipeline_run(
                                              rg_name, 
                                              df_name, run_id,                                                                 
                                              datetime.now() - timedelta(1),
                                              datetime.now() + timedelta(1)))
        # Parse activity run metadata.
        for activity_run in activity_runs:
            if re.match(r""Lookup_[A-Z]"", activity_run.activity_name):
                pass
            elif re.match(r""If_[A-Z]"", activity_run.activity_name):
                pass
            elif re.match(r""ForEach_[A-Z]"", activity_run.activity_name):
                pass
            elif re.match(r""Success [A-Z]"", activity_run.activity_name):
                pass
            else:
                activity_name = activity_run.activity_name
                start_time = activity_run.activity_run_start
                start_time = start_time.replace(tzinfo=None)
                end_time = activity_run.activity_run_end
                end_time = end_time.replace(tzinfo=None)
                activity_run_status = activity_run.status
                data_read = activity_run.output['dataRead'] / (1000*1000)
                data_write = activity_run.output['dataWritten'] / (1000*1000)
                rows_read = activity_run.output['rowsRead']
                rows_write = activity_run.output['rowsCopied']
                copy_duration = activity_run.output['copyDuration']
                error_message = activity_run.output['errors']

                data = {'Pipeline_Name': pipeline_name, 'Pipeline_Run_Id':run_id,
                        'Pipeline_Run_Status': pipeline_run_status,
                        'Activity_Name': activity_name, 
                        'Activity_Start_Time': start_time,
                        'Activity_End_Time': end_time, 
                        'Activity_Run_Status': activity_run_status,
                        'Data_Read': data_read, 'Data_Written': data_write,
                        'Rows_Read': rows_read, 'Rows_Written': rows_write,
                        'Copy_Duration': copy_duration, 
                        'Error_Message': error_message}

                df = pd.DataFrame([data])
                df_stats = df_stats.append(df)
</code></pre>
"
"57210351","Azure Data flow taking 7 mins to process 3 records","<p>The issue i am facing is that Azure Data flow taking around 7 mins to process 3 records </p>

<p>Scenario it have two data source CSV files which is joining and then saving it in the Database but it is taking <strong>7 min some time 8 mins</strong> i don't know why its taking so much time can anyone help me out how to tune it </p>

<p><a href=""https://i.stack.imgur.com/j0vNf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j0vNf.png"" alt=""enter image description here""></a>  </p>
","<azure><azure-data-factory>","2019-07-25 21:28:54","139","-2","2","57210731","<p>Currently, Mapping Data Flow has a long spin-up time.  This means that those 7 minutes were creating compute resources.  The actual processing time is closer to 3 seconds or less.  There is work underway to alleviate this pain point.</p>

<p>You should be able to verify this in the monitoring section.</p>
"
"57210351","Azure Data flow taking 7 mins to process 3 records","<p>The issue i am facing is that Azure Data flow taking around 7 mins to process 3 records </p>

<p>Scenario it have two data source CSV files which is joining and then saving it in the Database but it is taking <strong>7 min some time 8 mins</strong> i don't know why its taking so much time can anyone help me out how to tune it </p>

<p><a href=""https://i.stack.imgur.com/j0vNf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j0vNf.png"" alt=""enter image description here""></a>  </p>
","<azure><azure-data-factory>","2019-07-25 21:28:54","139","-2","2","57267803","<p>To execute your data flow without waiting to provision compute resources, use the pipeline debug feature with the data flow debug session turned on. That will pre-warm Databricks for you and keep it warm for 60 minutes.</p>
"
"57195977","How to keep folder structure in blob storage when using Data Factory to copy file","<p>I want to copy some files in Blob. My blob Storage has a structure like this:</p>

<pre><code>test(blob name)
       A(folder)
           A1.csv
           A2.csv
       B
           B1.csv
           B2.csv
</code></pre>

<p>However, when I try to use the copy activity in Data Factory, I lose my folder A, B and my result is:</p>

<pre><code>test(blob name)
     A1.csv
     A2.csv         
     B1.csv
     B2.csv
</code></pre>

<p>So, how can I keep A, B and folder tree?</p>

<p>Thanks for your help,    </p>
","<azure><azure-blob-storage><azure-data-factory>","2019-07-25 06:55:06","1009","0","2","57208006","<p>Can you check your copy activity? In the sink tab it should be like this:</p>

<p><a href=""https://i.stack.imgur.com/ybCOG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ybCOG.png"" alt=""Copy behavior""></a></p>

<p>You probable have selected ""Flatten hierarchy"".</p>

<p>Hope this helped!</p>
"
"57195977","How to keep folder structure in blob storage when using Data Factory to copy file","<p>I want to copy some files in Blob. My blob Storage has a structure like this:</p>

<pre><code>test(blob name)
       A(folder)
           A1.csv
           A2.csv
       B
           B1.csv
           B2.csv
</code></pre>

<p>However, when I try to use the copy activity in Data Factory, I lose my folder A, B and my result is:</p>

<pre><code>test(blob name)
     A1.csv
     A2.csv         
     B1.csv
     B2.csv
</code></pre>

<p>So, how can I keep A, B and folder tree?</p>

<p>Thanks for your help,    </p>
","<azure><azure-blob-storage><azure-data-factory>","2019-07-25 06:55:06","1009","0","2","57208083","<p>Assuming you are using one copy activity to copy all files to the target folder, make sure that the copyBehavior property of your copy activity is set to PreserveHierarchy.  </p>

<p>PreserveHierarchy preserves the file hierarchy in the target folder. The relative path of source file to source folder is identical to the relative path of target file to target folder. You can learn more in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#copy-activity-properties"" rel=""nofollow noreferrer"">docs</a>.</p>

<p><a href=""https://i.stack.imgur.com/t9qQB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t9qQB.png"" alt=""enter image description here""></a></p>
"
"57192228","Using Function app connector in ADF - How to override parameters in CI-CD?","<p>I need a work around pretty quickly - this was a late surprise in the dev process when we added an Az function to our development ADF pipeline.  </p>

<p>When you use a function app in ADF V2, when you generate the ARM template, it does not parameterize the key references unlike in other linked services.  Ugh!</p>

<p>So for CI/CD scenarios, when we deploy we now have a fixed function app reference.  What we'd like to do is the same as other linked services - override the key parameters to point to the correct Dev/UAT /Production environment versions of the functions.</p>

<p>I can think of dirty hacks using powershell to overwrite (does powershell support ADF functions yet?  don't know - in January they didn't).</p>

<p>Any other ideas on how to override function app linked service settings?  </p>

<p>the key parameters are under typeProperties (assuming the function key is in keyvault):</p>

<pre><code>{""functionAppUrl:=""https://xxx.azurewebsites.net""}
{""functionkey"":{""store"":{""referenceName""=""xxxKeyVaultLS""}}}
{""functionkey"":{""secretName""=""xxxKeyName""}}
</code></pre>

<p>Right now these are hard coded from the UI settings - no parameter and no default.</p>
","<azure-data-factory>","2019-07-24 22:52:32","659","0","1","58849318","<p>ok, eventually got back to this.  </p>

<p>The solution looks a lot but it is pretty simple.  </p>

<p>In my devops release, I create a Powershell task after both the data factory ARM template has been deployed and the powershell task for deployment.ps1 with the ""predeployment=$false"" setting has run (see ADF CI/CD <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#sample-prepostdeployment-script"" rel=""nofollow noreferrer"">here</a>.)</p>

<p>I have a json file for each environment (dev/uat/prod) in my git repo (I actually use a separate ""common"" repo to store scripts apart from the ADF git repo and its alias in DevOps is ""_Common"" - you'll see this below in the -File parameter of the script).</p>

<p>The json file to replace the deployed function linked service is a copy of the function linked service json in ADF and looks like this for DEV:</p>

<p>(scripts/Powershell/dev.json)</p>

<pre><code>{
    ""name"": ""FuncLinkedServiceName"",
    ""type"": ""Microsoft.DataFactory/factories/linkedservices"",
    ""properties"": {
        ""annotations"": [],
        ""type"": ""AzureFunction"",
        ""typeProperties"": {
            ""functionAppUrl"": ""https://myDEVfunction.azurewebsites.net"",
            ""functionKey"": {
                ""type"": ""AzureKeyVaultSecret"",
                ""store"": {
                    ""referenceName"": ""MyKeyvault_LS"",
                    ""type"": ""LinkedServiceReference""
                },
                ""secretName"": ""MyFunctionKeyInKeyvault""
            }
        },
        ""connectVia"": {
            ""referenceName"": ""MyintegrationRuntime"",
            ""type"": ""IntegrationRuntimeReference""
        }
    }
}
</code></pre>

<p>...and the PROD file would be like this:</p>

<p>(scripts/Powershell/prod.json)</p>

<pre><code>{
    ""name"": ""FuncLinkedServiceName"",
    ""type"": ""Microsoft.DataFactory/factories/linkedservices"",
    ""properties"": {
        ""annotations"": [],
        ""type"": ""AzureFunction"",
        ""typeProperties"": {
            ""functionAppUrl"": ""https://myPRODfunction.azurewebsites.net"",
            ""functionKey"": {
                ""type"": ""AzureKeyVaultSecret"",
                ""store"": {
                    ""referenceName"": ""MyKeyvault_LS"",
                    ""type"": ""LinkedServiceReference""
                },
                ""secretName"": ""MyFunctionKeyInKeyvault""
            }
        },
        ""connectVia"": {
            ""referenceName"": ""MyintegrationRuntime"",
            ""type"": ""IntegrationRuntimeReference""
        }
    }
}
</code></pre>

<p>then in the devops pipeline, I use a Powershell script block that looks like this:</p>

<pre><code>Set-AzureRMDataFactoryV2LinkedService -ResourceGroup ""$(varRGName)"" -DataFactoryName ""$(varAdfName)"" -Name ""$(varFuncLinkedServiceName)"" -File ""$(System.DefaultWorkingDirectory)/_Common/Scripts/Powershell/$(varEnvironment).json"" -Force
</code></pre>

<p>or for Az</p>

<pre><code>Set-AzDataFactoryV2LinkedService -ResourceGroupName ""$(varRGName)"" -DataFactoryName ""$(varAdfName)"" -Name ""$(varFuncLinkedServiceName)"" -DefinitionFile ""$(System.DefaultWorkingDirectory)/_Common/Scripts/Powershell/Converter/$(varEnvironment).json"" -Force
</code></pre>

<p>Note:</p>

<ul>
<li>the $(varXxx) are defined in my pipeline variables e.g. 

<ul>
<li>varFuncLinkServiceName = FuncLinkedServiceName.</li>
<li>varEnvironment = ""DEV"", ""UAT"", ""PROD"" depending on the target release</li>
</ul></li>
<li>Force is used because the Linked service must already exist in the Data Factory ARM deployment and then we need to force the overwrite of just the function linked service.</li>
</ul>

<p>Hopefully MSFT will release a function app linked service that uses parameters but until then, this has got us moving with the release pipeline.</p>

<p>HTH.  Mark.</p>

<p><strong>Update:</strong>  Added the Az cmdlet version of the AzureRM command and changed to Set (""New-Az..."" worked but in the new Az - there is only Set- for V2 linked services).</p>
"
"57189240","Azure DataFactory Copy Data - how to know it has done copying data?","<p>We have a bunch of files in azure blob storage as a tsv format and we want to move them to destination which is ADLS Gen 2 and parquet format. We want this activity on daily basis. So the ADF pipeline will write bunch of parquet files in folders which will have date in them. for example</p>

<p>../../YYYYMMDD/*.parquet</p>

<p>On the other side we have API which will access this. How does the API know that the data migration is completed for a particular day or not?</p>

<p>Basically is there an in built ADF feature to write done file or _SUCCESS file which API can rely on?</p>

<p>Thanks</p>
","<azure><parquet><azure-data-factory>","2019-07-24 18:33:32","454","0","2","57192180","<p>Why not simply call the API to let it know from ADF using Web activity?</p>

<p>You can use Web Activity to even pass the name of processed file as URL or body parameters to that the API knows what to process.</p>
"
"57189240","Azure DataFactory Copy Data - how to know it has done copying data?","<p>We have a bunch of files in azure blob storage as a tsv format and we want to move them to destination which is ADLS Gen 2 and parquet format. We want this activity on daily basis. So the ADF pipeline will write bunch of parquet files in folders which will have date in them. for example</p>

<p>../../YYYYMMDD/*.parquet</p>

<p>On the other side we have API which will access this. How does the API know that the data migration is completed for a particular day or not?</p>

<p>Basically is there an in built ADF feature to write done file or _SUCCESS file which API can rely on?</p>

<p>Thanks</p>
","<azure><parquet><azure-data-factory>","2019-07-24 18:33:32","454","0","2","57195639","<p>Provide two ways here for you.Let me say,from the perspective of ADF copy activity execution results, it can be divided into active way and passive way.</p>

<p>1.Active way,you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-pipeline-activity#type-properties"" rel=""nofollow noreferrer"">waitOnCompletion</a> feature in execute pipeline activity. 
<a href=""https://i.stack.imgur.com/WdzyP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WdzyP.png"" alt=""enter image description here""></a></p>

<p>After that,execute a web activity to trigger your custom api.Please see this case:<a href=""https://stackoverflow.com/questions/54881313/azure-data-factory-how-to-trigger-a-pipeline-after-another-pipeline-completed-s/54882309#54882309"">Azure Data Factory: How to trigger a pipeline after another pipeline completed successfully</a></p>

<p>2.Passive way,you could use monitor feature of ADF pipeline. Please see the example of <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-dot-net#monitor-a-pipeline-run"" rel=""nofollow noreferrer"">.net sdk</a>:</p>

<pre><code>Console.WriteLine(""Checking copy activity run details..."");

RunFilterParameters filterParams = new RunFilterParameters(
    DateTime.UtcNow.AddMinutes(-10), DateTime.UtcNow.AddMinutes(10));
ActivityRunsQueryResponse queryResponse = client.ActivityRuns.QueryByPipelineRun(
    resourceGroup, dataFactoryName, runResponse.RunId, filterParams);
if (pipelineRun.Status == ""Succeeded"")
    Console.WriteLine(queryResponse.Value.First().Output);
else
    Console.WriteLine(queryResponse.Value.First().Error);
Console.WriteLine(""\nPress any key to exit..."");
Console.ReadKey();
</code></pre>

<p>Check the status is <code>successed</code>, then do your custom business.</p>
"
"57188513","Azure Data Factory designer in Visual Studio 2019 project","<p>I've run into a release pipeline issue with the Azure Data Factory.  In most types of Azure entities (such as a web app or sql project), I can develop the project in Visual Studio and use ADO to create a build and releases to deploy the project to Azure. Using variables and libraries, I can create the appropriate release definitions variables on a per-environment basis, and ensure that the build I deploy is the same for each step of the release pipeline (i.e dev -> tst -> stg -> prd). You know, the normal continuous integration/continuous deployment process?</p>

<p>However with Azure Data factories, it seems that I have to create the data factory in the Azure Portal directly for an environment and then create it again for another environment in the pipeline (I know I can export and reimport manually).</p>

<p>What I would like is to be able to create an Azure Data Factory project in Visual Studio (2019), maintain it in Visual Studio with a similar designer like the one in the Azure portal, check it into git and deploy it with a build and release in ADO.</p>

<p>Since creating an Azure Data Factory project doesn't seem possible (or am I missing something?), what is the recommended approach to working with Azure data factories in a continuous integration/continuous deployment ADO environment?</p>
","<azure-data-factory><designer><visual-studio-2019>","2019-07-24 17:41:41","7358","6","1","57188822","<p>ADFV2 does not have a plugin for Visual Studio, most of the effort has been on the UX side. 
We recommend you to use ADF UI as your development tool, where you can define your workflow easily and validate your changes.</p>

<p>For CICD, you can integrate your Dev factory with GIT, and then setup CICD in the below way
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a>.</p>

<p>Every publish to Dev factory will trigger your release pipeline which can take the build and deploy to remaining stages.</p>
"
"57183650","how to use substring and cast as time in u-sql","<p>how can we substring a column and cast it as time in u-sql</p>

<pre><code>@mergeddata =
SELECT DISTINCT a.hta_coinsid.Substring(12, 5).time 
AS CreatedTime                    FROM @file1 AS AS;
</code></pre>
","<azure><azure-data-factory><azure-data-lake><u-sql>","2019-07-24 13:04:06","110","0","1","57205859","<p>Have you viewed the documentation? Specifically <a href=""https://learn.microsoft.com/en-us/u-sql/operators/csharp-functions-and-operators#substring"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/u-sql/operators/csharp-functions-and-operators#substring</a> </p>
"
"57183034","Retry of batch operation in Azure Table","<p>I am trying insert data from Azure data lake store to Azure table through Azure Data Factory. The data in Azure Data Lake file is in same schema to that of final Azure Table sink. </p>

<p>The ADF pipeline consist of single copy activity to copy from Azure Data Lake store to Azure Table. But the ADF pipeline is failing at times due to throttling. I cannot afford to rerun the complete pipeline as it takes hours.</p>

<p>I wanted to retry only the failed batch. But I don't see that as as option provided in Azure Table.</p>

<p>I found <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactories.models.copysink.sinkretrycount?view=azure-dotnet#Microsoft_Azure_Management_DataFactories_Models_CopySink_SinkRetryCount"" rel=""nofollow noreferrer"">SinkRetryCount</a> and <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactories.models.copysink.sinkretrywait?view=azure-dotnet"" rel=""nofollow noreferrer"">SinkRetryWait</a> as two parameters for AzureTableSink class, but I guess(since the doc doesn't mention properly) that would be for the complete pipeline.</p>

<p>I have two questions:</p>

<ol>
<li>What does SinkRetryCount and SinkRetryWait actually mean?</li>
<li>Is there a ways to retry a batch if it fails either through setting of parameters or making a different activity graph in ADF pipeline?</li>
</ol>
","<c#><azure><azure-table-storage><azure-data-factory>","2019-07-24 12:31:45","641","1","1","57199434","<p>Have you tried below:</p>

<ul>
<li><p>If your process ensures a clean state as the very first step, similar to Command Design pattern’s undo (but more naive), then your process can re-execute. </p>

<ul>
<li>With #1, you can safely use “retry” in your pipeline activities, along with sufficient time between retries.</li>
<li>this is an ADFv1 or v2 compatible approach</li>
</ul></li>
</ul>

<p>Reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-create-pipelines"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-create-pipelines</a></p>

<ul>
<li><p>If ADFv2, then you have more options and can have more complex logic to handle errors:</p>

<ul>
<li><p>for the activity that is failing, wrap this in an until-success loop, and be sure to include a bound on execution.  </p></li>
<li><p>you can add more activities in the loop to handle failure and log, notify, or resolve known failure conditions due to externalities out of your control.</p></li>
</ul></li>
<li><p>You can also use asynchronous communication to future process executions that save success to a central store.  Then later executions “if” I already was successful then stop processing before the activity.  </p>

<ul>
<li>this is powerful for more generalized pipelines, since you can choose where to begin</li>
</ul></li>
</ul>

<p>Check retries at ee retry at <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-factory-create-pipelines"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-factory-create-pipelines</a> .</p>

<pre><code>Retry: Number of retries before the data processing for the slice is marked as Failure. Activity execution for a data slice is retried up to the specified retry count. The retry is done as soon as possible after the failure.
</code></pre>

<p>Hope it helps.</p>
"
"57167710","Azure Data Factory Cannot Connect to Azure Cloud Server","<p>I am receiving a connection failed (cannot connect to SQL Server) while creating a Linked Service in Azure Data Factory.</p>

<p>The SQL Server I have created is a developer licensed SQL Server 2017 which i manually installed on an Azure VM. </p>

<p>I can connect to the sql server via SSMS just fine.</p>

<p>I have tried exposing 1433 to the world thinking that it was a security group issue but it is still failing.</p>

<p>I dont see an option to ""Allow access to Azure services"" from the portal for a VM.</p>

<p>Building an azure Sql Server isnt an option because of limitations with that version.</p>

<p>the error message just says ""Cannot Connect to...."" There is nothing in the VM sql server logs or other logs on the VM.</p>
","<sql-server><azure><virtual-machine><azure-data-factory>","2019-07-23 15:36:55","115","0","1","57170150","<p>I was able to connect finally. I used JSON connection formatting this time and again had to expose 1433 publicly.</p>
"
"57152216","How to convert CSV to ORC format using Azure Datafactory","<p>I am coping comma separated partition data files into ADLS using azure datafactory.</p>

<p>The requirement is to copy the comma separated files to ORC format with SNAPPY compression.</p>

<p>Is it possible to achieve this with ADF? if yes, then could you please help me?</p>
","<azure><azure-data-factory><orc><snappy>","2019-07-22 18:59:10","408","0","1","57164504","<p>Unfortunatelly, data factory can <strong>read</strong> from ZLIB and SNAPPY, but can only <strong>write</strong> ZLIB, which is the default for the orc file format.</p>

<p>More info here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#orc-format"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#orc-format</a></p>

<p>Hope this helped!!</p>
"
"57151408","Repeatable write to SQL Sink using Azure Data Factory is failing","<p>Scenario: I'm copying data from Azure Table Storage to an Azure SQL DB using an upsert stored procedure like this:</p>

<pre><code>CREATE PROCEDURE [dbo].[upsertCustomer] @customerTransaction dbo.CustomerTransaction READONLY
AS
BEGIN
    MERGE customerTransactionstable WITH (HOLDLOCK) AS target_sqldb
    USING @customerTransaction AS source_tblstg
    ON (target_sqldb.customerReferenceId = source_tblstg.customerReferenceId AND
        target_sqldb.Timestamp = source_tblstg.Timestamp)
    WHEN MATCHED THEN
        UPDATE SET
            AccountId = source_tblstg.AccountId,
            TransactionId = source_tblstg.TransactionId,
            CustomerName = source_tblstg.CustomerName
    WHEN NOT MATCHED THEN
        INSERT (
            AccountId,
            TransactionId,
            CustomerName,
            CustomerReferenceId,
            Timestamp
            )
        VALUES (
            source_tblstg.AccountId,
            source_tblstg.TransactionId,
            source_tblstg.CustomerName,
            source_tblstg.CustomerReferenceId,
            source_tblstg.Timestamp
            );
END

GO
</code></pre>

<p>where <strong>customerReferenceId</strong> &amp; <strong>Timestamp</strong> constitute the composite key for the <strong>CustomerTransactionstable</strong></p>

<p>However, when I update the rows in my source(Azure table) and rerun the Azure data factory, I see this error:</p>

<blockquote>
  <p>""ErrorCode=FailedDbOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A
  database operation failed with the following error: &apos;Violation of
  PRIMARY KEY constraint &apos;PK_CustomerTransactionstable&apos;.
  Cannot insert duplicate key in object
  &apos;dbo.CustomerTransactionstable&apos;. The duplicate key value is
  (Dec 31 1990 12:49AM&#44; ABCDEFGHIGK).\r\nThe statement has been
  terminated.&apos;,Source=.Net SqlClient Data
  Provider,SqlErrorNumber=2627,Class=14,ErrorCode=-2146232060,State=1,Errors=[{Class=14,Number=2627,State=1,Message=Violation
  of PRIMARY KEY constraint &apos;PK_CustomerTransactionstable&apos;""</p>
</blockquote>

<p>Now, I have verified that there's only one row in both the source and sink with a matching primary key, the only difference is that some columns in my source row have been updated.</p>

<p>This <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-repeatable-copy#repeatable-write-to-sqlsink"" rel=""nofollow noreferrer"">link</a> in the Azure documentation speaks about the repeatable copying, however I don't want to delete rows for a time range from my destination before inserting any data nor do I have the ability to add a new sliceIdentifierColumn to my existing table or any schema change. </p>

<p><strong>Questions</strong>: </p>

<ol>
<li>Is there something wrong with my upsert logic? If yes, is there a better way to do upsert to Azure SQL DBs? </li>
<li>If I choose to use a SQL cleanup script, is there a way to delete only those rows from my Sink that match my primary key?</li>
</ol>

<p><strong>Edit</strong>: </p>

<blockquote>
  <p>This has now been resolved.</p>
</blockquote>

<p><strong>Solution</strong>: </p>

<blockquote>
  <p>The primary key violation will only occur if it's trying to insert a
  record which already has a matching primary key. In my case there
  although there was just one record in the sink, the condition on which
  the merge was being done wasn't getting satisfied due to mismatch
  between <strong>datetime</strong> and <strong>datetimeoffset</strong> fields.</p>
</blockquote>
","<azure-sql-database><azure-table-storage><azure-data-factory>","2019-07-22 17:57:39","1346","0","1","57170230","<p>Have you tried it using ADF Data Flows with Mapping Data Flows instead of coding it through a stored procedure? It may be much easier for you for upserts with SQL.</p>

<p>With an Alter Row transformation, you can perform Upsert, Update, Delete, Insert via UI settings and picking a PK: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-alter-row"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-alter-row</a></p>

<p>You would still just need a Copy Activity prior to your Data Flow activity to copy the data from Table Storage. Put it in a Blob folder and then Data Flow can read the Source from there.</p>
"
"57151218","Connecting to Azure SQLDW in Azure Data Factory","<p>There is an Azure SQL Data Warehouse that I need to connect to for a linked service in Azure Data Factory. The ADW only supports <code>Active Directory - Integrated</code> authentication. How can I connect in Azure Data Factory with this authentication?  The only options in Azure Data Factory are <code>SQL Authentication</code>, <code>Managed Identity</code> and <code>Service Principal</code>.</p>

<p>We have credentials for individual users, not app. I have an AAD app registration we could possibly use.  Do I need to have the ADW add our AAD app? </p>

<p>I tried <code>SQL Authentication</code> (obviously didn't work), we don't have a managed instance (would be our ADF instance), and don't have an obvious service principal.</p>

<p>Thank you in advance for your help!</p>
","<azure-active-directory><azure-data-factory><azure-synapse>","2019-07-22 17:44:22","240","2","1","57152069","<p>Follow the steps at this link, and create a Managed Identity for your ADF.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse#managed-identity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse#managed-identity</a></p>

<p>Managed Identity (AAD authorisation) is not Managed Instance (database service).</p>
"
"57145892","Trying to Run Incremental Load Pipeline But failing","<p>Hi I have setup a simple pipeline that basically run each day and copies data between last day and the current day, my pipeline is failing and am not sure why, below is the attached code that I have been using can anyone help me why there is an error whenever I try to run this? my source is oracle and destination is Azure datawarehouse.</p>

<p>Code:</p>

<pre><code>{
    ""name"": ""Copy_sgd"",
    ""type"": ""Copy"",
    ""dependsOn"": [],
    ""policy"": {
        ""timeout"": ""7.00:00:00"",
        ""retry"": 0,
        ""retryIntervalInSeconds"": 30,
        ""secureOutput"": false,
        ""secureInput"": false
    },
    ""userProperties"": [
        {
            ""name"": ""Source"",
            ""value"": ""\""COMPANY_1_RPT\"".\""V7261_0059_GL_DETAIL_VIEW\""""
        },
        {
            ""name"": ""Destination"",
            ""value"": ""[COMPANY_1_RPT].[V7261_0059_GL_DETAIL_VIEW]""
        }
    ],
    ""typeProperties"": {
        ""source"": {
            ""type"": ""OracleSource"",
            ""oracleReaderQuery"": {
                ""value"": ""select * from \""COMPANY_1_RPT\"".\""V7261_0059_GL_DETAIL_VIEW\"" \nwhere \""GLPOSTINGDATE\"" &gt;= TO_DATE('@{formatDateTime(pipeline().parameters.windowStart, 'yyyy-MM-dd HH:mm' )}', 'YYYY-MM-DD HH24:MI') \nAND \""GLPOSTINGDATE\"" &lt; TO_DATE('@{formatDateTime(pipeline().parameters.windowEnd, 'yyyy-MM-dd HH:mm' )}', 'YYYY-MM-DD HH24:MI')"",
                ""type"": ""Expression""
            },
            ""partitionOption"": ""None""
        },
        ""sink"": {
            ""type"": ""SqlDWSink"",
            ""allowPolyBase"": false,
            ""preCopyScript"": {
                ""value"": ""delete * from \""COMPANY_1_RPT\"".\""V7261_0059_GL_DETAIL_VIEW\"" \nwhere \""GLPOSTINGDATE\"" &gt;= TO_DATE('@{formatDateTime(pipeline().parameters.windowStart, 'yyyy-MM-dd HH:mm' )}', 'YYYY-MM-DD HH24:MI') \nAND \""GLPOSTINGDATE\"" &lt; TO_DATE('@{formatDateTime(pipeline().parameters.windowEnd, 'yyyy-MM-dd HH:mm' )}', 'YYYY-MM-DD HH24:MI')"",
                ""type"": ""Expression""
            }
        },
        ""enableStaging"": true,
        ""stagingSettings"": {
            ""linkedServiceName"": {
                ""referenceName"": ""adfv2logs"",
                ""type"": ""LinkedServiceReference""
            },
            ""path"": ""adf-v2-logs""
        },
        ""enableSkipIncompatibleRow"": true,
        ""redirectIncompatibleRowSettings"": {
            ""linkedServiceName"": {
                ""referenceName"": ""adfv2logs"",
                ""type"": ""LinkedServiceReference""
            },
            ""path"": ""adf-v2-logs""
        },
        ""translator"": {
            ""type"": ""TabularTranslator"",
            ""mappings"": [
                {
                    ""source"": {
                        ""name"": ""SOURCENAME"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""SOURCENAME"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""VENDNAMEIDX"",
                        ""type"": ""Decimal""
                    },
                    ""sink"": {
                        ""name"": ""VENDNAMEIDX"",
                        ""type"": ""Decimal""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""VENDORID"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""VENDORID"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""VENDORNAME"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""VENDORNAME"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""NAME_DESCR"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""NAME_DESCR"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""NUM"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""NUM"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""DESCRIPTION"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""DESCRIPTION"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""DEBITAMT"",
                        ""type"": ""Double""
                    },
                    ""sink"": {
                        ""name"": ""DEBITAMT"",
                        ""type"": ""Single""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""CREDITAMT"",
                        ""type"": ""Double""
                    },
                    ""sink"": {
                        ""name"": ""CREDITAMT"",
                        ""type"": ""Single""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""AMT"",
                        ""type"": ""Double""
                    },
                    ""sink"": {
                        ""name"": ""AMT"",
                        ""type"": ""Single""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""ACRUCASHTYPE"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""ACRUCASHTYPE"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""DEPTIDX"",
                        ""type"": ""Decimal""
                    },
                    ""sink"": {
                        ""name"": ""DEPTIDX"",
                        ""type"": ""Decimal""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""DEPARTMENTID"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""DEPARTMENTID"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""DEPARTMENTNAME"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""DEPARTMENTNAME"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""ACCTIDX"",
                        ""type"": ""Decimal""
                    },
                    ""sink"": {
                        ""name"": ""ACCTIDX"",
                        ""type"": ""Decimal""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""GLACCOUNTID"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""GLACCOUNTID"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""GLACCOUNTNAME"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""GLACCOUNTNAME"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""GLACCOUNTTYPE"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""GLACCOUNTTYPE"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""COSTCENTERIDX"",
                        ""type"": ""Decimal""
                    },
                    ""sink"": {
                        ""name"": ""COSTCENTERIDX"",
                        ""type"": ""Decimal""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""COSTCENTERID"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""COSTCENTERID"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""COSTCENTERNAME"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""COSTCENTERNAME"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""CACMTY"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""CACMTY"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""CACROPRANCH"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""CACROPRANCH"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""PHASEIDX"",
                        ""type"": ""Decimal""
                    },
                    ""sink"": {
                        ""name"": ""PHASEIDX"",
                        ""type"": ""Decimal""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""PHASEID"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""PHASEID"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""PHASENAME"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""PHASENAME"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""GLPOSTINGDATE"",
                        ""type"": ""DateTime""
                    },
                    ""sink"": {
                        ""name"": ""GLPOSTINGDATE"",
                        ""type"": ""DateTime""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""SOURCEIDX"",
                        ""type"": ""Decimal""
                    },
                    ""sink"": {
                        ""name"": ""SOURCEIDX"",
                        ""type"": ""Decimal""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""SOURCEID"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""SOURCEID"",
                        ""type"": ""String""
                    }
                }
            ]
        }
    },
    ""inputs"": [
        {
            ""referenceName"": ""SourceDataset_sgd"",
            ""type"": ""DatasetReference""
        }
    ],
    ""outputs"": [
        {
            ""referenceName"": ""DestinationDataset_sgd"",
            ""type"": ""DatasetReference""
        }
    ]
}
</code></pre>

<p>Error</p>

<pre><code>{
    ""errorCode"": ""2200"",
    ""message"": ""ErrorCode=FailedDbOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A database operation failed with the following error: &amp;apos;Parse error at line: 1&amp;#44; column: 8: Incorrect syntax near &amp;apos;*&amp;apos;.&amp;apos;,Source=,''Type=System.Data.SqlClient.SqlException,Message=Parse error at line: 1&amp;#44; column: 8: Incorrect syntax near &amp;apos;*&amp;apos;.,Source=.Net SqlClient Data Provider,SqlErrorNumber=103010,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=103010,State=1,Message=Parse error at line: 1&amp;#44; column: 8: Incorrect syntax near &amp;apos;*&amp;apos;.,},],'"",
    ""failureType"": ""UserError"",
    ""target"": ""Copy_sgd""
}
</code></pre>
","<azure-data-factory><azure-synapse>","2019-07-22 12:17:45","230","0","2","57164804","<p>This is clearly a syntax error in your query. I almost always solve this kind of issue with these steps:</p>

<p>1- Run your pipeline
2- When it fails, check the input of the copy activity, one of its values will be the exact query that the pipeline is building. Copy that.
3- Try running that query manually over the database, it will probably have a syntax error. 
4- Fix the syntax in the query, then try replicating that fix in the pipeline.</p>

<p>And you should be good to go! If you need further help, post here and I'll try to help you.</p>

<p>Hope this helped!</p>
"
"57145892","Trying to Run Incremental Load Pipeline But failing","<p>Hi I have setup a simple pipeline that basically run each day and copies data between last day and the current day, my pipeline is failing and am not sure why, below is the attached code that I have been using can anyone help me why there is an error whenever I try to run this? my source is oracle and destination is Azure datawarehouse.</p>

<p>Code:</p>

<pre><code>{
    ""name"": ""Copy_sgd"",
    ""type"": ""Copy"",
    ""dependsOn"": [],
    ""policy"": {
        ""timeout"": ""7.00:00:00"",
        ""retry"": 0,
        ""retryIntervalInSeconds"": 30,
        ""secureOutput"": false,
        ""secureInput"": false
    },
    ""userProperties"": [
        {
            ""name"": ""Source"",
            ""value"": ""\""COMPANY_1_RPT\"".\""V7261_0059_GL_DETAIL_VIEW\""""
        },
        {
            ""name"": ""Destination"",
            ""value"": ""[COMPANY_1_RPT].[V7261_0059_GL_DETAIL_VIEW]""
        }
    ],
    ""typeProperties"": {
        ""source"": {
            ""type"": ""OracleSource"",
            ""oracleReaderQuery"": {
                ""value"": ""select * from \""COMPANY_1_RPT\"".\""V7261_0059_GL_DETAIL_VIEW\"" \nwhere \""GLPOSTINGDATE\"" &gt;= TO_DATE('@{formatDateTime(pipeline().parameters.windowStart, 'yyyy-MM-dd HH:mm' )}', 'YYYY-MM-DD HH24:MI') \nAND \""GLPOSTINGDATE\"" &lt; TO_DATE('@{formatDateTime(pipeline().parameters.windowEnd, 'yyyy-MM-dd HH:mm' )}', 'YYYY-MM-DD HH24:MI')"",
                ""type"": ""Expression""
            },
            ""partitionOption"": ""None""
        },
        ""sink"": {
            ""type"": ""SqlDWSink"",
            ""allowPolyBase"": false,
            ""preCopyScript"": {
                ""value"": ""delete * from \""COMPANY_1_RPT\"".\""V7261_0059_GL_DETAIL_VIEW\"" \nwhere \""GLPOSTINGDATE\"" &gt;= TO_DATE('@{formatDateTime(pipeline().parameters.windowStart, 'yyyy-MM-dd HH:mm' )}', 'YYYY-MM-DD HH24:MI') \nAND \""GLPOSTINGDATE\"" &lt; TO_DATE('@{formatDateTime(pipeline().parameters.windowEnd, 'yyyy-MM-dd HH:mm' )}', 'YYYY-MM-DD HH24:MI')"",
                ""type"": ""Expression""
            }
        },
        ""enableStaging"": true,
        ""stagingSettings"": {
            ""linkedServiceName"": {
                ""referenceName"": ""adfv2logs"",
                ""type"": ""LinkedServiceReference""
            },
            ""path"": ""adf-v2-logs""
        },
        ""enableSkipIncompatibleRow"": true,
        ""redirectIncompatibleRowSettings"": {
            ""linkedServiceName"": {
                ""referenceName"": ""adfv2logs"",
                ""type"": ""LinkedServiceReference""
            },
            ""path"": ""adf-v2-logs""
        },
        ""translator"": {
            ""type"": ""TabularTranslator"",
            ""mappings"": [
                {
                    ""source"": {
                        ""name"": ""SOURCENAME"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""SOURCENAME"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""VENDNAMEIDX"",
                        ""type"": ""Decimal""
                    },
                    ""sink"": {
                        ""name"": ""VENDNAMEIDX"",
                        ""type"": ""Decimal""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""VENDORID"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""VENDORID"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""VENDORNAME"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""VENDORNAME"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""NAME_DESCR"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""NAME_DESCR"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""NUM"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""NUM"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""DESCRIPTION"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""DESCRIPTION"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""DEBITAMT"",
                        ""type"": ""Double""
                    },
                    ""sink"": {
                        ""name"": ""DEBITAMT"",
                        ""type"": ""Single""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""CREDITAMT"",
                        ""type"": ""Double""
                    },
                    ""sink"": {
                        ""name"": ""CREDITAMT"",
                        ""type"": ""Single""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""AMT"",
                        ""type"": ""Double""
                    },
                    ""sink"": {
                        ""name"": ""AMT"",
                        ""type"": ""Single""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""ACRUCASHTYPE"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""ACRUCASHTYPE"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""DEPTIDX"",
                        ""type"": ""Decimal""
                    },
                    ""sink"": {
                        ""name"": ""DEPTIDX"",
                        ""type"": ""Decimal""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""DEPARTMENTID"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""DEPARTMENTID"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""DEPARTMENTNAME"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""DEPARTMENTNAME"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""ACCTIDX"",
                        ""type"": ""Decimal""
                    },
                    ""sink"": {
                        ""name"": ""ACCTIDX"",
                        ""type"": ""Decimal""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""GLACCOUNTID"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""GLACCOUNTID"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""GLACCOUNTNAME"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""GLACCOUNTNAME"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""GLACCOUNTTYPE"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""GLACCOUNTTYPE"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""COSTCENTERIDX"",
                        ""type"": ""Decimal""
                    },
                    ""sink"": {
                        ""name"": ""COSTCENTERIDX"",
                        ""type"": ""Decimal""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""COSTCENTERID"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""COSTCENTERID"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""COSTCENTERNAME"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""COSTCENTERNAME"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""CACMTY"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""CACMTY"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""CACROPRANCH"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""CACROPRANCH"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""PHASEIDX"",
                        ""type"": ""Decimal""
                    },
                    ""sink"": {
                        ""name"": ""PHASEIDX"",
                        ""type"": ""Decimal""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""PHASEID"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""PHASEID"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""PHASENAME"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""PHASENAME"",
                        ""type"": ""String""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""GLPOSTINGDATE"",
                        ""type"": ""DateTime""
                    },
                    ""sink"": {
                        ""name"": ""GLPOSTINGDATE"",
                        ""type"": ""DateTime""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""SOURCEIDX"",
                        ""type"": ""Decimal""
                    },
                    ""sink"": {
                        ""name"": ""SOURCEIDX"",
                        ""type"": ""Decimal""
                    }
                },
                {
                    ""source"": {
                        ""name"": ""SOURCEID"",
                        ""type"": ""String""
                    },
                    ""sink"": {
                        ""name"": ""SOURCEID"",
                        ""type"": ""String""
                    }
                }
            ]
        }
    },
    ""inputs"": [
        {
            ""referenceName"": ""SourceDataset_sgd"",
            ""type"": ""DatasetReference""
        }
    ],
    ""outputs"": [
        {
            ""referenceName"": ""DestinationDataset_sgd"",
            ""type"": ""DatasetReference""
        }
    ]
}
</code></pre>

<p>Error</p>

<pre><code>{
    ""errorCode"": ""2200"",
    ""message"": ""ErrorCode=FailedDbOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A database operation failed with the following error: &amp;apos;Parse error at line: 1&amp;#44; column: 8: Incorrect syntax near &amp;apos;*&amp;apos;.&amp;apos;,Source=,''Type=System.Data.SqlClient.SqlException,Message=Parse error at line: 1&amp;#44; column: 8: Incorrect syntax near &amp;apos;*&amp;apos;.,Source=.Net SqlClient Data Provider,SqlErrorNumber=103010,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=103010,State=1,Message=Parse error at line: 1&amp;#44; column: 8: Incorrect syntax near &amp;apos;*&amp;apos;.,},],'"",
    ""failureType"": ""UserError"",
    ""target"": ""Copy_sgd""
}
</code></pre>
","<azure-data-factory><azure-synapse>","2019-07-22 12:17:45","230","0","2","57177052","<p>Since you mentioned that the sink here is SQL , I am sure that the below syntax is wrong </p>

<p>""value"": ""delete * from XXXXX</p>

<p>It should be </p>

<p>""value"": ""delete from XXXXX .</p>
"
"57145400","Data Copy between ADLS Instances","<p>Copying data between various instances of ADLS using DISTCP</p>

<p>Hi All</p>

<p>Hope you are doing well.
We have a use case around using ADLS as different tiers of the ingestion process, just required you valuable opinions regarding the feasibility of the same.</p>

<p>INFRASTRUCTURE: There will two instances of ADLS named LAND &amp; RAW. LAND instance will be getting the file directly from the source while RAW instance will be getting the file once validations are passed in LAND instance. We also have a Cloudera cluster hosted on Azure platform which will have connectivity established to both the ADLS instances.</p>

<p>PROCESS: We will have a set of data &amp; control files landing in one of the ADLS instances (say landing). We need to run a spark code on Cloudera cluster to perform count validation between Data &amp; control file present in Land ADLS instance. Once the validation is successful, we want distcp command to copy data from Land ADLS instance to Raw ADLS instance. We are assuming that Distcp utility will be already installed on the Cloudera cluster.</p>

<p>Can you guys suggest if above approach looks fine?
Primarily our question is whether DISTCP utility will support data movement between two different ADLS instances?
We also considered other options like ADLCopy but Distcp appeared better.</p>

<p>NOTE: We haven't considered use Azure Data Factory since it may has certain security challenges though we know Data Factory is best suited for above use case.</p>
","<apache-spark><cloudera><azure-data-factory><azure-data-lake><distcp>","2019-07-22 11:47:14","92","1","1","60864627","<p>If your use case requires you to copy data between multiple storage accounts, <code>distcp</code> is the right way to execute this. </p>

<p>Note that even if you were to encapsulate this solution in data factory, the pipeline with copy activity will invoke distcp.</p>
"
"57143804","Estimate price azure when using Data Factory","<p>I want to copy CSV file in Blob to another Blob by Data Factory. I want to estimate the cost I have to pay. So, What will affect the amount I have to check.</p>

<p>Thanks for your help</p>
","<azure-data-factory><azure-blob-storage>","2019-07-22 10:16:07","4411","1","2","57144850","<p>The estimated amount you will pay is measured by AU hours. So you can run a job and see how much it will cost and then just do the mat.</p>

<p>For example. This job run 5 AU's for 33 seconds and it cost 0.07 EUR. </p>

<p><a href=""https://i.stack.imgur.com/Kt3gd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Kt3gd.png"" alt=""test job""></a></p>

<p>You can see in the AU analysis if you switch the n# of AU's to use the cost and if its more efective use more AU's to lower the cost or if less AU's can do the same job for less money.</p>

<p><a href=""https://i.stack.imgur.com/ZXP8N.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZXP8N.png"" alt=""AU analysis""></a></p>

<p>You can also try calculate the cost of any Azure services in the link provided by Azure:
<a href=""https://azure.microsoft.com/en-gb/pricing/calculator/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-gb/pricing/calculator/</a></p>
"
"57143804","Estimate price azure when using Data Factory","<p>I want to copy CSV file in Blob to another Blob by Data Factory. I want to estimate the cost I have to pay. So, What will affect the amount I have to check.</p>

<p>Thanks for your help</p>
","<azure-data-factory><azure-blob-storage>","2019-07-22 10:16:07","4411","1","2","57205874","<p>Data Factory pricing has several factors. You can find the current pricing <a href=""https://azure.microsoft.com/en-us/pricing/details/data-factory/data-pipeline/"" rel=""nofollow noreferrer"">here</a>
This is what it looks like today. 
<a href=""https://i.stack.imgur.com/9b1He.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9b1He.png"" alt=""Data Factory Pricing""></a>
<a href=""https://i.stack.imgur.com/jPcRA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jPcRA.png"" alt=""ADF Pricing 2""></a>
If you are copying files from one blob to another, you can use the Azure integration runtime. The act of copying the file is considered an activity. DIUs are the compute resources that do the work. When the copy activity runs on an Azure integration runtime, the minimal allowed Data Integration Units (formerly known as Data Movement Units) is <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance"" rel=""nofollow noreferrer"">two</a>.</p>

<p>You didn't specify how often you want to copy the files from one blob to another, but that factors into how many DIU-hours you will use. </p>

<p>So if I assume the following: </p>

<ul>
<li>1000 activity runs</li>
<li>30 DIU-hours</li>
<li>No self-hosted IR, no data flows</li>
<li>1 entity unit of read/write operations</li>
<li>1 entity unit of monitoring operations</li>
</ul>

<p>The <a href=""https://azure.microsoft.com/en-us/pricing/calculator/?&amp;OCID=AID2000128_SEM_brphHbCy&amp;MarinID=brphHbCy_221166233986_azure%20pricing%20calculator_e_c_2AiPkuTD_44568975337_kwd-310354828215&amp;lnkd=Google_Azure_Brand&amp;gclid=CjwKCAjwpuXpBRAAEiwAyRRPgbnWtGFFvkoSKAAk1rLeW5mIxp6Qh_QXyCBfOb1kSZMIyj44KdtBrhoC8m8QAvD_BwE"" rel=""nofollow noreferrer"">Azure Pricing calculator</a> gives me about $9 per month. You may come up with a different number if you adjust the activity runs and DIU-hours. If it is a single small file you are copying daily, you probably wouldn't use 30 DIU-hours. If it is actually 5 of the world's largest CSVs, and you copy them hourly, you might. The only real way to know the DIU hours is to build a pipeline and execute it once. </p>
"
"57142491","Parameterized Linked Service for MongoDB Connection","<p>I'm trying to parameterize the mongo DB connection's linked service in azure data factory. I have referred the document <a href=""https://i.stack.imgur.com/PkT5h.png"" rel=""nofollow noreferrer"">here</a>.
Below is the code I'm trying to update in the code of the linked service</p>

<pre><code>{
""name"": ""MongoDB"",
""properties"": {
    ""type"": ""MongoDbV2"",
    ""parameters"": {
        ""LSHeatStackConnection"": {
            ""type"": ""String"",
            ""defaultValue"": ""mongodb://username:password@1.1.1.4:1800/dbname?authMechanism=SCRAM-SHA-1;authSource=admin""
        }
    },
    ""annotations"": [],
    ""typeProperties"": {
        ""connectionString"": {
            ""type"": ""SecureString"",
            ""value"": ""@{linkedService().LSHeatStackConnection}""
        },
        ""database"": ""heatstack""
    },
    ""connectVia"": {
        ""referenceName"": ""DevIR"",
        ""type"": ""IntegrationRuntimeReference""
    }
}
}
</code></pre>

<p>But I'm getting the below error while saving the above code.</p>

<p>Failed to publish ****DB(linked service name). Reason: At least one resource deployment operation failed. Please list deployment operations for details. Please see <a href=""https://aka.ms/arm-debug"" rel=""nofollow noreferrer"">https://aka.ms/arm-debug</a> for usage details.</p>

<p>When I try edit the linked service by clicking the pencil icon<a href=""https://i.stack.imgur.com/Sapel.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Sapel.png"" alt=""enter image description here""></a>,  and click on test connection it says success. But when I click finish it throws error and it says Failed to save *****DB. Error: Failed to encrypt sub-resource payload</p>

<p>Below is the screenshot of the error.
<a href=""https://i.stack.imgur.com/PkT5h.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PkT5h.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2019-07-22 09:01:19","318","0","1","60460612","<p>Mongo DB (or Cosmos DB Mongo API)’s connection string format is different from other connectors’ which are key/value pairs, so it cannot fit payload encryption framework with parameters which requires the connection string’s format as key value pair when it’s parameterized; and it will fail if any parameter is specified in connection string during payload encryption.</p>

<p>As of now we can't directly parameterize the connection string for MongoDB. We have to store the Connection String in the Key Vault and then pass the secret value as a parameter. That is the only way to parameterize a mongoDB linked Services</p>

<p>If you have any feedback/suggestion related to this please share it in <em><a href=""https://feedback.azure.com/forums/270578-azure-data-factory"" rel=""nofollow noreferrer"">Azure Data Factory User voice forum</a></em></p>
"
"57136234","How to reiterate json response to extract a token and to make a restapi call with for-each activity","<p>i am using Copy Data activity(Can't use Web activity due to security reasons) to connect to Rest API(source) (via REST dataset)and I get a response in json format as shown below which is what i need, only that this is the first page and there are many other pages that i need to retrieve. The retrieved information is then saved in SQL database table (destination). In process to retrieve the next pages i need to extract the token from ""continuationToken"" and send it back to my REST dataset as an dynamic value. Not sure if this is the correct way but gave it a try.</p>

<pre><code>[  
   {  
      ""pagedResponse"":[  
         {  
            ""docType"":""current"",
            ""orgNumber"":""98789765"",
            ""persons"":[  

            ],
            ""clientId"":43533,
            ""name"":null,
            ""clientResponsible"":{
               ""id"":null,
               ""name"":null
            }
         }
      ],
      ""continuationToken"":{  
         ""token"":""-RID:Om1+ANeDbNMWASEAAAAAAAAA==#RT:1#TRC:10#ISrV:2#IEO:6554436"",
         ""range"":{  
            ""min"":"""",
            ""max"":""05C1DFFFFFFFFFFC""
         }
      }
   }
]
</code></pre>

<p>I tried to run some test by saving the json response as a file. Created Lookup activity(Lookup1) to read in the json file and in the then created a for-each activity and in the items i  added a dynamic content 
<code>@activity('Lookup1').output.value[0].continuationToken</code>
        but got this error message.:</p>

<pre><code>{
    ""errorCode"": ""InvalidTemplate"",
    ""message"": ""The function 'length' expects its parameter to be an 
     array or a string. The provided value is of type 'Object'."",
    ""failureType"": ""UserError"",
    ""target"": ""ForEach1""
}
</code></pre>

<p>expected result was:</p>

<p><code>""token"":""-RID:Om1+ANeDbNMWASEAAAAAAAAA==#RT:1#TRC:10#ISrV:2#IEO:6554436"",
             ""range"":{""min"":"""",""max"":""05C1DFFFFFFFFFFC""}</code></p>
","<rest><api><azure-data-factory>","2019-07-21 19:14:58","735","-1","1","57248784","<p>I think your solution mentioned in the question should work.I suppose that you want to know whether the <code>continuationToken</code> is null so that the loop should be ended,so you use <code>length</code> function.However,according to the error message:<code>The function 'length' expects its parameter to be an array or a string. The provided value is of type 'Object'.</code> It is stated in the document clearly:</p>

<p><a href=""https://i.stack.imgur.com/dsZt3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dsZt3.png"" alt=""enter image description here""></a></p>

<p>Then <code>continuationToken</code> is an object,should be used with <code>empty</code> function:</p>

<p><a href=""https://i.stack.imgur.com/N0pQl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/N0pQl.png"" alt=""enter image description here""></a></p>
"
"57119620","Unable to parse JSON list in Azure Data Factory ADF","<p>In my datafactory pipeline I hava a web activity which is giving below JSON response. In the next stored procedure activity I am unable parse the output parameter. I tried few methods.</p>

<p>I have set Content-Type <code>application/json</code> in web activity </p>

<p>Sample JSON:</p>

<pre><code>Output
{
    ""Response"": ""[{\""Message\"":\""Number of barcode(s) found:1\"",\""Status\"":\""Success\"",\""CCS Office\"":[{\""Name\"":\""Woodstock\"",\""CCS Description\"":null,\""BranchType\"":\""Sub CFS Office\"",\""Status\"":\""Active\"",\""Circle\"":\""NJ\""}]}]""
}
</code></pre>

<p>For parameter in stored procedure activity:</p>

<pre><code>@json(first(activity('Web1').output.Response))
</code></pre>

<p>output - System.Collections.Generic.List`1[System.Object]</p>

<pre><code>@json(activity('Web1').output.Response[0])
</code></pre>

<p>output - cannot be evaluated because property '0' cannot be selected. Property selection is not supported on values of type 'String'</p>

<pre><code>@json(activity('Web1').output.Response.Message)
</code></pre>

<p>output - cannot be evaluated because property 'Message' cannot be selected. Property selection is not supported on values of type 'String'</p>
","<json><azure-data-factory>","2019-07-19 21:08:49","8282","0","2","57210978","<p>Here is what I did:
I created a new pipeline, and created a parameter of type 'object' using your 'output' in its entirety:</p>

<pre><code>{     ""Response"": ""[{\""Message\"":\""Number of barcode(s) found:1\"",\""Status\"":\""Success\"",\""CCS Office\"":[{\""Name\"":\""Woodstock\"",\""CCS Description\"":null,\""BranchType\"":\""Sub CFS Office\"",\""Status\"":\""Active\"",\""Circle\"":\""NJ\""}]}]"" }
</code></pre>

<p>I created a variable and setVariable activity.  Variable is of type string.  The dynamic expression I used is:</p>

<pre><code>@{json(pipeline().parameters.output.response)[0]}
</code></pre>

<p>Let me break down and explain.  The {curly braces} were necessary because variable is of type string.  You may not want/need them.</p>

<pre><code>json(....)
</code></pre>

<p>was necessary because data type for the value of 'response' was left as a string.  Whether it being string is correct behavior or not is a different discussion.  By converting from string to json, I can now do the final piece.</p>

<pre><code>[0]
</code></pre>

<p>Now works because Data Factory sees the contents as an objects rather than string literal.  This conversion seems to have been applied to the nested contents as well, because without the encapsulating {curly braces} to convert to string, I would get a type error from my setVariable activity, as the variable is of type string.</p>

<p>Entire pipeline code:</p>

<pre><code>{
""name"": ""pipeline11"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""Set Variable1"",
            ""type"": ""SetVariable"",
            ""dependsOn"": [],
            ""userProperties"": [],
            ""typeProperties"": {
                ""variableName"": ""thing"",
                ""value"": {
                    ""value"": ""@{json(pipeline().parameters.output.response)[0]}"",
                    ""type"": ""Expression""
                }
            }
        }
    ],
    ""parameters"": {
        ""output"": {
            ""type"": ""object"",
            ""defaultValue"": {
                ""Response"": ""[{\""Message\"":\""Number of barcode(s) found:1\"",\""Status\"":\""Success\"",\""CCS Office\"":[{\""Name\"":\""Woodstock\"",\""CCS Description\"":null,\""BranchType\"":\""Sub CFS Office\"",\""Status\"":\""Active\"",\""Circle\"":\""NJ\""}]}]""
            }
        }
    },
    ""variables"": {
        ""thing"": {
            ""type"": ""String""
        }
    },
    ""annotations"": []
}
</code></pre>

<p>}</p>
"
"57119620","Unable to parse JSON list in Azure Data Factory ADF","<p>In my datafactory pipeline I hava a web activity which is giving below JSON response. In the next stored procedure activity I am unable parse the output parameter. I tried few methods.</p>

<p>I have set Content-Type <code>application/json</code> in web activity </p>

<p>Sample JSON:</p>

<pre><code>Output
{
    ""Response"": ""[{\""Message\"":\""Number of barcode(s) found:1\"",\""Status\"":\""Success\"",\""CCS Office\"":[{\""Name\"":\""Woodstock\"",\""CCS Description\"":null,\""BranchType\"":\""Sub CFS Office\"",\""Status\"":\""Active\"",\""Circle\"":\""NJ\""}]}]""
}
</code></pre>

<p>For parameter in stored procedure activity:</p>

<pre><code>@json(first(activity('Web1').output.Response))
</code></pre>

<p>output - System.Collections.Generic.List`1[System.Object]</p>

<pre><code>@json(activity('Web1').output.Response[0])
</code></pre>

<p>output - cannot be evaluated because property '0' cannot be selected. Property selection is not supported on values of type 'String'</p>

<pre><code>@json(activity('Web1').output.Response.Message)
</code></pre>

<p>output - cannot be evaluated because property 'Message' cannot be selected. Property selection is not supported on values of type 'String'</p>
","<json><azure-data-factory>","2019-07-19 21:08:49","8282","0","2","70015238","<p>I had the similar problem and this is how I resolved the issue.</p>
<p>I passed the value of Response as a string to lookup activity which calls a stored procedure in Azure SQL. The stored procedure parses the string using Json_value and return the individual key, value as a row. Now output of lookup activity can be accessed directly from preceding activities.</p>
"
"57113920","Add AzureApplicationID to Active Directory Group with SamAccountName","<p>I have give access to Azure Active Directory Group to a path on a Datalake.
I would like to give access to my Datafactory to this path.
So, I would like to add the application ID to Azure Active Directory Group.
My helpdesk team ask me for the SamAccountName.
I don't know what to answer to the Helpdesk Team.</p>

<p>Could you help me ?</p>
","<azure><authentication><azure-data-factory>","2019-07-19 13:50:32","77","0","1","57138714","<p>The <code>sAMAccountName</code> is in on-premise AD, in Azure AD, there is no <code>sAMAccountName</code>. (There is an <code>onPremisesSamAccountName</code> of <a href=""https://learn.microsoft.com/en-us/graph/api/resources/user?view=graph-rest-beta#properties"" rel=""nofollow noreferrer""><code>User</code></a> object, not sure if your helpdesk team points it).</p>

<p>If you want to add the AD App(service principal) to the AAD Group, the service principal name or the application id is enough. </p>

<p>Navigate to the specific group in the <code>Azure Active Directory</code> in the portal -> <code>Members</code> -> <code>Add members</code> -> search for the <code>service principal name or the application id</code> -> <code>Select</code>.</p>
"
"57108711","Table Storage query CopyPipeline","<p>I need configure Azure Data Factory CopyPipeline source query for copying data from Storage Table by PartitionKey, and I can not find how I can do it ...</p>

<p>Which language is used for querying here? I found something about <code>FetchXML</code>, but it is totally unknown to me. Is there some other variant?</p>

<p>For example (in T-SQL) - I need simply something like this:</p>

<pre><code>SELECT * FROM [StorageTableName] WHERE PartitionKey = [MyPartKey]
</code></pre>

<p>Thanks for the help...</p>

<p>Edit:
I found this article (<a href=""https://learn.microsoft.com/cs-cz/azure/data-factory/v1/data-factory-azure-table-connector#azure-table-copy-activity-type-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/cs-cz/azure/data-factory/v1/data-factory-azure-table-connector#azure-table-copy-activity-type-properties</a>) and try query example: </p>

<p>""azureTableSourceQuery"": ""$$Text.Format('PartitionKey ge \'{0:yyyyMMddHH00_0000}\' and PartitionKey le \'{0:yyyyMMddHH00_9999}\'', SliceStart)""</p>

<p>, but I got just error:</p>

<p>A storage operation failed with the following error 'The remote server returned an error: (400) Bad Request.'.. Activity ID: ...</p>
","<azure><azure-data-factory>","2019-07-19 08:43:29","73","0","1","57109046","<p>The table storage query code is like this:</p>

<pre><code>1. (PartitionKey eq 'Sales') and (RowKey eq 'Smith')
2. PartitionKey eq '1' and RowKey ge '2' 
3. PartitionKey eq 'Sales' and LastName eq 'Smith'
</code></pre>

<p>Replace the '0' and '1' with your PartitionKey and RowKey value.</p>

<p><a href=""https://i.stack.imgur.com/ePHHv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ePHHv.png"" alt=""enter image description here""></a></p>

<p>You can reference this document <a href=""https://learn.microsoft.com/en-us/azure/storage/tables/table-storage-design-for-query#how-your-choice-of-partitionkey-and-rowkey-impacts-query-performance"" rel=""nofollow noreferrer"">How your choice of PartitionKey and RowKey impacts query performance</a>.</p>

<p>Hope this helps.</p>
"
"57108597","Convert data type and concat column in u-sql","<p>How to convert datatype and concat columns in U-SQL?</p>

<pre><code>@output =
    SELECT DISTINCT (kco.ToString + ""-"" + hta_ref.ToString + ""-"" + his_ref.ToString) AS hs_isstaskID,
                    (kco.ToString + ""-"" + his_ref.ToString) AS HS_IssueID,
                    (kco.ToString + ""-"" + hta_ref.ToString) AS hs_task_ID,
                    *
    FROM @input;
</code></pre>
","<azure><azure-data-factory><azure-data-lake><u-sql>","2019-07-19 08:36:10","339","1","1","57111061","<p><code>ToString</code> is a method so normally takes brackets, eg</p>

<pre><code>@output =
    SELECT DISTINCT (kco.ToString() + ""-"" + hta_ref.ToString() + ""-"" + his_ref.ToString()) AS hs_isstaskID,
                    (kco.ToString() + ""-"" + his_ref.ToString()) AS HS_IssueID,
                    (kco.ToString() + ""-"" + hta_ref.ToString()) AS hs_task_ID,
                    *
    FROM @input;
</code></pre>

<p>If the data conversions cause problems then you may need to implement <code>TryParse</code> as per <a href=""https://sqlplayer.net/2017/10/functions-in-the-usql-the-hidden-gem-in-the-summer-2017-update/"" rel=""nofollow noreferrer"">here</a>.</p>
"
"57095001","Azure SQL DB scaling with API","<p>Im using a ADF web activity to scale up/down my azure sql db.</p>

<p>Works perfectly with this code:</p>

<pre><code>{""sku"":{""name"":""GP_Gen4_4"",""tier"":""GeneralPurpose""},""location"":""West Europe""}
</code></pre>

<p>However, this doesnt include the azure hybrid benifit saving.</p>

<p>Based on the below link maybe it's something like this</p>

<pre><code>{“sku”:
{“name”:”GP_Gen4_4″,”tier”:”GeneralPurpose”}
,”licenseType”:
{“BasePrice”:”SOMETHING_HERE”,”LicenseIncluded “:”SOMETHING_HERE”}
,”location”:”West Europe”
}
</code></pre>

<p>however I'm not sure the values to put?</p>

<p><a href=""https://learn.microsoft.com/en-gb/rest/api/sql/databases/update#databaselicensetype"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-gb/rest/api/sql/databases/update#databaselicensetype</a></p>
","<azure><azure-sql-database><azure-data-factory>","2019-07-18 12:56:35","522","1","1","57108289","<p>Based on the rest api <a href=""https://learn.microsoft.com/en-gb/rest/api/sql/databases/update#databaselicensetype"" rel=""nofollow noreferrer"">document</a>, the licenseType need to be defined in the <code>properties</code> object,not in the <code>sku</code> object.</p>

<p><a href=""https://i.stack.imgur.com/1l2cQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1l2cQ.png"" alt=""enter image description here""></a></p>

<p>Refer to the example provided in above link,it should be something like:</p>

<pre><code>{
  ""sku"": {
    ""name"": ""S1"",
    ""tier"": ""Standard""
  },
  ""properties"": {
    ""licenseType"": {
        ""BasePrice"":""&lt;&gt;"",
        ""LicenseIncluded"":""&lt;&gt;""
    }
  },
  ""location"": ""southeastasia""
}
</code></pre>
"
"57088082","Using powershell how do you rerun failed pipeline in Azure Data Factory by run ID?","<p>In Azure Data factory I need to re-run around 4000+ failed pipeline. It is possible to do in the Azure portal UI but I am try to automate the run-run process in powershell. </p>

<p>I am not able to find the command/steps in powershell to ru-run a failed pipeline by the run ID.</p>
","<azure><powershell><azure-data-factory>","2019-07-18 06:11:55","1479","2","2","57088442","<p>I just find a tutorial <a href=""https://blogs.msdn.microsoft.com/karang/2015/11/12/azure-data-factory-detecting-and-re-running-failed-adf-slices/"" rel=""nofollow noreferrer"">Azure Data Factory: Detecting and Re-Running failed ADF Slices</a>, it provides you the Powershell script to automate failed pipeline.</p>

<p>Powershell script:</p>

<pre><code>Login-AzureRmAccount
$slices= @()
$tableName=@()
$failedSlices= @()
$failedSlicesCount= @()
$tableNames=@()

$Subscription=""Provide Subscription ID""  

  Select-AzureRMSubscription -SubscriptionId  $Subscription    
$DataFactoryName=""Provide Data Factory Name""
$resourceGroupName =""Porvide Resource Group Name for Data Factory""

$startDateTime =""2015-05-01"" #Start Date for Slices
$endDateTime=""2015-08-01"" # End Date for Slices


#Get Dataset names in Data Factory - you can exlicitly give a table name using $tableName variable if you like to run only for an individual tablename
$tableNames = Get-AzureRMDataFactoryDataset -DataFactoryName $DataFactoryName -ResourceGroupName $resourceGroupName | ForEach {$_.DatasetName}

$tableNames #lists tablenames

foreach ($tableName in $tableNames)
{
    $slices += Get-AzureRMDataFactorySlice -DataFactoryName $DataFactoryName -DatasetName $tableName -StartDateTime $startDateTime -EndDateTime $endDateTime -ResourceGroupName $resourceGroupName -ErrorAction Stop
}


$failedSlices = $slices | Where {$_.Status -eq 'Failed'}

$failedSlicesCount = @($failedSlices).Count

if ( $failedSlicesCount -gt 0 ) 
{

    write-host ""Total number of slices Failed:$failedSlicesCount""
    $Prompt = Read-host ""Do you want to Rerun these failed slices? (Y | N)"" 
    if ( $Prompt -eq ""Y"" -Or $Prompt -eq ""y"" )
    {

        foreach ($failed in $failedSlices)
        {
            write-host ""Rerunning slice of Dataset ""$($failed.DatasetName)"" with StartDateTime ""$($failed.Start)"" and EndDateTime ""$($failed.End)"""" 
            Set-AzureRMDataFactorySliceStatus -UpdateType UpstreamInPipeline -Status Waiting -DataFactoryName $($failed.DataFactoryName) -DatasetName $($failed.DatasetName) -ResourceGroupName $resourceGroupName -StartDateTime ""$($failed.Start)"" -EndDateTime ""$($failed.End)"" 


        }
    }

}
else
{
    write-host ""There are no Failed slices in the given time period.""
}
</code></pre>

<p>Hope this helps.</p>
"
"57088082","Using powershell how do you rerun failed pipeline in Azure Data Factory by run ID?","<p>In Azure Data factory I need to re-run around 4000+ failed pipeline. It is possible to do in the Azure portal UI but I am try to automate the run-run process in powershell. </p>

<p>I am not able to find the command/steps in powershell to ru-run a failed pipeline by the run ID.</p>
","<azure><powershell><azure-data-factory>","2019-07-18 06:11:55","1479","2","2","61662226","<p>I found this question while searching for a solution to the same problem, so here's an answer with what I've found.</p>
<p>If you want to re-run the entire pipeline and you don't care about it being technically considered a re-run by Data Factory, you could use the <code>Get-AzDataFactoryV2PipelineRun</code> cmdlet to get a list of runs, filter to the failed runs, then use the same parameters in a call to <code>Invoke-AzureRmDataFactoryV2Pipeline</code></p>
<p>It looks like fairly soon that cmdlet will be updated to allow a true re-run (based on the response to <a href=""https://github.com/MicrosoftDocs/azure-docs/issues/53823"" rel=""nofollow noreferrer"">this issue</a> someone raised on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-visually#next-steps"" rel=""nofollow noreferrer"">Visually monitor Azure Data Factory</a> doc from Microsoft).</p>
<p>If you're in a hurry to be able to do a true re-run, the functionality is already included in the <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/createrun"" rel=""nofollow noreferrer"">REST API</a> by calling <code>createRun</code> with some optional parameters.</p>
<p>Edit: This was added to the Azure PowerShell module v4.8.0 released in October 2020 (<a href=""https://learn.microsoft.com/en-us/powershell/module/az.datafactory/invoke-azdatafactoryv2pipeline?view=azps-4.8.0"" rel=""nofollow noreferrer"">docs</a>). You can now pass a pipeline run ID to the <code>Invoke-AzDataFactoryV2Pipeline</code> cmdlet using <code>-ReferencePipelineRunId</code> to have it use the parameters from that run. You can also use the <code>-StartFromFailure</code> switch to have it only rerun the failed activities.</p>
"
"57080702","vague error message when exploring the contents of an S3 bucket","<p>I've created a linked service to S3 which connects without any problems, however when I create a data set and attempt to explore the contents of the bucket, I get the following error message:</p>

<p>The file operation is failed.. Activity ID:dee933af-54f6-4146-87f9-4158d98a09b9</p>

<p>is there anyway I can obtain information that might help be troubleshoot what the problem is.</p>
","<azure-data-factory>","2019-07-17 16:44:01","55","0","1","57086467","<p>from the backend logs, we can see that the error is complaining about : The remote name could not be resolved: ""adf-<strong><em>.10.</em></strong>.112.**"". Could you double check whether the bucket name is valid, and it can be accessed from your self-hosted IR?</p>

<p>If the bucket name is correct. You can try using Azure IR to do the browse. If it works, it means the failure is caused by a connectivity issue from your self-hosted IR to the S3 account (e.g. firewall setting, proxy)</p>
"
"57075324","get entities all latest records in u-sql script","<p>how to fetch all employees latest records in U-SQL</p>

<pre><code>@employees = 
    SELECT * FROM 
        ( VALUES
        (1, ""Noah"",   100, (int?)10000, new DateTime(2012,05,31), ""cell:030-0074321,office:030-0076545""),
 (1, ""Noah"",   100, (int?)10000, new DateTime(2012,06,30), ""cell:030-0074321,office:030-0076545""),
        (2, ""Sophia"", 100, (int?)15000, new DateTime(2012,03,19), ""cell:(5) 555-4729,office:(5) 555-3745""),
        (3, ""Liam"",   100, (int?)30000, new DateTime(2014,09,14), ""cell:(5) 555-3932""),
        (4, ""Amy"",    100, (int?)35000, new DateTime(1999,02,27), ""cell:(171) 555-7788,office:(171) 555-6750, home:(425) 555-6238""),

        (5, ""amy"", 100, (int?)15000, new DateTime(2015,01,12), ""cell:0921-12 34 65,office:0921-12 34 67""),
        (6, ""Emma"",   200, (int?)8000,  new DateTime(2014,03,08), (string)null),
        (7, ""Jacob"",  200, (int?)8000,  new DateTime(2014,09,02), """"),
        (8, ""Olivia"", 200, (int?)8000,  new DateTime(2013,12,11), ""cell:88.60.15.31,office:88.60.15.32""),
        (9, ""Mason"",  300, (int?)50000, new DateTime(2016,01,01), ""cell:(91) 555 22 82,office:(91) 555 91 99, home:(425) 555-2819""),
        (10, ""Ava"",   400, (int?)15000, new DateTime(2014,09,14), ""cell:91.24.45.40,office:91.24.45.41""),
        (11, ""Ethan"", 400, (int?)9000,  new DateTime(2015,08,22), ""cell:(604) 555-4729,office:(604) 555-3745""),
        (12, ""David"", 800, (int?)100,   new DateTime(2016,11,01), ""cell:(171) 555-1212""),
        (13, ""Andrew"", 100, (int?)null, new DateTime(1995,07,16), ""cell:(1) 135-5555,office:(1) 135-4892""),
        (14, ""Jennie"", 100, (int?)34000, new DateTime(2000,02,12), ""cell:(5) 555-3392,office:(5) 555-7293"")
        ) AS T(EmpID, EmpName, DeptID, Salary, StartDate, PhoneNumbers);

@result =
    SELECT *
    FROM @employees
    ORDER BY EmpID DESC
    ;    

OUTPUT @result
TO ""sample.txt""
USING Outputters.Csv();
</code></pre>
","<azure><azure-data-factory><azure-data-lake><u-sql>","2019-07-17 12:00:05","69","0","1","57078752","<p>Use one of the analytic functions like <code>ROW_NUMBER</code> or <code>RANK</code>, partition by <code>EmpId</code> and order by <code>Start Date</code> but <em>descending</em>, something like this:</p>

<pre><code>@result =
    SELECT *
    FROM
    (
    SELECT *, ROW_NUMBER() OVER( PARTITION BY EmpID ORDER BY StartDate DESC ) AS rn
    FROM @employees
    ) AS x
    WHERE rn == 1;    
</code></pre>
"
"57056928","Copy On-Prem Flat file to Azure Blob using Azure Data Factory V2","<p>I am trying to load a <strong>Flat file</strong> to <strong>BLOB</strong> using the <strong>ADF V2</strong>. I have installed the Self Hosted Integration Runtime for the same. The Integration Runtime on Local Machine shows that is successfully connected to the cloud Service as in the SC below.  However while making the LinkedService to the on Prem File, some credentials are required. I am not sure of what <strong>UserName</strong> or <strong>Password</strong> should be fed in. I have tried both On-Prem and Azure passwords (Wanted to try). Please see the SC.</p>

<p>Could you please guide as how the connection can be made to a local Flat file in my case. </p>

<p>Thanks 
- Akshay
<a href=""https://i.stack.imgur.com/0mN17.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0mN17.png"" alt=""Screenshot""></a></p>
","<azure><azure-data-factory>","2019-07-16 12:09:07","3009","1","2","57063174","<p><strong>Note:</strong> You can choose a file while creating the File System as a source in ADF.</p>

<p>You may follow the following steps to select the text file while creating File system as source:</p>

<p>First create a linked service as follows:</p>

<p>Host: <code>**C:\AzureLearn\**</code></p>

<p><a href=""https://i.stack.imgur.com/ZGcU1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZGcU1.png"" alt=""enter image description here""></a></p>

<p>Create a copy activity and select Source as follows:</p>

<p>Click on Source => New </p>

<p><a href=""https://i.stack.imgur.com/rAirI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rAirI.png"" alt=""enter image description here""></a></p>

<p>Select New DataSet => Select File => File System and continue</p>

<p><a href=""https://i.stack.imgur.com/itRQ6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/itRQ6.png"" alt=""enter image description here""></a></p>

<p>Select Format= > Choose DelimitedText and continue</p>

<p><a href=""https://i.stack.imgur.com/BbATO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BbATO.png"" alt=""enter image description here""></a></p>

<p>=> Select previously created File system linked service and click on browse.</p>

<p><a href=""https://i.stack.imgur.com/SthfF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SthfF.png"" alt=""enter image description here""></a></p>

<p>Choose a file or folder.</p>

<p>Here you can find the file located under the previously selected folder while creating File System.</p>

<p><a href=""https://i.stack.imgur.com/WN4Jm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WN4Jm.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"57056928","Copy On-Prem Flat file to Azure Blob using Azure Data Factory V2","<p>I am trying to load a <strong>Flat file</strong> to <strong>BLOB</strong> using the <strong>ADF V2</strong>. I have installed the Self Hosted Integration Runtime for the same. The Integration Runtime on Local Machine shows that is successfully connected to the cloud Service as in the SC below.  However while making the LinkedService to the on Prem File, some credentials are required. I am not sure of what <strong>UserName</strong> or <strong>Password</strong> should be fed in. I have tried both On-Prem and Azure passwords (Wanted to try). Please see the SC.</p>

<p>Could you please guide as how the connection can be made to a local Flat file in my case. </p>

<p>Thanks 
- Akshay
<a href=""https://i.stack.imgur.com/0mN17.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0mN17.png"" alt=""Screenshot""></a></p>
","<azure><azure-data-factory>","2019-07-16 12:09:07","3009","1","2","57068627","<p>The LinkedService connection to BLOB or AzureSQL Server are being blocked by the firewall of my organisation. It won't let my system Integration runtime connect my resources to the public cloud.
I followed the same steps on my personal machine and everything went smoothly. Will get the firewall restrictions sorted and update this link for more information.</p>
"
"57046575","Driver connection to project","<p>I'm trying to follow <a href=""https://arcanecode.com/2014/01/14/importing-mongodb-data-using-ssis-2012/"" rel=""nofollow noreferrer"">this guide</a> on how to convert a MongoDB database to a SQL Server on Azure. I cannot connect the driver to the project to pull data.</p>

<blockquote>
  <p>Failure adding assembly to the cache: Attempt to install an assembly without a strong name</p>
</blockquote>

<p>I've already looked for other solutions to this problem on StackOverflow and they don't seem to apply to my problem. Is this guide the correct procedure to move data into the SQL Server?</p>
","<c#><sql-server><mongodb><visual-studio><azure-data-factory>","2019-07-15 20:12:43","74","2","1","57067390","<p>You could consider using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">Copy Activity</a> which supports <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-mongodb"" rel=""nofollow noreferrer"">MongoDB connector</a> as input and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database"" rel=""nofollow noreferrer"">SQL DB connector</a> as output.</p>

<p>Please refer to this detailed <a href=""https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/data-factory/v1/data-factory-on-premises-mongodb-connector.md"" rel=""nofollow noreferrer"">doc</a>.</p>
"
"57045979","Azure Data Flow Getting Error on Update and Insert","<p>When i am joining Two tables and allowing Alter Update and Insert getting this Error </p>

<p><strong>Allowing delete, insert, or update requires an Alter Row transformation to set row policies.</strong></p>

<p><a href=""https://i.stack.imgur.com/j0vNf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j0vNf.png"" alt=""Screenshot""></a></p>

<p>Then i created the Alter Rows Now i am getting this Error When i am running trigger</p>

<blockquote>
  <p>{""error"":{""code"":""BadRequest"",""message"":""ErrorCode=InvalidTemplate,
  ErrorMessage=Unable to parse expression 'Order
  ID'"",""target"":""pipeline/SaleDB/runid/1cf1e695-dff8-4330-8401-b4ebb86d573f"",""details"":null}}</p>
</blockquote>
","<azure><azure-data-factory><dataflow>","2019-07-15 19:21:15","1124","0","1","57070027","<p>Error regarding 'Order ID', this column is a duplicate mostly coming from the Distinct (Aggregate) and Select1 (duplicate stream for OriginalData). So you can put a select before sink drop the duplicate column. That will solve this problem.</p>

<p>Regarding AlterRow. Only alter row is the transformation which can mark rows to update, delete, upserts. In case you intention is to update some rows, then you need Alter Row, in case intention is not to update then you can remove Allow Update from sink.</p>
"
"57044070","Is there a way to author a SQL query from inside Azure Data Factory?","<p>Trying to find a way to author SQL queries from inside of ADF, is this possible or would a view need to be created to do something similar?</p>
","<sql><sql-server><azure-data-factory>","2019-07-15 16:48:55","2310","-1","3","57047826","<p>Yes , we can depending upon what is that you are trying to achieve , copy activity,  lookup activity are few examples </p>
"
"57044070","Is there a way to author a SQL query from inside Azure Data Factory?","<p>Trying to find a way to author SQL queries from inside of ADF, is this possible or would a view need to be created to do something similar?</p>
","<sql><sql-server><azure-data-factory>","2019-07-15 16:48:55","2310","-1","3","57049011","<p>Most of time, yes you can author SQL queries from inside of ADF. </p>

<p>For SQL server and Azure SQL database, copy data from or to SQL server or Azure SQL database support you use the SQL query to select the data.</p>

<p>For example, using 'Copy Data' to copy data between Azure SQL:
<a href=""https://i.stack.imgur.com/7hz6w.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7hz6w.png"" alt=""enter image description here""></a></p>

<p><strong>But these SQL queries are used for Source dataset settings</strong>.</p>

<p>There are a lot examples can help you understand Azure Data Factory Copy Actives.</p>

<p>Such as: <a href=""https://marlonribunal.com/copy-data-from-on-premise-sql-server-to-azure-database-using-azure-data-factory/"" rel=""nofollow noreferrer"">Copy Data From On-Premise SQL Server To Azure Database Using Azure Data Factory</a>.</p>

<p>I suggest you read more about <a href=""https://learn.microsoft.com/en-us/azure/data-factory/"" rel=""nofollow noreferrer"">Azure Data Factory Document</a>. You will have the answer more clearly.</p>

<p>Hope this helps.</p>
"
"57044070","Is there a way to author a SQL query from inside Azure Data Factory?","<p>Trying to find a way to author SQL queries from inside of ADF, is this possible or would a view need to be created to do something similar?</p>
","<sql><sql-server><azure-data-factory>","2019-07-15 16:48:55","2310","-1","3","57071815","<p>It depends on what you mean with ""author SQL queries"".</p>

<p>If you want to write sql queries based on input from other activities and execute them you can do this by using dynamic content (click on ""add dynamic content"" on the query field where you want to execute the sql query, e.g. in copy data or lookup)</p>

<p>If you want to run sql code on a data source from a data factory e.g. to create a table there you can use the lookup activity and just add a ;select 1; in the end. e.g.</p>

<pre><code>Select * into newtable from oldtable; select 1;
</code></pre>

<p>Also you could use a stored procedure on a data source e.g. azure sql or sql server and provide properties from data factory.</p>

<p>For all this best to create a dataset where you leave the ""table"" property empty and use this.</p>
"
"57043250","How to use merge functionality in data flow azure","<p>I know we can use Joins but i am having issues let me explains.</p>

<p>If i have a CSV file which have some data and there is row which have these values </p>

<p><strong>ID:3 ,Name:Sam ,Role 2</strong></p>

<p>Now the Data Source have the same data but name has been change from Sam to Man </p>

<p><strong>ID:3 ,Name:man ,Role 2</strong></p>

<p>In SSIS we use Merge which update the record.As i am learning Data Flow I have no idea What to use here</p>

<p>CSV-->Join-- Sink ? </p>
","<azure><azure-data-factory><dataflow>","2019-07-15 15:50:34","68","0","1","57082370","<p>In ADF Data Flows, Join will be the most natural transformation to use here.</p>

<p>Kamil Nowinski put together a nice cheat sheet for ADF-to-SSIS data flow translations: <a href=""https://sqlplayer.net/2018/12/azure-data-factory-v2-and-its-available-components-in-data-flows/"" rel=""nofollow noreferrer"">https://sqlplayer.net/2018/12/azure-data-factory-v2-and-its-available-components-in-data-flows/</a></p>
"
"57043010","Subtract number of days based on provided date parameter","<p>I created a pipeline in data factory and I want to retrieve data from a source for the current month and for the previous month. When I run the pipeline I give the needed parameter named ExtractDate. The format is MM/DD/YYYY . 
For the current month I used the following expression in 'Set Variable' activity: </p>

<pre><code>@replace(item().Query,'EXTRACTDATE',formatDateTime(variables('ExtractDate'), 'yyyyMM'))
</code></pre>

<p>And for the previous month I tried:</p>

<pre><code>@adddays(variables('ExtractDate'),-28)
</code></pre>

<p>The problem appears when the user will set when running the pipeline the date 07/31/2019 for example. Then the previous month will still be July. And if I increase the number to 31, then there is a possibility that the user will introduce 03/01/2019 and from March it will skip the month of February.  </p>

<p>I tried to think of a solution, but unfortunately there is no 'addmonths' available in data factory. </p>

<p>Any ideas please?...I've spent 2 days on this issue..</p>
","<c#><azure><azure-data-factory>","2019-07-15 15:37:00","3094","1","2","57049386","<p><code>addMonths</code> and <code>addYears</code> are not supported by ADF so far.Please vote up this <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/37471768-adfv2-date-function-need-addmonths-and-addyears"" rel=""nofollow noreferrer"">thread</a> to push the progress.</p>

<p>My trick is use combination of <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">bulit-in functions</a> in ADF. Please see my test:</p>

<p>This month is very simple:</p>

<pre><code>@concat(substring('07/16/2019',6,4),substring('07/16/2019',0,2))
</code></pre>

<p>output:</p>

<p><a href=""https://i.stack.imgur.com/pz2Be.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pz2Be.png"" alt=""enter image description here""></a></p>

<p>Last month is little complex.It should check if it is the first month of the year.</p>

<pre><code>@if(equals(substring('07/16/2019',0,2),'01'), 
concat(
string(sub(
int(substring('07/16/2019',6,4)),1)),
'12'),
string(sub(
int(concat(substring('07/16/2019',6,4),
substring('07/16/2019',0,2))),1)
)
)
</code></pre>

<p>if the input param is 01/16/2019,then the output looks like:</p>

<p><a href=""https://i.stack.imgur.com/WByKa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WByKa.png"" alt=""enter image description here""></a></p>

<p>My test is based on the static value,please replace it with your variable.</p>

<hr>

<p>Just for summarize:</p>

<p>The final working dynamic content should be like as below:</p>

<pre><code>@if( equals(variables('SubstringMonth'),'01'), 
concat(string(sub(int(variables('SubstringYear')),1)),'12'), 
concat(variables('SubstringYear'),string(if(or(equals(sub(int(variables('SubstringMonth')),1),11),equals(sub(int(variables('SubstringMonth')),1),10)), 
sub(int(variables('SubstringMonth')),1) , 
concat('0',string(sub(int(variables('SubstringMonth')),1) )))) ))
</code></pre>
"
"57043010","Subtract number of days based on provided date parameter","<p>I created a pipeline in data factory and I want to retrieve data from a source for the current month and for the previous month. When I run the pipeline I give the needed parameter named ExtractDate. The format is MM/DD/YYYY . 
For the current month I used the following expression in 'Set Variable' activity: </p>

<pre><code>@replace(item().Query,'EXTRACTDATE',formatDateTime(variables('ExtractDate'), 'yyyyMM'))
</code></pre>

<p>And for the previous month I tried:</p>

<pre><code>@adddays(variables('ExtractDate'),-28)
</code></pre>

<p>The problem appears when the user will set when running the pipeline the date 07/31/2019 for example. Then the previous month will still be July. And if I increase the number to 31, then there is a possibility that the user will introduce 03/01/2019 and from March it will skip the month of February.  </p>

<p>I tried to think of a solution, but unfortunately there is no 'addmonths' available in data factory. </p>

<p>Any ideas please?...I've spent 2 days on this issue..</p>
","<c#><azure><azure-data-factory>","2019-07-15 15:37:00","3094","1","2","58992439","<p>For the previous month from today you could use</p>

<pre><code>    formatDateTime(AddToTime(utcnow(), -1, 'Month'), 'yyyy-MM-dd')
</code></pre>
"
"57041400","Data Factory v2 add Query to Table Storage with function addmonths()","<p>We want to filter the data from a Table Storage since the first day of the last month using the utcnow() function. </p>

<p>For this we are applying a workaround in the first 5 days of the month using the following query: Timestamp ge datetime'@{formatDateTime(adddays(utcnow(),-5),'yyyy-MM-01')).
However this implementation is limited to the trigger until a specific day of the month. </p>

<p>Is there any other way to use a function as addmonths(-1) for this query?</p>

<p>Thanks in advance!</p>
","<azure-table-storage><azure-data-factory>","2019-07-15 14:01:10","714","2","1","57069697","<p><code>addMonths</code> and <code>addYears</code> are not supported by ADF so far which is mentioned in my previous case:<a href=""https://stackoverflow.com/questions/57043010/subtract-number-of-days-based-on-provided-date-parameter/57049386#57049386"">Subtract number of days based on provided date parameter</a> </p>

<p>Provide a trick for you,using substring and math function in dynamic content:</p>

<pre><code>@formatDateTime(adddays(utcnow(),mul(add(int(substring(formatDateTime(utcnow(),'yyyy-MM-dd'),8,2)),1),-1)),'yyyy-MM-01')
</code></pre>

<p>For example, utcnow is <code>7/17/2019....</code>, the result calculated by above expression is :</p>

<p><a href=""https://i.stack.imgur.com/8WUVl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8WUVl.png"" alt=""enter image description here""></a></p>

<p>My idea is minus days+1 based on the now date. Please do a try.</p>
"
"57041039","Calling Azure AD secure functions from Azure Data Factory","<p>I recently secured my azure functions by Azure Active Directory. Hence you need access token set in auth header to enable you to call them. I am successfully able to do that from my Front end Angular apps. But in my backend I have Azure Data factory as well, How can i enable Azure Data factory to use Azure AD while calling functions and not host key?</p>
","<azure><azure-active-directory><azure-functions><azure-data-factory>","2019-07-15 13:40:35","763","1","2","57047873","<p>You can use the web activity to get the bearer token and then pass this to the subsequent calls .</p>
"
"57041039","Calling Azure AD secure functions from Azure Data Factory","<p>I recently secured my azure functions by Azure Active Directory. Hence you need access token set in auth header to enable you to call them. I am successfully able to do that from my Front end Angular apps. But in my backend I have Azure Data factory as well, How can i enable Azure Data factory to use Azure AD while calling functions and not host key?</p>
","<azure><azure-active-directory><azure-functions><azure-data-factory>","2019-07-15 13:40:35","763","1","2","57047944","<p>Azure Data Factory supports Managed Identity:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-factory-service-identity"" rel=""nofollow noreferrer"">Managed identity for Data Factory</a></p>
<blockquote>
<p>When creating a data factory, a managed identity can be created along with factory creation. The managed identity is a managed application registered to Azure Activity Directory, and represents this specific data factory.</p>
<p>Managed identity for Data Factory benefits the following features:</p>
<ul>
<li>Store credential in Azure Key Vault, in which case data factory managed identity is used for Azure Key Vault authentication.</li>
<li>Connectors including Azure Blob storage, Azure Data Lake Storage Gen1, Azure Data Lake Storage Gen2, Azure SQL Database, and Azure SQL Data Warehouse.</li>
<li>Web activity.</li>
</ul>
</blockquote>
"
"57040646","How to pass data generated by a Databricks notebook to a Python step?","<p>I am building an Azure Data Factory v2, which comprises</p>

<ul>
<li>A Databricks step to query large tables from Azure Blob storage and generate a tabular result <code>intermediate_table</code>;</li>
<li>A Python step (which does several things and would be cumbersome to put in a single notebook) to read the <code>processed_table</code> and generate the final output.</li>
</ul>

<p>And looks like this</p>

<p><a href=""https://i.stack.imgur.com/g0CW8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g0CW8.png"" alt=""enter image description here""></a></p>

<p>The notebook generates a <code>pyspark.sql.dataframe.DataFrame</code> which I tried to save into parquet format with attempts like</p>

<pre><code>processed_table.write.format(""parquet"").saveAsTable(""intermediate_table"", mode='overwrite')
</code></pre>

<p>or </p>

<pre><code>processed_table.write.parquet(""intermediate_table"", mode='overwrite')
</code></pre>

<p>Now, I would like the Python step to re-read the intermediate result, ideally with a <code>postprocess.py</code> file with a syntax like</p>

<pre><code>import pandas as pd
intermediate = pd.read_parquet(""intermediate_table"")
</code></pre>

<p>after having installed <code>fastparquet</code> inside my Databricks cluster.<br>
This is (not surprisingly...) failing with errors like</p>

<blockquote>
  <p>FileNotFoundError: [Errno 2] No such file or directory:
  './my_processed_table'</p>
</blockquote>

<p>I assume the file is not found because the Python file is not accessing the data in the right context/path.</p>

<p>How should I amend the code above, and what would be the best/canonical ways to pass data across such steps in a pipeline? (any other advice on common/best practices to do this are welcome)</p>
","<python><pyspark><azure-data-factory><azure-databricks><fastparquet>","2019-07-15 13:20:01","1254","0","1","57048056","<p>One way to run the pipeline successfully is to have in the Databricks notebook a cell like</p>

<pre><code>%python

spark.conf.set(""spark.sql.execution.arrow.enabled"", ""true"")
import pandas as pd
processed_table.toPandas().to_parquet(""/dbfs/intermediate"", engine=""fastparquet"", compression = None)
</code></pre>

<p>and then have in <code>preprocess.py</code></p>

<pre><code>import pandas as pd
intermediate = pd.read_parquet(""/dbfs/intermediate"")
</code></pre>

<p>not sure if that's good practice (it works though).</p>
"
"57032626","How to use azure data factory migrate table in storage account, that have column is many type","<p>I want to use Data Factory to migrate data in the storage account, but data in the original table is a many type ex: some data in column <code>int</code>, <code>String</code>, <code>DateTime</code>.<br>
When I use Data Factory I need to specify the data type, so how I can definite dynamic type and copy column. Because all data migrate parsed to <code>String</code> type, so how can I keep value type of column?<br>
This my data in the original table
<a href=""https://i.stack.imgur.com/hDEje.png"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p>Thanks for your help</p>
","<azure-data-factory>","2019-07-15 02:26:48","144","2","2","57049363","<p>Not sure if I am understanding the question , but let me first put forward my understanding , you want to copy  a table lets say sourceT1 to SinkT1 , if that's the case you can always use the copy activity and then map the columns . When you map the columns it does set the data type also . </p>
"
"57032626","How to use azure data factory migrate table in storage account, that have column is many type","<p>I want to use Data Factory to migrate data in the storage account, but data in the original table is a many type ex: some data in column <code>int</code>, <code>String</code>, <code>DateTime</code>.<br>
When I use Data Factory I need to specify the data type, so how I can definite dynamic type and copy column. Because all data migrate parsed to <code>String</code> type, so how can I keep value type of column?<br>
This my data in the original table
<a href=""https://i.stack.imgur.com/hDEje.png"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p>Thanks for your help</p>
","<azure-data-factory>","2019-07-15 02:26:48","144","2","2","57088158","<p>According my experience in Data factory, Data Factory can not help you keep value type of column in source table. You must specify the data type in sink dataset.</p>

<p>Copy Data:
<a href=""https://i.stack.imgur.com/CgKLs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CgKLs.png"" alt=""enter image description here""></a></p>

<p>As you have tried, if you didn't set the sink data type, the column type will passed  <code>String</code> in default.</p>

<p>I have an idea is that cope the data twice, each time copy the different entity column. The sink dataset support 'Merge' and 'Replace'. </p>

<p>Hope this helps.</p>
"
"57028175","How to QA test Azure Data Factory?","<p>I am from QA team. My dev team has created pipelines in Azure Data factory. They want me to QA test them. I need to write manual test cases and later after some time I also need to automate this. Please guide me how/ what to test using manual test case. Also suggest me automation tool for later stage that I should use to create automation test cases. Selenium?  </p>
","<azure><azure-data-factory>","2019-07-14 14:15:54","5686","1","3","57028656","<p>You can take a look at this blog post, it really helped me when I started with testing in ADF: <a href=""https://blogs.msdn.microsoft.com/karang/2018/11/18/azure-data-factory-v2-pipeline-functional-testing/"" rel=""nofollow noreferrer"">https://blogs.msdn.microsoft.com/karang/2018/11/18/azure-data-factory-v2-pipeline-functional-testing/</a></p>

<p>You won't be able to test everything in Data Factory, at most you can check if connection strings are correct, queries dont break, objects are present (in database or blob storage or whatever you data source is), etc. Testing if the end result of a pipeline is what you intended to do, is highly dependent of the use case and most of the time its not worth it.</p>

<p>I'm not an expert, but as far as I know, Selenium is used to automate browser testing related stuff. Here you won't need a complex framework, you can get away with using a Powershell script as described in the blog post, but you also have other options like Python, .NET, REST api.</p>

<p>Hope this helped!!</p>
"
"57028175","How to QA test Azure Data Factory?","<p>I am from QA team. My dev team has created pipelines in Azure Data factory. They want me to QA test them. I need to write manual test cases and later after some time I also need to automate this. Please guide me how/ what to test using manual test case. Also suggest me automation tool for later stage that I should use to create automation test cases. Selenium?  </p>
","<azure><azure-data-factory>","2019-07-14 14:15:54","5686","1","3","57036681","<p>Our Q&amp;A team just changes the settings to see the pipeline behavior, uses not normal data to push trough the pipeline, different time zones and timestamps and etc.. But the majority of the test are the final pipeline results.</p>
"
"57028175","How to QA test Azure Data Factory?","<p>I am from QA team. My dev team has created pipelines in Azure Data factory. They want me to QA test them. I need to write manual test cases and later after some time I also need to automate this. Please guide me how/ what to test using manual test case. Also suggest me automation tool for later stage that I should use to create automation test cases. Selenium?  </p>
","<azure><azure-data-factory>","2019-07-14 14:15:54","5686","1","3","63391663","<p>I have used a Specflow project (<a href=""https://specflow.org/"" rel=""nofollow noreferrer"">https://specflow.org/</a>) and supporting .Net code to set up the tests and execute the pipeline on test files held in the project. You can automate this into your build or release pipelines.</p>
"
"57009244","Azure Data Factory - Source to Sink custom mapping converting string to object","<p>Source: Azure storage table has three fields: PartitionKey(string), RowKey(string) and Value (string).  </p>

<p>Sink: cosmo db container with three fields: id (string), RowKey(string) and Value (object). </p>

<p>I want to convert the source's value to an object instead of a string so that cosmodb indexes it that way.   If I do a mapping via the UI it is synced over as a plain string:</p>

<pre><code>{  ""Value"": ""{\""abc\"":\""def\"",\""id\"":\""1a076c19ff8b41489563453ffbbbb931\""}"" }
</code></pre>

<p>Where I want it to be like:</p>

<pre><code>{ ""Value"": {""abc"":""def"", ""id"":""123""} }
</code></pre>

<p>I need to use the dynamic mappings but after reading the documentation it isn't clear how to do this.</p>
","<azure><azure-cosmosdb><azure-data-factory>","2019-07-12 14:40:50","1039","0","2","57036731","<p>As I know, no such features could help you convert string data to object format in adf cosmos db configuration. </p>

<p>Please consider other ways. Since you are using adf to import data so that you can't use <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/programming#database-pre-triggers"" rel=""nofollow noreferrer"">PreTrigger</a> to change the format of created documents.PreTrigger need to be invoked by code or rest api. </p>

<p>So, as workaround, I suggest you using <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-cosmos-db-triggered-function"" rel=""nofollow noreferrer"">Azure Function Cosmos DB Trigger</a> to process every document when they imported into database. Please refer to my function code:</p>

<pre><code>using System.Collections.Generic;
using Microsoft.Azure.Documents;
using Microsoft.Azure.WebJobs;
using Microsoft.Azure.WebJobs.Host;
using Newtonsoft.Json.Linq;
using System;
using Microsoft.Azure.Documents.Client;

namespace TestADF
{
    public static class Function1
    {
        [FunctionName(""Function1"")]
        public static void Run([CosmosDBTrigger(
            databaseName: ""db"",
            collectionName: ""item"",
            ConnectionStringSetting = ""documentdbstring"",
            LeaseCollectionName = ""leases"")]IReadOnlyList&lt;Document&gt; input, TraceWriter log)
        {
            if (input != null &amp;&amp; input.Count &gt; 0)
            {
                log.Verbose(""Start........."");
                String endpointUrl = ""https://***.documents.azure.com:443/"";
                String authorizationKey = ""key"";
                String databaseId = ""db"";
                String collectionId = ""item"";

                DocumentClient client = new DocumentClient(new Uri(endpointUrl), authorizationKey);

                for (int i = 0; i &lt; input.Count; i++)
                {
                    Document doc = input[i];
                    if ((doc.GetPropertyValue&lt;String&gt;(""Value"") == null) || (!doc.GetPropertyValue&lt;String&gt;(""Value"")))
                    {                       
                        String V= doc.GetPropertyValue&lt;String&gt;(""Value"");
                        JObject obj = JObject.Parse(V);

                        doc.SetPropertyValue(""Value"", obj );

                        client.ReplaceDocumentAsync(UriFactory.CreateDocumentUri(databaseId, collectionId, doc.Id), doc);

                        log.Verbose(""Update document Id "" + doc.Id);
                    }

                }
            }
        }
    }
}
</code></pre>
"
"57009244","Azure Data Factory - Source to Sink custom mapping converting string to object","<p>Source: Azure storage table has three fields: PartitionKey(string), RowKey(string) and Value (string).  </p>

<p>Sink: cosmo db container with three fields: id (string), RowKey(string) and Value (object). </p>

<p>I want to convert the source's value to an object instead of a string so that cosmodb indexes it that way.   If I do a mapping via the UI it is synced over as a plain string:</p>

<pre><code>{  ""Value"": ""{\""abc\"":\""def\"",\""id\"":\""1a076c19ff8b41489563453ffbbbb931\""}"" }
</code></pre>

<p>Where I want it to be like:</p>

<pre><code>{ ""Value"": {""abc"":""def"", ""id"":""123""} }
</code></pre>

<p>I need to use the dynamic mappings but after reading the documentation it isn't clear how to do this.</p>
","<azure><azure-cosmosdb><azure-data-factory>","2019-07-12 14:40:50","1039","0","2","57049422","<p>You need to use the data flow ( which is in preview at this time ) , but this is preetty straight forward . The thing which I am not sure if converting the string to a JSON , but there is function which you can use to convert to JSON its called ""json"" , please do let me know how it goes .</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-create"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-create</a></p>
"
"57007733","Is it possible to add a column with specific value to data in azure data factory pipeline","<p>I am copying data from system X to a blob storage as parquet file or excel file is it possible to add one more step that can help me add one more column with the pipeline run ID or Trigger ID?</p>

<p>Thank you in Advance</p>

<p><a href=""https://i.stack.imgur.com/KmdJC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KmdJC.png"" alt=""enter image description here""></a></p>
","<azure><logging><metadata><pipeline><azure-data-factory>","2019-07-12 13:13:24","2883","1","1","57008208","<p>Where are you extracting the data from? If its a database, it is easy because you can add it in the sql statement used when selecting data. For example:</p>

<pre><code>select *, NewColumn='Value' from yourTable
</code></pre>

<p>If you want a solution for every data source possible, you can use the derived column transformation in data flow: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column</a></p>

<p>Also you can add data from the pipeline itself using string functions, for example:</p>

<pre><code>@concat('select *, pipeId= ''', pipeline().RunId,''' from SalesLT.Address')
</code></pre>

<p>This will select all the fields, and an additional field called pipeId which will have the same value for every row, and will be the pipeline run id.</p>

<p>Hope this helped!!</p>
"
"57006125","VS update broke the microsoft.analytics.dll","<p>I have a solution with two projects (C# and U-SQL) to run in the Azure Data Factory. I have referenced microsoft.analytics to use some analytics functions. </p>

<p>After the VS update to 15.9.14 on July 9, 2019 the c# project was not building giving the error that the assembly is missing but it was not missing. If I clicked to the error it go away and recognize the assembly. Then I try to build again and same error appears. </p>

<p>So I check the dll and the version was the same as before update but after I opened it with dotPeek I saw that the assembly target framework was changed to 4.7.2.</p>

<p>My project is on 4.5 because of the <strong>official azure data factory documentation</strong> says that we <strong>can run projects only up to framework 4.5</strong> so I can't upgrade to 4.7 or above.</p>

<p>You can see the difference of those dll. 
<a href=""https://i.stack.imgur.com/VrRwS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VrRwS.png"" alt=""enter image description here""></a></p>

<p><em>Assemblies location:</em></p>

<blockquote>
  <p>C:\Program Files (x86)\Microsoft Visual
  Studio\2017\Enterprise\Common7\IDE\PublicAssemblies</p>
</blockquote>

<p>So to resolve the problem I froze those dll and added as external references to my project in order to compile and build it and it worked.</p>

<p>If I want to create a new U-SQL project I can only pick 4.7.2.
<a href=""https://i.stack.imgur.com/00PGT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/00PGT.png"" alt=""enter image description here""></a></p>

<p>My question is, how I can <strong>run U-SQL projects</strong> and analytics assemblies on an framework that is still <strong>not supported</strong> in the azure data lake?
If the Data Lake now supports <strong>4.7.2</strong> why there is no documentation for it or at least something to let us know?
Is this an VS minor update with a <strong>bug</strong>?</p>
","<c#><visual-studio><azure-data-factory><azure-data-lake><u-sql>","2019-07-12 11:30:07","104","2","1","57097081","<p>So after some Microsoft tickets they don't know how to respond to this. They said to try run some methods that are only present in the 4.7.2 framework in the data lake.
So I tried those methods and the jobs didn't run for obvious reasons.</p>

<p><strong>Conclusion:</strong></p>

<ul>
<li>VS migrated the U-SQL projects to 4.7.2 but we can't run custom c# code for this target framework in the Azure.</li>
<li>Data Lake Analytics runs only 4.5 as said in documentation.</li>
</ul>

<p>For now our project  will remain 4.5.</p>
"
"56996858","How to set up output path while copying data from Azure Cosmos DB to ADLS Gen 2 via Azure Data Factory","<p>I have a cosmos DB collection in the following format:</p>
<pre><code>{
    &quot;deviceid&quot;: &quot;xxx&quot;,
    &quot;partitionKey&quot;: &quot;key1&quot;,
    .....
    &quot;_ts&quot;: 1544583745
}
</code></pre>
<p>I'm using Azure Data Factory to copy data from Cosmos DB to ADLS Gen 2. If I copy using a copy activity, it is quite straightforward.  However, my main concern is the output path in ADLS Gen 2.  Our requirements state that we need to have the output path in a specific format.  Here is a sample of the requirement:</p>
<pre><code>outerfolder/version/code/deviceid/year/month/day
</code></pre>
<p>Now since deviceid, year, month, day are all in the payload itself I can't find a way to use them except create a lookup activity and use the output of the lookup activity in the copy activity.</p>
<p><a href=""https://i.stack.imgur.com/2terZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2terZ.png"" alt=""enter image description here"" /></a></p>
<p>And this is how I set the ouput folder using the dataset property:</p>
<p><a href=""https://i.stack.imgur.com/lS2In.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lS2In.png"" alt=""enter image description here"" /></a></p>
<p>I'm using SQL API on Cosmos DB to query the data.</p>
<p>Is there a better way I can achieve this?</p>
","<azure><azure-data-factory>","2019-07-11 20:29:29","150","0","1","56997210","<p>I think that your way works, but its not the cleanest. What I'd do is create a different variable inside the pipeline for each one: version, code, deviceid, etc. Then, after the lookup you can assign the variables, and finally do the copy activity referencing the pipeline variables.</p>

<p><a href=""https://i.stack.imgur.com/DZc7Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DZc7Q.png"" alt=""Proposed pipeline""></a></p>

<p>It may look kind of redundant, but think of someone (or you 2 years from now) having to modify the pipeline and if you are not around (or have forgotten), this way makes it clear how it works, and what you should modify.</p>

<p>Hope this helped!!</p>
"
"56995437","Azure Data Factory - Dynamic Account information - Parameterization of Connection","<p>The documentation demonstrates how to create a parameter for a connected service but not how to actual pass in that parameter from a dataset or activity. Basically the connection string is coming from a lookup foreach loop and I want to connect to a storage table.</p>

<p>The connection looks like this.  The test works when passing in a correct parameter:</p>

<pre><code>{
    ""name"": ""StatsStorage"",
    ""properties"": {
        ""type"": ""AzureTableStorage"",
        ""parameters"": {
            ""connectionString"": {
                ""type"": ""String""
            }
        },
        ""annotations"": [],
        ""typeProperties"": {
            ""connectionString"": ""@{linkedService().connectionString}""
        }
    }
}
</code></pre>

<p>The dataset is the following, I'm struggling to determine how to set the connectionString parameter for the connection.  The dataset has two parameters, the connectionstring from the db and the tablename that it needs to connect to:</p>

<pre><code>{
    ""name"": ""TestTable"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""StatsStorage"",
            ""type"": ""LinkedServiceReference""
        },
        ""parameters"": {
            ""ConnectionString"": {
                ""type"": ""string""
            },
            ""TableName"": {
                ""type"": ""string""
            }
        },
        ""annotations"": [],
        ""type"": ""AzureTable"",
        ""schema"": [],
        ""typeProperties"": {
            ""tableName"": {
                ""value"": ""@dataset().TableName"",
                ""type"": ""Expression""
            }
        }
    }
}
</code></pre>

<p>How do I set the connection string on the connection?</p>
","<azure><azure-data-factory>","2019-07-11 18:37:09","224","0","1","56999748","<ol>
<li>First, you can't make the whole connection string as an expression. You need provide accountName and accountKey sperately. Refer this post about how to do it. <a href=""https://stackoverflow.com/questions/56932728/how-to-provide-connection-string-dynamically-for-azure-table-storage-blob-storag/56934195#56934195"">How to provide connection string dynamically for azure table storage/blob storage in Azure data factory Linked service</a></li>
<li>Then, if you are using ADF UI, it will guide you how to provide value for linked service. For example, if you have two dataset parameters, you could specify it as following. <a href=""https://i.stack.imgur.com/9a0tr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9a0tr.png"" alt=""enter image description here""></a></li>
<li>If you want to see json code, you could click the code icon on the top left corner. 
<a href=""https://i.stack.imgur.com/CZhcg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CZhcg.png"" alt=""enter image description here""></a></li>
<li>I am using azure blob as an example, but the azure table is almost the same. 
Hope it could help. </li>
</ol>
"
"56995216","Do we have Merge functionality in Data Flow Azure","<p>What i am trying to do on Azure Data flow is that Source data update to my destination database It may Update records or enter new rows respect to source data.</p>

<p>I know we have similar functionality in SSIS package called merge, but I am not sure we have that functionality on data Flow, so is it someway around we can achieve this.</p>
","<azure><azure-data-factory>","2019-07-11 18:19:10","38","0","1","56997051","<p>Its not called Merge, but you can achieve what you want by using the Join transformation.</p>

<p>Read more here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-join"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-join</a> It even mentions SSIS merge and compares to it.</p>

<p>Hope this helped!!</p>
"
"56991718","Extracting array properties from Cosmos DB documents using Azure Data Factory","<p>I have an Azure Data Factory v2 pipeline that's pulling data from a Cosmos DB collection. This collection has a property that's an array.</p>

<p>I want to, at the least, be able to dump that entire property's value into a column in SQL Azure. I don't need it parsed (although that would be great too), but ADF lists this column as ""Unsupported Type"" in the dataset definition and listed it in the Excluded Columns section.</p>

<p>Here is an example of the JSON I'm working with. The property I want is ""MyArrayProperty"":</p>

<pre><code>{
    ""id"": ""c4e2012e-af82-4c48-8960-11e0436e6d3f"",
    ""Created"": ""2019-06-14T16:04:13.9572567Z"",
    ""Updated"": ""2019-06-14T16:04:14.1920988Z"",
    ""IsActive"": true,
    ""MyArrayProperty"": [
                {
                    ""SomeId"": ""a4427015-ca69-4958-90d3-0918fd5dcac1"",
                    ""SomeName"": ""BlahBlah""
                }
            ]
    }
}
</code></pre>

<p>I've tried manually specifying a column in the ADF data source like ""MyArrayProperty"" and using a string data type, but the value always comes across as null.</p>
","<json><azure><azure-cosmosdb><azure-data-factory>","2019-07-11 14:36:05","999","0","2","56995875","<p>please check <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping"" rel=""nofollow noreferrer"">this</a> document about schema mapping example between MongoDB and Azure SQL. Basically you should define your <em>collectionReference</em> that will iterate through your nested array of objects and do cross apply. </p>
"
"56991718","Extracting array properties from Cosmos DB documents using Azure Data Factory","<p>I have an Azure Data Factory v2 pipeline that's pulling data from a Cosmos DB collection. This collection has a property that's an array.</p>

<p>I want to, at the least, be able to dump that entire property's value into a column in SQL Azure. I don't need it parsed (although that would be great too), but ADF lists this column as ""Unsupported Type"" in the dataset definition and listed it in the Excluded Columns section.</p>

<p>Here is an example of the JSON I'm working with. The property I want is ""MyArrayProperty"":</p>

<pre><code>{
    ""id"": ""c4e2012e-af82-4c48-8960-11e0436e6d3f"",
    ""Created"": ""2019-06-14T16:04:13.9572567Z"",
    ""Updated"": ""2019-06-14T16:04:14.1920988Z"",
    ""IsActive"": true,
    ""MyArrayProperty"": [
                {
                    ""SomeId"": ""a4427015-ca69-4958-90d3-0918fd5dcac1"",
                    ""SomeName"": ""BlahBlah""
                }
            ]
    }
}
</code></pre>

<p>I've tried manually specifying a column in the ADF data source like ""MyArrayProperty"" and using a string data type, but the value always comes across as null.</p>
","<json><azure><azure-cosmosdb><azure-data-factory>","2019-07-11 14:36:05","999","0","2","57011871","<p>There may be a better way to solve this problem, but I ended up creating a second copy activity which uses a query against Cosmos rather than a collection based capture. The query flattened the array like so:</p>

<pre><code>SELECT m.id, c.SomeId, c.SomeName
FROM myCollection m join c in m.MyArrayProperty
</code></pre>

<p>I then took this data set and dumped it into a table in SQL then did my other work inside SQL Azure itself. You could also use the new Join pipeline task to do this in memory before it gets to the destination.</p>
"
"56989959","Why is data not being transferred to my sink table after successfully completed Data Flow in Azure Data Factory?","<p>I am very new to both SQL and Azure Data Factory and am trying to import some data from one table to another in the same azure sql database using azure data factory. To be able to use the data in my sink table, i need to transform some of the rows in the source. 
My flow looks like this:<a href=""https://i.stack.imgur.com/3y6YN.png"" rel=""nofollow noreferrer"">Data flow in Azure data factory</a>.</p>

<p>The dataflow executes successfully:
<a href=""https://i.stack.imgur.com/Y5etC.png"" rel=""nofollow noreferrer"">Data flow results</a>.
However, data rows are not being copied to my sink table. </p>

<p>I've even tried the ""Recreate"" table option on the sink, I can see that my column names in the sink table are being overwritten to match the source table, but still no rows are being transferred to the sink, they stay empty.</p>

<p>Any suggestions on what I might be doing wrong?</p>

<p>Thanks in advance!</p>
","<sql><azure><pipeline><azure-data-factory><dataflow>","2019-07-11 13:06:05","1440","0","1","56990362","<p>Found out what was wrong. In the alter row step of the data flow, I had an update condition first and insert condition second. When I removed ""Update"" as the first condition all data was successfully inserted in my sink table.
I originally thought that alter row step operated the conditions with an ""OR"" statement, but it seems it goes in order first to last statement. Since my first one was an update of the data that was not present in my sink table, it did not seem to jump to the insert condition at all. </p>
"
"56984261","Conditional deployment of Azure Data Factory pipelines to another resource group","<p>I'm deploying my Azure Data Factory from one resource group (say <code>dev</code>) to another (say <code>prod</code>) using Azure DevOps Release Pipeline as mentioned <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">here</a>. </p>

<p>At any point of time, in my <code>dev</code> Data Factory, I will have some pipelines which are ready to be published/deployed to <code>prod</code> and some which are not.</p>

<p>Currently, whenever I hit <code>Publish</code> on the Data Factory Author portal, all my <code>dev</code> Data Factory pipelines are deployed to the <code>prod</code> Data Factory. However, I want only a select few pipelines to be deployed to my <code>prod</code> Data Factory and not all of them. </p>

<p>I figured that Azure Resource Manager offers the <a href=""https://learn.microsoft.com/en-us/azure/azure-resource-manager/resource-group-authoring-templates#condition"" rel=""nofollow noreferrer""><code>condition</code></a> attribute. However, I cannot use this since my template is automatically generated (in <code>adf-publish</code> branch). </p>

<p>Any help on how I can conditionally deploy only certain <code>dev</code> Data Factory pipelines to my <code>prod</code> Data Factory is much appreciated.</p>
","<azure><azure-devops><azure-resource-manager><azure-data-factory><continuous-delivery>","2019-07-11 07:56:28","611","2","1","57072699","<p>It is not supported as the doc stated. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#unsupported-features"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#unsupported-features</a></p>
"
"56984020","Azure Data Factory Expression Query for Copy activity","<p>I am trying to copy data from Table storage to another Table storage of a different storage account, for that, I am using Copy activity in Azure data factory.</p>

<p>I want to filter rows that are going to be copied to the sink table storage, for that Azure data factory gives an option to define a query.
I want to apply a filter on the Partition key whose datatype is String but holds numeric values.
I am looking at this documentation:
<a href=""https://learn.microsoft.com/en-us/azure/devops/pipelines/process/expressions?view=azure-devops"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/devops/pipelines/process/expressions?view=azure-devops</a>
there it says that type conversion is implicit for comparison operators like ""eq"", ""le"", ""ge"" etc </p>

<p>So if my query is ""<code>PartitionKey eq 0</code>"" it fails and gives this error:</p>

<pre><code>A storage operation failed with the following error 'The remote server returned an error: (400) Bad Request.'.. Activity ID:edf8e608-d25e
</code></pre>

<p>But if I define my query as ""<code>PartitionKey eq '0'</code>"" it works.</p>

<p>I want to fetch rows with in the certain range of numbers for that I need to cast my partition key to a numeric value, How do I do that?</p>

<p>Also the ""<code>startsWith</code>"" and ""<code>endsWith</code>"" don't work
e.g, this query <code>PartitionKey startsWith '10'</code> gives the same error as above.</p>

<p>Looks like this:
<a href=""https://i.stack.imgur.com/cPems.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cPems.png"" alt=""enter image description here""></a> 
Thanks in advance.</p>
","<azure><copy><azure-table-storage><azure-data-factory>","2019-07-11 07:42:41","1512","0","1","56989843","<p>Firstly, to make sure that your query works - you can use <strong>Storage Explorer (preview)</strong> in Azure Portal to build the query in <em>Query Builder</em> mode:<br>
<a href=""https://i.stack.imgur.com/Z3Mla.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Z3Mla.png"" alt=""Query built with Query builder""></a>  </p>

<p>and then switch to <em>Text Editor</em>: </p>

<p><a href=""https://i.stack.imgur.com/WxwS5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WxwS5.png"" alt=""enter image description here""></a></p>

<p>Now, you are sure that you have got <strong>right query</strong>.<br>
Let's apply this query to ADF. Without dynamic content - it will be exactly the same query:<br>
<a href=""https://i.stack.imgur.com/AYhtL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AYhtL.png"" alt=""enter image description here""></a></p>

<p><strong>In order to create a dynamic query</strong> - we need to add variables or parameters to define the boundary:<br>
<a href=""https://i.stack.imgur.com/ZTi86.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZTi86.png"" alt=""enter image description here""></a></p>

<p>Afterward, create a dynamic content in query field, replacing query:</p>

<pre><code>PartitionKey ge '0' and PartitionKey le '1'
</code></pre>

<p>with the following form using <strong>concat</strong> function:</p>

<pre><code>@concat('PartitionKey ge ''0'' and PartitionKey lt ''1''')
</code></pre>

<p>Notice, that I must enquote single quote (') by adding extra one ('').<br>
In the end - we need just to replace hard-coded values with previously defined parameters:</p>

<pre><code>@concat('PartitionKey ge ''',pipeline().parameters.PartitionStart,''' and PartitionKey lt ''',pipeline().parameters.PartitionEnd,'''')
</code></pre>

<p>That's all. I hope that I explain how to achieve that by building dynamic content (query) in Azure Data Factory.</p>
"
"56974054","Is there a way to create a single copy data pipeline that shares a single source data set and file system connection pointing to different drives?","<p>I'm attempting to deploy an azure data factory with a copy data pipeline that pulls files from one or more deployed / on-prem file system paths and dumps them in blob storage. The source file paths on the file system may span multiple different drives (e.g. - C:\fileshare1 vs D:\fileshare2) and may include network locations referenced via UNC paths (e.g. - \\localnetworkresource\fileshare3).  </p>

<p>I'd like to configure a single local file system connection and source data set and just parameterize the linked service's host property. Then my pipeline would just iterate over a collection of file share paths and reuse the dataset and linked service connection. However, it doesn't look like there's any way to have the data set or pipeline  provide the host information to the linked service.  It's certainly possible to provide folder information from the pipeline and dataset, but that will be concatenated to the host specified in the linked service connection and therefore won't allow me access to different drives or network resources.  </p>

<p>It was reasonably straightforward to do this by configuring separate linked service connections, data sets and pipelines for each distinct file share that needed to be included, but I'd prefer to manage a single pipeline.</p>
","<azure-resource-manager><azure-data-factory>","2019-07-10 15:30:34","347","0","1","57002139","<p>Yes, you can parameterize linked service. </p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services</a></p>

<p>Currently, ADF UI only supports 8 kinds of linked service parameterization. But actually all the linked service are supported in ADF Runtime. You could use json code to do it. 
Refer these two post here:</p>

<p><a href=""https://stackoverflow.com/questions/56995437/azure-data-factory-dynamic-account-information-parameterization-of-connectio/56999748#56999748"">Azure Data Factory - Dynamic Account information - Parameterization of Connection</a></p>

<p><a href=""https://stackoverflow.com/questions/56932728/how-to-provide-connection-string-dynamically-for-azure-table-storage-blob-storag/56934195#56934195"">How to provide connection string dynamically for azure table storage/blob storage in Azure data factory Linked service</a></p>
"
"56959547","Azure data factory copy data","<p>I want to get the same schema as my source dataset without creating it in the Azure database.</p>

<p>Can I get the same schema as the source data set without creating a table in an azure database while copying the data?</p>
","<azure><azure-data-factory>","2019-07-09 19:40:43","53","0","1","56965249","<blockquote>
  <p>Can I get the same schema as the source data set without creating a
  table in azure data base while copying the data.</p>
</blockquote>

<p>Copy activity needs real being input and output data source.If you want to get the schema, you have to set a real being dataset. It could not be limited to azure database, it could be some <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest"" rel=""nofollow noreferrer"">rest api</a> or on-prem <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system"" rel=""nofollow noreferrer"">File system</a> which provides data schema.</p>
"
"56949173","How can I consume this Rest API in Azure Data Factory","<p>I have a REST API I need to call from Azure Data Factory and insert the data into a SQL table. </p>

<p>The format of the JSON returned from the API is in the following format: </p>

<pre><code>{
    ""serviceResponse"": {
        ""supportOffice"": ""EUKO"",
        ""totalPages"": 5,
        ""pageNo"": 1,
        ""recordsPerPage"": 1000,
        ""projects"": [
            { ""projectID"":1 ...} , { ""projectID"":2 ...} ,...
        ]
    }
}
</code></pre>

<p>the URL is in the format
<a href=""http://server.com/api/Projects?pageNo=1"" rel=""nofollow noreferrer"">http://server.com/api/Projects?pageNo=1</a></p>

<p>I have managed to set up a RestService to call the API and return the JSON and a SQL Sink that will take the JSON and pass it to a stored procedure that then stores the data. </p>

<p>However, what I am struggling with is how to handle the pagination. </p>

<p>I have tried: </p>

<ol>
<li><p>Pagination options on the RestService:  I don't think this will work as it only allows for an XPATH that returns the full next URL.  I can't see that it will allow the URL to be computed from the totalPages and pageNo.   (or I couldn't get it to work) </p></li>
<li><p>I tried to add a Web call to the API before the processing to then calculate the number of pages.  While not ideal it did work, until I hit the 1mb/1min limit as some responses are quite big.  This is not going to work. </p></li>
<li><p>I've tried to see if the API could change, but that is not possible. </p></li>
</ol>

<p>I was wondering if anyone has any ideas on how I could get this working, or has succesfully consumed a similar API?</p>
","<azure><rest><azure-data-factory>","2019-07-09 09:14:36","4348","1","2","56962908","<p>The following explanation will walk through creating a pipeline that looks like the following. Notice it uses Stored Procedure activities, Web Activities, and For Each activities.
<a href=""https://i.stack.imgur.com/SlATh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SlATh.png"" alt=""pipeline screenshot"" /></a></p>
<p><a href=""https://i.stack.imgur.com/ievsx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ievsx.png"" alt=""for each activities screenshot"" /></a></p>
<p>First provision an Azure SQL DB, setup the AAD administrator, then grant the ADF MSI permissions in the database as described <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database?#managed-identity"" rel=""nofollow noreferrer"">here</a>. Then create the following table and two stored procedures:</p>
<pre><code>CREATE TABLE [dbo].[People](
    [id] [int] NULL,
    [email] [varchar](255) NULL,
    [first_name] [varchar](100) NULL,
    [last_name] [varchar](100) NULL,
    [avatar] [nvarchar](1000) NULL
)

GO
/*
sample call:
exec uspInsertPeople @json = '{&quot;page&quot;:1,&quot;per_page&quot;:3,&quot;total&quot;:12,&quot;total_pages&quot;:4,&quot;data&quot;:[{&quot;id&quot;:1,&quot;email&quot;:&quot;george.bluth@reqres.in&quot;,&quot;first_name&quot;:&quot;George&quot;,&quot;last_name&quot;:&quot;Bluth&quot;,&quot;avatar&quot;:&quot;https://s3.amazonaws.com/uifaces/faces/twitter/calebogden/128.jpg&quot;},{&quot;id&quot;:2,&quot;email&quot;:&quot;janet.weaver@reqres.in&quot;,&quot;first_name&quot;:&quot;Janet&quot;,&quot;last_name&quot;:&quot;Weaver&quot;,&quot;avatar&quot;:&quot;https://s3.amazonaws.com/uifaces/faces/twitter/josephstein/128.jpg&quot;},{&quot;id&quot;:3,&quot;email&quot;:&quot;emma.wong@reqres.in&quot;,&quot;first_name&quot;:&quot;Emma&quot;,&quot;last_name&quot;:&quot;Wong&quot;,&quot;avatar&quot;:&quot;https://s3.amazonaws.com/uifaces/faces/twitter/olegpogodaev/128.jpg&quot;}]}'
*/
create proc uspInsertPeople @json nvarchar(max)
as
begin
insert into People (id, email, first_name, last_name, avatar)
select d.*
from OPENJSON(@json)
WITH (
        [data] nvarchar(max) '$.data' as JSON
)
CROSS APPLY OPENJSON([data], '$')
    WITH (
        id int '$.id',
        email varchar(255) '$.email',
        first_name varchar(100) '$.first_name',
        last_name varchar(100) '$.last_name',
        avatar nvarchar(1000) '$.avatar'
    ) d;
end

GO

create proc uspTruncatePeople
as
truncate table People


</code></pre>
<p>Next, in Azure Data Factory v2 create a new pipeline, rename it to ForEachPage then go to the Code view and paste in the following JSON:</p>
<pre><code>{
    &quot;name&quot;: &quot;ForEachPage&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;GetTotalPages&quot;,
                &quot;type&quot;: &quot;WebActivity&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Truncate SQL Table&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;url&quot;: {
                        &quot;value&quot;: &quot;https://reqres.in/api/users?page=1&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;method&quot;: &quot;GET&quot;
                }
            },
            {
                &quot;name&quot;: &quot;ForEachPage&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;GetTotalPages&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@range(1,activity('GetTotalPages').output.total_pages)&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;GetPage&quot;,
                            &quot;type&quot;: &quot;WebActivity&quot;,
                            &quot;dependsOn&quot;: [],
                            &quot;policy&quot;: {
                                &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                                &quot;retry&quot;: 0,
                                &quot;retryIntervalInSeconds&quot;: 30,
                                &quot;secureOutput&quot;: false,
                                &quot;secureInput&quot;: false
                            },
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;url&quot;: {
                                    &quot;value&quot;: &quot;@concat('https://reqres.in/api/users?page=',item())&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                },
                                &quot;method&quot;: &quot;GET&quot;
                            }
                        },
                        {
                            &quot;name&quot;: &quot;uspInsertPeople stored procedure&quot;,
                            &quot;type&quot;: &quot;SqlServerStoredProcedure&quot;,
                            &quot;dependsOn&quot;: [
                                {
                                    &quot;activity&quot;: &quot;GetPage&quot;,
                                    &quot;dependencyConditions&quot;: [
                                        &quot;Succeeded&quot;
                                    ]
                                }
                            ],
                            &quot;policy&quot;: {
                                &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                                &quot;retry&quot;: 0,
                                &quot;retryIntervalInSeconds&quot;: 30,
                                &quot;secureOutput&quot;: false,
                                &quot;secureInput&quot;: false
                            },
                            &quot;userProperties&quot;: [],
                            &quot;typeProperties&quot;: {
                                &quot;storedProcedureName&quot;: &quot;[dbo].[uspInsertPeople]&quot;,
                                &quot;storedProcedureParameters&quot;: {
                                    &quot;json&quot;: {
                                        &quot;value&quot;: {
                                            &quot;value&quot;: &quot;@string(activity('GetPage').output)&quot;,
                                            &quot;type&quot;: &quot;Expression&quot;
                                        },
                                        &quot;type&quot;: &quot;String&quot;
                                    }
                                }
                            },
                            &quot;linkedServiceName&quot;: {
                                &quot;referenceName&quot;: &quot;lsAzureDB&quot;,
                                &quot;type&quot;: &quot;LinkedServiceReference&quot;
                            }
                        }
                    ]
                }
            },
            {
                &quot;name&quot;: &quot;Truncate SQL Table&quot;,
                &quot;type&quot;: &quot;SqlServerStoredProcedure&quot;,
                &quot;dependsOn&quot;: [],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;storedProcedureName&quot;: &quot;[dbo].[uspTruncatePeople]&quot;
                },
                &quot;linkedServiceName&quot;: {
                    &quot;referenceName&quot;: &quot;lsAzureDB&quot;,
                    &quot;type&quot;: &quot;LinkedServiceReference&quot;
                }
            }
        ],
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p>Create a lsAzureDB linked service to Azure SQL DB setting it to use the MSI for authentication.</p>
<p>This pipeline calls a <a href=""https://reqres.in/api/users?page=1"" rel=""nofollow noreferrer"">sample paged API</a> (which works at the moment but it not an API I manage so may stop working at some point) to demonstrate how to loop and how to take the results of the Web Activities and insert them to a SQL table via a stored procedure call and JSON parsing in the stored procedure. The loop will run with parallelism but certainly you could change settings on the ForEachPage activity to make it run in serial.</p>
"
"56949173","How can I consume this Rest API in Azure Data Factory","<p>I have a REST API I need to call from Azure Data Factory and insert the data into a SQL table. </p>

<p>The format of the JSON returned from the API is in the following format: </p>

<pre><code>{
    ""serviceResponse"": {
        ""supportOffice"": ""EUKO"",
        ""totalPages"": 5,
        ""pageNo"": 1,
        ""recordsPerPage"": 1000,
        ""projects"": [
            { ""projectID"":1 ...} , { ""projectID"":2 ...} ,...
        ]
    }
}
</code></pre>

<p>the URL is in the format
<a href=""http://server.com/api/Projects?pageNo=1"" rel=""nofollow noreferrer"">http://server.com/api/Projects?pageNo=1</a></p>

<p>I have managed to set up a RestService to call the API and return the JSON and a SQL Sink that will take the JSON and pass it to a stored procedure that then stores the data. </p>

<p>However, what I am struggling with is how to handle the pagination. </p>

<p>I have tried: </p>

<ol>
<li><p>Pagination options on the RestService:  I don't think this will work as it only allows for an XPATH that returns the full next URL.  I can't see that it will allow the URL to be computed from the totalPages and pageNo.   (or I couldn't get it to work) </p></li>
<li><p>I tried to add a Web call to the API before the processing to then calculate the number of pages.  While not ideal it did work, until I hit the 1mb/1min limit as some responses are quite big.  This is not going to work. </p></li>
<li><p>I've tried to see if the API could change, but that is not possible. </p></li>
</ol>

<p>I was wondering if anyone has any ideas on how I could get this working, or has succesfully consumed a similar API?</p>
","<azure><rest><azure-data-factory>","2019-07-09 09:14:36","4348","1","2","60064832","<p>This approach doesn't work for several reasons however the main issue is that Pipeline ""Copy Data"" Activity is unable to index into the deeply nested arrays.</p>

<p>I can wild card the first level of array but anything deeper requires and actual integer index value. As long as there is only one item in the array it's great after that however we would be missing data.</p>

<pre><code> {
   ""source"": {
      ""path"": ""$['myObject']['element'][*]['externalUID'][0]['provider']""
    },
    sink"": {
       name"": ""EXTERNALUID_PROVIDER""
    }
},
</code></pre>
"
"56948054","Why does Azure Data Factory seemingly insist on inserting DateTimes as string?","<p>I'm trying to set up an Azure Data Factory to copy and denormalize my data from a AzureSQL database to another AzureSQL database for reporting/BI purposes with a data flow, but I ran into a problem with inserting dates.</p>

<p>This is the definition of my dataflow.</p>

<pre><code>{
    ""name"": ""dataflow1"",
    ""properties"": {
        ""type"": ""MappingDataFlow"",
        ""typeProperties"": {
            ""sources"": [
                {
                    ""dataset"": {
                        ""referenceName"": ""AzureSqlTable1"",
                        ""type"": ""DatasetReference""
                    },
                    ""name"": ""source1""
                }
            ],
            ""sinks"": [
                {
                    ""dataset"": {
                        ""referenceName"": ""AzureSqlTable2"",
                        ""type"": ""DatasetReference""
                    },
                    ""name"": ""sink1""
                }
            ],
            ""script"": ""\n\nsource(output(\n\t\tBucketId as string,\n\t\tStreamId as string,\n\t\tStreamIdOriginal as string,\n\t\tStreamRevision as integer,\n\t\tItems as integer,\n\t\tCommitId as string,\n\t\tCommitSequence as integer,\n\t\tCommitStamp as timestamp,\n\t\tCheckpointNumber as long,\n\t\tDispatched as boolean,\n\t\tHeaders as binary,\n\t\tPayload as binary\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tisolationLevel: 'READ_UNCOMMITTED',\n\tformat: 'table') ~&gt; source1\nsource1 sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'table',\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:false,\n\tupsertable:false,\n\tmapColumn(\n\t\tBucketId,\n\t\tCommitStamp\n\t)) ~&gt; sink1""
        }
    }
}
</code></pre>

<p>and these are the definitions of my source</p>

<pre><code>{
    ""name"": ""AzureSqlTable1"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""Source_Test"",
            ""type"": ""LinkedServiceReference""
        },
        ""annotations"": [],
        ""type"": ""AzureSqlTable"",
        ""schema"": [
            {
                ""name"": ""BucketId"",
                ""type"": ""varchar""
            },
            {
                ""name"": ""StreamId"",
                ""type"": ""char""
            },
            {
                ""name"": ""StreamIdOriginal"",
                ""type"": ""nvarchar""
            },
            {
                ""name"": ""StreamRevision"",
                ""type"": ""int"",
                ""precision"": 10
            },
            {
                ""name"": ""Items"",
                ""type"": ""tinyint"",
                ""precision"": 3
            },
            {
                ""name"": ""CommitId"",
                ""type"": ""uniqueidentifier""
            },
            {
                ""name"": ""CommitSequence"",
                ""type"": ""int"",
                ""precision"": 10
            },
            {
                ""name"": ""CommitStamp"",
                ""type"": ""datetime2"",
                ""scale"": 7
            },
            {
                ""name"": ""CheckpointNumber"",
                ""type"": ""bigint"",
                ""precision"": 19
            },
            {
                ""name"": ""Dispatched"",
                ""type"": ""bit""
            },
            {
                ""name"": ""Headers"",
                ""type"": ""varbinary""
            },
            {
                ""name"": ""Payload"",
                ""type"": ""varbinary""
            }
        ],
        ""typeProperties"": {
            ""tableName"": ""[dbo].[Commits]""
        }
    }
}
</code></pre>

<p>and sink data sets</p>

<pre><code>{
    ""name"": ""AzureSqlTable2"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""Dest_Test"",
            ""type"": ""LinkedServiceReference""
        },
        ""annotations"": [],
        ""type"": ""AzureSqlTable"",
        ""schema"": [],
        ""typeProperties"": {
            ""tableName"": ""dbo.Test2""
        }
    }
}
</code></pre>

<p>When running my pipeline with the data flow I get the following error:</p>

<pre><code>Activity dataflow1 failed: DF-EXEC-1 Conversion failed when converting date and/or time from character string.
com.microsoft.sqlserver.jdbc.SQLServerException: Conversion failed when converting date and/or time from character string.
    at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:258)
    at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:256)
    at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:108)
    at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:28)
    at com.microsoft.sqlserver.jdbc.SQLServerBulkCopy.doInsertBulk(SQLServerBulkCopy.java:1611)
    at com.microsoft.sqlserver.jdbc.SQLServerBulkCopy.access$200(SQLServerBulkCopy.java:58)
    at com.microsoft.sqlserver.jdbc.SQLServerBulkCopy$1InsertBulk.doExecute(SQLServerBulkCopy.java:709)
    at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7151)
    at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:2478)
    at com.microsoft.sqlserver.jdbc.SQLServerBulkCopy.sendBulkLoadBCP(SQLServerBulkCopy.java:739)
    at com.microsoft.sqlserver.jdbc.SQLServerBulkCopy.writeToServer(SQLServerBulkCopy.java:1684)
    at com.microsoft.sqlserver.jdbc.SQLServerBulkCopy.writeToServer(SQLServerBulkCopy.java:669)
    at com.microsoft.azure.sqldb.spark.connect.DataFrameFunctions.com$microsoft$azure$sqldb$spark$connect$DataFrameFunctions$$bulkCopy(DataFrameFunctions.scala:127)
    at com.microsoft.azure.sqldb.spark.connect.DataFrameFunctions$$anonfun$bulkCopyToSqlDB$1.apply(DataFrameFunctions.scala:72)
    at com.microsoft.azure.sqldb.spark.connect.DataFrameFunctions$$anonfun$bulkCopyToSqlDB$1.apply(DataFrameFunctions.scala:72)
    at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:948)
    at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:948)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2226)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2226)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:124)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:459)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1401)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
</code></pre>

<p>My Azure SQL audit log shows the following statement that failed (which is not a huge surprise considering that it uses <code>VARCHAR(50)</code> as type for <code>[CommitStamp]</code>:</p>

<pre><code>INSERT BULK dbo.T_301fcb5e4a4148d4a48f2943011b2f04 (
  [BucketId] NVARCHAR(MAX), 
  [CommitStamp] VARCHAR(50), 
  [StreamId] NVARCHAR(MAX), 
  [StreamIdOriginal] NVARCHAR(MAX),
  [StreamRevision] INT,
  [Items] INT,
  [CommitId] NVARCHAR(MAX),
  [CommitSequence] INT, 
  [CheckpointNumber] BIGINT, 
  [Dispatched] BIT,
  [Headers] VARBINARY(MAX),
  [Payload] VARBINARY(MAX),
  [r8e440f7252bb401b9ead107597de6293] INT) 
with (ROWS_PER_BATCH = 4096, TABLOCK)
</code></pre>

<p>I have absolutely no idea why this occurs. It looks like the schema information is correct but somehow it seems the data factory/data flow wants to insert the <code>CommitStamp</code> as a string type.</p>

<p>As requested, the output from the data flow/code/plan view:</p>

<pre><code>

source(output(
        BucketId as string,
        StreamId as string,
        StreamIdOriginal as string,
        StreamRevision as integer,
        Items as integer,
        CommitId as string,
        CommitSequence as integer,
        CommitStamp as timestamp,
        CheckpointNumber as long,
        Dispatched as boolean,
        Headers as binary,
        Payload as binary
    ),
    allowSchemaDrift: true,
    validateSchema: false,
    isolationLevel: 'READ_UNCOMMITTED',
    format: 'table',
    schemaName: '[dbo]',
    tableName: '[Commits]',
    store: 'sqlserver',
    server: 'sign2025-sqldata.database.windows.net',
    database: 'SignPath.Application',
    user: 'Sign2025Admin',
    password: '**********') ~&gt; source1
source1 sink(allowSchemaDrift: true,
    validateSchema: false,
    format: 'table',
    deletable:false,
    insertable:true,
    updateable:false,
    upsertable:false,
    mapColumn(
        BucketId,
        CommitStamp
    ),
    schemaName: 'dbo',
    tableName: 'Test2',
    store: 'sqlserver',
    server: 'sign2025-sqldata.database.windows.net',
    database: 'SignPath.Reporting',
    user: 'Sign2025Admin',
    password: '**********') ~&gt; sink1
</code></pre>
","<azure-sql-database><azure-data-factory>","2019-07-09 08:08:11","1903","4","2","56949894","<p>I created a data flow to copy data from an Azure SQL database to another Azure SQL database. It succeeded to covert <code>datatime2</code> to <code>VARCHAR(50)</code>.</p>

<p>This is the definition of my dataflow:</p>

<pre><code>{
    ""name"": ""dataflow1"",
    ""properties"": {
        ""type"": ""MappingDataFlow"",
        ""typeProperties"": {
            ""sources"": [
                {
                    ""dataset"": {
                        ""referenceName"": ""DestinationDataset_sto"",
                        ""type"": ""DatasetReference""
                    },
                    ""name"": ""source1""
                }
            ],
            ""sinks"": [
                {
                    ""dataset"": {
                        ""referenceName"": ""DestinationDataset_mex"",
                        ""type"": ""DatasetReference""
                    },
                    ""name"": ""sink1""
                }
            ],
            ""script"": ""\n\nsource(output(\n\t\tID as integer,\n\t\ttName as string,\n\t\tmyTime as timestamp\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tisolationLevel: 'READ_UNCOMMITTED',\n\tformat: 'table') ~&gt; source1\nsource1 sink(input(\n\t\tID as integer,\n\t\ttName as string,\n\t\tmyTime as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'table',\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:false,\n\tupsertable:false) ~&gt; sink1""
        }
    }
}
</code></pre>

<p>The definitions of my source:</p>

<pre><code>{
    ""name"": ""DestinationDataset_sto"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureSqlDatabase1"",
            ""type"": ""LinkedServiceReference""
        },
        ""annotations"": [],
        ""type"": ""AzureSqlTable"",
        ""schema"": [
            {
                ""name"": ""ID"",
                ""type"": ""int"",
                ""precision"": 10
            },
            {
                ""name"": ""tName"",
                ""type"": ""varchar""
            },
            {
                ""name"": ""myTime"",
                ""type"": ""datetime2"",
                ""scale"": 7
            }
        ],
        ""typeProperties"": {
            ""tableName"": ""[dbo].[demo]""
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p>My sink settings:</p>

<pre><code>{
    ""name"": ""DestinationDataset_mex"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureSqlDatabase1"",
            ""type"": ""LinkedServiceReference""
        },
        ""annotations"": [],
        ""type"": ""AzureSqlTable"",
        ""schema"": [
            {
                ""name"": ""ID"",
                ""type"": ""int"",
                ""precision"": 10
            },
            {
                ""name"": ""tName"",
                ""type"": ""varchar""
            },
            {
                ""name"": ""myTime"",
                ""type"": ""varchar""
            }
        ],
        ""typeProperties"": {
            ""tableName"": ""[dbo].[demo1]""
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p>Here are my data flow steps.
<a href=""https://i.stack.imgur.com/ltZQz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ltZQz.png"" alt=""enter image description here""></a></p>

<p><strong>Step 1: Source settings:</strong>
<a href=""https://i.stack.imgur.com/PI7gl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PI7gl.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/Dza02.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dza02.png"" alt=""enter image description here""></a></p>

<p><strong>Step 2: Sink  settings:</strong>
<a href=""https://i.stack.imgur.com/DdGvk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DdGvk.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/ZSw5d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZSw5d.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/VPojB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VPojB.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/21sbw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/21sbw.png"" alt=""enter image description here""></a></p>

<p>Running succeeded:
<a href=""https://i.stack.imgur.com/gvXUu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gvXUu.png"" alt=""enter image description here""></a></p>

<p>The table demo and demo1 almost have the same schema except the <code>myTime</code>.</p>

<p>My source table and it's data:</p>

<p><a href=""https://i.stack.imgur.com/HNT4V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HNT4V.png"" alt=""enter image description here""></a></p>

<p>My sink table and the data copied from <code>demo</code>:</p>

<p><a href=""https://i.stack.imgur.com/KdIkq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KdIkq.png"" alt=""enter image description here""></a></p>

<p>Data Flow plan:</p>

<pre><code>source(output(
        ID as integer,
        tName as string,
        myTime as timestamp
    ),
    allowSchemaDrift: true,
    validateSchema: true,
    isolationLevel: 'SERIALIZABLE',
    format: 'table',
    schemaName: '[dbo]',
    tableName: '[demo]',
    store: 'sqlserver',
    server: '****.database.windows.net',
    database: '****',
    user: 'ServerAdmin',
    password: '**********') ~&gt; source1
source1 sink(input(
        ID as integer,
        tName as string,
        myTime as string
    ),
    allowSchemaDrift: true,
    validateSchema: false,
    format: 'table',
    deletable:false,
    insertable:true,
    updateable:false,
    upsertable:false,
    schemaName: '[dbo]',
    tableName: '[demo1]',
    store: 'sqlserver',
    server: '****.database.windows.net',
    database: '****',
    user: 'ServerAdmin',
    password: '**********') ~&gt; sink1
</code></pre>

<p><strong>Update1:</strong></p>

<p>I create the sink table manually and found that:</p>

<blockquote>
  <p>Data Flow  can convert <code>datatime2</code>  to <code>VARCHAR()</code>(maybe <code>NVARCHAR()</code>) , <code>date</code> ,<code>datetimeoffset</code>.</p>
</blockquote>

<p>When I try the date type <code>time</code>, <code>datetime</code>, <code>datetime2</code>, <code>smalldatetime</code>, Data Flow always gives the error:</p>

<pre><code>""message"": ""DF-EXEC-1 Conversion failed when converting date and/or time from character 
</code></pre>

<p><strong>Update 2019-7-11:</strong></p>

<p>I asked Azure Support for help and they replied me: this is a bug of Data Flow and there is no solution for now.
<a href=""https://i.stack.imgur.com/olSJs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/olSJs.png"" alt=""enter image description here""></a></p>

<p><strong>Update 2019-7-12:</strong></p>

<p>I tested with Azure Support and they conform this is a bug. Here is the new email:
<a href=""https://i.stack.imgur.com/lYqQc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lYqQc.png"" alt=""enter image description here""></a></p>

<p>They also told me that the <strong>fix is already made and it will be deployed in next deployment train. This could be end of next week</strong>.</p>

<p>Hope this helps.</p>
"
"56948054","Why does Azure Data Factory seemingly insist on inserting DateTimes as string?","<p>I'm trying to set up an Azure Data Factory to copy and denormalize my data from a AzureSQL database to another AzureSQL database for reporting/BI purposes with a data flow, but I ran into a problem with inserting dates.</p>

<p>This is the definition of my dataflow.</p>

<pre><code>{
    ""name"": ""dataflow1"",
    ""properties"": {
        ""type"": ""MappingDataFlow"",
        ""typeProperties"": {
            ""sources"": [
                {
                    ""dataset"": {
                        ""referenceName"": ""AzureSqlTable1"",
                        ""type"": ""DatasetReference""
                    },
                    ""name"": ""source1""
                }
            ],
            ""sinks"": [
                {
                    ""dataset"": {
                        ""referenceName"": ""AzureSqlTable2"",
                        ""type"": ""DatasetReference""
                    },
                    ""name"": ""sink1""
                }
            ],
            ""script"": ""\n\nsource(output(\n\t\tBucketId as string,\n\t\tStreamId as string,\n\t\tStreamIdOriginal as string,\n\t\tStreamRevision as integer,\n\t\tItems as integer,\n\t\tCommitId as string,\n\t\tCommitSequence as integer,\n\t\tCommitStamp as timestamp,\n\t\tCheckpointNumber as long,\n\t\tDispatched as boolean,\n\t\tHeaders as binary,\n\t\tPayload as binary\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tisolationLevel: 'READ_UNCOMMITTED',\n\tformat: 'table') ~&gt; source1\nsource1 sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'table',\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:false,\n\tupsertable:false,\n\tmapColumn(\n\t\tBucketId,\n\t\tCommitStamp\n\t)) ~&gt; sink1""
        }
    }
}
</code></pre>

<p>and these are the definitions of my source</p>

<pre><code>{
    ""name"": ""AzureSqlTable1"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""Source_Test"",
            ""type"": ""LinkedServiceReference""
        },
        ""annotations"": [],
        ""type"": ""AzureSqlTable"",
        ""schema"": [
            {
                ""name"": ""BucketId"",
                ""type"": ""varchar""
            },
            {
                ""name"": ""StreamId"",
                ""type"": ""char""
            },
            {
                ""name"": ""StreamIdOriginal"",
                ""type"": ""nvarchar""
            },
            {
                ""name"": ""StreamRevision"",
                ""type"": ""int"",
                ""precision"": 10
            },
            {
                ""name"": ""Items"",
                ""type"": ""tinyint"",
                ""precision"": 3
            },
            {
                ""name"": ""CommitId"",
                ""type"": ""uniqueidentifier""
            },
            {
                ""name"": ""CommitSequence"",
                ""type"": ""int"",
                ""precision"": 10
            },
            {
                ""name"": ""CommitStamp"",
                ""type"": ""datetime2"",
                ""scale"": 7
            },
            {
                ""name"": ""CheckpointNumber"",
                ""type"": ""bigint"",
                ""precision"": 19
            },
            {
                ""name"": ""Dispatched"",
                ""type"": ""bit""
            },
            {
                ""name"": ""Headers"",
                ""type"": ""varbinary""
            },
            {
                ""name"": ""Payload"",
                ""type"": ""varbinary""
            }
        ],
        ""typeProperties"": {
            ""tableName"": ""[dbo].[Commits]""
        }
    }
}
</code></pre>

<p>and sink data sets</p>

<pre><code>{
    ""name"": ""AzureSqlTable2"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""Dest_Test"",
            ""type"": ""LinkedServiceReference""
        },
        ""annotations"": [],
        ""type"": ""AzureSqlTable"",
        ""schema"": [],
        ""typeProperties"": {
            ""tableName"": ""dbo.Test2""
        }
    }
}
</code></pre>

<p>When running my pipeline with the data flow I get the following error:</p>

<pre><code>Activity dataflow1 failed: DF-EXEC-1 Conversion failed when converting date and/or time from character string.
com.microsoft.sqlserver.jdbc.SQLServerException: Conversion failed when converting date and/or time from character string.
    at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:258)
    at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:256)
    at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:108)
    at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:28)
    at com.microsoft.sqlserver.jdbc.SQLServerBulkCopy.doInsertBulk(SQLServerBulkCopy.java:1611)
    at com.microsoft.sqlserver.jdbc.SQLServerBulkCopy.access$200(SQLServerBulkCopy.java:58)
    at com.microsoft.sqlserver.jdbc.SQLServerBulkCopy$1InsertBulk.doExecute(SQLServerBulkCopy.java:709)
    at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7151)
    at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:2478)
    at com.microsoft.sqlserver.jdbc.SQLServerBulkCopy.sendBulkLoadBCP(SQLServerBulkCopy.java:739)
    at com.microsoft.sqlserver.jdbc.SQLServerBulkCopy.writeToServer(SQLServerBulkCopy.java:1684)
    at com.microsoft.sqlserver.jdbc.SQLServerBulkCopy.writeToServer(SQLServerBulkCopy.java:669)
    at com.microsoft.azure.sqldb.spark.connect.DataFrameFunctions.com$microsoft$azure$sqldb$spark$connect$DataFrameFunctions$$bulkCopy(DataFrameFunctions.scala:127)
    at com.microsoft.azure.sqldb.spark.connect.DataFrameFunctions$$anonfun$bulkCopyToSqlDB$1.apply(DataFrameFunctions.scala:72)
    at com.microsoft.azure.sqldb.spark.connect.DataFrameFunctions$$anonfun$bulkCopyToSqlDB$1.apply(DataFrameFunctions.scala:72)
    at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:948)
    at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:948)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2226)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2226)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:124)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:459)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1401)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
</code></pre>

<p>My Azure SQL audit log shows the following statement that failed (which is not a huge surprise considering that it uses <code>VARCHAR(50)</code> as type for <code>[CommitStamp]</code>:</p>

<pre><code>INSERT BULK dbo.T_301fcb5e4a4148d4a48f2943011b2f04 (
  [BucketId] NVARCHAR(MAX), 
  [CommitStamp] VARCHAR(50), 
  [StreamId] NVARCHAR(MAX), 
  [StreamIdOriginal] NVARCHAR(MAX),
  [StreamRevision] INT,
  [Items] INT,
  [CommitId] NVARCHAR(MAX),
  [CommitSequence] INT, 
  [CheckpointNumber] BIGINT, 
  [Dispatched] BIT,
  [Headers] VARBINARY(MAX),
  [Payload] VARBINARY(MAX),
  [r8e440f7252bb401b9ead107597de6293] INT) 
with (ROWS_PER_BATCH = 4096, TABLOCK)
</code></pre>

<p>I have absolutely no idea why this occurs. It looks like the schema information is correct but somehow it seems the data factory/data flow wants to insert the <code>CommitStamp</code> as a string type.</p>

<p>As requested, the output from the data flow/code/plan view:</p>

<pre><code>

source(output(
        BucketId as string,
        StreamId as string,
        StreamIdOriginal as string,
        StreamRevision as integer,
        Items as integer,
        CommitId as string,
        CommitSequence as integer,
        CommitStamp as timestamp,
        CheckpointNumber as long,
        Dispatched as boolean,
        Headers as binary,
        Payload as binary
    ),
    allowSchemaDrift: true,
    validateSchema: false,
    isolationLevel: 'READ_UNCOMMITTED',
    format: 'table',
    schemaName: '[dbo]',
    tableName: '[Commits]',
    store: 'sqlserver',
    server: 'sign2025-sqldata.database.windows.net',
    database: 'SignPath.Application',
    user: 'Sign2025Admin',
    password: '**********') ~&gt; source1
source1 sink(allowSchemaDrift: true,
    validateSchema: false,
    format: 'table',
    deletable:false,
    insertable:true,
    updateable:false,
    upsertable:false,
    mapColumn(
        BucketId,
        CommitStamp
    ),
    schemaName: 'dbo',
    tableName: 'Test2',
    store: 'sqlserver',
    server: 'sign2025-sqldata.database.windows.net',
    database: 'SignPath.Reporting',
    user: 'Sign2025Admin',
    password: '**********') ~&gt; sink1
</code></pre>
","<azure-sql-database><azure-data-factory>","2019-07-09 08:08:11","1903","4","2","56979023","<p>Looks like your Sink dataset defines myTime as a String:</p>

<p>sink(input(
        ID as integer,
        tName as string,
        myTime as string
    )</p>

<p>Can you change that to timestamp or Date, whichever you'd like to land it as?</p>

<p>Alternatively, you can land the data in a temporary staging table in SQL by setting ""Recreate table"" on the Sink and let ADF generate a new table definition on the fly using the data types of your mapped fields in the data flow.</p>
"
"56938934","Unable to implement a retry for a webhook activity in Azure Data Factory V2","<p>I have an Azure Data Factory V2 Pipeline with a Webhook Activity, and it has been working correctly, however I've been experiencing some 'Bad Request' errors, due the network and/or availability issues with the Azure Automation Service.</p>

<p>The solution here is to manually re-run this pipeline and works, is there a way of adding a retry like the CopyActivity to a Webhook Activity in ADF V2? and if that is not possible, are there any workarounds that you might know?</p>

<p><a href=""https://i.stack.imgur.com/lCpjQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lCpjQ.png"" alt=""enter image description here""></a></p>

<p>Not sure if this is possible now but I created a feature suggestion:</p>

<p><a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/38096407-add-retry-policy-to-webhook-activity"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory/suggestions/38096407-add-retry-policy-to-webhook-activity</a></p>
","<azure-data-factory><azure-automation>","2019-07-08 16:18:46","1027","1","2","57005252","<p>Alternative is to use WebActivity, which has retry option.. you can have ForEach Activity with Wait activity combination with 30 sec or 1 min wait interval in case you want to re-try based on few scenarios.</p>
"
"56938934","Unable to implement a retry for a webhook activity in Azure Data Factory V2","<p>I have an Azure Data Factory V2 Pipeline with a Webhook Activity, and it has been working correctly, however I've been experiencing some 'Bad Request' errors, due the network and/or availability issues with the Azure Automation Service.</p>

<p>The solution here is to manually re-run this pipeline and works, is there a way of adding a retry like the CopyActivity to a Webhook Activity in ADF V2? and if that is not possible, are there any workarounds that you might know?</p>

<p><a href=""https://i.stack.imgur.com/lCpjQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lCpjQ.png"" alt=""enter image description here""></a></p>

<p>Not sure if this is possible now but I created a feature suggestion:</p>

<p><a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/38096407-add-retry-policy-to-webhook-activity"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory/suggestions/38096407-add-retry-policy-to-webhook-activity</a></p>
","<azure-data-factory><azure-automation>","2019-07-08 16:18:46","1027","1","2","58642555","<p>Another way is to dodge Webhook activity and use Web Activity. Just make sure that you add response to your HTTP Listener like Logic App. Here is a blog which elucidates on the same:
<a href=""https://www.data4v.com/making-adf-web-activity-synchronous-with-logic-app/"" rel=""nofollow noreferrer"">https://www.data4v.com/making-adf-web-activity-synchronous-with-logic-app/</a></p>
"
"56938829","How do I backup data to azure blob from azure cosmos db during a specific stage?","<p>I have Azure cosmos DB account what I want to do is backup data which is one month old from azure cosmos DB to Azure blob storage using my node app. I have already created pipeline and have triggered it by using create run pipeline API for Nodejs (using Azure data factory). But I am not able to figure out how to make the pipeline selective for data which is one month old from the current date. Any suggestions for that?</p>

<p><strong>EDIT</strong>: Actually I want to run the API daily so that it backs up data which is one month old. For example, let's say I get 100 entries today in my cosmos DB, so the pipeline should select data from current date - 30 days and should back it up so that at any point my Azure cosmos DB has data for recent 30 days only and rest are backed up to Azure blob.</p>
","<node.js><azure><azure-cosmosdb><azure-blob-storage><azure-data-factory>","2019-07-08 16:12:00","2821","0","2","56942174","<p>Not sure what pipeline you're referring to. That said: Cosmos DB doesn't have any built-in backup tools. You'd need to select and copy this data programmatically.</p>

<p>If using the MongoDB API, you could pass a query parameter to the <code>mongoexport</code> command-line tool (to serve as your date filter), but you'd still need to run <code>mongoexport</code> from your VM, write to a local directory, then copy to blob storage (I don't know if you can install/run MongoDB tools in something like Azure Functions or a DevOps pipeline).</p>
"
"56938829","How do I backup data to azure blob from azure cosmos db during a specific stage?","<p>I have Azure cosmos DB account what I want to do is backup data which is one month old from azure cosmos DB to Azure blob storage using my node app. I have already created pipeline and have triggered it by using create run pipeline API for Nodejs (using Azure data factory). But I am not able to figure out how to make the pipeline selective for data which is one month old from the current date. Any suggestions for that?</p>

<p><strong>EDIT</strong>: Actually I want to run the API daily so that it backs up data which is one month old. For example, let's say I get 100 entries today in my cosmos DB, so the pipeline should select data from current date - 30 days and should back it up so that at any point my Azure cosmos DB has data for recent 30 days only and rest are backed up to Azure blob.</p>
","<node.js><azure><azure-cosmosdb><azure-blob-storage><azure-data-factory>","2019-07-08 16:12:00","2821","0","2","56947472","<p>Just a supplement to @David's answer here.If you mean Cosmos DB SQL API, it has automatic backup mechanism based on this link:<a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/online-backup-and-restore#automatic-and-online-backups"" rel=""nofollow noreferrer"">Automatic and online backups</a>.</p>

<blockquote>
  <p>With Azure Cosmos DB, not only your data, but also the backups of your
  data are highly redundant and resilient to regional disasters. The
  automated backups are currently taken every four hours and at any
  point of time, the latest two backups are stored. If you have
  accidentally deleted or corrupted your data, you should contact Azure
  support within eight hours so that the Azure Cosmos DB team can help
  you restore the data from the backups.</p>
</blockquote>

<p>However,you cannot access this backup directly. Azure Cosmos DB will use this backup only if a backup restore is initiated.</p>

<p>But the document provides two <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/online-backup-and-restore#options-to-manage-your-own-backups"" rel=""nofollow noreferrer"">options</a> to manage your own backups.</p>

<ul>
<li>1.Use Azure Data Factory to move data periodically to a storage of your choice.</li>
<li>2.Use Azure Cosmos DB change feed to read data periodically for full backups, as well as for incremental changes, and store it in your own
storage.</li>
</ul>

<p>You could use trigger the copy activity in ADF to transfer data in the schedule.If you want to filter data by date,you could learn about <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/working-with-dates"" rel=""nofollow noreferrer"">_ts</a> in cosmos db which represents the latest modified time of data.</p>
"
"56932728","How to provide connection string dynamically for azure table storage/blob storage in Azure data factory Linked service","<p>Dynamically changing the connection string for Tablestorage  or blob storage in Azure data factory. Currently, I could see such option for database related dataset? How to achieve the same in Table or Blob storage</p>
","<azure><azure-data-factory>","2019-07-08 10:11:18","1888","2","2","56934195","<p>I believe this is what you wanted. 
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services</a>
As doc mentioned, UI only supports 8 linked service. For others, you could change json code directly following the same pattern. </p>

<pre><code>{
""name"": ""AzureBlobStorage12"",
""type"": ""Microsoft.DataFactory/factories/linkedservices"",
""properties"": {
    ""parameters"": {
        ""accountName"": {
            ""type"": ""String""
        },
        ""accountKey"": {
            ""type"": ""String""
        }
    },
    ""annotations"": [],
    ""type"": ""AzureBlobStorage"",
    ""typeProperties"": {
        ""connectionString"": ""DefaultEndpointsProtocol=https;AccountName=@{linkedService().accountName};AccountKey=@{linkedService().accountKey};EndpointSuffix=core.windows.net;""
    }
}
</code></pre>

<p>}</p>

<p><a href=""https://i.stack.imgur.com/uBtip.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uBtip.png"" alt=""enter image description here""></a>
You can't put the entire connection string as an expression. You need parameterize every part separately. Make sure you noticed the prameters field. 
And then every time you use the linked service, you will be able to pass different values to it.  </p>
"
"56932728","How to provide connection string dynamically for azure table storage/blob storage in Azure data factory Linked service","<p>Dynamically changing the connection string for Tablestorage  or blob storage in Azure data factory. Currently, I could see such option for database related dataset? How to achieve the same in Table or Blob storage</p>
","<azure><azure-data-factory>","2019-07-08 10:11:18","1888","2","2","57051427","<p>In the New Linked service Azure table storage and Click on Advanced and check Specify Dynamic contents in JSON format
 <a href=""https://i.stack.imgur.com/bohzI.png"" rel=""nofollow noreferrer"">adf</a></p>

<p><code>
Copy the below JSON to make it Table Storage Parameterize :
{
    ""name"": ""Table"",
    ""type"": ""Microsoft.DataFactory/factories/linkedservices"",
    ""properties"": {
        ""type"": ""AzureTableStorage"",
        ""typeProperties"": {
            ""sasUri"": {
                ""type"": ""SecureString"",
                ""value"": ""@{linkedService().sasUriParam}""
            }
        },
        ""parameters"": {
            ""sasUriParam"": {
                ""type"": ""String""
            }
        },
        ""annotations"": []
    }
}
</code></p>
"
"56931626","Where and when to transform data with Azure data factory Data Flow activity?","<p>I'm trying to transform data on the cloud with Azure Data Factory Data Flow Activity, previously I have created a pipeline that contains copy activity and some other(it takes lists of table names from SQL server and passed it as a parameter to another activity that does the actual copy), and I learned to created data flow activity to transform data and put it on another activity, when do I need to trigger that data flow activity.</p>
","<azure><azure-storage><azure-blob-storage><azure-data-factory><azure-synapse>","2019-07-08 09:07:31","145","1","1","56959559","<p>As @Kamil said, you can add a data flow like any other activity. Just look for it in the Acitivities menu as shown below, add dependencies from or to another activities, select the data flow you want to execute and you are good to go!</p>

<p><a href=""https://i.stack.imgur.com/g01no.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g01no.png"" alt=""Adding a data flow to a pipeline""></a></p>

<p>Hope this helped!</p>
"
"56931207","ADF Data Flows: invalid column name","<p>I want to create a simple azure data factory process to read a file (csv) from blob storage and write it to an azure sql database using data flows.</p>

<p>the source dataset has a column with the name ""myTime"" of type ""string"".</p>

<p>I added a ""derived column"" to add a new column with the name ""customTime"" with the expression ""currentTimestamp()""</p>

<p>finally, in the sql sink, I mapped ""customTime"" to my DateTime field on the database.</p>

<p><a href=""https://i.stack.imgur.com/cm50P.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cm50P.png"" alt=""enter image description here""></a></p>

<p>If I preview the data in data flows, everything looks alright, I can see both fields (myTime, customTime). When I debug the pipeline, I get the following exception:</p>

<pre class=""lang-java prettyprint-override""><code>Activity myActivity failed: DF-SYS-01 at Sink 'sqlsink': java.sql.BatchUpdateException: Invalid column name 'myTime'
</code></pre>

<p>Any idea why the sql sink is linked to ""myTime"" and not ""customTime""? I don't see any reference to ""myTime"" except that this is part of the input schema.</p>

<p>Thank you very much and best regards
Michael</p>
","<azure><azure-data-factory>","2019-07-08 08:41:27","5683","2","1","56944703","<p>This error is caused by the column mapping. You create the new column &quot;<code>customTime</code>&quot;, but in your table the column name is &quot;myTime&quot;.</p>
<p>What I found that is the csv file column name and schema must be same with the Sink table, or it will gives an error when debugging, this my error message:</p>
<p><a href=""https://i.stack.imgur.com/RdO2B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RdO2B.png"" alt=""enter image description here"" /></a></p>
<blockquote>
<p><strong>Solution 1:</strong>: choose the &quot;Recreate table&quot; in <strong>Sink settings</strong>. But this will change your table schema: <code>myTime</code> to <code>customTime</code>.</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/Nfpwa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Nfpwa.png"" alt=""enter image description here"" /></a></p>
<blockquote>
<p><strong>Solution 2:</strong> Add the same column name with Sink table in <strong>Derived Column's Settings</strong>.</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/ZN8MO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZN8MO.png"" alt=""enter image description here"" /></a></p>
<p>I test with the same opreation with and it worked succeeded.</p>
<p>I read a file (csv) from blob storage and write it to my Azure SQL database using data flows. I create a the Sink table with same schema.</p>
<p>This My csv file:</p>
<p><a href=""https://i.stack.imgur.com/pjufp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pjufp.png"" alt=""enter image description here"" /></a></p>
<p>My data flow:
<a href=""https://i.stack.imgur.com/ednHC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ednHC.png"" alt=""enter image description here"" /></a></p>
<p><strong>Step 1: Source settings</strong>:
<a href=""https://i.stack.imgur.com/VX8Zc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VX8Zc.png"" alt=""enter image description here"" /></a></p>
<p><strong>Step 2: Derived Column's Settings</strong>
<a href=""https://i.stack.imgur.com/wybYJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wybYJ.png"" alt=""enter image description here"" /></a></p>
<p><strong>Step 3: Sink setting:</strong>
<a href=""https://i.stack.imgur.com/NGdnk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NGdnk.png"" alt=""enter image description here"" /></a></p>
<p><strong>Running succeeded:</strong>
<a href=""https://i.stack.imgur.com/4HYy8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4HYy8.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/PdyNd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PdyNd.png"" alt=""enter image description here"" /></a></p>
<p>Hope this helps.</p>
"
"56928607","Connecting DataFactory to Lotus notes DB","<p>I have a requirement to connect to lotus notes DB which is there in On-Prem. There is no built-in connector available in Azure DataFactory V2) to connect to it. </p>

<p>I've read an article about how to get data from lotus notes:</p>

<ol>
<li>Using ODBC driver connection.</li>
<li>.Net code to read it.</li>
</ol>

<p>Did anyone tried above possibility ? If so let us know whether it is possible, and if so what are Pros &amp; Cons.</p>
","<azure><lotus-notes><azure-data-factory>","2019-07-08 04:54:36","452","0","1","56959680","<p>Have you tried using the ODBC connector from data data factory? I have never had to use it, but im pretty sure it will work as any other.</p>

<p>Just go to Connections, click on New, then type ODBC and fill the form! Make sure to select the on premise integration runtime when filling the form, so it connects through that to the database.</p>

<p>Here are some screenshots to help you:</p>

<p><a href=""https://i.stack.imgur.com/8oeuo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8oeuo.png"" alt=""New Linked Service""></a></p>

<p><a href=""https://i.stack.imgur.com/9yKNn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9yKNn.png"" alt=""ODBC Selection""></a></p>

<p><a href=""https://i.stack.imgur.com/P6Bub.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P6Bub.png"" alt=""Fill the form""></a></p>

<p>Hope this helped!</p>
"
"56927668","Transforming data type in Azure Data Factory","<p>I have a ""Copy"" step in my Azure Data Factory pipeline which copies data from CSV file to MSSQL.</p>

<p>Unfortunately, all columns in CSV comes as <code>String</code> data type. How can I change these data types to match the data type in SQL table.</p>

<p><a href=""https://i.stack.imgur.com/XywgU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XywgU.png"" alt=""enter image description here""></a></p>

<p>Here is how the data is available in CSV file.</p>

<p><a href=""https://i.stack.imgur.com/IUDIS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IUDIS.png"" alt=""enter image description here""></a></p>

<p>I would like to change data type of <code>WIPStateKey</code> to <code>Integer</code> and <code>ReportDt</code> to <code>Timestamp</code>. I do not seem to find an option to achieve this.</p>
","<azure-data-factory>","2019-07-08 02:19:25","7775","4","1","56927868","<p>Yes as you said ""all columns in CSV comes as String data type"".</p>

<p>But when using  a copy active, choose the csv file as the source, we can import the schema and change the column data type.</p>

<p>I created a demo.csv file for test:</p>

<p><a href=""https://i.stack.imgur.com/hUlc2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hUlc2.png"" alt=""enter image description here""></a></p>

<p>I copy data from my demo.csv file to my Azure SQL database.
<a href=""https://i.stack.imgur.com/2saSJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2saSJ.png"" alt=""enter image description here""></a></p>

<p>During file format setting, we can change the column data type:
<a href=""https://i.stack.imgur.com/kB5u7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kB5u7.png"" alt=""enter image description here""></a></p>

<p>Table mapping:
<a href=""https://i.stack.imgur.com/tYURS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tYURS.png"" alt=""enter image description here""></a></p>

<p>Column mapping:
<a href=""https://i.stack.imgur.com/VZlxr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VZlxr.png"" alt=""enter image description here""></a></p>

<p>Copy completed:
<a href=""https://i.stack.imgur.com/dRMHy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dRMHy.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/lzwzo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lzwzo.png"" alt=""enter image description here""></a></p>

<p>Hope this helps</p>
"
"56927395","Resource Specific destination table for Azure Data Factory diagnostic settings using Terraform","<p>I am successfully deploying an Azure Data Factory instance and enabling Diagnostic Settings for Azure Monitor and Log Analytics using Terraform.
In the Azure Portal I can see that, for Data Factory, one can select the destination table where data will be stored in Log Analytics: either ""Azure Diagnostics"" or ""Resource Specific"", which seems to be recommended.</p>

<p><a href=""https://i.stack.imgur.com/qeArT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qeArT.png"" alt=""enter image description here""></a></p>

<p>By default, the ""Azure diagnostics"" option is set. </p>

<p>How can I set the destination table to ""Resource specific"" in my Terraform code? I could not find anything in the documentation indicating how to do so.</p>

<p>Here is my code:</p>

<pre><code># Azure Data Factory diagnostic settings
resource ""azurerm_monitor_diagnostic_setting"" ""edp_adf"" {
  name                       = azurerm_data_factory.edp.name
  target_resource_id         = azurerm_data_factory.edp.id
  log_analytics_workspace_id = data.azurerm_log_analytics_workspace.cloud_services.id

  log {
    category = ""ActivityRuns""
    enabled  = true

    retention_policy {
      enabled = true
      days    = 31
    }
  }

  log {
    category = ""PipelineRuns""
    enabled  = true

    retention_policy {
      enabled = true
      days    = 31
    }
  }

  log {
    category = ""TriggerRuns""
    enabled  = true

    retention_policy {
      enabled = true
      days    = 31
    }
  }

  metric {
    category = ""AllMetrics""

    retention_policy {
      enabled = true
      days    = 7
    }
  }
}
</code></pre>
","<azure><terraform><azure-data-factory><terraform-provider-azure><azure-monitoring>","2019-07-08 01:33:00","724","2","1","57720793","<p>This is indeed possible now with the <strong>azurerm</strong> provider version <strong>1.33.0</strong></p>

<p>Just add:</p>

<pre><code>log_analytics_destination_type = ""Dedicated""
</code></pre>

<p>to the Terraform code shown in the question.</p>
"
"56909697","How to persist null properties in Azure table storage?","<p>I'm trying to download some json data to a SQL DB via Azure data factory v2. I'm storing the data in an Azure table field as an intermediate step. However, some properties in my source json might be null, but I still want to maintain them as columns having null values in my azure table to simplify the column mapping to the SQL DB table later. </p>

<p>I read this in the documentation:</p>

<blockquote>
  <p>The Table service does not persist null values for properties. When querying entities, the above property types are all non-nullable. When writing entities, the above property types are all nullable, and any property with a null value is handled as if the payload did not contain that property.</p>
</blockquote>

<p>I've tried adding a DefaultValueAttribute like <code>[DefaultValue(null)]</code> but that doesn't work. </p>

<p>Also, adding a default value to the property doesn't work</p>

<pre><code>public string streetName { get; set; } = ""None"";
</code></pre>

<p>Is there a way in which I can still maintain the null properties?</p>
","<azure-storage><data-annotations><azure-table-storage><azure-data-factory><json-deserialization>","2019-07-05 22:09:56","609","1","1","56927584","<p>As my understanding, azure storage table itself doesn't have schema. </p>

<p>If in the first row, you insert 1 to columnA, 2 to columnB,
Then in the second row, you insert 3 to columnA, 4 to columnC. 
Then if you try to preview data with ADF or azure storage explorer, you will get null for columnC in the first row, and null for columnB in second row. </p>

<p>So to have that null column, seems you only need have a row with null value for that column. </p>

<p>Another way to avoid this issue: maybe you could consider use azure blob storage  as an intermediate step? ADF supports coping data from azure blob storage to azure sql directly. And azure blob storage can store json files. </p>
"
"56908297","In DataFactory, what is a good strategy to migrate data into Dynamics365 using Dynamics Web API?","<p>I need to migrate data using DataFactory to Dynamics365. The Dynamics365 connector is not enough for me since one of the requirements is to only update those attributes that have been modified since last migration - not the whole register. The other requirement is that sometimes we have to 'null' values in destination.</p>

<p>I believe that I can do that by generating a different JSON for register and migrate them using the Web API.</p>

<p>I thought in putting these calls in an Azure Functions, but I believe that they are not meant to be used like this - even though with the right pricing plan they can run with no limit of time.</p>

<p>I think I'm doing it wrong and I can't figure out the right way.</p>

<p>Could you share your experience or point of view?</p>
","<azure-data-factory><dynamics-365>","2019-07-05 19:21:13","69","0","1","57000070","<p>The correct way to interact with Dynamics 365 from other application is either directly with the WebAPI or using C#'s SDK, in both scenarios, for create or update multiple records the best way to do it (as far as i know) is using <a href=""https://learn.microsoft.com/en-us/dynamics365/customer-engagement/developer/org-service/use-executemultiple-improve-performance-bulk-data-load"" rel=""nofollow noreferrer"">ExecuteMultipleRequest</a> Message, this allow you to set it with updates, creates, deletes and execute then in one request.</p>
"
"56904252","Can derived columns in ADF Dataflow be used to map without creating duplicate columns?","<p>I'm trying to change column names using derived columns. I'm able to do this with one mapping per column but after that I get a 'Duplicate Column' problem.</p>

<p>Can this be done with derived columns?</p>

<p>Thanks!</p>
","<azure-data-factory><derived-column>","2019-07-05 13:43:28","223","0","1","56948906","<p>I got past this by making multiple Derived Columns and doing a nested mapping type thing.</p>
"
"56901303","Access Azure Data Factory V2 programmatically: The Resource Microsoft.DataFactory/dataFactories/ under resource group was not found","<p>I'm trying to access Azure Data Fabric V2 programmatically.</p>

<p>First, I had created an App Registration in Azure portal and a Client secret. Then I gave Contributor permission to this App registration on the entire suscription, and also in the resource group where my data factory lives.
Using this credentials I'm able to login to the portal and create an DataFactoryManagementClient</p>

<pre><code>private void CreateAdfClient()
{
    var authenticationContext = new AuthenticationContext($""https://login.windows.net/{tenantId}"");
    var credential = new ClientCredential(clientId: appRegistrationClientId, clientSecret: appRegistrationClientkey);
    var result = authenticationContext.AcquireTokenAsync(resource: ""https://management.core.windows.net/"", clientCredential: credential).ConfigureAwait(false).GetAwaiter().GetResult();

    if (result == null)
    {
        throw new InvalidOperationException(""Failed to obtain the JWT token"");
    }

    var token = result.AccessToken;

    var tokenCloudCredentials = new TokenCloudCredentials(subscriptionId, token);
    datafactoryClient = new DataFactoryManagementClient(tokenCloudCredentials);
}
</code></pre>

<p>However, when I try to get my pipeline with</p>

<pre><code>var pipeline = datafactoryClient.Pipelines.Get(resourceGroup, dataFactory, pipelineName);
</code></pre>

<p>it throws an error:</p>

<blockquote>
  <p>System.Private.CoreLib: Exception while executing function:
  StartRawMeasuresSync. Microsoft.Azure.Management.DataFactories:
  ResourceNotFound: The Resource
  'Microsoft.DataFactory/dataFactories/MyPipeline' under resource group
  'MyResGroup' was not found.</p>
</blockquote>

<p>I had verified that the resource group, the data factory name and the pipeline name are correct, but it keeps throwing this error.</p>
","<azure><azure-data-factory>","2019-07-05 10:28:42","317","0","1","58184225","<p>I had the same issue, and it was due to referencing the Nuget package for Azure Data Factory v1 instead of v2.</p>

<ul>
<li>Version 1: Microsoft.Azure.Management.DataFactories</li>
<li>Version 2: Microsoft.Azure.Management.DataFactory</li>
</ul>
"
"56890551","I can't use a Dynamic concatenation in Azure Function Name inside Data Factory pipeline","<p>I have a concat expression defined in the Function Name setting of an Azure Function in my pipeline, where it concatenates the API Query with the current filename that I want to run on this function. When I debug the pipeline, it fails without giving me any feedback. It just says ""AzureFunction failed:""</p>

<p>If I manually insert the string, it works fine. </p>

<p>the concat expression is:  </p>

<pre><code>@concat('HttpTrigger?filename=', variables('filename'))
</code></pre>

<p>I'm new to Azure, any way I can debug this?</p>
","<azure-data-factory>","2019-07-04 15:01:12","3600","1","2","56898870","<p>You could use Set <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-set-variable-activity"" rel=""nofollow noreferrer"">Variable Activity</a> with your Azure Function Activity.</p>

<p><a href=""https://i.stack.imgur.com/E225Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E225Q.png"" alt=""enter image description here""></a>
In the Variable Activity, set the value of the variable.</p>

<p><a href=""https://i.stack.imgur.com/juagv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/juagv.png"" alt=""enter image description here""></a></p>

<p>Then refer the variable in the Azure Function Activity:</p>

<pre><code>@concat('HttpTriggerJS1?name=',variables('name'))
</code></pre>
"
"56890551","I can't use a Dynamic concatenation in Azure Function Name inside Data Factory pipeline","<p>I have a concat expression defined in the Function Name setting of an Azure Function in my pipeline, where it concatenates the API Query with the current filename that I want to run on this function. When I debug the pipeline, it fails without giving me any feedback. It just says ""AzureFunction failed:""</p>

<p>If I manually insert the string, it works fine. </p>

<p>the concat expression is:  </p>

<pre><code>@concat('HttpTrigger?filename=', variables('filename'))
</code></pre>

<p>I'm new to Azure, any way I can debug this?</p>
","<azure-data-factory>","2019-07-04 15:01:12","3600","1","2","66583126","<p><strong>try this way:</strong>
@concat(variables('FirstName') ,variables('LastName'))</p>
"
"56887342","How to delete a Linked Service in Data Factory?","<p>I have a Data Factory pipeline that contains a bunch of Linked Services. Some of them are not used anymore and I would like to delete them. However, when I try to delete some of them, Data Factory complains that it is still used with the following error:</p>

<blockquote>
  <p>Error: Not able to delete linked service, message returned from
  service - The document cannot be deleted since it is referenced by
  staging_zone_dim.</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/0UQq1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0UQq1.png"" alt=""Error""></a></p>

<p><strong>Question</strong></p>

<p>I made sure that no Datasets or Pipelines references it and still, I do get the error message when I try to delete the linked services. What am I missing?</p>
","<azure><azure-data-factory>","2019-07-04 11:50:52","5346","2","2","56890944","<p>You should have a look in the published view of the data factory and delete the pipelines there too before deleting the linked service. </p>

<p>You can switch here by choosing data factory instead of Azure DevOps:</p>

<p><a href=""https://i.stack.imgur.com/dz2hQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dz2hQ.png"" alt=""enter image description here""></a></p>
"
"56887342","How to delete a Linked Service in Data Factory?","<p>I have a Data Factory pipeline that contains a bunch of Linked Services. Some of them are not used anymore and I would like to delete them. However, when I try to delete some of them, Data Factory complains that it is still used with the following error:</p>

<blockquote>
  <p>Error: Not able to delete linked service, message returned from
  service - The document cannot be deleted since it is referenced by
  staging_zone_dim.</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/0UQq1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0UQq1.png"" alt=""Error""></a></p>

<p><strong>Question</strong></p>

<p>I made sure that no Datasets or Pipelines references it and still, I do get the error message when I try to delete the linked services. What am I missing?</p>
","<azure><azure-data-factory>","2019-07-04 11:50:52","5346","2","2","56935367","<p>I have been able to work around this issue by directly going to the Git repository that contained the <code>json</code> files and deleting the unused configuration files. </p>
"
"56887080","Azure Data Factory: Copy data (or Data Flow) from postgresql database","<p>I'm a little bit lost regarding the Azure data factory. </p>

<p>My goal is to retrieve data from our company postgresql database and transform it into an Azure SQL database (which can later be accessed by Power BI).</p>

<p>I created a pipeline and tried to use ""Data Flow"" as an activity. However, I cannot select my postgresql database as a source there. Valid sources are for example other Azure SQL databases.</p>

<p>So I tried to perform the ""Copy Data"" activity to copy data from the postgresql database into an <code>Azure SQL database (1)</code> and afterwards transform the data using a ""Data Flow"" into an <code>Azure SQL database (2)</code> (which has a different table structure).</p>

<p>Is that a good and valid approach?</p>

<p>There are a couple of problems I'm having with this approach:</p>

<ul>
<li>I cannot select multiple tables from my source postgresql dataset (neither from my target dataset <code>Azure SQL database (1)</code>). Azure gives me the option to select a single table or ""None"". I could cope with this problem if I would create several datasets each with a different table but that seems wrong and not feasible.</li>
<li>I am not sure what would happen if the data is already present in the <code>Azure SQL database (1)</code>. In the ""Data Flow"" activity I have the option to select ""Allow insert"", ""Allow update"" or different key columns. In the  ""Copy Data"" activity I do not have this opportunity. </li>
</ul>

<p>Could someone guide me into the right direction please?</p>
","<azure><azure-data-factory>","2019-07-04 11:35:05","2370","1","1","56899252","<p>There are three options to do the processing of the transformation:</p>

<ol>
<li><p>On data source side
You can either put the logic into a stored procedure (not sure if that is possible with postgresql) or into sql query directly. Then fetch only the result table.</p>

<ul>
<li>Less network traffic</li>
<li>more load on source, maybe not allowed</li>
</ul></li>
<li><p>Processing on Data Factory Integration Runtime
This would be the option with Data Flow. Here the tables are copied to the integration runtime, then processed and then the result is copied to your sink.
Since this is a quiet new option not a lot of connections are available. you might need to workaround with copy data to ASQL Server first.</p>

<ul>
<li>probably slower, depending on the sizes of your resources</li>
<li>easy to build logic/visual interface</li>
</ul></li>
<li><p>Processing on sink
Copy raw data to ASQL Server and run a query or stored procedure there. </p>

<ul>
<li>most flexibility since you can resize ASQL if it takes more resources</li>
<li>fast processing</li>
<li>more flexible if delta ingestion and processing is possible (doing upserts / merges)</li>
<li>lot of maintenance</li>
</ul></li>
</ol>

<p>All three are valid options and completely depend on your use case and requirements (regarding operations, SLAs, ...)</p>

<p>About the problem with selecting multiple tables:
You will need to do every table separately, but if they all follow the same logic, you can create a watermark table, lookup all the tablenames in there and loop over the copy module. That way you only have one pipeline that copies all the tables sequentially.</p>

<p>About the data present:
copy only inserts the data. If you want to truncate the table first, you can add that as a ""pre copy script"".</p>

<p>If I forgot anything please comment, I will update the post accordingly </p>
"
"56886989","Execute SSIS Package in Azure VM from Data Factory","<p>Is it possible to execute an SSIS package from a Data Factory pipeline where the SSIS packages are deployed to an existing SQL Server instance on a VM also in Azure?</p>

<p>I looked into creating a self-hosted integration runtime but this is only data migration related.  Also looked at executing a stored procedure but there is no returned value on success/fail etc.</p>

<p>Thanks</p>
","<azure><ssis><virtual-machine><azure-data-factory>","2019-07-04 11:29:43","163","1","1","57041099","<p>Spoke to Microsoft and they confirmed you cannot run or trigger SSIS packages that are not hosted in the Azure Integration Runtime.  No plans to change that anytime soon. </p>
"
"56876302","The length of execution ouput is over limit (around 1M currently) in azure adf webactivity rest api","<p>Calling azure rest api (/consumption/usagedetails) in webactivity using azure data factory .It throws error The length of execution output is over limit (around 1M currently).Any suggestion</p>
","<azure><azure-data-factory>","2019-07-03 18:46:56","3516","4","3","56880056","<p>Please reference this blog: <a href=""https://stackoverflow.com/questions/52843154/web-activity-throws-overlimit-error-when-calling-rest-api"">Web activity throws overlimit error when calling rest api</a>, Jay Gong has given the answer.</p>

<p>Web activity has times out limitation for 1 minute. Also, based on the above error <code>The length of execution ouput is over limit (around 1M currently).</code>, web activity also has output size limitation for 1 MB.</p>

<p>You could find the limitation rules from <a href=""https://learn.microsoft.com/en-us/azure/azure-subscription-service-limits#data-factory-limits"" rel=""nofollow noreferrer"">Data Factory limits</a> and some of the them could be adjusted if you ask for <a href=""https://azure.microsoft.com/en-us/blog/azure-limits-quotas-increase-requests/"" rel=""nofollow noreferrer"">Contact Support</a>.</p>

<p>According the Data Factory limits and your data size, choose the right Azure Data Factory component. Such as Yuvarajan said: You can create a HTTP link service and HTTP data set and pull the data from REST API.</p>

<p>Hope this helps.</p>
"
"56876302","The length of execution ouput is over limit (around 1M currently) in azure adf webactivity rest api","<p>Calling azure rest api (/consumption/usagedetails) in webactivity using azure data factory .It throws error The length of execution output is over limit (around 1M currently).Any suggestion</p>
","<azure><azure-data-factory>","2019-07-03 18:46:56","3516","4","3","56880895","<p>Web activity has  limitation of output size of 1 MB. You can create a HTTP link service and
HTTP data set and pull the data from REST API.</p>
"
"56876302","The length of execution ouput is over limit (around 1M currently) in azure adf webactivity rest api","<p>Calling azure rest api (/consumption/usagedetails) in webactivity using azure data factory .It throws error The length of execution output is over limit (around 1M currently).Any suggestion</p>
","<azure><azure-data-factory>","2019-07-03 18:46:56","3516","4","3","73697237","<p>If you have SQL Server 2017 (14.x) and later (or SQL Managed Instance), you can use python to do this instead. Recommend to use Postman to generate the python script. As an additional extra, you can use the openjson function to convert turn this into tabular format too.</p>
<pre><code>DECLARE @response NVARCHAR(MAX)

EXECUTE sp_execute_external_script @language = N'Python'
    , @script = N'

import requests
import json

url = &quot;https://...................................&quot;

payload = json.dumps({
  &quot;api_key&quot;: &quot;............................&quot;
})
headers = {
  ''Subscription-Key'': ''.........................'',
  ''Content-Type'': ''application/json'',
  ''Ocp-Apim-Trace'': ''false''
}

response = requests.request(&quot;POST&quot;, url, headers=headers, data=payload)
response = response.text
'
,@params = N'@response NVARCHAR(MAX) OUTPUT'
,@response=@response OUTPUT

INSERT INTO dbo.mytable ([response])
SELECT @response
</code></pre>
"
"56873333","Best practice to move local files to BLOB (copy / verify / delete)","<p>I have a local Integration runtime installed on a client machine (MS Server 2012R2) with access to a local folder there. Another program spits out .csv files periodically ~ 1 every 5 minutes.
I need to copy them as is to blob storage (binaries is fine). Once they are successfully copied i need to go back and delete just those files that were uploaded i.e. do not delete any files that may have been added since the pipeline run started.</p>

<p>I know there's no 'move' activity in ADF and I've seen some different ways to couple together copy &amp; delete activities with a ForEach to do a move but am unsure of best practice. </p>

<p>I'm trying to do all of this in the UI (no code) and have no problem doing a simple file copy or delete activity - im just unsure of how to link them together with a ForEach - or if thats even the best approach?</p>

<p>expected results is to handle when there's no files to copy but if there is ADF should copy all files available then delete only those that were copied successfully.</p>
","<azure-data-factory>","2019-07-03 15:19:26","1250","1","1","56899665","<p>Provide two ways for your references:</p>

<p>First one,as you mentioned in your question,using <strong>Azure Data Factory</strong>.</p>

<p>You could use Copy Activity to transfer data from on-premise folder into Azure Blob Storage. Then use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/delete-activity"" rel=""nofollow noreferrer"">Delete Activity</a> to delete files in the on-premise folder.</p>

<p>Run the entire pipeline in the schedule, follow this <a href=""https://azure.microsoft.com/en-us/blog/incrementally-copy-new-files-by-lastmodifieddate-with-azure-data-factory/"" rel=""nofollow noreferrer"">guide</a> to filter files by the file latest modified time.Between copy activity and delete activity, use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-if-condition-activity"" rel=""nofollow noreferrer"">if-condition activity</a>  to control the transmission process. If the copy activity has been executed successfully, delete actions will be continued.</p>

<p>Second one,using <strong>Az Copy</strong>.</p>

<p>Az copy could migrate on-premises data to cloud storage. (Detailed <a href=""https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-migrate-on-premises-data?tabs=linux#create-a-scheduled-task"" rel=""nofollow noreferrer"">link</a>)</p>

<p>Below example command recursively copies data from a local directory to a blob container. A fictitious SAS token is appended to the end of the of the container URL.</p>

<pre><code>azcopy cp ""C:\local\path"" ""https://account.blob.core.windows.net/mycontainer1/?sv=2018-03-28&amp;ss=bjqt&amp;srt=sco&amp;sp=rwddgcup&amp;se=2019-05-01T05:01:17Z&amp;st=2019-04-30T21:01:17Z&amp;spr=https&amp;sig=MGCXiyEzbtttkr3ewJIh2AR8KrghSy1DGM9ovN734bQF4%3D"" --recursive=true
</code></pre>

<p>You still could run the az copy command in the schedule: <a href=""https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-migrate-on-premises-data?tabs=linux#create-a-scheduled-task"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-migrate-on-premises-data?tabs=linux#create-a-scheduled-task</a></p>

<p>In addition,you could refer to this <a href=""https://learn.microsoft.com/en-us/azure/storage/common/storage-choose-data-transfer-solution?toc=%2Fazure%2Fstorage%2Ffiles%2Ftoc.json"" rel=""nofollow noreferrer"">link</a> to get know about multiple Azure solution for data transfer.</p>
"
"56872562","Data flow sink to csv not using file name","<p>I'm starting to use the Dataflow preview from Data Factory, and one problem I'm having is doing a sink to delimited file on Blob Storage. I try to select the ""Output to single file"" option in settings, but when the dataflow runs, it generates a folder with a ID, and inside that folder a csv file is created with the pattern ""part-0000-.csv"", among with other files that I do not know the meaning of, not following the filename I inputed in settings.</p>

<p>How do I set the data flow to make the transformations and sink into a csv file on blob storage with a name pattern of my choosing??</p>
","<azure-data-factory>","2019-07-03 14:39:28","736","0","1","56875749","<p>I found out what was the problem. The Blob Storage connection needs to have a folder specified in the destination, otherwise the sink creates a structure that ignores the filename specified in the settings.</p>
"
"56872183","What might cause the 'InternalServerError executing request' when running a manually triggered pipeline?","<p>The setup of the pipeline is a simple import from a <code>.csv</code> file stored in Azure Blob Storage to an Azure SQL database table.
When I run the pipeline in Debug by using the 'Debug' button in the portal, the job finishes in 8 seconds.
When I run the pipeline with the <code>Add trigger\Trigger now</code> button it runs for 20+ minutes and fails with the error 'InternalServerError executing request'.</p>

<p>I recreated the pipeline and components from scratch and tried using a Data Flow (Preview) and a Copy Data, both give the same result.</p>

<p>The expected output is a successful run of the pipeline, the actual output is the 'InternalServerError executing request' error.</p>
","<azure><azure-sql-database><azure-data-factory>","2019-07-03 14:19:12","6802","2","3","56872803","<p>The problem was with source control, which we recently enabled. The 'Add trigger\Trigger now' uses the published version of the pipeline. The Debug uses the currently saved version of the pipeline. The 20 minutes timeout and the 'InternalServerError executing request' is a poor way of saying: 'You did not publish your pipeline yet' :)</p>
"
"56872183","What might cause the 'InternalServerError executing request' when running a manually triggered pipeline?","<p>The setup of the pipeline is a simple import from a <code>.csv</code> file stored in Azure Blob Storage to an Azure SQL database table.
When I run the pipeline in Debug by using the 'Debug' button in the portal, the job finishes in 8 seconds.
When I run the pipeline with the <code>Add trigger\Trigger now</code> button it runs for 20+ minutes and fails with the error 'InternalServerError executing request'.</p>

<p>I recreated the pipeline and components from scratch and tried using a Data Flow (Preview) and a Copy Data, both give the same result.</p>

<p>The expected output is a successful run of the pipeline, the actual output is the 'InternalServerError executing request' error.</p>
","<azure><azure-sql-database><azure-data-factory>","2019-07-03 14:19:12","6802","2","3","57426056","<p>Just to add another possible cause in case someone else stumbles upon this:</p>

<p>I had the same error multiple times when I had many concurrent pipeline runs, in my case triggered by hundreds of new files in a OneDrive folder (""manually"" triggering the pipeline via Azure Logic App). Some of the runs succeeded, some of them failed. When I reran the failed runs or loaded fewer files at once, it worked.</p>

<p>So the Data Factory might not be ready yet to handle parallel execution very well.</p>
"
"56872183","What might cause the 'InternalServerError executing request' when running a manually triggered pipeline?","<p>The setup of the pipeline is a simple import from a <code>.csv</code> file stored in Azure Blob Storage to an Azure SQL database table.
When I run the pipeline in Debug by using the 'Debug' button in the portal, the job finishes in 8 seconds.
When I run the pipeline with the <code>Add trigger\Trigger now</code> button it runs for 20+ minutes and fails with the error 'InternalServerError executing request'.</p>

<p>I recreated the pipeline and components from scratch and tried using a Data Flow (Preview) and a Copy Data, both give the same result.</p>

<p>The expected output is a successful run of the pipeline, the actual output is the 'InternalServerError executing request' error.</p>
","<azure><azure-sql-database><azure-data-factory>","2019-07-03 14:19:12","6802","2","3","71828638","<p>Just to add another possible cause in case someone else stumbles upon this:
Check if the data factory is down from the Resource Health tab.
I was getting Internal Server Error for all the sandbox runs.</p>
"
"56870243","Recommendation for storing and querying DataFactory run log?","<p>I'd like to store and query the OUTPUT and ERROR data generated during a DataFactory run. The data is returned when calling Get-AzDataFactoryV2ActivityRun.</p>

<p>The intention is to use it to monitore possible pipeline execution error, duration, etc in a easy and fast way.</p>

<p>The data ressembles JSON format. What would be nice is to visualize the summary of each execution through some html. Should I store this log into a MongoDB?</p>

<p>Is there an easy and better way to centralize the log info of the multiple execution of different pipelines? </p>

<pre><code>
ResourceGroupName : Test
DataFactoryName   : DFTest
ActivityRunId     : 00000000-0000-0000-0000-000000000000
ActivityName      : If Condition1
PipelineRunId     : 00000000-0000-0000-0000-000000000000
PipelineName      : Test
Input             : {}
Output            : {}
LinkedServiceName :
ActivityRunStart  : 03/07/2019 11:27:21
ActivityRunEnd    : 03/07/2019 11:27:21
DurationInMs      : 000
Status            : Succeeded
Error             : {errorCode, message, failureType, target}

Activity 'Output' section:
""firstRow"": {
  ""col1"": 1
}
""effectiveIntegrationRuntime"": ""DefaultIntegrationRuntime (West Europe)""
</code></pre>
","<json><azure><azure-data-factory>","2019-07-03 12:31:39","64","0","1","56878562","<p>This is probably not the best way how you can monitor your ADF pipelines.<br>
Have you considered to use Azure Monitor?<br>
Find out more:<br>
- <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor</a><br>
- <a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/visualizations"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-monitor/visualizations</a> </p>
"
"56869262","Data flow source with wild card chars filename","<p>I am trying to pass dynamic path to data flow source as below.</p>

<p>-->  data/dev/int007/in/src_int007_src_snk_opp_*.tsv</p>

<p>Its not working. </p>

<p>Anyone knows how generate dynamic path with partial filename and wildcard chars ?
I have already tried to use dynamic content options.</p>

<p><a href=""https://i.stack.imgur.com/CoqOl.png"" rel=""nofollow noreferrer"">data flow source</a>
<a href=""https://i.stack.imgur.com/uZOgA.png"" rel=""nofollow noreferrer"">error</a></p>
","<azure><azure-data-factory>","2019-07-03 11:38:04","2275","1","2","56895906","<p>The container is required and can't be wildcard, you can check if 'data' is your blob container.</p>

<p>in your case:</p>

<p>cantainer:  data</p>

<p>wildcardpath: dev/int007/in/src_int007_src_snk_opp_*.tsv</p>

<p><a href=""https://i.stack.imgur.com/8Gjcn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8Gjcn.png"" alt=""pic""></a></p>
"
"56869262","Data flow source with wild card chars filename","<p>I am trying to pass dynamic path to data flow source as below.</p>

<p>-->  data/dev/int007/in/src_int007_src_snk_opp_*.tsv</p>

<p>Its not working. </p>

<p>Anyone knows how generate dynamic path with partial filename and wildcard chars ?
I have already tried to use dynamic content options.</p>

<p><a href=""https://i.stack.imgur.com/CoqOl.png"" rel=""nofollow noreferrer"">data flow source</a>
<a href=""https://i.stack.imgur.com/uZOgA.png"" rel=""nofollow noreferrer"">error</a></p>
","<azure><azure-data-factory>","2019-07-03 11:38:04","2275","1","2","56908912","<p>'DataFlow expression has error' results when you have invalid syntax for your expression when using dynamic content. In this scenario, I do not think dynamic content is necessary. </p>

<p>As @Atvoid, point your DelimitedText dataset at your blob container 'data' and specify your wildcard path in the dataset source to be 'dev/int007/in/src_int007_src_snk_opp_*.tsv'.</p>

<p>You can verify your wildcard path is working by turning on debug and checking the data preview in your source</p>
"
"56864108","Data sync between temporal tables in SQL Server","<p>I want to Sync data between two databases which have system-versioned(temporal) tables. The data sync option in Azure does not support temporal tables and I need to find another way of doing the sync between the databases. </p>

<p>I want to copy the data from one temporal table to another temporal table of another database using Azure data factory.Will Azure data factory support the data sync between temporal tables? </p>

<p>What will be the best way of syncing data between the two databases that have identical temporal tables?</p>
","<sql-server><azure><azure-data-factory><data-synchronization><temporal-tables>","2019-07-03 06:26:15","1568","0","2","56864620","<p>I created a system-versioned(temporal) table in my Azure SQL database followed this document <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/tables/creating-a-system-versioned-temporal-table?view=sql-server-2017#creating-a-temporal-table-with-a-default-history-table"" rel=""nofollow noreferrer"">Creating a temporal table</a>:</p>

<pre><code>CREATE TABLE Department   
(    
     DeptID int NOT NULL PRIMARY KEY CLUSTERED  
   , DeptName varchar(50) NOT NULL  
   , ManagerID INT  NULL  
   , ParentDeptID int NULL  
   , SysStartTime datetime2 GENERATED ALWAYS AS ROW START NOT NULL  
   , SysEndTime datetime2 GENERATED ALWAYS AS ROW END NOT NULL  
   , PERIOD FOR SYSTEM_TIME (SysStartTime, SysEndTime)     
)   
WITH    
   (   
      SYSTEM_VERSIONING = ON (HISTORY_TABLE = dbo.DepartmentHistory)   
   )   
;
</code></pre>

<p>I created two temporal tables Department and Department2 in different Azure SQL Server. </p>

<p>I test in the Data Factory copy active, choose the Department as source dataset, we can see the temporal table from the settings:
<a href=""https://i.stack.imgur.com/GuCLM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GuCLM.png"" alt=""enter image description here""></a></p>

<p>Choose the Department2 as link dataset, Table mapping::
<a href=""https://i.stack.imgur.com/mKWQV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mKWQV.png"" alt=""enter image description here""></a></p>

<p>Column mapping:
<a href=""https://i.stack.imgur.com/HzKRI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HzKRI.png"" alt=""enter image description here""></a></p>

<p>Active run succeeded:
<a href=""https://i.stack.imgur.com/W86lm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W86lm.png"" alt=""enter image description here""></a></p>

<p>That's mean Azure Data Factory support copy the data from one temporal table to another temporal table of another database.</p>

<p><strong>Update:</strong></p>

<p>Please reference this document: <a href=""https://sqland.wordpress.com/2016/06/30/temporal-tables-in-sql-server-2016-part-iii/"" rel=""nofollow noreferrer"">Temporal Tables in SQL Server 2016 – Part III</a>.</p>

<p>Now, starting by INSERT, remember there are two datetime2 special columns in a temporal table declared as GENERATED ALWAYS AS ROW START / END.
These are the PERIOD columns and they are mandatory but you cannot insert an explicit value into a GENERATED ALWAYS column. Its values will be automatically filled.
When you insert a new row in the table, the “ROW START” column will have the value of SYSUTCDATETIME() (yes, don’t forget it’s UTC time!) and the “ROW END” column will have the value: ‘9999-12-31 23:59:59.9999999’
Basically when inserting a new row in a temporal table, regarding these two columns, you can use of the following options:
1. Use a column list and omit these two columns;
2. Use a column list without omitting these two columns and specify DEFAULT in the values list for each.
3. Don’t use a column list and specify DEFAULT in the values list for each.</p>

<p>Congratulations, the error has solved by yourself:</p>

<blockquote>
  <ol>
  <li>Loading the tables after switching off the versioning on the destination table and it worked.</li>
  </ol>
</blockquote>

<p><code>ALTER TABLE [dbo].[Department] SET(SYSTEM_VERSIONING = OFF); ALTER TABLE [dbo].[Department] DROP PERIOD FOR SYSTEM_TIME;</code> </p>

<blockquote>
  <ol start=""2"">
  <li>Then after the copy active done, switch it back.</li>
  </ol>
</blockquote>

<pre><code>ALTER TABLE [dbo].[Department] ADD PERIOD FOR SYSTEM_TIME (SysStartTime, SysEndTime); ALTER TABLE [dbo].[Department] SET(SYSTEM_VERSIONING = ON);
</code></pre>

<p>Hope this helps.</p>
"
"56864108","Data sync between temporal tables in SQL Server","<p>I want to Sync data between two databases which have system-versioned(temporal) tables. The data sync option in Azure does not support temporal tables and I need to find another way of doing the sync between the databases. </p>

<p>I want to copy the data from one temporal table to another temporal table of another database using Azure data factory.Will Azure data factory support the data sync between temporal tables? </p>

<p>What will be the best way of syncing data between the two databases that have identical temporal tables?</p>
","<sql-server><azure><azure-data-factory><data-synchronization><temporal-tables>","2019-07-03 06:26:15","1568","0","2","59258515","<p>It sounds like this thread got the answer it needed, but for future searches... the Azure SQL Data Sync within Azure SQL Database <strong>does</strong> support syncing temporal tables.</p>

<p>You just have to remember to not sync the system generated date columns (because you can't explicitly INSERT into those columns, and Data Sync will try :-))</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/sql-database/sql-database-sync-data#sync-req-lim"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/sql-database/sql-database-sync-data#sync-req-lim</a></p>

<p>HTH</p>
"
"56863346","how to change Data Factory in Microsoft Integration Runtime COnfiguration Manager?","<p>I have Installed Microsoft Integration Runtime configuration Manager When I have Migrated Data from On-Premise SQL Server to Azure Data Lake and when I'm trying to use for another Azure Data Factory I don't find a space to add new key for the data factory. How to do it.
Thanks in Advance </p>
","<azure><azure-data-lake><azure-data-factory><azure-synapse>","2019-07-03 05:19:09","10034","10","3","56863689","<p>You can reuse an existing self-hosted integration runtime infrastructure that you already set up in a data factory. This enables you to create a linked self-hosted integration runtime in a different data factory by referencing an existing self-hosted IR (shared).</p>
<p>To share a self-hosted integration runtime by using PowerShell, see <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-shared-self-hosted-integration-runtime-powershell"" rel=""nofollow noreferrer"">Create a shared self-hosted integration runtime in Azure Data Factory with PowerShell</a>.</p>
<p>For a twelve-minute introduction and demonstration of this feature, watch the following video: <a href=""https://www.youtube.com/watch?v=i1bCkENsaLY"" rel=""nofollow noreferrer"">Hybrid data movement across multiple Azure Data Factories</a>.</p>
<p>For more details, refer &quot;<a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#sharing-the-self-hosted-integration-runtime-with-multiple-data-factories"" rel=""nofollow noreferrer"">Sharing the self-hosted integration runtime with multiple data factories</a>&quot;.</p>
"
"56863346","how to change Data Factory in Microsoft Integration Runtime COnfiguration Manager?","<p>I have Installed Microsoft Integration Runtime configuration Manager When I have Migrated Data from On-Premise SQL Server to Azure Data Lake and when I'm trying to use for another Azure Data Factory I don't find a space to add new key for the data factory. How to do it.
Thanks in Advance </p>
","<azure><azure-data-lake><azure-data-factory><azure-synapse>","2019-07-03 05:19:09","10034","10","3","57572793","<p>On the machine where your Integration Runtime is installed, you should have a file named:</p>

<p><code>C:\Program Files\Microsoft Integration Runtime\3.0\PowerShellScript\RegisterIntegrationRuntime.ps1</code></p>

<p>Running it with your <code>domain\username</code> as your <code>$credential</code> and your <code>Key1</code> from ADF as your <code>$gatewayKey</code> will result in a re-registration, binding your local IR process to the IR identity in your new Data Factory.</p>

<p>Source: <a href=""https://github.com/MicrosoftDocs/azure-docs/issues/7956"" rel=""noreferrer"">https://github.com/MicrosoftDocs/azure-docs/issues/7956</a></p>
"
"56863346","how to change Data Factory in Microsoft Integration Runtime COnfiguration Manager?","<p>I have Installed Microsoft Integration Runtime configuration Manager When I have Migrated Data from On-Premise SQL Server to Azure Data Lake and when I'm trying to use for another Azure Data Factory I don't find a space to add new key for the data factory. How to do it.
Thanks in Advance </p>
","<azure><azure-data-lake><azure-data-factory><azure-synapse>","2019-07-03 05:19:09","10034","10","3","72692209","<p>I cannot comment on Casper Lehmann's post, but I wanted to say that I tried running the script on PowerShell core (version 7.2.4) and it didn't work; however, in regular PowerShell (included in Windows) it works. Just FYI.</p>
"
"56856123","Which country does the datacenter sit for East Asia and SouthEast Asia","<p>Can someone provide information which geographical location does ""East Asia"" and ""SouthEast Asia"" Azure Data center sit? </p>
","<azure><microsoft-graph-api><azure-data-factory>","2019-07-02 15:51:20","556","2","2","56856304","<p>Azure data center <code>East Asia</code>  located in <code>Hong Kong</code> and <code>Southeast Asia</code>  located <code>Singapore</code></p>

<p>You can found all the azure data center list <a href=""https://azure.microsoft.com/en-us/global-infrastructure/locations/"" rel=""nofollow noreferrer"">here</a></p>
"
"56856123","Which country does the datacenter sit for East Asia and SouthEast Asia","<p>Can someone provide information which geographical location does ""East Asia"" and ""SouthEast Asia"" Azure Data center sit? </p>
","<azure><microsoft-graph-api><azure-data-factory>","2019-07-02 15:51:20","556","2","2","56878973","<p>As mentioned above. There are details on data centers on the <a href=""https://azure.microsoft.com/en-us/global-infrastructure/locations/"" rel=""nofollow noreferrer"">Azure Data Center list</a>.</p>

<p>As you tagged Microsoft Graph. You will also want to see what is available in <a href=""https://learn.microsoft.com/en-us/graph/deployments"" rel=""nofollow noreferrer"">each data center</a>.</p>
"
"56854801","Azure Data Factory - Calling Logic App Synchronously","<p>As part of our flow we are calling an logic app from our ADF. Logic app calls other  website with REST calls and some other task related to containers. </p>

<p>However, the problem is,  Call from ADF to Logic App is Asynchronous, So even if Logic App fails ADF pipeline is success. </p>

<p>Is there any way to either make it synchronous, such that ADF pipeline wait until Logic App is finishes, and it fails or pass based on the status of logic App. </p>

<p>or, is there any way I can poll the status Logic App from within ADF? </p>
","<azure><azure-data-factory>","2019-07-02 14:33:25","1322","0","2","56856682","<p>You can create a Custom Web API request that receives an success or fail state and you then you can send that parameter to your pipeline.</p>

<p>For example you create a custom task for the API request.
<a href=""https://i.stack.imgur.com/QF66o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QF66o.png"" alt=""enter image description here""></a></p>

<p>Then you build the pipeline with an Lookup to the received state and does an action.</p>

<p><a href=""https://i.stack.imgur.com/mxLUf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mxLUf.png"" alt=""enter image description here""></a></p>
"
"56854801","Azure Data Factory - Calling Logic App Synchronously","<p>As part of our flow we are calling an logic app from our ADF. Logic app calls other  website with REST calls and some other task related to containers. </p>

<p>However, the problem is,  Call from ADF to Logic App is Asynchronous, So even if Logic App fails ADF pipeline is success. </p>

<p>Is there any way to either make it synchronous, such that ADF pipeline wait until Logic App is finishes, and it fails or pass based on the status of logic App. </p>

<p>or, is there any way I can poll the status Logic App from within ADF? </p>
","<azure><azure-data-factory>","2019-07-02 14:33:25","1322","0","2","56857332","<p>I'm a dev in ADF.</p>

<p>You can use Webhook activity.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-webhook-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-webhook-activity</a> to call your logic app passing the callback url as a parameter. At the end of your logic app, you can add an action to call this callback url and pass the status that you need for your ADF to mark it as succeeded/failed</p>
"
"56854139","Azure ARM Template deployment of DataFactory with Azure DevOps Git configuration","<p>I am attempting to deploy an Azure DataFactory resource and configure it to use Azure DevOps Git for source control. The Azure Devops organization, repository and collaboration branch all exist. </p>

<p>When I deploy the template, the DataFactory resource is created but it is not wired up to source control. My account has access to the Azure DevOps organization and I can manually wire up source control</p>

<p>I am using the following template:</p>

<pre><code>{
    ""contentVersion"": ""1.0.0.0"",
    ""$schema"": ""https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#"",
    ""variables"": {
        ""repoConfiguration"": {
            ""accountName"": ""my-account"",
            ""collaborationBranch"": ""dev"",
            ""lastCommitId"": """",
            ""projectName"": ""Azure"",
            ""repositoryName"": ""golaat"",
            ""rootFolder"": ""/"",
            ""tenantId"": """",
            ""type"": ""FactoryVSTSConfiguration""
        }
    },
    ""resources"": [
        {
            ""type"": ""Microsoft.DataFactory/factories"",
            ""apiVersion"": ""2018-06-01"",
            ""name"": ""my-resource-golaat8-adf"",
            ""location"": ""eastus2"",
            ""identity"": {
              ""type"": ""SystemAssigned""
            },
            ""properties"": {
              ""repoConfiguration"": ""[variables('repoConfiguration')]""
            },
            ""resources"": []
          }
        ]
}
</code></pre>
","<git><azure><azure-resource-manager><azure-data-factory><azure-deployment>","2019-07-02 13:56:30","1630","2","1","56863897","<p>You need to get the repoConfiguration from variables as following: </p>

<p>""repoConfiguration"": ""[variables('repoConfiguration')]""</p>

<p>Do not miss the Square Brackets. I tried at my side and got a success. </p>

<pre><code>{
    ""$schema"": ""https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#"",
    ""contentVersion"": ""1.0.0.0"",
    ""parameters"": {
        ""name"": {
            ""defaultValue"": ""myv2datafactory"",
            ""type"": ""String""
        },
        ""location"": {
            ""defaultValue"": ""East US"",
            ""type"": ""String""
        },
        ""apiVersion"": {
            ""defaultValue"": ""2018-06-01"",
            ""type"": ""String""
        },
        ""gitAccountName"": {
            ""type"": ""String""
        },
        ""gitRepositoryName"": {
            ""type"": ""String""
        },
        ""gitBranchName"": {
            ""defaultValue"": ""master"",
            ""type"": ""String""
        },
        ""gitRootFolder"": {
            ""defaultValue"": ""/"",
            ""type"": ""String""
        },
        ""gitProjectName"": {
            ""type"": ""String""
        }
    },
    ""variables"": {
        ""repoConfiguration"": {
            ""type"": ""FactoryVSTSConfiguration"",
            ""accountName"": ""[parameters('gitAccountName')]"",
            ""repositoryName"": ""[parameters('gitRepositoryName')]"",
            ""collaborationBranch"": ""[parameters('gitBranchName')]"",
            ""rootFolder"": ""[parameters('gitRootFolder')]"",
            ""projectName"": ""[parameters('gitProjectName')]""
        }
    },
    ""resources"": [
        {
            ""type"": ""Microsoft.DataFactory/factories"",
            ""apiVersion"": ""[parameters('apiVersion')]"",
            ""name"": ""[parameters('name')]"",
            ""location"": ""[parameters('location')]"",
            ""identity"": {
                ""type"": ""SystemAssigned""
            },
            ""properties"": {
                ""repoConfiguration"": ""[variables('repoConfiguration')]""
            }
        }
    ]
}
</code></pre>
"
"56850765","How to transform data from on-premises sql/azure sql database in the cloud using Azure Data Factory?","<p>Suppose I have an SQL table as follows,</p>

<pre><code>ID, Name, city
1, saran, Chennai
2, raj, Chennai
3, Kumar, NYC
</code></pre>

<p>suppose I wanna transform name from saran to raj, what component I have to use, because in Azure data factory I can see N number of components. Lets say azure databricks, u-sql, ect as data transform activity.</p>

<p>So please tell me that Which one I have to choose, if I have data from normal relational DB.</p>
","<azure><azure-sql-database><azure-data-factory>","2019-07-02 10:48:01","67","0","1","56863074","<p>You can use any of those to transform the data, coding with whichever language/platform you prefer.</p>

<p>Or you can just design your transformations visually in ADF's Mapping Data Flows without any coding.</p>
"
"56850383","My pipeline triggers twice, but it is set to trigger once a day","<p>I have simple pipeline with copy activity that copies tables from on-premise to azure data warehouse and i have schedules trigger that should run once a day. But last 3-4 days it is triggered twice the same time both. One thing that has been changed last time, is Azure DevOps Git usage, we uploaded data factory to it by arm template. And now our project is in 'Data Factory' and also 'Azure DevOps Git'. But in Azure DevOps Git there are no pipelines.</p>
","<azure-devops><azure-data-factory>","2019-07-02 10:26:16","1159","0","1","56850510","<p>oh, i have found solution, in the left bottom corner of the data factory, in triggers menu you can edit trigger, and there is number of pipelines, and somehow my pipeline was mentioned there twice.</p>
"
"56832792","Azure data factory activity execute after all other copy data activities have completed","<p>I've got an Azure Data Factory V2 pipeline with multiple Copy Data activities that run in parallel.</p>

<p><a href=""https://i.stack.imgur.com/1bI9X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1bI9X.png"" alt=""enter image description here""></a></p>

<p>I have a Pause DW web hook pauses an Azure data warehouse after each run. This activity is set to run after the completion of one of the longest running activities in the pipeline.  The pipeline is set to trigger nightly.</p>

<p>Unfortunately, the time taken to run copy data activities varies because it depends on transactions that have been processed in the business, which varies each day.  This means, I can't predict which activity of those that run in parallel will finish last.  This means, often the whole pipeline fails because the DW has been paused before some of the activities have started.</p>

<p>What's the best way of running an activity only after all other activities in the pipeline have completed?</p>

<p>I have tried to add an If activity to the pipeline like this:</p>

<p><a href=""https://i.stack.imgur.com/G4EXM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G4EXM.png"" alt=""enter image description here""></a></p>

<p>However, I then run into this error during validation:</p>

<p><em>If Condition1
The output of activity 'Copy small tables' can't be referenced since it has no output.</em></p>

<p><a href=""https://i.stack.imgur.com/gFjcS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gFjcS.png"" alt=""enter image description here""></a></p>

<p>Does anyone have any idea how I can move this forwards?</p>

<p>thanks</p>
","<azure><azure-data-factory><azure-synapse>","2019-07-01 09:01:51","1940","1","2","56834466","<p>Just orchestrate all your parallel activities towards PAUSE DWH activity. Then it will be executed after all your activities are completed. </p>
"
"56832792","Azure data factory activity execute after all other copy data activities have completed","<p>I've got an Azure Data Factory V2 pipeline with multiple Copy Data activities that run in parallel.</p>

<p><a href=""https://i.stack.imgur.com/1bI9X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1bI9X.png"" alt=""enter image description here""></a></p>

<p>I have a Pause DW web hook pauses an Azure data warehouse after each run. This activity is set to run after the completion of one of the longest running activities in the pipeline.  The pipeline is set to trigger nightly.</p>

<p>Unfortunately, the time taken to run copy data activities varies because it depends on transactions that have been processed in the business, which varies each day.  This means, I can't predict which activity of those that run in parallel will finish last.  This means, often the whole pipeline fails because the DW has been paused before some of the activities have started.</p>

<p>What's the best way of running an activity only after all other activities in the pipeline have completed?</p>

<p>I have tried to add an If activity to the pipeline like this:</p>

<p><a href=""https://i.stack.imgur.com/G4EXM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G4EXM.png"" alt=""enter image description here""></a></p>

<p>However, I then run into this error during validation:</p>

<p><em>If Condition1
The output of activity 'Copy small tables' can't be referenced since it has no output.</em></p>

<p><a href=""https://i.stack.imgur.com/gFjcS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gFjcS.png"" alt=""enter image description here""></a></p>

<p>Does anyone have any idea how I can move this forwards?</p>

<p>thanks</p>
","<azure><azure-data-factory><azure-synapse>","2019-07-01 09:01:51","1940","1","2","56842318","<p>I think you can use the Execute pipeline activity . </p>

<p>Let the trigger point to the new pipeline which has the ""Execute activity "" which points to the current ADF with the copy activity , please do select the option Advanced -> Wait for completion . Once the execute pipeline is done it should to move to the webhook activity which should have the logic to pause the DW .</p>

<p>Let me know how this goes .</p>
"
"56832634","How to re-try an ADF pipeline execution until conditions are met","<p>An ADF pipeline needs to be executed on a daily basis, lets say at 03:00 h AM.</p>

<p>But prior execution we also need to check if the data sources are available.</p>

<p>Data is provided by an external agent, it periodically loads the corresponding data into each source table and let us know when this process is completed using a flag-table: if data source 1 is ready it set flag to 1.</p>

<p>I don't find a way to implement this logic with ADF.</p>

<p>We would need something that, for instance, at 03.00 h would trigger an 'element' that checks the flags, if the flags are not up don't launch the pipeline. Past, lets say, 10 minutes, check again the flags, and be like this for at most X times OR until the flags are up.</p>

<p>If the flags are up, launch the pipeline execution and stop trying to launch the pipeline any further.</p>

<p>How would you do it?</p>

<p>The logic per se is not complicated in any way, but I wouldn't know where to implement it. Should I develop an Azure Funtions that launches the Pipeline or is there a way to achieve it with an out-of-the-box AZDF activity?</p>

<p><a href=""https://i.stack.imgur.com/jgoBO.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jgoBO.jpg"" alt=""Graphic description""></a></p>
","<azure><azure-functions><azure-data-factory>","2019-07-01 08:50:23","1251","0","1","56834620","<p>There is a UNTIL iteration activity where you can check if your clause. 
Example: </p>

<ul>
<li>Your azure function (AF) checking the flag and returns 0 or 1.</li>
<li>Build ADF pipeline with UNTIL activity where you check the output of AF (if its 1 do something). In UNTIL activity you can have your process step. For example, you have a variable flag that will before until activity is 0. In your until you check if it's 1. if it is do your processing step, if its not, put WAIT activity on 10 min or so. </li>
</ul>

<p>So you have the ability in ADF to iterate until something it's not satisfied. 
Hope that this will help you :)</p>
"
"56819603","How do I extract a string between two characters using ADFs expression builder?","<p>I'm trying to extract part of a file name using expressions in ADF expression builder. The part I'm trying to extract is dynamic in size but always appears between ""_"" and ""-"".</p>

<p>How can I go about doing this extraction?</p>

<p>Thanks!</p>
","<azure><expression><azure-data-factory>","2019-06-29 17:41:55","4228","0","1","56829444","<p>Suppose there's a pipeline parameter named filename, you could use the below expression to extract value between '_' and '-', e.g. input 'ab_cd-', you would get 'cd' as output:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>@{substring(pipeline().parameters.fileName, add(indexOf(pipeline().parameters.fileName, '_'),1),sub(indexOf(pipeline().parameters.fileName, '-'),3))}</code></pre>
</div>
</div>
</p>

<p>You may want to check the documentation of Expressions and functions in Azure Data Factory for more details: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#string-functions"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#string-functions</a></p>
"
"56814174","Get files list after azure data factory copy activity","<p>Is there a method that gives me the list of files copied in azure data lake storage after a copy activity in azure data factory? I have to copy data from a datasource and after i have to skip files based on a particular condition. Condition must check also file path and name with other data from sql database. any idea?</p>
","<azure><copy><azure-data-factory><azure-data-lake>","2019-06-29 00:02:46","2703","0","1","56830855","<p>As of now, there's no function to get the files list after a copy activity. You can however use a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">get Metadata</a> activity or a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">Lookup Activity</a> and chain a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-filter-activity"" rel=""nofollow noreferrer"">Filter activity</a> to it to get the list of files based on your condition.</p>

<p>There's a workaround that you can check out <a href=""http://The%20solution%20was%20actually%20quite%20simple%20in%20this%20case.%20I%20just%20created%20another%20pipeline%20in%20Azure%20Data%20Factory,%20which%20was%20triggered%20by%20a%20Blob%20Created%20event,%20and%20the%20folder%20and%20filename%20passed%20as%20parameters%20to%20my%20notebook.%20Seems%20to%20work%20well,%20and%20a%20minimal%20amount%20of%20configuration%20or%20code%20required.%20Basic%20filtering%20can%20be%20done%20with%20the%20event,%20and%20the%20rest%20is%20up%20to%20the%20notebook.%20%20For%20anyone%20else%20stumbling%20across%20this%20scenario,%20details%20below:%20%20https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger"" rel=""nofollow noreferrer"">here</a>. </p>

<p>""The solution was actually quite simple in this case. I just created another pipeline in Azure Data Factory, which was triggered by a Blob Created event, and the folder and filename passed as parameters to my notebook. Seems to work well, and a minimal amount of configuration or code required. Basic filtering can be done with the event, and the rest is up to the notebook.</p>

<p>For anyone else stumbling across this scenario, details below:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger</a>""</p>
"
"56804751","Can Azure Data factory publish data to Rest API","<p>We need the ADF to publish the data to a 3rd Party application exposing its REST API, can it be done via ADF or need we have a custom .Net code to do the data posting.</p>

<p>From the documentation it seems it can only retrieve the data <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-rest</a></p>
","<azure><azure-data-factory>","2019-06-28 09:55:07","3226","0","2","56806074","<p>This table shows which connectors are supported as source and sink. None of the web-based connectors is currently supported as sink. So no, you cannot do this with the built-in copy activity.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats</a></p>
"
"56804751","Can Azure Data factory publish data to Rest API","<p>We need the ADF to publish the data to a 3rd Party application exposing its REST API, can it be done via ADF or need we have a custom .Net code to do the data posting.</p>

<p>From the documentation it seems it can only retrieve the data <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-rest</a></p>
","<azure><azure-data-factory>","2019-06-28 09:55:07","3226","0","2","56830904","<p>You can publish data to a REST API from within ADF by using a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">web activity</a> (recommended) or using a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">custom activity</a> (using .NET code).</p>

<p>If you want to publish the output of an activity, you can chain the web activity to the activity you want to send the result of. You can then in your web activity reference the output of the previous activity using a dynamic expression similar to this :</p>

<pre><code>@string(activity('CopyFromBlobToBlob').output)
</code></pre>

<p>Hope this helps.</p>
"
"56799413","specifying non default YARN queue in Azure Data Factory v1 activity","<p>We have a ADFv1 pipeline with an HDInsightHive type activity that submits hive script to a Hadoop HDIndight cluster. Looking at the JSON for the pipeline, there doesn't seem to be any way to specify a YARN queue that the job should be submitted to. </p>

<p>So it's assuming that the job is always to be submitted to the default queue. I didn't find anything in ADFv1 documentation yet to specify queue name (assuming we actually create more YARN queues on the cluster using capacity scheduler). </p>

<p>Can someone provide sample JSON for specifying a YARN queue in an activity if it is possible at all? Also, my requirement is specifically for ADFv1, I would also like to know if this is a limitation of ADFv1, is it fixed in ADFv2?</p>
","<azure><azure-data-factory><azure-hdinsight>","2019-06-27 23:19:55","128","0","1","56848118","<p>Currently, Azure Data Factory doesn't support submitting an activity to a specific queue.</p>

<p>Azure Data Factory activity always submitted to the default queue.</p>

<p>I would suggest you vote up an idea submitted by another Azure customer.</p>

<p><a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/32956186-hdinsightspark-activity-should-support-additional"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory/suggestions/32956186-hdinsightspark-activity-should-support-additional</a></p>

<p>All of the feedback you share in these forums will be monitored and reviewed by the Microsoft engineering teams responsible for building Azure.</p>
"
"56797425","How to trigger pipeline by Azure create run pipelines API?","<p>I have an Azure data factory subscription, I want to trigger pipeline through my node.js application. The pipeline transfers data from azure cosmos to Azure blob storage. I have triggered the pipeline once manually. And I was trying to do the same with a node.js app, but it throws the following error:</p>

<pre><code>{ Error: tunneling socket could not be established, cause=connect ETIMEDOUT 10.0.0.2:8080
    at ClientRequest.onError (/home/deepti/Desktop/dbmanager/node_modules/tunnel-agent/index.js:177:17)
    at Object.onceWrapper (events.js:286:20)
    at ClientRequest.emit (events.js:198:13)
    at Socket.socketErrorListener (_http_client.js:392:9)
    at Socket.emit (events.js:198:13)
    at emitErrorNT (internal/streams/destroy.js:91:8)
    at emitErrorAndCloseNT (internal/streams/destroy.js:59:3)
    at process._tickCallback (internal/process/next_tick.js:63:19) code: 'ECONNRESET' }
</code></pre>

<p>And here's my code in node.js app :</p>

<pre><code>Request.post(""POST https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/pipelines/{pipelineName}/createRun?api-version=2018-06-01"",(err,res,body)=&gt;{
        if(err) {
            return console.log(err);
        }
        console.dir(JSON.parse(body));
        });
</code></pre>

<p>Can anyone please guide me how do I rectify this error and proceed further. Do I need to generate some authorization token too?</p>

<p>Also, I actually want to back up data that is 1 month old from cosmos DB to Azure Blob, is that possible?</p>
","<node.js><azure><azure-data-factory>","2019-06-27 19:18:49","744","1","1","56802252","<blockquote>
  <p>1.Can anyone please guide me how do I rectify this error and proceed further. Do I need to generate some authorization token too?</p>
</blockquote>

<p>Yes,before process of creating pipeline run , you need to generate Authorization token in Headers. You could refer to <a href=""https://github.com/AzureAD/azure-activedirectory-library-for-nodejs#server-to-server-via-client-credentials"" rel=""nofollow noreferrer"">ADAL nodejs sdk</a> to generate token. If you don't know what are these parameters:</p>

<p><a href=""https://i.stack.imgur.com/xJIdH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xJIdH.png"" alt=""enter image description here""></a></p>

<p>,then please refer to this <a href=""https://learn.microsoft.com/en-us/azure/active-directory/develop/v1-oauth2-client-creds-grant-flow"" rel=""nofollow noreferrer"">tutorial</a> and this <a href=""https://arjunphp.com/how-to-execute-azure-rest-apis-node-js/"" rel=""nofollow noreferrer"">blog</a> to get concepts of them.
And please note that you also need to grant ADFpermission to your ad app. </p>

<p><a href=""https://i.stack.imgur.com/1elie.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1elie.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/qfsAf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qfsAf.png"" alt=""enter image description here""></a></p>

<p>Finally,you could call the <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/createrun"" rel=""nofollow noreferrer"">Pipelines - Create Run</a> REST api and provide auth token in the <code>header</code>.</p>

<blockquote>
  <p>2.Also, I actually want to back up data that is 1 month old from cosmos DB to Azure Blob, is that possible?</p>
</blockquote>

<p>If you want to back up azure blob storage data,please refer to this <a href=""https://samcogan.com/backup-for-azure-blob-storage/"" rel=""nofollow noreferrer"">link</a>.You could use AzCopy in the schedule to back up the data during the stage.</p>
"
"56782516","how to Set File Expiry on file Azure Datalake gen1 file at creation time?","<p>Using Azure Data Analytics to run U-SQL queries to write csvs in Azure Data Factory, how can I get an expiry value set on the files?</p>

<p>I have a Azure Service Fabric app with a service that writes json files to azure data lakes.  In this write process using the SDK, I'm able to apply an expiry value to those json files. I then go on to process those files further using ADLA with U-SQL in ADF.  From what I read in older posts, U-SQL doesn't support setting the expire time. However is it possible to set the expire time through other activities in the Factory? or am I going to need my service to monitor and update expiry values?</p>
","<azure><azure-data-lake><u-sql><azure-data-factory>","2019-06-27 00:35:03","623","0","1","56832954","<p>Unfortunately, Azure Data Lake Gen1 don’t have ability to set expiration of files when the file is created and U-SQL doesn't support setting the expire time.</p>

<p>I would suggest you to vote up an idea submitted by another Azure customer.</p>

<p><a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/20308555-setting-expiration-in-azure-data-lake-store"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory/suggestions/20308555-setting-expiration-in-azure-data-lake-store</a></p>

<p><a href=""https://feedback.azure.com/forums/327234-data-lake/suggestions/17579557-support-expiration-of-files-in-adls"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/327234-data-lake/suggestions/17579557-support-expiration-of-files-in-adls</a></p>

<p>All of the feedback you share in these forums will be monitored and reviewed by the Microsoft engineering teams responsible for building Azure.</p>
"
"56782351","ADF SQL Query with single quotes around dates","<p>I'm trying to do a simple pull from oracle based on a date. Eventually, I need this query to be dynamic hence using @concat() function in ADF. For simplicity sake I want to run the below oracle query in ADF with concat.</p>

<p>Since oracle requires single quotes I'm trying to escape it by using two single quotes. </p>

<p>oracle query:</p>

<pre class=""lang-sql prettyprint-override""><code>select * from schema.customer where updated &gt; to_date('06/25/2019','MM/DD/YYYY')
</code></pre>

<p>here's my ADF text:</p>

<blockquote>
  <p>@concat('select * from iris.iris_customer where updated >
  to_date(','''06/25/2019''','''MM/DD/YYYY''',');')</p>
</blockquote>

<p>I'm using to_date because I was getting the 'not a valid month' error and thought I could resolve it. Here's the error I'm getting:</p>

<blockquote>
  <p>{
      ""errorCode"": ""2200"",
      ""message"": ""Failure happened on 'Source' side. ErrorCode=UserErrorUnclassifiedError,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Odbc
  Operation
  Failed.,Source=Microsoft.DataTransfer.ClientLibrary.Odbc.OdbcConnector,''Type=System.Data.Odbc.OdbcException,Message=ERROR
  [22008] [Microsoft][ODBC Oracle Wire Protocol
  driver][Oracle]ORA-01843: not a valid month,Source=msora28.dll,'"",
      ""failureType"": ""UserError"",
      ""target"": ""copy_to_adl"" }</p>
</blockquote>
","<oracle><azure><escaping><concatenation><azure-data-factory>","2019-06-27 00:04:52","3196","1","2","56787540","<p>you should use <code>''</code> for escaping single quote. </p>

<p>My Example: <code>@{concat('DELETE FROM MyTable WHERE [ColumnDate] LIKE','''',pipeline().parameters.processYear,pipeline().parameters.processMonth,'%','''')}</code></p>
"
"56782351","ADF SQL Query with single quotes around dates","<p>I'm trying to do a simple pull from oracle based on a date. Eventually, I need this query to be dynamic hence using @concat() function in ADF. For simplicity sake I want to run the below oracle query in ADF with concat.</p>

<p>Since oracle requires single quotes I'm trying to escape it by using two single quotes. </p>

<p>oracle query:</p>

<pre class=""lang-sql prettyprint-override""><code>select * from schema.customer where updated &gt; to_date('06/25/2019','MM/DD/YYYY')
</code></pre>

<p>here's my ADF text:</p>

<blockquote>
  <p>@concat('select * from iris.iris_customer where updated >
  to_date(','''06/25/2019''','''MM/DD/YYYY''',');')</p>
</blockquote>

<p>I'm using to_date because I was getting the 'not a valid month' error and thought I could resolve it. Here's the error I'm getting:</p>

<blockquote>
  <p>{
      ""errorCode"": ""2200"",
      ""message"": ""Failure happened on 'Source' side. ErrorCode=UserErrorUnclassifiedError,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Odbc
  Operation
  Failed.,Source=Microsoft.DataTransfer.ClientLibrary.Odbc.OdbcConnector,''Type=System.Data.Odbc.OdbcException,Message=ERROR
  [22008] [Microsoft][ODBC Oracle Wire Protocol
  driver][Oracle]ORA-01843: not a valid month,Source=msora28.dll,'"",
      ""failureType"": ""UserError"",
      ""target"": ""copy_to_adl"" }</p>
</blockquote>
","<oracle><azure><escaping><concatenation><azure-data-factory>","2019-06-27 00:04:52","3196","1","2","70788148","<p>You can also use this SQL Syntax inside Expression builder in ADF Data Flows to escape single quotes:</p>
<p>&quot;SELECT * FROM myTable WHERE Id = {$myVar} AND Gender = 'M'&quot;</p>
"
"56781684","Deployment scenario of git integrated Azure Data Factory via arm template","<p>What happens if you have multiple features being tested in test environment of a ADF V2 test data factory and only one or few of them is ready for production deployment. How do we hande this type of deployment scenario in Microsoft recommended CICD model of git/vsts integrated adf v2 through arm template</p>

<p>Consider we have dev test and prod environment of ADF v2. The dev environment is git integrated. The developers have debuged their changes and merged with collaboration​ branch after pull request. The changes are published and deployed to test environment first. Here many features are getting tested but few are ready for prod and few are not, how do we move the ones which are ready since tge arm template takes the entire factory?</p>
","<deployment><azure-resource-manager><azure-data-factory><azure-rm-template>","2019-06-26 22:26:48","413","0","1","56784274","<p>this is somewhat of a strange question. you can apply same logic to anything, how do you create a feature for an application since application is only deployed as a single entity. answer would be: use git flow or something akin to that. Use feature branches and promotions.</p>
"
"56777280","SSIS execution with connection to blob storage by AccountKey","<p>From this afternoon, we face some issues on several environments to execute SSIS managed by Azure Data Factory.</p>

<p>The SSIS package needs access to blob storage, done so by using a connection string with <code>AccountKey</code>.</p>

<p>We get now a systemic error:</p>

<pre><code>{
  ""SsisDBOperationGuid"": ""XXXXXXXXXX"",
  ""SsisDBOperationId"": XX,
  ""OperationErrorMessages"": ""6/26/2019 4:08:08 PM +00:00 : LVL0_XXX_Main:Error: Property \""AccountKey\"" is not specified.\n6/26/2019 4:08:08 PM +00:00 : LVL0_XXX_Main:Error: Connection \""CN_AzureBlobStorage_MasterData\"" failed validation.\r\n\n"",
  ""effectiveIntegrationRuntime"": ""XXXX-MD-SSIS-IntegrationRuntime (West Europe)"",
  ""executionDuration"": 21558
}
</code></pre>

<p>Any idea about it ?</p>

<p>Thank you</p>
","<azure><azure-data-factory>","2019-06-26 16:24:36","520","0","1","56787077","<p>we have contacted microsoft, yesterday they did a security update that cause that issue. A few minutes ago, they did a rollout, so all goes fine now.</p>
"
"56775548","Add Dynamic Content - Azure Data Factory ADF V2","<p>I need to add a Dynamic Content in an ADF. In such a way that it needs to read the  files from the folder with name as  ‘StartDateOfMonth-EndDateOfMonth’ as below format.</p>

<p>Result: 20190601-20190630</p>
","<azure><expression><azure-data-factory>","2019-06-26 14:48:00","817","0","1","56781733","<p>Here are a few steps for how you can achieve that:</p>

<ol>
<li>in DataSet:

<ul>
<li>create parameter ""Date""</li>
<li>set up a connection to one selected file</li>
<li>now, replace ""File"" field with expression, similar to the following:</li>
</ul></li>
</ol>

<p>@concat('filedata_',dataset().Date,'.csv')</p>

<ol start=""2"">
<li>in Pipeline:

<ul>
<li>when using above DataSet, you just must pass the value, which you can set up by 'Set Variable'</li>
</ul></li>
</ol>
"
"56773769","Is there any automated way to convert the CSV files in Azure Blob Storage to Excel tables?","<p>I am trying to export the content of csv files stored in Azure Blob Storage to Excel tables in an automated way. I did some research and found in few blog articles that Azure Logic App could be used for the conversion. I tried to implement something similar but couldn't succeed.</p>

<p>Any suggestions/help would be appreciated.</p>

<p>Thanks.</p>
","<excel><azure><csv><azure-logic-apps><azure-data-factory>","2019-06-26 13:17:51","2187","0","1","56775901","<p>If you really want to go this route <a href=""https://stackoverflow.com/questions/56605095/how-to-convert-csv-to-json-using-template-via-azure-logic-app"">I built this the other day</a>, not that I think this is the best way to handle this but it is a way. You can build further upon this example, change the input to storage blob and the output to excel. I am just pasting the extra step where I set the output to Excel add a row into a table. <strong>Keep in mind you will need to purge the header and the last row</strong> So you need to at least fix that part.</p>

<p><a href=""https://i.stack.imgur.com/UhI2w.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UhI2w.png"" alt=""Excel table""></a></p>

<p>Find the entire flow in the other question I linked to earlier. The difference is just that I now look in a storage blob, compose the output and in the end, I write to the Excel table
<a href=""https://i.stack.imgur.com/FD1Zn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FD1Zn.png"" alt=""Flow""></a></p>
"
"56767724","Copy data from multiple csv files into one csv file","<p>I have multiple csv files in my azure blob storage which I wish to append into one csv file also stored in azure blob storage using the azure data factory pipeline. The problem is that all the columns of the source files are not present in the sink file and vice versa and also all the source files are not identical. I just want to map the columns I need from source files to the columns in sink file. The copy activity in the data factory is not allowing me to do so.</p>
","<azure><azure-blob-storage><azure-data-factory>","2019-06-26 07:44:14","1759","-1","2","56800261","<p>According my experience, if your csv files don't have the same format or columns with sink file, you can not merge or append to one csv file.  During the Mapping step, we can not import the schema and mapping the columns you need.</p>

<p>Azure Data Factory doesn't support to do that.</p>
"
"56767724","Copy data from multiple csv files into one csv file","<p>I have multiple csv files in my azure blob storage which I wish to append into one csv file also stored in azure blob storage using the azure data factory pipeline. The problem is that all the columns of the source files are not present in the sink file and vice versa and also all the source files are not identical. I just want to map the columns I need from source files to the columns in sink file. The copy activity in the data factory is not allowing me to do so.</p>
","<azure><azure-blob-storage><azure-data-factory>","2019-06-26 07:44:14","1759","-1","2","56801790","<p>As @LeonYue said, it doesn't support on Azure Data Factory now. However, per my experience, as a workaround solution, you can consider to create a Python script using <code>pandas</code> to do that and run as WebJob of Azure App Service or on Azure VM for acceleration between Azure Storage and other Azure services.</p>

<p>The steps of the workaround solution is like below.</p>

<ol>
<li><p>Maybe these csv files are all in a container of Azure Blob Storage, so you need to list them in container via <a href=""https://azure-storage.readthedocs.io/ref/azure.storage.blob.baseblobservice.html#azure.storage.blob.baseblobservice.BaseBlobService.list_blob_names"" rel=""nofollow noreferrer""><code>list_blob_names</code></a> and generate their urls with sas token for pandas <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html"" rel=""nofollow noreferrer"">read_csv</a> function, the code as below.</p>

<pre><code>from azure.storage.blob.baseblobservice import BaseBlobService
from azure.storage.blob import ContainerPermissions
from datetime import datetime, timedelta

account_name = '&lt;your account name&gt;'
account_key = '&lt;your account key&gt;'
container_name = '&lt;your container name&gt;'

service = BaseBlobService(account_name=account_name, account_key=account_key)
token = service.generate_container_shared_access_signature(container_name, permission=ContainerPermissions.READ, expiry=datetime.utcnow() + timedelta(hours=1),)

blob_names = service.list_blob_names(container_name)
blob_urls_with_token = (f""https://{account_name}.blob.core.windows.net/{container_name}/{blob_name}?{token}"" for blob_name in blob_names)

#print(list(blob_urls_with_token))
</code></pre></li>
<li><p>To directly read csv file by <code>read_csv</code> function to get a pandas dataframe.</p>

<pre><code>import pandas as pd

for blob_url_with_token in blob_urls_with_token:
    df = pd.read_csv(blob_url_with_token)
</code></pre></li>
<li><p>You can follow your want to operate these dataframe by pandas, and then write to Azure Blob Storage as a single csv file by using Azure Storage SDK for Python.</p></li>
</ol>

<p>Hope it helps.</p>
"
"56757785","transform data in azure data factory using python data bricks","<p>I have the task to transform and consolidate millions of single JSON file into BIG CSV files.</p>

<p>The operation would be very simple using a copy activity and mapping the schemas, I have already tested, the problem is that a massive amount of files have bad JSON format.</p>

<p>I know what is the error and the fix is very simple too, I figured that I could use a Python Data brick activity to fix the string and then pass the output to a copy activity that could consolidate the records into a big CSV file. </p>

<p>I have something in mind like this, I'm not sure if this is the proper way to address this task. I don't know to use the output of the Copy Activy in the Data Brick activity
<a href=""https://i.stack.imgur.com/fFhVk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fFhVk.png"" alt=""enter image description here""></a></p>
","<python><azure><azure-data-factory>","2019-06-25 15:47:59","892","0","2","56798029","<p>Copy JSON file to storage (e.g. BLOB) and you can get access to the storage from Databricks. Then you can fix the file using Python and even transform to the required format having cluster run.</p>

<p>So, in Copy Data activity do the copy of the files to BLOB if you haven't them there yet.</p>
"
"56757785","transform data in azure data factory using python data bricks","<p>I have the task to transform and consolidate millions of single JSON file into BIG CSV files.</p>

<p>The operation would be very simple using a copy activity and mapping the schemas, I have already tested, the problem is that a massive amount of files have bad JSON format.</p>

<p>I know what is the error and the fix is very simple too, I figured that I could use a Python Data brick activity to fix the string and then pass the output to a copy activity that could consolidate the records into a big CSV file. </p>

<p>I have something in mind like this, I'm not sure if this is the proper way to address this task. I don't know to use the output of the Copy Activy in the Data Brick activity
<a href=""https://i.stack.imgur.com/fFhVk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fFhVk.png"" alt=""enter image description here""></a></p>
","<python><azure><azure-data-factory>","2019-06-25 15:47:59","892","0","2","56801586","<p>It sounds like you want to transform a large number of single JSON file using Azure Data Factory, but it does not support on Azure now as @KamilNowinski said. However, now that you were using Azure Databricks, to write a simple Python script to do the same thing is easier for you. So a workaound solution is to directly use Azure Storage SDK and <code>pandas</code> Python package to do that via few steps on Azure Databricks.</p>

<ol>
<li><p>Maybe these JSON files are all in a container of Azure Blob Storage, so you need to list them in container via <a href=""https://azure-storage.readthedocs.io/ref/azure.storage.blob.baseblobservice.html#azure.storage.blob.baseblobservice.BaseBlobService.list_blob_names"" rel=""nofollow noreferrer""><code>list_blob_names</code></a> and generate their urls with sas token for pandas <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html"" rel=""nofollow noreferrer""><code>read_json</code></a> function, the code as below.</p>

<pre><code>from azure.storage.blob.baseblobservice import BaseBlobService
from azure.storage.blob import ContainerPermissions
from datetime import datetime, timedelta

account_name = '&lt;your account name&gt;'
account_key = '&lt;your account key&gt;'
container_name = '&lt;your container name&gt;'

service = BaseBlobService(account_name=account_name, account_key=account_key)
token = service.generate_container_shared_access_signature(container_name, permission=ContainerPermissions.READ, expiry=datetime.utcnow() + timedelta(hours=1),)

blob_names = service.list_blob_names(container_name)
blob_urls_with_token = (f""https://{account_name}.blob.core.windows.net/{container_name}/{blob_name}?{token}"" for blob_name in blob_names)

#print(list(blob_urls_with_token))
</code></pre></li>
<li><p>Then, you can read these JSON file directly from blobs via <code>read_json</code> function to create their pandas Dataframe.</p>

<pre><code>import pandas as pd

for blob_url_with_token in blob_urls_with_token:
    df = pd.read_json(blob_url_with_token)
</code></pre>

<p>Even if you want to merge them to a big CSV file, you can first merge them to a big Dataframe via pandas functions listed in <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/frame.html#combining-joining-merging"" rel=""nofollow noreferrer""><code>Combining / joining / merging</code></a> like <code>append</code>.</p></li>
<li><p>To write a dataframe to a csv file, I think it's very easy by <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html"" rel=""nofollow noreferrer""><code>to_csv</code></a> function. Or you can convert a pandas dataframe to a PySpark dataframe on Azure Databricks, as the code below.</p>

<pre><code>from pyspark.sql import SQLContext
from pyspark import SparkContext

sc = SparkContext()
sqlContest = SQLContext(sc)
spark_df = sqlContest.createDataFrame(df)
</code></pre></li>
</ol>

<p>So next, whatever you want to do, it's simple. And if you want to schedule the script as notebook in Azure Databricks, you can refer to the offical document <a href=""https://docs.azuredatabricks.net/user-guide/jobs.html"" rel=""nofollow noreferrer""><code>Jobs</code></a> to run Spark jobs.</p>

<p>Hope it helps.</p>
"
"56751473","How to read the files from Azure Blob Storage with folder structure as 'StartDateOfMonth-EndDatefMonth'?","<p>Scenario </p>

<p>We have azure blob storage container with following folder structure. 
•   20190601-20190630</p>

<p>Basically, this folder will contain daily CSV files for the given month. </p>

<p>This folder structure is dynamic. So, in the next month, folder 20190701-20190731 will be populated with daily CSV files.</p>

<p>Problem</p>

<p>On daily basis, need to move these files from azure blob storage to azure data lake using azure data factory (v2). </p>

<p>How to specify folder structure (dynamically) in the Input Dataset (Azure Blob Storage) in Azure Data Factory(V2)? </p>

<p>Example: 
20190601-20190630/*.CSV    for the month June 2019 </p>

<p>Basically, StartDateOfMonth and EndDateOfMonth are dynamic.</p>

<p>Thanks in Advance</p>
","<azure-data-factory>","2019-06-25 10:02:15","281","1","1","56800142","<p>You could configure your dataset folder path like:</p>

<pre><code>   ""folderPath"": {
                        ""value"": ""@concat( 
                               formatDateTime(pipeline().parameters.scheduledRunTimeStart, 'yyyyMMdd'), 
                               '-',
                               formatDateTime(pipeline().parameters.scheduledRunTimeEnd, 'yyyyMMdd')
                               , '/'
                        ""type"": ""Expression""
    }
</code></pre>

<p>And pass the parameters into dataset:</p>

<pre><code>""parameters"": {
    ""scheduledRunTimeStart"": {
        ""type"": ""String""
    },
    ""scheduledRunTimeEnd"": {
        ""type"": ""String""
    }
}
</code></pre>
"
"56750276","speed up copy task in azure Data factory","<p>I have a Copy job should copy 100 GB of excel files between two Azure DataLake.</p>

<pre><code> ""properties"": {
        ""activities"": [
            {
                ""name"": ""Copy Data1"",
                ""type"": ""Copy"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""AzureDataLakeStoreSource"",
                        ""recursive"": true,
                        ""maxConcurrentConnections"": 256
                    },
                    ""sink"": {
                        ""type"": ""AzureDataLakeStoreSink"",
                        ""maxConcurrentConnections"": 256
                    },
                    ""enableStaging"": false,
                    ""parallelCopies"": 32,
                    ""dataIntegrationUnits"": 256
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""SourceLake"",
                        ""type"": ""DatasetReference""
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""DestLake"",
                        ""type"": ""DatasetReference""
                    }
                ]
            }
        ],
</code></pre>

<p>my throughput is about 4 MB/s. As I read <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance#performance-reference"" rel=""nofollow noreferrer"">here</a> it should be 56 MB/s. What should I do to reach this throughput?</p>
","<azure><azure-data-factory>","2019-06-25 08:56:28","514","0","1","56786026","<p>You can use the Copy actives <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance#performance-tuning-steps"" rel=""nofollow noreferrer"">Performance tuning</a> to help you tune the performance of your Azure Data Factory service with the copy activity.</p>

<p><strong>Summary</strong>:</p>

<p>Take these steps to tune the performance of your Azure Data Factory service with the copy activity.</p>

<ol>
<li><p>Establish a baseline. During the development phase, test your pipeline by using the copy activity against a representative data sample. Collect execution details and performance characteristics following copy activity monitoring.</p></li>
<li><p>Diagnose and optimize performance. If the performance you observe doesn't meet your expectations, identify performance bottlenecks. Then, optimize performance to remove or reduce the effect of bottlenecks.</p></li>
</ol>

<p>In some cases, when you run a copy activity in Azure Data Factory, you see a ""Performance tuning tips"" message on top of the copy activity monitoring page, as shown in the following example. The message tells you the bottleneck that was identified for the given copy run. It also guides you on what to change to boost copy throughput. </p>

<p>Your file is about 100 GB size. But test files for file-based stores are multiple files with 10 GB in size. The performance may be different.</p>

<p>Hope this helps.</p>
"
"56747581","Capture HTTP 404 in Azure Data Factory","<p>How do we capture HTTP 404 and not fail an ADF pipeline</p>

<p>The requirement is to check the HTTP response. If the HTTP response is 404, by default the Copy Activity fails with an error. But, we do not want to fail the pipeline. Instead, we want to check the response and stop further processing.</p>

<p>We have tried using Lookup activity, but the Lookup activity also fails, if we have a HTTP 404 response. We can write a Custom API and check the response in there, but, we do not want to do that.</p>

<p>Is there any native ADF way to accomplish this.</p>
","<azure><azure-data-factory>","2019-06-25 05:48:35","944","1","1","56748611","<p>1.You could get idea of <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-validation-activity"" rel=""nofollow noreferrer"">Validation activity</a>:</p>

<p>Activity will block execution until it has validated this dataset reference exists and that it meets the specified criteria, or timeout has been reached.</p>

<p>2.Also you could try to use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-pipeline-activity"" rel=""nofollow noreferrer"">Execute Pipeline activity</a> to execute your specific pipeline and get the output of activity.(<a href=""https://stackoverflow.com/questions/51625549/error-handling-on-azure-data-factory-v2"">Error Handling on Azure Data Factory V2</a>)</p>

<p>BTW,you could vote up this <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/34301983-throw-error-activity"" rel=""nofollow noreferrer"">feedback case</a> to push the progress about Throw Error Activity in ADF.</p>
"
"56738689","Error While deploying Data Factory ARM template","<p>Have been deploying ADFv2 ARM template to another datafactory within same resource group ,and storage account also in same RG, While i deploy i get error for some linked services.</p>

<p>I have linked service to connect my computer and local db server using Selfhosted-IR ,Both are failing. 
I am passing Override parameters for linked services.</p>

<p>Any help here is much appreciated.</p>

<p>Thanks.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>""error"": {
    ""code"": ""BadRequest"",
    ""message"": ""Failed to encrypt`` sub-resource payload {\r\n  \""Id\"": \""/subscriptions/xxxxxxxxx/resourceGroups/rgvs/providers/Microsoft.DataFactory/factories/TestADF/linkedservices/ConnOnPremComputer\"",\r\n  \""Name\"": \""ConnOnPremComputer\"",\r\n  \""Properties\"": {\r\n    \""annotations\"": [],\r\n    \""type\"": \""FileServer\"",\r\n    \""typeProperties\"": {\r\n      \""host\"": \""****************\"",\r\n      \""userId\"": \""************\"",\r\n      \""password\"": \""*************\""\r\n    },\r\n    \""connectVia\"": {\r\n      \""referenceName\"": \""OnPremToAzure\"",\r\n      \""type\"": \""IntegrationRuntimeReference\""\r\n    }\r\n  }\r\n} and error is: Failed to encrypted linked service credentials on self-hosted IR 'OnPremToAzure', reason is: NotFound, error message is: No online instance.."",
    ""target"": ""/subscriptions/xxxxxxxxxx/resourceGroups/rgvs/providers/Microsoft.DataFactory/factories/TestADF/linkedservices/ConnOnPremComputer"",
    ""details"": null
  }
} undefined
2019-06-24T13:54:48.9695328Z ##[error]BadRequest: {
  ""error"": {
    ""code"": ""BadRequest"",
    ""message"": ""Failed to encrypt sub-resource payload {\r\n  \""Id\"": \""/subscriptions/xxxxxxxxx/resourceGroups/rgvs/providers/Microsoft.DataFactory/factories/balptestadf/linkedservices/ConnLocalHostBackOffice\"",\r\n  \""Name\"": \""ConnLocalHostBackOffice\"",\r\n  \""Properties\"": {\r\n    \""parameters\"": {\r\n      \""Password\"": {\r\n        \""type\"": \""String\"",\r\n        \""defaultValue\"": \""Password@1234\""\r\n      }\r\n    },\r\n    \""annotations\"": [],\r\n    \""type\"": \""SqlServer\"",\r\n    \""typeProperties\"": {\r\n      \""connectionString\"": \""***************\"",\r\n      \""userName\"": \""***************\"",\r\n      \""password\"": \""*****************\""\r\n    },\r\n    \""connectVia\"": {\r\n      \""referenceName\"": \""OnPremToAzure\"",\r\n      \""type\"": \""IntegrationRuntimeReference\""\r\n    }\r\n  }\r\n} and error is: Format of the initialization string does not conform to specification starting at index 0.."",
    ""target"": ""/subscriptions/xxxxxxxxxxxx/resourceGroups/rgvs/providers/Microsoft.DataFactory/factories/testADF/linkedservices/ConnLocalHost"",
    ""details"": null
  }
} undefined</code></pre>
</div>
</div>
</p>
","<azure><azure-data-factory>","2019-06-24 14:26:39","2120","0","1","56832670","<p>There are many reason for this error message:</p>

<p>From the error message ""<strong>Failed to encrypted linked service credentials</strong> on self-hosted IR 'OnPremToAzure', reason is: NotFound, error message is: <strong>No online instance</strong>""</p>

<p>Make sure self-hosted IR is online, when you are deploying. Otherwise you may get this error message because self-hosted IR will be used to encrypt your payload.</p>

<p>To encrypt the sensitive data from the JSON payload on an on-premises self-hosted integration runtime, run New-AzDataFactoryV2LinkedServiceEncryptedCredential, and pass on the JSON payload. This cmdlet ensures the credentials are encrypted using DPAPI and stored on the self-hosted integration runtime node locally. The output payload containing the encrypted reference to the credential can be redirected to another JSON file (in this case 'encryptedLinkedService.json').</p>

<pre><code>New-AzDataFactoryV2LinkedServiceEncryptedCredential -DataFactoryName $dataFactoryName -ResourceGroupName $ResourceGroupName -Name ""SqlServerLinkedService"" -DefinitionFile "".\SQLServerLinkedService.json"" &gt; encryptedSQLServerLinkedService.json
</code></pre>

<p>For more details, refer ""<a href=""https://learn.microsoft.com/en-us/azure/data-factory/encrypt-credentials-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">Encrypt credentials for on-premise data stores in ADF</a>"".</p>

<p>Hope this helps.</p>
"
"56735848","How to find the latest file in folder using azure data factory v2 (adf)","<p>I am trying to read the latest blob file(csv) using azure data factory v2. The file name also contains date(YYYY-MM-DD mm:ss-abcd.csv). I need to read the data from the latest file present and load into table storage. Could you please help me with how to read the latest file using ADF</p>
","<azure><azure-data-factory>","2019-06-24 11:44:45","3880","0","1","56743750","<p>Hello Faiz Rahman and thank you for your question.  The date format you chose has the useful feature of lexicographic sorting matching chronological sorting.  This means, once you have a list of blobs, extracting the date and comparing is all that is needed.</p>

<p>If you have a very large list of blobs, this might not be practical.  In that case, whenever you write a new blob, keep track of it somewhere, say, ""maxBlobName.txt"", and have pipeline read that to get the name of the most recent file.</p>

<p>Here is some example code for comparing the date portion of your blob names.  To adapt for your purposes, you will need to use GetMetadata activity to fetch the blob names, and some string functions to extract only the date portion of the name for comparison.</p>

<pre><code>{
""name"": ""pipeline9"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""ForEach1"",
            ""type"": ""ForEach"",
            ""dependsOn"": [
                {
                    ""activity"": ""init array"",
                    ""dependencyConditions"": [
                        ""Succeeded""
                    ]
                }
            ],
            ""typeProperties"": {
                ""items"": {
                    ""value"": ""@variables('list')"",
                    ""type"": ""Expression""
                },
                ""isSequential"": true,
                ""activities"": [
                    {
                        ""name"": ""If Condition1"",
                        ""type"": ""IfCondition"",
                        ""typeProperties"": {
                            ""expression"": {
                                ""value"": ""@greater(item(),variables('max'))"",
                                ""type"": ""Expression""
                            },
                            ""ifTrueActivities"": [
                                {
                                    ""name"": ""write new max"",
                                    ""type"": ""SetVariable"",
                                    ""typeProperties"": {
                                        ""variableName"": ""max"",
                                        ""value"": {
                                            ""value"": ""@item()"",
                                            ""type"": ""Expression""
                                        }
                                    }
                                }
                            ]
                        }
                    }
                ]
            }
        },
        {
            ""name"": ""init array"",
            ""type"": ""SetVariable"",
            ""typeProperties"": {
                ""variableName"": ""list"",
                ""value"": {
                    ""value"": ""@split(pipeline().parameters.input,',')"",
                    ""type"": ""Expression""
                }
            }
        }
    ],
    ""parameters"": {
        ""input"": {
            ""type"": ""string"",
            ""defaultValue"": ""'2019-07-25','2018-06-13','2019'-06-24','2019-08-08','2019-06-23'""
        }
    },
    ""variables"": {
        ""max"": {
            ""type"": ""String"",
            ""defaultValue"": ""0001-01-01""
        },
        ""list"": {
            ""type"": ""Array""
        }
    }
}
</code></pre>

<p>}</p>
"
"56711868","Cannot set up code repository of Azure Devops for Azure Data Factory V2","<p>I am using my personal Microsoft email account. I have an Azure DevOps account and have set up the organization and have some projects. I have created an Azure Data Factory V2 project. Both the DevOps email account and the Azure Portal email account are the same email.</p>

<p>When I set up a code repository for the Azure Data Factory V2 project on Azure Portal I selected the following options. However, the DevOps account is not showing up.</p>

<ul>
<li>Repository Type: Azure DevOps Git Azure </li>
<li>Active Directory: Default</li>
<li>Directory Azure DevOps Account: empty on the drop-down list <strong><em>(this is the issue I'm having)</em></strong></li>
</ul>

<p>I followed MS documentation, went to the Azure Activity Directory, selected Enterprise applications and added this email address with the default role.</p>

<p>I expect to see my Azure DevOps account show up when I set up code repository on Azure Data Factory V2.</p>

<p>Could you please advise how to resolve this?</p>
","<azure-devops><azure-active-directory><azure-data-factory>","2019-06-22 01:13:28","701","1","1","57577482","<p>I faced the same issue. The solution is to associate your Azure DevOps organization to active directory. Navigate to Organization settings => in General section => Select Azure active directory . which allows you to associate organization. Hope it helps. </p>

<p>Thanks
Srini</p>
"
"56697841","how to get the pipeline name dynamically in ADF?","<p>I am trying to implement mail notification in pipeline in ADF.</p>

<p>So basically I want to send the a notification mail about the successful execution of pipeline in ADF.
I have created a demo pipeline where I m coping the data. this is working fine.
And I have created a SSIS pkg that will send email notification(also working fine.)</p>

<p>Now I want to have the pipeline name so that the notification will have pipeline name also dynamically.</p>
","<pipeline><azure-data-factory>","2019-06-21 06:17:06","2364","1","1","56720214","<p>You can use the following expression which uses system variables to get the current pipeline name:</p>
<pre><code>@pipeline().Pipeline
</code></pre>
<p>See <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables?"" rel=""nofollow noreferrer"">this link</a> for more system variables.</p>
"
"56692225","Azure Data Factory copy from Azure Block to CosmosDB is slow","<p>I have a BlockBlob in Premium Azure Storage.
It's a 500mb zip file containing around 280 million phone numbers in CSV format.</p>

<p>I've created a Pipeline in ADF to unzip this and copy the entries into Cosmos DB SQL API, but it took 40 hours to complete. The goal is to update the DB nightly with a diff in the information.</p>

<p>My Storage Account and Cosmos DB are located in the same region.
The Cosmos DB partition key is the area code and that seems to distribute well.</p>

<p>Currently, at 20,000 RU's I've scaled a few time, but the portal keeps telling me to scale more. They are suggesting 106,000 RU's which is $6K a month.
Any ideas on practical ways I can speed this up?</p>

<p><a href=""https://i.stack.imgur.com/P1yPq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P1yPq.png"" alt=""Azure Data Factory Pipeline Image""></a></p>

<p>-- Update.</p>

<p>I've tried importing the unzipped file, but it doesn't appear any faster. Slower in fact, despite reporting more peak connections.
<a href=""https://i.stack.imgur.com/QTq4m.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QTq4m.png"" alt=""Importing unzipped DB""></a></p>

<p>I'm now trying to dynamically scale up/down the RU's to a really high number when it's time to start the transfer. Still playing with numbers. Not sure the formula to determine the number of RUs I need to transfer this 10.5GB in X minutes. </p>
","<azure><azure-storage><azure-cosmosdb><azure-blob-storage><azure-data-factory>","2019-06-20 18:53:18","931","0","1","57108034","<p>I ended up dynamically scaling the throughput with Azure Functions. Yes, the price for Cosmos would have been very expensive if I left the RUs very high. However, I only need that high when I'm doing the data ingestion and then scale back down. I used a Logic App to call Azure Function to ""Scale the RUs up"" then kicks off my Azure Data Factory Pipeline. When it's down it calls the Azure Function to scale down.</p>
"
"56690430","How to use dynamic content in relative URL in Azure Data Factory","<p>I have a REST data source where I need pass in multiple parameters to build out a dataset in Azure Data Factory V2.</p>

<p>I have about 500 parameters that I need to pass in so don’t want to pass these individually. I can manually put these in a list (I don’t have to link to another data source to source these). The parameters would be something like [a123, d345, e678] </p>

<p>I'm working in the UI. I cannot figure out how to pass these into the relative URL (where it says Parameter) to then form the dataset. I could do this in Power BI using functions and parameters but can't figure it out in Azure Data Factory as I'm completely new to it. I'm using the Copy Data functionality in ADF to do this.</p>

<p>The sink would be a json file in an Azure blob that I can then access via Power BI. I'm fine with this part.</p>

<p><a href=""https://i.stack.imgur.com/RlJIc.png"" rel=""nofollow noreferrer"">Relative URL with Parameter requirement</a></p>

<p><a href=""https://i.stack.imgur.com/ps63Z.png"" rel=""nofollow noreferrer"">How to add dynamic content</a></p>
","<azure><parameters><azure-data-factory>","2019-06-20 16:36:05","5222","1","1","56732067","<p>I'm afraid that your requirement can't be implemented.As you know,<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest"" rel=""nofollow noreferrer"">ADF REST dataset</a> is used to retrieving data from a REST endpoint by using the GET or POST methods with Http request. No way to configure a list of parameters in the <code>relativeUrl</code> property which ADF would loop it automaticlly for you.</p>

<p>Two ways to retrieve your goal:</p>

<p>1.Loop your parameter array ,pass single item into <code>relativeUrl</code> to execute copy activity individually.Using this way,you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">foreach activity</a> in the ADF.</p>

<p>2.Write a overall api to accept list paramter from the <code>requestBody</code>,execute your business in the api inside with loop.</p>
"
"56685107","How to run Powershell from Azure DataFactory? .ps1 file","<p>I have powershell command which runs on Automation Runbook to add new user in Azure Analysis Database ,now i am trying to execute the same command through Custom Activity of Azure Data factory but not able to execute it.
My powershell script is saved in script.ps1 file which is place at storage account and i am using command powershell .\script.ps1 to execute it..Please suggest...</p>
","<powershell><azure-data-factory><azure-analysis-services>","2019-06-20 11:35:05","2177","-1","1","56700896","<p>To run a powershell script from ADF, you can use Custom activity in ADFv2. The custom activity can run the powershell script on an Azure Batch pool of virtual machines.</p>

<p>Custom activity doc: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity</a> </p>

<p>Also for your reference : <a href=""https://stackoverflow.com/questions/52721399/how-to-run-powershell-from-azure-data-factory"">How to run PowerShell from Azure Data Factory</a></p>

<p>Thanks!</p>
"
"56680955","Error calling the azure function from data factory pipeline trigger","<p>I am calling an http triggered Azure function app in data factory pipeline using ADF function activity. It is executing successfully in debug mode, but when I publish that pipeline and run the same code using data factory triggers I get below error-</p>

<pre><code>{
    ""errorCode"": ""3600"",
    ""message"": ""Object reference not set to an instance of an object."",
    ""failureType"": ""UserError"",
    ""target"": ""AzureFunction""
}
</code></pre>

<p>Please let me know if I need to make some additional properties changes or I am missing anything here. Also is there any way I can see what URL is getting generated when I call function app through function activity in ADF. </p>

<p>I have tried calling same function app using web activity in ADF and that is working fine in both debug and trigger mode.</p>

<p>Linked service code for Azure function</p>

<pre><code>{
    ""name"": ""linkedservicesAzureFunctions"",
    ""type"": ""Microsoft.DataFactory/factories/linkedservices"",
    ""properties"": {
        ""typeProperties"": {
            ""functionAppUrl"": ""https://xyz.azurewebsites.net"",
            ""functionKey"": {
                ""type"": ""AzureKeyVaultSecret"",
                ""store"": {
                    ""type"": ""LinkedServiceReference"",
                    ""referenceName"": ""linkedservicesKeyVault""
                },
                ""secretName"": ""functions-host-key-default""
            }
        },
        ""type"": ""AzureFunction""
    }
}
</code></pre>
","<azure><azure-functions><azure-data-factory><azure-triggers>","2019-06-20 07:21:55","1185","0","1","56831699","<p>There is a known Bug in Azure Data Factory and they are working on that. For now if you are creating the Azure Data Factory using .NET SDK then you'll need to set Headers like this in Azure Function Activity.</p>

<pre><code>new AzureFunctionActivity
                {
                    Name = ""CopyFromBlobToSnowFlake"",
                    LinkedServiceName = new LinkedServiceReference(pipelineStructure.AzureFunctionLinkedService),
                    Method = ""POST"",
                    Body = body,
                    FunctionName = ""LoadBlobsIntoSnowFlake"",
                    Headers = new Dictionary&lt;string, string&gt;{ },
                    DependsOn = new List&lt;ActivityDependency&gt;
                    {
                        new ActivityDependency{
                            Activity = ""CopyFromOPSqlServerToBlob"",
                            DependencyConditions= new List&lt;string&gt;{""Succeeded"" }
                        }
                    }
                }
</code></pre>

<p>If you are creating Azure Function Activity through UI then just update the description of Activity then Publish and Headers will automatically get Initialized.</p>
"
"56672959","How to trigger Azure Datafactory-v2 via rest api?","<p>I am new to Azure and Azure Data Factory. I have built a pipeline in Azure Data Factory and now I am trying to trigger the pipeline via rest API from javascript but I am not able to find any concrete solution. Can anybody help?</p>
","<azure><azure-data-factory>","2019-06-19 17:21:19","9641","3","1","56678075","<p>Based on your description, you already have created pipeline in your adf account.You could use Azure Data Factory <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/createrun"" rel=""nofollow noreferrer"">Pipelines - Create Run</a> REST api to execute it.</p>

<p>Before this process, you need to generate Authorization token in Headers. You could refer to <a href=""https://github.com/AzureAD/azure-activedirectory-library-for-nodejs#server-to-server-via-client-credentials"" rel=""nofollow noreferrer"">ADAL nodejs sdk</a> to generate token. If you don't know what are these parameters:</p>

<p><a href=""https://i.stack.imgur.com/xJIdH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xJIdH.png"" alt=""enter image description here""></a></p>

<p>,then please refer to this <a href=""https://learn.microsoft.com/en-us/azure/active-directory/develop/v1-oauth2-client-creds-grant-flow"" rel=""nofollow noreferrer"">tutorial</a> and this <a href=""https://arjunphp.com/how-to-execute-azure-rest-apis-node-js/"" rel=""nofollow noreferrer"">blog</a> to get concepts of them.
And please note that you also need to grant ADFpermission to your ad app. </p>

<p><a href=""https://i.stack.imgur.com/1elie.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1elie.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/qfsAf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qfsAf.png"" alt=""enter image description here""></a></p>

<p>Finally,you could call the <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/createrun"" rel=""nofollow noreferrer"">Pipelines - Create Run</a> REST api and provide auth token in the <code>header</code>.</p>
"
"56664701","shared condaenv for multiple users on Windows","<h2>Current Unexpected Behavior</h2>

<p>We have a computer with multiple (auto)users. (on Microsoft Azure).<br>
If we try to use <code>pip install</code> on a conda environment that was created by an other user, we get a ""Access is denied"" error.</p>

<h2>Steps to Reproduce</h2>

<p>We have installed anaconda as admin using following command:</p>

<pre><code>C:\Windows\Temp\Anaconda3.exe /InstallationType=AllUsers /AddToPath=1 /RegisterPython=1 /S
</code></pre>

<p>If I create a condaenv using one user, lets say<strong>_azbatchtask_20</strong>, we are able to activate it, install packages inside, and deactivate it:</p>

<pre><code>conda create -p D:\batch\tasks\shared\test-condaenv-users python=3.7 -y
conda activate D:\batch\tasks\shared\test-condaenv-users
pip install see
deactivate
</code></pre>

<p>However if we now login under <strong>_azbatchtask_21</strong>, and try install new packages in this same environment, we get weird error ""Acess is denied"":</p>

<pre><code>conda activate D:\batch\tasks\shared\test-condaenv-users
</code></pre>

<p>works, but:</p>

<pre><code>pip install nltk

&gt; Access is denied
</code></pre>

<p>In case useful, we checked permissions of the newly created condaenv folder using</p>

<pre><code>    icacls D:\batch\tasks\shared\test-condaenv-users

    NT AUTHORITY\SYSTEM:(I)(F)
    NT AUTHORITY\SYSTEM:(I)(OI)(CI)(IO)(F)
    ae0f56229000000\_azbatch:(I)(F)
    ae0f56229000000\_azbatch:(I)(OI)(CI)(IO)(F)
    BUILTIN\Administrators:(I)(F)
    BUILTIN\Administrators:(I)(OI)(CI)(IO)(F)
    ae0f56229000000\WATASK_COMMON_GROUP:(I)(F)
    ae0f56229000000\WATASK_COMMON_GROUP:(I)(OI)(CI)(IO)(F)
    NT AUTHORITY\Authenticated Users:(I)(F)
    NT AUTHORITY\Authenticated Users:(I)(OI)(CI)(IO)(F)
</code></pre>

<p>We also tried to grant full permission to everyone using following command (but <code>pip install</code> still fails after that)</p>

<pre><code>icacls D:\batch\tasks\shared\test-condaenv-users /grant ""Everyone:(OI)(CI)F"" /T
</code></pre>

<h2>Environment Information</h2>

<p><code><code>conda info</code></code><p>
</p>

<pre><code>     active environment : D:\batch\tasks\shared\ChinaPortWeather22_condavenv37
    active env location : D:\batch\tasks\shared\ChinaPortWeather22_condavenv37
            shell level : 1
       user config file : D:\Users\_azbatchtask_74\.condarc
 populated config files : 
          conda version : 4.6.11
    conda-build version : 3.17.8
         python version : 3.7.3.final.0
       base environment : C:\ProgramData\Anaconda3  (read only)
           channel URLs : https://repo.anaconda.com/pkgs/main/win-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/free/win-64
                          https://repo.anaconda.com/pkgs/free/noarch
                          https://repo.anaconda.com/pkgs/r/win-64
                          https://repo.anaconda.com/pkgs/r/noarch
                          https://repo.anaconda.com/pkgs/msys2/win-64
                          https://repo.anaconda.com/pkgs/msys2/noarch
          package cache : C:\ProgramData\Anaconda3\pkgs
                          D:\Users\_azbatchtask_74\.conda\pkgs
                          D:\Users\_azbatchtask_74\AppData\Local\conda\conda\pkgs
       envs directories : D:\Users\_azbatchtask_74\.conda\envs
                          C:\ProgramData\Anaconda3\envs
                          D:\Users\_azbatchtask_74\AppData\Local\conda\conda\envs
               platform : win-64
             user-agent : conda/4.6.11 requests/2.21.0 CPython/3.7.3 Windows/10 Windows/10.0.14393
          administrator : False
             netrc file : None
           offline mode : False
</code></pre>

<p></p></p>

<p><code><code>conda list --show-channel-urls</code></code><p>
</p>

<pre><code># packages in environment at D:\batch\tasks\shared\ChinaPortWeather22_condavenv37:
#
# Name                    Version                   Build  Channel
ca-certificates           2019.5.15                     0    defaults
certifi                   2019.3.9                 py37_0    defaults
openssl                   1.1.1c               he774522_1    defaults
pip                       19.1.1                   py37_0    defaults
python                    3.7.3                h8c8aaf0_1    defaults
setuptools                41.0.1                   py37_0    defaults
sqlite                    3.28.0               he774522_0    defaults
vc                        14.1                 h0510ff6_4    defaults
vs2015_runtime            14.15.26706          h3a45250_4    defaults
wheel                     0.33.4                   py37_0    defaults
wincertstore              0.2                      py37_0    defaults

</code></pre>

<p></p></p>

<p>EDIT 27th JUNE: I noticed that there is no <code>/bin/</code> folder inside the created conda environment, as it should be as explaned in documentation here</p>

<p>Find below the structure I have </p>

<pre><code>Directory of D:\batch\tasks\shared\test-condaenv-users

06/27/2019  04:29 AM    &lt;DIR&gt;          .
06/27/2019  04:29 AM    &lt;DIR&gt;          ..
04/20/2018  01:28 PM            19,208 api-ms-win-core-console-l1-1-0.dll
04/20/2018  01:28 PM            18,696 api-ms-win-core-datetime-l1-1-0.dll
04/20/2018  01:28 PM            18,696 api-ms-win-core-debug-l1-1-0.dll
04/20/2018  01:28 PM            18,696 api-ms-win-core-errorhandling-l1-1-0.dll
04/20/2018  01:29 PM            22,280 api-ms-win-core-file-l1-1-0.dll
04/20/2018  01:37 PM            18,696 api-ms-win-core-file-l1-2-0.dll
04/20/2018  01:37 PM            18,696 api-ms-win-core-file-l2-1-0.dll
04/20/2018  01:37 PM            18,696 api-ms-win-core-handle-l1-1-0.dll
04/20/2018  01:37 PM            19,208 api-ms-win-core-heap-l1-1-0.dll
04/20/2018  01:37 PM            18,696 api-ms-win-core-interlocked-l1-1-0.dll
04/20/2018  01:37 PM            19,720 api-ms-win-core-libraryloader-l1-1-0.dll
04/20/2018  01:37 PM            21,256 api-ms-win-core-localization-l1-2-0.dll
04/20/2018  01:37 PM            19,208 api-ms-win-core-memory-l1-1-0.dll
04/20/2018  01:37 PM            18,696 api-ms-win-core-namedpipe-l1-1-0.dll
04/20/2018  01:37 PM            19,720 api-ms-win-core-processenvironment-l1-1-0.dll
04/20/2018  01:37 PM            20,744 api-ms-win-core-processthreads-l1-1-0.dll
04/20/2018  01:37 PM            19,208 api-ms-win-core-processthreads-l1-1-1.dll
04/20/2018  01:37 PM            18,184 api-ms-win-core-profile-l1-1-0.dll
04/20/2018  01:37 PM            19,208 api-ms-win-core-rtlsupport-l1-1-0.dll
04/20/2018  01:37 PM            18,696 api-ms-win-core-string-l1-1-0.dll
04/20/2018  01:37 PM            20,744 api-ms-win-core-synch-l1-1-0.dll
04/20/2018  01:37 PM            19,208 api-ms-win-core-synch-l1-2-0.dll
04/20/2018  01:37 PM            19,720 api-ms-win-core-sysinfo-l1-1-0.dll
04/20/2018  01:37 PM            19,208 api-ms-win-core-timezone-l1-1-0.dll
04/20/2018  01:37 PM            18,696 api-ms-win-core-util-l1-1-0.dll
04/20/2018  01:37 PM            19,720 api-ms-win-crt-conio-l1-1-0.dll
04/20/2018  01:37 PM            22,792 api-ms-win-crt-convert-l1-1-0.dll
04/20/2018  01:37 PM            19,208 api-ms-win-crt-environment-l1-1-0.dll
04/20/2018  01:37 PM            20,744 api-ms-win-crt-filesystem-l1-1-0.dll
04/20/2018  01:37 PM            19,720 api-ms-win-crt-heap-l1-1-0.dll
04/20/2018  01:37 PM            19,208 api-ms-win-crt-locale-l1-1-0.dll
04/20/2018  01:37 PM            27,912 api-ms-win-crt-math-l1-1-0.dll
04/20/2018  01:37 PM            26,888 api-ms-win-crt-multibyte-l1-1-0.dll
04/20/2018  01:37 PM            71,432 api-ms-win-crt-private-l1-1-0.dll
04/20/2018  01:37 PM            19,720 api-ms-win-crt-process-l1-1-0.dll
04/20/2018  01:37 PM            23,304 api-ms-win-crt-runtime-l1-1-0.dll
04/20/2018  01:37 PM            24,840 api-ms-win-crt-stdio-l1-1-0.dll
04/20/2018  01:37 PM            24,840 api-ms-win-crt-string-l1-1-0.dll
04/20/2018  01:37 PM            21,256 api-ms-win-crt-time-l1-1-0.dll
04/20/2018  01:37 PM            19,208 api-ms-win-crt-utility-l1-1-0.dll
11/13/2018  04:56 PM           329,368 concrt140.dll
06/27/2019  04:29 AM    &lt;DIR&gt;          conda-meta
06/27/2019  04:29 AM    &lt;DIR&gt;          DLLs
06/27/2019  04:29 AM    &lt;DIR&gt;          include
06/27/2019  04:29 AM    &lt;DIR&gt;          Lib
06/27/2019  04:29 AM    &lt;DIR&gt;          Library
06/27/2019  04:29 AM    &lt;DIR&gt;          libs
03/25/2019  08:21 PM            12,769 LICENSE_PYTHON.txt
11/13/2018  04:56 PM           625,808 msvcp140.dll
11/13/2018  04:56 PM            31,896 msvcp140_1.dll
11/13/2018  04:56 PM           195,248 msvcp140_2.dll
04/24/2019  08:30 PM            95,232 python.exe
04/24/2019  08:30 PM           438,272 python.pdb
04/24/2019  08:30 PM            51,712 python3.dll
04/24/2019  08:30 PM         3,745,792 python37.dll
04/24/2019  08:30 PM         9,523,200 python37.pdb
04/24/2019  08:30 PM            93,696 pythonw.exe
04/24/2019  08:30 PM           438,272 pythonw.pdb
06/27/2019  04:29 AM    &lt;DIR&gt;          Scripts
06/27/2019  04:29 AM    &lt;DIR&gt;          tcl
06/27/2019  04:29 AM    &lt;DIR&gt;          Tools
04/20/2018  01:37 PM         1,016,584 ucrtbase.dll
11/13/2018  04:56 PM           386,720 vccorlib140.dll
11/13/2018  04:56 PM           155,280 vcomp140.dll
11/13/2018  04:56 PM            87,200 vcruntime140.dll
              56 File(s)     18,091,625 bytes
              11 Dir(s)  69,282,779,136 bytes free
""\Lib"" 
 Volume in drive D is Temporary Storage
 Volume Serial Number is 2AF3-E29E

 Directory of D:\batch\tasks\shared\test-condaenv-users\Lib

06/27/2019  04:29 AM    &lt;DIR&gt;          .
06/27/2019  04:29 AM    &lt;DIR&gt;          ..
03/25/2019  08:21 PM             5,580 abc.py
03/25/2019  08:21 PM            32,814 aifc.py
03/25/2019  08:21 PM               477 antigravity.py
03/25/2019  08:21 PM            95,103 argparse.py
03/25/2019  08:21 PM            12,575 ast.py
03/25/2019  08:21 PM            11,328 asynchat.py
06/27/2019  04:29 AM    &lt;DIR&gt;          asyncio
03/25/2019  08:21 PM            20,118 asyncore.py
03/25/2019  08:21 PM            20,380 base64.py
03/25/2019  08:21 PM            31,489 bdb.py
03/25/2019  08:21 PM            13,954 binhex.py
03/25/2019  08:21 PM             2,557 bisect.py
03/25/2019  08:21 PM            12,410 bz2.py
03/25/2019  08:21 PM            24,826 calendar.py
03/25/2019  08:21 PM            34,549 cgi.py
03/25/2019  08:21 PM            12,018 cgitb.py
03/25/2019  08:21 PM             5,435 chunk.py
03/25/2019  08:21 PM            14,860 cmd.py
03/25/2019  08:21 PM            10,619 code.py
03/25/2019  08:21 PM            36,287 codecs.py
03/25/2019  08:21 PM             5,994 codeop.py
06/27/2019  04:29 AM    &lt;DIR&gt;          collections
03/25/2019  08:21 PM             4,064 colorsys.py
03/25/2019  08:21 PM            13,649 compileall.py
06/27/2019  04:29 AM    &lt;DIR&gt;          concurrent
03/25/2019  08:21 PM            54,283 configparser.py
03/25/2019  08:21 PM            23,774 contextlib.py
03/25/2019  08:21 PM               129 contextvars.py
03/25/2019  08:21 PM             8,815 copy.py
03/25/2019  08:21 PM             7,017 copyreg.py
03/25/2019  08:21 PM             5,805 cProfile.py
03/25/2019  08:21 PM             3,346 crypt.py
03/25/2019  08:21 PM            16,180 csv.py
06/27/2019  04:29 AM    &lt;DIR&gt;          ctypes
06/27/2019  04:29 AM    &lt;DIR&gt;          curses
03/25/2019  08:21 PM            48,508 dataclasses.py
03/25/2019  08:21 PM            86,298 datetime.py
06/27/2019  04:29 AM    &lt;DIR&gt;          dbm
03/25/2019  08:21 PM               320 decimal.py
03/25/2019  08:21 PM            84,387 difflib.py
03/25/2019  08:21 PM            19,888 dis.py
06/27/2019  04:29 AM    &lt;DIR&gt;          distutils
03/25/2019  08:21 PM           104,284 doctest.py
03/25/2019  08:21 PM             2,815 dummy_threading.py
06/27/2019  04:29 AM    &lt;DIR&gt;          email
06/27/2019  04:29 AM    &lt;DIR&gt;          encodings
06/27/2019  04:29 AM    &lt;DIR&gt;          ensurepip
03/25/2019  08:21 PM            34,778 enum.py
03/25/2019  08:21 PM             9,830 filecmp.py
03/25/2019  08:21 PM            14,568 fileinput.py
03/25/2019  08:21 PM             4,056 fnmatch.py
03/25/2019  08:21 PM            15,143 formatter.py
03/25/2019  08:21 PM            23,639 fractions.py
03/25/2019  08:21 PM            35,257 ftplib.py
03/25/2019  08:21 PM            32,441 functools.py
03/25/2019  08:21 PM             4,756 genericpath.py
03/25/2019  08:21 PM             7,489 getopt.py
03/25/2019  08:21 PM             5,994 getpass.py
03/25/2019  08:21 PM            21,967 gettext.py
03/25/2019  08:21 PM             5,638 glob.py
03/25/2019  08:21 PM            20,338 gzip.py
03/25/2019  08:21 PM             9,534 hashlib.py
03/25/2019  08:21 PM            23,017 heapq.py
03/25/2019  08:21 PM             6,517 hmac.py
06/27/2019  04:29 AM    &lt;DIR&gt;          html
06/27/2019  04:29 AM    &lt;DIR&gt;          http
06/27/2019  04:29 AM    &lt;DIR&gt;          idlelib
03/25/2019  08:21 PM            53,292 imaplib.py
03/25/2019  08:21 PM             3,795 imghdr.py
03/25/2019  08:21 PM            10,536 imp.py
06/27/2019  04:29 AM    &lt;DIR&gt;          importlib
03/25/2019  08:21 PM           117,615 inspect.py
03/25/2019  08:21 PM             3,517 io.py
03/25/2019  08:21 PM            75,072 ipaddress.py
06/27/2019  04:29 AM    &lt;DIR&gt;          json
03/25/2019  08:21 PM             2,245 keyword.py
06/27/2019  04:29 AM    &lt;DIR&gt;          lib2to3
03/25/2019  08:21 PM             5,312 linecache.py
03/25/2019  08:21 PM            78,027 locale.py
06/27/2019  04:29 AM    &lt;DIR&gt;          logging
03/25/2019  08:21 PM            12,983 lzma.py
03/25/2019  08:21 PM             6,123 macpath.py
03/25/2019  08:21 PM            78,654 mailbox.py
03/25/2019  08:21 PM             8,104 mailcap.py
03/25/2019  08:21 PM            20,880 mimetypes.py
03/25/2019  08:21 PM            23,035 modulefinder.py
06/27/2019  04:29 AM    &lt;DIR&gt;          msilib
06/27/2019  04:29 AM    &lt;DIR&gt;          multiprocessing
03/25/2019  08:21 PM             5,566 netrc.py
03/25/2019  08:21 PM            43,088 nntplib.py
03/25/2019  08:21 PM            22,340 ntpath.py
03/25/2019  08:21 PM             2,584 nturl2path.py
03/25/2019  08:21 PM            10,244 numbers.py
03/25/2019  08:21 PM             5,824 opcode.py
03/25/2019  08:21 PM            10,863 operator.py
03/25/2019  08:21 PM            60,371 optparse.py
03/25/2019  08:21 PM            37,756 os.py
03/25/2019  08:21 PM            49,389 pathlib.py
03/25/2019  08:21 PM            62,544 pdb.py
03/25/2019  08:21 PM            57,837 pickle.py
03/25/2019  08:21 PM            91,220 pickletools.py
03/25/2019  08:21 PM             8,916 pipes.py
03/25/2019  08:21 PM            21,461 pkgutil.py
04/24/2019  07:46 PM            47,067 platform.py
03/25/2019  08:21 PM            29,885 plistlib.py
03/25/2019  08:21 PM            14,964 poplib.py
03/25/2019  08:21 PM            15,772 posixpath.py
03/25/2019  08:21 PM            20,884 pprint.py
03/25/2019  08:21 PM            22,046 profile.py
03/25/2019  08:21 PM            27,313 pstats.py
03/25/2019  08:21 PM             4,763 pty.py
03/25/2019  08:21 PM            15,137 pyclbr.py
03/25/2019  08:21 PM           106,731 pydoc.py
06/27/2019  04:29 AM    &lt;DIR&gt;          pydoc_data
03/25/2019  08:21 PM             8,001 py_compile.py
03/25/2019  08:21 PM            11,359 queue.py
03/25/2019  08:21 PM             7,254 quopri.py
03/25/2019  08:21 PM            27,557 random.py
03/25/2019  08:21 PM            15,192 re.py
03/25/2019  08:21 PM             5,267 reprlib.py
03/25/2019  08:21 PM             7,097 rlcompleter.py
03/25/2019  08:21 PM            11,959 runpy.py
03/25/2019  08:21 PM             6,442 sched.py
03/25/2019  08:21 PM             2,038 secrets.py
03/25/2019  08:21 PM            18,561 selectors.py
03/25/2019  08:21 PM             8,527 shelve.py
03/25/2019  08:21 PM            12,956 shlex.py
03/25/2019  08:21 PM            41,329 shutil.py
03/25/2019  08:21 PM             2,123 signal.py
06/27/2019  04:29 AM    &lt;DIR&gt;          site-packages
03/25/2019  08:21 PM            21,649 site.py
03/25/2019  08:21 PM            34,711 smtpd.py
03/25/2019  08:21 PM            44,210 smtplib.py
03/25/2019  08:21 PM             7,086 sndhdr.py
03/25/2019  08:21 PM            27,363 socket.py
03/25/2019  08:21 PM            26,922 socketserver.py
06/27/2019  04:29 AM    &lt;DIR&gt;          sqlite3
03/25/2019  08:21 PM            26,872 sre_compile.py
03/25/2019  08:21 PM             7,177 sre_constants.py
03/25/2019  08:21 PM            39,305 sre_parse.py
03/25/2019  08:21 PM            45,432 ssl.py
03/25/2019  08:21 PM             5,038 stat.py
03/25/2019  08:21 PM            20,651 statistics.py
03/25/2019  08:21 PM            11,564 string.py
03/25/2019  08:21 PM            12,917 stringprep.py
03/25/2019  08:21 PM               257 struct.py
03/25/2019  08:21 PM            69,966 subprocess.py
03/25/2019  08:21 PM            18,375 sunau.py
03/25/2019  08:21 PM             2,131 symbol.py
03/25/2019  08:21 PM             7,274 symtable.py
04/24/2019  07:46 PM            24,649 sysconfig.py
03/25/2019  08:21 PM            24,317 sysconfig.py.orig
03/25/2019  08:21 PM            11,408 tabnanny.py
03/25/2019  08:21 PM            92,617 tarfile.py
03/25/2019  08:21 PM            23,135 telnetlib.py
03/25/2019  08:21 PM            26,710 tempfile.py
06/27/2019  04:29 AM    &lt;DIR&gt;          test
03/25/2019  08:21 PM            19,494 textwrap.py
03/25/2019  08:21 PM             1,003 this.py
03/25/2019  08:21 PM            48,104 threading.py
03/25/2019  08:21 PM            13,444 timeit.py
06/27/2019  04:29 AM    &lt;DIR&gt;          tkinter
03/25/2019  08:21 PM             3,763 token.py
03/25/2019  08:21 PM            27,030 tokenize.py
03/25/2019  08:21 PM            28,543 trace.py
03/25/2019  08:21 PM            23,438 traceback.py
03/25/2019  08:21 PM            17,076 tracemalloc.py
03/25/2019  08:21 PM               879 tty.py
03/25/2019  08:21 PM           143,602 turtle.py
06/27/2019  04:29 AM    &lt;DIR&gt;          turtledemo
03/25/2019  08:21 PM             9,897 types.py
03/25/2019  08:21 PM            54,602 typing.py
06/27/2019  04:29 AM    &lt;DIR&gt;          unittest
06/27/2019  04:29 AM    &lt;DIR&gt;          urllib
03/25/2019  08:21 PM             6,816 uu.py
03/25/2019  08:21 PM            29,449 uuid.py
06/27/2019  04:29 AM    &lt;DIR&gt;          venv
03/25/2019  08:21 PM            20,242 warnings.py
03/25/2019  08:21 PM            18,229 wave.py
03/25/2019  08:21 PM            20,689 weakref.py
03/25/2019  08:21 PM            23,097 webbrowser.py
06/27/2019  04:29 AM    &lt;DIR&gt;          wsgiref
03/25/2019  08:21 PM             5,913 xdrlib.py
06/27/2019  04:29 AM    &lt;DIR&gt;          xml
06/27/2019  04:29 AM    &lt;DIR&gt;          xmlrpc
03/25/2019  08:21 PM             7,535 zipapp.py
03/25/2019  08:21 PM            80,379 zipfile.py
03/25/2019  08:21 PM             1,801 _bootlocale.py
03/25/2019  08:21 PM            26,424 _collections_abc.py
03/25/2019  08:21 PM             8,749 _compat_pickle.py
03/25/2019  08:21 PM             5,340 _compression.py
03/25/2019  08:21 PM             5,117 _dummy_thread.py
03/25/2019  08:21 PM            14,598 _markupbase.py
03/25/2019  08:21 PM            19,138 _osx_support.py
03/25/2019  08:21 PM           228,535 _pydecimal.py
03/25/2019  08:21 PM            91,166 _pyio.py
03/25/2019  08:21 PM             6,186 _py_abc.py
03/25/2019  08:21 PM             3,115 _sitebuiltins.py
03/25/2019  08:21 PM            25,504 _strptime.py
03/25/2019  08:21 PM             7,214 _threading_local.py
03/25/2019  08:21 PM             5,679 _weakrefset.py
03/25/2019  08:21 PM             5,101 __future__.py
03/25/2019  08:21 PM                64 __phello__.foo.py
06/27/2019  04:29 AM    &lt;DIR&gt;          __pycache__
             172 File(s)      4,330,005 bytes
              34 Dir(s)  69,282,762,752 bytes free
""\Lib\site-packages"" 
 Volume in drive D is Temporary Storage
 Volume Serial Number is 2AF3-E29E

 Directory of D:\batch\tasks\shared\test-condaenv-users\Lib\site-packages

06/27/2019  04:29 AM    &lt;DIR&gt;          .
06/27/2019  04:29 AM    &lt;DIR&gt;          ..
06/27/2019  04:29 AM    &lt;DIR&gt;          certifi
06/20/2019  10:54 PM             2,990 certifi-2019.06.16-py3.7.egg-info
04/23/2019  01:18 AM               126 easy_install.py
06/27/2019  04:29 AM    &lt;DIR&gt;          pip
06/27/2019  04:29 AM    &lt;DIR&gt;          pip-19.1.1-py3.7.egg-info
06/27/2019  04:29 AM    &lt;DIR&gt;          pkg_resources
03/25/2019  08:21 PM               119 README.txt
06/27/2019  04:29 AM    &lt;DIR&gt;          setuptools
06/27/2019  04:29 AM    &lt;DIR&gt;          setuptools-41.0.1-py3.7.egg-info
06/27/2019  04:29 AM    &lt;DIR&gt;          wheel
06/27/2019  04:29 AM    &lt;DIR&gt;          wheel-0.33.4-py3.7.egg-info
06/28/2018  10:27 PM             5,788 wincertstore-0.2-py3.7.egg-info
10/23/2013  07:30 AM            11,873 wincertstore.py
06/27/2019  04:29 AM    &lt;DIR&gt;          __pycache__
               5 File(s)         20,896 bytes
              11 Dir(s)  69,282,762,752 bytes free
</code></pre>

<p>""\Library"" 
     Volume in drive D is Temporary Storage
     Volume Serial Number is 2AF3-E29E</p>

<pre><code> Directory of D:\batch\tasks\shared\test-condaenv-users\Library

06/27/2019  04:29 AM    &lt;DIR&gt;          .
06/27/2019  04:29 AM    &lt;DIR&gt;          ..
06/27/2019  04:29 AM    &lt;DIR&gt;          bin
05/29/2019  06:11 PM               412 ct_log_list.cnf
05/29/2019  06:11 PM               412 ct_log_list.cnf.dist
06/27/2019  04:29 AM    &lt;DIR&gt;          include
06/27/2019  04:29 AM    &lt;DIR&gt;          lib
06/27/2019  04:29 AM    &lt;DIR&gt;          misc
05/29/2019  06:11 PM            10,909 openssl.cnf
05/29/2019  06:11 PM            10,909 openssl.cnf.dist
06/27/2019  04:29 AM    &lt;DIR&gt;          ssl
               4 File(s)         22,642 bytes
               7 Dir(s)  69,282,762,752 bytes free
""\Library\bin"" 
 Volume in drive D is Temporary Storage
 Volume Serial Number is 2AF3-E29E

 Directory of D:\batch\tasks\shared\test-condaenv-users\Library\bin

06/27/2019  04:29 AM    &lt;DIR&gt;          .
06/27/2019  04:29 AM    &lt;DIR&gt;          ..
04/20/2018  01:28 PM            19,208 api-ms-win-core-console-l1-1-0.dll
04/20/2018  01:28 PM            18,696 api-ms-win-core-datetime-l1-1-0.dll
04/20/2018  01:28 PM            18,696 api-ms-win-core-debug-l1-1-0.dll
04/20/2018  01:28 PM            18,696 api-ms-win-core-errorhandling-l1-1-0.dll
04/20/2018  01:29 PM            22,280 api-ms-win-core-file-l1-1-0.dll
04/20/2018  01:37 PM            18,696 api-ms-win-core-file-l1-2-0.dll
04/20/2018  01:37 PM            18,696 api-ms-win-core-file-l2-1-0.dll
04/20/2018  01:37 PM            18,696 api-ms-win-core-handle-l1-1-0.dll
04/20/2018  01:37 PM            19,208 api-ms-win-core-heap-l1-1-0.dll
04/20/2018  01:37 PM            18,696 api-ms-win-core-interlocked-l1-1-0.dll
04/20/2018  01:37 PM            19,720 api-ms-win-core-libraryloader-l1-1-0.dll
04/20/2018  01:37 PM            21,256 api-ms-win-core-localization-l1-2-0.dll
04/20/2018  01:37 PM            19,208 api-ms-win-core-memory-l1-1-0.dll
04/20/2018  01:37 PM            18,696 api-ms-win-core-namedpipe-l1-1-0.dll
04/20/2018  01:37 PM            19,720 api-ms-win-core-processenvironment-l1-1-0.dll
04/20/2018  01:37 PM            20,744 api-ms-win-core-processthreads-l1-1-0.dll
04/20/2018  01:37 PM            19,208 api-ms-win-core-processthreads-l1-1-1.dll
04/20/2018  01:37 PM            18,184 api-ms-win-core-profile-l1-1-0.dll
04/20/2018  01:37 PM            19,208 api-ms-win-core-rtlsupport-l1-1-0.dll
04/20/2018  01:37 PM            18,696 api-ms-win-core-string-l1-1-0.dll
04/20/2018  01:37 PM            20,744 api-ms-win-core-synch-l1-1-0.dll
04/20/2018  01:37 PM            19,208 api-ms-win-core-synch-l1-2-0.dll
04/20/2018  01:37 PM            19,720 api-ms-win-core-sysinfo-l1-1-0.dll
04/20/2018  01:37 PM            19,208 api-ms-win-core-timezone-l1-1-0.dll
04/20/2018  01:37 PM            18,696 api-ms-win-core-util-l1-1-0.dll
04/20/2018  01:37 PM            19,720 api-ms-win-crt-conio-l1-1-0.dll
04/20/2018  01:37 PM            22,792 api-ms-win-crt-convert-l1-1-0.dll
04/20/2018  01:37 PM            19,208 api-ms-win-crt-environment-l1-1-0.dll
04/20/2018  01:37 PM            20,744 api-ms-win-crt-filesystem-l1-1-0.dll
04/20/2018  01:37 PM            19,720 api-ms-win-crt-heap-l1-1-0.dll
04/20/2018  01:37 PM            19,208 api-ms-win-crt-locale-l1-1-0.dll
04/20/2018  01:37 PM            27,912 api-ms-win-crt-math-l1-1-0.dll
04/20/2018  01:37 PM            26,888 api-ms-win-crt-multibyte-l1-1-0.dll
04/20/2018  01:37 PM            71,432 api-ms-win-crt-private-l1-1-0.dll
04/20/2018  01:37 PM            19,720 api-ms-win-crt-process-l1-1-0.dll
04/20/2018  01:37 PM            23,304 api-ms-win-crt-runtime-l1-1-0.dll
04/20/2018  01:37 PM            24,840 api-ms-win-crt-stdio-l1-1-0.dll
04/20/2018  01:37 PM            24,840 api-ms-win-crt-string-l1-1-0.dll
04/20/2018  01:37 PM            21,256 api-ms-win-crt-time-l1-1-0.dll
04/20/2018  01:37 PM            19,208 api-ms-win-crt-utility-l1-1-0.dll
11/13/2018  04:56 PM           329,368 concrt140.dll
06/27/2019  04:29 AM             6,548 c_rehash.pl
05/29/2019  06:11 PM         3,407,360 libcrypto-1_1-x64.dll
05/29/2019  06:11 PM        10,088,448 libcrypto-1_1-x64.pdb
05/29/2019  06:11 PM           681,472 libssl-1_1-x64.dll
05/29/2019  06:11 PM         2,322,432 libssl-1_1-x64.pdb
11/13/2018  04:56 PM           625,808 msvcp140.dll
11/13/2018  04:56 PM            31,896 msvcp140_1.dll
11/13/2018  04:56 PM           195,248 msvcp140_2.dll
05/29/2019  06:11 PM           542,720 openssl.exe
05/29/2019  06:11 PM         2,527,232 openssl.pdb
04/22/2019  06:33 PM         1,028,608 sqlite3.dll
04/22/2019  06:33 PM         1,303,552 sqlite3.exe
04/20/2018  01:37 PM         1,016,584 ucrtbase.dll
11/13/2018  04:56 PM           386,720 vccorlib140.dll
11/13/2018  04:56 PM           155,280 vcomp140.dll
11/13/2018  04:56 PM            87,200 vcruntime140.dll
              57 File(s)     25,601,052 bytes
               2 Dir(s)  69,282,762,752 bytes free
 Volume in drive D is Temporary Storage
 Volume Serial Number is 2AF3-E29E

 Directory of D:\batch\tasks\shared\test-condaenv-users\libs

06/27/2019  04:29 AM    &lt;DIR&gt;          .
06/27/2019  04:29 AM    &lt;DIR&gt;          ..
04/24/2019  08:30 PM           170,564 python3.lib
04/24/2019  08:29 PM           342,616 python37.lib
04/24/2019  08:30 PM             1,750 _tkinter.lib
               3 File(s)        514,930 bytes
               2 Dir(s)  69,282,762,752 bytes free
</code></pre>
","<python><anaconda><virtualenv><azure-data-factory>","2019-06-19 09:43:04","2168","5","1","56768984","<p>It sure looks like pip tries to install packages globally instead of inside the environment, so I've compiled a list of approaches you might want to try to force local pip usage:</p>

<ul>
<li>Install pip during environment creation:</li>
</ul>

<pre><code>conda create -p D:\batch\tasks\shared\test-condaenv-users python=3.7 pip
</code></pre>

<ul>
<li>Install pip after environment creation (local pip will supposedly take effect after you deactivate and reactivate environment):</li>
</ul>

<pre><code>conda create -p D:\batch\tasks\shared\test-condaenv-users python=3.7
conda activate D:\batch\tasks\shared\test-condaenv-users
conda install pip
conda deactivate
</code></pre>

<ul>
<li>Call local pip explicitly:</li>
</ul>

<pre><code>conda activate D:\batch\tasks\shared\test-condaenv-users
D:\batch\tasks\shared\test-condaenv-users\bin\pip install nltk
</code></pre>

<p><strong>Reading list</strong>:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/41060382/using-pip-to-install-packages-to-anaconda-environment"">Using Pip to install packages to Anaconda Environment</a></li>
<li><a href=""https://github.com/ContinuumIO/anaconda-issues/issues/1429"" rel=""noreferrer"">https://github.com/ContinuumIO/anaconda-issues/issues/1429</a></li>
</ul>

<p>Hope it helps.</p>
"
"56661107","Get the list of scheduled triggers","<p>We are trying to get the Maximum scheduled trigger time from the list of scheduled trigger in ADF</p>

<p>We have one ADF pipeline, which has multiple scheduled trigger. The pipeline will run at 6:10, 6:20, 6:30, 6:40...... till 10 AM UTC at a gap of every 10 minutes. Is there any possible way to get the Max of scheduled trigger i.e. 10 AM UTC in my case.</p>

<p>We have tried with several system variables, but none worked. We might take an API approach to get the job done, but I want to stay native to the ADF world.</p>
","<azure-data-factory>","2019-06-19 06:14:23","735","1","1","56680051","<p>You could refer to ADF REST API:<a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/triggerruns/querybyfactory"" rel=""nofollow noreferrer"">Trigger Runs - Query By Factory</a>. </p>

<p>In the request body,define the <code>lastUpdatedAfter</code> and <code>lastUpdatedBefore</code> properties,like the below example:</p>

<pre><code>{
  ""lastUpdatedAfter"": ""2018-06-16T00:36:44.3345758Z"",
  ""lastUpdatedBefore"": ""2018-06-16T00:49:48.3686473Z"",
  ""filters"": [
    {
      ""operand"": ""TriggerName"",
      ""operator"": ""Equals"",
      ""values"": [
        ""exampleTrigger""
      ]
    }
  ]
}
</code></pre>

<p>Then loop the trigger runs data from response to get the max row. </p>

<blockquote>
  <p>We might take an API approach to get the job done</p>
</blockquote>

<p>You could use <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook"" rel=""nofollow noreferrer"">Azure Http Trigger Function</a>,or use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">Web Activity</a> in the ADF to call your specific api.</p>
"
"56640577","Azure data factory data flow task cannot take on prem as source","<p><a href=""https://i.stack.imgur.com/oJXAa.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oJXAa.jpg"" alt=""enter image description here""></a></p>

<p>Hey, 
I am having trouble in creating a Data Flows task which uses an on-prem source. Is this not possible in the Preview version?</p>

<p>I have created a self hosted IR to connect ADF to my laptop, and that is what I want to use. In the pic below I am trying to create a dataset off self hosted IR. It works great in Copy task but for Data Flows it is greyed out.</p>
","<azure><dataflow><azure-data-factory>","2019-06-18 01:12:45","3617","2","3","56641045","<p>Reproduce your issue on my side ,however nothing about this feature is claimed on the official document. As you can see everywhere about data flow cliams that:</p>

<p><a href=""https://i.stack.imgur.com/jGbIg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jGbIg.png"" alt=""enter image description here""></a></p>

<p>You could submit any voice here:</p>

<p><a href=""https://i.stack.imgur.com/68n4d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/68n4d.png"" alt=""enter image description here""></a></p>

<p>Also found a <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/37472797-data-flows-add-support-to-access-on-premise-sql-a"" rel=""nofollow noreferrer"">feedback</a> for data flow in ADF for your reference.If you need push the progress of it,you could vote up it. Also,i would suggest you referring to the comments in the link:</p>

<blockquote>
  <p>For access to the 80+ ADF connectors, use Copy Activity to stage data
  for transformation.</p>
  
  <p>Data Flows will access data in your lake (Blob, ADB, ADW, ADLS) for
  transformation.</p>
  
  <p>Think of Copy Activity as your data heavy-lifting activity and Data
  Flow as your data transformation engine.</p>
</blockquote>
"
"56640577","Azure data factory data flow task cannot take on prem as source","<p><a href=""https://i.stack.imgur.com/oJXAa.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oJXAa.jpg"" alt=""enter image description here""></a></p>

<p>Hey, 
I am having trouble in creating a Data Flows task which uses an on-prem source. Is this not possible in the Preview version?</p>

<p>I have created a self hosted IR to connect ADF to my laptop, and that is what I want to use. In the pic below I am trying to create a dataset off self hosted IR. It works great in Copy task but for Data Flows it is greyed out.</p>
","<azure><dataflow><azure-data-factory>","2019-06-18 01:12:45","3617","2","3","56642147","<p>For this question, I asked Azure support for help and they replied me with the answer:</p>

<p><strong>Answer:</strong></p>

<p>This means on-premise SQL server is not supported as dataset in data flow in current stage.</p>

<p>Screen shot:
<a href=""https://i.stack.imgur.com/RHmEb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RHmEb.png"" alt=""enter image description here""></a></p>

<p><strong>Update</strong>:</p>

<p>Data flow now only support Azure IR so it doesn’t support on-premise dataset. 
Refer to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#integration-runtime-types"" rel=""nofollow noreferrer"">Integration runtime types</a>.
<a href=""https://i.stack.imgur.com/h9AAy.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/h9AAy.jpg"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"56640577","Azure data factory data flow task cannot take on prem as source","<p><a href=""https://i.stack.imgur.com/oJXAa.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oJXAa.jpg"" alt=""enter image description here""></a></p>

<p>Hey, 
I am having trouble in creating a Data Flows task which uses an on-prem source. Is this not possible in the Preview version?</p>

<p>I have created a self hosted IR to connect ADF to my laptop, and that is what I want to use. In the pic below I am trying to create a dataset off self hosted IR. It works great in Copy task but for Data Flows it is greyed out.</p>
","<azure><dataflow><azure-data-factory>","2019-06-18 01:12:45","3617","2","3","56656943","<p>If your goal is to use visual data transformations in ADF using Mapping Data Flows with on-prem data, then build a pipeline with a Copy Activity first. Use the Self-Hosted Integration Runtime with the Copy Activity to stage your data in Blob Store. Then add a subsequent Execute Data Flow activity to transform that data.</p>

<p>I made video on how to do this:</p>

<p><a href=""https://www.youtube.com/watch?v=IN-4v0e7UIs"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=IN-4v0e7UIs</a></p>
"
"56631762","Parameterised datasets in Azure Data Factory","<p>I'm wondering if anyone has any experience in calling datasets dynamically in Azure Data Factory.  The situation we have is that we dynamically sweep all tables in from IaaS (on-premise SQL Server installations on an Azure VM) application systems to a data lake.  We want to have one pipeline that can pass server name, database name, user name and password to the pipeline's activities.  The pipelines will then sweep whatever source they've been told to read from the parameters.  The source systems are currently within a separate subscription and domain within our Enterprise Agreement.</p>

<p>We have looked into using the AutoResolveIntegrationRuntime on a generic SQL Server dataset but, as it is Azure and the runtimes on the VMs are self-hosted, it can't resolve and we get 'cannot connect' errors. So,</p>

<p>i) I don't know if this problem goes away if they are in the same subscription and domain?</p>

<p>That leaves whether anyone can assist with:</p>

<p>ii) A way of getting a dynamic runtime to resolve which SQL Server runtime it should use (we have one per VM for resilience purposes, but they can all see each other's instances).  We don't want to parameterise a linked service on a particular VM as it places reliance for other VMs on that single VM.</p>

<p>iii) Ability to parameterise a dataset to call a runtime (doesn't look possible in the UI).</p>

<p>iv) Ability to parameterise the source and sink connections with pipeline activities to call a dataset parameter.</p>
","<azure><azure-data-factory>","2019-06-17 12:53:19","1548","1","1","56651523","<p>Servers, database, tableNames are possible to be dynamic by using parameters. The key problem here is that all the reference in ADF can’t be parameterized, like linked services reference in dataset, integrationRuntime reference in linked service. If you don’t have too many selfhosted integrationRuntime, maybe you can try setup different pipelines for different network?</p>
"
"56630362","feeding filenames in azure databrick activity one by one in azure datafactory pipeline","<p>I am trying to create a data factory pipeline where one activity injects names of files (from a container or some other folder) in databricks activity one by one to be processed in the order of incoming. How do I achieve it?</p>
","<azure-pipelines><azure-data-factory><azure-databricks>","2019-06-17 11:27:50","779","1","2","56641084","<p>Hello Surbhi Tayal and thank you for your inquiry.  By order of incoming I assume you mean the first one sent to databricks should be the first one finished, as opposed to processing in parallel.</p>

<p>To accomplish this, you will need the following:</p>

<ul>
<li>a pipeline variable of type array.</li>
<li>a mechanism to populate the array variable with the file names.  It may be a GetMetadata activity or pipeline parameters or something else.  Add more details to question if you need help with this.</li>
<li>a Databricks resource and Linked Service</li>
</ul>

<p>In your pipeline create a ForEach activity.  In the settings mark the 'Sequential' option.  Failure to do so will cause your activities to be sent in parallel, in stead of one-by-on.
In the settings, reference the array variable in the 'Items'.  The expression looks like <code>@variables('myVariableName')</code>.
Inside the ForEach activity's activities, place a Databricks type of activity.  The choices are 'Notebook', 'Jar', and 'Python'.  For mine, I used Notebook.  Notebook was easier to set up, because of the UI 'Browse' option.  Set the activity to use the appropriate Linked Service first.  Set the 'Python file' / 'Notebook path' / 'Main class name'.
Expand the 'Parameters' section and add a new parameter.  Give the parameter(s) the same name as in the Databricks script.  The value should be <code>@string(item())</code> (may differ if your enumerable is not a simple array of primitives).  This grabs the item from the ForEach activity and ensures it is a string.
Set Libraries if necessary.</p>

<p>When you try to run/debug, be aware that Databricks can take a long time to spin up the cluster.  This increases the pipeline running time.</p>
"
"56630362","feeding filenames in azure databrick activity one by one in azure datafactory pipeline","<p>I am trying to create a data factory pipeline where one activity injects names of files (from a container or some other folder) in databricks activity one by one to be processed in the order of incoming. How do I achieve it?</p>
","<azure-pipelines><azure-data-factory><azure-databricks>","2019-06-17 11:27:50","779","1","2","56642253","<p>Depending on the source type and the frequency the files are ingested it might be also an option to use spark structured streaming. For streaming data sources also files are supported as a source - Reads files written in a directory as a stream of data. Supported file formats are text, csv, json, orc, parquet (see the docs of the DataStreamReader interface for a more up-to-date list, and supported options for each file format). Note that the files must be atomically placed in the given directory, which in most file systems, can be achieved by file move operations.</p>

<pre><code>streamingInputDF = (
  spark
    .readStream           # Similar to Batch just using `readStream` instead of `read`
    .schema(jsonSchema)               
    .json(inputPath)
)
</code></pre>

<p>If you don' t want to run the notebook permernantly use the trigger once option. With the trigger once option output is written once for available data, without this option output stream would run permanentely:</p>

<pre><code>streamingOutputDF \
    .coalesce(1) \
    .writeStream \
    .format(""parquet"") \
    .partitionBy('ingest_date') \
    .option(""checkpointLocation"", checkPointPath) \
    .option(""path"", targetPath) \
    .trigger(once=True) \
    .start()
</code></pre>

<p>You can use Data Factory to trigger the Databricks notebook without parameter in this scenario.</p>
"
"56630239","AzureDataFactory how to store output of Filter action","<p>I am using Filter action in pipeline on Json from blob file. How to store filter output to blob/any storage?</p>

<p>Regards</p>
","<azure-data-factory>","2019-06-17 11:20:09","434","0","1","56651575","<p>If you mean filter activity, please check this doc. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-filter-activity#type-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-filter-activity#type-properties</a> </p>
"
"56627680","Azure DataFactory DelimitedText dataset with parametrized schema","<p>I'm trying to create a generic CSV dataset with parametrized filename and schema to be able to use it in foreach loops with file lists and I'm having some trouble on publishing, and I don't know if I'm doing something wrong or if the framework docs are not correct.</p>

<p>According to <a href=""https://learn.microsoft.com/bs-latn-ba/rest/api/datafactory/datasets/createorupdate#delimitedtextdataset"" rel=""nofollow noreferrer"">documentation</a> the schema description is:</p>

<blockquote>
  <p>Columns that define the physical type schema of the dataset. Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.</p>
</blockquote>

<p>I have a dataset with a parameter named Schema of type Array and the ""schema"" set to an expression that returns this parameter:</p>

<pre><code>{
    ""name"": ""GenericCSVFile"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""LinkedServiceReferenceName"",
            ""type"": ""LinkedServiceReference""
        },
        ""parameters"": {
            ""Schema"": {
                ""type"": ""array""
            },
            ""TableName"": {
                ""type"": ""string""
            },
            ""TableSchema"": {
                ""type"": ""string""
            }
        },
        ""folder"": {
            ""name"": ""Folder""
        },
        ""type"": ""DelimitedText"",
        ""typeProperties"": {
            ""location"": {
                ""type"": ""AzureDataLakeStoreLocation"",
                ""fileName"": {
                    ""value"": ""@concat(dataset().TableSchema,'.',dataset().TableName,'.csv')"",
                    ""type"": ""Expression""
                },
                ""folderPath"": ""Path""
            },
            ""columnDelimiter"": "","",
            ""escapeChar"": ""\\"",
            ""firstRowAsHeader"": true,
            ""quoteChar"": ""\""""
        },
        ""schema"": {
            ""value"": ""@dataset().Schema"",
            ""type"": ""Expression""
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p>However, when I publish, i get the following error:</p>

<pre><code>Error code: BadRequest
Inner error code: InvalidPropertyValue
Message: Invalid value for property 'schema'
</code></pre>

<p>Am I doing something wrong? are the docs wrong? </p>
","<azure><azure-data-factory>","2019-06-17 08:39:30","299","0","1","56658871","<p>Yes, this is the expected behavior. If you need to set dynamic value for column mapping, please ignore schema in DelimitedText dataset, which is more for a visually display of physical schema information and would not take effect when do copy activity column mapping. The expression setting for it is also not allowed. You could configure mapping as an expression to achieve this goal and pass it a proper value when trigger run. 
<a href=""https://i.stack.imgur.com/zIkpM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zIkpM.png"" alt=""enter image description here""></a></p>
"
"56627588","How to copy files to different directories based on the file name in ADFv2","<p>I'm trying to copy files to some folder location in ADLS. Here i'm trying to copy based on the file name. Here my source and destination is ADLS only.</p>

<p>For Ex. </p>

<pre><code>    If filename is ABC.csv then copy to ABC folder

    If filename is DEF.csv then copy to DEF folder

    If filename is XYZ.csv then copy to XYZ folder and so on.
</code></pre>

<p>Please help me achieving this using azure data factory v2.</p>

<p>I know this is easy task in SSIS. But in ADFv2 i tried using Foreach loop to iterate thru all the files (i have only 3 files) and inside foreach loop there is IF condition to check the file name. Not sure how to change the destination folder location in the run time.</p>
","<azure><azure-data-lake><azure-data-factory>","2019-06-17 08:33:59","1517","2","2","56642048","<p>You could achieve this using dynamic content in the filepath of the sink. So instead using an if-then-else-logic to define the target folder, build it dynamically depending on the filename.</p>

<p>In the following example the target folder is constructed dynamically with the trigger starttime: </p>

<pre><code> ""folder"": {
        ""name"": ""Target""
    },
    ""type"": ""AzureDataLakeStoreFile"",
    ""typeProperties"": {
        ""fileName"": """",
        ""folderPath"": {
            ""value"": ""@CONCAT('/tenant02/data/raw/corporate/hmaserver/Ingest_Date=', formatDateTime(trigger().startTime, 'yyyy-MM-dd/'))"",
            ""type"": ""Expression""
        }
</code></pre>

<p>In the Data Factory V2 UI edit the sink data set to do this:</p>

<p><a href=""https://i.stack.imgur.com/tVdfG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tVdfG.png"" alt=""ADF Sink Dataset""></a></p>

<p>Add the dynamic content in the connection:</p>

<p><a href=""https://i.stack.imgur.com/fZ85m.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fZ85m.png"" alt=""Dynamic Content for sink""></a></p>

<p>You could do this similar, e. g. replacing the file extension:</p>

<pre><code>@replace(item().name, '.csv', '')
</code></pre>

<p>Another option could be to using a Databricks activity for this. See my following <a href=""https://stackoverflow.com/a/54028526/9678762"">Answer</a> for a complex copy logic in a Python Notebook.</p>
"
"56627588","How to copy files to different directories based on the file name in ADFv2","<p>I'm trying to copy files to some folder location in ADLS. Here i'm trying to copy based on the file name. Here my source and destination is ADLS only.</p>

<p>For Ex. </p>

<pre><code>    If filename is ABC.csv then copy to ABC folder

    If filename is DEF.csv then copy to DEF folder

    If filename is XYZ.csv then copy to XYZ folder and so on.
</code></pre>

<p>Please help me achieving this using azure data factory v2.</p>

<p>I know this is easy task in SSIS. But in ADFv2 i tried using Foreach loop to iterate thru all the files (i have only 3 files) and inside foreach loop there is IF condition to check the file name. Not sure how to change the destination folder location in the run time.</p>
","<azure><azure-data-lake><azure-data-factory>","2019-06-17 08:33:59","1517","2","2","56691397","<p>You can using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-if-condition-activity"" rel=""nofollow noreferrer"">If Condition activity</a> in Azure Data Factory like bellow: </p>

<p>Define an <code>If Condition</code> to check if file name is ABC. </p>

<ul>
<li>If true, define an activity to copy it to folder ABC. </li>
<li>If false, insert inside <code>ifFalseActivities</code> another <code>If Condition</code> to check if file name is DEF then write activity for copying file to destination folder based on result of this activity.</li>
</ul>

<p>If you can define the <code>destination folder</code> based on file name before you pass the data to ADF (such as you define the <code>file name</code>, <code>destination folder</code> returned from <code>stored procedure</code>) then inside <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">Lookup activity</a>, you can using dynamic content for <code>File path</code> and <code>file name</code>.</p>
"
"56616119","how to copy files from a web site to azure blob storage","<p>I am trying to copy files from this site <a href=""http://nemweb.com.au/Reports/Current/Daily_Reports/"" rel=""nofollow noreferrer"">http://nemweb.com.au/Reports/Current/Daily_Reports/</a> to my azure blob storage account</p>

<p>my first option was to try Azure data factory, but it end up copying the html, which obviously not what I am looking for but rather the zip files inside</p>

<p>my question is ADF the right tool for that, or should I look at something else, any direction will be much appreciate it.</p>

<p>currently I am using Powerquery to read the data, and it is great,  unfortunately, PowerBI service require a gateway to refresh, which is not very practical in my case, hence, I am looking for other option in Microsoft data stack</p>

<p>edit : I am going with the python route but happy to hear any alternative</p>
","<etl><azure-blob-storage><powerquery><azure-data-factory>","2019-06-16 04:37:54","183","0","1","56617991","<p>I think I find the solution, Python, it has an excellent integration with azure blob, and the code to download the files is very easy, now I need to figure out which is the best service to run a python script on the cloud</p>

<pre><code>import re
import urllib.request
from urllib.request import urlopen

url = ""http://nemweb.com.au/Reports/Current/Daily_Reports/""
result = urlopen(url).read().decode('utf-8')

pattern = re.compile(r'[\w.]*.zip')
filelist = pattern.findall(result )
for x in filelist:
      urllib.request.urlretrieve(url+x, x)
</code></pre>
"
"56600969","Azure Data Factory Wildcard Characters","<p>I got a sftp location where generally .csv files are put, and we process or pull the file present there using ADF Copy activity. When no file is present and we give the exact filename and run the pipeline, it fails which is as expected. But when we give a wildcard character such as abc*.csv and run the pipeline with no file present in the sftp location, the copy activity passes though rows written is 0. Can anyone tell me why this happens. We are using Adf v2.</p>
","<azure><sftp><azure-data-factory>","2019-06-14 15:12:44","4090","2","1","56641152","<p>The answer to this is nuanced.  Here is the difference:</p>

<p>When you give an exact filename, but the file doesn't exist, Data Factory tries to get it, but the request is returned a 'file not found' error.  This is passed up to the activity, and is recognized as a failure.</p>

<p>When you give a wildcard, this is really asking ""Get me a list of files that fit this pattern, and then Copy each of them"".  When there are no files matching the pattern, the result is an empty list.  Since the list length is 0, no requests to fetch any file is made, meaning there is no opportunity to be served a 'file not found' error.</p>

<p>This is my reasoning from my experience with Data Factory.  I am not a member of the development team.</p>
"
"56593476","Is there any automated way of exporting Azure SQL database table data to Excel?","<p>I am trying to automatically export the Azure SQL database table data to Excel sheets.I tried to achieve it with Azure Data Factory but couldn't succeed as Azure data factory doesn't have direct support for Excel. I found in some documentation where it was mentioned that SQL database should be exported as text file first. Following that documentation, i exported the SQL database data as CSV file in Azure Blob Storage using Azure Data Factory and couldn't proceed further. Is there any way to convert that CSV in Azure Blob to Excel in automated way? Are there any better alternatives for the overall process?</p>
","<excel><azure><azure-sql-database><azure-blob-storage><azure-data-factory>","2019-06-14 07:31:13","3357","2","3","56596645","<p>You can have an Azure Function Activity in your Azure Data Factory pipeline and chain it to your Copy Activity. By chaining the activities, you are making sure that the Azure Function Activity is invoked only once the csv file is written successfully.</p>

<p>In the Azure Function, you can use a language of your choice to write code to convert the csv file to xls. </p>

<p>There are a bunch of libraries that you can use to convert csv to xls. Some of them are below :</p>

<ul>
<li><a href=""https://github.com/faisalman/simple-excel-js"" rel=""nofollow noreferrer"">Simple Excel for Javascript</a></li>
<li><a href=""https://stackoverflow.com/questions/3189308/convert-csv-to-xls-in-java"">Some ideas to do it in Java</a></li>
<li><a href=""http://coderscrowd.com/app/public/codes/view/201"" rel=""nofollow noreferrer"">Python</a></li>
</ul>

<p>Hope this helps.</p>
"
"56593476","Is there any automated way of exporting Azure SQL database table data to Excel?","<p>I am trying to automatically export the Azure SQL database table data to Excel sheets.I tried to achieve it with Azure Data Factory but couldn't succeed as Azure data factory doesn't have direct support for Excel. I found in some documentation where it was mentioned that SQL database should be exported as text file first. Following that documentation, i exported the SQL database data as CSV file in Azure Blob Storage using Azure Data Factory and couldn't proceed further. Is there any way to convert that CSV in Azure Blob to Excel in automated way? Are there any better alternatives for the overall process?</p>
","<excel><azure><azure-sql-database><azure-blob-storage><azure-data-factory>","2019-06-14 07:31:13","3357","2","3","56663221","<p>I didn't find the way to convert that CSV file in Azure Blob to Excel in automated way.
But I find that there is tool <a href=""https://www.filesculptor.com/download/"" rel=""nofollow noreferrer"">FileSculptor</a> can help you convert csv file to Excel automatically with scheduled tasks.</p>

<p><strong>Main Benefits</strong>:</p>

<ul>
<li>Convert between file formats CSV, XML, XLS and XLSX</li>
<li>Support for spreadsheets from Excel 97 to Excel 2019</li>
<li>Support for international character sets (unicode)</li>
<li>Select and reorder fields</li>
<li>Create calculated fields</li>
<li>Create an icon on the desktop to convert files with one click</li>
<li>Automatically convert files using scheduled tasks</li>
<li>Additional utility to convert files from DOS command prompt or .BAT
files.</li>
</ul>

<p>For more details, please reference this tutorial: <a href=""https://www.filesculptor.com/"" rel=""nofollow noreferrer"">Convert Between CSV, XML, XLS and XLSX Files</a>.</p>

<p>And about how to export Azure SQL database to a csv file, this tutorial <a href=""https://sqlbak.com/blog/export-sql-table-to-excel/"" rel=""nofollow noreferrer"">How to export SQL table to Excel</a> gives you two exampes:</p>

<ol>
<li>Export SQL table to <a href=""https://sqlbackupandftp.com/sqltoexcel"" rel=""nofollow noreferrer"">Excel using Sql</a> to Excel Utility. Perhaps the simplest way to export SQL table to Excel is using Sql to Excel utility that actually creates a CSV file that can be opened with Excel. It doesn’t require installation and everything you need to do is to connect to your database, select a database and tables you want to export.</li>
</ol>

<p><a href=""https://i.stack.imgur.com/CovQn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CovQn.png"" alt=""enter image description here""></a></p>

<ol start=""2"">
<li><a href=""https://solutioncenter.apexsql.com/how-to-import-and-export-sql-server-data-to-an-excel-file/"" rel=""nofollow noreferrer"">Export SQL table to Excel using SSMS</a>.</li>
</ol>

<p>I did't find the way which automatically export the Azure SQL database table data to Excel sheets. Azure SQL database doesn't support SQL Server agent. </p>

<p>Hope this helps. </p>
"
"56593476","Is there any automated way of exporting Azure SQL database table data to Excel?","<p>I am trying to automatically export the Azure SQL database table data to Excel sheets.I tried to achieve it with Azure Data Factory but couldn't succeed as Azure data factory doesn't have direct support for Excel. I found in some documentation where it was mentioned that SQL database should be exported as text file first. Following that documentation, i exported the SQL database data as CSV file in Azure Blob Storage using Azure Data Factory and couldn't proceed further. Is there any way to convert that CSV in Azure Blob to Excel in automated way? Are there any better alternatives for the overall process?</p>
","<excel><azure><azure-sql-database><azure-blob-storage><azure-data-factory>","2019-06-14 07:31:13","3357","2","3","57453454","<p>Exporting data from SSMS to Excel using Copy-Paste from DataGrid or exporting to .csv will cause the loss of the data types, which again, will cost you additional work when importing these data into Excel (import as text).</p>

<p>I have developed SSMSBoost add-in for SSMS, which allows to copy-paste data to Excel using <strong>native Excel clipboard format</strong>, preserving all data types.</p>

<p>Additionally, you can use SSMSBoost to create <strong>"".dqy""</strong> query file from your current SQL Script and open it in excel (in this case Excel will use provided connection information and SQL text to execute the query directly against your database).
I hope this helps.</p>
"
"56590079","Append more data to serialized json blobs","<p>I'm trying to query employee data from a rest endpoint periodically and save it to my SQL Server DB using an ADF copy activity. All employee data that is queried on the day is added to an azure blob with the filename corresponding to that day. In my Azure function, I serialize my object using <code>JsonConvert.SerializeObject(employee)</code> and store it as <strong>fileName_currentDate.txt</strong> after which the data looks something like this:</p>

<pre><code>[
 {
  name: ""abc"",
  address: ""pqr""
 },
 {
  name:""efg"",
  address: ""efg""
 }
]
</code></pre>

<p><strong>Problem</strong>: I want to keep appending more employees to the file with the matching date part. </p>

<p><strong>Constraint</strong>: I don't want to download the existing blob to the server using something like <code>blob.DownloadToStream(stream)</code>, de-serialize it ,add the extra data to my object, and then save it back to the blob container. This will be an expensive operation when the number of times I need to append is large and also when the blob size doesn't fit into memory.</p>

<p>What are my options here?</p>

<p>I see that there's support for <strong>Append Blob Storage</strong> which is what I plan to use in my app, however I cannot understand how I'd add more data to my json blob.</p>
","<json><serialization><azure-blob-storage><azure-data-factory>","2019-06-14 00:36:34","1545","1","1","56625673","<p>This <a href=""https://azure.microsoft.com/en-us/blog/incrementally-copy-new-files-by-lastmodifieddate-with-azure-data-factory/"" rel=""nofollow noreferrer"">link</a> indicates that you could configure LastModifiedDate property in the copy activity to build a pipeline with the capability of incrementally copying new and changed file.</p>

<p>However,it is really inappropriate to constantly add data to a single file, which can lead to very slow loading and difficult viewing of data. Based on your business requirements,i would suggest you to increment data in a unit time(e.g. every day) and store the new data into separate files named by the timestamp.</p>

<p>If you do so, maybe <a href=""https://learn.microsoft.com/en-us/azure/stream-analytics/"" rel=""nofollow noreferrer"">azure stream analytics</a> is the more suitable choice for you i think.</p>

<p>ASA supports <a href=""https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-inputs#stream-data-from-blob-storage"" rel=""nofollow noreferrer"">Azure Blob Storage input</a> and <a href=""https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-sql-output-perf"" rel=""nofollow noreferrer"">Azure SQL DB as output</a>.But it has below <a href=""https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-inputs#stream-data-from-blob-storage"" rel=""nofollow noreferrer"">features</a> for your references.</p>

<blockquote>
  <p>Stream Analytics does not support adding content to an existing blob
  file. Stream Analytics will view each file only once, and any changes
  that occur in the file after the job has read the data are not
  processed. Best practice is to upload all the data for a blob file at
  once and then add additional newer events to a different, new blob
  file.</p>
</blockquote>
"
"56587640","Azure Data Factory - Process SSIS Output","<p>I'm working to lift an SSIS package into Azure Data Factory V2, and I have successfully set up an IR and executed the package. </p>

<p>Now I'm attempting to work with the results in ADF. This package was originally designed to return a recordset to the calling client. Now that I'm in ADF, I'd like to take the recordset produced by the package and copy it to table storage. However, I see no way to access this recordset from within the ADF pipeline.</p>

<p>Is it possible to access and process this recordset from the host ADF pipeline, or will the package itself have to be modified to no longer return a recordset and perform the copy instead?</p>
","<ssis><azure-table-storage><azure-data-factory>","2019-06-13 19:47:25","222","2","1","56652183","<p>In the SSIS create a text file as output and copy it to a location/folder in blob or even your on premise folder.</p>

<ul>
<li><p>If you run SSIS on premise,  store it in an on premise folder , use AZCopy 
tool to move it Azure blob to a BLOB</p>

<p><a href=""https://blogs.technet.microsoft.com/canitpro/2015/12/28/step-by-step-using-azcopy-to-transfer-files-to-azure/"" rel=""nofollow noreferrer"">https://blogs.technet.microsoft.com/canitpro/2015/12/28/step-by-step-using-azcopy-to-transfer-files-to-azure/</a></p>

<p>Otherwise, you run SSIS on Azure as you mentioned .Copy the output of
your rowset to a flat file using flat file connection manager.Create another dataflow task in which you can upload the file  to Azure BLOB in </p></li>
</ul>

<p><a href=""https://www.powerobjects.com/blog/2018/11/20/uploading-azure-blob-ssis/"" rel=""nofollow noreferrer"">https://www.powerobjects.com/blog/2018/11/20/uploading-azure-blob-ssis/</a></p>

<p>Now your Azure pipleline can access that BLOB as a source in the copy
   activity and dump it to the table storage as a sink.</p>

<p>Let me know if you need more details on the implementation.</p>
"
"56583501","azure data factory: iterate over millions of files","<p>Previously I had a problem on how to merge several JSON files into one single file,
which I was able to resolve it with the answer of this <a href=""https://stackoverflow.com/questions/56550727/azure-data-factory-how-to-merge-all-files-of-a-folder-into-one-file/"">question</a>.</p>

<p>At first, I tried with just some files by using wild cards in the file name in the connection section of the input dataset. But when I remove the file name, theory tells me that all of the files in all folders would be loaded recursively as I checked the copy recursively option, in the source section of the copy activity.</p>

<p>The problem is that when I manually trigger the pipeline after removing the file name from the input of the data set, only some of the files get loaded and the task ends successfully but only loading around 400+ files, each folder has 1M+ files, I want to create BIG csv files by merging all the small JSON files of the source (I already was able to create csv file by mapping the schemas in the copy activity).</p>

<p><a href=""https://i.stack.imgur.com/8YpSm.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8YpSm.jpg"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/ctGxr.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ctGxr.jpg"" alt=""enter image description here""></a></p>
","<azure><azure-blob-storage><azure-data-factory>","2019-06-13 15:07:12","776","1","1","56587695","<p>It is probably stopping due to a timeout or out of memory exception.</p>

<p>One solution is to loop over the contents of the directory using </p>

<blockquote>
  <p>Directory.EnumerateFiles(searchDir)</p>
</blockquote>

<p>This way you can process all the files without having the list / contents of all files in memory at the same time.</p>
"
"56575317","How to implement a ""different from"" logical function","<p>I use Get Metadata to retrieve files name in a blob container. I also have a README.md in this same blob container.
I would like to be able to apply filter and set variable value in function of the files present in the blob container, but without having consideration of the README.md file. How is it possible?</p>

<p>As an example, here is a logic I would like to implement for setting Variable value:</p>

<pre><code>@if(and(equals(greater(activity('FilterOnOthers').output.FilteredItemsCount,0),true),not(equals(activity('FilterOnOthers').output.Value[0],'README.md'))),'YES','NO')
</code></pre>

<p>But it does not work as expected.</p>

<p>Thank you for your help</p>
","<azure><azure-data-factory>","2019-06-13 07:24:05","90","0","2","56579875","<p>You can use an If Loop condition. In the If Loop condition check for the metadata output. The condition should be file name README.md. Use your desired activity inside the If Loop based on either TRUE/FALSE</p>
"
"56575317","How to implement a ""different from"" logical function","<p>I use Get Metadata to retrieve files name in a blob container. I also have a README.md in this same blob container.
I would like to be able to apply filter and set variable value in function of the files present in the blob container, but without having consideration of the README.md file. How is it possible?</p>

<p>As an example, here is a logic I would like to implement for setting Variable value:</p>

<pre><code>@if(and(equals(greater(activity('FilterOnOthers').output.FilteredItemsCount,0),true),not(equals(activity('FilterOnOthers').output.Value[0],'README.md'))),'YES','NO')
</code></pre>

<p>But it does not work as expected.</p>

<p>Thank you for your help</p>
","<azure><azure-data-factory>","2019-06-13 07:24:05","90","0","2","56591512","<p>Great question! Here's an almost fool-proof way to do so :</p>

<ul>
<li>Create a variable of Array type in your pipeline, say 'x'.</li>
<li>Have the get metadata activity to read the folder and it's childItems by adding Child Items in the field list of the Dataset as shown below (highlighted) :</li>
</ul>

<p><a href=""https://i.stack.imgur.com/e3Cdn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e3Cdn.png"" alt=""enter image description here""></a></p>

<ul>
<li>After getting the list of child items as an array in the output activity of the Get Metadata activity, chain a ForEach activity as shown in the above screenshot.</li>
<li>In the ForEach activity, for Items, use expression : @activity('Get Metadata1').output.childItems</li>
<li>In the activities tab of the forEach activity, create an ifCondition activity.
In the ifCondition activity, specify the condition. eg- @equals(item().name, 'README.md').</li>
<li>In the Activities tab of the ifCondition, add an ""Append Variable"" activity for false condition.</li>
<li>In the Append Variable, append value : @item().name to the variable 'x'.</li>
<li>Now your variable 'x' has all values except 'README.md'.</li>
</ul>

<p>Hope I was clear in the explanation.</p>
"
"56570970","How to use parameter for Databricks existing cluster id in Data Factory?","<p>For some background, I am working in a development Azure Data Factory where I use the generated ARM templates to deploy to other test/prod environments.</p>

<p>The problem I am having is when trying to reference an existing cluster id in my Azure Databricks linked service. This cluster id gets passed into the different accounts where the cluster does not exist. This linked service in used in multiple pipelines so I want to be able to change it in one place.</p>

<p>I want to be able to have a parameter which I can override during the Azure DevOps release pipeline to map to the correct environment cluster. But as Data Factory generates the ARM template I don't have much control over it.</p>

<p>This is an example of what the arm template looks like.</p>

<pre><code>""name"": ""[concat(parameters('factoryName'), '/my-linked-service')]"",
""type"": ""Microsoft.DataFactory/factories/linkedServices"",
""apiVersion"": ""2018-06-01"",
""properties"": {
    ""description"": ""Databricks connection"",
    ""parameters"": {
        ""test"": {
            ""type"": ""string""
        }
    },
    ""annotations"": [],
    ""type"": ""AzureDatabricks"",
    ""typeProperties"": {
        ""domain"": ""https://australiaeast.azuredatabricks.net"",
        ""accessToken"": {
            ""type"": ""AzureKeyVaultSecret"",
            ""store"": {
                ""referenceName"": ""keyName"",
                ""type"": ""LinkedServiceReference""
            },
            ""secretName"": ""secretName""
        },
        ""existingClusterId"": ""1234-56789-abc123""
    }
</code></pre>

<p><a href=""https://i.stack.imgur.com/BEz2Z.png"" rel=""nofollow noreferrer"">Databricks Linked Service</a></p>
","<azure><azure-devops><azure-data-factory><azure-databricks>","2019-06-12 22:07:18","2660","2","1","56575546","<p>Only certain fields are parameterised by default. But you can set a template to customise them. It's not very pretty - the full guide is here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template</a></p>

<p>For your example I have a template that looks like this:</p>

<pre><code>{
    ""Microsoft.DataFactory/factories/linkedServices"": {
        ""*"": {
            ""properties"": {
                ""typeProperties"": {
                    ""existingClusterId"": ""=""
                }
            }
        }
    }
}
</code></pre>

<p>The file must be named arm-template-parameters-definition.json and placed in the root of your repo.</p>
"
"56565351","PATCH request in Azure Data Factory","<p>I want to scale an elastic pool in Azure Data Factory before my pipeline runs. However, for scaling an elastic pool or azure data warehouse I have to perform a PATCH request. This is the first time I heard of a PATCH method and after browsing the glorious web I haven't found any useful information other than creating an Azure Logic app or Azure Functions app. </p>

<p>I want to limit the orchestration to Azure Data Factory: the documentation can be found here: <a href=""https://learn.microsoft.com/en-us/rest/api/sql/elasticpools/update"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/sql/elasticpools/update</a></p>
","<azure><patch><azure-data-factory>","2019-06-12 15:08:08","388","0","1","56632949","<p>Currently, Azure Data Factory doesn’t support PATCH request.</p>

<p>I would suggest you to vote up an idea submitted by another Azure customer.</p>

<p><a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/34475443-support-patch-method-in-web-activity"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory/suggestions/34475443-support-patch-method-in-web-activity</a></p>

<p>All of the feedback you share in these forums will be monitored and reviewed by the Microsoft engineering teams responsible for building Azure.</p>
"
"56557354","Azure Data Factory - Copy Activity Json Array","<p>I'm trying to pull data from MongoDB in Azure Data Factory V2. The data structure is some thing like below.</p>

<pre><code>{
   ""Username"":""Joe"",
   ""Cities""[""Delhi"",""Mumbai""]
}
</code></pre>

<p>Now I wan't to do the cross-apply and get the data as given below.</p>

<pre><code>Name | City
----------
Joe  | Delhi
Joe  | Mumbai
</code></pre>

<p>I have already read the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping"" rel=""nofollow noreferrer"">doc</a>, but I'm not able to find a solution for this particular array structure.</p>
","<mongodb><azure-data-factory>","2019-06-12 08:03:32","1255","0","1","56651684","<p>Check the example for mongo db to oracle.</p>

<p>CollectionReference is what you wanted.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#alternative-schema-mapping"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#alternative-schema-mapping</a></p>
"
"56554782","Get ADF Pipeline JSON from C#","<p>I am trying to get the entire JSON of an ADF pipeline (both v1 &amp; v2) from C#</p>

<p>I could see documents that provide details abut creating pipelines from C#, unfortunately I am not sure of the right method/endpoint to perform this operation in C#.</p>

<pre><code>var client = new DataFactoryManagementClient(cred) { SubscriptionId = subscriptionId };

var factory = client.Factories.Get("""", """");

var pipeline = client.Pipelines.Get("""", """", """");


</code></pre>

<p>Unfortunately that doesn't provide me what am looking for, I believe the JSON are formatted as C# objects in <code>factory</code> and <code>client</code></p>

<p>Am I right with the above statement ? Will I have all the JSON details in C# objects?</p>
","<c#><azure><azure-data-factory>","2019-06-12 04:40:54","1164","0","1","56558891","<p>Azure Data factory exposes rest api and SDK which you can call for both V1 and V2 which gives you the pipeline result in json .</p>

<p>For example : Below api will give you the list of pipeline in json format for one Factory:</p>

<pre><code>GET https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/pipelines?api-version=2018-06-01
</code></pre>

<p>same set of operation you can do it using SDK too by using below method:</p>

<pre><code>public static Microsoft.Rest.Azure.IPage&lt;Microsoft.Azure.Management.DataFactory.Models.PipelineResource&gt; ListByFactory (this Microsoft.Azure.Management.DataFactory.IPipelinesOperations operations, string resourceGroupName, string factoryName);
</code></pre>

<p><a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.pipelinesoperationsextensions.listbyfactory?view=azure-dotnet"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.pipelinesoperationsextensions.listbyfactory?view=azure-dotnet</a></p>

<p>Similarly if you are looking for one particular pipeline , you can call below API:</p>

<pre><code>GET https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/pipelines/{pipelineName}?api-version=2018-06-01
</code></pre>

<p>same set of operation you can do it using SDK too by using below method:</p>

<pre><code>public static Microsoft.Azure.Management.DataFactory.Models.PipelineResource Get (this Microsoft.Azure.Management.DataFactory.IPipelinesOperations operations, string resourceGroupName, string factoryName, string pipelineName, string ifNoneMatch = null);
</code></pre>

<p><a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.pipelinesoperationsextensions.get?view=azure-dotnet#Microsoft_Azure_Management_DataFactory_PipelinesOperationsExtensions_Get_Microsoft_Azure_Management_DataFactory_IPipelinesOperations_System_String_System_String_System_String_System_String_"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.pipelinesoperationsextensions.get?view=azure-dotnet#Microsoft_Azure_Management_DataFactory_PipelinesOperationsExtensions_Get_Microsoft_Azure_Management_DataFactory_IPipelinesOperations_System_String_System_String_System_String_System_String_</a></p>

<p>As you can see all the SDK method returns below c# model object which you can serialize in JSON format using NewtonsoftJson library.</p>

<p>Hope it answer your question. Let me know if you any assistance .</p>
"
"56552503","Use Azure Functions to automate saving Excel to CSV","<p>Trying to use Azure Functions to automate saving Excel file out to blob as a CSV so I can consume in either Logic Apps or Azure Data Factory. Am looking to use ExcelDataReader C# library and I can get the NuGet package to download to my Function but am stuck after that.</p>

<p>Currently it appears I am stuck because the File.Open command looks for a file at a local path and I get the following error:</p>

<p>using (var stream = File.Open(filePath, FileMode.Open, FileAccess.Read))</p>

<p>The filename, directory name, or volume label syntax is incorrect : 'D:\Program Files (x86)\SiteExtensions\Functions\2.0.12507\64bit....'</p>

<p>Any suggestions you all have about saving out XLSX via Azure Functions?</p>
","<c#><azure><azure-functions><azure-data-factory><azure-logic-apps>","2019-06-11 22:39:29","4292","3","2","56553485","<p>You don't have to open a stream manually, Azure Functions Binding can do that for you for both reading and writing.</p>

<p>ex:</p>

<pre><code>[FunctionName(""ConvertExcelToCSV"")]
    public static async Task RunAsync(
        [BlobTrigger(""excel-files/{blobName}"")] Stream excelFileInput,
        [Blob(""csv-files/{blobName}"", FileAccess.Write)] Stream csvFileOutput,
        CancellationToken token,
        ILogger log)
    {
        log.LogInformation($""Do your processing on the excelFileInput file here."");
        //Do your processing on another steam. Maybe, MemoryStream
        await memoryStream.CopyToAsync(csvFileOutput, 4096, token);
    }
</code></pre>
"
"56552503","Use Azure Functions to automate saving Excel to CSV","<p>Trying to use Azure Functions to automate saving Excel file out to blob as a CSV so I can consume in either Logic Apps or Azure Data Factory. Am looking to use ExcelDataReader C# library and I can get the NuGet package to download to my Function but am stuck after that.</p>

<p>Currently it appears I am stuck because the File.Open command looks for a file at a local path and I get the following error:</p>

<p>using (var stream = File.Open(filePath, FileMode.Open, FileAccess.Read))</p>

<p>The filename, directory name, or volume label syntax is incorrect : 'D:\Program Files (x86)\SiteExtensions\Functions\2.0.12507\64bit....'</p>

<p>Any suggestions you all have about saving out XLSX via Azure Functions?</p>
","<c#><azure><azure-functions><azure-data-factory><azure-logic-apps>","2019-06-11 22:39:29","4292","3","2","56556345","<p>If you use <code>Environment.CurrentDirectory</code> to get the executing directory, it will response the directory you displaying. And this directory in the azure kudu, it's not allowed to create files so your excel file is not there. You could use <code>context.FunctionDirectory</code> to get the current function directory (e.g. when running on Azure)</p>

<p>ex:</p>

<pre><code>public static void Run([TimerTrigger(""0 */1 * * * *"")]TimerInfo myTimer, ILogger log, ExecutionContext context)
        {
            var excelFilePath = context.FunctionDirectory + @""\Book.xlsx"";
            var destinationCsvFilePath = context.FunctionDirectory + @""\test.csv"";

            System.Text.Encoding.RegisterProvider(System.Text.CodePagesEncodingProvider.Instance);

            var stream = new FileStream(excelFilePath, FileMode.Open, FileAccess.Read, FileShare.ReadWrite);

            IExcelDataReader reader = null;

            reader = ExcelReaderFactory.CreateOpenXmlReader(stream);

            var ds = reader.AsDataSet(new ExcelDataSetConfiguration()
            {
                ConfigureDataTable = (tableReader) =&gt; new ExcelDataTableConfiguration()
                {
                    UseHeaderRow = false
                }
            });

            var csvContent = string.Empty;
            int row_no = 0;
            while (row_no &lt; ds.Tables[0].Rows.Count)
            {
                var arr = new List&lt;string&gt;();
                for (int i = 0; i &lt; ds.Tables[0].Columns.Count; i++)
                {
                    arr.Add(ds.Tables[0].Rows[row_no][i].ToString());
                }
                row_no++;
                csvContent += string.Join("","", arr) + ""\n"";
            }
            StreamWriter csv = new StreamWriter(destinationCsvFilePath, false);
            csv.Write(csvContent);
            csv.Close();

            log.LogInformation($""C# Timer trigger function executed at: {DateTime.Now}"");
        }
</code></pre>

<p>And just update your excel here:</p>

<p><a href=""https://i.stack.imgur.com/oYBmv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oYBmv.png"" alt=""enter image description here""></a></p>
"
"56550727","azure data factory: how to merge all files of a folder into one file","<p>I need to create a big file, by merging multiple files scattered in several subfolders contained in an Azure Blob Storage, also a transformation needs to be done, each file contains a JSON array of a single element, so the final file, will contain an array of JSON elements.</p>

<p>The final purpose is to process that Big file in a Hadoop &amp; MapReduce job.</p>

<p>The layout of the original files is similar to this:</p>

<pre><code>folder
 - month-01
   - day-01
        - files...

- month-02
    - day-02
        - files...
</code></pre>
","<json><azure><azure-data-factory>","2019-06-11 19:42:10","15643","2","1","56553706","<p>I did a test based on your descriptions,please follow my steps.</p>

<p><strong><em>My simulate data:</em></strong></p>

<p><code>test1.json</code> resides in the folder: <code>date/day1</code></p>

<p><a href=""https://i.stack.imgur.com/bYkUK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/bYkUK.png"" alt=""enter image description here""></a></p>

<p><code>test2.json</code> resides in the folder: <code>date/day2</code></p>

<p><a href=""https://i.stack.imgur.com/kVrMY.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/kVrMY.png"" alt=""enter image description here""></a></p>

<p><code>Source DataSet</code>,set the file format setting as <code>Array of Objects</code> and file path as <code>root path</code>.</p>

<p><a href=""https://i.stack.imgur.com/5UmYV.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/5UmYV.png"" alt=""enter image description here""></a></p>

<p><code>Sink DataSet</code>,set the file format setting as <code>Array of Objects</code> and file path as the file you want to store the final data.</p>

<p><a href=""https://i.stack.imgur.com/Mpjx2.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Mpjx2.png"" alt=""enter image description here""></a></p>

<p>Create <code>Copy Activity</code> and set the <code>Copy behavior</code> as <code>Merge Files</code>.</p>

<p><a href=""https://i.stack.imgur.com/gRG23.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/gRG23.png"" alt=""enter image description here""></a></p>

<p><strong><em>Execution result:</em></strong></p>

<p><a href=""https://i.stack.imgur.com/qnrzn.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/qnrzn.png"" alt=""enter image description here""></a></p>

<p>The destination of my test is still Azure Blob Storage, you could refer to this <a href=""https://hadoop.apache.org/docs/current/hadoop-azure/index.html"" rel=""noreferrer"">link</a> to learn about Hadoop supports Azure Blob Storage.</p>
"
"56530184","How to pass content of file as a pipeline parameter","<p>I have a pipeline that accepts an array as parameters.
Currently, an array has been hardcoded as the default value.</p>

<p>Is it possible to make this dynamic? there is a file called Array.txt in azure blob which is updated frequently, how can I extract the content of Array.txt and pass it as parameter values to the Pipeline.</p>

<p>I tried using Lookup but receive error 'Object cannot be passed, pipeline is expecting an Array'</p>
","<azure><parameter-passing><azure-data-factory>","2019-06-10 16:30:30","358","0","1","56658921","<p>Please make sure the data in Array.txt is array format-compatible, then use a lookup activity for data extracting, pass <strong>@array(activity('Lookup1').output.value)</strong> to the subsequent activity. Remember to use <strong>@array()</strong> function to convert data into an Array.</p>
"
"56527167","Azure Data Factory Pipeline 'On Failure'","<p>I am setting up an ADF pipeline to copy blob into an Azure SQL DB. I have a Iteration activity in my pipeline, where I have set up a counter to loop and copy only if the blob exists.</p>

<p>This works great except for some random PK violations, which I will have to check manually. So I edited my pipeline to log the error, and continue. So I set up the pipeline as such. 
<a href=""https://i.stack.imgur.com/4jbmm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4jbmm.png"" alt=""re""></a>
If the copy activity fails due to Primary Key Violation, <em>(for now)</em> ignore, but log the details using a stored procedure and continue as usual i.e. update the loop counter to get the next folder. </p>

<p>Unfortunately, the <strong><em>success</em></strong> of Log Failure does not execute the ""Set Variable"" activity. So it goes back in an infinite loop keep coming back with the same exception, but the Stored Procedure activity itself is logging the error message correctly. </p>

<p>If I create a new ""Set Variable"" and do exactly what the SetLoopVariable does, it seems to be okay. but that means I have to copy every activity after that to have two separate paths. Which I feel is redundant. </p>

<p><strong>BACKGROUND</strong>: My file structure is container/YYYY/MM/dd/HH/mm, there will be at least one file per hour, but not for every minute of the day so I to do a check if the folder exists before attempting to copy. </p>
","<azure><azure-data-factory>","2019-06-10 13:14:41","6606","2","2","60782562","<p>This is by design. SetVariable will only be called if Copy Data succeeds <em>and</em> fails, since <a href=""https://datasavvy.me/2018/10/02/data-factory-v2-activity-dependencies-are-a-logical-and/"" rel=""nofollow noreferrer"">Data Factory V2 Activity Dependencies are a Logical AND</a>.</p>
"
"56527167","Azure Data Factory Pipeline 'On Failure'","<p>I am setting up an ADF pipeline to copy blob into an Azure SQL DB. I have a Iteration activity in my pipeline, where I have set up a counter to loop and copy only if the blob exists.</p>

<p>This works great except for some random PK violations, which I will have to check manually. So I edited my pipeline to log the error, and continue. So I set up the pipeline as such. 
<a href=""https://i.stack.imgur.com/4jbmm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4jbmm.png"" alt=""re""></a>
If the copy activity fails due to Primary Key Violation, <em>(for now)</em> ignore, but log the details using a stored procedure and continue as usual i.e. update the loop counter to get the next folder. </p>

<p>Unfortunately, the <strong><em>success</em></strong> of Log Failure does not execute the ""Set Variable"" activity. So it goes back in an infinite loop keep coming back with the same exception, but the Stored Procedure activity itself is logging the error message correctly. </p>

<p>If I create a new ""Set Variable"" and do exactly what the SetLoopVariable does, it seems to be okay. but that means I have to copy every activity after that to have two separate paths. Which I feel is redundant. </p>

<p><strong>BACKGROUND</strong>: My file structure is container/YYYY/MM/dd/HH/mm, there will be at least one file per hour, but not for every minute of the day so I to do a check if the folder exists before attempting to copy. </p>
","<azure><azure-data-factory>","2019-06-10 13:14:41","6606","2","2","62000996","<p>Thomas answer is correct.  I had this exact issue recently.  In case it helps someone else, I realised it means the arrows don’t represent a flow but a dependency.  The box only runs if all the preceding tasks are done, which is impossible in your case because it depends on the copy both succeeding and failing.</p>

<p>To solve your case just duplicate the ‘set loop variable’ in your error handling path.</p>

<p>However you might then have the same problem  that I now have here <a href=""https://stackoverflow.com/questions/62001161/azure-data-factory-handling-inner-failure-in-until-for-activity"">Azure data factory: Handling inner failure in until/for activity</a></p>
"
"56522290","Azure data factory pipeline failure email alerts to distribution list","<p>Azure data factory is not sending pipeline failure alerts to email distribution list.</p>

<p>It is able to send emails to individual email address but not to a distribution list.</p>

<p>Is there any special set up needed to send emails to distribution list here ?</p>
","<azure><azure-data-factory>","2019-06-10 07:33:27","619","1","1","64722042","<p>I was facing the same issue. Here are my findings:</p>
<p>The catch is many a times, DLs(distribution lists) have <strong>delivery restrictions</strong>. So they won't accept emails from <strong>external tenants</strong>(in this case, Microsoft).</p>
<p><strong>Azure</strong> emails are sent from the following emails:</p>
<ul>
<li>azure-noreply@microsoft.com</li>
<li>azureemail-noreply@microsoft.com</li>
<li>alerts-noreply@mail.windowsazure.com</li>
</ul>
<p>So make sure you enable your DL(s) to receive emails from the above mentioned Microsoft email ids. There is no other special setup.</p>
"
"56503245","Azure Data Factory pipeline to start SSIS Integration Runtime authorization error","<p>I created a Web task in Azure Data Factory to start/stop SSIS Integration Runtime. However, when I try to run the pipeline, I keep getting this error - Activity Start My IR failed: </p>

<blockquote>
  <p>{""error"":{""code"":""AuthorizationFailed"",""message"":""The client
  '2b36c922-34cf-43d8-be05-5d757017e2ed' with object id
  '2b36c922-34cf-43d8-be05-5d757017e2ed' does not have authorization to
  perform action
  'Microsoft.DataFactory/factories/integrationruntimes/start/action'
  over scope
  '/subscriptions/e8e76bdc-4591-4936-9899-31bffef9c5cd/resourcegroups/XXXXXBI-RG/providers/Microsoft.DataFactory/factories/XXXXXBI-DF/integrationruntimes/XXXXXSSIS-IR'.""}}</p>
</blockquote>

<p>I followed this instruction - <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-schedule-azure-ssis-integration-runtime"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-schedule-azure-ssis-integration-runtime</a></p>

<p>For Authentication, select MSI to use the managed identity for your ADF, see Managed identity for Data Factory article for more info.</p>

<p>What is the issue?</p>
","<azure><ssis><azure-data-factory>","2019-06-08 03:44:30","1237","5","1","56521079","<p>According to the error, you may not add the service principal to the ADF as a role. </p>

<p>You need to assign the managed identity for your ADF a <code>Contributor</code> role to itself, so Web activities in its pipelines can call REST API to start/stop Azure-SSIS IRs provisioned in it. </p>

<p>Follow the <code>step 5</code> in this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-schedule-azure-ssis-integration-runtime#create-your-pipelines"" rel=""nofollow noreferrer"">link</a>:</p>

<p><a href=""https://i.stack.imgur.com/AQk6b.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AQk6b.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/vCamY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vCamY.png"" alt=""enter image description here""></a></p>
"
"56501708","Azure Data Factory CSV with double quotes","<p>I have a pipeline that retrieves an FTP hosted CSV file. It is comma delimited with double quote identifiers. The issue exists where a string is encapsulated in double quotes, but the string itself contains double quotes.</p>

<p>string example: <code>""Spring Sale"" this year.</code></p>

<p>How it looks in the csv (followed and lead by two null columns): </p>

<pre class=""lang-none prettyprint-override""><code>"""",""""""Spring Sale"""" this year"",""""
</code></pre>

<p>SSIS handles this fine, but Data Factory wants to transform it into an extra column that isn't separated by a comma. I have removed the extra quotes on this line and it works fine.</p>

<p>Is there a way around this besides altering the source?</p>
","<azure><double-quotes><azure-data-factory>","2019-06-07 22:04:36","12016","9","1","56620533","<p>I got this to work using <code>Escape character</code> set as quote (<code>""</code>) with the Azure Data Factory Copy Task.  Screen shot:</p>

<p><a href=""https://i.stack.imgur.com/Igi3x.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Igi3x.png"" alt=""ADF Copy Task""></a></p>

<p>This was based on a file as per your spec:</p>

<p><code>"""",""""""Spring Sale"""" this year"",""""</code></p>

<p>and also worked as in insert into an Azure SQL Database table.  The sample JSON:</p>

<pre><code>{
    ""name"": ""DelimitedText1"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""linkedService2"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""DelimitedText"",
        ""typeProperties"": {
            ""location"": {
                ""type"": ""AzureBlobStorageLocation"",
                ""fileName"": ""quotes.txt"",
                ""container"": ""someContainer""
            },
            ""columnDelimiter"": "","",
            ""escapeChar"": ""\"""",
            ""quoteChar"": ""\""""
        },
        ""schema"": [
            {
                ""name"": ""Prop_0"",
                ""type"": ""String""
            },
            {
                ""name"": ""Prop_1"",
                ""type"": ""String""
            },
            {
                ""name"": ""Prop_2"",
                ""type"": ""String""
            }
        ]
    }
}
</code></pre>

<p>Maybe the example file is too simple but it did work for me in this configuration.</p>

<p>Alternately, just use SSIS and host it in Data Factory.</p>
"
"56500910","ADP FTP Linked Service losing connection mid-file transfer","<p>I am trying to copy data from an (azure vm) ftp hosted .csv file. When i execute the data pipeline i can see the ftp log and it initiates the file transfer, but at ~11mb it servers, attempts to reconnects, successfully reconnects, but then immediately disconnects.</p>

<p>Has anyone encountered this?</p>

<p>I can successfully transfer the file from cyberduck to local machine - and if i delete a lot of data from the csv, making it much smaller in size, the pipeline works correctly.</p>

<p>I have gone through passive ftp settings and vm firewall settings but i still cannot successfully get the file to completely transfer.</p>
","<azure><azure-data-factory>","2019-06-07 20:34:12","46","0","1","56501724","<p>I realized this is not a data transfer issue but a file format issue. It seem like the FTP transfer protocol doesn't download the entire file and then load - it seems to do partials and there was a piece of it that was breaking (double quotes in a string and double quote identifier"")</p>

<p>So it's an issue but a different issue and not related to this question.</p>
"
"56496579","Understanding the microsoft.storage.blobcreated event","<p>I'm trying to use an event trigger to load data from Azure blob to a SQL DB. </p>

<p>What I want to understand is when does the event <strong>Microsoft.storage.blobcreated</strong> actually get triggered, in the case when I'm creating large files in Azure blob storage?</p>

<ol>
<li><p>Is it when my file was just created and which is still loading data or </p></li>
<li><p>When the file has finished loading all the data</p></li>
</ol>

<p>In the first case, the copy activity may lose out on some data as the file hasn't yet completely downloaded all the data.</p>

<p>Also, while we're at it, is there a way we can handle any modifications to an existing blob so that the modified file also gets downloaded to my SQL DB?</p>
","<azure><azure-data-factory>","2019-06-07 14:48:19","1391","0","1","56520075","<p>Only when your file is created finished in Azure blob storage, you will have the blob properties, such as URL、TYPE and SIZE:
<a href=""https://i.stack.imgur.com/gYALl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gYALl.png"" alt=""enter image description here""></a></p>

<p>The <strong>Microsoft.storage.blobcreated</strong> event need these blob properties as event properties, You can get this from the blob created event example:</p>

<pre><code>[{
  ""topic"": ""/subscriptions/{subscription-id}/resourceGroups/Storage/providers/Microsoft.Storage/storageAccounts/xstoretestaccount"",
  ""subject"": ""/blobServices/default/containers/testcontainer/blobs/testfile.txt"",
  ""eventType"": ""Microsoft.Storage.BlobCreated"",
  ""eventTime"": ""2017-06-26T18:41:00.9584103Z"",
  ""id"": ""831e1650-001e-001b-66ab-eeb76e069631"",
  ""data"": {
    ""api"": ""PutBlockList"",
    ""clientRequestId"": ""6d79dbfb-0e37-4fc4-981f-442c9ca65760"",
    ""requestId"": ""831e1650-001e-001b-66ab-eeb76e000000"",
    ""eTag"": ""0x8D4BCC2E4835CD0"",
    ""contentType"": ""text/plain"",
    ""contentLength"": 524288,
    ""blobType"": ""BlockBlob"",
    ""url"": ""https://example.blob.core.windows.net/testcontainer/testfile.txt"",
    ""sequencer"": ""00000000000004420000000000028963"",
    ""storageDiagnostics"": {
      ""batchId"": ""b68529f3-68cd-4744-baa4-3c0498ec19f0""
    }
  },
  ""dataVersion"": """",
  ""metadataVersion"": ""1""
}]
</code></pre>

<p>For more details, please see: <a href=""https://learn.microsoft.com/en-us/azure/event-grid/event-schema-blob-storage#event-properties"" rel=""nofollow noreferrer"">Event properties</a>.</p>

<p>It means that you data will not be loaded to you SQL database until the file created finished.</p>

<p>For your another question,  is there a way we can handle any modifications to an existing blob so that the modified file also gets downloaded to my SQL DB?</p>

<p>Yes, there is. For example， you can using Logic app with the trigger: <strong><a href=""https://learn.microsoft.com/en-us/connectors/azureblobconnector/#triggers"" rel=""nofollow noreferrer"">Azure Blob Storage - When a blob is added or modified</a></strong>. You can reference this <a href=""https://learn.microsoft.com/en-us/azure/connectors/connectors-create-api-azureblobstorage#add-blob-storage-trigger"" rel=""nofollow noreferrer"">tutorial</a>.</p>

<p>Hope this helps.</p>
"
"56489342","REST API call from Azure Data Factory","<p>Azure Dta Factory Question -
In REST API, I am getting bearer token(as POST operation) in POSTMAN &amp; in Logic Apps after passing 
Content-Type &amp; apikey in Header
client_id,client_secret,grant_type in body</p>

<p>For getting Bearer token in Azure Data Factory in Web activity i am passing 
Content-Type &amp; apikey in Header</p>

<p>in body i am passing 
client_id:XXXX&amp;
client_secret:XXXX&amp;
grant_type:XXXX</p>

<p>and getting below error 
{
    ""errorCode"": ""2108"",
    ""message"": ""{\n   \""errors\"" : [ {\n      \""message\"": \""Unknown server error\"",\n      \""type\"": \""UnknownError\""\n   } ]\n}\n"",
    ""failureType"": ""UserError"",
    ""target"": ""Web1""
}</p>

<p>Please help.</p>

<p>I referred 
<a href=""https://social.msdn.microsoft.com/Forums/Lync/en-US/c826f2a8-fa23-4513-bf72-51c97f36b407/the-format-of-body-for-a-post-request-in-web-activity-in-azure-data-factory?forum=AzureDataFactory"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/Lync/en-US/c826f2a8-fa23-4513-bf72-51c97f36b407/the-format-of-body-for-a-post-request-in-web-activity-in-azure-data-factory?forum=AzureDataFactory</a></p>

<hr>

<p>Azure Dta Factory Question - In REST API, I am getting bearer token(as POST operation) in POSTMAN &amp; in Logic Apps after passing Content-Type &amp; apikey in Header client_id,client_secret,grant_type in body</p>

<p>For getting Bearer token in Azure Data Factory in Web activity i am passing Content-Type &amp; apikey in Header</p>

<p>in body i am passing </p>

<p>{ 
'client_id': 'XXX',
'client_secret': 'XXX',
'grant_type': 'XXX'
}</p>

<p>and getting below error </p>

<p>{
    ""errorCode"": ""2108"",
    ""message"": ""HTTP Status 406 – Not Acceptableh1 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:22px;} h2 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:16px;} h3 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:14px;} body {font-family:Tahoma,Arial,sans-serif;color:black;background-color:white;} b {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;} p {font-family:Tahoma,Arial,sans-serif;background:white;color:black;font-size:12px;} a {color:black;} a.name {color:black;} .line {height:1px;background-color:#525D76;border:none;}<h1>HTTP Status 406 – Not Acceptable</h1>"",
    ""failureType"": ""UserError"",
    ""target"": ""Web1""
}</p>
","<azure-data-factory>","2019-06-07 06:55:11","1071","0","1","56521718","<p>What is the format of the client_secret you are passing. Declare one more parameter in the body that is Content-Type: application/json;</p>
"
"56482382","Is there a way to have Azure Data Factories use Windows Integrated Authentication for an On-Prem File System Linked Service?","<p>I'm trying create an Azure Data Factory to copy files from a local, on-prem file system to Blob Storage. However, when creating the Linked Service the only apparent options are to specify a username directly in the ADF or via KeyVault. I want to use the current Windows Login to perform the file copy and not have to manage credentials. I'm fine with running the service as a specific user, but that doesn't solve the ADF wanting a username / password.</p>
","<azure><active-directory><azure-data-factory>","2019-06-06 17:24:16","148","0","1","56744547","<p>As you mentioned you are trying to copy from in-premise db the cloud . The integration which you are talking about can be achieved by using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-factory-service-identity#generate-managed-identity"" rel=""nofollow noreferrer"">managed identity</a> .but if I am understanding right you must be using an IR which you are running locally , when you update your credential in future , you will have to update that on the IR also .Going the AKV  rourte should not be very difficult .</p>
"
"56476131","Expression for Lookup activity result when stored procedure returns scalar value","<p>I'm using a Lookup activity that returns an scalar result and I want to use that result to build a dinamic query using a concat expression. However, I'm receiving an error complaining that my SQL query is not well formated. This is how I'm building the query:</p>

<pre><code>@concat('SELECT v.CscadaEventId as EventId, 
v.EndDate as EndDateUtc 
FROM v_cscadaevents v 
INNER JOIN cscadaevents e
ON e.cscadaeventId = v.CscadaEventId 
WHERE v.CscadaEventId IN(', activity('LookupUnfinishedAlarms').output.firstRow, ') AND e.EndDate IS NOT NULL;')
</code></pre>

<p>I expect that to return a query like this:</p>

<pre><code>SELECT v.CscadaEventId as EventId, 
v.EndDate as EndDateUtc 
FROM v_cscadaevents v 
INNER JOIN cscadaevents e
ON e.cscadaeventId = v.CscadaEventId WHERE v.CscadaEventId IN(2329390,2340616,2342078,2345857,2361240,2362088,2362574,2377062,2378594,2379357) AND e.EndDate IS NOT NULL;
</code></pre>

<p>I had see some examples where the lookup return multiple columns, and the right expression is <code>activity('LookupUnfinishedAlarms').output.firstRow.myColumnName</code> but what about when the lookup activity return an scalar value, as in  my case?</p>

<p>This is the full error so far:</p>

<blockquote>
  <p>You have an error in your SQL syntax; check the manual that
  corresponds to your MariaDB server version for the right syntax to use
  near
  &apos;\""output\"":\""2329390&#44;2340616&#44;2342078&#44;2345857&#44;2361240&#44;2362088&#44;2362574&#44;2377062&#44;237859&apos;
  at line
  6,Source=Microsoft.DataTransfer.Runtime.GenericOdbcConnectors,''Type=System.Data.Odbc.OdbcException,Message=ERROR
  [42000] [Microsoft][MariaDB] You have an error in your SQL syntax;
  check the manual that corresponds to your MariaDB server version for
  the right syntax to use near
  &apos;\""output\"":\""2329390&#44;2340616&#44;2342078&#44;2345857&#44;2361240&#44;2362088&#44;2362574&#44;2377062&#44;237859&apos;
  at line 6,Source=MariaDBODBC_sb64.dll</p>
</blockquote>
","<azure-data-factory>","2019-06-06 10:53:55","665","0","1","56481400","<p>Ok, just for the records, I found the solution. The expression must be:</p>

<pre><code>@concat('SELECT v.CscadaEventId as EventId, 
v.EndDate as EndDateUtc 
FROM v_cscadaevents v 
INNER JOIN cscadaevents e
ON e.cscadaeventId = v.CscadaEventId 
WHERE v.CscadaEventId IN(', activity('LookupUnfinishedAlarms').output.firstRow.output, ') AND e.EndDate IS NOT NULL;')
</code></pre>

<p>So, the default column becomes <code>output</code></p>
"
"56461275","How can I access pipeline parameters from data flow?","<p>I have a pipeline with a parameter (type int). Inside the pipeline, at one point, I have a data flow. This data flow needs to reference this parameter in order to filter data by it and add it as a new derived column. However, from the data flow, I am unable to access the parameter defined in the pipeline in which the data flow itself resides.</p>

<p>Pipeline:</p>

<pre><code>    {
    ""name"": ""TestPipeline"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""TestDataFlow"",
                ""type"": ""ExecuteDataFlow"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""typeProperties"": {
                    ""dataflow"": {
                        ""referenceName"": ""TestDataFlow"",
                        ""type"": ""DataFlowReference""
                    }
                }
            }
        ],
        ""parameters"": {
            ""CompanyId"": {
                ""type"": ""int"",
                ""defaultValue"": 1
            }
        }
    }
}
</code></pre>

<p>Data flow:</p>

<pre><code>    {
    ""name"": ""TestDataFlow"",
    ""properties"": {
        ""type"": ""MappingDataFlow"",
        ""typeProperties"": {
            ""sources"": [
                {
                    ""dataset"": {
                        ""referenceName"": ""DBEmployee"",
                        ""type"": ""DatasetReference""
                    },
                    ""name"": ""Employees"",
                    ""script"": ""source(output(\n\t\tId as long,\n\t\tName as string,\n\t\tSurname as string,\n\t\tEmail as string,\n\t\tPosition as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'table') ~&gt; Employees""
                }
            ],
            ""transformations"": [
                {
                    ""name"": ""AddColumnId"",
                    ""script"": ""Employees derive(ColumnId = ERROR_FUNCTION('@pipeline(__SINGLE_QUOTE__TestPipeline__SINGLE_QUOTE__).parameters.CompanyId')) ~&gt; AddColumnId""
                }
            ]
        }
    }
}
</code></pre>

<p>My query is simple. How can I access the pipeline's parameter ""CompanyId"" from the transformations inside the dataflow?</p>

<p>Thanks!</p>
","<parameters><transformation><pipeline><azure-data-factory>","2019-06-05 13:16:03","898","0","2","56486312","<p>I have used a parameter to filter incoming data from the dataset in a data flow but not seen how to reference a parameter in a data flow to do manipulation within the flow (i.e. does not appear in the list available to ""filters"" blocks for example).  Could you take in the parameter, send it to the source as part of the query and then filter and/or return the value as a column in the result set and do it that way?  Have not tried this.</p>

<p><strong>Referencing a pipeline  parameter:</strong></p>

<p>If you have a dataset with a parameter, then the dataflow will be able to supply that parameter in the dataflow settings panel (appears under ""source parameters""). That value can be supplied from the pipeline parameter.  <em>Without the dataset parameter - the ""source parameters"" option does not appear.</em></p>

<p>In the example below, source1 is my dataset reference in the dataflow dataflow1.  It takes a parameter called ""test"".</p>

<p>My pipeline has a parameter called Test1.  This is passed to the dataflow.  </p>

<p>Note the warning in the UI about how to debug.</p>

<pre><code>{
""name"": ""pipeline1"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""dataflow1"",
            ""type"": ""ExecuteDataFlow"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false,
                ""secureInput"": false
            },
            ""typeProperties"": {
                ""dataflow"": {
                    ""referenceName"": ""dataflow1"",
                    ""type"": ""DataFlowReference"",
                    ""datasetParameters"": {
                        ""source1"": {
                            ""test"": {
                                ""value"": ""@pipeline().parameters.Test1"",
                                ""type"": ""Expression""
                            }
                        }
                    }
                }
            }
        }
    ],
    ""parameters"": {
        ""Test1"": {
            ""type"": ""string""
        }
    }
}
</code></pre>

<p>}</p>
"
"56461275","How can I access pipeline parameters from data flow?","<p>I have a pipeline with a parameter (type int). Inside the pipeline, at one point, I have a data flow. This data flow needs to reference this parameter in order to filter data by it and add it as a new derived column. However, from the data flow, I am unable to access the parameter defined in the pipeline in which the data flow itself resides.</p>

<p>Pipeline:</p>

<pre><code>    {
    ""name"": ""TestPipeline"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""TestDataFlow"",
                ""type"": ""ExecuteDataFlow"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""typeProperties"": {
                    ""dataflow"": {
                        ""referenceName"": ""TestDataFlow"",
                        ""type"": ""DataFlowReference""
                    }
                }
            }
        ],
        ""parameters"": {
            ""CompanyId"": {
                ""type"": ""int"",
                ""defaultValue"": 1
            }
        }
    }
}
</code></pre>

<p>Data flow:</p>

<pre><code>    {
    ""name"": ""TestDataFlow"",
    ""properties"": {
        ""type"": ""MappingDataFlow"",
        ""typeProperties"": {
            ""sources"": [
                {
                    ""dataset"": {
                        ""referenceName"": ""DBEmployee"",
                        ""type"": ""DatasetReference""
                    },
                    ""name"": ""Employees"",
                    ""script"": ""source(output(\n\t\tId as long,\n\t\tName as string,\n\t\tSurname as string,\n\t\tEmail as string,\n\t\tPosition as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'table') ~&gt; Employees""
                }
            ],
            ""transformations"": [
                {
                    ""name"": ""AddColumnId"",
                    ""script"": ""Employees derive(ColumnId = ERROR_FUNCTION('@pipeline(__SINGLE_QUOTE__TestPipeline__SINGLE_QUOTE__).parameters.CompanyId')) ~&gt; AddColumnId""
                }
            ]
        }
    }
}
</code></pre>

<p>My query is simple. How can I access the pipeline's parameter ""CompanyId"" from the transformations inside the dataflow?</p>

<p>Thanks!</p>
","<parameters><transformation><pipeline><azure-data-factory>","2019-06-05 13:16:03","898","0","2","59886893","<p>Your Data flow parameter has to be of STRING type for you to choose the parameter from the pipeline. You can always convert this string to another datatype in your Data Flow if you want to use a date or an integer in your data flow.</p>
"
"56455105","Logic App not parsing body from ADF anymore","<p>I'm triggering Logic Apps (around 30) from the Data Factory V2. I am passing a body to the HTTP trigger, which is in JSON in Data Factory V2. The body is different for almost all Logic Apps.</p>

<p>Last week there was an issue that the 'When HTTP Request is received' step is not processing the body from the Data Factory in a correct matter.</p>

<p>Please note that both the Logic Apps and Data Factory haven't changed in months and were working without any problems up to last week.</p>

<p>This happened last week also, but this resolved 'itself', suggesting it was an issue at Logic App side. Currently all Logic Apps keep failing. I've tried rerunning the Logic Apps many times. @AzureSupport redirected me to our CSP, but they are not really helping at the moment.</p>

<p>Body in the ADF pipeline (sanitized the url):</p>

<pre><code>""typeProperties"": {
    ""url"": ""https://prod-50.westeurope.logic.azure.com:443 /&lt;....&gt;"",
    ""method"": ""POST"",
    ""body"": {
        ""customer"": ""@pipeline().parameters.customer"",
        ""token"": ""@pipeline().parameters.token"",
        ""tennant"": ""@pipeline().parameters.tennant"",
        ""baseuri"": ""@pipeline().parameters.baseuri"",
        ""connectorTrans"": ""@pipeline().parameters.connectorTrans"",
        ""connectorNonTrans"": ""@pipeline().parameters.connectorNonTrans"",
        ""datum"": ""@formatDateTime(adddays(utcnow(),-1),'s')""
    }
}
</code></pre>

<p>The last succesful run parsed the body from the Data Factory as follows (sanitized ofcourse):</p>

<pre><code>""body"": {
    ""customer"": ""&lt;customerName&gt;"",
    ""token"": ""&lt;token&gt;"",
    ""tennant"": null,
    ""baseuri"": ""&lt;baseUri&gt;"",
    ""connectorTrans"": ""&lt;connectorName&gt;"",
    ""connectorNonTrans"": ""&lt;connectorName2&gt;"",
    ""datum"": ""&lt;date&gt;""
}
</code></pre>

<p>The runs that are failing are all showing the same problem, the body is not being parsed correctly:</p>

<pre><code>""body"": ""{\r\n  \""customer\"": \""&lt;customerName&gt;\"",\r\n  \""token\"": \""&lt;token&gt;\"",\r\n  \""tennant\"": null,\r\n  \""baseuri\"": \""&lt;baseUri&gt;\"",\r\n  \""connectorTrans\"": \""&lt;connectorName&gt;\"",\r\n  \""connectorNonTrans\"": \""&lt;connectorName2&gt;\"",\r\n  \""datum\"": \""&lt;date&gt;\""\r\n}""
</code></pre>

<p>It is all in one single line, including \r\n and escape characters.</p>

<p>This is resulting in the Logic App not being able to use the values in the fields passed on by the Data Factory.</p>

<p>All help or pointers are much appreciated.</p>

<p>Running the Logic App from Postman, with the exact same body as from the Data Factory is working without any problems.</p>
","<json><azure-logic-apps><azure-data-factory>","2019-06-05 06:16:51","273","0","1","56463293","<p>I faced the same issue, you need to add header content type application/json in your web component in ADF which calling logic app. </p>

<p><a href=""https://i.stack.imgur.com/zKEic.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zKEic.png"" alt=""Check this""></a></p>
"
"56435952","Collection to exclude array items in azure data factory","<p>In Azure data factory we have Collections like:</p>

<pre><code>intersection([1, 2, 3], [101, 2, 1, 10],[6, 8, 1, 2])
</code></pre>

<p>whic returns <code>[1, 2]</code> </p>

<p>I'm looking for opposite to intersection. For given two arrays, it should return elements which are not present in first array.</p>

<pre><code>Ex: If we pass two arrays, 
    Array a: [1,2,3,4,5]
    Array b: [1,2,3]

Ouput: [4,5]
</code></pre>

<p>One way is to add two <strong>lookup</strong> activites resulting two arrays and <strong>Foreach</strong> of item in first array, if not present in second item array execute another activity.</p>

<p>Is there any better way to do in Azure Data factory?</p>

<p>Thanks</p>
","<azure-data-factory>","2019-06-04 00:20:25","5694","3","3","56454778","<p>Did you try solving this with a Join transformation in ADF Mapping Data Flows?</p>
"
"56435952","Collection to exclude array items in azure data factory","<p>In Azure data factory we have Collections like:</p>

<pre><code>intersection([1, 2, 3], [101, 2, 1, 10],[6, 8, 1, 2])
</code></pre>

<p>whic returns <code>[1, 2]</code> </p>

<p>I'm looking for opposite to intersection. For given two arrays, it should return elements which are not present in first array.</p>

<pre><code>Ex: If we pass two arrays, 
    Array a: [1,2,3,4,5]
    Array b: [1,2,3]

Ouput: [4,5]
</code></pre>

<p>One way is to add two <strong>lookup</strong> activites resulting two arrays and <strong>Foreach</strong> of item in first array, if not present in second item array execute another activity.</p>

<p>Is there any better way to do in Azure Data factory?</p>

<p>Thanks</p>
","<azure-data-factory>","2019-06-04 00:20:25","5694","3","3","57795931","<p><code>intersection</code> works with arrays.  If you have two arrays (Array being a native Azure Data Factory type) then I don't think Lookup will work as it works with datasets?  It's a shame Azure Data Factory (ADF v2) does not have the complement to <code>union</code> and <code>intersection</code> in terms of set operations which might be <code>complement</code> or <code>except</code> or <code>minus</code>.</p>

<p>However you can work around this using a <code>for each</code> loop and an <code>if</code> condition.  For example, I have an ADF v2 pipeline with two array parameters like this:</p>

<p><a href=""https://i.stack.imgur.com/Iu6cr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Iu6cr.png"" alt=""ADF pipeline pic 1""></a></p>

<p>The <code>for each</code> loop loops through the a1 parameter.  Set the Items property to this:</p>

<pre><code>@pipeline().parameters.a1
</code></pre>

<p>Within the <code>for each</code> loop, add an <code>if</code> condition:</p>

<p><a href=""https://i.stack.imgur.com/jo0Pz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jo0Pz.png"" alt=""ADF pipeline pic 2""></a></p>

<p>Set the <code>if</code> condition to use the contains function to check the a2 parameter for the present of the current item in the <code>for each</code> loop ( ie <code>item()</code> ):</p>

<pre><code>@contains(pipeline().parameters.a2,item())
</code></pre>

<p>Finally add an 'if false' activity, with an Append variable task, eg</p>

<p><a href=""https://i.stack.imgur.com/2XRz4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2XRz4.png"" alt=""ADF pipeline pic 3""></a></p>

<p><code>a3</code> is a variable of type Array.</p>

<p><a href=""https://i.stack.imgur.com/5VVgJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5VVgJ.png"" alt=""ADF pipeline pic 4""></a></p>

<p>No need to spin up a Spark cluster to solve this particular problem just yet ; )</p>
"
"56435952","Collection to exclude array items in azure data factory","<p>In Azure data factory we have Collections like:</p>

<pre><code>intersection([1, 2, 3], [101, 2, 1, 10],[6, 8, 1, 2])
</code></pre>

<p>whic returns <code>[1, 2]</code> </p>

<p>I'm looking for opposite to intersection. For given two arrays, it should return elements which are not present in first array.</p>

<pre><code>Ex: If we pass two arrays, 
    Array a: [1,2,3,4,5]
    Array b: [1,2,3]

Ouput: [4,5]
</code></pre>

<p>One way is to add two <strong>lookup</strong> activites resulting two arrays and <strong>Foreach</strong> of item in first array, if not present in second item array execute another activity.</p>

<p>Is there any better way to do in Azure Data factory?</p>

<p>Thanks</p>
","<azure-data-factory>","2019-06-04 00:20:25","5694","3","3","75222869","<p>I would suggest that you use the Filter activity.</p>
<p>In my case I'm working with 2 list of files, and loading the difference</p>
<pre><code>ListA = [1,2,3,4,5]
ListB = [1,2,3]
ListA - ListB = [4,5]
</code></pre>
<p>Using the Filter Activity:</p>
<pre><code>Items Box : @variables('ListB')

Condition Box : @not(contains(variables('ListA'),item()))
</code></pre>
<p><a href=""https://i.stack.imgur.com/U5Awp.png"" rel=""nofollow noreferrer"">This is a pipeline preview</a></p>
<p><a href=""https://i.stack.imgur.com/eyiWN.png"" rel=""nofollow noreferrer"">Filter Activity</a></p>
<p><a href=""https://i.stack.imgur.com/VXBIA.png"" rel=""nofollow noreferrer"">Items</a></p>
<p><a href=""https://i.stack.imgur.com/wkf7v.png"" rel=""nofollow noreferrer"">Condition</a></p>
"
"56434004","Azure Data Factory- Referencing Lookup activities in Queries","<p>I'm following a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-change-tracking-feature-powershell"" rel=""nofollow noreferrer"">tutorial on Azure Data Factory migration from Azure SQL to Blob through pipelines.</a> While most of the concepts make sense, the 'Copy Data' query is a bit confusing. I have a background in writing Oracle SQL, but Azure SQL on ADF is pretty different and I'm struggling to find specific technical documentation, probably because it's not widely adopted yet.</p>

<p>Pipeline configuration shown below:
<a href=""https://i.stack.imgur.com/xgWJJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xgWJJ.png"" alt=""enter image description here""></a>
Query is posted below:</p>

<pre class=""lang-sql prettyprint-override""><code>SELECT data_source_table.PersonID,data_source_table.Name,data_source_table.Age, 
CT.SYS_CHANGE_VERSION, SYS_CHANGE_OPERATION
FROM data_source_table 
RIGHT OUTER JOIN CHANGETABLE(CHANGES data_source_table, 
@{activity('LookupLastChangeTrackingVersionActivity').output.firstRow.SYS_CHANGE_VERSION}) 
AS CT ON data_source_table.PersonID = CT.PersonID 
WHERE CT.SYS_CHANGE_VERSION &lt;= 
@{activity('LookupCurrentChangeTrackingVersionActivity').output.firstRow.CurrentChangeTrackingVersion}
</code></pre>

<p>Output to the sink Blob as a result of the 'Copy Data' query:</p>

<pre><code>2,name2,14,4,U
7,name7,51,3,I
8,name8,31,5,I
9,name9,38,6,I
</code></pre>

<p>Couple questions I had:</p>

<ol>
<li>There's a lot of external referencing from other activities in the 'Copy Data' query like <code>@{activity('...').output.firstRow.CurrentChangeTrackingVersion</code>. Is there a way to know the appropriate syntax to referencing external activities? Can't find any good documentation the syntax, like what <code>.firstRow</code> is or what the <code>changetable</code> output looks like. I can't replicate this query in SSMS, which makes it a bit of a black box for me.</li>
<li><strong>SYS_CHANGE_OPERATION</strong> appears in the <code>SELECT</code> with no table name prefix. Is this directly querying from the table in SourceDataset? (It points to data_source_table, which has <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/system-functions/changetable-transact-sql?view=sql-server-2017#changetable-changes"" rel=""nofollow noreferrer"">table tracking enabled</a>) My main confusion stems from how table tracking information is stored in the enabled tables. Is there a way to show all the table's tracked changes in SSMS? I see <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/system-functions/changetable-transact-sql?view=sql-server-2017#return-values"" rel=""nofollow noreferrer"">some documentation on what the return values</a>, but it's hard for me to visualize it without seeing it on the table, so an output query of some return values would be nice. </li>
<li>LookupLastChangeTracking activity queries in all rows from a table (which when I checked, is just one row), but LookupCurrentChangeTracking activity uses a <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/track-changes/work-with-change-tracking-sql-server?view=sql-server-2017#Obtain"" rel=""nofollow noreferrer"">CHANGE_TRACKING function</a> to pull the version of the data sink in table_store_ChangeTracking_version. Why does it use a function when the data sink's version is already recorded in table_store_ChangeTracking_version? </li>
</ol>

<p>Sorry for the many questions, but I can't find any way to make this learning curve a bit less steep. Any guides or resources would be awesome!</p>
","<sql-server><azure><azure-sql-database><azure-data-factory><change-tracking>","2019-06-03 20:17:25","6737","0","1","56452767","<p>There is an article to get the same thing done from the UI and it will help you understand it better .
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-change-tracking-feature-portal"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-change-tracking-feature-portal</a> .</p>

<p>1 . These are the Lookup activity ,. very straight forward , please read about them here .
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity</a></p>

<p>2.SYS_CHANGE_OPERATION is a column on data_source_table and so that should be fine . Regarding the details on the how the change tracking (CT) is stored , I am not sure if all the system table are exposed on Azure SQL , but we did had few table on the on-prem version of the SQL which could be queried if needed . But for this exercise I think that will be an over kill .</p>
"
"56430542","Source and Sink data from/to Azure Data Lake Store gen1 with Azure data factory's (ADF) Data Flow (DF)","<p>I have a Azure Data Lake Store gen1 (ADLS-1) and a Azure Data Factory (ADF) (V2) with Data Flow (DF). When I create a new DF in ADF and select in the Source and/or Sink node a dataset from ADLS-1, I get the following validation` error (in DF):</p>

<p><code>source1
AzureDataLakeStore does not support MSI authentication in Data Flow.</code></p>

<p>Does this mean that I cannot use DF with ADLS-1 or is this some kind of authentication problem?</p>

<p>List of thing I've tried:</p>

<ul>
<li>I have given the ADF resource an Owner role in Access control (IAM) of the ADLS-1</li>
<li>I have given the ADF resource all (read, write, etc) permissions in the ADLS-1 folder of the dataset</li>
<li>I can copy data from and to the ADLS-1 in a ADF pipeline (so outside DF)</li>
<li>I can select datasets in the source and sink node of DF for datasets from ADLS-2 (gen 2) (so here I didn't get the error)</li>
<li>I can create a pipeline which copies first a dataset from ADLS-1 to ADLS-2 and then process it with DF (and copy it back). This workaround is pretty tedious and I do not have a ADLS-2 in production (for now). </li>
<li>It says <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-store"" rel=""noreferrer"">here</a> that the supported capabilities for ADLS-1 includes Mapping data flow (DF).</li>
</ul>

<p>If someone knows a method to use DF with ADLS-1 or rule out its capabilities that would be pretty helpful. </p>
","<dataflow><azure-data-factory>","2019-06-03 15:39:44","1349","5","1","56432694","<p>MSI auth is not yet currently supported in Mapping Data Flows in ADF.</p>
"
"56430028","how to handle variable column headers (row location) in azure data factory","<p>We ingest data from multiple customers and have no control over the format of the data. The data pertains to the same subject matter but the file names, column names, headers, and row headings are all variable.
Schema drift capabilities in data flow mapping looks like it will handle the variable file and column names but i'm not sure how best to handle the fact that the column headings could be on row 1 or 2 or 10, etc.
Previously we used some Python code to figure this out, is there any capability within Data Factory to accommodate this level of variability?</p>
","<azure><azure-data-factory>","2019-06-03 15:04:30","1077","0","1","56454973","<p>You will need a rule to determine which row has the headers. Then, you can use a Filter transform to filter out the header row from the data rows.</p>

<p>But if you need the names of the headers in your flow, then you'll need to first run that file through a separate data flow that rewrites the file with the header row as the first row.</p>

<p>You can do this by adding 2 sources to a data flow, both pointing to the same file. Then, filter OUT the header row from one source and filter IN just the header row in the 2nd source.</p>

<p>Union those 2 streams back together and write to a new file in the Sink.</p>
"
"56427600","How to Convert ssis Rawfiles to Parquet","<p>Currently we are using rawfiles destination in SSIS and i'm wondering is there way to create parquet destination in ssis or converting rawfiles into parquet ? Any help regarding this is much appreciated.</p>
","<azure><ssis><ssis-2012><azure-data-factory>","2019-06-03 12:40:53","1829","1","1","56436068","<p>If you are using Azure Data Factory, you can refer to the following link to learn more about writing to parquet files:</p>

<ul>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-parquet?view=tfs-2018"" rel=""nofollow noreferrer"">Parquet format in Azure Data Factory</a></li>
</ul>

<p>If you are using SSIS, there is no parquet destination component to be able to write data into parquet files or to convert Raw files into parquet, but there are some component that let you write into HDFS files which may helps in some workaounds:</p>

<ul>
<li><a href=""https://learn.microsoft.com/en-us/sql/integration-services/hadoop-and-hdfs-support-in-integration-services-ssis?view=sql-server-2017"" rel=""nofollow noreferrer"">Hadoop and HDFS Support in Integration Services (SSIS)</a></li>
<li><a href=""https://learn.microsoft.com/en-us/sql/integration-services/connection-manager/hadoop-connection-manager?view=sql-server-2017"" rel=""nofollow noreferrer"">Hadoop Connection Manager</a></li>
<li><a href=""https://learn.microsoft.com/en-us/sql/integration-services/data-flow/hdfs-file-destination?view=sql-server-2017"" rel=""nofollow noreferrer"">HDFS File Destination</a></li>
</ul>
"
"56426673","ADF (azure data factory) copy data from SQL Server to Cosmos DB with field contain json object","<p>I have SQL Database contain one to many relations between tables. I wrote a query so that it contains rows with fields contains json (for related table rows). 
Here is the query - </p>

<pre><code>select msg.MessageId as id
      ,msg.CreatedDate as [CreatedDate]
      ,
      (select [RecipientTypeId] as [RecipientTypeId]
      ,[RecipientId] as [RecipientId]
      ,[mr.CreatedDate] as [CreatedDate]
      ,[IsRead] as [IsRead]
      ,[ReadDate] as [ReadDate]
       from [dbo].[MsgRecipients] mr
       where msg.messageid = mr.messageid  FOR JSON PATH, INCLUDE_NULL_VALUES) as Recipients
       ,
       (select 
       [Data] as [Data]
      ,[Value] as [Value]
      ,[mc.SomeId] as [SomeId]
       from [dbo].[MessageContent] mc
       where msg.messageid = mc.messageid  FOR JSON PATH, WITHOUT_ARRAY_WRAPPER, INCLUDE_NULL_VALUES) as MessageContent
       from [dbo].[Messages] msg
</code></pre>

<p>Following is the results of query - </p>

<p><a href=""https://i.stack.imgur.com/CKQB7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CKQB7.png"" alt=""enter image description here""></a></p>

<p>Here I am getting two fields with Recipients contains array of objects and Messages contains set of object.</p>

<p>In ADF I wrote this - </p>

<pre><code>{
    ""name"": ""CopyPipeline_SQL_to_Cosmos"",
    ""properties"": {
        ""description"": ""CopyPipeline_SQL_to_Cosmos"",
        ""activities"": [
            {
                ""name"": ""Copy_lbp"",
                ""type"": ""Copy"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""userProperties"": [
                    {
                        ""name"": ""Destination"",
                        ""value"": ""Messages1""
                    }
                ],
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""AzureSqlSource"",
                        ""sqlReaderQuery"": {
                            ""value"": ""select msg.MessageId as id, msg.CreatedDate as [CreatedDate], (select [RecipientTypeId] as [RecipientTypeId], [RecipientId] as [RecipientId], [mr.CreatedDate] as [CreatedDate], [ReadDate] as [ReadDate] from [dbo].[MsgRecipients] mr where msg.messageid = mr.messageid  FOR JSON PATH, INCLUDE_NULL_VALUES) as Recipients, (select  [Data] as [Data], [Value] as [Value], [mc.SomeId] as [SomeId] from [dbo].[MessageContent] mc where msg.messageid = mc.messageid  FOR JSON PATH, WITHOUT_ARRAY_WRAPPER, INCLUDE_NULL_VALUES) as MessageContent from [dbo].[Messages] msg  where CreatedDate &gt;= '@{formatDateTime(pipeline().parameters.windowStart, 'yyyy-MM-dd HH:mm' )}' AND CreatedDate &lt; '@{formatDateTime(pipeline().parameters.windowEnd, 'yyyy-MM-dd HH:mm' )}'"",
                            ""type"": ""Expression""
                        }
                    },
                    ""sink"": {
                        ""type"": ""DocumentDbCollectionSink"",
                        ""nestingSeparator"": """",
                        ""writeBatchSize"": 10000,
                        ""writeBehavior"": ""upsert""
                    },
                    ""enableStaging"": false
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""SourceDataset_lbp"",
                        ""type"": ""DatasetReference""
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""DestinationDataset_lbp"",
                        ""type"": ""DatasetReference""
                    }
                ]
            },
            {
                ""name"": ""Custom1"",
                ""type"": ""Custom"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                }
            }
        ],
        ""parameters"": {
            ""windowStart"": {
                ""type"": ""String""
            },
            ""windowEnd"": {
                ""type"": ""String""
            }
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>

<p>I am getting records as this -> </p>

<p><a href=""https://i.stack.imgur.com/zKSD8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zKSD8.png"" alt=""enter image description here""></a></p>

<p>In above image, You can notice that in Recipients and MessageContent it has added whole json. </p>

<p>Whereas I am expecting this -</p>

<p><a href=""https://i.stack.imgur.com/BtsDL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BtsDL.png"" alt=""enter image description here""></a></p>
","<azure><azure-sql-database><azure-data-factory><azure-cosmosdb-sqlapi>","2019-06-03 11:45:29","498","0","1","56438877","<p>I don't have the data handy so I cannot test , but I am think that you will have to use the ""Schema mapping"" .</p>

<p>Please read about structure here 
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#alternative-schema-mapping"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#alternative-schema-mapping</a></p>
"
"56423718","Values to configure pagination rules in Rest API connector in azure data factory to get data and load into sql database","<p>Need values to configure in Rest API connector in Azure data factory recently getting only 1000 records. Need how to setup configuration so that the values can be looped.(Pagination rules to configure continuous token).</p>
","<azure><azure-data-factory>","2019-06-03 08:35:08","1818","0","1","56436989","<p>Based on the official <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest#pagination-support"" rel=""nofollow noreferrer"">document</a>,ADF pagination rules only support below patterns.</p>

<p><a href=""https://i.stack.imgur.com/FxARs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FxARs.png"" alt=""enter image description here""></a></p>

<p>I think you could adopt the pattern: <code>Next request’s query parameter = property value in current response body</code> to set the page size, then pass it into next request as parameter.</p>

<p>As mentioned in the rules, the connector will stop iterating when it gets HTTP status code 204 (No Content), or any of the JSONPath expression in ""paginationRules"" returns null.</p>
"
"56416169","Execute Snowflake Stored Procedure with Azure Data Factory","<p>I have a linked service is up and running from which I was able to run simple queries.</p>

<p>My intention is to encapsulate the majority of the logic of the ETL in a stored procedure, scheduled and executed from Azure Data Factory ""Lookup"" component.
However, it seems that I'm unable to execute it with the regular <code>CALL test_snowflake_sp()</code> that works so well from the workbench.</p>

<p>My stored procedure is comprised of a simple query followed by a ""failure""/""success"" message, I get the following error message:</p>

<blockquote>
  <p>Activity Get Value failed: Failure happened on 'Source' side.   ErrorCode=UserErrorOdbcInvalidQueryString,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The following ODBC Query is not valid: &apos;CALL test_snowflake_sp()&apos;.,Source=Microsoft.DataTransfer.Runtime.GenericOdbcConnectors,'</p>
</blockquote>

<p>Unfortunately I don't have a Python access to run it in code.</p>

<p>Has anyone figured it out?</p>

<p>Thanks in advance</p>
","<azure><azure-data-factory><snowflake-cloud-data-platform>","2019-06-02 14:56:18","3468","1","1","58002003","<p>You can call Snowflake stored procs fine from a Lookup using exactly the syntax from your example.  Be sure you grant the Data Factory user ""usage"" permissions on the proc, and re-grant any time you ""create or replace"" the proc  (""grant usage on procedure test_snowflake_sp() to role datafactory""  assuming you have created a role for the ADF user).</p>
"
"56414334","How to import your example files into my Azure Data Factory","<p>I believe many of you have ADF experiences, and may be have seen Mark Kromer's example of azure data flows (<a href=""https://github.com/kromerm/adfdataflowdocs"" rel=""nofollow noreferrer"">https://github.com/kromerm/adfdataflowdocs</a>)</p>

<p>I am a beginner using ADF and azure Data Flows especially. I am very curious of these examples, and I really want to import all your examples files (json) to my newly created data factory. It must be an easier way than creating all activities, connections, datasets and others manually. The templates are of course good but I want to test out your example code in my azure portal and in my data factory.</p>

<p>And just a second question: I am a SSIS man used control flows with master packages executing other packages. When I am now building a data warehouse in ADF with many dimension tables and fact tables, is it best practice to have separate data flows or should I build general data flows that either have many parallel upserts to different dimensions? I think I need some guiding here</p>

<p>Thank you</p>

<p>Regards Geir</p>
","<azure><azure-sql-database><azure-storage><azure-data-factory>","2019-06-02 10:49:46","48","-1","1","56421169","<p>I've been publishing those example data flows as ADF pipeline templates.</p>

<p>In your browser, go to ADF and then click Factory Resources > Pipeline from Template. You should see a Data Flow category. Many of the examples are in there.</p>

<p>In ADF, you can also have master / child packages. You'll use Execute Pipeline in ADF instead of execute package.</p>

<p>ADF supports re-use and generalization of data flow patterns, so that you could process multiple dimension tables in a single data flow. You'll have to weigh the value of doing that vs. supporting it long-term as well as the danger of creating a very complex data flow.</p>
"
"56412782","Powershell JSON transformation removing unicode escape chars without removing literal \n","<p>My issue us similiar to this question:</p>

<p><a href=""https://stackoverflow.com/questions/32959431/json-file-to-powershell-and-back-to-json-file"">Json file to powershell and back to json file</a></p>

<p>When importing and exporting ARM templates in powershell, using <code>Convert-FromJson</code> and <code>Convert-ToJson</code>, introduces unicode escape sequences.</p>

<p>I used the code <a href=""https://www.cryingcloud.com/blog/2017/05/02/replacefix-unicode-characters-created-by-convertto-json-in-powershell-for-arm-templates"" rel=""nofollow noreferrer"">here</a> to unescape again.</p>

<p>Some example code (mutltiline for clarity):</p>

<pre><code>$armADF = Get-Content -Path $armFile -Raw | ConvertFrom-Json
$armADFString = $armADF | ConvertTo-Json -Depth 50
$armADFString | 
    ForEach-Object { [System.Text.RegularExpressions.Regex]::Unescape($_) } | 
    Out-File $outputFile
</code></pre>

<p><a href=""https://learn.microsoft.com/en-us/dotnet/api/system.text.regularexpressions.regex.unescape?view=netframework-4.8"" rel=""nofollow noreferrer"">Here's</a> the doco I've been reading for <code>Unescape</code></p>

<p>Results in the the output file being identical <em>except</em> that all instances of literal <code>\n</code> (that were in the original JSON file) are turned into actual carriage returns. Which breaks the ARM template.</p>

<p>If I don't include the Unescape code, the <code>\n</code> are preserved but so are the unicode characters which also breaks the ARM template.</p>

<p>It seems like I need to pre-escape the <code>\n</code> so when I call Unescape they are turned into nice little <code>\n</code>. I've tried a couple of things like adding this before calling unescape.</p>

<pre><code>$armADFString = $armADFString -replace(""\\n"",""\u000A"")
</code></pre>

<p>Which does not give me the results I need.</p>

<p>Anyone come across this and solved it? Any accomplished escape artists?</p>
","<regex><powershell><azure-rm-template><azure-data-factory>","2019-06-02 06:49:00","1534","4","1","56413007","<p>I reread the Unescape doco and noticed that it would also basically remove leading <code>\</code> characters so I tried this unlikely bit of code:</p>

<pre><code>$armADF = Get-Content -Path $armFile -Raw | ConvertFrom-Json
$armADFString = $armADF | ConvertTo-Json -Depth 50
$armADFString = $armADFString -replace(""\\n"",""\\n"")
$armADFString | 
    ForEach-Object { [System.Text.RegularExpressions.Regex]::Unescape($_) } | 
    Out-File $outputFile
</code></pre>

<p>Of course - replacing <code>\\n</code> with <code>\\n</code> makes complete sense :|</p>

<p>More than happy for anyone to pose a more elegant solution.</p>

<p>EDIT: I am deploying ADF ARM templates which are themselves JSON based. TO cut a long story short I also found I needed to add this to stop it unescaping legitimately escaped quotes:</p>

<pre><code>$armADFString = $armADFString -replace('\\""','\\""')
</code></pre>
"
"56401786","dataset validation policy minimumSizeMB for Azure Blob","<p>If I set minimumSizeMB policy for an external dataset of type AzureBlob, does it mean that ADF is going to check total file size in FolderPath (I could have hundreds of files in a given FolderPath with varying size) or it is going to consider only those files in the FolderPath as input which qualify minimumSizeMB?</p>
","<azure><azure-data-factory>","2019-05-31 21:13:26","24","0","1","56402139","<p>I confirmed through an experiment that it's the total size of all the files</p>
"
"56400863","Best practices for parameterizing load of multiple CSV files in Data Factory","<p>I am experimenting with Azure Data Factory to replace some other data-load solutions we currently have, and I'm struggling with finding the best way to organize and parameterize the pipelines to provide the scalability we need.</p>

<p>Our typical pattern is that we build an integration for a particular Platform. This ""integration"" is essentially the mapping and transform of fields from their data files (CSVs) into our Stage1 SQL database, and by the time the data lands in there, the data types should be set properly and the indexes set.</p>

<p>Within each Platform, we have Customers. Each Customer has their own set of data files that get processed in that Customer context -- within the scope of a Platform, all Customer files follow the same schema (or close to it), but they all get sent to us separately. If you looked at our incoming file store, it might look like (simplified, there are 20-30 source datasets per customer depending on platform):</p>

<ol>
<li>Platform

<ol>
<li>Customer A

<ol>
<li>Employees.csv</li>
<li>PayPeriods.csv</li>
<li>etc</li>
</ol></li>
<li>Customer B

<ol>
<li>Employees.csv</li>
<li>PayPeriods.csv</li>
<li>etc</li>
</ol></li>
</ol></li>
</ol>

<p>Each customer lands in their own SQL schema. So after processing the above, I should have CustomerA.Employees and CustomerB.Employees tables. (This allows a little bit of schema drift between customers, which does happen on some platforms. We handle it later in our stage 2 ETL process.)</p>

<p>What I'm trying to figure out is:</p>

<p>What is the best way to setup ADF so I can effectively manage one set of mappings per platform, and automatically accommodate any new customers we add to that platform without having to change the pipeline/flow?</p>

<p>My current thinking is to have one pipeline per platform, and one dataflow per file per platform. The pipeline has a variable, ""schemaname"", which is set using the path of the file that triggered it (e.g. ""CustomerA""). Then, depending on file name, there is a branching conditional that will fire the right dataflow. E.g. if it's ""employees.csv"" it runs one dataflow, if it's ""payperiods.csv"" it loads a different dataflow. Also, they'd all be using the same generic target sink datasource, the table name being parameterized and those parameters being set in the pipeline using the schema variable and the filename from the conditional branch.</p>

<p>Are there any pitfalls to setting it up this way? Am I thinking about this correctly?</p>
","<azure-data-factory>","2019-05-31 19:38:24","561","2","1","56421389","<p>This sounds solid. Just be aware that you if you define column-specific mappings with expressions that expect those columns to be present, you may have data flow execution failures if those columns are not present in your customer source files.</p>

<p>The ways to protect against that in ADF Data Flow is to use column patterns. This will allow you to define mappings that are generic and more flexible.</p>
"
"56394092","Can't connect to SFTP (with private key file) in Copy Data Tool","<p>I am trying to copy data from SFTP to blob but got stuck when creating SFTP source.</p>

<p>I have the connection details and can easily connect on Filezilla or WinSCP. However, I am unable to get it to work in Azure data factory.</p>

<p>I am not using code but the user interface.</p>

<p>The connection details on the page creating the SFTP source:
Connect via integration runtime: AutoResolveIntegrationRuntime (default)</p>

<pre><code>Host: xyz

Port: 22 (can't remove it as it doesn't like it)

SSH Host Key Validation: Enable SSH Host Key Validation

SSH Host Key Finger-print: taken from WinSCP - Session - Server/protocol information

Authentication type: SSH Public Key Authentication   -can't use basic as the private key holds the security info

User name:XXX

Private Key Type: Use Key Content

Private key content: loaded the .ppk file, tried also tried loading the .pem file and got different errors

Pass Phrase: none
</code></pre>

<p>When setting up this sftp in WinSCP or FileZilla it automatically converted the provided .pem file into .ppk.
When I loaded the .ppk file into ADF I got an error: Invalid Sftp credential provided for 'SshPublicKey' authentication type. 
When I loaded the .pem file I got: Meet network issue when connect to Sftp server 'spiderftp.firstgroup.com', SocketErrorCode: 'TimedOut'. </p>

<p>I have also tried 'Disable SSH Host Key Validation' in SSH Host Key Validation and made no difference.</p>

<p>I have also opened the .ppk file in PuttyGen and used that host key finger print and still no luck.</p>

<p>Only getting these 2 errors depending on which file I load.</p>

<p>Can't find anything about this online so would be grateful for some advice.</p>
","<azure-data-factory>","2019-05-31 11:17:52","7371","2","2","56416466","<p>Have you read this note in this doc?</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sftp#using-ssh-public-key-authentication"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-sftp#using-ssh-public-key-authentication</a></p>

<p>SFTP connector supports RSA/DSA OpenSSH key. Make sure your key file content starts with ""-----BEGIN [RSA/DSA] PRIVATE KEY-----"". If the private key file is a ppk-format file, please use Putty tool to convert from .ppk to OpenSSH format.</p>
"
"56394092","Can't connect to SFTP (with private key file) in Copy Data Tool","<p>I am trying to copy data from SFTP to blob but got stuck when creating SFTP source.</p>

<p>I have the connection details and can easily connect on Filezilla or WinSCP. However, I am unable to get it to work in Azure data factory.</p>

<p>I am not using code but the user interface.</p>

<p>The connection details on the page creating the SFTP source:
Connect via integration runtime: AutoResolveIntegrationRuntime (default)</p>

<pre><code>Host: xyz

Port: 22 (can't remove it as it doesn't like it)

SSH Host Key Validation: Enable SSH Host Key Validation

SSH Host Key Finger-print: taken from WinSCP - Session - Server/protocol information

Authentication type: SSH Public Key Authentication   -can't use basic as the private key holds the security info

User name:XXX

Private Key Type: Use Key Content

Private key content: loaded the .ppk file, tried also tried loading the .pem file and got different errors

Pass Phrase: none
</code></pre>

<p>When setting up this sftp in WinSCP or FileZilla it automatically converted the provided .pem file into .ppk.
When I loaded the .ppk file into ADF I got an error: Invalid Sftp credential provided for 'SshPublicKey' authentication type. 
When I loaded the .pem file I got: Meet network issue when connect to Sftp server 'spiderftp.firstgroup.com', SocketErrorCode: 'TimedOut'. </p>

<p>I have also tried 'Disable SSH Host Key Validation' in SSH Host Key Validation and made no difference.</p>

<p>I have also opened the .ppk file in PuttyGen and used that host key finger print and still no luck.</p>

<p>Only getting these 2 errors depending on which file I load.</p>

<p>Can't find anything about this online so would be grateful for some advice.</p>
","<azure-data-factory>","2019-05-31 11:17:52","7371","2","2","64785685","<p>Got this working today. Like you, could connect using WinSCP and failed when using ADF.</p>
<p>The link Fang Liu shared contains our answers, but <strong>my issue was not the private key</strong>. I suspect Fang's suggestion resolved your problem and I'm sharing my answer here to help others who may encounter similar.</p>
<p><strong>My issue:</strong>
When using Private Key Authentication in ADF the password becomes a <strong>Pass Phrase</strong> and you no longer have the ability to supply a password. <strong>To overcome the problem we disabled password authentication</strong> for the user and the SFTP connection started working.</p>
<p>As stated in the documentation. The <strong>Pass Phrase</strong> is used to <strong>decrypt</strong> the private <strong>key if</strong> it is <strong>encrypted</strong>.</p>
<p><strong>Also worth noting:</strong></p>
<ol>
<li>If you store the contents of the private key in Key Vault you need
to base64 encode the entire contents of the exported key and use
that string. This includes &quot;-----BEGIN RSA PRIVATE KEY-----&quot; and the
end. The same applies if you want to paste the value into the
textbox of the SFTP linked service edit screen.</li>
<li>I did not try to manually edit the JSON of the Linked Service to explicitly provide a password and this could be workaround for someone to test if they are unable to disable the password.</li>
<li>I used PuTTYGen to export the PPK to a private key and had the same fingerprint issue too so I just disabled cert validation. Funnily you can use the fingerprint provided by the error and it passes validation so not sure where the bug lies. :-)</li>
</ol>
"
"56376270","Lookup activity in an Azure Data Factory Data Copy Source - Syntax Error","<p>Working through this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-portal"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-portal</a></p>

<p>I've one Lookup activity called GetCurrentWatermarkValue with sqlReaderQuery: </p>

<pre><code>Select WaterMarkValue as CurrentWatermarkValue\nfrom WatermarkTable
</code></pre>

<p>I've another activity called GetNewWatermarkValue with sqlReaderQuery:</p>

<pre><code>select max(createdon) as NewWatermarkValue from shipment
</code></pre>

<p>I'm then trying to use them both in the source for a Data Copy activity using </p>

<pre><code>select *
from Shipment
where CreatedOn &gt; '@{activity('GetCurrentWatermarkValue').output.firstRow.CurrentWatermarkValue}' 
and CreatedOn &lt;= '@{activity('GetNewWatermarkValue').output.firstRow.NewWatermarkValue}'
</code></pre>

<p>The Preview data button is greyed (but enabled when i remove the where condition) so there is obviouusly something wrong</p>

<p>After I've set the sink i try to set up the Mapping. Clicking Import schema on the Mapping tab gives:</p>

<pre><code>A database operation failed with the following error: 'Incorrect syntax near 'GetCurrentWatermarkValue'.'. Activity ID:98794aa9-c866-48d6-b9ff-9cb277bac6ed
</code></pre>

<p>I thought maybe I should be using the add dynamic content option but that just gives</p>

<pre><code>Query is required
</code></pre>

<p>I've read somewhere that the when First Row Only is set on the Lookup activity the text after firstRow. should be [TableName] but that doesn't seem right.</p>

<p>Lookup: </p>

<pre><code>{
    ""name"": ""GetCurrentWatermarkValue"",
    ""type"": ""Lookup"",
    ""policy"": ...,
    ""typeProperties"": {
        ""source"": {
            ""type"": ""AzureSqlSource"",
            ""sqlReaderQuery"": ""Select WaterMarkValue as CurrentWatermarkValue\nfrom WatermarkTable""
        },
        ""dataset"": {
            ""referenceName"": ""WatermarkTable"",
            ""type"": ""DatasetReference""
        }
    }
}
</code></pre>

<p>Lookup:</p>

<pre><code>{
    ""name"": ""GetNewWatermarkValue"",
    ""type"": ""Lookup"",
    ""policy"": ...,
    ""typeProperties"": {
        ""source"": {
            ""type"": ""AzureSqlSource"",
            ""sqlReaderQuery"": ""select max(createdon) as NewWatermarkValue from shipment""
        },
        ""dataset"": {
            ""referenceName"": ""ShipmentsTable"",
            ""type"": ""DatasetReference""
        }
    }
}
</code></pre>

<p>Data Copy:</p>

<pre><code>{
    ""name"": ""ArchiveShipments"",
    ""type"": ""Copy"",
    ""dependsOn"": [
        {
            ""activity"": ""GetCurrentWatermarkValue"",
            ""dependencyConditions"": [
                ""Succeeded""
            ]
        },
        {
            ""activity"": ""GetNewWatermarkValue"",
            ""dependencyConditions"": [
                ""Succeeded""
            ]
        }
    ],
    ""policy"": ...,
    ""typeProperties"": {
        ""source"": {
            ""type"": ""AzureSqlSource"",
            ""sqlReaderQuery"": {
                ""value"": ""select *\nfrom Shipment\nwhere CreatedOn &gt; '@{activity('GetCurrentWatermarkValue').output.firstRow.CurrentWatermarkValue}' \nand CreatedOn &lt;= '@{activity('GetNewWatermarkValue').output.firstRow.NewWatermarkValue}'"",
                ""type"": ""Expression""
            }
        },
        ""sink"": {
            ""type"": ""AzureSqlSink""
        },
        ""enableStaging"": false
    },
    ""inputs"": [
        {
            ""referenceName"": ""ShipmentsTable"",
            ""type"": ""DatasetReference""
        }
    ],
    ""outputs"": [
        {
            ""referenceName"": ""ShipmentArchiveTable"",
            ""type"": ""DatasetReference""
        }
    ]
}
</code></pre>
","<azure><azure-data-factory>","2019-05-30 10:18:16","894","0","1","56384634","<p>This looks like a syntax issue, try this is the copy activity query:</p>

<pre><code>@{CONCAT('select * from Shipment where CreatedOn &gt; ', activity('GetCurrentWatermarkValue').output.firstRow.CurrentWatermarkValue, ' and CreatedOn &lt;= ', activity('GetNewWatermarkValue').output.firstRow.NewWatermarkValue}
</code></pre>

<p>Dont use a mapping unless you want to manually set it, if column names are the same, then just click ""Clear"".</p>

<p>Hope this helped!</p>
"
"56376102","Check Name Availability of Azure Data Factory (ADF), Time Series Insights (TSI) and Stream Analytics Job resources","<p>I am looking for a API or documentation for querying for name availability of Azure Data Factory (ADF), Time Series Insights (TSI) and Stream Analytics Job resources similar to <a href=""https://learn.microsoft.com/en-us/rest/api/keyvault/vaults/checknameavailability"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/keyvault/vaults/checknameavailability</a>. </p>

<p>I have tried looking at azure-arm-datafactory, azure-arm-streamanalytics node libraries but couldn't find functionality to check name availability of resources.  </p>

<p>I am looking for something similar to below.. </p>

<pre><code>import KeyVaultMangementClient from 'azure-arm-keyvault';
const client = new KeyVaultMangementClient(this._credentials, this._subscriptionId);
        return client.vaults.checkNameAvailability({name: keyVaultName})
            .then((result: any) =&gt; {
                console.log(result.nameAvailable);
                return Promise.resolve(result.nameAvailable);
            });
</code></pre>
","<azure><azure-data-factory><azure-stream-analytics><azure-timeseries-insights>","2019-05-30 10:09:02","190","1","1","56393368","<p>AFAIK this (check name availability) feature is currently unsupported for ADF, TSI, Stream Analytics. So I would recommend you to please add a new feature request on <a href=""https://feedback.azure.com/forums/34192--general-feedback"" rel=""nofollow noreferrer"">uservoice</a> with your suggestion(s) and let us know the link when you do it so that others can vote as well and which would in turn would raise visibility and priority on it.</p>
"
"56365183","Adding custom attribute to SQL DB using dynamic parameter in Azure Data factory","<p>I have a CSV file that I need to ingest in MS SQL DB using ADF V2. Now thats a simple thing to achieve and i am able to do it. But I each file also has a Asset ID  associated with it which is dynamically generated you can think of it as a GUID. </p>

<p>Hence essentially, it is there so that in future I have a way to associate which record belong to which CSV as there can be multiple CSVs ingested.</p>

<p>I am able to retrieve the asset ID in form of dynamic parameter till Pipeline but I am not able to add it to the database. Essentially there will be one additional column called assetID against each record in DB. But how to populate it with the dynamic parameter, I am not able to find a way. Can any body help? or if there is any other way to associate a Unique ID to those records of file in DB. </p>
","<azure><azure-data-lake><azure-data-factory>","2019-05-29 16:44:42","58","0","1","56373218","<p>There are a couple of ways to go about this :</p>

<ol>
<li><p>For each iteration(single csv file), use a Custom activity, read the csv file, use the pipeline parameter for the attribute you want to add and add it to the DB and then write the record to the table.</p></li>
<li><p>You can use a Stored Procedure activity and pass the parameter as shown in the below screenshot to add the column that you want to.</p></li>
</ol>

<p><a href=""https://i.stack.imgur.com/UGu4f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UGu4f.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"56358751","azure data factory v2 copy data activity recursive","<p>I am new to azure Data factory v2</p>

<p>I have a folder having 2 files F1.csv and F2.csv in a blob storage.</p>

<p>I have a created a copy data pipeline activity to load the data from file to a table in azure DWH with 3 parameters and copy recursively was made to false.</p>

<p>Parameter1: container</p>

<p>Parameter2: directory</p>

<p>Parameter3: F1.csv</p>

<p>executed successfully when used the above parameters for the copy data activity.</p>

<p>But the data has been loaded from two files, only one file has given as parameter for the activity</p>
","<azure><pipeline><azure-data-factory>","2019-05-29 10:47:08","1098","0","1","56366649","<p>Can you please check the ""wildcard file name"" parameter ( Select the pipeline and look under the source tab )  ?</p>

<p>If it is set as  <em>.</em>  please remove that and it should work . </p>
"
"56350179","Read SharePoint online lists from Azure Data Factory","<p>I'm trying to read data from a list in the SharePoint online, using Azure Data Factory. It should be possible using OData. I'm struggling to authenticate from ADF. </p>

<p>From Chrome I can access the list data without problems using URL like this:</p>

<p><a href=""https://mycompany.sharepoint.com/_vti_bin/listdata.svc/MyListName"" rel=""nofollow noreferrer"">https://mycompany.sharepoint.com/_vti_bin/listdata.svc/MyListName</a></p>

<p>I have account name that can read the list, and its password. I'm struggling with authentication in ADF. I tried basic and windows, with no success. For the others, I do not know what to set...</p>

<p>Short step by step instructions of how to read a SharePoint online list in ADF is highly welcome.</p>
","<azure><odata><sharepoint-online><azure-data-factory>","2019-05-28 21:17:35","1569","0","1","56695544","<p>I am in the process of writing out how to do so in a MSDN thread:</p>

<p><a href=""https://social.msdn.microsoft.com/Forums/en-US/85ab91ef-0336-4832-b750-2ed007b030a9/access-sharepoint-list-data-using-adf?forum=AzureDataFactory"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/en-US/85ab91ef-0336-4832-b750-2ed007b030a9/access-sharepoint-list-data-using-adf?forum=AzureDataFactory</a></p>
"
"56342061","How to copy the data based on the Last modified time using Azure data factory from FTP Source?","<p>I am trying to create a pipeline where the pipeline needs to trigger only when the file is modified in the FTP Server.</p>

<p>I have used GET METADATA activity to get the lastmodified date and used IF activity to copy the data.
Below is the expression i have used in IF activity</p>

<p>@less(activity('GET_DATA').output.lastModified,formatDateTime(utcnow(),'yyyy-MM-dd HH:mm:ss'))</p>

<p>I would want the lasted updated file to be copied into the Destination.</p>

<p>So Can anyone please suggest on how to model the pipeline for this?</p>
","<azure><azure-data-factory>","2019-05-28 12:15:06","2476","1","1","56416194","<p>Here is a guide for incremental load. Hope it helps
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-lastmodified-copy-data-tool"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-lastmodified-copy-data-tool</a></p>

<p>Also there is a template for incremental load.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/solution-template-copy-new-files-lastmodifieddate"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/solution-template-copy-new-files-lastmodifieddate</a></p>
"
"56337163","when try to add dynamic file name (linked server) in azure data factory V2 error encountered","<p>I am new to the Azure Data factory V2 and blob storage.
When try to add the file connection(linked server) in copy data from blob storage dynamically the following error is encountered while trying to map the columns by importing the schema from file </p>

<p><strong><em>""Failed to convert the value in 'container' property to 'System.String' type. Please make sure the payload structure and value are correct..""</em></strong></p>

<p><strong>I tried:</strong></p>

<p>used static parameters and assign the static parameters to the linked connection</p>
","<azure><azure-blob-storage><azure-data-factory>","2019-05-28 07:35:14","4745","2","1","56338953","<p>Please refer to my working steps:</p>

<p>1.Configuration of Azure Blob Storage Source DataSet:</p>

<p><a href=""https://i.stack.imgur.com/WcNQW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WcNQW.png"" alt=""enter image description here""></a></p>

<p>parameters:</p>

<p><a href=""https://i.stack.imgur.com/xrI8N.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xrI8N.png"" alt=""enter image description here""></a></p>

<p>2.Configuration of Azure Blob Storage Sink DataSet:</p>

<p><a href=""https://i.stack.imgur.com/CCZoN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CCZoN.png"" alt=""enter image description here""></a></p>

<p>3.Configuration of Copy Activity,you could modify the value of parameters here:</p>

<p><a href=""https://i.stack.imgur.com/elB9g.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/elB9g.png"" alt=""enter image description here""></a></p>

<p>4.Result:</p>

<p><a href=""https://i.stack.imgur.com/AMsdI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AMsdI.png"" alt=""enter image description here""></a></p>
"
"56335756","Files with different headers to copy data in azure dwh table","<p>I am new to the Azure Data Warehouse and Azure Data factory V2.</p>

<p>I Have a folder With 10 files in azure blob, Each file is having different headers but there will be a common 5 headers in all files.</p>

<p><strong>Things I have tried,</strong></p>

<p>Created 10 linked servers to the files and using copy data task moved the data to the specific tables.
But I required only 5 columns from those tables</p>

<p><strong>Question</strong></p>

<p>Is there any way that I can have a table with 5 columns and a dynamic file connection to load data from 5 files.</p>

<p><em>Example:</em>
File1 : 10 headers
File2 : 11 headers
File3 : 7 headers
.
.
.
.
File10 : 15 headers</p>

<p>There will be a common of 5 columns in these files and that data has to be loaded common Table with 5 columns.</p>
","<azure><azure-blob-storage><azure-data-factory><azure-synapse>","2019-05-28 05:50:37","398","1","3","56357482","<p>You could adopt below idea:</p>

<p>1.Use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">LookUp Activity</a> to grab the fileName list stored in your specific azure blob storage container or other paths. Please refer to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity#use-the-lookup-activity-result-in-a-subsequent-activity"" rel=""nofollow noreferrer"">link</a>.</p>

<p>2.Use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">ForEach Activity</a> to loop the output of LookUp Activity<code>[@activity('MyLookupActivity').output]</code>.</p>

<p>3.Inside the ForEach Activity, use copy activity and configure the source blob storage dataset path as  <code>specific containerName/dynamic fileName</code>.(Like the solution in this case:<a href=""https://stackoverflow.com/questions/56337163/when-try-to-add-dynamic-file-name-linked-server-in-azure-data-factory-v2-error/56338953#56338953"">when try to add dynamic file name (linked server) in azure data factory V2 error encountered</a>)</p>
"
"56335756","Files with different headers to copy data in azure dwh table","<p>I am new to the Azure Data Warehouse and Azure Data factory V2.</p>

<p>I Have a folder With 10 files in azure blob, Each file is having different headers but there will be a common 5 headers in all files.</p>

<p><strong>Things I have tried,</strong></p>

<p>Created 10 linked servers to the files and using copy data task moved the data to the specific tables.
But I required only 5 columns from those tables</p>

<p><strong>Question</strong></p>

<p>Is there any way that I can have a table with 5 columns and a dynamic file connection to load data from 5 files.</p>

<p><em>Example:</em>
File1 : 10 headers
File2 : 11 headers
File3 : 7 headers
.
.
.
.
File10 : 15 headers</p>

<p>There will be a common of 5 columns in these files and that data has to be loaded common Table with 5 columns.</p>
","<azure><azure-blob-storage><azure-data-factory><azure-synapse>","2019-05-28 05:50:37","398","1","3","56396979","<p>When you create external tables in Azure SQL Data Warehouse with Polybase, they can either point to an individual file or a set of files as long as they in the same folder and in the same format.</p>

<p><strong>Individual file</strong></p>

<pre><code>WTIH (
    DATA_SOURCE = yourDataSource,
    LOCATION = N'/input/file1.txt',
    FILE_FORMAT = yourFileFormat,
    ...
    etc
</code></pre>

<p><strong>Folder</strong></p>

<pre><code>WTIH (
    DATA_SOURCE = yourDataSource,
    LOCATION = N'/input/',
    FILE_FORMAT = yourFileFormat,
    ...
    etc
</code></pre>

<p>So what you could do is create an external table for each file.  The external tables are only metadata structures, the data doesn't actually exist in the database.
These tables must include all columns for each table.  Then create a <code>UNION</code> statement over the five tables, selecting the five columns only and use a <code>CTAS</code> to import the data into the database:</p>

<pre><code>CREATE TABLE dbo.yourTable
WITH
(
    DISTRIBUTION = ROUND_ROBIN,
    CLUSTERED COLUMNSTORE INDEX
)
AS
SELECT col1, col2, col3, col4, col5
FROM externalTable1

UNION ALL

SELECT col1, col2, col3, col4, col5
FROM externalTable2

UNION ALL

SELECT col1, col2, col3, col4, col5
FROM externalTable3

etc
....
</code></pre>

<p>I probably would not use Data Factory as Polybase can do most of the work for you.</p>
"
"56335756","Files with different headers to copy data in azure dwh table","<p>I am new to the Azure Data Warehouse and Azure Data factory V2.</p>

<p>I Have a folder With 10 files in azure blob, Each file is having different headers but there will be a common 5 headers in all files.</p>

<p><strong>Things I have tried,</strong></p>

<p>Created 10 linked servers to the files and using copy data task moved the data to the specific tables.
But I required only 5 columns from those tables</p>

<p><strong>Question</strong></p>

<p>Is there any way that I can have a table with 5 columns and a dynamic file connection to load data from 5 files.</p>

<p><em>Example:</em>
File1 : 10 headers
File2 : 11 headers
File3 : 7 headers
.
.
.
.
File10 : 15 headers</p>

<p>There will be a common of 5 columns in these files and that data has to be loaded common Table with 5 columns.</p>
","<azure><azure-blob-storage><azure-data-factory><azure-synapse>","2019-05-28 05:50:37","398","1","3","56465468","<p>I agree with @wBob that Polybase can handle this for you. As an alternative, however, you could use Azure Data Factory Data Flow to process all the files and select just the 5 columns you want, then output those results to a blob sink. Then use the blob for Polybase, which would simplify your Polybase requirements, or use an ADF Copy activity to write them to your SQLDW. There are lots of options in this space.</p>
"
"56329603","Azure Data Factory - Error ingesting data from MySql or Blob Storage","<p>Since last week we're facing some issues involving Data Factory Copy Activities. The issue occurs when the data is already staged into the Blob Storage and it starts to be copied into the SQL DW.</p>

<p>What we can see is that the Throughput rate decreases till we get the following message error:</p>

<p><em>{ ""errorCode"": ""2200"", ""message"": ""ErrorCode=UserErrorFailedToConnectToSqlServer,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Cannot connect to SQL Server: 'despesas.database.windows.net', Database: 'csc', User: 'master'.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Data.SqlClient.SqlException,Message=A connection was successfully established with the server, but then an error occurred during the pre-login handshake. (provider: TCP Provider, error: 0 - The semaphore timeout period has expired.),Source=.Net SqlClient Data Provider,SqlErrorNumber=121,Class=20,ErrorCode=-2146232060,State=0,Errors=[{Class=20,Number=121,State=0,Message=A connection was successfully established with the server, but then an error occurred during the pre-login handshake. (provider: TCP Provider, error: 0 - The semaphore timeout period has expired.),},],''Type=System.ComponentModel.Win32Exception,Message=The semaphore timeout period has expired,Source=,'"", ""failureType"": ""UserError"", ""target"": ""Copy Data1""</em></p>

<p>Notice that the issue started last week!</p>
","<mysql><azure-blob-storage><azure-data-factory>","2019-05-27 16:28:29","974","0","1","56331208","<p>Try creating an Azure Integration Runtime in the same region of the SQL database, sometimes the auto detect region doesnt work.</p>

<p>Create an Azure IR in that region, and create the linked service again, using the new Integration Runtime.</p>

<p>That should solve your problem.</p>

<p>Hope this helped!</p>
"
"56304505","Data Factory Blob Storage Linked Service with Managed Identity : The remote server returned an error: (403)","<p>I have created a linked service connection to a storage account using a managed identity and it successfully validates but when I try to use the linked service on a dataset I get an error:</p>

<blockquote>
  <p>A storage operation failed with the following error 'The remote server returned an error: (403)</p>
</blockquote>

<p>The error is displayed when I attempt to browse the blob to set the file path.</p>

<p>The managed identity for the data factory has been assigned the Contributor role.</p>

<p>The blob container is set to private access.</p>

<p>Anyone know how I make this work?</p>
","<azure-blob-storage><azure-data-factory>","2019-05-25 11:42:22","1304","2","1","56306735","<p>Turns out I was using the wrong role. Just need to add an assignment for 
Storage Blob Data Contributor.</p>
"
"56297301","How to use azure functions to create trigger event based on files received in azure data lake store","<p>I have a .csv file in Azure data lake store, can I Use a function app to trigger an Azure Data Factory (ADF) event whenever the .csv file gets loaded in the data lake store. Current ADF supports event's for blob storage but not for data lakes.</p>

<p>Thanks,
Ravi</p>
","<azure><azure-functions><azure-data-factory><azure-function-app>","2019-05-24 17:39:59","605","0","1","56416287","<p>ADF pipeline can be triggered by rest API and power shell. Maybe you can use azure function to call rest API or powershell script to control ADF pipeline run.  </p>

<p><a href=""https://learn.microsoft.com/en-us/powershell/module/azurerm.datafactoryv2/Invoke-AzureRmDataFactoryV2Pipeline?view=azurermps-6.13.0"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/powershell/module/azurerm.datafactoryv2/Invoke-AzureRmDataFactoryV2Pipeline?view=azurermps-6.13.0</a></p>
"
"56296754","Azure Data Factory deflate without creating a folder","<p>I have a Data Factory v2 job which copies files from an SFTP server to an Azure Data Lake Gen2.</p>

<p>There is a mix of .csv files and .zip files (each containing only one csv file).</p>

<p>I have one dataset for copying the csv files and another for copying zip files (with Compressoin type set to ZipDeflate). The problem is that the ZipDeflate creates a new folder that contains the csv file and I need this to respect the folder hierarchy without creating any folders.</p>

<p>Is this possible in Azure Data Factory?</p>
","<azure><zip><azure-data-factory>","2019-05-24 16:55:52","1981","2","1","56451883","<p>Good question, I ran into similar trouble* and it doesn't seem to be well documented.</p>

<p>If I remember correctly Data Factory assumes ZipDeflate could contain more than one file and appears to create a folder no matter what.</p>

<p>If you have Gzip files on the other hand which only have a single file, then it will create only that.</p>

<p>You'll probably already know this bit, but having it in the forefront of your mind helped me realise the sensible default data factory has:</p>

<p>My understanding of it is that the Zip standard is an <em>archive</em> format which is happening to use the Deflate algorithm. Being an archive format it naturally can contain multiple files. </p>

<p>Whereas gzip (for example) is just the compression algorithm it doesn't support multiple files (unless tar archived first), so it will decompress to just a file without a folder.</p>

<hr>

<p>You could have an additional data factory step to take the hierarchy and copy it to a flat folder perhaps, but that leads to random file names (which you may or may not be happy with). For us it didn't work as our next step in the pipeline needed predictable filenames.</p>

<p>n.b. Data factory does not move files it copies them so if they're very large this could be a pain. You can trigger a meta data move operation via the data lake store API or Powershell etc however.</p>

<p>*Mine was slightly crazier situation in that I was receiving files named .gz from a source system but were in fact zip files in disguise! In the end the best option was to ask our source system to change to true gzip files.</p>
"
"56270348","ADF - startTime & endTime dynamic content - 1st and last day of the month","<p>I'm trying to add dynamic content to the relative URL of a REST connection in Azure Data factory that is making an API call to the azure consumption API. I want to automate the fetching of data from the current billing period which is defined by the 1st and last day of the current month.</p>

<p>I've tried adding the following dynamic content, but it is returning dates that are further away. I want to have it return the 1st and last day of the current month. I know this is not simple as there are not the same amount of days in each month.</p>

<pre><code>@concat('?startTime=', adddays(utcnow(),-31,'yyyy-MM-dd'),'&amp;endTime=', 
adddays(utcnow(),+31,'yyyy-MM-dd'))
</code></pre>

<p>Expected result: return 1st day of the month in startTime and last day of the month in endTime.</p>

<p>Actual results: </p>

<pre><code>{
""errorCode"": ""2200"",
""message"": ""Failure happened on 'Source' side. ErrorCode=UserErrorHttpStatusCodeIndicatingFailure,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The HttpStatusCode 400 indicates failure.  {\""error\"":{\""code\"":\""400\""&amp;#44;\""message\"":\""Parameter: endTime. Value is greater than 2019-06-01\""}},Source=Microsoft.DataTransfer.ClientLibrary,'"",
""failureType"": ""UserError"",
""target"": ""Copy Data2""
}
</code></pre>
","<azure><azure-data-factory><dynamic-content>","2019-05-23 07:43:52","2104","0","1","56270592","<p>first day: <code>adddays(utcnow(), 0,'yyyy-MM-'), '01'</code></p>

<p>expression:
<code>@concat('?startTime=', adddays(utcnow(), 0,'yyyy-MM-'), '01','&amp;endTime=', 
adddays(utcnow(), 0,'yyyy-MM-dd'))</code></p>
"
"56269964","How to create OLAP cube using azure data lake store?","<p>I have 3 csv data sets. I want to create an OLAP cube so that it creates a schema joining all the 3 data sets. How can we do it in Azure? Can we use Azure data lake for this?</p>
","<azure><ssas><azure-data-factory><azure-data-lake><olap-cube>","2019-05-23 07:18:21","1840","1","1","56336596","<p>In Azure, data held in OLTP systems such as Azure SQL Database is copied into the OLAP system, such as Azure Analysis Services. </p>

<p><a href=""https://i.stack.imgur.com/cVZho.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cVZho.jpg"" alt=""enter image description here""></a></p>

<p>In Azure, all of the following data stores will meet the core requirements for OLAP:</p>

<ul>
<li>SQL Server with Columnstore indexes </li>
<li>Azure Analysis Services SQL</li>
<li>Server Analysis Services (SSAS)</li>
</ul>

<p>For more details, refer “<a href=""https://learn.microsoft.com/en-in/azure/architecture/data-guide/relational-data/online-analytical-processing#olap-in-azure"" rel=""nofollow noreferrer"">OLAP in Azure</a>” and ""<a href=""https://learn.microsoft.com/en-us/sql/analysis-services/multidimensional-tutorial/lesson-2-defining-and-deploying-a-cube?view=sql-server-2017"" rel=""nofollow noreferrer"">Defining and Deploying a Cube</a>"".</p>

<p>Hope this helps.</p>
"
"56265151","AzureDataFactory Incremental Load using Python","<p>How do I create azure datafactory for incremental load using python?
Where should I mention file load option(Incremental Load:LastModifiedOn) while creating activity or pipeline??</p>

<p>We can do that using UI by selecting File Load Option. But how to do the same pragmatically using python?</p>

<p>Does python api for datafactory support this or not?</p>
","<python><azure-data-factory><incremental-load>","2019-05-22 21:18:51","294","0","2","56272420","<p>According to this <a href=""https://azure.microsoft.com/en-us/blog/incrementally-copy-new-files-by-lastmodifieddate-with-azure-data-factory/"" rel=""nofollow noreferrer"">blog</a>, you could only get the Incremental Load feature from ADF SDK by setting <code>modifiedDatetimeStart</code> and <code>modifiedDatetimeEnd</code> properties.</p>

<p>You could set the above properties in the <code>additional_properties</code> of azure_blob_dataset class.</p>

<p><a href=""https://i.stack.imgur.com/UoYEK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UoYEK.png"" alt=""enter image description here""></a></p>
"
"56265151","AzureDataFactory Incremental Load using Python","<p>How do I create azure datafactory for incremental load using python?
Where should I mention file load option(Incremental Load:LastModifiedOn) while creating activity or pipeline??</p>

<p>We can do that using UI by selecting File Load Option. But how to do the same pragmatically using python?</p>

<p>Does python api for datafactory support this or not?</p>
","<python><azure-data-factory><incremental-load>","2019-05-22 21:18:51","294","0","2","56279704","<p>My investigations suggest that the Python SDK has not yet implemented this feature.  I used the SDK to connect to my existing instance and fetched two example datasets.  I did not find anything that looked like the 'last modified date'.  I tried <code>dataset.serialize()</code> , <code>dataset.__dict__</code> , <code>dataset.properties.__dict__</code> .  I also tried <code>.__slots__</code> .</p>

<p>Trying <code>serialize()</code> is significant because there <em>ought</em> to be parity between the JSON generated in the GUI and the JSON generated by the Python.  The lack of parity suggests the SDK version lags behind the GUI version.</p>

<p>UPDATE: The SDK's are being updated.</p>
"
"56259168","Copy Blob Data To Sql Database in Azure Data Factory with Conditions","<p>I am performing a a trigger based pipeline to copy data from blob storage to SQL database. In every blob file there are bunch of JSONs from which I need to copy just few of them and I can differenciate them on the basis of a Key-value pair present in every JSON.</p>

<p>So How to filter those JSON containing that Value corresponding to a common key?</p>

<p>One Blob file looks like this. Now While the copy activity is happening ,it should filter data according to the Event- Name: ""..."".</p>

<p><a href=""https://i.stack.imgur.com/WN4Po.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WN4Po.png"" alt=""Inside BLOB File example (5 json present)""></a></p>
","<azure><azure-active-directory><azure-blob-storage><azure-data-factory>","2019-05-22 14:28:03","839","1","2","56262843","<p>Data factory in general only moves data, it doesnt modify it. What you are trying to do might be done using a staging table in the sink sql.</p>

<p>You should first load the json values as-is from the blob storage in the staging table, then copy it from the staging table to the real table where you need it, applying your logic to filter in the sql command used to extract it.</p>

<p>Remember that sql databases have built in functions to treat json values: <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/json/json-data-sql-server?view=sql-server-2017"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sql/relational-databases/json/json-data-sql-server?view=sql-server-2017</a></p>

<p>Hope this helped!</p>
"
"56259168","Copy Blob Data To Sql Database in Azure Data Factory with Conditions","<p>I am performing a a trigger based pipeline to copy data from blob storage to SQL database. In every blob file there are bunch of JSONs from which I need to copy just few of them and I can differenciate them on the basis of a Key-value pair present in every JSON.</p>

<p>So How to filter those JSON containing that Value corresponding to a common key?</p>

<p>One Blob file looks like this. Now While the copy activity is happening ,it should filter data according to the Event- Name: ""..."".</p>

<p><a href=""https://i.stack.imgur.com/WN4Po.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WN4Po.png"" alt=""Inside BLOB File example (5 json present)""></a></p>
","<azure><azure-active-directory><azure-blob-storage><azure-data-factory>","2019-05-22 14:28:03","839","1","2","56265030","<p>At this time we do not have an option for the copy activity to filter the content( with an exception of sql source )  . 
In your scenario it looks like that already know which values needs to omitted , on way to go will be have a ""Stored Procedure"" activity , after the copy activity which will be just delete the values which you don't want from the table ,this should be easy to implement but depending on the volume of data it may lead to performance issues . The other option is to have the JSON file cleaned on the storage side before it is ingested .  </p>
"
"56253651","How to write partitioned parquet files to blob storage","<p>I want to load data from On Premise SQL SERVER to blob storage with copy activity in ADF, the target file is parquet, the size of this one is 5 Gb.</p>

<p>The pipeline work well and he wrote one parquet file, now i need to split this file in multiple parquet file to optimise loading data with Poly base and for another uses.</p>

<p>With Spark we can partition file in multiple file by this syntaxe : </p>

<p>df.repartition(5).write.parquet(""path"")</p>
","<azure><parquet><azure-data-factory>","2019-05-22 09:32:41","5822","0","1","56256492","<p>Short question, short answer.</p>

<p>Partitioned data: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-read-write-partitioned-data"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-read-write-partitioned-data</a></p>

<p>Parquet format: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-parquet"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/format-parquet</a></p>

<p>Blob storage connector: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage</a></p>

<p>Hope this helped!</p>
"
"56247531","Fail to create in-demand hadoop cluster in Azure Data Factory; additionalProperties is not active","<p>It's my first time trying out the Azure data factory so I hope this is not a bad question to ask. </p>

<p>So I'm using the Azure portal trying to create an on-demand hadoop cluster as one of the linked service in Azure Data Factory following the steps in the <a href=""https://learn.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-create-linux-clusters-adf#create-linked-services"" rel=""nofollow noreferrer"">tutorial</a>. </p>

<p>But whenever I click create, the following error message pops up. </p>

<blockquote>
  <p>Failed to save HDinisghtLinkedService. Error: An additional property 'subnetName' has been specified but additionalProperties is not active.The relevant property is 'HDInsightOnDemandLinkedServiceTypeProperties'.The error occurred at the location 'body/properties/typeProperties' in the request.;An additional property 'virtualNetworkId' has been specified but additionalProperties is not active.The relevant property is 'HDInsightOnDemandLinkedServiceTypeProperties'.The error occurred at the location 'body/properties/typeProperties' in the request.</p>
</blockquote>

<p>I couldn't understand why it requires the 'subnetName' and 'virtualNetworkId'. But I tried putting values under <strong>Advanced Properties -> Chose Vnet and Subnet -> From Azure subscription -></strong> and put in the <strong>existing vitrual network ID and subnet name</strong>. But the problem still present and the same error message shows up. </p>

<p>Other background information:</p>

<p>For the tutorial I posted above, I did not use its powershell code. I have existing resource group and created a new storage account on the Azure portal. 
I also created a new app registration in Azure Active Directory and retrieve principal service application ID and authentication key following this <a href=""https://learn.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal"" rel=""nofollow noreferrer"">link</a></p>

<p>Some parameters:</p>

<ul>
<li>Type: On-demand HDInsight</li>
<li>Azure Storage Linked Service: the one listed in the connection</li>
<li>Cluster size: 1 (for testing)</li>
<li>Service principal id/service principal key: described above</li>
<li>Version: 3.6
...</li>
</ul>

<p>Any thoughts or anything I might be doing wrong?</p>
","<azure><cluster-computing><azure-data-factory><azure-hdinsight><ondemand>","2019-05-21 23:34:09","191","1","1","56286279","<p>From the error message, it clearly states that “subnetName” is not active, which means it has not created at all.</p>

<p><strong>Note:</strong> If you want to create on-demand cluster within your Vnet, then first create Vnet and Subnet and the pass the following values.</p>

<p>Advanced Properties are not mandatory to create a on-demand cluster.</p>

<p>Have you tried created on-demand cluster without passing the Vnet and Subnet?</p>

<p><a href=""https://i.stack.imgur.com/f1fpi.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f1fpi.jpg"" alt=""enter image description here""></a></p>

<p>Hope this helps. Do let us know if you any further queries.</p>
"
"56241649","Azure DataFactory ForEach Copy activity is not iterating through but instead pulling all files in blob. Why?","<p>I have a pipeline in DF2 that has to look at a folder in blob and process each of the 145 files sequentially into a database table. After each file has been loaded into the table, a stored procedure should be trigger that will check each record and either insert it, or update an existing record into a master table. </p>

<p>Looking online I feel as though I have tried every combination of ""Get MetaData"", ""For Each"", ""LookUp"" and ""Assign Variable"" activates that have been suggested but for some reason my Copy Data STILL picks up all files at the same time and runs 145 times.  </p>

<p>Recently found a blog online that I followed to use ""Assign Variable"" as it will be useful for multiple file locations but it does not work for me. I need to read the files as CSVs to tables and not binary objects so therefore I think this is my issue. </p>

<pre><code>    {
        ""name"": ""BulkLoadPipeline"",
        ""properties"": {
            ""activities"": [
                {
                    ""name"": ""GetFileNames"",
                    ""type"": ""GetMetadata"",
                    ""policy"": {
                        ""timeout"": ""7.00:00:00"",
                        ""retry"": 0,
                        ""retryIntervalInSeconds"": 30,
                        ""secureOutput"": false,
                        ""secureInput"": false
                    },
                    ""typeProperties"": {
                        ""dataset"": {
                            ""referenceName"": ""DelimitedText1"",
                            ""type"": ""DatasetReference"",
                            ""parameters"": {
                                ""fileName"": ""@item()""
                            }
                        },
                        ""fieldList"": [
                            ""childItems""
                        ],
                        ""storeSettings"": {
                            ""type"": ""AzureBlobStorageReadSetting""
                        },
                        ""formatSettings"": {
                            ""type"": ""DelimitedTextReadSetting""
                        }
                    }
                },
                {
                    ""name"": ""CopyDataRunDeltaCheck"",
                    ""type"": ""ForEach"",
                    ""dependsOn"": [
                        {
                            ""activity"": ""BuildList"",
                            ""dependencyConditions"": [
                                ""Succeeded""
                            ]
                        }
                    ],
                    ""typeProperties"": {
                        ""items"": {
                            ""value"": ""@variables('fileList')"",
                            ""type"": ""Expression""
                        },
                        ""isSequential"": true,
                        ""activities"": [
                            {
                                ""name"": ""WriteToTables"",
                                ""type"": ""Copy"",
                                ""policy"": {
                                    ""timeout"": ""7.00:00:00"",
                                    ""retry"": 0,
                                    ""retryIntervalInSeconds"": 30,
                                    ""secureOutput"": false,
                                    ""secureInput"": false
                                },
                                ""typeProperties"": {
                                    ""source"": {
                                        ""type"": ""DelimitedTextSource"",
                                        ""storeSettings"": {
                                            ""type"": ""AzureBlobStorageReadSetting"",
                                            ""wildcardFileName"": ""*.*""
                                        },
                                        ""formatSettings"": {
                                            ""type"": ""DelimitedTextReadSetting""
                                        }
                                    },
                                    ""sink"": {
                                        ""type"": ""AzureSqlSink""
                                    },
                                    ""enableStaging"": false,
                                    ""translator"": {
                                        ""type"": ""TabularTranslator"",
                                        ""mappings"": [
                                            {
                                                ""source"": {
                                                    ""name"": ""myID"",
                                                    ""type"": ""String""
                                                },
                                                ""sink"": {
                                                    ""name"": ""myID"",
                                                    ""type"": ""String""
                                                }
                                            },
                                            {
                                                ""source"": {
                                                    ""name"": ""Col1"",
                                                    ""type"": ""String""
                                                },
                                                ""sink"": {
                                                    ""name"": ""Col1"",
                                                    ""type"": ""String""
                                                }
                                            },
                                            {
                                                ""source"": {
                                                    ""name"": ""Col2"",
                                                    ""type"": ""String""
                                                },
                                                ""sink"": {
                                                    ""name"": ""Col2"",
                                                    ""type"": ""String""
                                                }
                                            },
                                            {
                                                ""source"": {
                                                    ""name"": ""Col3"",
                                                    ""type"": ""String""
                                                },
                                                ""sink"": {
                                                    ""name"": ""Col3"",
                                                    ""type"": ""String""
                                                }
                                            },
                                            {
                                                ""source"": {
                                                    ""name"": ""Col4"",
                                                    ""type"": ""String""
                                                },
                                                ""sink"": {
                                                    ""name"": ""Col4"",
                                                    ""type"": ""String""
                                                }
                                            },
                                            {
                                                ""source"": {
                                                    ""name"": ""DW Date Created"",
                                                    ""type"": ""String""
                                                },
                                                ""sink"": {
                                                    ""name"": ""DW_Date_Created"",
                                                    ""type"": ""String""
                                                }
                                            },
                                            {
                                                ""source"": {
                                                    ""name"": ""DW Date Updated"",
                                                    ""type"": ""String""
                                                },
                                                ""sink"": {
                                                    ""name"": ""DW_Date_Updated"",
                                                    ""type"": ""String""
                                                }
                                            }
                                        ]
                                    }
                                },
                                ""inputs"": [
                                    {
                                        ""referenceName"": ""DelimitedText1"",
                                        ""type"": ""DatasetReference"",
                                        ""parameters"": {
                                            ""fileName"": ""@item()""
                                        }
                                    }
                                ],
                                ""outputs"": [
                                    {
                                        ""referenceName"": ""myTable"",
                                        ""type"": ""DatasetReference""
                                    }
                                ]
                            },
                            {
                                ""name"": ""CheckDeltas"",
                                ""type"": ""SqlServerStoredProcedure"",
                                ""dependsOn"": [
                                    {
                                        ""activity"": ""WriteToTables"",
                                        ""dependencyConditions"": [
                                            ""Succeeded""
                                        ]
                                    }
                                ],
                                ""policy"": {
                                    ""timeout"": ""7.00:00:00"",
                                    ""retry"": 0,
                                    ""retryIntervalInSeconds"": 30,
                                    ""secureOutput"": false,
                                    ""secureInput"": false
                                },
                                ""typeProperties"": {
                                    ""storedProcedureName"": ""[TL].[uspMyCheck]""
                                },
                                ""linkedServiceName"": {
                                    ""referenceName"": ""myService"",
                                    ""type"": ""LinkedServiceReference""
                                }
                            }
                        ]
                    }
                },
                {
                    ""name"": ""BuildList"",
                    ""type"": ""ForEach"",
                    ""dependsOn"": [
                        {
                            ""activity"": ""GetFileNames"",
                            ""dependencyConditions"": [
                                ""Succeeded""
                            ]
                        }
                    ],
                    ""typeProperties"": {
                        ""items"": {
                            ""value"": ""@activity('GetFileNames').output.childItems"",
                            ""type"": ""Expression""
                        },
                        ""isSequential"": true,
                        ""activities"": [
                            {
                                ""name"": ""Create list from variables"",
                                ""type"": ""AppendVariable"",
                                ""typeProperties"": {
                                    ""variableName"": ""fileList"",
                                    ""value"": ""@item().name""
                                }
                            }
                        ]
                    }
                }
            ],
            ""variables"": {
                ""fileList"": {
                    ""type"": ""Array""
                }
            }
        }
    }
</code></pre>

<p>The Details screen of the pipleline output shows the pipeline loops for the number of items in the blob but each time, the Copy Data and Stored Procedure are run for each file in the list at once as opposed to one at a time. </p>

<p>I feel like I am close to the answer but missing one vital part. Any help or suggestions are GREATLY appreciated.</p>
","<json><azure><foreach><iterator><azure-data-factory>","2019-05-21 15:28:23","714","0","1","56250293","<p>Your payload is not correct.</p>

<ol>
<li>GetMetadata actvitiy should not use the same dataset with Copy Activity.</li>
<li>GetMetadata activity should reference a dataset with a folder, the folder contains all file you want to deal with. but your dataset has 'filename' parameter.</li>
<li>use the output of the getMetadata activity as the input of forEach activity.
<a href=""https://i.stack.imgur.com/L29NW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L29NW.png"" alt=""childItems""></a></li>
</ol>
"
"56230050","Getting records that failed to sync during Azure Data Factory ""Copy Data"" step","<p>Is it possible, in some way, to get rows or any info about rows which were failed during Azure Data Factory Copy Data action executing and pass it to the next action in pipe?</p>

<p>Currently I sync data from Azure Sql Database into Azure Search Index. I picked up ""Skip and log incompatible rows"" as Fault tolerance action. 
In documentation I've found the next info:</p>

<blockquote>
  <p>In addition, you can log the incompatible rows in Azure Blob storage
  or Azure Data Lake Store. You can then examine the log to learn the
  cause for the failure, fix the data on the data source, and retry the
  copy activity.</p>
</blockquote>

<p>But I'd really love to have some non manual way of resync for failed rows. Is there any way to do so out of the box (without creating any custom actions which get log file from storage, parse it and resync)? I have feeling that Fault tolerance option ""Add dynamic content"" might do the thing, but its functionality isn't really rich and is limited by using few functions and runtime variables.</p>
","<azure><azure-data-factory><azure-cognitive-search>","2019-05-21 01:44:34","965","0","1","56252574","<p>The fault tolerance doesn't support automatic way of resync.</p>

<p>If you want to do that, I think you can connect any activity after your copy acitivty which will be triggered on copy finished. Then the fault store will contain the bad data, you can run any activity against it.</p>

<p>The fault data csv format is <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-fault-tolerance#monitor-skipped-rows"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-fault-tolerance#monitor-skipped-rows</a></p>

<blockquote>
  <p>data1, data2, data3, ""UserErrorInvalidDataValue"", ""Column 'Prop_2' contains an invalid value 'data3'. Cannot convert 'data3' to type 'DateTime'.""
  data4, data5, data6, ""2627"", ""Violation of PRIMARY KEY constraint 'PK_tblintstrdatetimewithpk'. Cannot insert duplicate key in object 'dbo.tblintstrdatetimewithpk'. The duplicate key value is (data4).""</p>
</blockquote>
"
"56229715","Sorting on Items from Lookup Activity","<p>I have a pipeline in ADF V-2 that reads the table list from OnPrem SqlServer using a Lookup activity, I then pass those 4 table's list to ""for each activity"" which is configured to a batch count of 2. This then reads the data for any 2 tables out of my list of 4 and loads them to ADLS. I want to control the order in which its picking up these first 2 tables and so on, probably by sorting the array of items returned by the lookup activity by Priority field. How to achieve sorting on items retrieved from the Lookup activity?</p>
","<azure-data-factory>","2019-05-21 00:35:59","1054","0","2","56252645","<p>You can write a query in your lookup activity SQL source:</p>

<p><code>SELECT * from xxx 
ORDER BY Col1</code></p>
"
"56229715","Sorting on Items from Lookup Activity","<p>I have a pipeline in ADF V-2 that reads the table list from OnPrem SqlServer using a Lookup activity, I then pass those 4 table's list to ""for each activity"" which is configured to a batch count of 2. This then reads the data for any 2 tables out of my list of 4 and loads them to ADLS. I want to control the order in which its picking up these first 2 tables and so on, probably by sorting the array of items returned by the lookup activity by Priority field. How to achieve sorting on items retrieved from the Lookup activity?</p>
","<azure-data-factory>","2019-05-21 00:35:59","1054","0","2","56279820","<p>The Lookup <em>activity itself</em> does not apply sorting.  If you want to control the order of a forEach activity, you <strong><em>must</em></strong> select the Sequential flag in the forEach activity.  Otherwise Data Factory will attempt to run them in parallel.</p>

<p>As @Atvoid suggested, applying <code>order by</code> as part of the SQL query in your Lookup activity would be the simplest solution to sort.</p>
"
"56229576","How do I get the Trigger name of parent pipeline inside the child pipeline, without using parameters","<p>I have the following set up in my ADF</p>

<p>Main pipeline -> child pipeline1 -> child pipeline2</p>

<p>‘Trigger_A’ is setup for the Main pipeline. Is there a way to get this trigger name in child pipeline2 WITHOUT using parameters?</p>

<p>In other words, if I do @pipeline().triggerName in child pipeline2, I get a uniquie Id, but I want to get value ‘Trigger_A’.</p>

<p>Please let me know.</p>
","<azure-data-factory>","2019-05-21 00:11:20","1112","0","1","56252289","<p>Based on my <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables"" rel=""nofollow noreferrer"">research</a>,the <code>@pipeline().triggerName</code> can only be used in the scope of specific pipeline.(For you, it's the Main Pipeline)</p>

<p>If you want to refer to the value of trigger name, you need to pass the trigger name as parameter into the sub pipeline.</p>

<p>Another idea(A little trouble): if the trigger name is static,you could write a custom rest api to get the value and use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">Web Activity</a> in the sub pipelines.</p>
"
"56226415","How to determine the Publish source for ADFv2?","<p>When using a Git-backed ADFv2, I'm trying to detect when a user publishes directly to the Factory vs when a user publishes to the Factory via the Git collaboration branch.</p>

<p>I've tried looking at the Activity Log, but I can't distinguish between events from a Git Publish vs events from a Direct Publish.</p>

<p>I see that this information is at least visible in the UI. Is this persisted anywhere? Is there any way to obtain this message?</p>

<p><a href=""https://i.stack.imgur.com/sPoSp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sPoSp.png"" alt=""enter image description here""></a></p>

<p>Is there another way to do this?</p>
","<azure><azure-data-factory>","2019-05-20 18:35:09","69","0","1","56254350","<p>According to my observation, the messages don't persisted anywhere.It is just temporarily stored in the browser's cache, and disappears once you refreshes it.</p>

<p><a href=""https://i.stack.imgur.com/m4LL3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m4LL3.png"" alt=""enter image description here""></a></p>

<p>In the ADF active log,you only could see the epitomize of operations.</p>

<p><a href=""https://i.stack.imgur.com/hRctv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hRctv.png"" alt=""epitomize""></a></p>

<p>I supposed that you only can distinguish between events from a Git Publish vs events from a Direct Publish in the Dev ops page.</p>

<p>You could see the direct publish will leave the comments in the commits list.</p>

<p><a href=""https://i.stack.imgur.com/0IXwH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0IXwH.png"" alt=""enter image description here""></a></p>

<p>And if you release any updates in the Git publish, you could remark some prefix,like <code>--from git---</code>.</p>

<p><a href=""https://i.stack.imgur.com/aCWoD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aCWoD.png"" alt=""enter image description here""></a></p>
"
"56224407","How to transform xml data using datafactory pipeline","<p><strong>How do we save data inside of an XML payload to blob storage?</strong></p>

<p><strong>input</strong></p>

<pre><code>&lt;root&gt;
  &lt;alexIsAwesome&gt;yes he is&lt;/alexIsAwesome&gt;
  &lt;bytes&gt;sdfsdfjijOIJOISJDFQPWORPJkjsdlfkjlksdf==&lt;/bytes&gt;
&lt;/root&gt;
</code></pre>

<p><strong>desired result</strong></p>

<pre><code>&lt;root&gt;
  &lt;alexIsAwesome&gt;yes he is&lt;/alexIsAwesome&gt;
  &lt;bytes&gt;/blob/path/toSavedPayload&lt;/bytes&gt;
&lt;/root&gt;
</code></pre>

<ol>
<li>save bytes somewhere in blob</li>
<li>replace bytes with URI of where bytes were saved</li>
</ol>

<p><strong>How do we use data factory to extract a node from XML and save it to blob?</strong></p>
","<xml><azure-data-factory>","2019-05-20 15:54:52","5010","13","3","56231314","<p>Currently, ADF doesn’t support XML natively. But </p>

<ol>
<li>You may write your own code and then use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">custom activity</a> of ADF.</li>
<li>SSIS has built-in support for <a href=""https://www.mssqltips.com/sqlservertip/3141/importing-xml-documents-using-sql-server-integration-services/"" rel=""nofollow noreferrer"">XML as a source</a>. Maybe you could take a look.</li>
</ol>
"
"56224407","How to transform xml data using datafactory pipeline","<p><strong>How do we save data inside of an XML payload to blob storage?</strong></p>

<p><strong>input</strong></p>

<pre><code>&lt;root&gt;
  &lt;alexIsAwesome&gt;yes he is&lt;/alexIsAwesome&gt;
  &lt;bytes&gt;sdfsdfjijOIJOISJDFQPWORPJkjsdlfkjlksdf==&lt;/bytes&gt;
&lt;/root&gt;
</code></pre>

<p><strong>desired result</strong></p>

<pre><code>&lt;root&gt;
  &lt;alexIsAwesome&gt;yes he is&lt;/alexIsAwesome&gt;
  &lt;bytes&gt;/blob/path/toSavedPayload&lt;/bytes&gt;
&lt;/root&gt;
</code></pre>

<ol>
<li>save bytes somewhere in blob</li>
<li>replace bytes with URI of where bytes were saved</li>
</ol>

<p><strong>How do we use data factory to extract a node from XML and save it to blob?</strong></p>
","<xml><azure-data-factory>","2019-05-20 15:54:52","5010","13","3","58972563","<p>For that case you have to use some custom code to do this. I would choose from these options</p>

<ul>
<li>Azure Functions - only for some simple data processing</li>
<li>Azure Databricks - in the case you need to process some big XML data</li>
</ul>
"
"56224407","How to transform xml data using datafactory pipeline","<p><strong>How do we save data inside of an XML payload to blob storage?</strong></p>

<p><strong>input</strong></p>

<pre><code>&lt;root&gt;
  &lt;alexIsAwesome&gt;yes he is&lt;/alexIsAwesome&gt;
  &lt;bytes&gt;sdfsdfjijOIJOISJDFQPWORPJkjsdlfkjlksdf==&lt;/bytes&gt;
&lt;/root&gt;
</code></pre>

<p><strong>desired result</strong></p>

<pre><code>&lt;root&gt;
  &lt;alexIsAwesome&gt;yes he is&lt;/alexIsAwesome&gt;
  &lt;bytes&gt;/blob/path/toSavedPayload&lt;/bytes&gt;
&lt;/root&gt;
</code></pre>

<ol>
<li>save bytes somewhere in blob</li>
<li>replace bytes with URI of where bytes were saved</li>
</ol>

<p><strong>How do we use data factory to extract a node from XML and save it to blob?</strong></p>
","<xml><azure-data-factory>","2019-05-20 15:54:52","5010","13","3","60601067","<p>As Azure Data Factory does not support XML natively, I would suggest you to go for SSIS package.</p>

<ol>
<li>In the Data flow task, have XML source and read bytes from the xml into a variable of DT_Image datatype.</li>
<li>Create a script task, which uploads the byte array (DT_Image) got in step no.1 to azure blob storage as mentioned in the below. Code slightly modified for the requirement. <a href=""https://stackoverflow.com/questions/18597043/upload-a-single-file-to-blob-storage-azure"">Reference of SO post</a></li>
</ol>

<pre><code>using Microsoft.WindowsAzure.Storage;
using Microsoft.WindowsAzure.Storage.Auth;
using Microsoft.WindowsAzure.Storage.Blob;    

// Retrieve storage account from connection string.
    CloudStorageAccount storageAccount = CloudStorageAccount.Parse(""StorageKey"");

// Create the blob client.
CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient();

// Retrieve reference to a previously created container.
CloudBlobContainer container = blobClient.GetContainerReference(""mycontainer"");

// Retrieve reference to a blob named ""myblob"".
CloudBlockBlob blockBlob = container.GetBlockBlobReference(""myblob"");

byte[] byteArrayIn = Dts.Variables[""User::ImageVariable""].Value;

// Create or overwrite the ""myblob"" blob with contents from a local file.
using (var memoryStream = new MemoryStream(byteArrayIn);)
{
    blockBlob.UploadFromStream(memoryStream);
}
</code></pre>

<ol start=""3"">
<li>Now, host this SSIS Package in SSIS Runtime in Azure Data Factory and execute the SSIS package.</li>
</ol>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime"" rel=""nofollow noreferrer"">SSIS Runtime in Azure DataFactory</a></p>
"
"56223760","reading from configuration database inside of data factory","<p>I need to create a data factory pipeline to move data from sftp to blob storage. At this point, I'm only doing a POC, and I'd like know how I would read configuration settings and kick off my pipeline based on those configuration settings. </p>

<p>Example of config settings would be <strong>(note that there would be around 1000 of these):</strong></p>

<pre><code>+--------------------+-----------+-----------+------------------------+----------------+
|    sftp server     | sftp user | sftp pass |       blob dest        |    interval    |
+--------------------+-----------+-----------+------------------------+----------------+
| sftp.microsoft.com | alex      | myPass    | /myContainer/destroot1 | every 12 hours |
+--------------------+-----------+-----------+------------------------+----------------+
</code></pre>

<p><strong>How do you kick off a pipeline using some external configuration file/store?</strong></p>
","<azure-data-factory>","2019-05-20 15:12:56","189","0","1","56231286","<p>Take a look at <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">Lookup activity</a> and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services"" rel=""nofollow noreferrer"">linked service Parameterize</a> </p>
"
"56221434","Azure Data Factory - How to handle nested Array inside JSON data to import to Blob Storage","<p>I am creating a pipeline for importing JSON data from a REST source to Blob Storage. 
However, I have a problem because there is a nested array inside the array that contains the main data. </p>

<p>The JSON data is like following:</p>

<pre><code>{
  ""@odata.context"": ""&lt;Context&gt;"",
  ""@odata.nextLink"": ""&lt;Next Link&gt;"",
  ""value"": [
    {
      ""@odata.type"": ""&lt;Type&gt;"",
      ""reportRefreshDate"": ""2019-05-18"",
      ""userPrincipalName"": ""abc@xyz.net"",
      ""displayName"": ""abc"",
      ""isDeleted"": false,
      ""deletedDate"": null,
      ""hasExchangeLicense"": true,
      ""hasOneDriveLicense"": true,
      ""hasSharePointLicense"": true,
      ""hasSkypeForBusinessLicense"": true,
      ""hasYammerLicense"": true,
      ""hasTeamsLicense"": true,
      ""exchangeLastActivityDate"": ""2019-05-17"",
      ""oneDriveLastActivityDate"": null,
      ""sharePointLastActivityDate"": null,
      ""skypeForBusinessLastActivityDate"": null,
      ""yammerLastActivityDate"": null,
      ""teamsLastActivityDate"": null,
      ""exchangeLicenseAssignDate"": ""2019-03-19"",
      ""oneDriveLicenseAssignDate"": ""2018-07-06"",
      ""sharePointLicenseAssignDate"": ""2018-07-06"",
      ""skypeForBusinessLicenseAssignDate"": ""2018-05-22"",
      ""yammerLicenseAssignDate"": ""2018-05-22"",
      ""teamsLicenseAssignDate"": ""2018-05-22"",
      ""assignedProducts"": [
        ""OFFICE 365 E3"",
        ""OFFICE 365 E5""
      ]
    }
  ]
}
</code></pre>

<p>As you could notice here, there is a nested array named ""assignedProducts"" inside the main array ""value""</p>

<p>I defined the schema of the blob storage as following:
<a href=""https://i.stack.imgur.com/VIVAo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VIVAo.png"" alt=""enter image description here""></a></p>

<p>And when I define the mapping between the source and sink, I could not map the nested array, it shows like following:</p>

<p><a href=""https://i.stack.imgur.com/Y5SdQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Y5SdQ.png"" alt=""enter image description here""></a></p>

<p>To the best of my knowledge, it is possible to make a loop for the array. But for the nested array, it seems to be difficult.</p>

<p>Could anybody suggest me what I should do to handle the nested array? It would be even great if converting this array into string or something else.</p>

<p>Thanks in advance :-)</p>
","<azure-blob-storage><azure-data-factory>","2019-05-20 12:53:12","5491","2","1","56230328","<p>I simulate some sample data like yours:</p>

<pre><code>[  
   {  
      ""numberArray"":[  
         ""1"",
         ""2"",
         ""3""
      ],
      ""name"":""A""
   },
   {  
      ""numberArray"":[  
         ""4"",
         ""5"",
         ""6""
      ],
      ""name"":""B""
   }
]
</code></pre>

<p>Then I configure my Blob Storage DataSet as below,please set the <code>File Pattern</code> as <code>Array of Objects</code>.</p>

<p><a href=""https://i.stack.imgur.com/tgrfF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tgrfF.png"" alt=""enter image description here""></a></p>

<p>Result:</p>

<p><a href=""https://i.stack.imgur.com/977Cq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/977Cq.png"" alt=""enter image description here""></a></p>
"
"56218868","How to connect to share point on premise server using azure logic apps or Azure data factory?","<p>we have a share point installed on our organisation server. so I wanted to connect  to my organisations share point server using from azure logic app.
I have tried by installing on premise gateway in share point server buti'm unable to get the file path url from premise server.can any one help me with any link or process how to connect it?</p>
","<azure-data-factory><azure-logic-apps><sharepoint-2016>","2019-05-20 10:19:50","377","0","1","56388642","<p>Before you can connect to on-premises data sources from Azure Logic Apps, download and install the on-premises data gateway on a local computer. The gateway works as a bridge that provides quick data transfer and encryption between data sources on premises (not in the cloud) and your logic apps.</p>

<p>Here are the <a href=""https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-gateway-install#prerequisites"" rel=""nofollow noreferrer"">prerequisites</a></p>

<p><strong><a href=""https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-gateway-install#install-data-gateway"" rel=""nofollow noreferrer"">Steps to install data gateway</a></strong></p>

<ol>
<li><p><a href=""https://aka.ms/on-premises-data-gateway-installer"" rel=""nofollow noreferrer"">Download, save, and run the gateway installer on a local computer</a>.</p></li>
<li><p>Accept the default installation path, or specify the location on your computer where you want to install the gateway.</p></li>
<li><p>Review and accept the terms of use and privacy statement, and then choose Install.</p></li>
</ol>

<p><a href=""https://i.stack.imgur.com/nCUuc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nCUuc.png"" alt=""enter image description here""></a></p>

<ol start=""4"">
<li>After the gateway successfully installs, provide the email address for your work or school account, and choose Sign in.</li>
</ol>

<p><a href=""https://i.stack.imgur.com/uHnPM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uHnPM.png"" alt=""enter image description here""></a></p>

<ol start=""5"">
<li>Choose Register a new gateway on this computer > Next, which registers your gateway installation with the <a href=""https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-gateway-install#gateway-cloud-service"" rel=""nofollow noreferrer"">gateway cloud service</a>.</li>
</ol>

<p><a href=""https://i.stack.imgur.com/iaLz4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iaLz4.png"" alt=""enter image description here""></a></p>

<ol start=""6"">
<li>Provide this information for your gateway installation:

<ul>
<li>The name you want for your installation</li>
<li>The recovery key you want to create, which must have at least eight characters</li>
<li>Confirmation for your recovery key</li>
</ul></li>
</ol>

<p><a href=""https://i.stack.imgur.com/fldyh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fldyh.png"" alt=""enter image description here""></a></p>

<ol start=""7"">
<li>Check the region selected for the gateway cloud service and Azure Service Bus that's used by your gateway installation.</li>
</ol>

<p><a href=""https://i.stack.imgur.com/Mk2Ca.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Mk2Ca.png"" alt=""enter image description here""></a></p>

<ol start=""8"">
<li><p>To accept the default region, choose Configure. Or, to change the default region, follow these steps:</p>

<ul>
<li><p>Next to the current region, select Change Region.
<a href=""https://i.stack.imgur.com/m1obn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m1obn.png"" alt=""enter image description here""></a></p></li>
<li><p>On the next page, open the Select Region list, select the region you want, and choose Done.
<a href=""https://i.stack.imgur.com/T4XTj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T4XTj.png"" alt=""enter image description here""></a></p></li>
</ul></li>
<li><p>After the confirmation page appears, choose <strong>Close</strong>.
The installer confirms that your gateway is now online and ready for use.</p></li>
</ol>

<p><a href=""https://i.stack.imgur.com/2kl6L.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2kl6L.png"" alt=""enter image description here""></a></p>

<ol start=""10"">
<li>Now register your gateway in Azure by <a href=""https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-gateway-connection"" rel=""nofollow noreferrer"">creating an Azure resource for your gateway installation</a>.</li>
</ol>
"
"56212643","collectionReference Issue with copying Rest API data to Azure SQL DB","<p>When I copy Rest API data to Azure SQL, there is weird issue. If I use ADF V2 Copy Wizard to generate the pipeline, it can insert 3 following records into my Azure SQL DB table.</p>

<p>taskId taskName error error_description
1 abc success success
2 cde success success
3 efg sucess success</p>

<p>If I do any tiny modification like modifying the pipeline description on this pipeline, after that, if I run the updated pipeline, it can only copies following 1 record into the azure sql table.</p>

<p>taskId taskName error error_description
null null success success</p>

<p>note:
Rest API returned data format is as followings:</p>

<pre><code>{
    ""data"": [       
        {
            ""taskId"": 1,
            ""taskName"": ""abc""
        },
        {
            ""taskId"": 2,
            ""taskName"": ""efg""
        },
        {
            ""taskId"": 3,
            ""taskName"": ""ggg""
        }
    ],
    ""error"": ""success"",
    ""error_Description"": ""successful""
}
</code></pre>

<p>The pipeline json code is as followings:</p>

<pre><code>{
    ""name"": ""pipeline3"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""Copy_v9u"",
                ""type"": ""Copy"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""userProperties"": [
                    {
                        ""name"": ""Source"",
                        ""value"": ""api/getTask""
                    },
                    {
                        ""name"": ""Destination"",
                        ""value"": ""[abc].[table1]""
                    }
                ],
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""RestSource"",
                        ""httpRequestTimeout"": ""00:01:40"",
                        ""requestInterval"": ""00.00:00:00.010""
                    },
                    ""sink"": {
                        ""type"": ""AzureSqlSink"",
                        ""preCopyScript"": ""truncate table abc.table1""
                    },
                    ""enableStaging"": false,
                    ""translator"": {
                        ""type"": ""TabularTranslator"",
                        ""mappings"": [
                            {
                                ""source"": {
                                    ""path"": ""taskId""
                                },
                                ""sink"": {
                                    ""name"": ""taskId"",
                                    ""type"": ""Int32""
                                }
                            },
                            {
                                ""source"": {
                                    ""path"": ""taskName""
                                },
                                ""sink"": {
                                    ""name"": ""taskName"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""path"": ""$.error""
                                },
                                ""sink"": {
                                    ""name"": ""error"",
                                    ""type"": ""String""
                                }
                            },
                            {
                                ""source"": {
                                    ""path"": ""$.error_Description""
                                },
                                ""sink"": {
                                    ""name"": ""error_Description"",
                                    ""type"": ""String""
                                }
                            }
                        ]
                    }
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""SourceDataset_v9u"",
                        ""type"": ""DatasetReference""
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""DestinationDataset_v9u"",
                        ""type"": ""DatasetReference""
                    }
                ]
            }
        ]
    },
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>

<p>On the mapping of the copy activity, I checked the ""Collection Reference"" since the ""data""on json result contains the array data.</p>

<p>I think it is caused by current ADF doesn't save collectionReference on the json file and once I republish it, the collectionReference information is lost somehow. My previous project is working file, I compared the pipeline json file, I found previous one uses ""schemaMapping"" instead of ""mappings"" and previous one contains ""collectionReference"", however current one doesn't contains ""collectionReference"". Please see following code for 3 months' ago pipeline copying Rest API to sql db:</p>

<pre><code>""translator"": {
                        ""type"": ""TabularTranslator"",
                        ""schemaMapping"": {
                            ""$.count"": ""count"",
                            ""$.next"": ""next"",
                            ""$.previous"": ""previous"",
                            ""product_id"": ""product_id"",
                            ""product_name"": ""product_name"",
                            ""city"": ""city"",
                            ""country"": ""country""
                        },
                        ""collectionReference"": ""$.results""
                  }

</code></pre>

<p>The pipeline should still copy 3 records into the table after I did these tiny modification. In addition, is there any reason the ""collectionReference"" no longer appears in the json file of the pipeline? How the pipeline knows which node has array collections?</p>

<p>Can you please advise the solution to resolve this issue?</p>
","<azure-data-factory>","2019-05-19 22:52:19","1176","0","1","56215055","<p>Thanks for reporting this issue. The 'collectionReference' property indeed got lost after republishing the pipeline. The fix for this issue will be rolled out to product at the end of this week. Please feel free to create pipeline from Copy wizard as a workaround. Thanks.</p>

<p>Also, as you can see, we previously used ""schemaMapping"" to describe mapping relationship between source and sink. Now it's replaced by ""mappings"". Both of them are working and it is not the cause of the issue you met. </p>

<p>The useful doc link for you: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping</a></p>
"
"56212149","Does adf V2 support oracle pl/sql?","<p>Does azure data factory copy activity supports oracle pl sql query?
The sample select statement is owrking but what about pl/sql programes</p>
","<azure-data-factory>","2019-05-19 21:28:05","211","-1","1","56215810","<p>Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-oracle#copy-activity-properties"" rel=""nofollow noreferrer"">official document</a>,copy activity only supports normal sql in the transactions. Pl sql is not supported by ADF connectors directly.</p>

<p>However,you could consider using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity#custom-activity"" rel=""nofollow noreferrer"">Custom Activity</a> when you want to transform/process data in a way that isn't supported by Data Factory.</p>

<p>Please follow the details in this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity#custom-activity"" rel=""nofollow noreferrer"">link</a>.It is rely on the Azure Batch Job in the VM. In that Job,you could execute pl sql code block by python package <code>cx_Oracle</code>,please see this <a href=""https://dzone.com/articles/execute-plsql-calls-with-python-and-cx-oracle"" rel=""nofollow noreferrer"">detailed document</a>.</p>
"
"56185574","SimeReplacing WeekOfYear Dynamically","<p>I am using copy activity in ADF in which i have following pseudo query:</p>

<pre><code>SELECT
    {
        [Measures].[**************],
        [Measures].[**************],
        [Measures].[**************]
    } ON COLUMNS,
NON EMPTY 
    { 
        [0CALWEEK].[LEVEL01].MEMBERS 
    } 
DIMENSION PROPERTIES  MEMBER_CAPTION, MEMBER_UNIQUE_NAME ON ROWS 
FROM cubeName/ReportName
SAP VARIABLES [SAP_VARIABLE_NAME] 
INCLUDING [0CALWEEK].[201905]
</code></pre>

<p>In the above i want to replace '201905' dynamically,there is no current function availabe to get WeekOfYear in ADF Expressions and functions OR can i use MDX Query to generate WeekOfYear </p>
","<mdx><azure-data-factory><mdx-query>","2019-05-17 11:32:58","55","0","1","56192499","<p>Take a look at this example how it changes value for a member value. Please note I have answerd if from phone so some syntax issue might exist.</p>

<p>With member measures.t
As
Case when</p>

<p>[0CALWEEK].[LEVEL01]. currentmember.name=""'201905""
Then 1
Else
0
End</p>

<p>SELECT
    {
        [Measures].[t]
    } ON COLUMNS,
NON EMPTY 
    { 
        [0CALWEEK].[LEVEL01].MEMBERS 
    }  ON ROWS 
FROM cubeName/ReportName</p>
"
"56183945","Convert JSON file from flat to nested arrays in Azure Data Factory","<p>I'm trying to copy data from my Oracle DB into the Search Index and I'm using Azure Data Factory to copy data from oracle to Azure Blob Storage. 
How I can use it, to import data as nested JSON file.
At now, after query Oracle I get data like this:</p>

<pre><code>[{""BOOKING_ID"":1.0,""REFERENCES"":""ABC00001"",""ROUTES"":{""ROUTE"":1.0,""DESTINATION"":""Atlanta, USA"",""ORIGIN"":""New York, USA""}}
,{""BOOKING_ID"":2.0,""REFERENCES"":""ABC00322"",""ROUTES"":{""ROUTE"":2.0,""DESTINATION"":""Las Vegas, USA"",""ORIGIN"":""Los Angeles, USA""}}
,{""BOOKING_ID"":3.0,""REFERENCES"":""ABC32322"",""ROUTES"":{""ROUTE"":3.0,""DESTINATION"":""Berlin, GER"",""ORIGIN"":""Moscow, RUS""}}
,{""BOOKING_ID"":4.0,""REFERENCES"":""ABC543345"",""ROUTES"":{""ROUTE"":4.0,""DESTINATION"":""Rome, ITA"",""ORIGIN"":""Bejin, CHN""}}
,{""BOOKING_ID"":5.0,""REFERENCES"":""ABC51145"",""ROUTES"":{""ROUTE"":5.0,""DESTINATION"":""Warsaw, POL"",""ORIGIN"":""Copenhagen, DEN""}}
,{""BOOKING_ID"":5.0,""REFERENCES"":""ABC51145"",""ROUTES"":{""ROUTE"":6.0,""DESTINATION"":""Copenhaged, DEN"",""ORIGIN"":""Paris, FRA""}}
,{""BOOKING_ID"":5.0,""REFERENCES"":""ABC51145"",""ROUTES"":{""ROUTE"":7.0,""DESTINATION"":""Paris, FRA"",""ORIGIN"":""Madrid, ESP""}}
]
</code></pre>

<p>but I need data like this:</p>

<pre><code>[
  {
    ""BOOKING_ID"": 1.0,
    ""REFERENCES"": ""ABC00001"",
    ""ROUTES"": [
      {
        ""ROUTE"": 1.0,
        ""DESTINATION"": ""Atlanta, USA"",
        ""ORIGIN"": ""New York, USA""
      }
    ]
  },
  {
    ""BOOKING_ID"": 2.0,
    ""REFERENCES"": ""ABC00322"",
    ""ROUTES"": [
      {
        ""ROUTE"": 2.0,
        ""DESTINATION"": ""Las Vegas, USA"",
        ""ORIGIN"": ""Los Angeles, USA""
      }
    ]
  },
  {
    ""BOOKING_ID"": 3.0,
    ""REFERENCES"": ""ABC32322"",
    ""ROUTES"": [
      {
        ""ROUTE"": 3.0,
        ""DESTINATION"": ""Berlin, GER"",
        ""ORIGIN"": ""Moscow, RUS""
      }
    ]
  },
  {
    ""BOOKING_ID"": 4.0,
    ""REFERENCES"": ""ABC543345"",
    ""ROUTES"": [
      {
        ""ROUTE"": 4.0,
        ""DESTINATION"": ""Rome, ITA"",
        ""ORIGIN"": ""Bejin, CHN""
      }
    ]
  },
  {
    ""BOOKING_ID"": 5.0,
    ""REFERENCES"": ""ABC51145"",
    ""ROUTES"": [
      {
        ""ROUTE"": 5.0,
        ""DESTINATION"": ""Warsaw, POL"",
        ""ORIGIN"": ""Copenhagen, DEN""
      },
      {
        ""ROUTE"": 6.0,
        ""DESTINATION"": ""Copenhaged, DEN"",
        ""ORIGIN"": ""Paris, FRA""
      },
      {
        ""ROUTE"": 7.0,
        ""DESTINATION"": ""Paris, FRA"",
        ""ORIGIN"": ""Madrid, ESP""
      }
    ]
  }
]
</code></pre>

<p><strong>UPDATE</strong>
I use Azure Functions with lodash, but now I'm trying to receive the JSON from Azure Blob Storage. Problem is, what when I try to read the JSON, I have results like this:</p>

<pre><code>""type"": ""Buffer"",
    ""data"": [
        239,
        187,
        191,
        91,
        123,
...
</code></pre>

<p>and all data is in byte type.</p>
","<arrays><json><azure><azure-data-factory>","2019-05-17 09:53:12","1175","1","1","56216750","<p>Your requirement is group by the <code>BOOKING_ID</code>, merge the <code>ROUTES</code> object into one array. It can't be implemented in the copy activity directly.</p>

<p>Two ideas:</p>

<p>1.Using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">Web Activity</a> + <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure Function Activity</a>.</p>

<p>In the Web Activity,encapsulate query methods into REST API and return the flat json data.Pass the output of Web Activity into Azure Function Activity. In the Azure Function method,loop the array json data into nested arrays following your needs then configure the output of Azure Function as Azure Blob Storage.(Please refer to this <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob"" rel=""nofollow noreferrer"">link</a>)</p>

<p>2.Using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">Custom Activity</a>.</p>

<p>You could execute scripts in the Azure Batch Job rely on VM. Such as you could use <a href=""https://oracle.github.io/python-cx_Oracle/"" rel=""nofollow noreferrer"">cx-Oracle</a> package to query the json data order by the BOOKING_ID,then using python code to loop the result and convert it following your needs.</p>
"
"56164362","Azure Data Factory - How to handle CSV data from Microsoft graph REST source","<p>I am making a pipeline of Azure Data Factory to get data from Microsoft Graph to store in Azure Data Lake through the REST API.</p>

<p>What I want to store is the data about <a href=""https://learn.microsoft.com/en-us/graph/api/reportroot-getoffice365activeuserdetail?view=graph-rest-1.0&amp;tabs=cs"" rel=""nofollow noreferrer"">Office 365 active users</a>.</p>

<p>The problem is this kind of data is returned under the CSV format, not the JSON one. Following the document about <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest"" rel=""nofollow noreferrer"">copying data from REST source</a>, they only accept the JSON format.</p>

<p>Actually I could use Batch service to handle this kind of problem but I do not appreciate this much. I have thought about changing the format of the returned data from REST source but it seems difficult. So, I would want to ask if there is any possible solution to store this kind of data without using Batch Service.</p>

<p>Thanks in advance.</p>
","<azure><microsoft-graph-api><azure-data-factory>","2019-05-16 08:41:13","501","1","1","56167621","<p>In fact it is supported to return results in JSON format by providing <code>$format</code> query option, for example: <code>$format=application/json</code> (<a href=""https://github.com/microsoftgraph/microsoft-graph-docs/blob/master/api-reference/beta/api/reportroot-getoffice365activeuserdetail.md"" rel=""nofollow noreferrer"">details</a>)  </p>

<blockquote>
  <p>Note: at the moment <code>application/json</code> format is <strong>only</strong> supported
  under <code>beta</code> API version</p>
</blockquote>

<p>Example:</p>

<pre><code>GET https://graph.microsoft.com/beta/reports/getOffice365ActiveUserDetail(period='D7')?$format=application/json
</code></pre>
"
"56161735","is there a way to write u-sql queries without using EXTRACT","<p>I have a metadata activity output which is a json of blobs in my container. I want to input these names into my foreach activity where some u-sql query is performed on the blob as per the file name. Is it possible?</p>
","<json><azure><azure-data-factory><u-sql>","2019-05-16 06:02:45","57","0","1","56225293","<p>You need to include either a <code>SELECT</code> or an <code>EXTRACT</code>. Since you are pulling from files, you are going to want to use <code>EXTRACT</code>.</p>

<p>If I understand your question correctly, you want to run different U-SQL scripts based on the file name.</p>

<p>There are a couple ways to do this:</p>

<p>1) use If conditions in Data Factory to call different U-SQL scripts based on the file name. Nesting the if statements will allow you to have more than two options. There are <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">several string manipulation</a> functions to help you with this. Say one path is <code>@item.Contains('a')</code>.</p>

<pre><code>{
    ""name"": ""&lt;Name of the activity&gt;"",
    ""type"": ""IfCondition"",
    ""typeProperties"": {
            ""expression"":  {
                 ""value"":  ""@item() == &lt;file name&gt;"", 
                 ""type"": ""Expression""
             }
            ""ifTrueActivities"": [
            {
                ""&lt;U-SQL script = 1&gt;""
            }
        ],

        ""ifFalseActivities"": [
            {
                ""&lt;U-SQL script 2&gt;""
            }
            ]
    }
}
</code></pre>

<p>2) The second option is to use a single U-SQL script and do the split from there. Again, <a href=""https://learn.microsoft.com/en-us/u-sql/operators/csharp-functions-and-operators"" rel=""nofollow noreferrer"">string manipulation functions</a> can help via pattern matching. There is some advantage to this as far as organization goes as you can store the unique scripts in stored procedures and the U-SQL script would simply check the file name passed in and call the relevant stored proc.</p>

<pre><code>//This would be added by data factory
DECLARE @fileName = ""/Samples/Data/SearchLog.tsv"";

IF @fileName == ""/Samples/Data/SearchLog.tsv""
THEN
    @searchlog =
        EXTRACT UserId int,
                Start DateTime,
                Region string,
                Query string,
                Duration int?,
                Urls string,
                ClickedUrls string
        FROM ""/Samples/Data/SearchLog.tsv""
        USING Extractors.Tsv();

    OUTPUT @searchlog
    TO @fileName
    USING Outputters.Csv();

ELSE
    @searchlog =
        EXTRACT UserId int,
                Start DateTime,
                Region string,
                Query string,
                Duration int?,
                Urls string,
                ClickedUrls string
        FROM @fileName
        USING Extractors.Tsv();

    OUTPUT @searchlog
    TO ""/output/SearchLogResult1.csv""
    USING Outputters.Csv();
END;
</code></pre>

<p>Something to think about is that Data Lake Analytics is going to be more efficient if you can combine multiple files into one statement. You can have multiple <code>EXTRACT</code> and <code>OUTPUT</code> statements. I would encourage you to explore whether or not you could use pattern matching in your <code>EXTRACT</code> statements to split the U-SQL processing without needing the foreach loop in data factory.</p>
"
"56160306","For Azure Data Factories is there a way to 'Validate all' using powershell rather than the GUI?","<ul>
<li>A working Azure Data Factory (ADF) exists that contains pipelines with activities that are dependent on database tables</li>
<li>The definition of a database table changes</li>
<li>The next time the pipeline runs it fails</li>
<li>Of course we can set up something so it fails gracefully but ...</li>
<li>I need to proactively execute a scheduled Powershell script that iterates through all ADFs (iterating is easy) to do the equivalent of the 'Validate All' (validating is impossible?) functionality that the GUI provides</li>
<li>I do realise that the Utopian CI/CD DevOps environment I dream about will one day in the next year or so achieve this via other ways</li>
<li>I need the automation validation method today - not in a year!</li>
</ul>

<p>I've looked at what I think are all of the powershell cmdlets available and short of somehow deleting and redeploying each ADF (fraught with danger) I can't find a simple method to validate an Azure Data Factory via Powershell.</p>

<p>Thanks in advance</p>
","<powershell><azure-data-factory>","2019-05-16 03:15:51","630","1","2","56263476","<p>In the "".Net"" SDK, each of the models has a ""Validate()"" method.  I have not yet found anything similar in the Powershell commands.
In my experience, the (GUI) validation is not foolproof.  Some things are only tested at runtime.</p>
"
"56160306","For Azure Data Factories is there a way to 'Validate all' using powershell rather than the GUI?","<ul>
<li>A working Azure Data Factory (ADF) exists that contains pipelines with activities that are dependent on database tables</li>
<li>The definition of a database table changes</li>
<li>The next time the pipeline runs it fails</li>
<li>Of course we can set up something so it fails gracefully but ...</li>
<li>I need to proactively execute a scheduled Powershell script that iterates through all ADFs (iterating is easy) to do the equivalent of the 'Validate All' (validating is impossible?) functionality that the GUI provides</li>
<li>I do realise that the Utopian CI/CD DevOps environment I dream about will one day in the next year or so achieve this via other ways</li>
<li>I need the automation validation method today - not in a year!</li>
</ul>

<p>I've looked at what I think are all of the powershell cmdlets available and short of somehow deleting and redeploying each ADF (fraught with danger) I can't find a simple method to validate an Azure Data Factory via Powershell.</p>

<p>Thanks in advance</p>
","<powershell><azure-data-factory>","2019-05-16 03:15:51","630","1","2","68682174","<p>I know it has been a while and you said you didn't want the validation to work in an year - but after a couple of years we finally have both the Validate all and Export ARM template features from the Data Factory user experience via a publicly available npm package <a href=""https://www.npmjs.com/package/@microsoft/azure-data-factory-utilities"" rel=""nofollow noreferrer"">@microsoft/azure-data-factory-utilities</a>. The full guidance can be found on this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment-improvements"" rel=""nofollow noreferrer"">documentation</a>.</p>
"
"56147615","Get-AzDataFactoryV2 : HTTP Status Code: NotFound","<p>I wrote this <code>Azure PowerShell</code> script </p>

<pre><code>$DataFactoryName = ""BI-Dashboard-DataFactory-2""
$ResourceGroupName = ""BI-Dashboard-ResourceGroup-2""
$ResourceGroup = Get-AzResourceGroup -Name $ResourceGroupName

# Write-Output $DataFactory.DataFactoryName 
if(-not $ResourceGroup)
 {
   $ResourceGroup= New-AzResourceGroup $ResourceGroupName -location 'westeurope'
   Write-Output "" Resource Group Created Successfully ""      
}
else 
{
 # Resource Group Already Exists
 Write-Output ""Resource Group Exists"" 
}

$DataFactory = Get-AzDataFactoryV2 -Name $DataFactoryName -ResourceGroupName $ResourceGroup.ResourceGroupName

 if (-not $DataFactory)
 {
    $DataFactory = Set-AzDataFactoryV2 -ResourceGroupName $ResourceGroup.ResourceGroupName -Location $ResourceGroup.Location -Name $DataFactoryName
    Write-Output "" Data Factory Created Successfully ""
 }
 else 
 {
  Write-Output ""Data Factory {0} Already Exists"" -f $DataFactory.DataFactoryName 
 }
</code></pre>

<p>some time ago and if <code>Resource</code> or <code>Data Factory</code> does not exist it didn't throw any exception, it simply executed if block. 
<a href=""https://i.stack.imgur.com/QjaXX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QjaXX.png"" alt=""enter image description here""></a></p>

<p>I have created a new subscription and executing the same <code>PowerShell</code> script against new subscription and now receives this exception in red color as well as execution of if block. I need to know whether something is changed in <code>Azure Resource Manager</code> when its accepting this <code>PowerShell</code> request to display error message or this is not an issue.</p>
","<azure><powershell><azure-powershell><azure-data-factory>","2019-05-15 10:52:48","1761","0","1","56148467","<p>You will receive this error message ""Get-AzDataFactoryV2 : HTTP Status Code: NotFound"", when the resource doesn't exists in the resource group.</p>

<p>The script first looks for the resource group exists or not, then it will check for the data factory exists in the resource group or not.</p>

<p>If the resource exists gives the results, else it throws the error message.</p>

<p><strong>Example:</strong> In my resource group named chpradeep, I have a data factory name ""chepra"".</p>

<p><strong>Case1:</strong> (Success) If I run the below cmdlet gives the results because the data factory named chepra exists in the resource group.</p>

<p>Get-AzDataFactoryV2 -ResourceGroupName ""chpradeep"" -Name chepra</p>

<p><a href=""https://i.stack.imgur.com/QLW3U.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QLW3U.jpg"" alt=""enter image description here""></a>  </p>

<p><strong>Case2:</strong> (Error) If I run the below cmdlet gives the error message because the data factory named alpha doesn't exists in the resource group.</p>

<p>Get-AzDataFactoryV2 -ResourceGroupName ""chpradeep"" -Name alpha</p>

<p><a href=""https://i.stack.imgur.com/DSZ45.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DSZ45.jpg"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"56144240","Azure DataFactory V2: Use SecureString Parameter in dynamic content","<p>I have a secure string parameter in data fatory</p>

<p><a href=""https://i.stack.imgur.com/dlvow.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dlvow.png"" alt=""enter image description here""></a></p>

<p>According to another post this is the way to access said parameter</p>

<pre><code>@{pipeline().parameters.ChassisSqlUsername.value}
</code></pre>

<p>This use to and still give a validation error within the dynamic content window:
<a href=""https://i.stack.imgur.com/DYOJz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DYOJz.png"" alt=""enter image description here""></a></p>

<p>However the pipeline worked like this without issue. This validation error did not block publishing.</p>

<p>Asof today I can no longer publish if that validation error is present
<a href=""https://i.stack.imgur.com/Irawb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Irawb.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/JXCzC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JXCzC.png"" alt=""enter image description here""></a></p>

<p><strong>How do I get around this?</strong></p>
","<azure-data-factory>","2019-05-15 07:56:38","849","3","2","61597030","<p>Refer it in this way:</p>

<pre><code>@{pipeline().parameters.ChassisSqlUsername}
</code></pre>

<p>It will work.</p>
"
"56144240","Azure DataFactory V2: Use SecureString Parameter in dynamic content","<p>I have a secure string parameter in data fatory</p>

<p><a href=""https://i.stack.imgur.com/dlvow.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dlvow.png"" alt=""enter image description here""></a></p>

<p>According to another post this is the way to access said parameter</p>

<pre><code>@{pipeline().parameters.ChassisSqlUsername.value}
</code></pre>

<p>This use to and still give a validation error within the dynamic content window:
<a href=""https://i.stack.imgur.com/DYOJz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DYOJz.png"" alt=""enter image description here""></a></p>

<p>However the pipeline worked like this without issue. This validation error did not block publishing.</p>

<p>Asof today I can no longer publish if that validation error is present
<a href=""https://i.stack.imgur.com/Irawb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Irawb.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/JXCzC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JXCzC.png"" alt=""enter image description here""></a></p>

<p><strong>How do I get around this?</strong></p>
","<azure-data-factory>","2019-05-15 07:56:38","849","3","2","69660132","<p>I don't know why the syntax of ADF is so terrible.</p>
<pre><code>@{json(string(pipeline().parameters.ChassisSqlUsername)).value}
</code></pre>
"
"56135311","Azure Data Factory Parameter / Extended Property value leading special character issue","<p>Data Factory parameter value starting with [ (square bracket) causing issue publishing to Data Factory from Master branch. I have added some parameters for our Azure Analysis Services processing to help streamline the deployment between environments and one of the values is a client secret that starts with a leading square bracket. This works fine when working on the pipeline in a branch or deploying to master branch, but when we publish it to the actual Data Factory it adds an extra square bracket in the value. I tried to update it directly in the Data Factory and publish, but same thing. How can we resolve this and get this bug resolved? I also see that this is an issue with our Extended Property values in Custom Activities.</p>

<p>So in the screenshot this parameter value should only have one square bracket at the beginning:
<a href=""https://i.stack.imgur.com/UeACP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UeACP.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2019-05-14 17:04:31","427","0","2","56139422","<p>Just tested with a pipeline parameter and cannot reproduce your error. Try reading the Json code, maybe that gives you a clue. </p>

<p>How are you loading your parameters? Maybe its an issue with the way or code you are using to create the pipelines.</p>

<p>Hope this helped!</p>
"
"56135311","Azure Data Factory Parameter / Extended Property value leading special character issue","<p>Data Factory parameter value starting with [ (square bracket) causing issue publishing to Data Factory from Master branch. I have added some parameters for our Azure Analysis Services processing to help streamline the deployment between environments and one of the values is a client secret that starts with a leading square bracket. This works fine when working on the pipeline in a branch or deploying to master branch, but when we publish it to the actual Data Factory it adds an extra square bracket in the value. I tried to update it directly in the Data Factory and publish, but same thing. How can we resolve this and get this bug resolved? I also see that this is an issue with our Extended Property values in Custom Activities.</p>

<p>So in the screenshot this parameter value should only have one square bracket at the beginning:
<a href=""https://i.stack.imgur.com/UeACP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UeACP.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2019-05-14 17:04:31","427","0","2","56172760","<p>This is the current workaround until the bug is resolved by the ADF team and I have tested this and it works.</p>

<p>Please find below update from PG and let me know the work around status.</p>

<p>There's already a work item created for this bug.
As a workaround, you can turn this feature off until the bug is fixed.
1. Append this to your URL: &amp;feature.escapeBracket=false
2. Refresh the page with the new URL
3. Remove the extra bracket from the pipeline parameter value
4. publish your changes
5. Refresh page to verify everything is back to normal</p>

<p>Please let me know if you have any questions/concerns on this issue.</p>
"
"56131849","Is there any way to filter out some data in copy activity when source is Blob storage and sink is SQL database?","<p>I am trying to copy data from Azure blobs to azure SQL database using Azure Data Factory. </p>

<p>The azure blobs are incrementally stored everytime in the storage account. They are just JSON having key-value pairs. So I want to filter the data on the basis of one key-value before it get copied inside the SQL databse.</p>
","<azure><etl><azure-blob-storage><azure-pipelines><azure-data-factory>","2019-05-14 13:37:50","359","1","1","56133702","<p>you could use stored procedure to do some filtering before writing into your db.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#invoking-stored-procedure-for-sql-sink"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#invoking-stored-procedure-for-sql-sink</a></p>
"
"56131373","Is there any way for execute a stored procedure from an Oracle database?","<p>I have a connection to an Oracle database, but I don't know how execute a stored procedure. Is it possible from Data Factory?</p>
","<azure><azure-data-factory>","2019-05-14 13:14:15","1182","0","3","56133775","<p>ADF currently only support stored procedure for azure SQL and SQL server.</p>

<p>Maybe you could consider custom activity. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity</a></p>
"
"56131373","Is there any way for execute a stored procedure from an Oracle database?","<p>I have a connection to an Oracle database, but I don't know how execute a stored procedure. Is it possible from Data Factory?</p>
","<azure><azure-data-factory>","2019-05-14 13:14:15","1182","0","3","62161527","<p>Can you try using a Lookup Activity, and where there is a choice of using a table or query, select query.  Since this provides a place to write your own statement, you can try writing an execute stored proc statement.</p>

<p>One thing to remember about Lookup activity, is it expects data to be returned.  If your stored proc does not return data, you may need to add an additional statement to return dummy data.</p>

<p>I have only done this with SQL before, but I see no reason it wouldn't work with Oracle as well.</p>

<p><a href=""https://i.stack.imgur.com/3sEZY.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3sEZY.jpg"" alt=""enter image description here""></a></p>
"
"56131373","Is there any way for execute a stored procedure from an Oracle database?","<p>I have a connection to an Oracle database, but I don't know how execute a stored procedure. Is it possible from Data Factory?</p>
","<azure><azure-data-factory>","2019-05-14 13:14:15","1182","0","3","65622119","<p>I used a Lookup Activity and a SELECT statement of DUAL TABLE. Due to the stored procedures can not be call from a statement SELECT. I created an oracle function and the function calls the stored procedure. The function returns a value and this value is received by the lookup activity.
When you define the function, you have to add the statement PRAGMA AUTONOMOUS_TRANSACTION. This is because Oracle does not allow to execute DML instructions with a SELECT statement by default. Then, you need to define that DML instructions in the Stored Procedure will be an autonomous transaction.</p>
<pre><code>--Tabla
CREATE TABLE empleados(
   emp_id NUMBER(9),
   nombre VARCHAR2(100),
   CONSTRAINT empleados_pk PRIMARY KEY(emp_id),
);

create or replace procedure insert_empleado (numero in NUMBER, nombre in VARCHAR2) is
 begin
    INSERT INTO empleados (emp_id, nombre)
    Values(numero, nombre);
COMMIT;
end;

create or replace function funcinsert_empleado (numero in NUMBER, nombre in VARCHAR2)
return VARCHAR2 
is
PRAGMA AUTONOMOUS_TRANSACTION;
begin
  insert_empleado (numero, nombre);
  return 'done';
end;
--statement in query of lookup
SELECT  funcinsert_empleado ('1', 'Roger Federer') 
FROM DUAL;
</code></pre>
<p><a href=""https://i.stack.imgur.com/K1qPC.png"" rel=""nofollow noreferrer"">Example lookup</a></p>
<p>This is example in Spanish. <a href=""https://dev.to/maritzag/ejecutar-un-stored-procedure-de-oracle-desde-data-factory-2jcp"" rel=""nofollow noreferrer"">https://dev.to/maritzag/ejecutar-un-stored-procedure-de-oracle-desde-data-factory-2jcp</a></p>
"
"56131349","On-Prem SQL server to ADLS Gen2 though ADF V2","<p>I have a pipeline which just copies data from On premise SQl Server database onto ADLS Gen2. It's very simple pipeline but I could see an error as in below but I don't have any parameter mentioned in accesskey in pipeline. :</p>

<h2>error message</h2>

<p>Activity Copy Data1 failed: 'Type=System.ArgumentException,Message=The required property is not specified.
Parameter name: accountKey,Source=Microsoft.DataTransfer.Common,'</p>

<h2>When I use copy data approach I see following error.</h2>

<p>The version ""3.10.6838.1"" of Self-hosted Integration Runtime ""integrationRuntime1"" is lower than 3.16.7033.3, which is not supported for your dataset. Please upgrade to the latest version.</p>
","<azure-data-factory>","2019-05-14 13:12:59","128","0","1","56132904","<p>I've upgraded self-hosted IR from 3.5 to 3.16. Now, am able to copy data from on premise sql server to adls gen2.</p>
"
"56128227","How to connect to a local Oracle database inside a virtual machine?","<p>I have an Oracle database inside a virtual machine and I want to connect to it through data factory. I have already created the self-hosted integration but I do not know what parameter to pass to the host when I create the Linked Service to be able to make the connection. What should I do? Thanks!</p>
","<azure><azure-data-factory>","2019-05-14 10:23:01","47","0","1","56130740","<p>In the host name you have to put localhost</p>
"
"56126304","How to setup project level parameters in Azure Data Factory V2","<p>We need to setup project level parameters which will used repeatedly across each of the pipelines in ADF V2. Currently we are repeating the same parameters in each pipeline. If at all we need to modify the parameter, we have to go manually into each of the pipeline's trigger and amend them which is tedious and erroneous.</p>

<p>is there any better way to setup them as project level parameters and they'll automatically initialised and assigned across the Data factory whenever pipeline triggers.</p>
","<azure><azure-data-factory>","2019-05-14 08:43:20","64","0","2","56127299","<p>Based on the official <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities#overview"" rel=""nofollow noreferrer"">document</a>, A pipeline is a logical grouping of activities that together perform a task. No more thing represents the group of pipelines so that you can't let pipelines share one same parameter.</p>

<p><a href=""https://i.stack.imgur.com/5Zfsv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5Zfsv.png"" alt=""enter image description here""></a></p>

<p>I have a workaround,using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-pipeline-activity"" rel=""nofollow noreferrer"">Execute pipeline activity</a> and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-set-variable-activity"" rel=""nofollow noreferrer"">Set Variable activity</a>.</p>

<p>You could invoke other pipelines in a Execute pipeline activity.Before that, you could set a variable in the pipeline. </p>

<p><a href=""https://i.stack.imgur.com/5MorP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5MorP.png"" alt=""enter image description here""></a></p>

<p>Then reset the value in the Set Variable activity to provide the same parameter for all the pipelines.</p>

<p><a href=""https://i.stack.imgur.com/JapXK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JapXK.png"" alt=""enter image description here""></a></p>

<p>Some details,please refer to this <a href=""https://www.sqlservercentral.com/blogs/setting-variables-in-azure-data-factory-pipelines"" rel=""nofollow noreferrer"">link</a>.</p>
"
"56126304","How to setup project level parameters in Azure Data Factory V2","<p>We need to setup project level parameters which will used repeatedly across each of the pipelines in ADF V2. Currently we are repeating the same parameters in each pipeline. If at all we need to modify the parameter, we have to go manually into each of the pipeline's trigger and amend them which is tedious and erroneous.</p>

<p>is there any better way to setup them as project level parameters and they'll automatically initialised and assigned across the Data factory whenever pipeline triggers.</p>
","<azure><azure-data-factory>","2019-05-14 08:43:20","64","0","2","56133343","<p>Thinking on the fly here, but have you considered reading a DB to get this parameter?</p>

<p>You can make every pipeline to do a lookup (LookupActivity) on a table where there is only one record with this parameter, and then set it (Set Variable Activity) as a variable, then use this variable. If at all you have to change this parameter, just change the record in the database and all pipelines will immediatly use this new value in their next executions.</p>

<p>Hope this helped!</p>
"
"56119601","How to create temp tables in SQL to be used in several ADF activities?","<p>I need to create a global temp table in my SQL Server while executing an Azure Data Factory pipeline. This table will be used in several activities.</p>

<p>I already tried several approaches including one using the Stored Procedure activity targeting the <code>sys.sp_executesql</code> SP and the <code>CREATE TABLE</code> statement as the parameter. With this approach the table is actually created, but it's automaticaly dropped a second later, I don't understand why.</p>

<p>This is the script used to create the temp table:</p>

<pre><code>CREATE TABLE ##tempGL
(
    GLAccount NVARCHAR(15),
    GLSubAccount NVARCHAR(15)
)
</code></pre>

<p>So, how can I create a SQL Server temp table from an Azure Data Factory Pipeline activity that persists until I dropped it?</p>
","<sql-server><azure-data-factory>","2019-05-13 20:39:00","4062","1","1","57318173","<p>I have been struggling with this myself. Apparently this is by design (see quote below from Microsoft employee) and it is not possible to achieve this using Azure Data Factory even though the documentation mentions that it is possible.</p>

<blockquote>
  <p>That is by design. We won’t keep connection between 2 activities.
  If you use a real table instead of temporary table. Then you will get the expected result.
  The suggestion is don’t used temporary table in ADF if the data need more than 1 activities to access.</p>
</blockquote>

<p><a href=""https://github.com/MicrosoftDocs/azure-docs/issues/35449#issuecomment-517451867"" rel=""nofollow noreferrer"">https://github.com/MicrosoftDocs/azure-docs/issues/35449#issuecomment-517451867</a></p>

<p>The reason this happens is the session is dropped when a pipeline activity ends, which causes the temporary table to also be dropped.</p>

<blockquote>
  <p>Global temporary tables are automatically dropped when the session that created the table ends and all other tasks have stopped referencing them. The association between a task and a table is maintained only for the life of a single Transact-SQL statement. This means that a global temporary table is dropped at the completion of the last Transact-SQL statement that was actively referencing the table when the creating session ended.</p>
</blockquote>

<p><a href=""https://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-transact-sql?view=sql-server-2017#temporary-tables"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-transact-sql?view=sql-server-2017#temporary-tables</a></p>

<p>Hopefully Microsoft fixes this at some point and makes it possible to use temporary tables across activities with Azure Data Factory.</p>

<hr>

<p>I have raised this as a suggestion for Azure here <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/38287108-persist-global-temporary-tables-between-activities"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory/suggestions/38287108-persist-global-temporary-tables-between-activities</a></p>

<p>For anyone reading this that might want his feature please upvote that suggestion.</p>
"
"56114068","Azure Data Factory Lookup Activity, Stored Procedure with Table Type parameter","<p>I'm setting up an Azure Data Factory to copy DB data between 2 Azure SQL Server. </p>

<p>I have a Stored Procedure at sink DB and SP has a user-defined table type parameter and some OUTPUT parameters.</p>

<p>I've succeeded with Copy Activity but found that OUTPUT parameters are not supported.
(<a href=""https://social.msdn.microsoft.com/Forums/azure/en-US/82e84ec4-fc40-4bd3-b6d5-b742f3cd1a33/adf-v2-how-to-check-if-stored-procedure-output-is-empty?forum=AzureDataFactory"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/azure/en-US/82e84ec4-fc40-4bd3-b6d5-b742f3cd1a33/adf-v2-how-to-check-if-stored-procedure-output-is-empty?forum=AzureDataFactory</a>)</p>

<p>So I tried with two Lookup Activity, one is to select multiple rows from the table(maybe into my table type), another is to execute SP. </p>

<p>But when setting the second Lookup Activity, there's no Table Type parameter setting with Stored Procedure Parameter.</p>

<p>Pressing ""Import parameter"" also didn't show a table type parameter.</p>

<p>How can I pass a table type parameter from lookup activity 1 to the second?</p>
","<azure><azure-data-factory>","2019-05-13 14:04:38","2112","0","1","56133965","<p>Table type is for sink stored procedure.
For look up activity, the stored procedure is used to read data. So I believe table type is not required.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#azure-sql-database-as-the-source"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#azure-sql-database-as-the-source</a></p>
"
"56112886","How to get just certain data from azure blob storage in Azure Data Factory performing copy activity with destination as SQL database?","<p>I am trying to perform Copy Activity inside Azure Data Factory where the source is Blob and destination is MySql database.</p>

<p>I do not want certain data to be copied during that activity and that can be determined by reading First key value Element of the Blob JSON.</p>

<p>Or maybe if it is not possible to stop reading all the BLOB data then atleast while writing inside the SQL table I want just some specific data.</p>

<p>I saw that there is Pre-copy Script in the copy activity. Can someone give an idea how to use it or what kind of script I can write there?</p>
","<sql><azure><etl><azure-blob-storage><azure-data-factory>","2019-05-13 12:59:38","104","0","1","56122669","<p>The pre copy script doesn't work for your requirement. you can use dataflow with a filter transformation to cleanse your data.</p>

<p><a href=""https://i.stack.imgur.com/XNDH8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XNDH8.png"" alt=""enter image description here""></a></p>

<p>The doc is <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-create"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-create</a></p>
"
"56111034","Is there any way to find the file which has error?","<p>There are multiple .gz files in my azure blob or file storage. Now, when I use the Data Factory pipelines to load the data, the copy data activity starts, &amp; then I monitor my pipeline, the activity had failed due to some inconsistent data. Infact, the pipeline has loaded some records.</p>

<p>So, is there any way to find which specific file has an error in data?</p>
","<azure><azure-storage><azure-data-factory>","2019-05-13 11:02:33","41","0","1","56111226","<p>You can log the failed rows here.
<a href=""https://i.stack.imgur.com/GBzYP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GBzYP.png"" alt=""pic""></a></p>
"
"56091107","Scheduling of jobs through SQL Server stored procedure","<p>I have to write a stored procedures for scheduling the Azure pipelines (Jobs).</p>

<ul>
<li>Frequency ----Number of times batch needs to run in a day</li>
<li>Timing column will have entry for batch start time</li>
</ul>

<p>Table A will have static entries for batches. Frequency denotes in a day how many times job will run and timing column will have the batch run time separated by comma(,)</p>

<pre><code>Batch_ID   Batch_Name  Frequency    Timing 
-----------------------------------------------
1          ABC           2          7:00,13:00
</code></pre>

<p>Table B will have listing of jobs corresponding to one particular batch.This table will be static and have one time entry like table B.</p>

<p>Table B</p>

<pre><code>Batch_ID   JOB_ID       JOB_NM
--------------------------------
1            1           Job_1
1            2           Job_1
</code></pre>

<p>Table C will contain the dependencies of the jobs in a batch</p>

<p>Table C</p>

<pre><code>Batch_ID    JOB_ID      DEPENDENY_JOB_ID
----------------------------------------
1             1
1             2              1
</code></pre>

<p>When Batch executes, table D will be populated with batch start time. </p>

<p>Table D</p>

<pre><code>Batch_ID   Batch_Name   Status    start_Time   end_time
-------------------------------------------------------
1             abc       Start     7:00
</code></pre>

<p>As soon as Table E is populated,table D will populated with Job details.Job 2 will start only when job 1 finishes.</p>

<p>Table E</p>

<pre><code>Batch_ID   Batch_Name  JOB_ID    JOB_NM      Start_Time     End_Time
----------------------------------------------------------------------
1            abc         1        Job_1       7:00
1            abc         2        Job_2       7:15
</code></pre>

<p>When Job 2 completes then we will update the Table D end time column.</p>

<p>Once first run is completed, we need to check frequency column of table A and run the job again (if it's more than 1) and do the entire exercise again.</p>

<p>In case our 1st batch didn't complete before the start time of batch 2 then we have to hold the 2nd batch until batch 1 is completed.</p>

<p>Could anyone help me how to start this?</p>
","<sql><sql-server><oracle><azure><azure-data-factory>","2019-05-11 14:10:31","75","0","1","56104589","<p>As @Gordon Linoff said, you are lacking a question on your ""question"". </p>

<p>If I can give an opinion on this, I dont think its a good design idea to split your logic between data factory and stored procedures in a database. Be mindful that in the future, the user mantaining the pipelines may not have access to the database and will not be able to understand half of it. Even if YOU are the one mantaining this, 2 years from now chances are you are going to forget what you did and following the line between 2 resources may take you more time than it should. It will also make troubleshooting harder.</p>

<p>It really depends on the scenario you are working on, but to sum it up: try to have everything logic related in one place.</p>

<p>Hope this helped!</p>
"
"56085286","How to force Azure Data Factory Data Flows to use Databricks","<p>I am working with Azure Data Factory and its new Data Flows feature. This is a GUI that is supposed to use Databricks to do data transformation, without writing any code. </p>

<p>All good so far. I have some examples working. My input data (from Azure Blob) is correctly transformed and joined to create the output (in Azure SQL). </p>

<p>The problem is that I have <em>no Databricks resource</em>. I deleted it. I also removed the Data Factory to Databricks connector. But I am still getting the right answers! </p>

<p>I suspect that my input sets are too small, or my transformations are too simple, so Data Factory is just handling them internally and knows it does not need the power of Databricks. But what do I have to do to force Data Factory to utilize Databricks? I want to test some things about that operation.</p>

<p>Another possibility is that Data Factory <em>is</em> using Databricks, but is doing so with its own Databricks resource rather than the users...??</p>
","<azure><azure-data-factory><dataflow><azure-databricks>","2019-05-10 21:55:38","981","2","1","56088728","<p>Azure Data Factory Data Flows <em>always</em> runs on Databricks behind-the-scenes. There is no way you can force (or disable) the use of Databricks.</p>

<p>In the early private preview, you had to configure and bring your own Databricks cluster. It was later changed, and as of May 2019, Azure Data Factory will manage the cluster for you.</p>

<p>(I have heard that they are planning to re-implement the bring-your-own-cluster feature at some point, but I haven't seen that confirmed publicly.)</p>

<p>If you turn on Data Flow Debug Mode or execute a pipeline with a Data Flow task, you will be billed for the cluster usage per vCore-hour. You can find all the details in the <a href=""https://azure.microsoft.com/en-us/pricing/details/data-factory/data-pipeline/"" rel=""nofollow noreferrer"">Data Pipeline Pricing and FAQ</a>.</p>
"
"56083827","Azure Data Factory sFTP Keep Connection Open","<p>I've created a pipeline in Data Factory that pulls several files from an sFTP server and places the data into Azure SQL Server Database tables.</p>

<p>Everything works fine in DF, but my sFTP server only allows 8 concurrent connections and 16 connections per minute. Data Factory is exceeding that limit because I believe it is disconnecting and reconnecting for each file.</p>

<p>I see in <a href=""https://learn.microsoft.com/en-us/connectors/ftp/"" rel=""nofollow noreferrer"">Azure <strong>FTP</strong> Connections</a> there is an option called <code>Close connection after request completion</code>. Will this alleviate my errors? Is there a similar flag for <strong>sFTP</strong> in Data Factory?</p>
","<azure><azure-data-factory>","2019-05-10 19:33:08","848","1","2","56127210","<p>The link that you shared is for a connector that is available only for Microsoft Flow, Power Apps and Logic Apps.</p>

<p>One way to achieve your intent is by using a custom activity in which you can write .NET code to configure and manage your SFTP connections. To read more about custom activities, please refer <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">this doc</a>.</p>
"
"56083827","Azure Data Factory sFTP Keep Connection Open","<p>I've created a pipeline in Data Factory that pulls several files from an sFTP server and places the data into Azure SQL Server Database tables.</p>

<p>Everything works fine in DF, but my sFTP server only allows 8 concurrent connections and 16 connections per minute. Data Factory is exceeding that limit because I believe it is disconnecting and reconnecting for each file.</p>

<p>I see in <a href=""https://learn.microsoft.com/en-us/connectors/ftp/"" rel=""nofollow noreferrer"">Azure <strong>FTP</strong> Connections</a> there is an option called <code>Close connection after request completion</code>. Will this alleviate my errors? Is there a similar flag for <strong>sFTP</strong> in Data Factory?</p>
","<azure><azure-data-factory>","2019-05-10 19:33:08","848","1","2","68193112","<p>To whom still suffering this.
We can limit the max connections in ADF, please see below screenshot.
<img src=""https://i.stack.imgur.com/EBErt.png"" alt=""SFTP connection limit"" /></p>
"
"56067722","Parameterize Azure Blob Storage Linked Service in ADF","<p>I would like to create an Azure Data Factory pipeline that copies a file to multiple storage accounts. My plan was to define the storage account connection info in a pipeline parameter as an array and use the ForEach activity to loop over each of the objects in the array passing the connection info to another pipeline. </p>

<pre><code>    [
  {
    ""destinationBlob"": {
      ""connectionString"": ""Conn1""
    }
  },
  {
    ""destinationBlob"": {
      ""connectionString"": ""Conn2""
    }
  },
  {
    ""destinationBlob"": {
      ""connectionString"": ""Conn3""
    }
  }
]
</code></pre>

<p>My question is, is it possible to parameterize the connection to an Azure Blob Storage Linked Service?</p>
","<azure><azure-blob-storage><azure-data-factory>","2019-05-09 21:19:17","1849","3","3","56072074","<p>Based on the <a href=""https://learn.microsoft.com/bs-latn-ba/azure/data-factory/parameterize-linked-services#supported-data-stores"" rel=""nofollow noreferrer"">document</a>, ADF parameterization of Linked Service does not support azure blob storage.</p>

<p><a href=""https://i.stack.imgur.com/8K0yv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8K0yv.png"" alt=""enter image description here""></a></p>

<p>So,you could copy the specific file into destinations apart only so far.</p>
"
"56067722","Parameterize Azure Blob Storage Linked Service in ADF","<p>I would like to create an Azure Data Factory pipeline that copies a file to multiple storage accounts. My plan was to define the storage account connection info in a pipeline parameter as an array and use the ForEach activity to loop over each of the objects in the array passing the connection info to another pipeline. </p>

<pre><code>    [
  {
    ""destinationBlob"": {
      ""connectionString"": ""Conn1""
    }
  },
  {
    ""destinationBlob"": {
      ""connectionString"": ""Conn2""
    }
  },
  {
    ""destinationBlob"": {
      ""connectionString"": ""Conn3""
    }
  }
]
</code></pre>

<p>My question is, is it possible to parameterize the connection to an Azure Blob Storage Linked Service?</p>
","<azure><azure-blob-storage><azure-data-factory>","2019-05-09 21:19:17","1849","3","3","56387542","<p>This can actually be done. Sample JSON:</p>
<pre><code>{
    &quot;name&quot;: &quot;DataLakeBlob&quot;,
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/linkedservices&quot;,
    &quot;properties&quot;: {
        &quot;parameters&quot;: {
            &quot;StorageAccountEndpoint&quot;: {
                &quot;type&quot;: &quot;String&quot;,
                &quot;defaultValue&quot;: &quot;https://testblobstorage.blob.core.windows.net&quot;
            }
        },
        &quot;type&quot;: &quot;AzureBlobStorage&quot;,
        &quot;typeProperties&quot;: {
            &quot;serviceEndpoint&quot;: &quot;@{linkedService().StorageAccountEndpoint}&quot;
        },
        &quot;description&quot;: &quot;Test Description&quot;
    }
}
</code></pre>
"
"56067722","Parameterize Azure Blob Storage Linked Service in ADF","<p>I would like to create an Azure Data Factory pipeline that copies a file to multiple storage accounts. My plan was to define the storage account connection info in a pipeline parameter as an array and use the ForEach activity to loop over each of the objects in the array passing the connection info to another pipeline. </p>

<pre><code>    [
  {
    ""destinationBlob"": {
      ""connectionString"": ""Conn1""
    }
  },
  {
    ""destinationBlob"": {
      ""connectionString"": ""Conn2""
    }
  },
  {
    ""destinationBlob"": {
      ""connectionString"": ""Conn3""
    }
  }
]
</code></pre>

<p>My question is, is it possible to parameterize the connection to an Azure Blob Storage Linked Service?</p>
","<azure><azure-blob-storage><azure-data-factory>","2019-05-09 21:19:17","1849","3","3","58111726","<p>Edit: This was acknowledged by Microsoft. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services#"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services#</a></p>

<p>For those looking for SAS token parameterization, you could use following JSON.
Be sure to check the checkbox like in screenshot for the JSON to apply.
<a href=""https://i.stack.imgur.com/8DjSw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8DjSw.png"" alt=""enter image description here""></a></p>

<pre><code>{
  ""type"":""Microsoft.DataFactory/factories/linkedservices"",
  ""properties"":{
  ""parameters"": {
        ""StorageAccountEndpoint"": {
            ""type"": ""String"",
            ""defaultValue"": ""https://&lt;&lt;yourstorageaccountname&gt;&gt;.blob.core.windows.net/?sv=2018-03-28&amp;ss=b&amp;srt=sco&amp;sp=rwdlac&amp;se=2019-10-20T16:33:57Z&amp;st=2019-09-20T08:33:57Z&amp;spr=https&amp;sig=lDrBjD%2BjM2T1XjRW997VPMqDp99ZxVoReyRK0VEX7zQ%3D""
        }
    },
    ""type"": ""AzureBlobStorage"",
    ""typeProperties"": {
        ""sasUri"": ""@{linkedService().StorageAccountEndpoint}""
    }

}}
</code></pre>
"
"56064440","Microsoft.DataTransfer.Common.Shared.HybridDeliveryException TypeName cannot be null","<p>I have a data factory that copies data from a restful webservice into an Azure Data Warehouse. I have tested and previewed all connections and datasets.<br>
I'm receiving the following error message.</p>

<pre><code>{
    ""errorCode"": ""2200"",  
    ""message"":   ""ErrorCode=InvalidParameter,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The value of the property &amp;apos;typeName&amp;apos; is invalid: &amp;apos;Value cannot be null.\r\nParameter name: typeName&amp;apos;.,Source=,''Type=System.ArgumentNullException,Message=Value cannot be null.\r\nParameter name: typeName,Source=Microsoft.DataTransfer.Common,'"",
    ""failureType"": ""UserError"",  
    ""target"": ""ImportLegs""  
}
</code></pre>

<p>Pipeline source</p>

<pre><code>{
    ""name"": ""Import Trip Data"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""ImportLegs"",
                ""type"": ""Copy"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""userProperties"": [
                    {
                        ""name"": ""Source"",
                        ""value"": ""flightleg?StartDate=01/01/2018&amp;EndDate=02/01/2018""
                    },
                    {
                        ""name"": ""Destination"",
                        ""value"": ""[Trip].[Leg]""
                    }
                ],
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""RestSource"",
                        ""httpRequestTimeout"": ""00:01:40"",
                        ""requestInterval"": ""00.00:00:00.010""
                    },
                    ""sink"": {
                        ""type"": ""SqlDWSink"",
                        ""allowPolyBase"": false,
                        ""writeBatchSize"": 10000
                    },
                    ""enableStaging"": false,
                    ""enableSkipIncompatibleRow"": true,
                    ""translator"": {
                        ""type"": ""TabularTranslator"",
                        ""mappings"": [
                            {
                                ""source"": {
                                    ""path"": ""id""
                                },
                                ""sink"": {
                                    ""name"": ""Origin""
                                }
                            },
                            {
                                ""source"": {
                                    ""path"": ""actualArrivalDateLocal""
                                },
                                ""sink"": {
                                    ""name"": ""Destination""
                                }
                            },
                            {
                                ""source"": {
                                    ""path"": ""actualArrivalDateUTC""
                                },
                                ""sink"": {
                                    ""name"": ""FlightLogDistance""
                                }
                            },
                            {
                                ""source"": {
                                    ""path"": ""actualBlockTime""
                                },
                                ""sink"": {
                                    ""name"": ""FlightLogFlightTime""
                                }
                            },
                            {
                                ""source"": {
                                    ""path"": ""actualDepartureDateLocal""
                                },
                                ""sink"": {
                                    ""name"": ""Aircraft""
                                }
                            },
                            {
                                ""source"": {
                                    ""path"": ""actualDepartureDateUTC""
                                },
                                ""sink"": {
                                    ""name"": ""ScheduledDepartDate""
                                }
                            }
                        ]
                    }
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""FlightLeg"",
                        ""type"": ""DatasetReference""
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""TripLegDW"",
                        ""type"": ""DatasetReference""
                    }
                ]
            }
        ]
    },
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>
","<azure><azure-data-factory><azure-synapse>","2019-05-09 17:13:17","6318","0","1","60199579","<p>I also had same error and after some research i found some solution. posting to help someone further. </p>

<p>Error-</p>

<pre><code>{
""errorCode"": ""2200"",
""message"": ""ErrorCode=UserErrorFailedS3FileReadOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The operation on file part-00000-.csv.gz under directory test-bucket/abc_backfill/abc_visits is failed due to exception. ,Source=Microsoft.DataTransfer.ClientLibrary.MultipartBinaryConnector,''Type=Amazon.S3.AmazonS3Exception,Message=Error making request with Error Code Forbidden and Http Status Code Forbidden. No further error information was returned by the service.,Source=AWSSDK.Core,''Type=Amazon.Runtime.Internal.HttpErrorResponseException,Message=The remote server returned an error: (403) Forbidden.,Source=AWSSDK.Core,''Type=System.Net.WebException,Message=The remote server returned an error: (403) Forbidden.,Source=System,'"",
""failureType"": ""UserError"",
""target"": ""Copy data1"",
""details"": []
</code></pre>

<p>}</p>

<p>Solution - I found this is some access issue. even while testing connection it was giving success but when executing pipeline it was failing with above message. i found, i am accessing wrong dir. as above abc_backfill is root dir or shared dir that having abc_visits and it was giving issue.</p>

<p>I found i have another shared dir with abc_testbackfill which pointing to same abc_visits. once i changed to abc_testbackfill in dataset. it started working.
so i belive i didn't had access on abc_backfill share and i was getting error.</p>
"
"56062241","How to copy complete tables between PostgreSQL Databases with Azure Data Factory","<p>I want to <strong>copy some tables</strong> from a <strong>production system</strong> to a <strong>test system</strong> on a regular basis. Both systems run a PostgreSQL server. I want to copy only specific tables from production to test.</p>

<p>I´ve already set up a <strong>foreach which iterates over the table names</strong> I want to copy. The problem is, that the table structures may change during development process and the copy job might fail.</p>

<p>So is there a way to use some kind auf ""automatic mapping""? Cause the tables in both systems always have exactly the same structure. Or is there some kind of ""Copy table"" procedure?</p>
","<azure><azure-data-factory>","2019-05-09 14:56:24","96","0","1","56062706","<p>You could remove mapping and structure in your pipeline . Then it will using the default mapping behavior. Given your tables always have the same schema, both mapping by name and mapping by order should work.</p>
"
"56058155","Creating a container in azure storage before uploading the data using ADF v2","<p>Thanks in advance, I'm new to ADF and have created a pipeline from ADF portal. Source On premise server folder and destination dataset is Azure Blob storage. I'm using tumbling window which passes the date start time and 
Date End time and only uploads the latest data using lastmodified datetime.</p>

<blockquote>
  <p>Query : If i want to create subcontainers on fly in azure storage i use /container/$monthvariable and it automatically creates a subcontainer based on the month variable</p>
</blockquote>

<p>example here my source is </p>

<pre><code>dfac/
$monthvariable = 5
</code></pre>

<p>if i put </p>

<pre><code>dfac/$monthvariable
</code></pre>

<p>then all the files will be uploaded under dfac/5/ and will look like below</p>

<pre><code>dfac/5/file1
dfac/5/file2
dfac/5/file3
</code></pre>

<p>Here in ADF i wanted to get the month of the pipeline month and add that in pipeline. Is that what i can do? and where can i define the variable?</p>

<pre><code>  {
            ""name"": ""Destination"",
            ""value"": ""dfac/$monthvariable""// does it work and is this the right way to do this stuff
        }
</code></pre>

<p>My Actual code looks like below.</p>

<pre><code>{
    ""name"": ""Copy_ayy"",
    ""type"": ""Copy"",
    ""policy"": {
        ""timeout"": ""7.00:00:00"",
        ""retry"": 2,
        ""retryIntervalInSeconds"": 30,
        ""secureOutput"": false,
        ""secureInput"": false
    },
    ""userProperties"": [
        {
            ""name"": ""Source"",
            ""value"": ""/*""
        },
        {
            ""name"": ""Destination"",
            ""value"": ""dfac/""
        }
    ],
    ""typeProperties"": {
        ""source"": {
            ""type"": ""FileSystemSource"",
            ""recursive"": true
        },
        ""sink"": {
            ""type"": ""BlobSink"",
            ""copyBehavior"": ""PreserveHierarchy""
        },
        ""enableStaging"": false
    },
    ""inputs"": [
        {
            ""referenceName"": ""SourceDataset_ayy"",
            ""type"": ""DatasetReference"",
            ""parameters"": {
                ""cw_modifiedDatetimeStart"": ""@pipeline().parameters.windowStart"",
                ""cw_modifiedDatetimeEnd"": ""@pipeline().parameters.windowEnd""
            }
        }
    ],
    ""outputs"": [
        {
            ""referenceName"": ""DestinationDataset_ayy"",
            ""type"": ""DatasetReference""
        }
    ]
}
</code></pre>
","<azure><azure-storage><azure-data-factory>","2019-05-09 11:14:19","115","0","1","56062550","<p>I believe you are using copy data tool. Then you can also use it help you for the destination path part. It will help you create the parameters. 
<a href=""https://i.stack.imgur.com/0fQLb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0fQLb.png"" alt=""enter image description here""></a></p>
"
"56048460","Error using data factory for copyactivity from blob storage as source","<p>Why do I keep getting this error while using a folder from a blob container  as source (which contains only one GZ compressed file) in copy activity in data factory v2 and as sink another blob storage (but I want the file decompressed)?</p>

<pre><code> ""message"":""ErrorCode=UserErrorFormatIsRequired,
'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,
Message=Format setting is required for file based store(s) in this scenario.,Source=Microsoft.DataTransfer.ClientLibrary,'"",
</code></pre>

<p>I know it means I need to specify explicitly the format for my sink dataset, but I am not sure how to do that.</p>
","<azure><azure-data-lake><azure-data-factory>","2019-05-08 20:18:20","1406","1","3","56051047","<p>The easiest way to do this: go to the dataset, and click on the tab Schema, then Import Schema.</p>

<p><a href=""https://i.stack.imgur.com/G0Xux.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G0Xux.png"" alt=""enter image description here""></a></p>

<p>Hope this helped!!</p>
"
"56048460","Error using data factory for copyactivity from blob storage as source","<p>Why do I keep getting this error while using a folder from a blob container  as source (which contains only one GZ compressed file) in copy activity in data factory v2 and as sink another blob storage (but I want the file decompressed)?</p>

<pre><code> ""message"":""ErrorCode=UserErrorFormatIsRequired,
'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,
Message=Format setting is required for file based store(s) in this scenario.,Source=Microsoft.DataTransfer.ClientLibrary,'"",
</code></pre>

<p>I know it means I need to specify explicitly the format for my sink dataset, but I am not sure how to do that.</p>
","<azure><azure-data-lake><azure-data-factory>","2019-05-08 20:18:20","1406","1","3","56056646","<p>I suggest using the copy data tool.</p>

<p>step 1</p>

<p><a href=""https://i.stack.imgur.com/hlc4V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hlc4V.png"" alt=""copy tool""></a></p>

<p>step 2
<a href=""https://i.stack.imgur.com/MjYiD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MjYiD.png"" alt=""binary copy""></a></p>
"
"56048460","Error using data factory for copyactivity from blob storage as source","<p>Why do I keep getting this error while using a folder from a blob container  as source (which contains only one GZ compressed file) in copy activity in data factory v2 and as sink another blob storage (but I want the file decompressed)?</p>

<pre><code> ""message"":""ErrorCode=UserErrorFormatIsRequired,
'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,
Message=Format setting is required for file based store(s) in this scenario.,Source=Microsoft.DataTransfer.ClientLibrary,'"",
</code></pre>

<p>I know it means I need to specify explicitly the format for my sink dataset, but I am not sure how to do that.</p>
","<azure><azure-data-lake><azure-data-factory>","2019-05-08 20:18:20","1406","1","3","56057362","<p>According you comment, I tried a lot times, unless you choose the compressed file as source dataset and import the schemas,  Azure Data factory copy actives will not help you decompress the file.</p>

<p>If the files in the the compressed file don't have the same schema, the copy active also could be failed. </p>

<p>Hope this helps.</p>
"
"56031451","Azure Data Factory using existing cluster in Databricks","<p>I have created a pipeline in Azure Data Factory. I created a Databricks workspace, notebook (with some code), and a cluster. I created the connection from ADF to DB. I tested the connection. All lights are green. I published the ADF pipeline. </p>

<p>When I trigger the job, it says SUCCESS. But nothing happens in Databricks. No job is created in DB. The code in the notebook cell is apparently not executed. (I know this because the code prints the current time.)</p>

<p>Has anyone done this successfully? </p>

<p>To be clear, I want Data Factory to use an <em>existing</em> cluster in Databricks, not create a new one. I have named the cluster in the pipeline setup params.</p>
","<azure-data-factory><azure-databricks>","2019-05-07 22:55:52","3596","1","2","56032370","<p>Please reference this tutorial: <a href=""https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/data-factory/transform-data-using-databricks-notebook.md"" rel=""nofollow noreferrer"">Run a Databricks notebook with the Databricks Notebook Activity in Azure Data Factory</a>.</p>

<p>In this tutorial, you use the Azure portal to create an Azure Data Factory pipeline that executes a Databricks notebook against the Databricks jobs cluster. It also passes Azure Data Factory parameters to the Databricks notebook during execution.</p>

<p>You perform the following steps in this tutorial:</p>

<ol>
<li>Create a data factory.</li>
<li>Create a pipeline that uses Databricks Notebook Activity.</li>
<li>Trigger a pipeline run.</li>
<li>Monitor the pipeline run.</li>
</ol>

<p>One of the difference is you don't need to create <code>new job cluster</code>, select <code>use an existing cluster</code>.</p>

<p>Hope this helps.</p>
"
"56031451","Azure Data Factory using existing cluster in Databricks","<p>I have created a pipeline in Azure Data Factory. I created a Databricks workspace, notebook (with some code), and a cluster. I created the connection from ADF to DB. I tested the connection. All lights are green. I published the ADF pipeline. </p>

<p>When I trigger the job, it says SUCCESS. But nothing happens in Databricks. No job is created in DB. The code in the notebook cell is apparently not executed. (I know this because the code prints the current time.)</p>

<p>Has anyone done this successfully? </p>

<p>To be clear, I want Data Factory to use an <em>existing</em> cluster in Databricks, not create a new one. I have named the cluster in the pipeline setup params.</p>
","<azure-data-factory><azure-databricks>","2019-05-07 22:55:52","3596","1","2","56058625","<p>Solved. The problem was that the notebook (containing my code) was within my User notebook folder. Data Factory did not have permission to see/use my notebook. I created the same notebook within the Shared folder and everything works fine.</p>

<p>I will point out that ADF should issue an error/warning if the named notebook cannot be seen or used. The ADF pipeline verified fine, reported a successful run, but just failed silently. </p>
"
"56027764","How to run a remote command (powershell/bash) against an existing Azure VM in Azure Data Factory V2?","<p>I've been trying to find a way to run a simple command against one of my existing Azure VMs using Azure Data Factory V2.</p>

<p>Options so far:</p>

<ul>
<li>Custom Activity/Azure Batch won't let me add existing VMs to the pool</li>
<li>Azure Functions - I have not played with this but I have not found any documentation on this using AZ Functions.</li>
<li>Azure Cloud Shell - I've tried <a href=""https://learn.microsoft.com/en-us/azure/virtual-machines/windows/run-command#powershell"" rel=""nofollow noreferrer"">this</a> using the browser UI and it works, however I cannot find a way of doing this via ADF V2</li>
</ul>

<p>The use case is the following:</p>

<p>There are a few tasks that are running locally (Azure VM) in task scheduler that I'd like to orchestrate using ADF as everything else is in ADF, these tasks are usually python applications that restore a SQL Backup and or purge some folders.</p>

<p>i.e.  <code>sqdb-restore -r myDatabase</code></p>

<p>where <code>sqldb-restore</code> is a command that is recognized locally after installing my local python library. Unfortunately the python app needs to live locally in the VM.</p>

<p>Any suggestions? Thanks.</p>
","<azure><powershell><azure-data-factory><azure-automation>","2019-05-07 17:31:05","1679","2","2","56028563","<p>This looks like a use case that is supported with Azure Automation, using a hybrid worker. Try reading here: <a href=""https://learn.microsoft.com/en-us/azure/automation/automation-hybrid-runbook-worker"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/automation/automation-hybrid-runbook-worker</a></p>

<p>You can call runbooks with webhooks in ADFv2, using the web activity.</p>

<p>Hope this helped!</p>
"
"56027764","How to run a remote command (powershell/bash) against an existing Azure VM in Azure Data Factory V2?","<p>I've been trying to find a way to run a simple command against one of my existing Azure VMs using Azure Data Factory V2.</p>

<p>Options so far:</p>

<ul>
<li>Custom Activity/Azure Batch won't let me add existing VMs to the pool</li>
<li>Azure Functions - I have not played with this but I have not found any documentation on this using AZ Functions.</li>
<li>Azure Cloud Shell - I've tried <a href=""https://learn.microsoft.com/en-us/azure/virtual-machines/windows/run-command#powershell"" rel=""nofollow noreferrer"">this</a> using the browser UI and it works, however I cannot find a way of doing this via ADF V2</li>
</ul>

<p>The use case is the following:</p>

<p>There are a few tasks that are running locally (Azure VM) in task scheduler that I'd like to orchestrate using ADF as everything else is in ADF, these tasks are usually python applications that restore a SQL Backup and or purge some folders.</p>

<p>i.e.  <code>sqdb-restore -r myDatabase</code></p>

<p>where <code>sqldb-restore</code> is a command that is recognized locally after installing my local python library. Unfortunately the python app needs to live locally in the VM.</p>

<p>Any suggestions? Thanks.</p>
","<azure><powershell><azure-data-factory><azure-automation>","2019-05-07 17:31:05","1679","2","2","56045787","<p>Thanks to @martin-esteban-zurita, his answer helped me to get to what I needed and this was a beautiful and fun experiment.</p>

<p>It is important to understand that <a href=""https://learn.microsoft.com/en-us/azure/automation/automation-intro"" rel=""nofollow noreferrer"">Azure Automation</a> is used for many things regarding resource orchestration in Azure (VMs, Services, DevOps), this automation can be done with Powershell and/or Python. </p>

<p>In this particular case I did not need to modify/maintain/orchestrate any Azure resource, I needed to actually run a Bash/Powershell command remotely into one of my existing VMs where I have multiple Powershell/Bash commands running recurrently in ""Task Scheduler"".
""Task Scheduler"" was adding unnecessary overhead to my data pipelines because it was unable to talk to ADF. </p>

<p>In addition, Azure Automation natively only runs Powershell/Python commands in Azure Cloud Shell which is very useful to orchestrate resources like turning on/off Azure VMs, adding/removing permissions from other Azure services, running maintenance or purge processes, etc, but I was still unable to run commands locally in an existing VM. This is where the <a href=""https://learn.microsoft.com/en-us/azure/automation/automation-hybrid-runbook-worker"" rel=""nofollow noreferrer"">Hybrid Runbook Worker</a> came into to picture. A Hybrid worker group</p>

<p>These are the steps to accomplish this use case.</p>

<p><strong>1. Create an Azure Automation Account</strong></p>

<p><strong>2. <a href=""https://learn.microsoft.com/en-us/azure/automation/automation-windows-hrw-install#automated-deployment"" rel=""nofollow noreferrer"">Install</a> the Windows Hybrid Worker in my existing VM . In my case it was tricky because my proxy was giving me some errors. I ended up downloading the Nuget Package and manually installing it.</strong></p>

<pre><code>.\New-OnPremiseHybridWorker.ps1 -AutomationAccountName &lt;NameofAutomationAccount&gt; -AAResourceGroupName &lt;NameofResourceGroup&gt;
-OMSResourceGroupName &lt;NameofOResourceGroup&gt; -HybridGroupName &lt;NameofHRWGroup&gt;
-SubscriptionId &lt;AzureSubscriptionId&gt; -WorkspaceName &lt;NameOfLogAnalyticsWorkspace&gt;
</code></pre>

<p>Keep in mind that in the above code, you will need to find your own parameter values, the only parameter that does not have to be found and will be created is <code>HybridGroupName</code> this will define the name of the Hybrid Group</p>

<p><strong>3. Create a PowerShell Runbook</strong> </p>

<pre><code>[CmdletBinding()]
Param
([object]$WebhookData) #this parameter name needs to be called WebHookData otherwise the webhook does not work as expected.
$VerbosePreference = 'continue'

#region Verify if Runbook is started from Webhook.

# If runbook was called from Webhook, WebhookData will not be null.
if ($WebHookData){

    # Collect properties of WebhookData
    $WebhookName     =     $WebHookData.WebhookName
    # $WebhookHeaders  =     $WebHookData.RequestHeader
    $WebhookBody     =     $WebHookData.RequestBody

    # Collect individual headers. Input converted from JSON.
    $Input = (ConvertFrom-Json -InputObject $WebhookBody)
    # Write-Verbose ""WebhookBody: $Input""
    #Write-Output -InputObject ('Runbook started from webhook {0} by {1}.' -f $WebhookName, $From)
}
else
{
   Write-Error -Message 'Runbook was not started from Webhook' -ErrorAction stop
}
#endregion

# This is where I run the commands that were in task scheduler

$callBackUri = $Input.callBackUri

 # This is extremely important for ADF
 Invoke-WebRequest -Uri $callBackUri -Method POST
</code></pre>

<p><strong>4. Create a Runbook Webhook pointing to the Hybrid Worker's VM</strong></p>

<p><a href=""https://i.stack.imgur.com/qE8pq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qE8pq.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/FHecS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FHecS.png"" alt=""enter image description here""></a></p>

<p><strong>4. Create a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-webhook-activity"" rel=""nofollow noreferrer"">webhook activity</a> in ADF where the above PowerShell runbook script will be called via a POST Method</strong></p>

<p>Important Note: When I created the webhook activity it was timing out after 10 minutes (default), so I noticed in the Azure Automation Account that I was actually getting INPUT data (WEBHOOKDATA) that contained a JSON structure with the following elements:</p>

<ul>
<li>WebhookName</li>
<li>RequestBody (This one contains whatever you add in the Body plus a default element called callBackUri) </li>
</ul>

<p>All I had to do was to invoke the <code>callBackUri</code> from Azure Automation. And this is why  in the PowerShell runbook code I added <code>Invoke-WebRequest -Uri $callBackUri -Method POST</code>. With this, ADF was succeeding/failing instead of timing out.</p>

<p>There are many other details that I struggled with when installing the hybrid worker in my VM but those are more specific to your environment/company.</p>
"
"56023614","How to zip many blob files into one .gzip using Azure Data Factory GUI?","<p>For example</p>

<p>ParentFolder
   File1
   File2
   ChildFolder
       File3
       File4</p>

<p>to </p>

<p>ParentFolder.gzip</p>

<p>I have trouble setting up the storage sink location for the compressed blobs. The best result I get is:</p>

<p>File1.gzip
File2.gzip
etc...</p>
","<azure><gzip><azure-data-factory>","2019-05-07 13:20:44","87","1","1","56054596","<p>Azure Data Factory doesn't support Zip a folder, I think you can only connect that <strong>Copy</strong> activity with a custom activity to Zip it by your customized code.</p>
"
"56018741","Azure Data Factory REST how to manage pagination as a GET url parameter retrieving a pagination id from previous call result","<p>I have an existing data source in Azure Data Factory calling a REST API</p>

<p>My last working config (built with the UI) without pagination uses dataset parameters to build a relative URL, it translates in the source as :</p>

<pre><code>""typeProperties"": {
  ""relativeUrl"": {
    ""value"": ""@concat('theRoute?cursor=',dataset().cursor,'&amp;nb=100&amp;q=aDateParameter:[',dataset().startDate, ' TO ', dataset().endDate, ']')"",
    ""type"": ""Expression""
  },
  ""requestMethod"": ""GET"",
}
</code></pre>

<p>(With the dataset parameter ""cursor"" default value being ""*"" to initialize a pagination)</p>

<p>Then in each response I will have to retrieve #.header.nextCursor to get the nextCursor id (# being the json root of the response), and before anybody asks, yes, there is indeed a nested doc named ""header"" in the json body, nextCursor is not in the http headers. </p>

<p>So I will have to replace in my relative URL the value of the ""cursor"" parameter with the id in #.header.cursor. But I fails to create the pagination rules. (Even if the doc explicitly list a ""Next request’s query parameter = property value in current response body"" scenario).</p>

<ul>
<li><p>Creating a new relative url in the pagination rules fails because I apparently can't use the #.header.json syntax in the Expression builder.</p></li>
<li><p>The QueryParameters syntax I found in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest#pagination-support"" rel=""nofollow noreferrer"">The doc</a> fails. I don't understand the doc. I tried 3 different ways 1) <code>""QueryParameters.cursor"": ""#.header.nextCursor</code> 2) <code>QueryParameters['cursor']: ""#.header.nextCursor</code> and 3) (which makes more sense to me) <code>""QueryParameters"": {""cursor"": ""#.header.nextCursor""}</code> But none worked.</p></li>
</ul>

<p>Also I'm wondering if it would simply inject this to replace the actual cursor parameter in my previous relativeURL... Or if I would somehow have to find another way than my concat expression to pass my url parameters for the original relative url (I can't find any documentation on a way to eventually use this QueryParameter syntax outside pagination rules, nor any doc or example for a POST data body dynamic construction syntax)</p>
","<rest><pagination><azure-data-factory><relative-url>","2019-05-07 08:45:20","3039","0","1","56124153","<p>If my understanding is correct, the relative URL for each page looks like 'theRoute?cursor={cursor}&amp;nb=100&amp;q=aDateParameter:[{startDate} TO {endDate}]'.</p>

<p>The {cursor} is from the JSON response body, $.header.nextCursor.</p>

<p>Sorry that the pagination rule today doesn’t support expressions like this, and we are aware of this kind of customer ask.  We will look into it to see how to support it in the future.</p>
"
"55995566","How do I Unzip and Execute a Batch Service job as part of Azure Data Factory","<p>Azure Data Factory can execute custom activities as Batch Service Jobs.  These jobs can run from an .exe (and associated dependencies) in a storage account which are copied across prior to execution.</p>

<p>There is a limitation on the files in the storage account that can be used:</p>

<blockquote>
  <p>Total size of resourceFiles cannot be more than 32768 characters</p>
</blockquote>

<p>The solution appears to be to zip the files in the storage account and unzip as part of the command.  <a href=""https://social.msdn.microsoft.com/Forums/en-US/ab57e810-94d7-4a48-a358-649f607c9717/azure-batch-resourcefiles-limitation"" rel=""nofollow noreferrer"">This post</a> suggests running the Batch Service Command in Azure Data Factory as:</p>

<blockquote>
  <p>Unzip.exe [myZipFilename] &amp;&amp; MyExeName.exe [cmdLineArgs]</p>
</blockquote>

<p>Running this locally on a Windows 10 machine works fine.  Setting this as the Command parameter on the batch service custom activity (using a Cloud Services Windows Server 2019 OS Image App Pool) results in:</p>

<blockquote>
  <p>caution: filename not matched:  &amp;&amp;</p>
</blockquote>

<p>It feels like something basic that I'm missing but I've tried various permutations and cannot get it to work.</p>
","<azure><cmd><unzip><azure-data-factory><azure-batch>","2019-05-05 19:22:36","1792","1","4","55996806","<p>Sharing few possibilities which could be happenening, I agree that something small is missing, if you want to share the exact commandline you are trying then feel free to share.</p>

<p>Few ideas which could be causing this behavioour:</p>

<ul>
<li><p>Please try your command-line under quotes from batch point of view like: <code>cmd /c ""Unzip.exe [myZipFilename] &amp;&amp; MyExeName.exe [cmdLineArgs]""</code> </p></li>
<li><p>Make sure your file exist i.e. one possibility is : that in the end the command is trying to run an empty string with <code>&amp;&amp;</code> i.e. <code>cmd /c ""unzip.exe ""empty"" &amp;&amp; ...</code> </p></li>
</ul>

<p>Hope one of the above 2 fixes, or feel free to add more detail et. al.</p>
"
"55995566","How do I Unzip and Execute a Batch Service job as part of Azure Data Factory","<p>Azure Data Factory can execute custom activities as Batch Service Jobs.  These jobs can run from an .exe (and associated dependencies) in a storage account which are copied across prior to execution.</p>

<p>There is a limitation on the files in the storage account that can be used:</p>

<blockquote>
  <p>Total size of resourceFiles cannot be more than 32768 characters</p>
</blockquote>

<p>The solution appears to be to zip the files in the storage account and unzip as part of the command.  <a href=""https://social.msdn.microsoft.com/Forums/en-US/ab57e810-94d7-4a48-a358-649f607c9717/azure-batch-resourcefiles-limitation"" rel=""nofollow noreferrer"">This post</a> suggests running the Batch Service Command in Azure Data Factory as:</p>

<blockquote>
  <p>Unzip.exe [myZipFilename] &amp;&amp; MyExeName.exe [cmdLineArgs]</p>
</blockquote>

<p>Running this locally on a Windows 10 machine works fine.  Setting this as the Command parameter on the batch service custom activity (using a Cloud Services Windows Server 2019 OS Image App Pool) results in:</p>

<blockquote>
  <p>caution: filename not matched:  &amp;&amp;</p>
</blockquote>

<p>It feels like something basic that I'm missing but I've tried various permutations and cannot get it to work.</p>
","<azure><cmd><unzip><azure-data-factory><azure-batch>","2019-05-05 19:22:36","1792","1","4","56039012","<p>Without full knowledge of the context in which ADF runs Custom Activity Commands on a Windows Batch Service Node I changed my setup to avoid expecting Unzip.exe to exist (which it appears not to when running under <code>cmd /c ""Unzip.exe""</code> rather than with just <code>Unzip.exe</code> as the command).</p>

<p>Now my storage account contents backing the custom activity has:</p>

<ul>
<li>executable.zip (my .NET Core Console application published for windows with all dependencies)</li>
<li>unzip.exe (taken from Git Bash on my local machine)

<ul>
<li>including the msys-2.0.dll and msys-bz2-1.dll dependencies</li>
</ul></li>
</ul>

<p>The command in ADF is then:</p>

<pre><code>cmd /c ""Unzip.exe executable-with-deps.zip &amp;&amp; executable.exe""
</code></pre>
"
"55995566","How do I Unzip and Execute a Batch Service job as part of Azure Data Factory","<p>Azure Data Factory can execute custom activities as Batch Service Jobs.  These jobs can run from an .exe (and associated dependencies) in a storage account which are copied across prior to execution.</p>

<p>There is a limitation on the files in the storage account that can be used:</p>

<blockquote>
  <p>Total size of resourceFiles cannot be more than 32768 characters</p>
</blockquote>

<p>The solution appears to be to zip the files in the storage account and unzip as part of the command.  <a href=""https://social.msdn.microsoft.com/Forums/en-US/ab57e810-94d7-4a48-a358-649f607c9717/azure-batch-resourcefiles-limitation"" rel=""nofollow noreferrer"">This post</a> suggests running the Batch Service Command in Azure Data Factory as:</p>

<blockquote>
  <p>Unzip.exe [myZipFilename] &amp;&amp; MyExeName.exe [cmdLineArgs]</p>
</blockquote>

<p>Running this locally on a Windows 10 machine works fine.  Setting this as the Command parameter on the batch service custom activity (using a Cloud Services Windows Server 2019 OS Image App Pool) results in:</p>

<blockquote>
  <p>caution: filename not matched:  &amp;&amp;</p>
</blockquote>

<p>It feels like something basic that I'm missing but I've tried various permutations and cannot get it to work.</p>
","<azure><cmd><unzip><azure-data-factory><azure-batch>","2019-05-05 19:22:36","1792","1","4","65685376","<p>I had the same problem and solved it with a short powershell script, that extracts my zip and start my Program. The generic script look like (called unzipandrun.ps1):</p>
<pre><code>[CmdletBinding()]
param (    
    [Parameter(Mandatory = $true)][String]$zipFile,    
    [Parameter(Mandatory = $true)][String]$executable,    
    [Parameter(Mandatory = $true)][String]$prm
)
#######
# Unzip
#######
Add-Type -AssemblyName System.IO.Compression.FileSystem
function Unzip
{
    param([string]$zipfile, [string]$outpath)
    [System.IO.Compression.ZipFile]::ExtractToDirectory($zipfile, $outpath)
}
&quot;Start Unzip $zipFile&quot; | Write-Host
Unzip $zipFile $pwd
###########
# Start exe
###########
&quot;Start $executable $prm&quot; | Write-Host
$cmd = &quot;&amp; $executable $prm&quot;
Invoke-Expression $cmd
</code></pre>
<p>So if I want to start myapp.exe with parameter /t 123 on azure batch I would start:</p>
<pre><code>PowerShell -NoProfile -ExecutionPolicy Bypass -Command &quot;unzipandrun.ps1 '/t 123'&quot;
</code></pre>
"
"55995566","How do I Unzip and Execute a Batch Service job as part of Azure Data Factory","<p>Azure Data Factory can execute custom activities as Batch Service Jobs.  These jobs can run from an .exe (and associated dependencies) in a storage account which are copied across prior to execution.</p>

<p>There is a limitation on the files in the storage account that can be used:</p>

<blockquote>
  <p>Total size of resourceFiles cannot be more than 32768 characters</p>
</blockquote>

<p>The solution appears to be to zip the files in the storage account and unzip as part of the command.  <a href=""https://social.msdn.microsoft.com/Forums/en-US/ab57e810-94d7-4a48-a358-649f607c9717/azure-batch-resourcefiles-limitation"" rel=""nofollow noreferrer"">This post</a> suggests running the Batch Service Command in Azure Data Factory as:</p>

<blockquote>
  <p>Unzip.exe [myZipFilename] &amp;&amp; MyExeName.exe [cmdLineArgs]</p>
</blockquote>

<p>Running this locally on a Windows 10 machine works fine.  Setting this as the Command parameter on the batch service custom activity (using a Cloud Services Windows Server 2019 OS Image App Pool) results in:</p>

<blockquote>
  <p>caution: filename not matched:  &amp;&amp;</p>
</blockquote>

<p>It feels like something basic that I'm missing but I've tried various permutations and cannot get it to work.</p>
","<azure><cmd><unzip><azure-data-factory><azure-batch>","2019-05-05 19:22:36","1792","1","4","69390366","<p>Another option, which doesn't require copying additional files along with zip file:</p>
<pre><code>cmd /c &quot;tar -xf MyExe.zip &amp; MyExe.exe&quot;
</code></pre>
<p><code>tar -xf MyExe.zip &amp; MyExe.exe</code> gives an error, but <code>cmd /c &quot;tar -xf MyExe.zip &amp; MyExe.exe&quot;</code> works fine on windows 10</p>
"
"55974980","How can I move files from a local file server (SFTP) directly into Snowflake?","<p>I am looking to move files from a couple of file servers (accessed via SFTP) into Snowflake directly using the Python Snowflake connector. </p>

<p>I have considered using Data Factory (ADF) to pull the files into blob storage and then to create an Azure stage to move the data into Snowflake. However, I am curious to see if there is a way to do this without storing the data in blob storage? In other words, can I move the files directly into Snowflake without storing them elsewhere first? I don't see a way to do this in ADF without storing in Blob Storage and the Snowflake documentation indicates that stages can be created via Azure/AWS or from a local file system, but can this be done for a file server that needs accessed via SFTP? I've been working on this with the Python connector but it still seems like the data must be stored somewhere else first (either local hard drive or blob storage) before it can be staged into Snowflake.</p>
","<python><azure><azure-data-factory><snowflake-cloud-data-platform><pysftp>","2019-05-03 17:57:53","3419","4","2","55989777","<p>Assuming you want to use Snowflake's bulk-load mechanisms (so <code>COPY</code> entire files, not <code>INSERT</code> single rows), the file always needs to end up in the blob storage at some moment.</p>

<ul>
<li>you can move the files from SFTP to blob storage yourself and then use a <code>COPY</code> in Snowflake</li>
<li>you can download the files to local filesystem, and then use Snowflake's <code>PUT</code> command to copy the files to the staging location, and then use <code>COPY</code> again</li>
</ul>

<p>There is no way to create a stage that would point to SFTP.</p>

<p>Some more info <a href=""https://docs.snowflake.net/manuals/user-guide/intro-summary-loading.html#data-file-details"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Note: In theory you <em>could</em> fetch the data from SFTP e.g. in Python and then use <code>INSERT</code> to insert the rows through SQL. Then you do not use the stage, but it's more complex and certainly not worth the effort.</p>
"
"55974980","How can I move files from a local file server (SFTP) directly into Snowflake?","<p>I am looking to move files from a couple of file servers (accessed via SFTP) into Snowflake directly using the Python Snowflake connector. </p>

<p>I have considered using Data Factory (ADF) to pull the files into blob storage and then to create an Azure stage to move the data into Snowflake. However, I am curious to see if there is a way to do this without storing the data in blob storage? In other words, can I move the files directly into Snowflake without storing them elsewhere first? I don't see a way to do this in ADF without storing in Blob Storage and the Snowflake documentation indicates that stages can be created via Azure/AWS or from a local file system, but can this be done for a file server that needs accessed via SFTP? I've been working on this with the Python connector but it still seems like the data must be stored somewhere else first (either local hard drive or blob storage) before it can be staged into Snowflake.</p>
","<python><azure><azure-data-factory><snowflake-cloud-data-platform><pysftp>","2019-05-03 17:57:53","3419","4","2","65480428","<p>In the last couple of months, Azure Data Factory now has native integration with Snowflake via the Snowflake connector. So the current answer would be that there is a connector that you can use in the ADF portal.</p>
<p><em>ADF Now Supports Data Integration with Snowflake</em>:</p>
<ul>
<li><a href=""https://techcommunity.microsoft.com/t5/azure-data-factory/adf-now-supports-data-integration-with-snowflake/ba-p/1447370"" rel=""nofollow noreferrer"">https://techcommunity.microsoft.com/t5/azure-data-factory/adf-now-supports-data-integration-with-snowflake/ba-p/1447370</a></li>
</ul>
<p><em>The Snowflake Connector is supported for a Copy Activity with source/sink matrix table, mapping data flow, and lookup activity</em>:</p>
<ul>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-snowflake"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-snowflake</a></li>
</ul>
"
"55968342","Azure Data Factory dynamic Copy Activity with multiple tables in source and destination using C# .NET","<p>I have written this <code>copy activity</code> in <code>C#</code> <code>.NET</code> that moves data from <code>Sql Server</code> to <code>Azure</code>. It uses one table at present for both source and destination and here is my code </p>

<pre><code>       private DatasetResource CreateDataSetResourceSqlServer(string tableName)
        {
            DatasetResource sqlDatasetResource = new DatasetResource(
                new SqlServerTableDataset
                {
                    LinkedServiceName = new LinkedServiceReference(LinkedServiceReferenceNameSqlServer),
                    TableName = tableName
                }
             );
            return sqlDatasetResource;
        }
        private CopyActivity CreateCopyActivity()
        {
            return new CopyActivity
            {
                Name = CopyActivityNameSqlServerToAzure,
                Inputs = new List&lt;DatasetReference&gt;() { new DatasetReference() { ReferenceName = DataSetNameSqlServer } },
                Outputs = new List&lt;DatasetReference&gt;() { new DatasetReference() { ReferenceName = DataSetNameAzureSql } },
                Source = new SqlSource { SqlReaderQuery = ""SELECT * FROM Table1"" },
                Sink = new SqlSink { }
            };
        }
</code></pre>

<p>this works but is not a <code>scalable</code> solution though <code>DataSet</code> creation is dynamic but <code>CopyActivity</code> isn't. I have 50 tables in source and destination and I have an idea that if I list all my table names in file and iterate through them but then how do i make <code>CopyActivity</code> with dynamic <code>Source</code> that can copy data for multiple tables. Also If new tables are added in future in file I don't have to write <code>CopyActivity</code> per table.
could any one help</p>
","<c#><.net><azure-data-factory>","2019-05-03 10:53:08","855","0","2","55993051","<p>you could use for each activity and the expression language. Please reference the example.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity#iteration-expression-language"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity#iteration-expression-language</a></p>
"
"55968342","Azure Data Factory dynamic Copy Activity with multiple tables in source and destination using C# .NET","<p>I have written this <code>copy activity</code> in <code>C#</code> <code>.NET</code> that moves data from <code>Sql Server</code> to <code>Azure</code>. It uses one table at present for both source and destination and here is my code </p>

<pre><code>       private DatasetResource CreateDataSetResourceSqlServer(string tableName)
        {
            DatasetResource sqlDatasetResource = new DatasetResource(
                new SqlServerTableDataset
                {
                    LinkedServiceName = new LinkedServiceReference(LinkedServiceReferenceNameSqlServer),
                    TableName = tableName
                }
             );
            return sqlDatasetResource;
        }
        private CopyActivity CreateCopyActivity()
        {
            return new CopyActivity
            {
                Name = CopyActivityNameSqlServerToAzure,
                Inputs = new List&lt;DatasetReference&gt;() { new DatasetReference() { ReferenceName = DataSetNameSqlServer } },
                Outputs = new List&lt;DatasetReference&gt;() { new DatasetReference() { ReferenceName = DataSetNameAzureSql } },
                Source = new SqlSource { SqlReaderQuery = ""SELECT * FROM Table1"" },
                Sink = new SqlSink { }
            };
        }
</code></pre>

<p>this works but is not a <code>scalable</code> solution though <code>DataSet</code> creation is dynamic but <code>CopyActivity</code> isn't. I have 50 tables in source and destination and I have an idea that if I list all my table names in file and iterate through them but then how do i make <code>CopyActivity</code> with dynamic <code>Source</code> that can copy data for multiple tables. Also If new tables are added in future in file I don't have to write <code>CopyActivity</code> per table.
could any one help</p>
","<c#><.net><azure-data-factory>","2019-05-03 10:53:08","855","0","2","56012635","<p>Since you are using the C# you may or may not use foreach activity as you will be reading the tablename into a variable and loop it through . I could suggest you to create a parameter in ADF for the tablename and use that in Source and the Sink.</p>
"
"55958728","How to integrate Data Factory with private GitHub repo","<p>I'm trying to get a Data Factory code repository working on GitHub from within the Azure Portal. </p>

<p>When I enter my organization's name, I can only list the public repositories. How do I use a private repo?</p>
","<azure><github><azure-data-factory>","2019-05-02 19:05:47","433","1","2","55959291","<p>Rather than check the ""Enable GIT"" box on the creation blade, leave the box unchecked, click through, then follow the instructions <a href=""https://learn.microsoft.com/en-us/azure/data-factory/author-visually#author-with-github-integration"" rel=""nofollow noreferrer"">here</a> by clicking the ""Author &amp; Monitor"" hyperlink on the Overview blade to launch your data factory.</p>

<p><a href=""https://i.stack.imgur.com/zeuJX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zeuJX.png"" alt=""enter image description here""></a></p>
"
"55958728","How to integrate Data Factory with private GitHub repo","<p>I'm trying to get a Data Factory code repository working on GitHub from within the Azure Portal. </p>

<p>When I enter my organization's name, I can only list the public repositories. How do I use a private repo?</p>
","<azure><github><azure-data-factory>","2019-05-02 19:05:47","433","1","2","55959454","<p>This was an issue with Data Factory not having access. A co-worker later went through the same process and got a popup within Azure Portal that allowed them to give Data Factory access to the private repos. Not sure why they got the dialog popup but I didn't. </p>

<p>This fixed the problem.</p>
"
"55954376","Azure Data Factory, Mongo DB Connectionstring has special character","<p>I have to connect MongoDB in azure data factory, the password of the mongoDB has  '@' sign in it, which seems to be causing some issue. is there any way to escape that character?</p>

<p>the connection string is some thing look alike given below,</p>

<pre><code>mongodb://username:p@ssword@abc.company.com:1800/db_name
</code></pre>
","<mongodb><azure-data-factory>","2019-05-02 14:13:25","738","0","2","55964141","<p>Please reference this tutorial: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-mongodb"" rel=""nofollow noreferrer"">Copy data from MongoDB using Azure Data Factory</a>.</p>

<p>The MongoDB connection string format is look like this:</p>

<pre><code>mongodb://[username:password@]host[:port][/[database][?options]]
</code></pre>

<p><a href=""https://i.stack.imgur.com/5lHvr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5lHvr.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"55954376","Azure Data Factory, Mongo DB Connectionstring has special character","<p>I have to connect MongoDB in azure data factory, the password of the mongoDB has  '@' sign in it, which seems to be causing some issue. is there any way to escape that character?</p>

<p>the connection string is some thing look alike given below,</p>

<pre><code>mongodb://username:p@ssword@abc.company.com:1800/db_name
</code></pre>
","<mongodb><azure-data-factory>","2019-05-02 14:13:25","738","0","2","55967475","<p>Actually, </p>

<p>I got the answer,</p>

<p>I have to use '%40' instead of '@'</p>

<p>so the connectionstring will go like this</p>

<p>mongodb://username:p%40ssword@abc.company.com:1800/db_name</p>
"
"55948066","How to implement updates from mySQL operational DB to Azure SQL DB for reporting","<p>We have an <strong>operational mySQL DB running on AWS</strong> for a transactional system and an <strong>Azure SQL DB for reporting</strong> with PowerBI. Now I'd like to regularly (e.g. every night) do an update of certain tables from the mySQL DB to the Azure SQL DB.</p>

<p>I found <a href=""https://learn.microsoft.com/de-de/azure/data-factory/tutorial-incremental-copy-overview"" rel=""nofollow noreferrer"">this description on how to do incremental copies using Azure Data Factory</a>, however the alternatives don't seem feasible to me:</p>

<ol>
<li><p><a href=""https://learn.microsoft.com/de-de/azure/data-factory/tutorial-incremental-copy-overview"" rel=""nofollow noreferrer"">Delta data loading from database by using a watermark</a> requires adding watermark columns to the source DB, but I don't want to make changes to the operational DB because it is managed and regularly updated by the transactional system.</p></li>
<li><p><a href=""http://Delta%20data%20loading%20from%20database%20by%20using%20a%20watermark"" rel=""nofollow noreferrer"">Delta data loading from SQL DB by using the Change Tracking technology</a> seems to require an SQL Server DB as the source if I understand this correctly.</p></li>
</ol>

<p>The remaining two alternatives apply only to updates from files, not DBs, to my understanding.</p>

<p>Are there other feasible alternatives based on the described conditions? They don't necessarily need to involve Azure Data Factory, however the updates should run completely automated in the cloud.</p>

<p>Maybe a non-incremental update (i.e. full replacement of the target DB tables every time) would be an option too, but I'm afraid that this would lead to high costs on the Azure SQL Server side - please share any experience on that as well, if available.</p>
","<mysql><sql-server><azure-sql-database><data-warehouse><azure-data-factory>","2019-05-02 08:04:37","131","0","1","55952400","<p>You can use the watermark pattern as descibed, but have the watermark table in your destination database. You'll have to make sure that your lookup activity gets the watermark from the destination database instead of the source database.</p>

<p>I think the example puts the watermark table in the source because the destination is a blob-store. In your case, the destination is an Azure SQL db, so you can create  it in the destination db.</p>

<p>Whether a full load is a good option really depends on the requirements of your reports and amount of data that you'll need to move. Maybe you just need data from the last month? In that case you could create a where clause in your source query to filter only the last month and truncate the destionation table in the pre copy script of your copy activity.</p>
"
"55940090","Azure Data Factory- Copy specific files from multiple Parent folders from FTP Server","<p>I am trying to copy the .ZIP files from FTP Server to Azure DataLake.
I need to copy specific files from specific parent folders(Totally i have 6 parent folders in the FTP)and this pipeline needs to scheduled.
So how should i provide the parameters such that Pipeline should select only the specific files from the different folders? </p>

<p>I have used Metadata Activity and tried creating pipelines but not sure how to provide the pipeline to pick only specific files!</p>
","<azure><ftp><azure-data-factory><azure-data-lake>","2019-05-01 17:24:21","2105","0","2","55947262","<p>Azure Data Factory supports compress/decompress data during copy. When you specify compression property in an input dataset, the copy activity read the compressed data from the source and decompress it; and when you specify the property in an output dataset, the copy activity compress then write data to the sink.</p>

<p>For example:</p>

<p>Read .zip file from FTP server, decompress it to get the files inside, and land those files in Azure Data Lake Store. You define an input FTP dataset with the compression type property as ZipDeflate.</p>

<p>For more details, please reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#compression-support"" rel=""nofollow noreferrer"">Compression support</a>.</p>

<p>Here's the tutorial about <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-ftp"" rel=""nofollow noreferrer"">Copy data from FTP server by using Azure Data Factory</a>.</p>

<p>Other format dataset
To copy data from FTP in ORC/Avro/JSON/Binary format, the following properties are supported in this link: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-ftp#other-format-dataset"" rel=""nofollow noreferrer"">Other format dataset</a>.</p>

<p><a href=""https://i.stack.imgur.com/Eliia.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Eliia.png"" alt=""enter image description here""></a></p>

<p><strong>Tips:</strong></p>

<ol>
<li>To copy all files under a folder, specify <strong>folderPath</strong> only.</li>
<li>To copy a single file with a given name, specify <strong>folderPath</strong> with folder part and <strong>fileName</strong> with file name.</li>
<li>To copy a subset of files under a folder, specify <strong>folderPath</strong> with folder part and <strong>fileName</strong> with wildcard filter.</li>
</ol>

<p>Hope this helps.</p>
"
"55940090","Azure Data Factory- Copy specific files from multiple Parent folders from FTP Server","<p>I am trying to copy the .ZIP files from FTP Server to Azure DataLake.
I need to copy specific files from specific parent folders(Totally i have 6 parent folders in the FTP)and this pipeline needs to scheduled.
So how should i provide the parameters such that Pipeline should select only the specific files from the different folders? </p>

<p>I have used Metadata Activity and tried creating pipelines but not sure how to provide the pipeline to pick only specific files!</p>
","<azure><ftp><azure-data-factory><azure-data-lake>","2019-05-01 17:24:21","2105","0","2","55952468","<p>You'll need to use the filter activity to filter only the folders / files that you need. I think you'll need 2 loops:</p>

<p>Loop 1: get metadata of folders -> Filter required folders -> foreach pipeline with loop 2
Loop 2: get meta data of files of files -> Filter required files -> copy required files</p>
"
"55938410","How do I pass an object from an azure data factory lookup to a notebook so I can use the object/json within a python script","<p>I am using Azure Data Factory for the first time.</p>

<p>I have a lookup with a source dataset that is a table from azure table storage</p>

<p>I have a notebook which has a Base Parameter called 'input' with the value:</p>

<pre><code>@activity('Lookup1').output
</code></pre>

<p>The notebook path refers to a python script that does nothing yet.</p>

<p>When I try and debug the pipeline I get and error:</p>

<pre><code>{
    ""errorCode"": ""2105"",
    ""message"": ""The value type 'System.Collections.Generic.Dictionary`2[[System.String, mscorlib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=xxx],[System.Object, mscorlib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=xxx]]', in key 'input' is not expected type 'System.String'"",
    ""failureType"": ""UserError"",
    ""target"": ""Notebook1""
}
</code></pre>

<p>I would like to be able to use the json object in the python script. Do I have to change the Base Parameter some way to serialise the object?</p>
","<azure><azure-table-storage><azure-data-factory><azure-databricks>","2019-05-01 15:09:37","4970","3","1","55952551","<p>You can convert it to a string using @string:
<a href=""https://learn.microsoft.com/nl-nl/azure/data-factory/control-flow-expression-language-functions#conversion-functions"" rel=""nofollow noreferrer"">https://learn.microsoft.com/nl-nl/azure/data-factory/control-flow-expression-language-functions#conversion-functions</a></p>

<p>This should work: @string(activity('Lookup1').output)</p>

<p>This converts the object to a json string, you can probably parse that in your notebook.</p>
"
"55934693","Converting String to JSON in Data Factory","<p>Moving data from SQL Server to Cosmos in Copy Activity of Data Factory v2. One of the column in SQL server has JSON object (Although dataType is (<code>varchar(MAX)</code>) and I have mapped it to one column  in Cosmos collection. The issue is it adds it as <code>String</code> NOT json object. How can we setup it up in Copy Activity so that data for that one particular column gets added as Json Object not string </p>

<p>It gets added as follows:</p>

<pre><code>MyObject:""{SomeField: ""Value"" }""
</code></pre>

<p>However I want this to be:</p>

<pre><code>MyObject:{SomeField: ""Value"" } // Without quotes so that it appears as json object rather than string
</code></pre>
","<azure-data-factory>","2019-05-01 10:15:45","4510","2","1","65976257","<p>Use <strong>JSON</strong> conversion function available in Data Factory.</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#json"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#json</a></p>
<pre><code>MyObject:json(&quot;{SomeField: &quot;Value&quot; }&quot;)
</code></pre>
<p>It will result as</p>
<pre><code>MyObject:{SomeField: &quot;Value&quot; }
</code></pre>
"
"55929640","How to truncate Dynamics 365 entities with Data Factory (and copy to Azure data lake)?","<p>I am currently using a Data Factory to copy entities from Dynamics 365 in bulk to an Azure Data Lake. The entities are saved as CSV files in the Data Lake every 24 hours. </p>

<p>Instead of bulk copying, I would like to truncate entities to new data and append to the files that already exist in the data lake. </p>

<p>I think this is a common operation for SQL databases, but can this be done between Dynamics 365 and a Data Lake?</p>
","<azure><dynamics-crm><azure-data-lake><azure-data-factory>","2019-04-30 22:55:49","442","1","2","55933843","<p>You could add a filter to your queries to get those records that have been modified within the last 24 hours.</p>

<p>Additionally you can setup Dynamics to replicate its data to an external SQL database.</p>

<p><a href=""https://learn.microsoft.com/en-us/dynamics365/customer-engagement/admin/replicate-data-microsoft-azure-sql-database"" rel=""nofollow noreferrer"">Replicate data to Azure SQL Database</a></p>
"
"55929640","How to truncate Dynamics 365 entities with Data Factory (and copy to Azure data lake)?","<p>I am currently using a Data Factory to copy entities from Dynamics 365 in bulk to an Azure Data Lake. The entities are saved as CSV files in the Data Lake every 24 hours. </p>

<p>Instead of bulk copying, I would like to truncate entities to new data and append to the files that already exist in the data lake. </p>

<p>I think this is a common operation for SQL databases, but can this be done between Dynamics 365 and a Data Lake?</p>
","<azure><dynamics-crm><azure-data-lake><azure-data-factory>","2019-04-30 22:55:49","442","1","2","55933878","<p>Azure Data Lake storage Gen2 as a source type only support three Copy behaviors.</p>

<p><a href=""https://i.stack.imgur.com/xvqNV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xvqNV.png"" alt=""enter image description here""></a></p>

<p>I tried the three cope behaviors, they all could not help you append to the files that already exist in the data lake. If you choose the exist file, when the copy active completed, the exist file will be overwrite. </p>

<p>Fore more details, you can reference: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage#azure-data-lake-storage-gen2-as-a-source-type"" rel=""nofollow noreferrer"">Azure Data Lake storage Gen2 as a source type</a>.</p>

<p>It can not be done between between Dynamics 365 and a Data Lake with Azure Data Factory.</p>

<p>Thanks for James Wood provided a good solution for us. And Combine my answer and his , the problem will be solved.</p>

<p>Hope this helps.</p>
"
"55927360","Azure Data Factory V2 - Cannot combine wildcard filenames with dynamic content filepath. Is there a databricks (ADB) solution or another ADF solution?","<p>I currently have an upstream process that dumps a near-random amount of .zip files to an Azure Data Lake Storage, with each folder being named something like FILES/PROCESSING/2019/04/19.  </p>

<p>I created an Azure Data Factory V2 (ADF) Copy Data process to dynamically grab any files in ""todays"" filepath, but there's a support issue with combining dynamic content filepaths and wildcard file names, like seen below. </p>

<p>Is there any workaround for this in ADF? </p>

<p>Thanks! </p>

<p>Here's my Linked Service's dynamic filepath with wildcard file names:</p>

<pre><code>FILES/PROCESSING/@formatDateTime(utcnow(),'yyyy')/@formatDateTime(utcnow(),'mm')/@formatDateTime(utcnow(),'dd')
</code></pre>

<p>and the wildcard is:</p>

<pre><code>/*.zip
</code></pre>

<p>I expect the process to run, but instead get this error message:</p>

<pre><code>Activity CopyNewFiles failed: Failure happened on 'Source' side. ErrorCode=UserErrorFileNotFound,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Cannot find the &amp;apos;Azure Data Lake Store&amp;apos; file. . Service request id: c0266e28-d841-40b7-b177-e67d5e5388a1 Response details: {""RemoteException"":{""exception"":""FileNotFoundException""&amp;#44;""message"":""File/Folder does not exist: /FILES/PROCESSING/2019/04/30 [c0266e28-d841-40b7-b177-e67d5e5388a1][2019-04-30T12:08:55.0353825-07:00]""&amp;#44;""javaClassName"":""java.io.FileNotFoundException""}},Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Net.WebException,Message=The remote server returned an error: (404) Not Found.,Source=System,'

</code></pre>

<p>Only the file path DOES exist. If I run the manual process pointing directly at the file without the dynamic content, it runs just fine. </p>

<p>I've looked through ADF documentation trying to see if this is a known bug, and I'm not seeing anything that fits the bill.</p>
","<azure><azure-data-factory><azure-databricks>","2019-04-30 19:12:56","3291","3","1","56056599","<p>This should work in your path:</p>

<pre><code>@Concat('FILES/PROCESSING/',utcnow('yyyy/MM/dd'))
</code></pre>

<p>and <code>*.zip</code> in file bit.</p>

<p>Only one @ at the start.</p>

<p>You can embed functions in the formula like how you have, but you need to put curly braces around each pipeline bit like below and then that directly substitutes the values into the code without the concat:</p>

<pre><code>FILES/PROCESSING/@{formatDateTime(utcnow(),'yyyy')}/@{formatDateTime(utcnow(),'MM')}/@{formatDateTime(utcnow(),'dd')}/*.zip
</code></pre>

<p>also note capital MM for month, mm is minutes :) </p>
"
"55924319","Can Azure trigger an on-premises script to run on a schedule?","<p>We have a lot of jobs that have to run on our local computers, but we want to be able to run and manage those jobs from the cloud. These are not computer-admin type jobs. They are more about business-related tasks.</p>

<p>We have thought seriously about doing this a couple different ways:</p>

<ol>
<li><strong>Logic Apps/Microsoft Flow</strong> can create a file in a folder on a <a href=""https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-using-file-connector"" rel=""nofollow noreferrer"">local computer</a> using the <a href=""https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-gateway-install"" rel=""nofollow noreferrer"">On-Premises Data Gateway</a>. We could then use this file as a trigger for an on-premises script that is running constantly and watching that folder. However, this feels clunky since Logic Apps isn't triggering the script directly but only via a simple file creation event. This approach would also require us to use a single username/password combination in Logic Apps and remember to keep that password up to date. </li>
<li><strong>Azure Event Grid</strong> can <a href=""https://azure.microsoft.com/en-us/blog/more-reliable-event-driven-applications-in-azure-with-an-updated-event-grid/"" rel=""nofollow noreferrer"">now</a> forward events from Azure to a Hybrid Connection which transfers that event to a specific port on a local machine. Theoretically we could have a PowerShell script <a href=""https://stackoverflow.com/a/13129162/2363207"">monitoring</a> that port and process the incoming event. To me, this seems like the best way to trigger a script on an on premises machine from the cloud, however I'm not sure if this will actually work the way I expect.</li>
</ol>

<p>We have also looked at a few other ways we might be able to leverage Azure for this kind of thing:</p>

<ol>
<li><strong>Azure Automation</strong> <a href=""https://learn.microsoft.com/en-us/azure/automation/automation-hybrid-runbook-worker"" rel=""nofollow noreferrer"">Hybrid Runbooks</a> can trigger jobs on premises. However, this service seems to be useful mostly for administrative tasks, not daily processes. </li>
<li><strong>Azure DevOps</strong> can trigger <a href=""https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/utility/powershell?view=azure-devops"" rel=""nofollow noreferrer"">scripts</a> to run on an on-premises computer using <a href=""https://learn.microsoft.com/en-us/azure/devops/pipelines/agents/agents?view=azure-devops"" rel=""nofollow noreferrer"">Self-Hosted Agents</a>. However, I don't think Azure DevOps is designed to trigger a production process on a set schedule. It's only meant for software development build pipelines. </li>
<li><strong>Azure Data Factory</strong> <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/team-data-science-process/move-sql-azure-adf"" rel=""nofollow noreferrer"">Integration Runtime</a> allows you to move data from an on-premises SQL Server to the cloud. This seems like an ideal platform for moving data from on-premises to the cloud but I don't think Azure Data Factory can trigger an actual on-premise script from the cloud. I think it can only work with on-premise SQL Server. </li>
</ol>

<p>So I'm trying to decide amongst these approaches, or see if there is a better way.</p>

<p>I think I'm going to try the Azure Event Grid approach and get the Hybrid Connection Manager installed on some local machines and then keep some PowerShell scripts running 24/7 to monitor the specified port. Once an event from Azure Event Grid is routed to the Hybrid Connection Manager, it will then route it to the port that PowerShell is listening to and PowerShell can then trigger the job that needs to be run on the local computer. </p>

<p>I really like that approach since I can now connect my local events on my local computer to Azure and third-party events using Azure Event Grid. To me that opens up a world of possibilities for integration amongst disparate systems. But before I take this approach, I want to make sure it is the best one. </p>
","<azure><azure-data-factory><azure-logic-apps><power-automate><azure-eventgrid>","2019-04-30 15:32:57","2238","2","1","55929502","<p>It is unclear what type of data you are going to be sending from the cloud. You need to think about </p>

<p>1)  Do you want Pull or Push capability to trigger the tasks?
I would recommend Pull if you want to carry out maintenance on your local computers. In addition to the fact your local computers have a limited capacity and can’t scale based on a load with Push. </p>

<p>2)  Do you want to install additional Server software?
Azure Service Bus doesn’t need any Gateway to work on premise.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/service-bus-messaging/"" rel=""nofollow noreferrer"">Azure Service Bus</a> can be implemented with a Windows Service, IIS always on application, it still gives you the flexibility of integrating with 3rd party software as it is can work with Logic Apps, Flow and Azure Functions can be used with Azure Service Bus. </p>
"
"55912375","Pipeline Upload is failing on Sink side with cryptic error message","<p>Pipeline is supposed copying several tables from on-prem SQL Server to ADLS parquet files (drop and create during each processing). The intake relies on self-hosted integration runtime. All tests during the configuration are successful (i.e. table structure is retrieved successfully, connection tests are all green etc.). Pipeline however generates following execution error:</p>

<p><code>Activity Copy_DetailPricingLevelHierarchy failed: Failure happened on 'Sink' side. ErrorCode=UserErrorFailedFileOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Upload file failed at path Intake/MySource\PricingLevelHierarchy.,Source=Microsoft.DataTransfer.Common,''Type=System.InvalidOperationException,Message=Internal connection fatal error. Error state: 18,Source=System.Data,'</code></p>

<p>I'm failing to understand what is actually failing.</p>

<p><strong>Validations and attempts to fix the issue</strong>:</p>

<ul>
<li>Input and output connection validation from ADF (success)</li>
<li>Connection validation from IR VM (success)</li>
<li>Using Service Principal key to ADF directly to rule-out Azure Vault authentication issue (no change; connection validation remain successful)</li>
<li>Replacing the IR hosted on VM with auto set-up IR on my personal machine (upload successful)</li>
<li>Checking the outbound ports 80 and 443 are open on VM (success)</li>
</ul>

<p>Please advise, what other configurations might be causing sink failure.</p>
","<azure-data-factory><azure-data-lake>","2019-04-30 00:21:37","2347","0","1","56048772","<p>To be able to use the Parquet format with a Self-Hosted Integration Runtime, you need to have a Java Runtime Environment (JRE) installed. </p>

<p>From the Microsoft documentation on Azure Data Factory supported file formats section on Parquet format:</p>

<blockquote>
  <p>For copy empowered by Self-hosted Integration Runtime e.g. between
  on-premises and cloud data stores, if you are not copying Parquet
  files as-is, you need to install the 64-bit JRE 8 (Java Runtime
  Environment) or OpenJDK on your IR machine.</p>
</blockquote>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#parquet-format"" rel=""nofollow noreferrer"">Source link</a></p>
"
"55910114","is there a way to put web-activity result into sql-table (sink)","<p>Unable to get the output of a web activity into a sql-table using azure-data-factory.</p>
<p>This is what i have done and where im getting stuck (Step 3).</p>
<p>Steps:
1.Get a token from a API-call</p>
<ol start=""2"">
<li><p>Get the results from API Call using token from step 1
and tkae this results in a successfull query that provides me with  'JSON'</p>
</li>
<li><p>Take the result from previous activity 'JSON' and put in in a azure sql database table.</p>
</li>
</ol>
<p><a href=""https://i.stack.imgur.com/hZMi6.jpg"" rel=""nofollow noreferrer"">azure-datafactory - web activites</a></p>
","<rest><azure-data-factory>","2019-04-29 20:05:54","289","0","1","55911614","<p>Can you not get this done using a copy activity ? You should configure the source as Web activity and sink as SQL. I was playing with this <a href=""http://dummy.restapiexample.com/api/v1/employees"" rel=""nofollow noreferrer"">http://dummy.restapiexample.com/api/v1/employees</a> and we needed to introduce the structure . This is what I did and it work . </p>

<pre><code>                    ""source"": {
                        ""type"": ""RestSource"",
                        ""httpRequestTimeout"": ""00:01:40"",
                        ""requestInterval"": ""00.00:00:00.010"",
                        ""structure"": [
                            {
                                ""id"": ""id""
                            },
                            {
                                ""employee_salary"": ""employee_salary""
                            }
                        ]
                    },
                    ""sink"": {
                        ""type"": ""SqlServerSink"",
                        ""writeBatchSize"": 10000
                    },
                    ""enableStaging"": false
                },

You can read more . https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping
</code></pre>
"
"55903621","Azure Data Factory enable PolyBase","<p>My account does not have right permission to create new schema on ADW, but if I put authorization end of Create command of T-SQL it works fine. </p>

<p>PolyBase requires creating table into the ADW, if I unbox the Allow PolyBase from Sink entire workflow is working. </p>

<p>Is there anyway to add property ""authorization"" into the ADF Copy Data?</p>
","<azure><azure-data-factory>","2019-04-29 12:40:53","264","0","2","56054696","<p>Does pre-copy script help ? You can try to create table by this script.
<a href=""https://i.stack.imgur.com/gqkMe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gqkMe.png"" alt=""enter image description here""></a></p>
"
"55903621","Azure Data Factory enable PolyBase","<p>My account does not have right permission to create new schema on ADW, but if I put authorization end of Create command of T-SQL it works fine. </p>

<p>PolyBase requires creating table into the ADW, if I unbox the Allow PolyBase from Sink entire workflow is working. </p>

<p>Is there anyway to add property ""authorization"" into the ADF Copy Data?</p>
","<azure><azure-data-factory>","2019-04-29 12:40:53","264","0","2","63481110","<p>To use PolyBase, the user that loads data into SQL Data Warehouse must have &quot;CONTROL&quot; permission on the target database.
One way to achieve that is to add the user as a member of the db_owner role.</p>
"
"55877880","Azure Data Factory v2 ""Invalid value for property 'type'"" when running pipeline","<p>I have manually created an ADF v2 pipeline in the editor and it works correctly.</p>

<p>I exported this to an ARM template, with the only changes to the generated script I made are to replace the parameters('factoryName') with variables('factoryName') (as I want to use a derived name for the factory instance). I have also tried re-creating the pipeline using the generated json and parameters file, however the same problem still exists.</p>

<p>Whenever I attempt to run the pipeline I get the error:</p>

<p><em>Invalid value for property 'type'</em> with absolutely no details that I can use to work out what the problem is. The error code is just <em>'BadRequest'</em></p>

<p>Using the validator in the ADF portal shows that there are no validation issues with my configuration.</p>

<p>I have seen another <a href=""https://stackoverflow.com/questions/53754827/adf-v2-failure-when-using-bool-variable"">linked post</a> that suggested that the Boolean data type was incorrect, I have tried changing these to boolean as well as Bool, however the error persists.</p>

<p>The task is a simple copy task that will copy from one Azure SQL database into another Azure SQL database, using a stored procedure on the destination database to perform an upsert.</p>

<p>I have re-created the ARM template twice in the ADF editor, both times have generated the same issue.</p>
","<azure><azure-sql-database><azure-rm-template><azure-data-factory>","2019-04-27 06:25:21","2772","1","1","56699619","<p>I was migrating data from MariaDB to Azure. Even I faced this issue for copy activity pipeline, I exported the pipeline made the environment changes, imported the ARM template and when I tried to run get this same issue.
I tried to debug/search, but found nothing helpful.</p>

<p>The only conclusion I came up after debugging was if the data size is more then the conversion of the Data type is taking a lot of time and is not happening when we import the template.</p>

<p>But if we create the pipeline manually on the portal it runs fine without any error.</p>

<p>Do try to create the pipeline manually with similar db details of the failed pipeline, it will work fine.</p>

<p>And do let me know if you have found an alternative fix for this via the ARM template.</p>
"
"55871003","How to Rename Self-hosted Integration Run Time Node","<p>I have configured <code>Self-hosted Integration Runtime Node</code> on my local dev machine to connect to <code>data-factory</code> in <code>Azure</code>.</p>

<p><a href=""https://i.stack.imgur.com/PV694.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PV694.png"" alt=""enter image description here""></a></p>

<p>I now want it to change to different name for example ""DEV-01-Machine"" but cannot see any option in Microsoft documentation. does anyone know how to rename </p>
","<azure><azure-data-factory>","2019-04-26 15:46:34","1626","1","1","55899022","<p>It can't be updated so far,two evidences:</p>

<p>1.Azure Data Factory <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/integrationruntimenodes/update"" rel=""nofollow noreferrer"">IR Node Update REST API</a>.</p>

<p>It only supports below parameter.</p>

<p><a href=""https://i.stack.imgur.com/6JBCY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6JBCY.png"" alt=""enter image description here""></a></p>

<p>You need to provide specific <code>node_name</code> to use this <code>update</code> api.</p>

<pre><code>from azure.common.credentials import ServicePrincipalCredentials
from azure.mgmt.resource import ResourceManagementClient
from azure.mgmt.datafactory import DataFactoryManagementClient
from azure.mgmt.datafactory.models import *

# Azure subscription ID
subscription_id = '***'

# This program creates this resource group. If it's an existing resource group, comment out the code that creates the resource group
rg_name = '***'

# The data factory name. It must be globally unique.
df_name = '***'

# Specify your Active Directory client ID, client secret, and tenant ID
credentials = ServicePrincipalCredentials(client_id='***',
                                          secret='***',
                                          tenant='***')
resource_client = ResourceManagementClient(credentials, subscription_id)
adf_client = DataFactoryManagementClient(credentials, subscription_id)

resource_client.resource_groups.get(rg_name)

# Create a data factory
df_resource = Factory(location='eastus')
df = adf_client.factories.get(rg_name, df_name, df_resource)
print(df)

integration_runtime_name = ""***"";
node_name = ""***"";
irn = adf_client.integration_runtime_nodes.get(rg_name, df_name, integration_runtime_name, node_name)
print(irn)
</code></pre>

<p>2.ADF portal.</p>

<p>No any options to update the name.</p>

<p><a href=""https://i.stack.imgur.com/NEwQW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NEwQW.png"" alt=""enter image description here""></a></p>

<p>BTW,based on my observation,the node name is identified by the local PC name automatically.</p>

<p><a href=""https://i.stack.imgur.com/7qkTb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7qkTb.png"" alt=""enter image description here""></a></p>

<p>You could try to update PC names to manage the name rule of nodes.</p>
"
"55862493","Optimizing SSIS package for millions of rows with Order by / sort in SQL command and Merge Join","<p>Hi i am currently trying to optimize an SSIS package when i do an Upsert/delete which handels about 93+ million rows from a table in a remote MariaDB source.
The table also contains approximately 63 columns.</p>

<p>Currently i'm using Sort and Merge Join in my package but as i've read some guides its recommended to do the sorting part in the server and not with the sort functionality in SSIS DT, as it puts a load on the SSIS Server Memory.</p>

<p>And as i'm currently using this solution in Azure Data Factory running the package fails (most often Times out, even though i've increased the Time Out properties both in package side and in Azure Data factory).</p>

<p>What is the recommended way to tackle this? </p>

<p>If i've understood it right and as i mentioned it before i can skip the load on the SISS server by sorting DB-Server-Side. But as i'm new to the whole SQL and SSIS stuff i'm not quite sure how a sort like that would be in the SQL Command.</p>

<p>Also i've though about batching but even here i'm uncertain how that would work in SSIS.</p>

<p>What is recommended here?</p>

<p>My SSIS-Package looks like this right now:</p>

<p>I Followed this type of example: <a href=""https://www.mssqltips.com/sqlservertip/5082/synchronize-table-data-using-a-merge-join-in-ssis/"" rel=""nofollow noreferrer"">Synchronize Table Data Using a Merge Join in SSIS</a></p>

<p><a href=""https://i.stack.imgur.com/EYLHf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EYLHf.png"" alt=""enter image description here""></a></p>

<p><em>(FYI: The red error icons are there because i lost connection during the screenshot, this is a fully working solution otherwise.)</em></p>
","<mysql><sorting><merge><ssis><azure-data-factory>","2019-04-26 07:10:10","1898","3","1","55862833","<p>I have two recommendations:</p>

<h2>Server side sorting</h2>

<p>In OLE DB Source change the access mode to SQL Command. And use ORDER BY clause:</p>

<pre><code>Select * from table ORDER BY col1, col2
</code></pre>

<p>After that you should open OLE DB Source advanced editor <em>(Right click on the OLE DB source, show advanced editor)</em> go to the columns tab and change the output<code>IsSorted</code> property to <code>True</code> and set change the <code>SortKeyPosition</code> for the columns used in the ORDER BY clause.</p>

<ul>
<li><a href=""http://tomaslind.net/2014/05/20/sorted-data-flows-ssis/"" rel=""nofollow noreferrer"">SSIS sorted data flows</a> </li>
<li><a href=""https://social.msdn.microsoft.com/Forums/sqlserver/en-US/6b9131af-4e00-4aee-9706-a8260d76087c/where-is-the-issorted-property?forum=sqlintegrationservices"" rel=""nofollow noreferrer"">Where is the IsSorted property?</a> </li>
</ul>

<h2>Read data in chunks</h2>

<p>I don't have good knowledge in MariaDB SQL syntax but i will provide some example in SQLite and Oracle:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/54168322/reading-huge-volume-of-data-from-sqlite-to-sql-server-fails-at-pre-execute/54172707#54172707"">Reading Huge volume of data from Sqlite to SQL Server fails at pre-execute</a></li>
<li><a href=""https://stackoverflow.com/questions/54925644/getting-top-n-to-n-rows-from-db2/54932949#54932949"">Getting top n to n rows from db2</a></li>
<li><a href=""https://stackoverflow.com/questions/55303314/ssis-failing-to-save-packages-and-reboots-visual-studio/55307412#55307412"">SSIS failing to save packages and reboots Visual Studio</a></li>
</ul>

<hr>

<h1>Update 1 - Package problems</h1>

<p>There are some problems in the package:</p>

<ul>
<li>You are reading and writing from the same table</li>
<li>You are performing Update and delete tables on a large amount of data</li>
<li>You are using Merge Join</li>
</ul>

<p><strong>Some recommendations:</strong></p>

<ul>
<li>Try using a staging table instead of reading and writing from the same table since you are reading, writing, deleting, updating from the same destination table.</li>
<li>Use partitioning in the destination table which allows to delete and update records from a specific partition instead of the entire table</li>
</ul>
"
"55861397","Improving copy activities performance on Azure Data Factory V2","<p>I have a pipeline in ADF to copy 18 tables from source to destination. 
Noticed that the copy performance on ADF is taking too long, like 12 hours. But when we performed the same using SSIS it's took only like 2 hours. I noticed the copy activity is static when it is almost at completion. Does anyone faces the same issue?</p>

<p><a href=""https://i.stack.imgur.com/l0PQe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l0PQe.png"" alt=""Image here""></a></p>
","<azure><azure-data-factory>","2019-04-26 05:40:30","1747","0","1","55993273","<p>You may want to take a look at the performance tuning guide.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance</a></p>
"
"55858289","Is there a way to save Data Factory web activity output to a file or database table?","<p>I am hitting an issue in Data factory when connecting to oauth 2 rest api, where I don’t seem to be able to find a way to save the web activity output to a file or database table, so then other pipelines can do a lookup to get the current access and refresh tokens. As you can see in the attached scheenshot, I can connect to the api and get the access and refresh tokens back in ADF, now I want to store these somewhere, but its seems you can only save datasets in ADF and this is just the output of the web activity and can’t be made into a dataset. I can set them to variables, but this still doesn’t let me save them anywhere. Does anyone know a way of pushing these token values somewhere, either a file in data lake or database table?</p>

<p><a href=""https://i.stack.imgur.com/NrTKl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NrTKl.png"" alt=""enter image description here""></a></p>
","<azure><oauth-2.0><token><azure-data-factory>","2019-04-25 22:10:35","2928","0","2","55895713","<p>You don't need to store these token values. The output of ""GetAccess_Token"" web activity can be passed as input to the other activity like <strong>activity('Get Access_Token').output.access_token</strong>. Please refer my answer in another similar post of mine here (<a href=""https://stackoverflow.com/questions/55643855/refresh-power-bi-dataset-through-azure-data-factory/55821044#55821044"">Refresh Power BI dataset through Azure Data factory</a>).</p>

<p>Hope this helps!!</p>

<p>Please mark it as answer so that other people can get benefit from it.</p>
"
"55858289","Is there a way to save Data Factory web activity output to a file or database table?","<p>I am hitting an issue in Data factory when connecting to oauth 2 rest api, where I don’t seem to be able to find a way to save the web activity output to a file or database table, so then other pipelines can do a lookup to get the current access and refresh tokens. As you can see in the attached scheenshot, I can connect to the api and get the access and refresh tokens back in ADF, now I want to store these somewhere, but its seems you can only save datasets in ADF and this is just the output of the web activity and can’t be made into a dataset. I can set them to variables, but this still doesn’t let me save them anywhere. Does anyone know a way of pushing these token values somewhere, either a file in data lake or database table?</p>

<p><a href=""https://i.stack.imgur.com/NrTKl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NrTKl.png"" alt=""enter image description here""></a></p>
","<azure><oauth-2.0><token><azure-data-factory>","2019-04-25 22:10:35","2928","0","2","55923547","<p>Screenshot added of pipeline using a stored procedure to stored the auth tokens to a database table. </p>

<p><a href=""https://i.stack.imgur.com/75zoi.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/75zoi.png</a></p>
"
"55857324","Using ADF REST connector to read and transform FHIR data","<p>I am trying to use Azure Data Factory to read data from a FHIR server and transform the results into newline delimited JSON (ndjson) files in Azure Blob storage. Specifically, if you query a FHIR server, you might get something like:</p>

<pre><code>{
    ""resourceType"": ""Bundle"",
    ""id"": ""som-id"",
    ""type"": ""searchset"",
    ""link"": [
        {
            ""relation"": ""next"",
            ""url"": ""https://fhirserver/?ct=token""
        },
        {
            ""relation"": ""self"",
            ""url"": ""https://fhirserver/""
        }
    ],
    ""entry"": [
        {
            ""fullUrl"": ""https://fhirserver/Organization/1234"",
            ""resource"": {
                ""resourceType"": ""Organization"",
                ""id"": ""1234"",
                // More fields
        },
        {
            ""fullUrl"": ""https://fhirserver/Organization/456"",
            ""resource"": {
                ""resourceType"": ""Organization"",
                ""id"": ""456"",
                // More fields
        },

        // More resources
    ]
}
</code></pre>

<p>Basically a bundle of resources. I would like to transform that into a newline delimited (aka ndjson) file where each line is just the json for a resource:</p>

<pre><code>{""resourceType"": ""Organization"", ""id"": ""1234"", // More fields }
{""resourceType"": ""Organization"", ""id"": ""456"", // More fields }
// More lines with resources
</code></pre>

<p>I am able to get the REST connector set up and it can query the FHIR server (including pagination), but no matter what I try I cannot seem to generate the ouput I want. I set up an Azure Blob storage dataset:</p>

<pre><code>{
    ""name"": ""AzureBlob1"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureBlobStorage1"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""AzureBlob"",
        ""typeProperties"": {
            ""format"": {
                ""type"": ""JsonFormat"",
                ""filePattern"": ""setOfObjects""
            },
            ""fileName"": ""myout.json"",
            ""folderPath"": ""outfhirfromadf""
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p>And configure a copy activity:</p>

<pre><code>{
    ""name"": ""pipeline1"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""Copy Data1"",
                ""type"": ""Copy"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""RestSource"",
                        ""httpRequestTimeout"": ""00:01:40"",
                        ""requestInterval"": ""00.00:00:00.010""
                    },
                    ""sink"": {
                        ""type"": ""BlobSink""
                    },
                    ""enableStaging"": false,
                    ""translator"": {
                        ""type"": ""TabularTranslator"",
                        ""schemaMapping"": {
                            ""resource"": ""resource""
                        },
                        ""collectionReference"": ""$.entry""
                    }
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""FHIRSource"",
                        ""type"": ""DatasetReference""
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""AzureBlob1"",
                        ""type"": ""DatasetReference""
                    }
                ]
            }
        ]
    },
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>

<p>But at the end (in spite of configuring the schema mapping), it the end result in the blob is always just the original bundle returned from the server. If I configure the output blob as being a comma delimited text, I can extract fields and create a flattened tabular view, but that is not really what I want. </p>

<p>Any suggestions would be much appreciated.</p>
","<azure><azure-data-factory><hl7-fhir>","2019-04-25 20:41:13","1105","3","3","55858510","<p>As briefly discussed in the comment, the <code>Copy Activity</code> does not provide much functionality aside from mapping data. As stated in the documentation, the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">Copy activity</a> does the following operations:</p>

<blockquote>
  <ol>
  <li>Reads data from a source data store.</li>
  <li>Performs serialization/deserialization, compression/decompression, column mapping, etc. It does these operations based on the
  configurations of the input dataset, output dataset, and Copy
  Activity.</li>
  <li>Writes data to the sink/destination data store.</li>
  </ol>
</blockquote>

<p>It does not look like that the <code>Copy Activity</code> does anything else aside from efficiently copying stuff around. </p>

<p>What I found out to be working was to use Databrick. </p>

<p>Here are the steps:</p>

<ol>
<li>Add a Databricks account to your subscription;</li>
<li>Go to the Databricks page by clicking the authoring button;</li>
<li>Create a notebook;</li>
<li>Write the script (Scala, Python or <a href=""https://dotnet.microsoft.com/apps/data/spark"" rel=""nofollow noreferrer"">.Net was recently announced</a>).</li>
</ol>

<p>The script would the following:</p>

<ol>
<li>Read the data from the Blob storage;</li>
<li>Filter out &amp; transform the data as needed; </li>
<li>Write the data back to a Blob storage;</li>
</ol>

<p>You can test your script from there and, once ready, you can go back to your pipeline and create a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-databricks-notebook"" rel=""nofollow noreferrer"">Notebook activity</a> that will point to your notebook containing the script.</p>

<p>I struggled coding in Scala but it was worth it :)</p>
"
"55857324","Using ADF REST connector to read and transform FHIR data","<p>I am trying to use Azure Data Factory to read data from a FHIR server and transform the results into newline delimited JSON (ndjson) files in Azure Blob storage. Specifically, if you query a FHIR server, you might get something like:</p>

<pre><code>{
    ""resourceType"": ""Bundle"",
    ""id"": ""som-id"",
    ""type"": ""searchset"",
    ""link"": [
        {
            ""relation"": ""next"",
            ""url"": ""https://fhirserver/?ct=token""
        },
        {
            ""relation"": ""self"",
            ""url"": ""https://fhirserver/""
        }
    ],
    ""entry"": [
        {
            ""fullUrl"": ""https://fhirserver/Organization/1234"",
            ""resource"": {
                ""resourceType"": ""Organization"",
                ""id"": ""1234"",
                // More fields
        },
        {
            ""fullUrl"": ""https://fhirserver/Organization/456"",
            ""resource"": {
                ""resourceType"": ""Organization"",
                ""id"": ""456"",
                // More fields
        },

        // More resources
    ]
}
</code></pre>

<p>Basically a bundle of resources. I would like to transform that into a newline delimited (aka ndjson) file where each line is just the json for a resource:</p>

<pre><code>{""resourceType"": ""Organization"", ""id"": ""1234"", // More fields }
{""resourceType"": ""Organization"", ""id"": ""456"", // More fields }
// More lines with resources
</code></pre>

<p>I am able to get the REST connector set up and it can query the FHIR server (including pagination), but no matter what I try I cannot seem to generate the ouput I want. I set up an Azure Blob storage dataset:</p>

<pre><code>{
    ""name"": ""AzureBlob1"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureBlobStorage1"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""AzureBlob"",
        ""typeProperties"": {
            ""format"": {
                ""type"": ""JsonFormat"",
                ""filePattern"": ""setOfObjects""
            },
            ""fileName"": ""myout.json"",
            ""folderPath"": ""outfhirfromadf""
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p>And configure a copy activity:</p>

<pre><code>{
    ""name"": ""pipeline1"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""Copy Data1"",
                ""type"": ""Copy"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""RestSource"",
                        ""httpRequestTimeout"": ""00:01:40"",
                        ""requestInterval"": ""00.00:00:00.010""
                    },
                    ""sink"": {
                        ""type"": ""BlobSink""
                    },
                    ""enableStaging"": false,
                    ""translator"": {
                        ""type"": ""TabularTranslator"",
                        ""schemaMapping"": {
                            ""resource"": ""resource""
                        },
                        ""collectionReference"": ""$.entry""
                    }
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""FHIRSource"",
                        ""type"": ""DatasetReference""
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""AzureBlob1"",
                        ""type"": ""DatasetReference""
                    }
                ]
            }
        ]
    },
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>

<p>But at the end (in spite of configuring the schema mapping), it the end result in the blob is always just the original bundle returned from the server. If I configure the output blob as being a comma delimited text, I can extract fields and create a flattened tabular view, but that is not really what I want. </p>

<p>Any suggestions would be much appreciated.</p>
","<azure><azure-data-factory><hl7-fhir>","2019-04-25 20:41:13","1105","3","3","55860661","<p>So I sort of found a solution. If I do the original step of converting where the bundles are simply dumped in the JSON file and then do a nother conversion from the JSON file to what I pretend to be a text file into another blob, I can get the njson file created. </p>

<p>Basically, define another blob dataset:</p>

<pre><code>{
    ""name"": ""AzureBlob2"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureBlobStorage1"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""AzureBlob"",
        ""structure"": [
            {
                ""name"": ""Prop_0"",
                ""type"": ""String""
            }
        ],
        ""typeProperties"": {
            ""format"": {
                ""type"": ""TextFormat"",
                ""columnDelimiter"": "","",
                ""rowDelimiter"": """",
                ""quoteChar"": """",
                ""nullValue"": ""\\N"",
                ""encodingName"": null,
                ""treatEmptyAsNull"": true,
                ""skipLineCount"": 0,
                ""firstRowAsHeader"": false
            },
            ""fileName"": ""myout.json"",
            ""folderPath"": ""adfjsonout2""
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p>Note that this one <code>TextFormat</code> and also note that the <code>quoteChar</code> is blank. If I then add another Copy Activity:</p>

<pre><code>{
    ""name"": ""pipeline1"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""Copy Data1"",
                ""type"": ""Copy"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""RestSource"",
                        ""httpRequestTimeout"": ""00:01:40"",
                        ""requestInterval"": ""00.00:00:00.010""
                    },
                    ""sink"": {
                        ""type"": ""BlobSink""
                    },
                    ""enableStaging"": false,
                    ""translator"": {
                        ""type"": ""TabularTranslator"",
                        ""schemaMapping"": {
                            ""['resource']"": ""resource""
                        },
                        ""collectionReference"": ""$.entry""
                    }
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""FHIRSource"",
                        ""type"": ""DatasetReference""
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""AzureBlob1"",
                        ""type"": ""DatasetReference""
                    }
                ]
            },
            {
                ""name"": ""Copy Data2"",
                ""type"": ""Copy"",
                ""dependsOn"": [
                    {
                        ""activity"": ""Copy Data1"",
                        ""dependencyConditions"": [
                            ""Succeeded""
                        ]
                    }
                ],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""BlobSource"",
                        ""recursive"": true
                    },
                    ""sink"": {
                        ""type"": ""BlobSink""
                    },
                    ""enableStaging"": false,
                    ""translator"": {
                        ""type"": ""TabularTranslator"",
                        ""columnMappings"": {
                            ""resource"": ""Prop_0""
                        }
                    }
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""AzureBlob1"",
                        ""type"": ""DatasetReference""
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""AzureBlob2"",
                        ""type"": ""DatasetReference""
                    }
                ]
            }
        ]
    },
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>

<p>Then it all works out. It is not ideal in that I now have two copies of the data in blobs, but one can easily be deleted, I suppose. </p>

<p>I would still love to hear about it if somebody has a one-step solution. </p>
"
"55857324","Using ADF REST connector to read and transform FHIR data","<p>I am trying to use Azure Data Factory to read data from a FHIR server and transform the results into newline delimited JSON (ndjson) files in Azure Blob storage. Specifically, if you query a FHIR server, you might get something like:</p>

<pre><code>{
    ""resourceType"": ""Bundle"",
    ""id"": ""som-id"",
    ""type"": ""searchset"",
    ""link"": [
        {
            ""relation"": ""next"",
            ""url"": ""https://fhirserver/?ct=token""
        },
        {
            ""relation"": ""self"",
            ""url"": ""https://fhirserver/""
        }
    ],
    ""entry"": [
        {
            ""fullUrl"": ""https://fhirserver/Organization/1234"",
            ""resource"": {
                ""resourceType"": ""Organization"",
                ""id"": ""1234"",
                // More fields
        },
        {
            ""fullUrl"": ""https://fhirserver/Organization/456"",
            ""resource"": {
                ""resourceType"": ""Organization"",
                ""id"": ""456"",
                // More fields
        },

        // More resources
    ]
}
</code></pre>

<p>Basically a bundle of resources. I would like to transform that into a newline delimited (aka ndjson) file where each line is just the json for a resource:</p>

<pre><code>{""resourceType"": ""Organization"", ""id"": ""1234"", // More fields }
{""resourceType"": ""Organization"", ""id"": ""456"", // More fields }
// More lines with resources
</code></pre>

<p>I am able to get the REST connector set up and it can query the FHIR server (including pagination), but no matter what I try I cannot seem to generate the ouput I want. I set up an Azure Blob storage dataset:</p>

<pre><code>{
    ""name"": ""AzureBlob1"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureBlobStorage1"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""AzureBlob"",
        ""typeProperties"": {
            ""format"": {
                ""type"": ""JsonFormat"",
                ""filePattern"": ""setOfObjects""
            },
            ""fileName"": ""myout.json"",
            ""folderPath"": ""outfhirfromadf""
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p>And configure a copy activity:</p>

<pre><code>{
    ""name"": ""pipeline1"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""Copy Data1"",
                ""type"": ""Copy"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""RestSource"",
                        ""httpRequestTimeout"": ""00:01:40"",
                        ""requestInterval"": ""00.00:00:00.010""
                    },
                    ""sink"": {
                        ""type"": ""BlobSink""
                    },
                    ""enableStaging"": false,
                    ""translator"": {
                        ""type"": ""TabularTranslator"",
                        ""schemaMapping"": {
                            ""resource"": ""resource""
                        },
                        ""collectionReference"": ""$.entry""
                    }
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""FHIRSource"",
                        ""type"": ""DatasetReference""
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""AzureBlob1"",
                        ""type"": ""DatasetReference""
                    }
                ]
            }
        ]
    },
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>

<p>But at the end (in spite of configuring the schema mapping), it the end result in the blob is always just the original bundle returned from the server. If I configure the output blob as being a comma delimited text, I can extract fields and create a flattened tabular view, but that is not really what I want. </p>

<p>Any suggestions would be much appreciated.</p>
","<azure><azure-data-factory><hl7-fhir>","2019-04-25 20:41:13","1105","3","3","69225279","<p>For anyone finding this post in the future you can just can use the $export api call to accomplish this. Note that you have to have a storage account linked to your Fhir server.</p>
<p><a href=""https://build.fhir.org/ig/HL7/bulk-data/export.html#endpoint---system-level-export"" rel=""nofollow noreferrer"">https://build.fhir.org/ig/HL7/bulk-data/export.html#endpoint---system-level-export</a></p>
"
"55844842","How to do Exception handling in Azure Data Factory V2","<p>Can I apply exception handling in Azure Data factory if some pipeline or activity fails and how can I implement exception handling by some TRY/CATCH methodologies ?</p>
","<azure-data-factory>","2019-04-25 08:21:54","6955","2","3","55846877","<p>I am developing <code>data factory</code> using <code>C#</code> <code>.NET</code> code and this is how I am implementing  and it works for me, everything is wrapped in one try catch. in the catch block you could send trace to <code>log file</code>, <code>database</code> or any other <code>service</code></p>

<p><a href=""https://i.stack.imgur.com/3Mzyj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3Mzyj.png"" alt=""enter image description here""></a></p>
"
"55844842","How to do Exception handling in Azure Data Factory V2","<p>Can I apply exception handling in Azure Data factory if some pipeline or activity fails and how can I implement exception handling by some TRY/CATCH methodologies ?</p>
","<azure-data-factory>","2019-04-25 08:21:54","6955","2","3","56047291","<p>Assuming you go the codeless route (not C# custom activity), then generally you use the success, error and completion (success or failure) outputs of an activity.  Route the success output to activities that run normally.  Route the error output to activities you want to execute on failure.</p>

<p>This is not a try/catch/finally paradigm but you can do most of what you want.</p>

<p><strong>Some things to try</strong></p>

<p>If you route two outputs to an input, this activity is only executed as a logical ""and"" of its inputs which can be useful.</p>

<p>i.e. if you have activity1, activity2 and activity3 and activity4.  </p>

<p>Route <strong>completion</strong> output of activity1 to activity2 and <strong>success</strong> output of activity2 to activity3.<br>
Then route <strong>failure</strong> output of activity1 to input of activity3.<br>
Then route <strong>success</strong> output of activity1 to activity4.
This sets up the following flow</p>

<ul>
<li>If activity1 succeeds, activity2 and 4 are executed is executed in parallel.</li>
<li>if activity1 fails, activity2 and 3 are executed in sequence.</li>
</ul>

<p>Activity 2 can be used for cleanup because it is executed for both conditions (which can help to implement a ""finally"" but not quite the same).</p>

<p>Activity3 is similar but not the same as a ""catch"" - it only runs when activity1 fails and activity2 succeeds.</p>

<p>Activity4 is executed only on success of activity1 (like the rest of a try block).</p>

<p>There are lots of logical combinations you can use to get different effects.</p>

<p>Caution over timeouts and cancellations, these are not handled as ""errors"" and do not have their own output routing.</p>
"
"55844842","How to do Exception handling in Azure Data Factory V2","<p>Can I apply exception handling in Azure Data factory if some pipeline or activity fails and how can I implement exception handling by some TRY/CATCH methodologies ?</p>
","<azure-data-factory>","2019-04-25 08:21:54","6955","2","3","62173647","<p>This is addressed by ""<a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities#activity-dependency"" rel=""nofollow noreferrer"">Activity dependency</a>"".</p>

<blockquote>
  <p>Activity Dependency defines how subsequent activities depend on
  previous activities, determining the condition of whether to continue
  executing the next task. An activity can depend on one or multiple
  previous activities with different dependency conditions.</p>
  
  <p>The different dependency conditions are: Succeeded, Failed, Skipped,
  Completed.</p>
</blockquote>
"
"55835934","Failed to encrypt sub-resource payload and the error is: Failed to encrypted linked service credentials on self-hosted IR","<p>I am developing an <a href=""https://en.wikipedia.org/wiki/Extract,_transform,_load"" rel=""nofollow noreferrer"">ETL</a> solution using .NET and Azure Data Factory to move data from an on-premises SQL&nbsp;Server to Azure. Self-hosted IR is set up correctly and is running on Azure Portal, but when I run the code I get this exception:</p>

<blockquote>
  <p>Failed to encrypt sub-resource payload and error is: Failed to encrypted linked service credentials on self-hosted IR reason is: InternalServerError, error message is: Internal Server Error..</p>
</blockquote>

<p><code>ConnectionString</code> for the on-premises SQL&nbsp;Server server is in plain text and not encrypted.</p>

<p><a href=""https://i.stack.imgur.com/jiVku.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jiVku.png"" alt=""Enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/RBnvC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RBnvC.png"" alt=""Enter image description here""></a></p>

<p>How can I fix this problem?</p>
","<azure><etl><azure-data-factory>","2019-04-24 17:55:36","3278","1","1","55846644","<p>I found a solution myself, and it works. I hope it helps someone who also encounters the same problem. </p>

<p>You need to set the value for <code>EncryptedCredential</code> and don't need to set the <code>Password</code> property.</p>

<p><a href=""https://i.stack.imgur.com/jwdcv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jwdcv.png"" alt=""Enter image description here""></a></p>

<p>And set <code>Connection String</code> like this:</p>

<p><a href=""https://i.stack.imgur.com/fdWIm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fdWIm.png"" alt=""Enter image description here""></a></p>
"
"55829314","Cannot implicitly convert type 'string' to 'Microsoft.Azure.Management.DataFactory.Models.SecretBase'","<p>I am developing an ETL solution using .NET and Azure Data Factory's <code>SqlServerLinkedService</code>  as part of data movement from on premise SQL Server to Azure.</p>

<p>I cannot set the password property as it's of type <code>SecretBase</code> which does not have any property/constructor:</p>

<p><a href=""https://i.stack.imgur.com/PWeyn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PWeyn.png"" alt=""enter image description here""></a></p>

<p>does anyone know how to set this property?</p>

<p>Below is my code:</p>

<p><a href=""https://i.stack.imgur.com/MZ1IT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MZ1IT.png"" alt=""enter image description here""></a></p>
","<c#><.net><azure><azure-data-factory>","2019-04-24 11:45:14","902","2","2","55829589","<p>Oddly, this only seems to be explained in <a href=""https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.models.SecretBase?view=azure-python"" rel=""nofollow noreferrer"">the Azure Python docs</a></p>
<blockquote>
<p>SecretBase class</p>
<p>The base definition of a secret type.</p>
<p>You probably want to use the sub-classes and not this class directly. Known sub-classes are: SecureString, AzureKeyVaultSecretReference</p>
</blockquote>
<p>So you can use the <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models.securestring?view=azure-dotnet"" rel=""nofollow noreferrer"">Azure SecureString implementation</a> to set the value, as it inherits from <code>SecretBase</code>.</p>
"
"55829314","Cannot implicitly convert type 'string' to 'Microsoft.Azure.Management.DataFactory.Models.SecretBase'","<p>I am developing an ETL solution using .NET and Azure Data Factory's <code>SqlServerLinkedService</code>  as part of data movement from on premise SQL Server to Azure.</p>

<p>I cannot set the password property as it's of type <code>SecretBase</code> which does not have any property/constructor:</p>

<p><a href=""https://i.stack.imgur.com/PWeyn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PWeyn.png"" alt=""enter image description here""></a></p>

<p>does anyone know how to set this property?</p>

<p>Below is my code:</p>

<p><a href=""https://i.stack.imgur.com/MZ1IT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MZ1IT.png"" alt=""enter image description here""></a></p>
","<c#><.net><azure><azure-data-factory>","2019-04-24 11:45:14","902","2","2","55829598","<p>I think i have found solution using <code>SecureString</code> which is derived from <code>SecretBase</code> and has <code>Property</code> to set string value</p>

<p><a href=""https://i.stack.imgur.com/IovzQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IovzQ.png"" alt=""enter image description here""></a></p>
"
"55828405","Handle >5000 rows in Lookup @ Azure Data Factory","<p>I have a Copy Activity which copies a Table from MySQL to Azure Table Storage.
This works great.
But when I do a Lookup on the Azure Table I get an error. (Too much Data)</p>

<p>This is as designed referred to the documentation: 
The Lookup activity has a maximum of 5,000 rows, and a maximum size of 2 MB.</p>

<p>Also there is a Workaround mentioned:
Design a two-level pipeline where the outer pipeline iterates over an inner pipeline, which retrieves data that doesn't exceed the maximum rows or size.</p>

<p>How can I do this? Is there a way to define a offset (e.g. only read 1000 rows)</p>

<p><a href=""https://i.stack.imgur.com/389dY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/389dY.png"" alt=""azure data factory""></a></p>
","<azure><azure-data-factory>","2019-04-24 10:54:00","7371","4","1","55856185","<p>Do you really need 5000 iterations of your foreach? What kind of process are you doing in the foreach, isn't there a more efficient way of doing that?</p>

<p>Otherwise, maybe the following solution might be possible.</p>

<p>Create a new pipeline with 2 integer variables: iterations and count with 0 as defaults.</p>

<p>First determine the needed number of iterations. Do a lookup to determine the total number of datasets. In your query divide this by 5000, add one and round it upwards. Set the value of the iterations variable to this value using the set variable activity.</p>

<p>Next, add a while loop with expression something like @less(variables('count'),variables('iterations')). in this while loop call your current pipeline and pass the count variable as a parameter. After the execute pipeline activity, set the count variable to +1.</p>

<p>In your current pipeline you can use the limit/offset clause in combination with the passed parameter in a MySQL query to get the first 0-5000 results for your first iteration, 5000-10000 for your second iteration etc..</p>

<p>If you really need to iterate on the table storage, the only solution I see is that you'll have to create pagination on the resultset yourself, you could use a logic app for this purpose and call it by using a webhook.</p>
"
"55825688","Tool recommendation for data transform","<p>I have large amounts of raw fault data in Power BI. </p>

<pre><code>code    time                    status  
x123    2019-04-22T23:57:00     ok  
x123    2019-04-23T01:00:00     faulty  
x123    2019-04-23T02:00:00     ok  
x123    2019-04-23T23:00:00     faulty  
x123    2019-04-24T01:00:00     ok  
</code></pre>

<p>I need to transform this to show how long an item has been in a faulty state on a given day. So on the 23rd, the item was in a faulty state between 1 and 2a.m and then again between 11pm until past midnight.</p>

<pre><code>code    day         % of day faulty  
x123    23/04/2019  8.30%           (2 hours)  
</code></pre>

<p>Can I do this easily in Power BI or should I use another tool such as Azure Data Factory?</p>
","<powerbi><dax><azure-data-factory>","2019-04-24 08:34:08","109","0","1","55828438","<p>Add the following Calculated Columns to your table:</p>

<p><code>Report Date = Table1[time].[Date]</code></p>

<pre><code>Fault Duration = 
VAR CurrentTime = Table1[time]
VAR CurrentCode = Table1[code]
VAR PreviousTime = 
    CALCULATE ( 
        MAX ( Table1[time] ),
        FILTER ( 
            Table1,
            Table1[time] &lt; CurrentTime &amp;&amp; 
            Table1[code] = CurrentCode
        )
    )
VAR NextTime = 
    CALCULATE ( 
        MIN ( Table1[time] ),
        FILTER ( 
            Table1,
            Table1[time] &gt; CurrentTime &amp;&amp; 
            Table1[code] = CurrentCode
        )
    )
VAR FaultyFrom = 
    IF(
        Table1[status] = ""faulty"",
        Table1[time],
        IF (
            DAY(PreviousTime) = DAY(Table1[time]),
            BLANK(),
            Table1[time].[Date]
        )
    )
VAR FaultyTo = 
    IF ( 
        Table1[status] = ""ok"",
        Table1[time],
        IF (
            DAY(NextTime) = DAY(Table1[time]),
            NextTime,
            Table1[time].[Date] + 1
        )
    )
RETURN
    IF(
        ISBLANK ( PreviousTime ) || ISBLANK ( NextTime ) || ISBLANK ( FaultyFrom ),
        BLANK(),
        FaultyTo - FaultyFrom
    )
</code></pre>

<p>Now create measures:</p>

<p><code>Faulty Hours = SUM ( Table1[Fault Duration] )</code></p>

<pre><code>Faulty % Day = 
    IF ( 
        HASONEVALUE ( Table1[Report Date] ),
        DIVIDE ( 
            [Faulty Hours],
            DISTINCTCOUNT ( Table1[code] ),
            BLANK()
        ),
        BLANK()
    )
</code></pre>

<p>Output:</p>

<p><a href=""https://i.stack.imgur.com/nXeG2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nXeG2.png"" alt=""enter image description here""></a></p>

<p>See <a href=""https://pwrbi.com/so_55825688/"" rel=""nofollow noreferrer"">https://pwrbi.com/so_55825688/</a> for a worked example PBIX file</p>
"
"55821661","Adding the Dynamic content in ADF V2","<p>I need to add Dynamic content to the file name which should be last day of the month. For Example if i run the job in April it should generate the file name as last day of the March.</p>

<p>Expected result be in the following format.</p>

<p>20190331.csv.zip</p>

<p>Thanks in Advance.</p>
","<azure><azure-data-factory>","2019-04-24 02:45:39","189","0","1","55848409","<p>I think something like this in your blob filename</p>

<pre><code>@concat(formatDateTime(adddays(utcnow(),mul(int(formatDateTime(utcnow(),'dd')),-1)),'yyyyMMdd'),'.csv.zip') 
</code></pre>

<p>get the day bit from <code>utcnow</code>, turn to an <code>int</code>, <code>mul</code> tiply it by -1, <code>adddays</code> the negative number to <code>utcnow</code> and then <code>formatdatetime</code> as yyyyMMdd then <code>concat</code> with whatever you want on the end</p>
"
"55817620","How can I query data in an Azure Analysis Service from a ASP.NET Core application?","<p>I have an cloud application that dumps all its data into an <code>Azure Data Lake</code>.</p>

<p>Using <code>Azure Data Factory</code>, I have built a pipeline that extracts and transforms the data from the lake and saves it in local <code>.csv</code> files.</p>

<p>These <code>.csv</code> files are accessible in an <code>Azure SQL Data Warehouse</code> as external files that are CTAS as proper SQL tables.</p>

<p>I have created an <code>Azure Analysis Service</code> instance that is hooked up to the warehouse, provides additionnal information out of the existing data and caches it.</p>

<p><strong>Question</strong></p>

<p>I want to make this data available from a Web API. I have created a <code>ASP.NET Core application</code> and I would like this application to connect the service to query the data. </p>

<p>Knowing that the client libraries (<a href=""https://www.nuget.org/packages/Microsoft.AnalysisServices.AdomdClient.retail.amd64/15.15.0"" rel=""nofollow noreferrer"">ADOMD</a>) are all .NET Framework libraries that can't loaded in a .NET Core app, how should I do this?</p>
","<azure><asp.net-core><azure-data-factory><azure-data-lake><azure-analysis-services>","2019-04-23 18:48:50","1252","0","3","55823449","<p>There are no official .NET Core-compatible ADOMD.NET yet:</p>

<p><a href=""https://github.com/dotnet/corefx/issues/20809"" rel=""nofollow noreferrer"">https://github.com/dotnet/corefx/issues/20809</a></p>

<p><a href=""https://feedback.azure.com/forums/908035-sql-server/suggestions/35508349-adomd-core"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/908035-sql-server/suggestions/35508349-adomd-core</a></p>

<p>You can try this unofficial port of ""Microsoft.AnalysisServices.AdomdClient"".</p>

<p>If your hosting environment is windows, you can create simple .NET Framework-based microservice that handles requests to SSAS from your main ASP.NET Core app</p>
"
"55817620","How can I query data in an Azure Analysis Service from a ASP.NET Core application?","<p>I have an cloud application that dumps all its data into an <code>Azure Data Lake</code>.</p>

<p>Using <code>Azure Data Factory</code>, I have built a pipeline that extracts and transforms the data from the lake and saves it in local <code>.csv</code> files.</p>

<p>These <code>.csv</code> files are accessible in an <code>Azure SQL Data Warehouse</code> as external files that are CTAS as proper SQL tables.</p>

<p>I have created an <code>Azure Analysis Service</code> instance that is hooked up to the warehouse, provides additionnal information out of the existing data and caches it.</p>

<p><strong>Question</strong></p>

<p>I want to make this data available from a Web API. I have created a <code>ASP.NET Core application</code> and I would like this application to connect the service to query the data. </p>

<p>Knowing that the client libraries (<a href=""https://www.nuget.org/packages/Microsoft.AnalysisServices.AdomdClient.retail.amd64/15.15.0"" rel=""nofollow noreferrer"">ADOMD</a>) are all .NET Framework libraries that can't loaded in a .NET Core app, how should I do this?</p>
","<azure><asp.net-core><azure-data-factory><azure-data-lake><azure-analysis-services>","2019-04-23 18:48:50","1252","0","3","61812938","<p>I tried the ""Unofficial.Microsoft.AnalysisServices.AdomdClient"" for getting data out of the cube via REST Api. It worked really well and the response time is great. I am just not confident that more advanced stuf (partition management. . .) will work with this package, since it does not seem to be developed regularly.</p>
"
"55817620","How can I query data in an Azure Analysis Service from a ASP.NET Core application?","<p>I have an cloud application that dumps all its data into an <code>Azure Data Lake</code>.</p>

<p>Using <code>Azure Data Factory</code>, I have built a pipeline that extracts and transforms the data from the lake and saves it in local <code>.csv</code> files.</p>

<p>These <code>.csv</code> files are accessible in an <code>Azure SQL Data Warehouse</code> as external files that are CTAS as proper SQL tables.</p>

<p>I have created an <code>Azure Analysis Service</code> instance that is hooked up to the warehouse, provides additionnal information out of the existing data and caches it.</p>

<p><strong>Question</strong></p>

<p>I want to make this data available from a Web API. I have created a <code>ASP.NET Core application</code> and I would like this application to connect the service to query the data. </p>

<p>Knowing that the client libraries (<a href=""https://www.nuget.org/packages/Microsoft.AnalysisServices.AdomdClient.retail.amd64/15.15.0"" rel=""nofollow noreferrer"">ADOMD</a>) are all .NET Framework libraries that can't loaded in a .NET Core app, how should I do this?</p>
","<azure><asp.net-core><azure-data-factory><azure-data-lake><azure-analysis-services>","2019-04-23 18:48:50","1252","0","3","62472538","<p>There is now an official version of ADOMD.NET for .NET Core available. See <a href=""https://stackoverflow.com/a/62470657/5070440"">another answer here for details</a>.</p>
"
"55815300","Check file encoding thanks to Azure Data Factory activity","<p>I'd like to be able to check the encoding of a input file in the flow of my pipeline. Any idea about to do that thanks to one of the activity provided by Azure Data Factory?</p>

<p>Thanks for the tips</p>
","<azure-data-factory>","2019-04-23 16:08:09","204","0","1","55817328","<p>It's actually not supported by any of the activities ""on the box"" at this time, but you are able to do that using other services with connectors available on ADF like Azure Function for example. But you will need to develop the algorithm to detect the encoding and an azure function service to do that ... (Of course other services like Azure Batch, Notebooks ... could be used) </p>

<p><em>Saying that, it could be really usefull to add this information into the Get Metadata Activity (just posted the idea to <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/37452187-add-encoding-into-the-get-a-file-s-metadata-activi"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/270578-data-factory/suggestions/37452187-add-encoding-into-the-get-a-file-s-metadata-activi</a>)</em></p>
"
"55814826","Is it possible to read excel file from azure storage file system using SSIS-IR running in azure data factory?","<p>I have excel file in azure storage file system. Also running SSIS-IR in adfv2. Is it possible to read excel file from file system using SSIS-IR ? Trying to read excel file and insert into sql db.</p>
","<ssis><azure-data-factory>","2019-04-23 15:39:26","1266","1","2","55825762","<p>Please consider using Copy Activity in ADF,please refer to the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats"" rel=""nofollow noreferrer"">Supported data stores  table</a> of it.</p>

<p>Configure source as <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-file-storage"" rel=""nofollow noreferrer"">Azure Storage File</a> and sink as <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server"" rel=""nofollow noreferrer"">SQL DB</a>.</p>
"
"55814826","Is it possible to read excel file from azure storage file system using SSIS-IR running in azure data factory?","<p>I have excel file in azure storage file system. Also running SSIS-IR in adfv2. Is it possible to read excel file from file system using SSIS-IR ? Trying to read excel file and insert into sql db.</p>
","<ssis><azure-data-factory>","2019-04-23 15:39:26","1266","1","2","55867807","<p>It's possible to load excel files using SSIS-IR, you need to install the required drivers on the SSIS runtime following the process here => <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-configure-azure-ssis-ir-custom-setup"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-configure-azure-ssis-ir-custom-setup</a> </p>

<p>Keep in mind SSIS-IR is expensive, you are also able to load the file with a custom azure function.</p>
"
"55799919","Horizontal/Vertical scaling of self hosted integration runtime","<p>We're looking for <em>automated</em> way to horizontally, vertically scale the pull of self hosted integration runtime virtual machines used in ADF.</p>

<p>Reading Microsoft docs  does not provide answer.</p>
","<azure><ado><azure-data-factory>","2019-04-22 19:20:33","956","3","2","55817833","<p>Well, I don't have the experience, so I can only give you a theoretical answer, but maybe it's helpfull for you. </p>

<p>AFAIK, neither way is configurable out-of-the-box. For scale-out you'll have to deploy an additional IR machine yourself. So probably you'll want to create an image that you can provision from docker or kubernetes and has the IR and pre-requirements installed. The IR installation provides an PowerShell script that can be used to create an automated connection.</p>

<p>For scale-up/down, you'll have to run some script that scales your vm. In an IaaS solution (f.e.) Azure VM, that should be doable with an API call to change your VM. </p>

<p>For both cases you'll have to have some kind of montitor in place that monitors the IR loads and makes changess as needed. I think the measures provided in the Data Factory should do. Maybe you can use Log Analyics to monitor the loads.</p>

<p>I'm curious about your use case for this.</p>
"
"55799919","Horizontal/Vertical scaling of self hosted integration runtime","<p>We're looking for <em>automated</em> way to horizontally, vertically scale the pull of self hosted integration runtime virtual machines used in ADF.</p>

<p>Reading Microsoft docs  does not provide answer.</p>
","<azure><ado><azure-data-factory>","2019-04-22 19:20:33","956","3","2","69724308","<p>My solution is just for scaling out/in since the VM must be restarted if you are scaling up/down, which causes downtime and job failures etc.</p>
<p>At a high level this solution requires just 3 simple things:</p>
<ol>
<li>Azure Metric Alert that fires when Scale-Out should occur (VM Start)</li>
<li>Azure Metric Alert that fires when Scale-In should occur (VM Deallocation)</li>
<li>Logic App that is triggered by Azure Alert and actually executes the Start/Stop of the VM, along with any other automation associated with this (eg posting to a Teams channel when Scale in/out occurs)</li>
</ol>
<p>Here are more of the details surrounding how we setup the conditions for the alerts, but the main thing to keep in mind is (IR CPU %, IR queue length, Number of Nodes, and possibly IR Memory)</p>
<p><strong>Scale-Out</strong></p>
<p><a href=""https://i.stack.imgur.com/Bn4NH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Bn4NH.png"" alt=""Scale-Out Conditions"" /></a></p>
<p><strong>Scale-In</strong></p>
<p><a href=""https://i.stack.imgur.com/L6XKz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L6XKz.png"" alt=""Scale-In Conditions"" /></a></p>
<p><strong>Actions for Alerts</strong></p>
<p>As you can see below we have the alert triggering 1 Logic App, using the payload that is passed to the Logic App, you can determine if the Logic App should be starting the VM, or stopping the VM. (As well as any other additional actions)
<a href=""https://i.stack.imgur.com/qJTLL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qJTLL.png"" alt=""Action Group"" /></a></p>
<p><strong>Logic App</strong></p>
<p><a href=""https://i.stack.imgur.com/3svXV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3svXV.png"" alt=""Logic App Development"" /></a></p>
<p>There is a small chance that due to timing (and depending on how many ADF's the IR is shared to), that pipeline activities could be sent to Node 2 at the same time a deallocation command is sent to the VM for Node 2. I have not seen this as of yet, but adjusting the alert conditions based on your need could help avoid this. Feel free to play around with the conditions of the alerts, granularity, thresholds, etc. This is not a one size fits all solution.</p>
"
"55795878","Two Queries in one copy activity Data Factory v2","<p>I'm new using Azure Data Factory and i would like to know if there is a mode to run two queries in one activity copy, for example:</p>

<p>I have a Data Set Oracle for my origin and other for my stage, both use Oracle DB.</p>

<p>i need copy all registries in a specific query, but before to run it, i need to alter session</p>

<pre><code>ALTER session SET nls_language = 'AMERICAN';
</code></pre>

<p>and immediately run my query</p>

<pre><code>Select * from ... where  .... 
</code></pre>

<p>This is posible? or it's not the way? thks for your answers</p>

<p><strong>Important:</strong> We don't have the posibility to create objects in DB Origin</p>
","<oracle><azure><oracle-adf><azure-data-factory>","2019-04-22 14:20:05","764","0","1","55798601","<p>Of course you can, in the copy activity, click on the tab Source, and then select Query. Input your 2 step query into the textbox and you are good to go.</p>

<p><a href=""https://i.stack.imgur.com/nfSuP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nfSuP.png"" alt=""Query in copy activity""></a></p>

<p>Hope this helped!</p>
"
"55791899","Need to insert csv source data to Azure SQL Database","<p>I have a source data in csv. I created a sql table to insert the csv data. My sql table has primary key column &amp; foreign key column in it. I cannot skip these 2 columns while mapping in Data factory. How to overcome this &amp; insert data ?</p>
","<azure><azure-data-factory>","2019-04-22 09:08:48","97","0","1","55844355","<p>Please refer to the rules in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#explicit-column-mapping"" rel=""nofollow noreferrer"">Schema mapping in copy activity</a>.</p>

<ul>
<li>Source data store query result does not have a column name that is
specified in the input dataset ""structure"" section. </li>
<li><p>Sink data store    (if with pre-defined schema) does not have a
column name that is    specified in the output dataset ""structure""
section. Either fewer    columns or more columns in the ""structure""
of sink dataset than    specified in the mapping. </p></li>
<li><p>Duplicate mapping.</p></li>
</ul>

<p>So,if your csv file does not cover all the columns in the sql database,copy activity can't work.</p>

<p>You could consider creating temporary table in sql database to match your csv file ,then use stored procedure to fill the exact table. Please refer to the detailed steps in this case to implement your requirement:<a href=""https://stackoverflow.com/questions/51352725/azure-data-factory-mapping-2-columns-in-one-column/51354266#51354266,please"">Azure Data Factory mapping 2 columns in one column</a></p>
"
"55771900","Read records from large xlsx/csv files and post it to azure service bus topic","<p>We receive large feed files(excel/csv) of ~5gb size every night, need to read and have to post one record after another from the file to azure service bus topic. Is it possible using Azure Data Factory, if yes, how? or any other better azure based solution available? please suggest.</p>
","<azure><azureservicebus><azure-data-factory>","2019-04-20 08:56:41","583","0","1","55788568","<p>Data Factory doesn't support Azure Service Bus topic. So it's impossible to using Azure Data Factory
<a href=""https://i.stack.imgur.com/85oiO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/85oiO.png"" alt=""enter image description here""></a></p>

<p>For more details, please see : <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats"" rel=""nofollow noreferrer"">Supported data stores and formats</a>.</p>

<p>I think <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-overview"" rel=""nofollow noreferrer"">Azure Function</a> may can help you achieve it.
You can reference :</p>

<ol>
<li><p><a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob"" rel=""nofollow noreferrer"">Azure Blob storage bindings for Azure Functions</a>.
This article explains how to work with Azure Blob storage bindings in Azure Functions. Azure Functions supports trigger, input, and output bindings for blobs. The article includes a section for each binding: Blob trigger, Blob input binding,Blob output binding.</p></li>
<li><p><a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-service-bus"" rel=""nofollow noreferrer"">Azure Service Bus bindings for Azure Functions</a>. This article explains how to work with Azure Service Bus bindings in Azure Functions. Azure Functions supports trigger and output bindings for Service Bus queues and topics</p></li>
</ol>

<p>Here is a blog about <a href=""https://stackoverflow.com/questions/52608947/copy-data-from-azure-service-bus-topic-to-a-storage-account-inside-azure"">Copy data from Azure Service Bus Topic to a Storage Account inside Azure</a>.</p>

<p>We don't know how much time it will take. Azure Functions has two kinds of pricing plans:Consumption plan and App Service plan.</p>

<p>Azure Functions in a Consumption plan are limited to 10 minutes for a single execution. In the Premium plan, the run duration defaults to 30 minutes to prevent runaway executions. However, you can modify the host.json configuration to make this unbounded for Premium plan apps.</p>

<p>For more details, please reference: <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-premium-plan"" rel=""nofollow noreferrer"">Azure Functions Premium plan</a>.</p>

<p>Maybe Azure Function is not fit for this long process, but can help you execute this long process. </p>

<p>Hope this helps.</p>
"
"55771619","Called pipeline does not fail when called from another pipeline","<p>I have a 'Main' pipeline which calls two child pipelines, main pipeline goes to success everytime when executed. Both child1 &amp; child2 pipelines are calling copy data activities, for child1 pipeline, to introduce a failure, I changed the datatype of one of the columns of the table where the data is copied, ran child1 individually and child1 fails. when I ran Main pipeline, which calls child1 and child2. child2 is called upon success of child1. In main pipeline child1 goes to success and calls child2, both got to success.</p>
","<azure><azure-pipelines><azure-data-factory>","2019-04-20 08:16:18","263","0","2","55820122","<p>At this point the execute pipeline works on ""fire and forget"" principle , but if you set the ""wait on completion"" flag , you will achieve what you intend .</p>

<p><a href=""https://i.stack.imgur.com/xlwob.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xlwob.png"" alt=""enter image description here""></a></p>
"
"55771619","Called pipeline does not fail when called from another pipeline","<p>I have a 'Main' pipeline which calls two child pipelines, main pipeline goes to success everytime when executed. Both child1 &amp; child2 pipelines are calling copy data activities, for child1 pipeline, to introduce a failure, I changed the datatype of one of the columns of the table where the data is copied, ran child1 individually and child1 fails. when I ran Main pipeline, which calls child1 and child2. child2 is called upon success of child1. In main pipeline child1 goes to success and calls child2, both got to success.</p>
","<azure><azure-pipelines><azure-data-factory>","2019-04-20 08:16:18","263","0","2","55895772","<p>You can add an <strong>IF condition</strong> activity in your child pipeline and validate your success/failure there. Refer screenshots below:</p>

<p><a href=""https://i.stack.imgur.com/6Xp7U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6Xp7U.png"" alt=""enter image description here""></a></p>

<p>And within that under <em>Activities</em> tab, use a ""Web activity"" on True or False (depending on your condition) to ThrowErrorOnFailure. </p>

<p><a href=""https://i.stack.imgur.com/npQdb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/npQdb.png"" alt=""enter image description here""></a></p>

<p>Use any <strong>invalid URL</strong> here:</p>

<p><a href=""https://i.stack.imgur.com/wGtho.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wGtho.png"" alt=""enter image description here""></a></p>

<p>Please mark it as Answer if this helped you so that other people can get benefit from it as well.</p>
"
"55764437","how to connect datafactory activities with python","<p>i am following this tutorial
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-python"" rel=""nofollow noreferrer"">quickstart tutorial</a></p>

<p>is great since teach how to use python to generate a data-factory resource, a pipeline and one activity, but if i need more than one, how do i connect them?</p>

<p>i assume that the azure portal and activities is just a  nice UI to generate a json file that have all the information of what activity is connected to another one?  </p>

<p>how i can connect the output of an activity to the input of another one?</p>
","<python><azure><azure-data-factory>","2019-04-19 15:49:26","87","0","1","55770882","<p>The <strong>CopyActivity</strong> used in the tutorial you are following provides the <strong>depends_on</strong> parameter, which can be used to connect activities within the pipeline, for example:</p>

<blockquote>
  <p><em>If</em> activity <strong>A</strong> <em>depends on</em> activity <strong>B</strong> <em>then</em> activity <strong>B</strong> will be executed prior to activity <strong>A</strong> in the pipeline.</p>
  
  <p>Moreover, activity <strong>A</strong> will only be executed <em>if</em> activity <strong>B</strong>
  succeeds.</p>
</blockquote>

<p>Find more details at <a href=""https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.models.copyactivity?view=azure-python"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.models.copyactivity?view=azure-python</a>.</p>
"
"55764375","how can i make a azure datafactory resource folder with python?","<p>Is it possible to create a folder with pipelines in data-factory using python?
was reading this tutorial:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-python"" rel=""nofollow noreferrer"">data-factory tutorial</a></p>

<p>it creates a pipeline and just one copy activity
Can I create a folder so I can put the pipeline inside?</p>
","<python><azure><azure-data-factory>","2019-04-19 15:44:52","156","0","1","55821465","<p>You can specify a folder before creating pipeline. Please see the sample code in your <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-python#full-script"" rel=""nofollow noreferrer"">link</a>.</p>

<pre><code>p_name = 'copyPipeline'
params_for_pipeline = {}
p_obj = PipelineResource(activities=[copy_activity], parameters=params_for_pipeline)
p = adf_client.pipelines.create_or_update(rg_name, df_name, p_name, p_obj)
</code></pre>

<p>Deep into the <code>PipelineResource</code> 's parameters, you could find <code>PipelineFolder</code>.</p>

<p><a href=""https://i.stack.imgur.com/hysHv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hysHv.png"" alt=""enter image description here""></a></p>

<p>So just add a param into above code.</p>

<pre><code>folder = PipelineFolder(name=""f1"");
p_obj = PipelineResource(activities=[copy_activity], parameters=params_for_pipeline, folder=folder)
</code></pre>
"
"55750517","Storing output of filter activity","<p>I'm creating a pipeline which filters the input from the source SQL table based on few columns (using Filter activity) and then copies it to an Azure SQL table. But the copy activity expects a SQL query input instead of an array. Clearly, I'm unable to save the output of a filter activity to a table or a file.</p>
<p>The error I'm facing is:</p>
<blockquote>
<p>Failed to convert the value in 'sqlReaderQuery' property to 'System.String' type.Please make sure the payload structure and value are correct.</p>
<p>Source=Microsoft.DataTransfer.DataContracts,''Type=System.InvalidCastException,Message=Object must implement IConvertible.</p>
</blockquote>
","<azure><azure-data-factory>","2019-04-18 16:47:30","637","0","1","55753608","<p>The filter activity will not have the output as a table ( which you will need for the copy activity ). Since in your case the source is SQL , you can use the SQL query to filter the data in the query itself when you are configuring the source . </p>
"
"55749786","Run a python script on Azure Batch","<p>Iam trying to execute a python script on azure batch which is a linux dsvm so that the script can install python packages and then execute the python script.</p>

<p>Below is the code i used:</p>

<pre><code>try:
   from pip import main as pipmain
except ImportError:
   from pip._internal import main as pipmain

try:
    import pandas as pd
except:

   pipmain(['install', 'pandas',""])

import pandas
</code></pre>

<p>When i run the python script on azure Batch command line , the pool task errors out at the last statement(import pandas) eventhough i can see in the stdout log file that the pandas, numpy etc packages are installed.</p>

<p>It seems that the packages are installed at some other location and while trying to import it is trying to import from some other location. It gives the error ImportError: No module named pandas in the stderr.txt file on the azure batch pool tasks.</p>

<p>The reason why iam trying to install python packages and importing it the same script is because the azure batch command line doesnt allow me to write 2 commands , something like</p>

<pre><code>pip install pandas
python test.py
</code></pre>

<p>where it first install the packages and then invokes script where it just does the import of pandas library.</p>

<p>I have also used the command in the <code>pip install pandas</code> and <code>pip install --install-option=""--prefix=$AZ_BATCH_TASK_WORKING_DIR"" pandas</code> at the start task of the batch pool. AZ_BATCH_TASK_WORKING_DIR as per my understanding is working directory to which the task and script has access when the task batch runs</p>

<p>Is there a way i run the python script successfully on Azure Batch. At the momemt iam only running one command: import pandas</p>
","<python><azure-data-factory><azure-batch>","2019-04-18 15:57:49","4862","3","2","55751158","<p>You need to provide a inline shell script to run your multiple commands and take advantage of shell expansion. Please see this <a href=""https://learn.microsoft.com/azure/batch/batch-api-basics#task"" rel=""nofollow noreferrer"">doc</a>. You'll want to run your two commands like:</p>

<pre><code>/bin/bash -c ""pip install pandas &amp;&amp; python test.py""
</code></pre>

<p>Additionally, tasks are run under context-specific directories (i.e., a start task runs in the start task directory versus a normal task will run in a different directory, although <code>$AZ_BATCH_TASK_WORKING_DIR</code> is named the same) and <a href=""https://learn.microsoft.com/azure/batch/batch-user-accounts"" rel=""nofollow noreferrer"">user identities</a> can also modify the user context for which a task is run.</p>
"
"55749786","Run a python script on Azure Batch","<p>Iam trying to execute a python script on azure batch which is a linux dsvm so that the script can install python packages and then execute the python script.</p>

<p>Below is the code i used:</p>

<pre><code>try:
   from pip import main as pipmain
except ImportError:
   from pip._internal import main as pipmain

try:
    import pandas as pd
except:

   pipmain(['install', 'pandas',""])

import pandas
</code></pre>

<p>When i run the python script on azure Batch command line , the pool task errors out at the last statement(import pandas) eventhough i can see in the stdout log file that the pandas, numpy etc packages are installed.</p>

<p>It seems that the packages are installed at some other location and while trying to import it is trying to import from some other location. It gives the error ImportError: No module named pandas in the stderr.txt file on the azure batch pool tasks.</p>

<p>The reason why iam trying to install python packages and importing it the same script is because the azure batch command line doesnt allow me to write 2 commands , something like</p>

<pre><code>pip install pandas
python test.py
</code></pre>

<p>where it first install the packages and then invokes script where it just does the import of pandas library.</p>

<p>I have also used the command in the <code>pip install pandas</code> and <code>pip install --install-option=""--prefix=$AZ_BATCH_TASK_WORKING_DIR"" pandas</code> at the start task of the batch pool. AZ_BATCH_TASK_WORKING_DIR as per my understanding is working directory to which the task and script has access when the task batch runs</p>

<p>Is there a way i run the python script successfully on Azure Batch. At the momemt iam only running one command: import pandas</p>
","<python><azure-data-factory><azure-batch>","2019-04-18 15:57:49","4862","3","2","64335658","<p>Although fpark's answer put me on the right track, I was not successful. However, I run into this <a href=""https://medium.com/analytics-vidhya/automating-python-based-data-transformations-with-azure-297218478fb7"" rel=""nofollow noreferrer"">post</a> which worked for me.</p>
<p>The start task command would be (for python3):</p>
<pre><code>/bin/bash -c &quot;sudo apt-get -y update &amp;&amp; sudo dpkg --configure -a &amp;&amp; sudo apt-get install -y python3-pip &amp;&amp; pip3 install --upgrade pip &amp;&amp; sudo pip3 install pandas&quot;
</code></pre>
"
"55746659","Need to connect ADF and BigQuery through IR","<p>I want to connect Azure data factory and Google Big query through IR, what are the values required and how can I access the data from Big Query</p>
","<google-bigquery><azure-data-factory>","2019-04-18 13:04:44","262","0","1","55841277","<p>ADF supports google big query connector: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-google-bigquery"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-google-bigquery</a>. An linkedsevice payload example is given below:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>{
    ""name"": ""GoogleBigQueryLinkedService"",
    ""properties"": {
        ""type"": ""GoogleBigQuery"",
        ""typeProperties"": {
            ""project"" : ""&lt;project ID&gt;"",
            ""additionalProjects"" : ""&lt;additional project IDs&gt;"",
            ""requestGoogleDriveScope"" : true,
            ""authenticationType"" : ""UserAuthentication"",
            ""clientId"": ""&lt;id of the application used to generate the refresh token&gt;"",
            ""clientSecret"": {
                ""type"": ""SecureString"",
                ""value"":""&lt;secret of the application used to generate the refresh token&gt;""
            },
            ""refreshToken"": {
                ""type"": ""SecureString"",
                ""value"": ""&lt;refresh token&gt;""
            }
        }
    }
}</code></pre>
</div>
</div>
</p>
"
"55746590","How to get dynamically all json files table data in a table(sql server data warehouse) using Azure Data Factory(Load from ADF to DWH)","<p>I have to get all json files data into a table from azure data factory to sql server data warehouse.i'm able to load the data into a table with static values (by giving column names in the dataset) but generating in dynamic i'm unable to get that using azure data factory.Can some help on this solution to get dynamically in azure data factory?
Many thanks in Advance.</p>

<p>json file data as follows:</p>

<p>{
""TABLE"": ""TEST_M1"",
""DATA"": [{
""DFG"": ""123456"",
""ADF"": ""SFSDF""
}, {
""DFG"": ""ABADHDD"",
""ADF"": ""GHB""
}
}</p>

<p>same as follows for different TABLE names(TEST_M2.....)</p>
","<azure-data-factory><azure-data-lake><azure-synapse>","2019-04-18 12:59:09","749","0","1","55841231","<p>You could invoke a stored procedure script in sql serer sink when doing copy. 
Stored procedure script defines the logic about how to generate dynamic value based on source json data. See an example: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#invoking-stored-procedure-for-sql-sink"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#invoking-stored-procedure-for-sql-sink</a></p>
"
"55732416","Microsoft Integration Runtime Configuration Manager Failed to connect to the database. Error message: Bad JSON escape sequence:","<p>I have successfully configured <code>Self-Hosted Integration Runtime</code> in <code>Azure Data Factory</code> on <code>Azure Portal</code> as well as <code>Self-Hosted IR Node</code> on my local machine and <code>linked service</code> connects successfully to my local <code>sql server</code> from <code>Azure Portal</code>
<a href=""https://i.stack.imgur.com/h7Nfs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/h7Nfs.png"" alt=""enter image description here""></a></p>

<p>but when I test connection  within IR connection manager on my local machine to connect to local <code>sql server</code>  it gives me this error </p>

<p><a href=""https://i.stack.imgur.com/dpGUk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dpGUk.png"" alt=""enter image description here""></a></p>

<p>can anyone help</p>
","<azure><azure-data-factory>","2019-04-17 16:42:18","1846","1","2","55739808","<p>You could refer to my working steps as below.</p>

<p>1.Follow this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">guide</a> Create Self IR on the portal.</p>

<p><a href=""https://i.stack.imgur.com/tgRmC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tgRmC.png"" alt=""enter image description here""></a></p>

<p>2.Get the Auth Key to start the ADF IR local management.</p>

<p><a href=""https://i.stack.imgur.com/VyAii.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VyAii.png"" alt=""enter image description here""></a></p>

<p>3.Fill the sql server name and database name in the Diagnostics Tag.</p>

<p><a href=""https://i.stack.imgur.com/lFQ10.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lFQ10.png"" alt=""enter image description here""></a></p>
"
"55732416","Microsoft Integration Runtime Configuration Manager Failed to connect to the database. Error message: Bad JSON escape sequence:","<p>I have successfully configured <code>Self-Hosted Integration Runtime</code> in <code>Azure Data Factory</code> on <code>Azure Portal</code> as well as <code>Self-Hosted IR Node</code> on my local machine and <code>linked service</code> connects successfully to my local <code>sql server</code> from <code>Azure Portal</code>
<a href=""https://i.stack.imgur.com/h7Nfs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/h7Nfs.png"" alt=""enter image description here""></a></p>

<p>but when I test connection  within IR connection manager on my local machine to connect to local <code>sql server</code>  it gives me this error </p>

<p><a href=""https://i.stack.imgur.com/dpGUk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dpGUk.png"" alt=""enter image description here""></a></p>

<p>can anyone help</p>
","<azure><azure-data-factory>","2019-04-17 16:42:18","1846","1","2","55744197","<p>The problem is Server name with single back slash. I copied Server name from ssms which uses </p>

<pre><code>R\SQLEXPRESS
</code></pre>

<p>but then i checked the connection string that Self-Hosted on Azure is using to connect to <code>on premise SQL Server</code> it uses double back slashes </p>

<pre><code>R\\SQLEXPRESS 
</code></pre>

<p>and after changing to double back slash it works 
<a href=""https://i.stack.imgur.com/6VCC8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6VCC8.png"" alt=""enter image description here""></a></p>
"
"55728157","Create Azure Data Factory Integration Runtime if it does not exist - Azure PowerShell","<p>I have already created <code>Azure Data Factory</code> <code>Integration Runtime</code> in Azure Portal and now want to create via PowerShell script </p>

<pre><code>   $IR = Get-AzDataFactoryV2IntegrationRuntime -DataFactoryName ""CappDashboardDataFactory"" -ResourceGroupName ""ADFResourceGroup"" -Name ""CappDashboardDataFactory-Selfhosted-IR""
if(-not $IR)
{
Set-AzDataFactoryV2IntegrationRuntime -ResourceGroupName ""ADFResourceGroup"" -DataFactoryName ""CappDashboardDataFactory"" -Name ""CappDashboardDataFactory-Selfhosted-IR"" -Type SelfHosted -Description ""selfhosted IR description""
# created successfully
 Write-Output ""Created Successfully""
}
else
{
 # already exists
 Write-Output ""Already Exists""
}
</code></pre>

<p>when i run script it does not display message from else block. can anyone help 
<a href=""https://i.stack.imgur.com/PKpOF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PKpOF.png"" alt=""enter image description here""></a></p>
","<azure><powershell><azure-data-factory><azure-powershell>","2019-04-17 13:01:14","507","1","2","55831517","<p>I tried the same code ,it worked for me .still if you want to cross verify , you can do that by calling below function.</p>

<pre><code>Get-AzDataFactoryV2IntegrationRuntime -ResourceGroupName rg-test-dfv2 -DataFactoryName test-df-eu2 -Name test-dedicated-ir

It will give you following result which you can verify later

    Location                     : West US
    NodeSize                     : Standard_D1_v2
    NodeCount                    : 1
    MaxParallelExecutionsPerNode : 1
    CatalogServerEndpoint        : test.database.windows.net
    CatalogAdminUserName         : test
    CatalogAdminPassword         : **********
    CatalogPricingTier           : S1
    VNetId                       : 
    Subnet                       : 
    State                        : Starting
    ResourceGroupName            : rg-test-dfv2
    DataFactoryName              : test-df-eu2
    Name                         : test-dedicated-ir
    Description                  : Reserved IR
</code></pre>

<p>Hope it helps.</p>
"
"55728157","Create Azure Data Factory Integration Runtime if it does not exist - Azure PowerShell","<p>I have already created <code>Azure Data Factory</code> <code>Integration Runtime</code> in Azure Portal and now want to create via PowerShell script </p>

<pre><code>   $IR = Get-AzDataFactoryV2IntegrationRuntime -DataFactoryName ""CappDashboardDataFactory"" -ResourceGroupName ""ADFResourceGroup"" -Name ""CappDashboardDataFactory-Selfhosted-IR""
if(-not $IR)
{
Set-AzDataFactoryV2IntegrationRuntime -ResourceGroupName ""ADFResourceGroup"" -DataFactoryName ""CappDashboardDataFactory"" -Name ""CappDashboardDataFactory-Selfhosted-IR"" -Type SelfHosted -Description ""selfhosted IR description""
# created successfully
 Write-Output ""Created Successfully""
}
else
{
 # already exists
 Write-Output ""Already Exists""
}
</code></pre>

<p>when i run script it does not display message from else block. can anyone help 
<a href=""https://i.stack.imgur.com/PKpOF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PKpOF.png"" alt=""enter image description here""></a></p>
","<azure><powershell><azure-data-factory><azure-powershell>","2019-04-17 13:01:14","507","1","2","56577427","<p>You have to adjust your if statement:</p>

<pre><code>if($null -eq $IR)
</code></pre>
"
"55727880","What are the pros & cons of CREATE EXTERNAL TABLE compared to CTAS?","<p>I am trying to build an Azure Data Factory pipeline that grabs data from an Azure Data Lake and transforms it. </p>

<p>I want to store the transformed data in an Azure SQL Data Warehouse (as facts and dimensions).</p>

<p>I ended up with two solutions to make the data available in the warehouse:</p>

<ol>
<li>Use <a href=""https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=sql-server-2017"" rel=""nofollow noreferrer"">CREATE EXTERNAL TABLE</a> and reference the transformed data that sit in the Lake in <code>.csv</code> files;</li>
<li>Use <a href=""https://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-as-select-azure-sql-data-warehouse?view=aps-pdw-2016-au7"" rel=""nofollow noreferrer"">CTAS</a> to copy the <code>.csv</code> files from the Lake into the Warehouse;</li>
</ol>

<p><strong>Question</strong></p>

<p>It is not obvious to me what the tradeoffs in play. On one hand, it seems to me that the 1st option avoids copies to the warehouse (is it though?). But the 2nd option would offer more options such as indexing.</p>

<p>What are the pros &amp; cons of CREATE EXTERNAL TABLE compared to CTAS?</p>
","<sql><azure><azure-data-factory><azure-data-lake><azure-synapse>","2019-04-17 12:46:39","857","1","1","55729792","<p>The main benefit of landing the data in the DW (using CTAS) is that subsequent queries against that data will be significantly faster. You'll get full distribution of the data across your nodes and distributions. Any queries that filter or join the table will be substantially faster.</p>

<p>The downside, as you point out, is that you're now storing another copy of the data ... assuming that you don't delete the file from your data lake once ingested.</p>

<p>Think of Create External Table as simply providing a mapping of the external file to a SQL data structure.</p>
"
"55727018","Can we pass Databricks output to function in an ADF Job?","<p>Can anyone help me with Databricks and Azure function?
I'm trying to pass data bricks JSON output to azure function body in ADF job, is it possible? 
If yes, How? 
If No, what other alternatives to do the same?</p>
","<python><json><azure-functions><azure-data-factory><azure-databricks>","2019-04-17 11:54:34","9318","4","2","55735608","<p>In Azure Databricks, there is a way to return a value on exit. <code>dbutils.notebook.exit(myReturnValueGoesHere)</code></p>

<p>In Azure Data Factory V2, the DatabricksNotebook activity outputs JSON with 3 fields:
""runPageUrl"" , a URL to see the output of the run.
""effectiveIntegrationRuntime"" , where the code is executing
""executionDuration""</p>

<p>If you use the above dbutils call,  a fourth field will appear in the output JSON
""runOutput"" , where the ""myReturnValueGoesHere"" is displayed.</p>

<p>If you do not use the above dbutils call, you can scrape information from the runPageUrl.</p>
"
"55727018","Can we pass Databricks output to function in an ADF Job?","<p>Can anyone help me with Databricks and Azure function?
I'm trying to pass data bricks JSON output to azure function body in ADF job, is it possible? 
If yes, How? 
If No, what other alternatives to do the same?</p>
","<python><json><azure-functions><azure-data-factory><azure-databricks>","2019-04-17 11:54:34","9318","4","2","60104599","<p>In the Notebook you can use:</p>

<p><code>dbutils.notebook.exit(myReturnValueGoesHere)</code> (as already mentioned)</p>

<p>and then in ADF the JSON is an object that sits on output.runOutput, so <code>@activity('RunNotebookActivityName').output.runOutput</code>.</p>

<p>If you return:</p>

<p><code>dbutils.notebook.exit('{""hello"": {""some"": {""object"": ""value""}}}')</code></p>

<p>you can read in ADF using:</p>

<p><code>@activity('RunNotebookActivityName').output.runOutput.hello.some.object</code></p>

<p>Cool hey?</p>

<p>For a full list of what you can do see:</p>

<p><a href=""https://the.agilesql.club/2020/02/passing-status-messages-and-results-back-from-databricks-to-adf/"" rel=""noreferrer"">https://the.agilesql.club/2020/02/passing-status-messages-and-results-back-from-databricks-to-adf/</a></p>

<p>ed</p>
"
"55712775","Can XML be mapped to a SQL Server table in ADF?","<p>We would like to use Azure Data Factory to read an XML document and be able to map the columns in the document to a SQL Server table so we can move the data contained in the document to a SQL table. Is this possible in ADF?</p>
","<azure-data-factory>","2019-04-16 16:29:22","419","1","2","55718468","<p>Please note that <code>XML</code> file type in copy activity is not supported based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs"" rel=""nofollow noreferrer"">document</a>.</p>

<p>I suggest you voting up an <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/17508058-xml-file-type-in-copy-activity-along-with-xml-sc"" rel=""nofollow noreferrer"">idea</a> submitted by another Azure customer.</p>

<p>All of the feedback you share in these forums will be monitored and reviewed by the Microsoft engineering teams responsible for building Azure.</p>

<p>As workarounds,maybe you could get some clues from this <a href=""https://www.mssqltips.com/sqlservertip/2899/importing-and-processing-data-from-xml-files-into-sql-server-tables/"" rel=""nofollow noreferrer"">link</a>.</p>
"
"55712775","Can XML be mapped to a SQL Server table in ADF?","<p>We would like to use Azure Data Factory to read an XML document and be able to map the columns in the document to a SQL Server table so we can move the data contained in the document to a SQL table. Is this possible in ADF?</p>
","<azure-data-factory>","2019-04-16 16:29:22","419","1","2","68556118","<p>Azure Data factory now supports XML format in both copy activity and mapping data flow.</p>
<p>Learn more from <a href=""https://techcommunity.microsoft.com/t5/azure-data-factory/azure-data-factory-adds-support-for-xml-format/ba-p/1529012"" rel=""nofollow noreferrer"">https://techcommunity.microsoft.com/t5/azure-data-factory/azure-data-factory-adds-support-for-xml-format/ba-p/1529012</a>.</p>
"
"55705555","how to configure data factory to connect to sql server behind NSG","<p>I have a sql server running on a VM which and have configured NSG for the VM. I would like use azure data factory to connect to the database. </p>

<p>How do I set an NSG rule to allow data factory to connect to sql server</p>
","<azure-data-factory><azure-nsg>","2019-04-16 10:03:25","334","0","1","55719139","<p>Unfortunately, It is not currently possible to identify the IP Address of the DF. You may also vote this <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/20565967-identify-ip-address-of-data-factory"" rel=""nofollow noreferrer"">feedback</a>. So one option is to whitelist the <a href=""https://www.microsoft.com/en-us/download/details.aspx?id=41653"" rel=""nofollow noreferrer"">Azure datacenter IP address</a> list with filtering DF region.</p>
"
"55695395","Azure Copy Activity Rest Results Unexpected","<p>I'm attempting to pull data from the Square Connect v1 API using ADF.  I'm utilizing a Copy Activity with a REST source.  I am successfully pulling back data, however, the results are unexpected.</p>

<p>The endpoint is /v1/{location_id}/payments.  I have three parameters, shown below.
<a href=""https://i.stack.imgur.com/0mopQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0mopQ.png"" alt=""Parameters""></a></p>

<p>I can successfully pull this data via Postman.</p>

<p><a href=""https://i.stack.imgur.com/CYc5E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CYc5E.png"" alt=""enter image description here""></a></p>

<p>The results are stored in a Blob and are as if I did not specify any parameters whatsoever.<br>
<a href=""https://i.stack.imgur.com/QeFbn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QeFbn.png"" alt=""enter image description here""></a></p>

<p>Only when I hardcode the parameters into the relative path 
<a href=""https://i.stack.imgur.com/jCheC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jCheC.png"" alt=""enter image description here""></a></p>

<p>do I get correct results.</p>

<p><a href=""https://i.stack.imgur.com/oRToa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oRToa.png"" alt=""enter image description here""></a></p>

<p>I feel I must be missing a setting somewhere, but which one?</p>
","<azure-data-factory><square-connect>","2019-04-15 18:38:38","50","0","1","55698773","<p>You can try setting the values you want into a setVariable activity, and then have your copyActivity reference those variables.  This will tell you whether it is an issue with the dynamic content or not.  I have run into some unexpected behavior myself.  The benefit of the intermediate setVariable activity is twofold.  Firstly it coerces the datatype, secondly, it lets you see what the value is.</p>

<p>My apologies for not using comments.  I do not yet have enough points to comment.</p>
"
"55692012","Unable to delete Azure Data Factory - DeleteFactoryBlockedByIR","<p>I'm attempting to delete a Azure Data Factory</p>

<pre><code>There was an error while deleting the data factory. Please stop all SSIS Integration Runtimes, remove all Azure VNet Integration Runtimes and remove all Self-Hosted Integration Runtimes sharing before deleting your Data Factory.
0 Azure VNet Integration Runtimes: .
0 Shared Self-hosted Integration Runtimes: .
1 Azure SSIS Integration Runtimes with status: Started: integrationRuntime1 ; . DeleteFactoryBlockedByIR
</code></pre>

<p>When in the UI, the delete options are greyed out.</p>

<p><a href=""https://i.stack.imgur.com/lNJVk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lNJVk.png"" alt=""enter image description here""></a></p>

<p>How can I delete the factory?</p>
","<azure><azure-data-factory>","2019-04-15 15:01:08","1799","1","1","55699962","<p>Noticed the log infomation in your error message：<code>1 Azure SSIS Integration Runtimes with status: Started:</code>.</p>

<p>Based on the steps which are listed in this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/manage-azure-ssis-integration-runtime#delete-an-azure-ssis-ir"" rel=""nofollow noreferrer"">document</a> and the details in the error message:</p>

<p><a href=""https://i.stack.imgur.com/b3b2d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/b3b2d.png"" alt=""enter image description here""></a></p>

<p>you need to stop the IRs first so that you can reconfigure or remove them.</p>
"
"55689221","How to trigger Azure Data Factory pipeline using PowerShell (Azure Automation)?","<p>I was wondering if it were possible to trigger Data Factory dynamically using PowerShell? Are there scripts available or some workarounds using APIs?</p>

<p>For example after scaling the database from S2 -> S9 using PowerShell I'd like to trigger specific Data Factory pipelines.</p>
","<azure><powershell><azure-data-factory>","2019-04-15 12:28:44","3712","3","1","55689606","<p>Is this something you might be looking for?</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers</a></p>

<p>Invoke-AzDataFactoryV2Pipeline -DataFactory $df -PipelineName ""Adfv2QuickStartPipeline"" -ParameterFile .\PipelineParameters.json</p>
"
"55669312","(how do I)Azure data Factory to (on prem) mysql DB connection using cert","<p>We need to pull data from a client on prem DB, they require us to use SSL+ their cert when connecting.</p>

<p>The only issue I'm having is: I have no idea where to ""import"" the PEM file for their certs in data factory. I know that I should use:""UseSystemTrustStore=&lt;0/1>;"" in my connection string when setting up the Data factory connection but can't understand where I should put the cert files. </p>

<p>PS. running DFv2 
also: my appologies for bad english It's not my first language.</p>

<p>-Duck</p>
","<mysql><azure-data-factory>","2019-04-13 19:56:09","404","1","1","55699004","<p>When dealing with on-premises data, usually you will want to use a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">self-hosted integration runtime</a>.  The self-hosted integration runtime will handle the connection issues.</p>

<p>You can store your certificates in Azure Key Vault, and then reference the certificate , as explained in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault"" rel=""nofollow noreferrer"">this document</a></p>
"
"55654371","Unable to Register Microsoft.DataFactory using Azure PowerShell","<p>I am new to <code>Azure Data Factory</code> and <code>PowerShell</code> and trying to register <code>Microsoft.DataFactory</code> in Azure subscription using following command in <code>Azure PowerShell</code></p>

<pre><code>Register-AzureRmResourceProvider -ProviderNamespace Microsoft.DataFactory
</code></pre>

<p>but getting this error.</p>

<p><a href=""https://i.stack.imgur.com/SCpWK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SCpWK.png"" alt=""enter image description here""></a></p>

<p>Could anyone help please</p>
","<azure><powershell><azure-data-factory><azure-powershell>","2019-04-12 14:54:32","75","0","1","55655734","<p>Close out your PowerShell console and reopen it. Then log back in, make sure you are on the right subscription (if you have multiple), and try the command again.</p>

<p><strong>Log Into Azure:</strong></p>

<blockquote>
  <p>New azure module command for that is -->
  Login-AzAccount . I think the old command was --> Login-AzureRmAccount
  .</p>
</blockquote>
"
"55652722","What is the difference between using a COPY DATA activity to a SQL table vs using CREATE EXTERNAL TABLE?","<p>I have a bunch of U-SQL activities that manipulates &amp; transform data in an <code>Azure Data Lake</code>. Out of this, I get a <code>csv</code> file that contains all my events. </p>

<p>Next I would just use a <code>Copy Data</code> activity to copy the <code>csv</code> file from the Data Lake directly into an <code>Azure SQL Data Warehouse</code> table. </p>

<p><a href=""https://i.stack.imgur.com/rabOH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rabOH.png"" alt=""Extract-Transform-Merge-Load""></a></p>

<ol>
<li>I extract the information from a bunch of <code>JSON</code> files stored in the Data Lake and create a staging <code>.csv</code> file;</li>
<li>I grab the staging <code>.csv</code> file &amp; a production <code>.csv</code> file and inject the latest change (and avoid duplicates) and save the production <code>.csv</code> file;</li>
<li>Copy the <code>.csv</code> production file directly to the Warehouse table.</li>
</ol>

<p>I realized that my table contains duplicated rows and, after having tested the <code>U-SQL</code> scripts, I assume that the <code>Copy Data</code> activity -somehow- merges the content of the <code>csv</code> file into the table. </p>

<p><strong>Question</strong></p>

<p>I am not convinced I am doing the right thing here. Should I define my warehouse table as an <a href=""https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=sql-server-2017"" rel=""nofollow noreferrer"">EXTERNAL</a> table that would get its data from the <code>.csv</code> production file? Or should I change my U-SQL to only include the latest changes?</p>
","<azure><azure-data-factory><azure-data-lake><u-sql><azure-synapse>","2019-04-12 13:26:38","681","0","1","55653262","<p>If you want to use external tables depends on your use case. If you want the data to be stored inside SQL DW for better performance, you have to copy it at some point, e.g. via a stored procedure. You could then just call the stored procedure from ADF, for instance.</p>

<p>Or, if you don't want to / cannot filter out data beforehand, you could also implement an ""Upsert"" stored procedure in your SQL DW and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure"" rel=""nofollow noreferrer"">call this</a> to insert your data instead of the copy activity.</p>
"
"55651479","How to handle a missing file in a U-SQL EXTRACT statement?","<p>I have the following <code>EXTRACT</code> statement that grabs the data from both a staging file and the production file:</p>

<pre><code>DECLARE @staging  string = ""/staging/events.csv"";
DECLARE @production string = ""/production/events.csv"";

@events =
        EXTRACT dimensionId         string
              , tenantId            string 
              , internalEntityId    long
              , name                string
              , modifiedTimestamp   DateTime
        FROM @staging, @production
        USING Extractors.Csv();
</code></pre>

<p>The statement will fail if either one of the two files is missing, which causes the whole <code>Azure Data Factory</code> pipeline to fail:</p>

<p><a href=""https://i.stack.imgur.com/x6L8v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x6L8v.png"" alt=""Input stream(s) don&#39;t exist""></a></p>

<p><strong>Question</strong></p>

<p>How can I gracefully handle a missing file in a <code>U-SQL</code> <code>EXTRACT</code> statement?</p>
","<c#><azure><azure-data-factory><azure-data-lake><u-sql>","2019-04-12 12:18:35","77","1","1","55659173","<p>I am not sure I can qualify this as ""Gracefully"" but at least it does the trick. </p>

<p>I can use the <a href=""https://learn.microsoft.com/en-us/u-sql/functions/metadata/file/file-exists"" rel=""nofollow noreferrer"">FILE.EXISTS</a> statement to test the presence of the file beforehand and choose to include it or not:</p>

<pre><code>DECLARE @staging  string = ""/staging/events.csv"";
DECLARE @production string = ""/production/events.csv"";

IF FILE.EXISTS(@production) == true THEN
 @events =
     EXTRACT    dimensionId         string
              , tenantId            string 
              , internalEntityId    long
              , name                string
              , modifiedTimestamp   DateTime
        FROM @staging, @production
        USING Extractors.Csv();
ELSE
@events =
     EXTRACT    dimensionId         string
              , tenantId            string 
              , internalEntityId    long
              , name                string
              , modifiedTimestamp   DateTime
        FROM @staging
        USING Extractors.Csv();
END;
</code></pre>
"
"55649080","Azure Data Factory: Move data from Google Bigquery to Azure Blob","<p>I am using Azure Data Factory. How can I move data from Google BigQuery to Azure Blob incrementally.</p>
","<google-bigquery><azure-blob-storage><azure-data-factory>","2019-04-12 09:57:22","1170","-5","1","55699378","<p>You can reference this document: <a href=""https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/data-factory/connector-google-bigquery.md#copy-data-from-google-bigquery-by-using-azure-data-factory"" rel=""nofollow noreferrer"">Copy data from Google BigQuery by using Azure Data Factory</a>.</p>

<p>This article outlines how to use Copy Activity in Azure Data Factory to copy data from Google BigQuery. It builds on the Copy Activity overview article that presents a general overview of the copy activity.</p>

<p>You can copy data from Google BigQuery to any supported sink data store. For a list of data stores that are supported as sources or sinks by the copy activity, see the Supported data stores table.</p>

<p>Azure Data Factory supports Azure Blob storage. To know more about Copy Actives, please reference <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">Copy Activity in Azure Data Factory</a>. </p>

<p>In Azure Data Factory, you can use Copy Activity to copy data among data stores located on-premises and in the cloud.</p>

<p>Hope this helps.</p>
"
"55644149","How to troubleshoot - Azure DataFactory - Copy Data Destination tables have not been properly configured","<p>I'm setting up a SQL Azure Copy Data job using Data Factory. For my source I'm selecting the exact data that I want. For my destination I'm selecting use stored procedure.  I cannot move forward from the table mapping page as it reports 'one or more destination tables have been been properly configured'. From what I can tell. Everything looks good as I can manually run the stored procedure from SQL without an issue. </p>

<p>I'm looking for troubleshooting advice on how to solve this problem as the portal doesn't appear to provide any more data then the error itself.</p>

<p>Additional but unrelated question: What is the benefit from me doing a copy job in data factory vs just having data factory call a stored procedure?</p>

<p>I've tried executing the stored procedure on via SQL. I discovered one problem with that as I had LastUpdatedDate in the TypeTable but it isnt actually an input value. After fixing that I'm able to execute the SP without issue.</p>

<p>Select Data from Source</p>

<pre><code>SELECT
                p.EmployeeNumber, 
                p.EmailName, 
            FROM PersonFeed AS p

</code></pre>

<p>Create table Type</p>

<pre><code>CREATE TYPE [person].[PersonSummaryType] AS TABLE(
    [EmployeeNumber] [int] NOT NULL,
    [EmailName] [nvarchar](30) NULL
)
</code></pre>

<p>Create UserDefined Stored procedure</p>

<pre><code>CREATE PROCEDURE spOverwritePersonSummary @PersonSummary [person].[PersonSummaryType] READONLY
AS
BEGIN
      MERGE [person].[PersonSummary] [target]
        USING @PersonSummary [source]
        ON [target].EmployeeNumber = [source].EmployeeNumber
        WHEN MATCHED THEN UPDATE SET
            [target].EmployeeNumber = [source].EmployeeNumber,
            [target].EmailName = [source].EmailName,
            [target].LastUpdatedDate = GETUTCDATE()
        WHEN NOT MATCHED THEN INSERT (
            EmployeeNumber, 
            EmailName, 
            LastUpdatedDate)
        VALUES(
            [source].EmployeeNumber, 
            [source].EmailName, 
            GETUTCDATE());
END
</code></pre>

<p>Datafactory UI when setting destination on the stored procedure reports ""one or more destination tables have been been properly configured""</p>
","<azure><azure-data-factory>","2019-04-12 03:38:05","120","1","1","55645087","<p>I believe the UI is broken when using the Copy Data. I was able to map directly to a table to get the copy job created then manually edit the JSON and everything worked fine. Perhaps the UI is new and that explains why all the support docs only refer only to the json? After playing with this more it looks like the UI sees the table type as schema.type, but it drops the schema for some reason. A simple edit in the JSON file corrects it.</p>
"
"55643855","Refresh Power BI dataset through Azure Data factory","<p>Is there a way to refresh Power BI dataset through Azure Data factory? </p>

<p>I found out that there is one REST API (<a href=""https://powerbi.microsoft.com/en-us/blog/announcing-data-refresh-apis-in-the-power-bi-service/"" rel=""nofollow noreferrer"">https://powerbi.microsoft.com/en-us/blog/announcing-data-refresh-apis-in-the-power-bi-service/</a>), which can be used to do this task but I am struggling to authenticate and pass an authorization token for the same. </p>

<p>Please suggest what other alternate options do we have?</p>
","<powerbi><azure-data-factory>","2019-04-12 02:58:49","2400","1","4","55647298","<p>Currently Azure Data Factory <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity#authentication"" rel=""nofollow noreferrer"">supports</a> only <code>Basic</code> and <code>Client certificate</code> authentication:</p>

<blockquote>
  <p>Authentication method used for calling the endpoint. Supported Types are ""Basic, or ClientCertificate.""</p>
</blockquote>
"
"55643855","Refresh Power BI dataset through Azure Data factory","<p>Is there a way to refresh Power BI dataset through Azure Data factory? </p>

<p>I found out that there is one REST API (<a href=""https://powerbi.microsoft.com/en-us/blog/announcing-data-refresh-apis-in-the-power-bi-service/"" rel=""nofollow noreferrer"">https://powerbi.microsoft.com/en-us/blog/announcing-data-refresh-apis-in-the-power-bi-service/</a>), which can be used to do this task but I am struggling to authenticate and pass an authorization token for the same. </p>

<p>Please suggest what other alternate options do we have?</p>
","<powerbi><azure-data-factory>","2019-04-12 02:58:49","2400","1","4","55821044","<p>I was able to refresh my Power BI dataset from Azure Data Factory using Power BI REST API's (POST/GET), the challenge was to pass an authentication token so that I figured out can be done by capturing ""access_token"" in a separate activity and then pass it to the POST activity which actually refreshes the dataset.</p>

<p>Here is the flow:</p>

<ol>
<li>Web Activity to get the <strong>token_type</strong> and <strong>access_token</strong>. I had to pass various parameters to pipeline (refer screenshot below).</li>
</ol>

<p>URL:  <a href=""https://login.microsoftonline.com/common/oauth2/token"" rel=""nofollow noreferrer"">https://login.microsoftonline.com/common/oauth2/token</a>  </p>

<p><a href=""https://i.stack.imgur.com/CVMz9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CVMz9.png"" alt=""enter image description here""></a></p>

<ol start=""2"">
<li>Another WEB activity to call POST API to refresh the dataset. In this I have passed the authentication tokens captured in previous Login Web activity (refer screenshot below):</li>
</ol>

<p><a href=""https://i.stack.imgur.com/SuPjk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SuPjk.png"" alt=""enter image description here""></a></p>

<p>Hope this helps!!</p>

<p>Feel free to reach out in case of any questions.</p>
"
"55643855","Refresh Power BI dataset through Azure Data factory","<p>Is there a way to refresh Power BI dataset through Azure Data factory? </p>

<p>I found out that there is one REST API (<a href=""https://powerbi.microsoft.com/en-us/blog/announcing-data-refresh-apis-in-the-power-bi-service/"" rel=""nofollow noreferrer"">https://powerbi.microsoft.com/en-us/blog/announcing-data-refresh-apis-in-the-power-bi-service/</a>), which can be used to do this task but I am struggling to authenticate and pass an authorization token for the same. </p>

<p>Please suggest what other alternate options do we have?</p>
","<powerbi><azure-data-factory>","2019-04-12 02:58:49","2400","1","4","58431518","<p>Matter of fact, there is an APP implementing REST API in Azure Logic Apps. You can invoke the logic app from the Azure Data Factory then. There is, however, one caveat. Number of refresh requests in last 24 hours are limited to 8 in the premium account - As mentioned in the REST API documentation <a href=""https://powerbi.microsoft.com/en-us/blog/announcing-data-refresh-apis-in-the-power-bi-service/"" rel=""nofollow noreferrer"">here</a>. </p>

<p>This is how it looks like in my Logic App Designer.</p>

<p><a href=""https://i.stack.imgur.com/1Ndtp.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1Ndtp.jpg"" alt=""This is how it will look like in the Logic App Designer""></a></p>

<p>Hope it helps.</p>
"
"55643855","Refresh Power BI dataset through Azure Data factory","<p>Is there a way to refresh Power BI dataset through Azure Data factory? </p>

<p>I found out that there is one REST API (<a href=""https://powerbi.microsoft.com/en-us/blog/announcing-data-refresh-apis-in-the-power-bi-service/"" rel=""nofollow noreferrer"">https://powerbi.microsoft.com/en-us/blog/announcing-data-refresh-apis-in-the-power-bi-service/</a>), which can be used to do this task but I am struggling to authenticate and pass an authorization token for the same. </p>

<p>Please suggest what other alternate options do we have?</p>
","<powerbi><azure-data-factory>","2019-04-12 02:58:49","2400","1","4","70869593","<p>While most of the answers that I found on the web suggest to use app registrations / service principles, logic apps or even azure functions, I figured out a way to skip all that. I have actually written a <a href=""https://www.tackytech.blog/how-to-refresh-power-bi-datasets-from-data-factory-with-managed-identity/"" rel=""nofollow noreferrer"">blogpost</a> about this.</p>
<p>Essentially, it is pretty similar to the app registration approach and it uses even the refreshes-PBI API, but instead of adding an app's ClientID/ApplicationID to the security group, you can just use the data factory's managed identity. With that, a token retrieval (plus secret in key vault) is not needed either. Just follow these steps:</p>
<ol>
<li>Create a security group in Azure and add the managed identity of the data factory to it</li>
<li>Align Power BI tenant settings to allow service principles to use APIs and add security group to be able use these settings. Also, add service principal to workspace.</li>
<li>Build web activity in Azure Data Factory pipeline as seen in this <a href=""https://i.stack.imgur.com/Axzcu.png"" rel=""nofollow noreferrer"">screenshot</a> or here:</li>
</ol>
<p>URL : <a href=""https://api.powerbi.com/v1.0/myorg/groups/%5BworkspaceID%5D/datasets/%5BdatasetID%5D/refreshes"" rel=""nofollow noreferrer"">https://api.powerbi.com/v1.0/myorg/groups/[workspaceID]/datasets/[datasetID]/refreshes</a></p>
<p>Method :  Post</p>
<p>Body : {“notifyOption” : “NoNotification”}</p>
<p>Authentication : Managed Identity</p>
<p>Resource : <a href=""https://analysis.windows.net/powerbi/api"" rel=""nofollow noreferrer"">https://analysis.windows.net/powerbi/api</a></p>
"
"55641250","azure blob download task issue","<p>While creating SSIS package with the azure blob download task, the files can be downloaded from Azure blob to my local machine. The SSIS execution is fine but once I deploy the package in SSISDB( In Azure Sql DB), the Azure blob download task fails as it is unable to access the local folder.</p>

<p><a href=""https://i.stack.imgur.com/qBUog.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qBUog.png"" alt=""The task details is mentioned here""></a></p>

<p>The task can't be run in the SSMS ( in Azure). the error showing as the local path can't be accessed.</p>
","<azure><ssis><azure-sql-database><azure-blob-storage><azure-data-factory>","2019-04-11 21:12:49","463","-1","1","55658990","<p>Please consider to join your Azure-SSIS IR to a Virtual Network (VNet) connected to your on-premises network, see <a href=""https://learn.microsoft.com/en-us/azure/data-factory/join-azure-ssis-integration-runtime-virtual-network"" rel=""nofollow noreferrer"">Join Azure-SSIS IR to a VNet</a>. This way you can provide a file share on-premises as a &quot;local directory&quot; on the Azure Blob Download Task.</p>
<p>To access a file share on premises from packages running in Azure, do the following things:</p>
<ol>
<li>Allow access through Windows firewall.</li>
<li>Join your Azure-SSIS IR to a VNet that is connected to the file share on premises.</li>
<li>Use SSISDB catalog.set_execution_credential stored procedure to provide
credentials.</li>
</ol>
"
"55630853","is it posible update row values from tables in Azure Data Factory?","<p>I have a dataset in Data Factory, and I would like to know if is possible update row values using only data factory activities, without data flow, store procedures, queries...</p>
","<azure><azure-data-factory>","2019-04-11 10:56:48","9790","0","4","55643296","<p><strong>Concepts</strong>:</p>

<p>Datasets:</p>

<p>Datasets represent data structures within the data stores, which simply point to or reference the data you want to use in your activities as inputs or outputs.</p>

<p>Now, a dataset is a named view of data that simply points or references the data you want to use in your activities as inputs and outputs. Datasets identify data within different data stores, such as tables, files, folders, and documents. For example, an Azure Blob dataset specifies the blob container and folder in Blob storage from which the activity should read the data.</p>

<p>Currently, according to my experience, it's impossible to update row values using only data factory activities. Azure Data Factory doesn't support this now.</p>

<p>Fore more details，please reference: </p>

<ol>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/introduction#datasets"" rel=""nofollow noreferrer"">Datasets</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-datasets-linked-services"" rel=""nofollow noreferrer"">Datasets and linked services in Azure Data Factory</a>.</li>
</ol>

<p>For example, when I use Copy Active, Data Factory doesn't provide my any ways to update the rows:
<a href=""https://i.stack.imgur.com/WJpHk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WJpHk.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"55630853","is it posible update row values from tables in Azure Data Factory?","<p>I have a dataset in Data Factory, and I would like to know if is possible update row values using only data factory activities, without data flow, store procedures, queries...</p>
","<azure><azure-data-factory>","2019-04-11 10:56:48","9790","0","4","59159634","<p>There is a way to do update (and probably any other SQL statement) from Data Factory, it's a bit tacky though.
The <strong>Loopup</strong> activity, can execute a set of statements in Query mode, ie:
<a href=""https://i.stack.imgur.com/khILx.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/khILx.png"" alt=""https://i.stack.imgur.com/khILx.png""></a></p>

<p><strong>The only condition is to end it with select, otherwise Lookup activity throws error.</strong></p>

<p>This works for Azure SQL, PostgreSQL, and most likely for any other DB Data Factory can connect to.</p>
"
"55630853","is it posible update row values from tables in Azure Data Factory?","<p>I have a dataset in Data Factory, and I would like to know if is possible update row values using only data factory activities, without data flow, store procedures, queries...</p>
","<azure><azure-data-factory>","2019-04-11 10:56:48","9790","0","4","66373667","<p>This is now possible in Azure Data Factory, your Data flow should have an Alter Row stage, and the Sink has a drop-down where you can select the key column for doing updates.</p>
<p><a href=""https://i.stack.imgur.com/1kscu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1kscu.png"" alt=""enter image description here"" /></a>
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-alter-row"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-alter-row</a></p>
"
"55630853","is it posible update row values from tables in Azure Data Factory?","<p>I have a dataset in Data Factory, and I would like to know if is possible update row values using only data factory activities, without data flow, store procedures, queries...</p>
","<azure><azure-data-factory>","2019-04-11 10:56:48","9790","0","4","67983076","<p>As mentioned in Above comment regarding ADF data flow, ADF data flow does not support on-permise sink or source, the sink &amp; source should reside in Azure SQL or Azure Data lake or any other AZURE data services.</p>
"
"55626813","How to replace Kusto cluster rows (Azure Data Explorer) when ingesting from Azure Table Store","<p>Using Azure Data Factory, I created a pipeline that ingests from source Azure Table Store to sink Kusto (Azure Data Explorer). The ingestion does an append to Kusto. However, I need to change this to do a set-or-replace ingestion rather than an append. I'd like to replace existing (PartitionKey, RowKey) rows found in my Kusto cluster with matching (PartitionKey, RowKey) from the Azure Table and append any rows where there is no match on (PartitionKey, RowKey). Any ideas?</p>
","<azure-data-factory><azure-data-explorer>","2019-04-11 07:23:05","1391","0","2","55659170","<p>Kusto only support Insert there wont be an update as per my knowledge..</p>
"
"55626813","How to replace Kusto cluster rows (Azure Data Explorer) when ingesting from Azure Table Store","<p>Using Azure Data Factory, I created a pipeline that ingests from source Azure Table Store to sink Kusto (Azure Data Explorer). The ingestion does an append to Kusto. However, I need to change this to do a set-or-replace ingestion rather than an append. I'd like to replace existing (PartitionKey, RowKey) rows found in my Kusto cluster with matching (PartitionKey, RowKey) from the Azure Table and append any rows where there is no match on (PartitionKey, RowKey). Any ideas?</p>
","<azure-data-factory><azure-data-explorer>","2019-04-11 07:23:05","1391","0","2","57139256","<p>You can only append the rows to existing tables.
One way to achieve your scenario is by having one more column called SnapshotTime or similar name. Whenever you have the latest data for some partition or category, you can set this to latest timestamp. When querying you can take the latest from this timestamp for every category of the data.</p>
"
"55622345","Azure Data Factory - Recording file name when reading all files in folder from Azure Blob Storage","<p>I have a set of CSV files stored in Azure Blob Storage. <br> I am reading the files into a database table using the Copy Data task. <br> The Source is set as the folder where the files reside, so it's grabbing it's file and loading it into the database. <br> The issue is that I can't seem to map the file name in order to read it into a column. I'm sure there are more complicated ways to do it, for instance first reading the metadata and then read the files using a loop, but surely the file metadata should be available to use while traversing through the files? <br>
Thanks</p>
","<azure><azure-data-factory>","2019-04-10 22:50:21","1241","-1","1","55635093","<p>This is not possible in a regular copy activity. Mapping Data Flows has this possibility, it's still in preview, but maybe it can help you out. If you check the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-source#source-file-management"" rel=""nofollow noreferrer"">documentation</a>, you find an option to specify a column to store file name.</p>

<p>It looks like this:
<a href=""https://i.stack.imgur.com/jTkXR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jTkXR.png"" alt=""enter image description here""></a></p>
"
"55609692","How transform a column befory copy?","<p>I would like to copy a table from sourceA to sourceB, but before I want to transform column values. How could I do it?  I've tried to do it in the mapping trying to add dynamic content but I think I'm not doing it correctly.</p>

<p>Thanks!</p>
","<azure><azure-data-factory>","2019-04-10 09:55:34","1056","1","4","55609834","<p>jnoguerm,please still adopt the stored procedure idea which I mentioned in the previous case:<a href=""https://stackoverflow.com/questions/55539415/how-to-copy-data-in-azure-data-factory-depending-on-the-values"">How to copy data in Azure Data Factory depending on the values?</a>.</p>

<p>You could copy the data from source table to the temporary table.And do the transform business and insert operations in the stored procedure.In this case: <a href=""https://stackoverflow.com/questions/51352725/azure-data-factory-mapping-2-columns-in-one-column/51354266#51354266,please"">Azure Data Factory mapping 2 columns in one column</a>,the questioner wants to merge two columns into one column(change,merge,calculate,update,reduce etc.),you could refer to the details in that page.</p>
"
"55609692","How transform a column befory copy?","<p>I would like to copy a table from sourceA to sourceB, but before I want to transform column values. How could I do it?  I've tried to do it in the mapping trying to add dynamic content but I think I'm not doing it correctly.</p>

<p>Thanks!</p>
","<azure><azure-data-factory>","2019-04-10 09:55:34","1056","1","4","55611658","<p>Your use case sounds like you could use the new Data Flow feature (currently in preview): <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-create"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-create</a> </p>
"
"55609692","How transform a column befory copy?","<p>I would like to copy a table from sourceA to sourceB, but before I want to transform column values. How could I do it?  I've tried to do it in the mapping trying to add dynamic content but I think I'm not doing it correctly.</p>

<p>Thanks!</p>
","<azure><azure-data-factory>","2019-04-10 09:55:34","1056","1","4","55665942","<p>Before I can answer your question, it will be relevant to know the volume and source of data.
However, there could be three possible solutions for this problem.</p>

<p><strong>1. Stored Procedure Activity in Azure Data Factory</strong> </p>

<ul>
<li>Use Copy Activity in Azure Data Factory to bring data to SQL(Staging tables), but if your source is SQL already then you can skip this step.</li>
<li>Write a stored procedure to include all the transformations that you need.</li>
<li>Use Stored procedure activity to make all the required transformations and insert the data to final table.</li>
</ul>

<p><strong>2. Use SSIS package in Azure Data Factory</strong></p>

<ul>
<li><p>You don't need to bring data to staging, you can create SSIS package using SQL Server Data Tools on your local machine. Add source, transformations and destinations</p></li>
<li><p>Add this SSIS package in Azure Data Factory, if it's mandatory for you to have Azure Data Factory otherwise you can just schedule your SSIS package using SQL Job Agent.</p></li>
</ul>

<p><strong>3. Use Data Flow with Azure Data Factory V2.0</strong></p>

<ul>
<li><p>Data Flow is in preview mode right now so you can not use this solution for any production ready solution.</p></li>
<li><p>While creating Azure Data Factory, it will ask you the option to choose from Data Factory v1, Data Factory V2 or Data Flow with Data Factory V2. So, you need to choose third option</p></li>
<li><p>Now, you can perform most of your transformations here that you could perform on SSIS package. It's easy to use, all drag and drop facilities for various transformations between source and destination. And this is the best solution even if the data volume is very high.</p></li>
</ul>

<p>Hope this will be helpful for you. All the best.
Cheers!</p>
"
"55609692","How transform a column befory copy?","<p>I would like to copy a table from sourceA to sourceB, but before I want to transform column values. How could I do it?  I've tried to do it in the mapping trying to add dynamic content but I think I'm not doing it correctly.</p>

<p>Thanks!</p>
","<azure><azure-data-factory>","2019-04-10 09:55:34","1056","1","4","55753371","<p>Use Mapping Data Flows in ADF: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column</a></p>
"
"55607073","how to map the columns from the restapi response to the database table columns?","<p>I am pulling the data from restapi in azure data factory and loading the data to sql server. after connecting the the restapi url the response of the data is given below . I need to map the columns to the table in the sql server. How can i map the columns from restapi response to table columns.</p>

<p>""@odata.context"": ""<a href=""https://io-dev.crm3.dynamics.com/api/data/v8.2/"" rel=""nofollow noreferrer"">https://io-dev.crm3.dynamics.com/api/data/v8.2/</a>$metadata#Microsoft.Dynamics.CRM.indskr_iOFetchAuditDataResponse"",
    ""JSONOutput"": ""{\""AuditDataSet\"":[{\""AuditID\"":\""f45eb7ab-0b57-e911-80f4-0004ff9d7444\"",\""ChangedBy\"":\""f8ac67d9-e7c0-e811-a979-000d3af49c6c\"",\""ChangedColumns\"":null,\""ChangedDate\"":\""2019-04-04T18:58:50Z\"",\""ChangedNewValues\"":null,\""ChangedOldValues\"":null,\""Event\"":33,\""SourceEntityName\"":\""indskr_positiongroups\"",\""SourceRecordID\"":\""b5ba61b6-e651-e911-a97f-000d3af49c44\"",\""SourceRecordName\"":\""Position Group</p>
","<azure-active-directory><azure-sql-database><azure-data-factory>","2019-04-10 07:32:47","452","0","1","55627997","<p>Here is a tutorial about <a href=""https://www.linkedin.com/pulse/azure-data-factory-copy-from-rest-api-sql-database-rohit-vangala"" rel=""nofollow noreferrer"">Azure Data Factory – Copy Data from REST API to Azure SQL Database</a>.</p>

<p>You should create a table on destination Azure SQL database firstly. </p>

<p>This tutorial can help you copy the data from restapi in azure data factory and loading the data to sql server step by step.</p>

<p>Such as mapping the columns to the table in the sql server:</p>

<p><strong>Table mapping</strong>:
<a href=""https://i.stack.imgur.com/vbiRQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vbiRQ.png"" alt=""enter image description here""></a></p>

<p><strong>Schema Mapping</strong>:
<a href=""https://i.stack.imgur.com/xGQC4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xGQC4.png"" alt=""enter image description here""></a></p>

<p>Hope this help.</p>
"
"55599455","Azure Data Factory: Whitespace in object name for hierarchial schema mapping","<p>I am using a Copy Data activity to copy data from a REST service to an Azure SQL Server. The REST service is returning a hierarchical JSON response and I am using schema mapping to push it into the SQL table. This is working as expected except for a few attributes that have whitespace in their name. Here is an example schema mapping with the last attribute defined the one in question: </p>

<pre><code>""translator"": {
    ""type"": ""TabularTranslator"",
    ""schemaMapping"": {
        ""id"": ""id"",
        ""type"": ""type"",
        ""lead_verb"": ""lead_verb"",
        ""lead_action_performed"": ""lead_action_performed"",
        ""created_at"": ""created_at"",
        ""lead_id"": ""lead_id"",
        ""selected_action"": ""selected_action"",
        ""rate_type"": ""rate_type"",
        ""channel_rate_id"": ""channel_rate_id"",
        ""tenant_id"": ""tenant_id"",
        ""unit_id"": ""unit_id"",
        ""created_by_id"": ""created_by_id"",
        ""source_id"": ""source_id"",
        ""lead_changes_humanized.Quoted Rate[1]"": ""lead_changes_humanized""
    },
    ""collectionReference"": ""$.lead_events""
}
</code></pre>

<p>This results in the following error:</p>

<pre><code>{
    ""errorCode"": ""2200"",
    ""message"": ""ErrorCode=UserErrorInvalidJsonArrayPathDefinition,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Error occurred when deserializing source JSON data. Please check whether the JsonPath in JsonNodeReference and JsonPathDefintion is valid.,Source=Microsoft.DataTransfer.Common,''Type=Newtonsoft.Json.JsonException,Message=Unexpected character while parsing path:  ,Source=Newtonsoft.Json,'"",
    ""failureType"": ""UserError"",
    ""target"": ""cpyCompaniesLeadEvents""
}
</code></pre>

<p>I have tried quoting path as follows:</p>

<pre><code>""lead_changes_humanized.\""Quoted Rate\""[1]"": ""lead_changes_humanized""
</code></pre>

<p>And, with single quotes:</p>

<pre><code>""lead_changes_humanized.'Quoted Rate'[1]"": ""lead_changes_humanized""
</code></pre>

<p>These all result in the same error. I have validated the JSON path using path validator.</p>

<p>Question: how can I deal with this whitespace?</p>
","<azure-data-factory>","2019-04-09 18:51:27","957","2","1","55604184","<p>Tried but failed to skip the copy activity validator. Please consider a workaround as below.</p>

<p>1.If you could totally control the rest api (named A)output,try to return <code>lead_changes_humanized.Quoted Rate[1]</code> as <code>lead_changes_humanized.Quoted_Rate[1]</code>.If you can't,create another your own rest api(named B) and invoke the A rest api inside B rest api,then process the output as <code>lead_changes_humanized.Quoted_Rate[1]</code>.</p>

<p>2.Then you could skip the validator with this trick.If you do concern the original format of the json key,please do the next step.</p>

<p>3.On the sql db side,you could copy the data from source data to the temporary table.And restore the original format in the stored procedure so that you could store the original format into the exact destination table.There are very detailed steps i did in my previous case: <a href=""https://stackoverflow.com/questions/51352725/azure-data-factory-mapping-2-columns-in-one-column/51354266#51354266,please"">Azure Data Factory mapping 2 columns in one column</a> refer to it.</p>
"
"55592158","Conversion of date format in CSV file","<p>Hi I have to load data from an Excel file to Azure data ware house, but during the conversion I had a problem converting the date formats in my excel file.</p>

<p>The date format in the Excel file is <code>Tue Feb 01 09:02:39 IST 2000</code>, but I need it in <code>yyyy-mm-dd</code>.</p>

<p>Can anyone help? I tried to change the date format while connecting the source, but it didn't help.</p>
","<excel><azure-data-factory>","2019-04-09 11:47:38","76","0","1","55800954","<p>I have never seen a date like that ( with the IST etc ) and I am forced to believe that at this time it is stored as string and not datetime .</p>

<p>Anyways since you are using excel you transform into different columns and then recreate the date field . The information like days and month can be stripped of as if you have the proper date , SQL have function which can be used to get these back .</p>

<p>You can transform to date by navigating to Excel ->Data -> Text to Columns.</p>
"
"55590026","Dynamic formatting of last modified filter in Data factory DataSet","<p>I'm trying to set the last modified filter in a azure data factory dataset dynamically. </p>

<p>I'm using the following expression:</p>

<p>@formatDateTime(adddays(utcnow(),-2),'yyyy-mm-ddThh:mm:ss.fffZ')</p>

<p><img src=""https://i.imgur.com/bUuFMMe.png"" alt=""Data factory formatting"" title=""Formatting as specified""></p>

<p>I'm getting the following error:</p>

<blockquote>
  <p>Activity Copy1 failed: Failure happened on 'Source' side. ErrorCode=UserErrorInvalidValueInPayload,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Failed to convert the value in &apos;modifiedDatetimeStart&apos; property to &apos;System.Nullable`1[[System.DateTime&#44; mscorlib&#44; Version&#61;4.0.0.0&#44; Culture&#61;neutral&#44; PublicKeyToken&#61;b77a5c561934e089]]&apos; type. Please make sure the payload structure and value are correct.,Source=Microsoft.DataTransfer.DataContracts,''Type=System.Reflection.TargetInvocationException,Message=Exception has been thrown by the target of an invocation.,Source=mscorlib,''Type=System.FormatException,Message=The DateTime represented by the string is not supported in calendar System.Globalization.GregorianCalendar.,Source=mscorlib,'</p>
</blockquote>

<p>I'm also not able to preview the data with this filter. I guess something is wrong here. Any ideas?</p>
","<azure><azure-data-factory>","2019-04-09 09:56:41","3916","1","5","55591782","<p>From the error message I understand that the string represenation of the date is not supported by the calander. </p>

<pre><code>The DateTime represented by the string is not supported in calendar 
</code></pre>

<p>Why do you need to format the string for the comparison?</p>
"
"55590026","Dynamic formatting of last modified filter in Data factory DataSet","<p>I'm trying to set the last modified filter in a azure data factory dataset dynamically. </p>

<p>I'm using the following expression:</p>

<p>@formatDateTime(adddays(utcnow(),-2),'yyyy-mm-ddThh:mm:ss.fffZ')</p>

<p><img src=""https://i.imgur.com/bUuFMMe.png"" alt=""Data factory formatting"" title=""Formatting as specified""></p>

<p>I'm getting the following error:</p>

<blockquote>
  <p>Activity Copy1 failed: Failure happened on 'Source' side. ErrorCode=UserErrorInvalidValueInPayload,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Failed to convert the value in &apos;modifiedDatetimeStart&apos; property to &apos;System.Nullable`1[[System.DateTime&#44; mscorlib&#44; Version&#61;4.0.0.0&#44; Culture&#61;neutral&#44; PublicKeyToken&#61;b77a5c561934e089]]&apos; type. Please make sure the payload structure and value are correct.,Source=Microsoft.DataTransfer.DataContracts,''Type=System.Reflection.TargetInvocationException,Message=Exception has been thrown by the target of an invocation.,Source=mscorlib,''Type=System.FormatException,Message=The DateTime represented by the string is not supported in calendar System.Globalization.GregorianCalendar.,Source=mscorlib,'</p>
</blockquote>

<p>I'm also not able to preview the data with this filter. I guess something is wrong here. Any ideas?</p>
","<azure><azure-data-factory>","2019-04-09 09:56:41","3916","1","5","55649474","<p>Perhaps, as a workaround, you could use this expression to get a rid of the extra characters in your datetime expression:</p>

<p><code>@substring(formatDateTime(adddays(utcnow(),-2), 'o'), 0, 23)</code></p>

<p>I tested this with utcnow() and it should return the datetime in the desired format:</p>

<p><code>""value"": ""2019-04-12T10:11:51.108Z""</code></p>
"
"55590026","Dynamic formatting of last modified filter in Data factory DataSet","<p>I'm trying to set the last modified filter in a azure data factory dataset dynamically. </p>

<p>I'm using the following expression:</p>

<p>@formatDateTime(adddays(utcnow(),-2),'yyyy-mm-ddThh:mm:ss.fffZ')</p>

<p><img src=""https://i.imgur.com/bUuFMMe.png"" alt=""Data factory formatting"" title=""Formatting as specified""></p>

<p>I'm getting the following error:</p>

<blockquote>
  <p>Activity Copy1 failed: Failure happened on 'Source' side. ErrorCode=UserErrorInvalidValueInPayload,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Failed to convert the value in &apos;modifiedDatetimeStart&apos; property to &apos;System.Nullable`1[[System.DateTime&#44; mscorlib&#44; Version&#61;4.0.0.0&#44; Culture&#61;neutral&#44; PublicKeyToken&#61;b77a5c561934e089]]&apos; type. Please make sure the payload structure and value are correct.,Source=Microsoft.DataTransfer.DataContracts,''Type=System.Reflection.TargetInvocationException,Message=Exception has been thrown by the target of an invocation.,Source=mscorlib,''Type=System.FormatException,Message=The DateTime represented by the string is not supported in calendar System.Globalization.GregorianCalendar.,Source=mscorlib,'</p>
</blockquote>

<p>I'm also not able to preview the data with this filter. I guess something is wrong here. Any ideas?</p>
","<azure><azure-data-factory>","2019-04-09 09:56:41","3916","1","5","55709398","<p>Actually the following commands are tested &amp; working after publish &amp; trigger:</p>
<p>@utcnow()</p>
<p>@adddays(utcnow(),-2)</p>
<p>It's the preview functionality in the front end that is not able to deal with the expressions. This will hopefully be solved by Microsoft.</p>
<p><img src=""https://i.imgur.com/oanuQzq.png"" alt=""screenshot"" title=""ADF"" /></p>
"
"55590026","Dynamic formatting of last modified filter in Data factory DataSet","<p>I'm trying to set the last modified filter in a azure data factory dataset dynamically. </p>

<p>I'm using the following expression:</p>

<p>@formatDateTime(adddays(utcnow(),-2),'yyyy-mm-ddThh:mm:ss.fffZ')</p>

<p><img src=""https://i.imgur.com/bUuFMMe.png"" alt=""Data factory formatting"" title=""Formatting as specified""></p>

<p>I'm getting the following error:</p>

<blockquote>
  <p>Activity Copy1 failed: Failure happened on 'Source' side. ErrorCode=UserErrorInvalidValueInPayload,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Failed to convert the value in &apos;modifiedDatetimeStart&apos; property to &apos;System.Nullable`1[[System.DateTime&#44; mscorlib&#44; Version&#61;4.0.0.0&#44; Culture&#61;neutral&#44; PublicKeyToken&#61;b77a5c561934e089]]&apos; type. Please make sure the payload structure and value are correct.,Source=Microsoft.DataTransfer.DataContracts,''Type=System.Reflection.TargetInvocationException,Message=Exception has been thrown by the target of an invocation.,Source=mscorlib,''Type=System.FormatException,Message=The DateTime represented by the string is not supported in calendar System.Globalization.GregorianCalendar.,Source=mscorlib,'</p>
</blockquote>

<p>I'm also not able to preview the data with this filter. I guess something is wrong here. Any ideas?</p>
","<azure><azure-data-factory>","2019-04-09 09:56:41","3916","1","5","56596420","<p>turns out you can solve the above prepending a conversion to string to your line above, so from</p>

<pre><code>@formatDateTime(adddays(utcnow(),-2),'yyyy-mm-ddThh:mm:ss.fffZ')
</code></pre>

<p>change it to</p>

<pre><code>@string(formatDateTime(adddays(utcnow(),-2),'yyyy-mm-ddThh:mm:ss.fffZ'))
</code></pre>

<p>it works on my end</p>
"
"55590026","Dynamic formatting of last modified filter in Data factory DataSet","<p>I'm trying to set the last modified filter in a azure data factory dataset dynamically. </p>

<p>I'm using the following expression:</p>

<p>@formatDateTime(adddays(utcnow(),-2),'yyyy-mm-ddThh:mm:ss.fffZ')</p>

<p><img src=""https://i.imgur.com/bUuFMMe.png"" alt=""Data factory formatting"" title=""Formatting as specified""></p>

<p>I'm getting the following error:</p>

<blockquote>
  <p>Activity Copy1 failed: Failure happened on 'Source' side. ErrorCode=UserErrorInvalidValueInPayload,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Failed to convert the value in &apos;modifiedDatetimeStart&apos; property to &apos;System.Nullable`1[[System.DateTime&#44; mscorlib&#44; Version&#61;4.0.0.0&#44; Culture&#61;neutral&#44; PublicKeyToken&#61;b77a5c561934e089]]&apos; type. Please make sure the payload structure and value are correct.,Source=Microsoft.DataTransfer.DataContracts,''Type=System.Reflection.TargetInvocationException,Message=Exception has been thrown by the target of an invocation.,Source=mscorlib,''Type=System.FormatException,Message=The DateTime represented by the string is not supported in calendar System.Globalization.GregorianCalendar.,Source=mscorlib,'</p>
</blockquote>

<p>I'm also not able to preview the data with this filter. I guess something is wrong here. Any ideas?</p>
","<azure><azure-data-factory>","2019-04-09 09:56:41","3916","1","5","64502504","<p>Encountered the same problem in data flow:</p>
<pre><code>currentUTC()
</code></pre>
<p>did not work for pulling the last modified file in blob storage but</p>
<pre><code>currentTimestamp()
</code></pre>
<p>did</p>
"
"55584515","Databricks/ADF python assistance","<p>Need help with executing python script from Azure databricks. 
Ask is to connect to databases using python script and read data from table and trigger email with the contents from table.</p>

<p>Here is my sample script. It works fine from my local, however i'm not sure how to make it work on Databricks or Data factory.</p>

<pre><code>import pyodbc
import settings
import sendgrid
import time
from sendgrid.helpers.mail import *

username = settings.username
password = settings.password
server = settings.server
database = settings.database
driver= '{SQL Server}'
connection_string = 'DRIVER={driver};PORT=1433;SERVER={server};DATABASE= 
{database};
UID={username};
PWD={password}'.format(driver=driver, server=server, 
database=database, username=username, password=password)
cnxn = pyodbc.connect(connection_string)

cursor= cnxn.cursor()
cursor.execute(""Select Job_status as Status, COUNT(*) AS 
count FROM demo_table group by Job_status"")
arr=[]
while 1:
row = cursor.fetchone()
if not row:
break
print(row.Status, row.count)
arr.append(row.Status+"" ""+str(row.count))
Status = arr[0] , arr[1]
cnxn.close()

sg = sendgrid.SendGridAPIClient(apikey='***********************')
from_email = Email(""********************"")
to_email = Email(""****************************"")
subject = ""Job Monitoring | Job Failures""
content = Content(""text/html"", value = 'Hi,Provided are the details of the 
jobs.' +str(Status[0])+''+str(Status[1]) +'Regards,Team')
mail = Mail(from_email, subject, to_email, content)
response = sg.client.mail.send.post(request_body=mail.get())""
</code></pre>

<p>Once the job is completed, It should ideally read data from the table and then trigger email with the job details.</p>

<p>Please help!</p>
","<python><azure><azure-data-factory><databricks><azure-databricks>","2019-04-09 03:05:00","940","3","2","55608125","<p>I see there are two third-party Python libraries required in your code, so first you need to install them in Azure Databricks, as below.</p>

<ol>
<li><p>Install the <code>sendgrid</code> package is relatively simple, as the figures below.</p>

<p>Fig 1.1. Click the <code>Launch Workspace</code> button in Azure portal and sign in.
<a href=""https://i.stack.imgur.com/adTnQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/adTnQ.png"" alt=""enter image description here""></a></p>

<p>Fig 1.2. Move to the tab <code>Clusters</code> and to <code>Create Cluster</code>, and then to click the <code>Libraries</code> link of your cluster
<a href=""https://i.stack.imgur.com/z7cif.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z7cif.png"" alt=""enter image description here""></a></p>

<p>Fig 1.3. Click the <code>Install New</code> button and select the <code>PyPI</code> library source to type the package name <code>sendgrid</code> to <code>Install</code>
<a href=""https://i.stack.imgur.com/oaBIC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oaBIC.png"" alt=""enter image description here""></a></p>

<p>Fig 1.4. Immediately the <code>sendgrid</code> package had installed
<a href=""https://i.stack.imgur.com/jMR6A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jMR6A.png"" alt=""enter image description here""></a></p></li>
<li><p>Follow the blog <a href=""https://datathirst.net/blog/2018/10/12/executing-sql-server-stored-procedures-on-databricks-pyspark"" rel=""nofollow noreferrer""><code>Executing SQL Server Stored Procedures from Databricks (PySpark)</code></a> to install <code>pyodbc</code> package with its required linux-packages, as below.</p>

<p>Fig 2.1. Create a new notebook <code>install_pyodbc</code> for next installation
<a href=""https://i.stack.imgur.com/d4a7b.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/d4a7b.png"" alt=""enter image description here""></a></p>

<p>Fig 2.2. To check the Linux distribution version and follow the blog to install these packages
<a href=""https://i.stack.imgur.com/tFWAQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tFWAQ.png"" alt=""enter image description here""></a></p>

<p>Fig 2.3. Try to connect and query the database via pyodbc, it works
<a href=""https://i.stack.imgur.com/Iw8XB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Iw8XB.png"" alt=""enter image description here""></a></p>

<pre><code>import pyodbc
connection_string = ""Driver={ODBC Driver 17 for SQL Server};Server=tcp:&lt;your db name&gt;.database.windows.net,1433;Database=&lt;db name&gt;;Uid=&lt;username&gt;@&lt;dbinstance name&gt;;Pwd=&lt;password&gt;;Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;""
cnxn = pyodbc.connect(connection_string)
cursor= cnxn.cursor()
cursor.execute(""select * from table"")
row = cursor.fetchone()
if row:
    print(row)
</code></pre></li>
</ol>

<p>Then, you can create a new notebook to run your code. And to integrate with Azure Data Factory, please refer to the offical document <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-databricks-python"" rel=""nofollow noreferrer""><code>Transform data by running a Python activity in Azure Databricks</code></a> to know how to do.</p>
"
"55584515","Databricks/ADF python assistance","<p>Need help with executing python script from Azure databricks. 
Ask is to connect to databases using python script and read data from table and trigger email with the contents from table.</p>

<p>Here is my sample script. It works fine from my local, however i'm not sure how to make it work on Databricks or Data factory.</p>

<pre><code>import pyodbc
import settings
import sendgrid
import time
from sendgrid.helpers.mail import *

username = settings.username
password = settings.password
server = settings.server
database = settings.database
driver= '{SQL Server}'
connection_string = 'DRIVER={driver};PORT=1433;SERVER={server};DATABASE= 
{database};
UID={username};
PWD={password}'.format(driver=driver, server=server, 
database=database, username=username, password=password)
cnxn = pyodbc.connect(connection_string)

cursor= cnxn.cursor()
cursor.execute(""Select Job_status as Status, COUNT(*) AS 
count FROM demo_table group by Job_status"")
arr=[]
while 1:
row = cursor.fetchone()
if not row:
break
print(row.Status, row.count)
arr.append(row.Status+"" ""+str(row.count))
Status = arr[0] , arr[1]
cnxn.close()

sg = sendgrid.SendGridAPIClient(apikey='***********************')
from_email = Email(""********************"")
to_email = Email(""****************************"")
subject = ""Job Monitoring | Job Failures""
content = Content(""text/html"", value = 'Hi,Provided are the details of the 
jobs.' +str(Status[0])+''+str(Status[1]) +'Regards,Team')
mail = Mail(from_email, subject, to_email, content)
response = sg.client.mail.send.post(request_body=mail.get())""
</code></pre>

<p>Once the job is completed, It should ideally read data from the table and then trigger email with the job details.</p>

<p>Please help!</p>
","<python><azure><azure-data-factory><databricks><azure-databricks>","2019-04-09 03:05:00","940","3","2","58300891","<p>I don't know if you  got this thing working, but it worked as a charm for me. One remark: use </p>

<p><code>sg = sendgrid.SendGridAPIClient('***********************')</code> </p>

<p>instead of </p>

<p><code>sg = sendgrid.SendGridAPIClient(**apikey=**'***********************')</code></p>

<p>I don't know if this is version dependent, but it was the only small issue I got.</p>
"
"55579778","Copy latest files from S3 to Azure Blob (using Azure Factory V2)","<p>I'm still new to Azure Data Factory and am trying to move files that are dumped in my S3 folder/bucket daily to Azure blob. I already created datasets (for source and sink) and linked services in Data Factory. </p>

<p>But since my S3 bucket receives new file every day, I'm wondering how to move <strong>the latest file that was dropped in the S3 (say at 5am EST) on a daily basis</strong>. I have looked through most of the answers online like <a href=""https://stackoverflow.com/questions/52989698/copy-data-from-azure-blob-storage-to-aws-s3"">this</a>, <a href=""https://stackoverflow.com/questions/52334901/azure-data-factory-copy-data-dynamically-get-last-blob"">this</a>, <a href=""https://stackoverflow.com/questions/53074516/incrementally-copy-s3-to-azure-blob"">this</a> and <a href=""https://stackoverflow.com/a/52340763/1330974"">this</a>. But none of them explains how to figure out which is the latest file in S3 (maybe <strong>based on last modified date/time or by matching the file name pattern that goes like this 'my_report_YYYYMMDD.csv.gz'</strong>) and only copy that file to the destination blob.</p>

<p>Thank you in advance for your help/answer!</p>
","<azure><azure-storage><azure-data-factory><azure-triggers>","2019-04-08 18:37:03","1635","3","2","55586848","<p>My idea as below:</p>

<p>1.Firstly,surely,configure your pipeline execution in the schedule trigger.Refer to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#schedule-trigger"" rel=""nofollow noreferrer"">link</a>.</p>

<p>2.Use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity#next-steps"" rel=""nofollow noreferrer"">Get metadata activity</a> ,which supports Amazon S3 Connector,to get the files in your S3 dataset.</p>

<p><a href=""https://i.stack.imgur.com/hHJIV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hHJIV.png"" alt=""enter image description here""></a></p>

<p>Get the last modified and file name etc. metadata.</p>

<p><a href=""https://i.stack.imgur.com/3xocw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3xocw.png"" alt=""enter image description here""></a></p>

<p>3.Put these metadata array which contains lastModified Time and file name into a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity#request-payload-schema"" rel=""nofollow noreferrer"">Web Activity</a> or <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure Function Activity</a>. In that rest api or function method,you could do a sort logical business to get the latest modified file.</p>

<p>4.Get the fileName from Web Activity or Azure Function Activity ,then copy it into Azure Blob Storage.</p>

<p>Another idea is using <a href=""https://learn.microsoft.com/zh-cn/azure/data-factory/v1/data-factory-use-custom-activities"" rel=""nofollow noreferrer"">Custom-Activity</a>.You could implement your requirements with .net code.</p>
"
"55579778","Copy latest files from S3 to Azure Blob (using Azure Factory V2)","<p>I'm still new to Azure Data Factory and am trying to move files that are dumped in my S3 folder/bucket daily to Azure blob. I already created datasets (for source and sink) and linked services in Data Factory. </p>

<p>But since my S3 bucket receives new file every day, I'm wondering how to move <strong>the latest file that was dropped in the S3 (say at 5am EST) on a daily basis</strong>. I have looked through most of the answers online like <a href=""https://stackoverflow.com/questions/52989698/copy-data-from-azure-blob-storage-to-aws-s3"">this</a>, <a href=""https://stackoverflow.com/questions/52334901/azure-data-factory-copy-data-dynamically-get-last-blob"">this</a>, <a href=""https://stackoverflow.com/questions/53074516/incrementally-copy-s3-to-azure-blob"">this</a> and <a href=""https://stackoverflow.com/a/52340763/1330974"">this</a>. But none of them explains how to figure out which is the latest file in S3 (maybe <strong>based on last modified date/time or by matching the file name pattern that goes like this 'my_report_YYYYMMDD.csv.gz'</strong>) and only copy that file to the destination blob.</p>

<p>Thank you in advance for your help/answer!</p>
","<azure><azure-storage><azure-data-factory><azure-triggers>","2019-04-08 18:37:03","1635","3","2","55818227","<p>(Side note: thanks to Jay Gong above for suggesting a solution)</p>
<p>I found the answer. It's simpler than I expected. There's <code>dynamic content/expression</code> that we can add to 'Filter by last modified' field of the S3 dataset. Please see the screenshot below where I show how I picked files that are no more than 5 hours old by using dynamic expression. More about these expressions can be read <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">here</a>.</p>
<p><a href=""https://i.stack.imgur.com/5Dpeh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5Dpeh.png"" alt=""enter image description here"" /></a></p>
<p>Hope this is helpful.</p>
"
"55577027","How can I efficiently prevent duplicated rows in my facts table?","<p>I have built a Data Factory pipeline which ETL the data from a Data Lake into a Datawarehouse. I chose the SCD type 1 for my dimensions. </p>

<p>My pipeline contains the following activities:</p>

<ol>
<li>[Stored Procedure] Clear staging tables;</li>
<li>[Stored Procedure] Get the timestamp of the last successful update;</li>
<li>[U-SQL] Extract the dimension data from the filtered files (the ones that have been modified since the last successful update) in Azure Data Lake, transform it and output it in a csv file;</li>
<li>[Copy Data] Load the csv into a SQL datawarehouse staging dimension table;</li>
<li>[Stored Procedure] Merge the data from the staging table into the production table;</li>
<li>[U-SQL] Extract the fact data from the files (the ones that have been modified since the last successful update) in Azure Data Lake, transform it and output it in a csv file;</li>
<li>[Copy Data] Load the csv into a SQL datawarehouse fact table;</li>
<li>[Stored Procedure] Update the timestamp of the successful update.</li>
</ol>

<p>The problem with this pipeline is that I end up with duplicated fact entries in my warehouse if the run the pipeline twice.</p>

<p><strong>Question</strong> </p>

<p>How can I efficiently prevent duplicated rows in my facts table, considering all <a href=""https://learn.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-tables-overview#unsupported-table-features"" rel=""nofollow noreferrer"">the unsupported features</a> in Azure SQL Data Warehouse?</p>

<p><strong>Update</strong></p>

<p>I have read another piece of information regarding the indexing (and the statistics) of a warehouse and how it must be rebuilt after an update.</p>

<p>Considering that, the simplest thing that I thought of was to apply the same principle to the facts as the one I am using for the Dimensions. I can load all the new facts in a staging table, but then use an index on the fact table to include only the facts that do not exist (the facts can't be updated right now).</p>
","<azure><azure-data-factory><azure-data-lake><azure-synapse>","2019-04-08 15:36:35","1215","3","1","55583780","<p>Do the lifting in Azure SQL Data Warehouse ... your performance will improve dramatically, and your problem will go away.</p>

<p>How many rows are in your filtered files? If it is in the millions to tens of millions, I think you can probably avoid the filter at the data lake stage. The performance of Polybase + SQL should overcome the additional data volume.</p>

<p>If you can avoid the filter, use this logic and throw away the U-SQL processing:</p>

<ul>
<li>Ingest files to staging table with suitable hash distribution</li>
<li>Take the latest version of each row (suitable for SCD1)</li>
<li>Merge stage to fact using a query like this:</li>
</ul>

<p>BK = Business Key column/s. COLn = non-key columns</p>

<pre><code>-- Get latest row for each business key to eliminate duplicates.

create table stage2 with (heap,distribution = hash(bk)) as
select  bk,
        col1,
        col2,
        row_number() over (partition by bk order by timestamp desc) rownum
from    stage
where   rownum = 1;

-- Merge the stage into a copy of the dimension

create table dimension_copy with (heap,distribution=replicate) as

select    s.bk,
          s.col1,
          s.col2
from      stage2 s
where     not exists (
              select  1
              from    schema.dimension d
              where   d.bk = s.bk)

union

select   d.bk,
         case when s.bk is null then d.col1 else s.col1 end,
         case when s.bk is null then d.col2 else s.col2 end
from     dimension d
         left outer join stage2 s on s.bk = d.bk;

-- Switch the merged copy with the original 

alter table dimension_copy switch to dimension with (truncate_target=on);

-- Force distribution of replicated table across nodes

select top 1 * from dimension;
</code></pre>
"
"55569437","Can i populate different SQL tables at once inside azure data factory when the source data set is Blob storage?","<p>I want to copy data from azure blob storage to azure sql database. The destination database is divided among different tables.</p>

<p>So is there any way in which i directly send the blob data to different sql tables using a single pipeline in one copy activity?</p>

<p>As this should be a trigger based pipeline so it is a continuous process, i created trigger for every hour but right now i can just send blob data to one table and then divide them into different table by invoking another pipeline where source and sink dataset both are SQL database.</p>

<p>Finding a solution for this</p>
","<azure><etl><azure-blob-storage><azure-data-factory><azure-sql-server>","2019-04-08 08:45:54","149","0","2","55593923","<p>You could use a stored procedure in your database as a sink in the copy activity. This way, you can define the logic in the stored procedure to write the data to your destination tables. You can find the description of the stored procedure sink <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#invoking-stored-procedure-for-sql-sink"" rel=""nofollow noreferrer"">here</a>.</p>

<p>You'll have to use a user defined table type for this solution, maintaining them can be difficult, if you run into issues, you can have a look at my &amp; BioEcoSS' answer in <a href=""https://stackoverflow.com/questions/11410722/altering-user-defined-table-types-in-sql-server/43608009#43608009"">this thread</a>.</p>
"
"55569437","Can i populate different SQL tables at once inside azure data factory when the source data set is Blob storage?","<p>I want to copy data from azure blob storage to azure sql database. The destination database is divided among different tables.</p>

<p>So is there any way in which i directly send the blob data to different sql tables using a single pipeline in one copy activity?</p>

<p>As this should be a trigger based pipeline so it is a continuous process, i created trigger for every hour but right now i can just send blob data to one table and then divide them into different table by invoking another pipeline where source and sink dataset both are SQL database.</p>

<p>Finding a solution for this</p>
","<azure><etl><azure-blob-storage><azure-data-factory><azure-sql-server>","2019-04-08 08:45:54","149","0","2","55684227","<p>According to my experience and Azure Data Factory doucmentation, we could not directly send the blob data to different sql tables using a single pipeline in one copy activity.</p>
<p>Because during Table mapping settings, One Copy Data Active only allows us select one corresponding table in the destination data store or specify the stored procedure to run at the destination.</p>
<p><a href=""https://i.stack.imgur.com/5jOjM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5jOjM.png"" alt=""enter image description here"" /></a></p>
<p>You don't need to create a new pipeline, just add a new copy data active, each copy active call different stored procedure.
<a href=""https://i.stack.imgur.com/IUVlK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IUVlK.png"" alt=""enter image description here"" /></a></p>
<p>Hope this helps.</p>
"
"55539415","How to copy data in Azure Data Factory depending on the values?","<p>my problem is this:
For example, I have a table with three columns in SQL Server table1(id, number1, number2) and other table2(id,finalNumber). How can I do a conditional copy? I want to copy the bigger number of each row in the table2(finalNumber). I thought a LookUp->IfConditionally but it doesn't work.</p>
","<azure><azure-data-factory>","2019-04-05 16:02:50","674","2","1","55569491","<p>From the example in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-if-condition-activity"" rel=""nofollow noreferrer"">IfCondition activity document</a>,it could be used to quote the output data from look up activities.Like <code>@{activity('MyLookupActivity').output.firstRow.count}</code>. You could refer to my previous case:<a href=""https://stackoverflow.com/questions/54934549/referencing-json-payload-value-in-azure-data-factory-for-if-condition/54939839#54939839"">Referencing JSON payload value in Azure Data Factory for If condition</a></p>

<p>In addition,one more idea is using stored procedure.You could this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#invoking-stored-procedure-for-sql-sink"" rel=""nofollow noreferrer"">link</a> to configure sql server stored procedure in your <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#sql-server-as-sink"" rel=""nofollow noreferrer"">sql server sink</a>.</p>

<p><a href=""https://i.stack.imgur.com/pwEwH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pwEwH.png"" alt=""enter image description here""></a></p>

<p>Then you could copy the data from source table to the temporary table.And do the number contrast business and insert operations in the stored procedure.There are very detailed steps i did in my previous case: <a href=""https://stackoverflow.com/questions/51352725/azure-data-factory-mapping-2-columns-in-one-column/51354266#51354266,please"">Azure Data Factory mapping 2 columns in one column</a> refer to it.</p>

<p>Any concern,let me know.</p>
"
"55538546","How do I copy data from REST API endpoint into Azure DB using Data Factory v2?","<p>I want to do a one time pull of everything in Wisconsin from this Web API Endpoint. <a href=""https://developers.google.com/civic-information/docs/v2/representatives/representativeInfoByDivision"" rel=""nofollow noreferrer"">https://developers.google.com/civic-information/docs/v2/representatives/representativeInfoByDivision</a></p>

<p>Then I want to load the data into an Azure DB that I created.</p>

<p>I've reviewed several articles and videos and I haven't made much progress. I currently have the following; an Azure subscription, a storage account, an Azure SQL Server and DB.  I've tried using the Copy Data activity in my Data Factory. For the source Base URL I'm using <a href=""https://www.googleapis.com/civicinfo/v2/representatives"" rel=""nofollow noreferrer"">https://www.googleapis.com/civicinfo/v2/representatives</a>. The ""Test Connection"" is successful.  For the next screen ""Data Set"" for the entry ""Relative URL"" I'm using the same thing and it's giving the following error. ""Relative Url Fail to read from REST resource""</p>

<p>I expect to be able to map the data from the Google API to columns in tables that I create in my Azure DB.</p>
","<rest><api><azure-data-factory>","2019-04-05 15:13:31","923","1","1","55572920","<p>Try <a href=""https://www.googleapis.com/civicinfo/v2/"" rel=""nofollow noreferrer"">https://www.googleapis.com/civicinfo/v2/</a> as base URL and representatives as relative URL. </p>

<p>And the end, relative URL will be concat with base URL. And  try rest connector of ADF. 
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-rest</a></p>
"
"55536422","Loading Data from Dynamics CRM to SQL Server","<p>Not able to get optionset string from Dynamics CRM to SQL Server using Azure data factory.</p>

<p>I am using Azure data factory to move data from Dynamics CRM to SQL DB. I used fetchXML query to get the data from source (CRM). I am able to get normal string and guid type values without any issue.</p>

<p>But the optionset field from CRM is coming as <code>Int32</code> type (ie, I am getting the value of optionset, not the string).</p>

<p>How can I fix this issue?</p>
","<dynamics-crm><azure-data-factory><fetchxml>","2019-04-05 13:23:33","1560","1","2","55543896","<p>Probably you are using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-dynamics-crm-office-365#dynamics-as-a-source-type"" rel=""nofollow noreferrer"">this approach</a> to get the fetchxml resultset as Dynamics source to dumping into SQL using Azure Data factory. The problem you are facing is unable to consume the formatted text value for that picklist option.</p>

<p>We will do consume the formatted value using below syntax in code: <a href=""https://mahadeomatre.blogspot.com/2016/11/use-fetchxml-to-retrieve-data-from-ms.html"" rel=""nofollow noreferrer"">Reference</a></p>

<pre><code>//Option set
var industrycodeValue = accountDetails['industrycode'];

var industrycodeTextValue = accountDetails['industrycode@OData.Community.Display.V1.FormattedValue'];
</code></pre>

<p>If you cannot do such thing, then it’s better to dump another table in your SQL called <code>stringmap</code> which will store all the picklist options across the system.</p>

<p>Then you can inner join both tables to get the necessary data.</p>

<pre><code>select INC.TicketNumber[Case ID],SMT.Value[Status Name], SMT.AttributeValue[Status Value]
from incident as INC inner join StringMap as SMT
on INC.StatusCode = SMT.AttributeValue
where SMT.AttributeName='statuscode' and SMT.ObjectTypeCode=112
</code></pre>

<p><a href=""https://community.dynamics.com/crm/f/117/t/205674"" rel=""nofollow noreferrer"">Read more</a></p>
"
"55536422","Loading Data from Dynamics CRM to SQL Server","<p>Not able to get optionset string from Dynamics CRM to SQL Server using Azure data factory.</p>

<p>I am using Azure data factory to move data from Dynamics CRM to SQL DB. I used fetchXML query to get the data from source (CRM). I am able to get normal string and guid type values without any issue.</p>

<p>But the optionset field from CRM is coming as <code>Int32</code> type (ie, I am getting the value of optionset, not the string).</p>

<p>How can I fix this issue?</p>
","<dynamics-crm><azure-data-factory><fetchxml>","2019-04-05 13:23:33","1560","1","2","64481355","<p>I'm going to have to sync the stringmaps entity too as I exceeded the link-entity limit for a given FetchXML query.</p>
<p>The following will bring in the textual value of an OptionSet selection.</p>
<pre><code>    &lt;link-entity name=&quot;stringmap&quot; from=&quot;attributevalue&quot; to=&quot;____&quot; visible=&quot;false&quot; link-type=&quot;outer&quot; alias=&quot;____&quot;&gt;
        &lt;filter type=&quot;and&quot;&gt;
            &lt;condition attribute=&quot;objecttypecode&quot; operator=&quot;eq&quot; value=&quot;___&quot;/&gt;
        &lt;/filter&gt;
        &lt;attribute name=&quot;value&quot;/&gt;
    &lt;/link-entity&gt;
</code></pre>
<p><strong>to</strong> should be the name of the OptionSet column on the host/root entity</p>
<p><strong>alias</strong> whatever you want to call the column in the output. I used the same value as <strong>to</strong></p>
<p><strong>value</strong> This is the object type code for your host/root entity. It is an integer value.</p>
"
"55533156","How activate Data Flow(Preview) in Azure Data Factory?","<p>I'm using Azure Data Factories but Data Flow (Preview) is not avaliable in my Data Factory. How could I activate this resource?</p>
","<azure-data-factory>","2019-04-05 10:14:14","114","2","1","55569813","<p>Please follow the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-create"" rel=""nofollow noreferrer"">Get-Start document</a> to create V2 data factory, the Data Flow Preview feature is supported by V2 version.</p>

<p><a href=""https://i.stack.imgur.com/DthbY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DthbY.png"" alt=""enter image description here""></a></p>

<p>Also,you could submit the infomation in this <a href=""https://forms.office.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR0bW_1HTuLVCg-zA7dpw8gFURFFCR04yQUpLTThXRktQV0VLREFWVTRBWi4u"" rel=""nofollow noreferrer"">page</a>.</p>

<p>Then you could create your Data Flow in the ADF UI.</p>

<p><a href=""https://i.stack.imgur.com/KiAM9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KiAM9.png"" alt=""enter image description here""></a></p>
"
"55532568","Azure Data Factory Pipelines: Creating pipelines with Python: Authentication (via az cli)","<p>I'm trying to create azure data factory pipelines via python, using the example provided by Microsoft here:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-python"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-python</a></p>

<pre><code>def main():

    # Azure subscription ID
    subscription_id = '&lt;Specify your Azure Subscription ID&gt;'

    # This program creates this resource group. If it's an existing resource group, comment out the code that creates the resource group
    rg_name = 'ADFTutorialResourceGroup'

    # The data factory name. It must be globally unique.
    df_name = '&lt;Specify a name for the data factory. It must be globally unique&gt;'

    # Specify your Active Directory client ID, client secret, and tenant ID
    credentials = ServicePrincipalCredentials(client_id='&lt;Active Directory application/client ID&gt;', secret='&lt;client secret&gt;', tenant='&lt;Active Directory tenant ID&gt;')
    resource_client = ResourceManagementClient(credentials, subscription_id)
    adf_client = DataFactoryManagementClient(credentials, subscription_id)

    rg_params = {'location':'eastus'}
    df_params = {'location':'eastus'}
</code></pre>

<p>However I cannot pass the credentials in as shown above since azure login is carried out as a separate step earlier in the pipeline, leaving me with an authenticated session to azure (no other credentials may be passed into this script).</p>

<p>Before I run the python code to create the pipeline, I do ""az login"" via a Jenkins deployment pipeline, which gets me an authenticated azurerm session. I should be able to re-use this session in the python script to get a data factory client, without authenticating again.</p>

<p>However, I'm unsure how to modify the client creation part of the code, as there do not seem to be any examples that make use of an already established azurerm session:</p>

<pre><code>    adf_client = DataFactoryManagementClient(credentials, subscription_id)

    rg_params = {'location':'eastus'}
    df_params = {'location':'eastus'}

 #Create a data factory
    df_resource = Factory(location='eastus')
    df = adf_client.factories.create_or_update(rg_name, df_name, df_resource)
    print_item(df)
    while df.provisioning_state != 'Succeeded':
        df = adf_client.factories.get(rg_name, df_name)
        time.sleep(1)

</code></pre>

<p>Microsofts authentication documentation suggests I can authenticate using a previously established session as follows:</p>

<pre><code>from azure.common.client_factory import get_client_from_cli_profile
from azure.mgmt.compute import ComputeManagementClient

client = get_client_from_cli_profile(ComputeManagementClient)
</code></pre>

<p>( ref: <a href=""https://learn.microsoft.com/en-us/python/azure/python-sdk-azure-authenticate?view=azure-python"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/python/azure/python-sdk-azure-authenticate?view=azure-python</a> )</p>

<p>This works, however azure data factory object instantiation fails with:</p>

<pre><code>Traceback (most recent call last):
  File ""post-scripts/check-data-factory.py"", line 72, in &lt;module&gt;
    main()
  File ""post-scripts/check-data-factory.py"", line 65, in main
    df = adf_client.factories.create_or_update(rg_name, data_factory_name, df_resource)

AttributeError: 'ComputeManagementClient' object has no attribute 'factories'

</code></pre>

<p>So perhaps some extra steps are required between this and getting a df object?</p>

<p>Any clue appreciated!</p>
","<python><azure><azure-data-factory>","2019-04-05 09:45:07","2098","1","1","55544134","<p>Just replace the class with the correct type:</p>

<pre><code>from azure.common.client_factory import get_client_from_cli_profile
from azure.mgmt.resource import ResourceManagementClient
from azure.mgmt.datafactory import DataFactoryManagementClient

resource_client = get_client_from_cli_profile(ResourceManagementClient)
adf_client = get_client_from_cli_profile(DataFactoryManagementClient)
</code></pre>

<p>The error you got is because you created a Compute client (to handle VM), not a ADF client. But yes, you found the right doc for your needs :)</p>

<p>(disclosure: I work at MS in the Python SDK team)</p>
"
"55519128","Does the linked service support dynamic json in azure data factory?","<p>Currently, I'm trying to set up a dynamic key vault linked service.
Unfortunately, whatever I try I'm not able to successfully test the connection.</p>

<pre><code>{
    ""name"": ""AzureKeyVault1"",
    ""properties"": {
        ""type"": ""AzureKeyVault"",
        ""typeProperties"": {
            ""baseUrl"": {
                ""value"": ""@concat('https://vault.azure.net','/')"",
                ""type"": ""Expression""
            }
        }
    }
}
</code></pre>

<p>The above code using concat is not a real use case but just a way to test if dynamic json is possible for linked service.
I was expecting (based on the documentation) that I could make the baseUrl property dynamic. Am I using the right formatting?</p>

<p>I get the following error:</p>

<p>Error: Error: Can't get property concat</p>
","<azure><azure-data-factory>","2019-04-04 14:58:14","580","2","2","55531191","<p>Wim,based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services"" rel=""nofollow noreferrer"">official document</a>,parameterize linked services only supports below services,not including Azure Key Vault.</p>

<p><a href=""https://i.stack.imgur.com/ttuFM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ttuFM.png"" alt=""enter image description here""></a></p>

<p>You could submit feedback <a href=""https://feedback.azure.com/forums/270578-data-factory"" rel=""nofollow noreferrer"">here</a> to push some improvements of azure data factory you want.</p>
"
"55519128","Does the linked service support dynamic json in azure data factory?","<p>Currently, I'm trying to set up a dynamic key vault linked service.
Unfortunately, whatever I try I'm not able to successfully test the connection.</p>

<pre><code>{
    ""name"": ""AzureKeyVault1"",
    ""properties"": {
        ""type"": ""AzureKeyVault"",
        ""typeProperties"": {
            ""baseUrl"": {
                ""value"": ""@concat('https://vault.azure.net','/')"",
                ""type"": ""Expression""
            }
        }
    }
}
</code></pre>

<p>The above code using concat is not a real use case but just a way to test if dynamic json is possible for linked service.
I was expecting (based on the documentation) that I could make the baseUrl property dynamic. Am I using the right formatting?</p>

<p>I get the following error:</p>

<p>Error: Error: Can't get property concat</p>
","<azure><azure-data-factory>","2019-04-04 14:58:14","580","2","2","56374402","<p>@Wim</p>

<p>To Answer your question, this is the correct formatting.</p>

<p>I have been able to parameterize a pipeline parameter, and then pass it as the baseurl in a dynamic expression. </p>
"
"55509486","Azure Data Factory ADFV2 Trigger Overlap","<p>I have a ADFV2 trigger that runs every 2 minutes. The pipeline that is called usually takes just over a minute to run but sometimes it takes over 2 minutes but if that happens the trigger kicks in again and runs regardless of the previous trigger still running or not. Is there any way to stop this overlap?
The trigger needs to run every 2 minutes.
Thanks.</p>
","<triggers><azure-data-factory>","2019-04-04 06:49:34","612","1","1","64216355","<p>There is a concurrency setting in the pipeline definition. Set it to 1.  The trigger will create an event, but it will be set to Queued state until previous job completes</p>
"
"55508948","passing pipeline parameter in column mapping","<p>i am trying to configure a default value as my sourceID column.
my source is a FILe and my sink is a db table.
hence, i cant define the sourceID in sql query.</p>

<p>i created a parameter and defined the value.
intent to pass the parameter in the column mapping's dynamic content string.
however, i couldnt get the parameter value loaded into my table.</p>

<p>please advice if i made mistake in the dynamic content definition or there is better way to achieve the desired outcome.</p>

<pre><code>    {
""type"":""TabularTranslator"",
""columnMappings"":{
    ""AS_OF_DATE"":""AS_OF_DATE"",
    ""SECTOR_ID"":""SECTOR_ID"",
    ""ASSET_GROUP"":""ASSET_GROUP"",
    ""REGION"":""REGION"",
    ""COUNTRY"":""COUNTRY"",    
    ""FIELD"":""FIELD"",
    ""SECTOR_FLAG"":""SECTOR_FLAG"",    
    ""PRODUCT"":""PRODUCT"",
    ""PERIODICITY"":""PERIODICITY"",
    ""UNIT_CODE"":""UNIT_CODE"",
    ""OPERATORSHIP"":""OPERATORSHIP"",
    ""OPRNAME"":""OPRNAME"",
    ""ACTUAL"":""ACTUAL"",
    ""PLAN"":""PLAN"",
    ""SOURCE_ID"":""@{pipeline().parameters.sourceIDmpm}""


    }
}
</code></pre>
","<azure><azure-data-factory>","2019-04-04 06:07:56","795","0","1","55704557","<p>To dynamically map column in copy activity, please define a parameter with type 'Object' in pipeline:</p>

<pre><code>""parameters"": {
""columnMapping"": {
""type"": ""Object""
}
 }
</code></pre>

<p>and reference this parameter in copy activity like:</p>

<pre><code>""translator"": {
""value"": ""@pipeline().parameters.columnMapping"",
""type"": ""Expression""
}
</code></pre>

<p>Please notice that you need to pass a JSON value to parameter. For example, I pass to the parameter </p>

<pre><code>'columnMapping'.
{
""type"": ""TabularTranslator"",
""columnMappings"": {
""Prop_0"": ""Prop_0"",
""Prop_1"": ""Prop_1"", ""Prop_2"": ""Prop_2""
}
}
</code></pre>

<p>For more details, you may refer the <a href=""https://social.msdn.microsoft.com/Forums/azure/en-US/3e87a117-b3e8-4554-b3af-9434a15e9c66/how-to-do-a-dynamic-column-mapping-in-copy-activity?forum=AzureDataFactory"" rel=""nofollow noreferrer"">MSDN</a> thread which addressing similar issue.</p>

<p>Hope this helps.</p>
"
"55508272","How can I sink the output of an Azure Data Factory activity to a SQL database?","<p>In my pipeline, I have a Lookup activity, which outputs it's data to an Azure Function activity.</p>

<p>I'd like to save the result of the Azure Function activity to a SQL Database.</p>

<p>The only activity I can see which writes to a SQL Database (or other Dataset) is the Copy Data activity, but that doesn't seem like it can take the Azure Function activity's output as it's input.</p>

<p>My ""ideal solution"" would by to have the inverse of the Lookup activity that takes an input from some other activity and sinks it to a dataset.</p>

<p>It seems a little weird to me that there is a generic Lookup activity to read data from a dataset, but no generic ""Store"" activity to write data to a dataset.</p>
","<azure><azure-data-factory>","2019-04-04 05:09:08","1007","3","1","55508808","<p>Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">supported connectors of copy activity</a>,azure function connector is not supported.You could adopt a workaround that using Azure Blob Storage as temporary space.</p>

<p>Process the data in azure function and output it to <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob"" rel=""nofollow noreferrer"">Azure Blob Storage</a>.</p>

<p>Configure <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage"" rel=""nofollow noreferrer"">Azure Blob Storage</a> as source and SQL Database as sink in copy activity.</p>
"
"55500943","ADF eroor while trying to get the output messages","<p>While executing i got the below error.</p>

<pre><code>Error
{
    ""errorCode"": ""BadRequest"",
    ""message"": ""Activity parent-control-sp failed: JSON text is not properly formatted. Unexpected character '\""' is found at position 0."",
    ""failureType"": ""UserError"",
    ""target"": ""Execute parent Pipeline""
}
</code></pre>

<p>I want the below message to be displayed in error table</p>

<blockquote>
  <p>Activity parent-control-sp failed: JSON text is not properly
  formatted. Unexpected character '\""' is found at position 0."",
  ""failureType"": ""UserError"", ""target"": ""Execute parent Pipeline"" }</p>
</blockquote>

<p>I have used <code>@{activity('Activity parent-control-sp').error.message}</code>. It is displaying the below message</p>

<blockquote>
  <p>JSON text is not properly formatted. Unexpected character '\""' is
  found at position 0."", ""failureType"": ""UserError"", ""target"": ""Execute
  parent Pipeline"" }</p>
</blockquote>

<p>How to get the upper message?</p>
","<error-handling><azure-data-factory>","2019-04-03 17:27:09","494","0","2","55838510","<p>I believe you are trying to use the Execute Pipeline activity and at this point the intend to to catch any error from the child activity . I think the below expression should help you</p>

<p>@concat(activity('Pipeline1').error.message,'failureType:',activity('Pipeline1').error.failureType)</p>
"
"55500943","ADF eroor while trying to get the output messages","<p>While executing i got the below error.</p>

<pre><code>Error
{
    ""errorCode"": ""BadRequest"",
    ""message"": ""Activity parent-control-sp failed: JSON text is not properly formatted. Unexpected character '\""' is found at position 0."",
    ""failureType"": ""UserError"",
    ""target"": ""Execute parent Pipeline""
}
</code></pre>

<p>I want the below message to be displayed in error table</p>

<blockquote>
  <p>Activity parent-control-sp failed: JSON text is not properly
  formatted. Unexpected character '\""' is found at position 0."",
  ""failureType"": ""UserError"", ""target"": ""Execute parent Pipeline"" }</p>
</blockquote>

<p>I have used <code>@{activity('Activity parent-control-sp').error.message}</code>. It is displaying the below message</p>

<blockquote>
  <p>JSON text is not properly formatted. Unexpected character '\""' is
  found at position 0."", ""failureType"": ""UserError"", ""target"": ""Execute
  parent Pipeline"" }</p>
</blockquote>

<p>How to get the upper message?</p>
","<error-handling><azure-data-factory>","2019-04-03 17:27:09","494","0","2","69156715","<p>Your error message is not a JSON message, so you cannot use the expression (<code>@{activity('Activity parent-control-sp').error.message}</code>) to get what you want</p>
<pre class=""lang-json prettyprint-override""><code>Error  &lt;&lt;== Remove this firstly!!!
{
    &quot;errorCode&quot;: &quot;BadRequest&quot;,
    &quot;message&quot;: &quot;Activity parent-control-sp failed: JSON text is not properly formatted. Unexpected character '\&quot;' is found at position 0.&quot;,
    &quot;failureType&quot;: &quot;UserError&quot;,
    &quot;target&quot;: &quot;Execute parent Pipeline&quot;
}
</code></pre>
<p>This code will get the substring of an Error message, and then you can use an expression.</p>
<pre class=""lang-sh prettyprint-override""><code># the substring
@json(substring(
activity('xxx').error.message,
indexof(activity('xxx').error.message,'{'),
add(sub(lastindexof(activity('xxx').error.message,'}'),indexof(activity('xxx').error.message,'{')),1)
))
# get what you want
.message
</code></pre>
"
"55500650","Self Hosted Integration Runtime is unavailable while creating a LinkedSerive In AzureDataFactory pipeline","<p><a href=""https://i.stack.imgur.com/Rn3Mm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Rn3Mm.png"" alt=""enter image description here""></a> I wanted to load the data from my on-premises  sql db to ADLS using ADF pipeline.In order to perform this activity I need  self hosted Integration Runtime.I downloaded and configured the Integration Runtime in my local machine which is running and connected to cloud service.The problem I'm facing is the integration runtime that is running is not available while creating the linked service</p>
","<c#><azure-data-factory><custom-activity>","2019-04-03 17:07:53","3903","1","1","55511799","<p>Before creating Linked Service; from “<strong>Connections</strong>” tab, I would request you to check the <strong>status</strong> of self-hosted integration run time which you have created. </p>

<p><a href=""https://i.stack.imgur.com/mdiBR.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mdiBR.jpg"" alt=""enter image description here""></a></p>

<p>If the status of self-hosted integration run time is <strong>running</strong>, then you can connect via integration runtime.</p>

<p><a href=""https://i.stack.imgur.com/wdkoc.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wdkoc.jpg"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"55495532","Pipelines in data factory are loading partial data","<p>I have created some pipelines in data factory and the pipelines are reading files from the source, but they are writing partial data and not all data. I am not able to identify the root cause.</p>

<p>I am using ADF V2. I extracted all the .gz files and the total row count was 64448 and there are total 15 files. I have then checked the destination and the pipeline has written around half the records ie 31210.</p>

<p>I also tried loading just 1 file, of 4500 records and it wrote 2100 records.</p>
","<azure><azure-sql-database><azure-storage><azure-data-factory>","2019-04-03 12:44:11","346","0","1","57609580","<p>You can use identity columns as the surrogate key to transfer data instead of timestamp column. </p>
"
"55475428","Splitting a Data Factor Pipeline based on the presence of a column","<p>I'm creating a pipeline to ingest a series of csvs into an Azure SQL Database</p>

<p>The CSVs come from a piece of medical software named <strong>SystmOne</strong>, the CSVs are either a <em>full</em> dataset or a <em>delta</em> dataset. The only difference in their schemas is the presence of an additional column: <code>RemovedData</code>.</p>

<p>The presence of this column will require an additional step in the pipeline (deleting any row from the database with <code>RemovedData == true</code>).</p>

<p>Is there a way in ADF or (ADF with Data Flow Preview) to query the file for the presence of a column and split the pipeline based on the result?</p>

<p>I have no control over the intial output of the file.</p>
","<azure-data-factory>","2019-04-02 12:59:09","219","0","1","55477117","<p>You can check the number of columns in your source dataset, with <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity#syntax"" rel=""nofollow noreferrer"">getMetadataActivty</a> columnCount property, and then with If Activity do what you want. </p>

<p><a href=""https://i.stack.imgur.com/kZvff.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kZvff.png"" alt=""GetMetadataActivty""></a></p>

<p>Expression in IF activity: <code>@equals(activity('YourGet Metadata').output.columnCount,numberOfColumns)</code></p>

<p>And then based on true or false you choose your dataset with propper schema.</p>
"
"55466799","Azure data factory logical function expression invalid","<p>I've got an ADF pipeline which is supposed to ETL job. I've added IF CONDITION which should evaluate the last result(of <strong>Copy Data</strong> Activity) obtained(If the output is <strong>Succeeded</strong> then true activity should be executed.). While writing the expression
I'm following the syntax but its failing in debugging.
What am I missing in the expression?</p>

<p>Further, I'll be evaluating output from <strong>Wait</strong> activity also since the output of Wait activity is <em>{}</em> how to evaluate the activity in the same way I'm evaluating <strong>Copy Data</strong> Activity?</p>

<p><a href=""https://i.stack.imgur.com/zOp9g.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zOp9g.jpg"" alt=""Expression""></a></p>

<p><a href=""https://i.stack.imgur.com/jxUNV.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jxUNV.jpg"" alt=""Debug Output""></a></p>

<p><a href=""https://i.stack.imgur.com/VlrZq.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VlrZq.jpg"" alt=""Output of FS1 to Blob""></a></p>
","<azure><azure-data-factory>","2019-04-02 04:04:20","3836","1","2","55488739","<p>I was having almost the same issue.
Please refer to the below post.</p>

<p><a href=""https://stackoverflow.com/questions/52612382/azure-data-factory-select-property-status-succeeded-from-previous-activity"">Solution</a></p>
"
"55466799","Azure data factory logical function expression invalid","<p>I've got an ADF pipeline which is supposed to ETL job. I've added IF CONDITION which should evaluate the last result(of <strong>Copy Data</strong> Activity) obtained(If the output is <strong>Succeeded</strong> then true activity should be executed.). While writing the expression
I'm following the syntax but its failing in debugging.
What am I missing in the expression?</p>

<p>Further, I'll be evaluating output from <strong>Wait</strong> activity also since the output of Wait activity is <em>{}</em> how to evaluate the activity in the same way I'm evaluating <strong>Copy Data</strong> Activity?</p>

<p><a href=""https://i.stack.imgur.com/zOp9g.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zOp9g.jpg"" alt=""Expression""></a></p>

<p><a href=""https://i.stack.imgur.com/jxUNV.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jxUNV.jpg"" alt=""Debug Output""></a></p>

<p><a href=""https://i.stack.imgur.com/VlrZq.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VlrZq.jpg"" alt=""Output of FS1 to Blob""></a></p>
","<azure><azure-data-factory>","2019-04-02 04:04:20","3836","1","2","62196565","<p>Not sure if you are still having a problem with this, but it appears that you have one too many parentheses.</p>

<pre><code>@equals(activity('FS1 to Blob').output.status), 'Success')
                                             ^
</code></pre>

<p>So it would appear that <code>equals()</code> only has the activity output as input.
Get rid of that and try it then!</p>
"
"55466448","Trigger next step in data factory after Tabular model refresh is complete","<p>I have setup an Azure automation account and Web hook to process my analysis services database. I am calling that using Web activity (POST method) in Azure Data Factory to trigger the cube refresh. Web activity method is working fine but it returns back without waiting for refresh to complete.</p>

<p>Now, I want to execute further steps only after the Cube processing is complete. Is there a way to detect when the cube is refreshed and then start the next step of activities in data factory?</p>
","<webhooks><azure-data-factory><azure-automation><azure-runbook>","2019-04-02 03:12:27","1711","3","1","55606184","<p>After lot of research, I was able to achieve this by using Microsoft recommended REST API's to process my Analysis Services Database and Get the Refresh status back.</p>

<p>Here are some helpful links below:</p>

<p>REST API: <a href=""https://learn.microsoft.com/en-us/azure/analysis-services/analysis-services-async-refresh"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/analysis-services/analysis-services-async-refresh</a></p>

<p>GitHub References: </p>

<ol>
<li><p>This link contains the documentation specifying the inputs and steps to Process Cube and Wait till the refresh is completed - <a href=""https://github.com/furmangg/automating-azure-analysis-services/blob/master/README.md#processazureas"" rel=""nofollow noreferrer"">https://github.com/furmangg/automating-azure-analysis-services/blob/master/README.md#processazureas</a></p></li>
<li><p>Code Repository: <a href=""https://github.com/furmangg/automating-azure-analysis-services/tree/master/ADFv2"" rel=""nofollow noreferrer"">https://github.com/furmangg/automating-azure-analysis-services/tree/master/ADFv2</a></p></li>
</ol>

<p>Unlike other solutions which leverage external services like Azure Logic Apps or custom ADF .NET activities running in Azure Batch, this approach uses only built-in activities which depend on no external services other than Azure Analysis Services. So, I changed my solution to NOT use Azure Automation account or Webhook to process the cube.</p>

<p>Feel free to get in touch if you need further details.</p>

<p>Hope this helps!!</p>
"
"55464883","Azure blob copy partition by date","<p>I have files in an <strong>Azure blob storage</strong> like this:</p>

<pre><code>entity
|- part001.parquet
|- part002.parquet
|- part003.parquet
|- part004.parquet
...
</code></pre>

<p>and this parquet data got a date column, let's name it <code>transaction_date</code></p>

<p>I want to make an Azure Data Factory pipeline to migrate all this data into another blob storage, like this:</p>

<pre><code>entity
|- YYYY
    |-MM
        |- entity_YYYYMMDD.parquet
           ...
|- YYYY
    |-MM
        |- entity_YYYYMMDD.parquet
           ...
</code></pre>

<p>So the files will contain only specific date transactions, based on <code>transaction_date</code>, to be easier to select them after.</p>

<p>Any way to do this using ADF or another Azure Stack tool?</p>
","<azure><parquet><azure-data-factory><azure-blob-storage>","2019-04-01 23:23:23","1149","2","1","55476615","<p>What you are after is dynamic partition or partition based on a field/column value.</p>

<p>We use Azure Databricks to handle such things and if need to be recurring then schedule the notebook via azure data factory v2. In the notebook you can have a pyspark script as follows (please note this code is just a pattern it is not tested)</p>

<pre><code>extractData = spark.read.parquet(""&lt;&lt;&lt;input blob storage path&gt;&gt;&gt;"")

extractData = extractData\
              .withColumn(""transaction_year"", year(""transaction_date""))\
              .withColumn(""transaction_month"", month(""transaction_date""))

extractData.write.mode(""overwrite"") \
    .partitionBy(""transaction_year"", ""transaction_month"") \
    .parquet(""&lt;&lt;&lt;output blob storage path&gt;&gt;&gt;"") 
</code></pre>

<p>Can we do using just azure data factory? Assuming you are using Azure Data Factory v2 - its hard (not impossible) to do partition based on a field value, compared to above.</p>

<p>Having said that there is public preview of Azure Data Factory Mapping Data Flow - under the covers it uses Azure Databricks for compute. I haven't tested/or played may be you could use a Transformation activity like <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-conditional-split"" rel=""nofollow noreferrer"">conditional split</a>. But again using Databricks is just simple.</p>
"
"55455863","How to pass Tumbling Window parameters to a Data Factory pipeline in the Data Factory UI?","<p>I have defined a pipeline in <code>Azure Data Factory</code> with a Tumbling Window trigger, as seen below:</p>

<p><a href=""https://i.stack.imgur.com/LDuIE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LDuIE.png"" alt=""Pipeline with the trigger""></a></p>

<p>I would like for my activities to receive the Tumbling window parameters (<code>trigger().outputs.windowStartTime</code> and <code>trigger().outputs.windowEndTime</code>) however I did not find any examples in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger"" rel=""nofollow noreferrer"">the documentation</a> showing how to do this in the UI.</p>

<p><strong>Question</strong></p>

<p>How can I pass the Tumbling Window parameters to a Data Factory pipeline in the Data Factory UI?</p>
","<azure><azure-data-factory><azure-databricks>","2019-04-01 13:06:15","3904","4","3","55459955","<blockquote>
<p>This answer is out of date. Parameters can be added directly in the UI - see my answer above.</p>
</blockquote>
<p><strong>Note:</strong> You cannot pass the Tumbling Windows parameters to a Data Factory pipeline in the ADF UI.</p>
<p>You need to pass the tumbling window parameters by following steps:</p>
<p>First create a Tumbling window trigger as per your requirement.</p>
<p>On the bottom left corner, you will find the &quot;<strong>Triggers</strong>&quot; tab =&gt; Click on <strong>Triggers</strong> and select the created trigger and click on &quot;<strong>Code</strong>&quot; and replace the parameters.</p>
<p><a href=""https://i.stack.imgur.com/rT5nH.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rT5nH.jpg"" alt=""enter image description here"" /></a></p>
<p>To use the WindowStart and WindowEnd system variable values in the pipeline definition, use your &quot;MyWindowStart&quot; and &quot;MyWindowEnd&quot; parameters, accordingly.</p>
<p><a href=""https://i.stack.imgur.com/ZTLvX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZTLvX.jpg"" alt=""enter image description here"" /></a></p>
<p>For more details, refer &quot;<a href=""https://social.msdn.microsoft.com/Forums/en-US/80fc8d7a-2c29-4a80-8e82-86b2913fc42c/tumbling-window-endtime-parameter-adfv2?forum=AzureDataFactory"" rel=""nofollow noreferrer"">MSDN</a>&quot; thread, which addressing similar issue.</p>
<p>Hope this helps.</p>
"
"55455863","How to pass Tumbling Window parameters to a Data Factory pipeline in the Data Factory UI?","<p>I have defined a pipeline in <code>Azure Data Factory</code> with a Tumbling Window trigger, as seen below:</p>

<p><a href=""https://i.stack.imgur.com/LDuIE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LDuIE.png"" alt=""Pipeline with the trigger""></a></p>

<p>I would like for my activities to receive the Tumbling window parameters (<code>trigger().outputs.windowStartTime</code> and <code>trigger().outputs.windowEndTime</code>) however I did not find any examples in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger"" rel=""nofollow noreferrer"">the documentation</a> showing how to do this in the UI.</p>

<p><strong>Question</strong></p>

<p>How can I pass the Tumbling Window parameters to a Data Factory pipeline in the Data Factory UI?</p>
","<azure><azure-data-factory><azure-databricks>","2019-04-01 13:06:15","3904","4","3","61359747","<p>Assuming the pipeline that you're triggering is already paramaterised then you're nearly there.</p>

<p>When adding the trigger you'll see a second screen to pass parameters from the trigger. </p>

<p><a href=""https://i.stack.imgur.com/lB935.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lB935.png"" alt=""enter image description here""></a></p>

<p>You can then add your functions prefixed with @. So:</p>

<pre><code>@trigger().outputs.windowStartTime
@trigger().outputs.windowEndTime
</code></pre>

<p>If you need to call a function on the parameter before you pass it you can do that too</p>

<pre><code>@addHours(trigger().outputs.windowEndTime,1)
</code></pre>
"
"55455863","How to pass Tumbling Window parameters to a Data Factory pipeline in the Data Factory UI?","<p>I have defined a pipeline in <code>Azure Data Factory</code> with a Tumbling Window trigger, as seen below:</p>

<p><a href=""https://i.stack.imgur.com/LDuIE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LDuIE.png"" alt=""Pipeline with the trigger""></a></p>

<p>I would like for my activities to receive the Tumbling window parameters (<code>trigger().outputs.windowStartTime</code> and <code>trigger().outputs.windowEndTime</code>) however I did not find any examples in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger"" rel=""nofollow noreferrer"">the documentation</a> showing how to do this in the UI.</p>

<p><strong>Question</strong></p>

<p>How can I pass the Tumbling Window parameters to a Data Factory pipeline in the Data Factory UI?</p>
","<azure><azure-data-factory><azure-databricks>","2019-04-01 13:06:15","3904","4","3","66829432","<p>This is because you set <code>@trigger().outputs.windowStartTime</code> and <code>@trigger().outputs.windowEndTime</code> in the variable. In fact, you should set them in the parameter, like this:</p>
<p><a href=""https://i.stack.imgur.com/lpdW1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lpdW1.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/QmIl3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QmIl3.png"" alt=""![enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/exRGD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/exRGD.png"" alt=""enter image description here"" /></a></p>
<p>Please let me know if still facing issue.</p>
"
"55455399","Performance tuning for on premises postgreSql to blob","<p>I am trying to move data from On premises Postgres to blob incrementally, but the data is moving very slow, are there any performance tuning steps to be followed?</p>
","<azure><azure-data-factory>","2019-04-01 12:41:52","102","0","1","55467155","<p>Welcome to Stackoverflow..!</p>

<p>I would suggest that you take these steps to tune the performance of your Data Factory service with Copy Activity:</p>

<p><strong>Establish a baseline:</strong> During the development phase, test your pipeline by using Copy Activity against a representative data sample. Collect execution details and performance characteristics following Copy activity monitoring.</p>

<p><strong>Diagnose and optimize performance:</strong> If the performance you observe doesn't meet your expectations, you need to identify performance bottlenecks. Then, optimize performance to remove or reduce the effect of bottlenecks.</p>

<p><strong>Expand the configuration to your entire data set:</strong> When you're satisfied with the execution results and performance, you can expand the definition and pipeline to cover your entire data set.</p>

<p>For more details, refer  <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance#performance-tuning-steps"" rel=""nofollow noreferrer"">Performance tuning steps</a> to learn about key factors that impact performance of data movement (Copy Activity) in Azure Data Factory and various ways to optimize it.</p>

<p>Hope this helps.</p>
"
"55434830","Azure Data Factory - Calling a REST call per row of imported data","<p>I would like to use Azure Data Factory to do the following...</p>

<p>Read in a CSV file, do a Google Maps Geocode lookup based on addresses in that file, and dump the results in Azure SQL.  I've been able to read from CSV and dump into Azure SQL, it's calling the rest service that I'm not sure how to do.  </p>

<p>Can anyone give me guidance on the best way to do this?  Thank you!</p>
","<azure><azure-data-factory>","2019-03-30 19:09:08","1106","1","1","55449877","<p>1.Use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">LookUp Activity</a> to read the content of csv file.</p>

<pre><code>@{activity('MyLookupActivity').output
</code></pre>

<p>2.Use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">ForEach Activity</a> to loop the output of LookUp Activity.</p>

<p><a href=""https://i.stack.imgur.com/M9Qjd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M9Qjd.png"" alt=""enter image description here""></a></p>

<p>3.Inside ForEach Activity,use copy activity. The source dataset please set <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest#dataset-properties"" rel=""nofollow noreferrer"">REST Dataset</a> to call geocode rest api. Configure the per row as body content.</p>

<pre><code>@item()
</code></pre>

<p><a href=""https://i.stack.imgur.com/EMXSV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EMXSV.png"" alt=""enter image description here""></a></p>

<p>4.Configure the azure sql database as sink dataset.</p>
"
"55417277","Web Activity Issue In Azure Data Factory","<p>Getiing Below Error While Executing my pipeline.</p>

<p>Error</p>

<p>{
    ""errorCode"": ""2108"",
    ""message"": ""Error calling the endpoint. Response status code: "",
    ""failureType"": ""UserError"",
    ""target"": ""Web1""
}</p>

<p>Here is My Code:</p>

<p>{
    ""name"": ""pipeline1"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""Web1"",
                ""type"": ""WebActivity"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""typeProperties"": {
                    ""url"": ""<a href=""http://00.00.00.00:8000/name?env=DEV"" rel=""nofollow noreferrer"">http://00.00.00.00:8000/name?env=DEV</a>"",
                    ""method"": ""GET""
                }
            }
        ]
    }
}</p>
","<azure><azure-data-factory>","2019-03-29 12:16:26","4414","0","1","55471211","<p>Make sure you have entered valid “url–Target endpoint and path”.</p>

<p>I have tried with valid url and the Job was successful.</p>

<p><strong>JSON Code for the web activity:</strong></p>

<pre><code>{
    ""name"": ""pipeline1"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""Webactivity"",
                ""type"": ""WebActivity"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""typeProperties"": {
                    ""url"": ""https://www.microsoft.com/"",
                    ""method"": ""GET"",
                    ""body"": """"
                }
            }
        ]
    }
}
</code></pre>

<p>Web Activity which is succeeded.</p>

<p><a href=""https://i.stack.imgur.com/1U7CH.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1U7CH.jpg"" alt=""enter image description here""></a></p>

<p>For more details, refer “<a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">Web activity in Azure Data Factory</a>”.</p>

<p>Hope this helps.</p>
"
"55410777","How to load assembly file with SSIS C# scripting component to Azure Data Factory integrated runtime?","<p>I have an SSIS package which contains a scripting component which extracts data from Dynamics 365. 
The package executes fine from Visual Studio and also from a local SQL server which I deployed it to to test.</p>

<p>I want to run the package from the cloud so I have setup an Azure integrated runtime to run the SSIS package using a pipeline, however when I try to execute the package from within Azure (using SSMS) I get a file not found error as below:</p>

<pre><code>Dynamics Extract to Staging:Error: System.IO.FileNotFoundException: Could 
Not load file or assembly 'Microsoft.Xrm.Sdk, Version=9.0.0.0, 
Culture=neutral, PublicKeyToken=31bf3856ad364e35' or one of its 
dependencies. The system cannot find the file specified.
</code></pre>

<p>I assume this is related to the assembly file not being loaded to Azure along with the package, although I would have thought it was included as part of the SSIS package given the package executes fine when run from the local SQL Server i tested on?</p>

<p>I have read through <a href=""https://alisharifiblog.wordpress.com/2015/06/19/add-dll-to-gac-for-microsoft-dynamics-crm-ssis-packages/"" rel=""nofollow noreferrer"">this</a> and a few others describing the issue but most seem to be related to adding the assembly to a local server environment.</p>

<p><a href=""https://stackoverflow.com/questions/49021176/azure-function-error-could-not-load-file-or-assembly-microsoft-xrm-sdk-versio"">This</a> also seems to refer to the same issue but is for functions and Im not sure how to apply this to my problem with executing the SSIS package in Azure data factory.</p>

<p>How do I ensure this assembly file is correctly deployed with the SSIS package to the Azure SSIS catalogue?</p>

<p>Any help appreciated.</p>
","<c#><azure><ssis><azure-data-factory>","2019-03-29 05:01:42","637","3","1","55413209","<p>I've been digging a bit further and found you need to install the assembly files to the integrated runtime node when it is first being spun up, this link from <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-configure-azure-ssis-ir-custom-setup"" rel=""nofollow noreferrer"">microsoft</a> didn't make much sense to me when i first read it but after reading this better explained example on <a href=""https://www.mssqltips.com/sqlservertip/5743/customized-setup-for-the-azuressis-integration-runtime/"" rel=""nofollow noreferrer"">mssqltips</a> and going back to it it reads a bit better.</p>

<p>I am in the process of running up the IR again at the moment so will see if this is indeed the solution i was looking for soon, thought id add this extra research to keep it all centralised.</p>

<p>EDIT: Yes, this was what i needed to do the package is now running correctly in the Azure IR, if your having this problem follow the steps in the two links above :)</p>
"
"55410736","How to map schema dynamically in Copy Activity when source and destination both are dynamic?","<p>I am working with a scenario where I have one azure file storage account in which I have different folders, each contains the *.csv file. I want to load each *.csv file in different Azure SQL Database tables dynamically by iterating over the my RootFolder.</p>

<p>The problem I am facing is my *.csv file contains more columns than my destination. When copy activity gets triggered following error is encountered:</p>

<p><a href=""https://i.stack.imgur.com/ifXUd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ifXUd.png"" alt=""enter image description here""></a></p>
","<ssis><azure-data-factory><azure-storage-files>","2019-03-29 04:56:36","241","1","1","55428679","<p>You could make your dataset schema as <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">expression</a>. And then use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">GetMetadataActivity</a> to get schema of each file. </p>
"
"55408389","How to create a dynamic API endpoint connection using HTTP or REST connectors in Azure Data Factory V2","<p>I have an external REST based API that I need to create a connection to in order to retrieve data on a regularly scheduled basis (for BI purposes). This API is fairly robust, and supports around 60 distinct endpoints. Also, this same API is used to access information across multiple client sub-domains (e.g. client1.apisource.com, client2.apisource.com, client3.apisource.com, etc.). In other words, the API endpoints are the same for each client subdomain.</p>

<p>So what I'm trying to figure out is whether it's possible to create a single ADF that contains a complete set of pipeline actions for each endpoint <b>THAT</b> uses a ""dynamic"" URL based on the client subdomains? In other words...what I'm trying to see if its possible to create a single ADF that can manage a dynamic list of base URLs.</p>

<p>I tried to parameterize the HTTP and REST connections, but this doesn't appear to Is this possible yet. Any thoughts? Thanks!</p>
","<rest><azure><api><httpurlconnection><azure-data-factory>","2019-03-28 23:31:46","1560","0","1","55436830","<p>Here is an <a href=""https://github.com/furmangg/automating-azure-sql-dw/blob/master/ADFv2/ResumeAzureSQLDW.json"" rel=""nofollow noreferrer"">example</a> of a Web Activity to call a REST API using parameters and expressions. The URL can be an expression like:</p>

<pre><code>@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Sql/servers/',pipeline().parameters.Server,'/databases/',pipeline().parameters.DW,'?api-version=2014-04-01')
</code></pre>
"
"55401257","ARM Template - Improvement - customSetupScriptProperties","<p>In ARM Template generated by ADF for deployment from Azure DevOps, would it be possible to add the parameter </p>

<p>INTEGRATION_RUNTIME_NAME_properties_typeProperties_ssisProperties_customSetupScriptPropertiesblob_<strong>ContainerUri</strong>,</p>

<p>in the same way there is</p>

<p>INTEGRATION_RUNTIME_NAME_properties_typeProperties_ssisProperties_customSetupScriptProperties_<strong>sasToken</strong></p>

<p>Thank you !</p>
","<azure-data-factory>","2019-03-28 15:21:43","76","0","1","55412707","<p>IntegrationRuntimeCustomSetupScriptProperties can be used as follows:</p>

<pre><code> ""customSetupScriptProperties"": {
                        ""blobContainerUri"": {
                            ""value"": ""-::String""
                        }
                    }
</code></pre>

<p>And</p>

<pre><code> ""customSetupScriptProperties"": {
                        ""sasToken"": {
                            ""value"": ""-::secureString""
                        }
                    }
</code></pre>

<p>Custom setup script properties for a managed dedicated integration runtime.</p>

<p><a href=""https://i.stack.imgur.com/jO9Nx.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jO9Nx.jpg"" alt=""enter image description here""></a></p>

<p>For more details, refer “<a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/integrationruntimes/createorupdate#definitions"" rel=""nofollow noreferrer"">Integration Runtimes</a>”.</p>

<p>Hope this helps.</p>
"
"55394759","Workaround to seeing data factory v2 debug runs","<p>I realise normally a debug run is not visible in the data factory v2 UI after closing the browser window, however unfortunately I needed to restart my machine unexpectedly and it's a long running pipeline.</p>

<p>I thought maybe the runs might be available via powershell, but I haven't had any luck.</p>

<p>The pipeline is likely still running.</p>

<p>We do have external logging, however ideally I'd like to see how long each activity is taking as I'm load testing.</p>

<p>And more importantly I do not want to do another run until I'm sure it's finished.... notably I'll run it from a trigger next time (just in case!).</p>

<hr>

<p>EDIT:
It looks like a sandbox id is used which is stored in the browser local storage and there appears to be undocumented API endpoints for gathering info using the sandbox id. But there doesn't appear to be a way of getting old sandbox id's so I'm probably out of luck.</p>
","<azure-data-factory>","2019-03-28 10:00:43","1642","2","2","55584093","<p>There is a button for view all debug runs. 
<a href=""https://i.stack.imgur.com/E5Poa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E5Poa.png"" alt=""enter image description here""></a></p>
"
"55394759","Workaround to seeing data factory v2 debug runs","<p>I realise normally a debug run is not visible in the data factory v2 UI after closing the browser window, however unfortunately I needed to restart my machine unexpectedly and it's a long running pipeline.</p>

<p>I thought maybe the runs might be available via powershell, but I haven't had any luck.</p>

<p>The pipeline is likely still running.</p>

<p>We do have external logging, however ideally I'd like to see how long each activity is taking as I'm load testing.</p>

<p>And more importantly I do not want to do another run until I'm sure it's finished.... notably I'll run it from a trigger next time (just in case!).</p>

<hr>

<p>EDIT:
It looks like a sandbox id is used which is stored in the browser local storage and there appears to be undocumented API endpoints for gathering info using the sandbox id. But there doesn't appear to be a way of getting old sandbox id's so I'm probably out of luck.</p>
","<azure-data-factory>","2019-03-28 10:00:43","1642","2","2","67597905","<p>Taken from <a href=""https://learn.microsoft.com/en-us/azure/data-factory/iterative-development-debugging#monitoring-debug-runs"" rel=""nofollow noreferrer"">Microsoft documentation</a>:</p>
<blockquote>
<p>To view a historical view of debug runs or see a list of all active debug runs, you can go into the Monitor experience.</p>
<p><a href=""https://i.stack.imgur.com/k4QMH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/k4QMH.png"" alt=""enter image description here"" /></a></p>
</blockquote>
"
"55382987","Unable to manually trigger pipeline - Trigger button grayed out","<p>The trigger button is grayed out and disabled.  I cannot select it.</p>

<p>Pipeline has been published and zero errors on validation.
Pipeline is running on a schedule with no errors, but want to run manually as needed.</p>

<p>Tried Edge and Chrome</p>

<p><a href=""https://i.stack.imgur.com/V86n3.png"" rel=""nofollow noreferrer"">Pipeline-Before-Publish</a></p>

<p><a href=""https://i.stack.imgur.com/oeeJ8.png"" rel=""nofollow noreferrer"">Pipeline-After-Publish</a></p>
","<azure-data-factory>","2019-03-27 17:12:57","335","0","1","55389648","<p>Didn't repro on my side. Which browser do you use? Could you share a screenshot of the disabled trigger button?</p>

<p>It seems like a bug, here is a workaround: 
on top of the trigger button, right click and then left click inspect</p>

<p><a href=""https://i.stack.imgur.com/Ysw1d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ysw1d.png"" alt=""on top of the trigger button, right click and then left click insect""></a></p>

<p>find the trigger tag and remove disable
<a href=""https://i.stack.imgur.com/wbAOh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wbAOh.png"" alt=""find the trigger tag and remove disable""></a></p>

<p>What's your factory version? If it's preview (2018-07-01-preview), then publish/debug/trigger are all blocked. Please refresh the page and go to overview page, you will see a prompt dialog suggesting you move to GA version.</p>
"
"55377204","Incremental Copy data from PostgreSql to Blob","<p>I am currently working on PostgreSql, and working to move Incremental data from PostgresDb to Blob, please help me with the solution, previously tried doing copy tool templates but no use.</p>

<p>I want to copy data incrementally from PostgreSql to Azure Blob.</p>
","<azure-data-factory>","2019-03-27 12:27:28","510","-1","1","55393797","<p>Please follow the solution in this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-overview#delta-data-loading-from-database-by-using-a-watermark"" rel=""nofollow noreferrer"">link</a> to define a watermark in your source database which has the last updated time stamp or an incrementing key.</p>

<p>1.<strong>Select the watermark column.</strong> Select one column in the source data store, which can be used to slice the new or updated records for every run. Normally, the data in this selected column (for example, last_modify_time or ID) keeps increasing when rows are created or updated.</p>

<p>2.<strong>Configure the query sql in copy activity.</strong></p>

<p>Such as <code>select * from table where modifyTime between 2019.3.27 and 2019.3.28</code></p>

<p><a href=""https://i.stack.imgur.com/MMYqX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MMYqX.png"" alt=""enter image description here""></a></p>

<p>3.<strong>Create a schedule trigger to run your pipeline and copy activity.</strong> Please see this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger"" rel=""nofollow noreferrer"">link</a>. You could trigger it every day to copy the data of yesterday incrementally.</p>

<p><a href=""https://i.stack.imgur.com/0agDa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0agDa.png"" alt=""enter image description here""></a></p>
"
"55345988","Filter blob data in Copy Activity","<p>I have a copy Activity that copies data from Blob to Azure Data Lake.  The Blob is populated by an Azure function with an event hub trigger. Blob files are appended with UNIX timestamp which is the event enqueued time in the event hub. Azure data factory is triggered every hour to merge the files and move them over to Data lake.</p>

<p><a href=""https://i.stack.imgur.com/J0TGi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/J0TGi.png"" alt=""enter image description here""></a></p>

<p>Inside the source dataset I have filters by Last Modified date in UTC time out of the box.  I can use this but it limits me to use Last modified date in the blob.  I want to use my own date filters and decide where I want to apply these filters.  Is this possible in Data factory?  If yes, can you please point me in the right direction. </p>

<p><a href=""https://i.stack.imgur.com/BGLkh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BGLkh.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2019-03-25 20:32:53","1496","3","1","55371141","<p>For ADF in any case,the only idea that came to my mind is using combination of <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">Look Up Activity</a> ,<a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">ForEach Activity</a> and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-filter-activity"" rel=""nofollow noreferrer"">Filter Activity</a>.Maybe it is kind of complex.</p>

<p>1.Use Look up to retrieve the data from the blob file.</p>

<p>2.Use ForEach Activity to loop the result and set your data time filters.</p>

<p>3.Inside the ForEach Activity, do the copy task.</p>

<p>Please refer to this <a href=""http://blogs.adatis.co.uk/matthow/post/Using-Lookup-Execute-Pipeline-and-For-Each-Activity-in-Azure-Data-Factory-V2"" rel=""nofollow noreferrer"">blog</a> to get some clues.</p>

<p>Reviewing your descriptions of all the tasks you did now, I suggest you getting an idea of <a href=""https://learn.microsoft.com/en-us/azure/stream-analytics/"" rel=""nofollow noreferrer"">Azure Stream Analytics Service</a>. No matter the data source is Event Hub or Azure Blob Storage, ASA supports them as <a href=""https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs"" rel=""nofollow noreferrer"">input</a>. And it supports ADL as <a href=""https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-outputs#azure-data-lake-store"" rel=""nofollow noreferrer"">output</a>.</p>

<p>You could create a job to configure input and output,then use popular <a href=""https://learn.microsoft.com/en-us/stream-analytics-query/stream-analytics-query-language-reference?toc=%2Fen-us%2Fazure%2Fstream-analytics%2FTOC.json&amp;bc=%2Fen-us%2Fazure%2Fbread%2Ftoc.json"" rel=""nofollow noreferrer"">SQL language</a> to filter your data however you want.Such as <a href=""https://learn.microsoft.com/en-us/stream-analytics-query/where-azure-stream-analytics"" rel=""nofollow noreferrer"">Where</a> operator or <a href=""https://learn.microsoft.com/en-us/stream-analytics-query/date-and-time-functions-azure-stream-analytics"" rel=""nofollow noreferrer"">DataTime Functions</a>.</p>
"
"55341186","Data Factory Email Errors when found on rows","<p>I'm using the Copy Data task in Data Factory to copy data from CSV files in Azure Files to a SQL Azure DB.</p>

<p>Within the task there is a setting called Fault tolerance which can be set to skip and log incompatible rows which writes an error log to Azure Blob Storage.</p>

<p>However I'd like the errors picked up from the file to be emailed to a user to action and also store the list of errors in a DB rather than a log file in blob storage.</p>
","<azure><csv><azure-sql-database><azure-data-factory>","2019-03-25 15:25:57","451","0","1","55372870","<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-fault-tolerance"" rel=""nofollow noreferrer"">All features</a> of Fault tolerance are established, no such email alert mechanism in that. However you could use workaround to implement your requirements.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-triggered-function#prerequisites"" rel=""nofollow noreferrer"">Blob Trigger Azure Function</a> to monitor the blob path you configured in the fault tolerance. Once the error logs streams into your blob file, you could collect the log and use send email sdk(For example,you could just configure the output as <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-sendgrid"" rel=""nofollow noreferrer"">SendGrid</a> service in MS) to the destinations you want.</p>

<p>As for store errors into DB,you could create another trigger function to configure the output as <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-table"" rel=""nofollow noreferrer"">Table Storage</a>.</p>

<p>Just a reminder,ADF has own monitor and alert mechanism. It's for all pipelines in ADF,not specific for copy activity. You could get an idea of it from this <a href=""https://azure.microsoft.com/en-us/blog/create-alerts-to-proactively-monitor-your-data-factory-pipelines/"" rel=""nofollow noreferrer"">link</a>.</p>
"
"55341125","Validate date column in CSV with Data Factory","<p>I'm attempting to use Data Factory to import a CSV file from Azure File storage into SQL Azure. So far I'm using the copy data task to read the CSV and save into SQL Azure.</p>

<p>My CSV file contains 2 date columns with the date in the format dd/MM/yyyy. How can I set Data Factory to validate and read dates in this format?</p>
","<csv><azure-sql-database><azure-data-factory>","2019-03-25 15:22:24","1341","0","1","55349124","<p>You can follow my step, I tried this and Data Factory can validate and read dates in ""dd/MM/yyyy"" format.</p>

<p>This is my csv file, also has two columns with the date in the format ""dd/MM/yyyy"".
<a href=""https://i.stack.imgur.com/5xTv4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5xTv4.png"" alt=""enter image description here""></a></p>

<p>The difference between us is I import my file  from Azure blob storage into my Azure SQL database by using Copy Data.</p>

<p>If you want Data Factory can validate and read dates in ""dd/MM/yyyy"" format, during File format settings, you must set the the schema,  specify the column type and the format. Please see this picture:
<a href=""https://i.stack.imgur.com/2Gjai.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2Gjai.png"" alt=""enter image description here""></a></p>

<p>After copy active completed,  the date with ""dd/MM/yyyy"" format will be parsed to the default format ""yyyy-MM-dd"".</p>

<p><a href=""https://i.stack.imgur.com/AmQLG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AmQLG.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"55334313","How can we map one table to multiple tables with different columns in sink DB in Azure data factory?","<p>how can we replicate data from one source table to two destination tables with different columns? In Azure data factory we have any in built services is there?</p>
","<azure-data-factory>","2019-03-25 09:02:43","280","1","1","55339250","<p>You could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">column mapping</a> of copy activity.</p>

<p>If you want to do this at the same time (in a single pipeline) you could use foreach activity and then pass different column mapping into the copy activity <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">with expression</a>. </p>

<p>To make it simple, you could just use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-copy-data-tool"" rel=""nofollow noreferrer"">copy data tool</a> to help you build the pipeline. </p>
"
"55320412","Passing values from parent pipeline to child pipeline in Azure Data Factory","<p>I'm pretty sure this is simple, but I can't seem to find this anywhere. I created a parameter in a parent pipeline (say the pipeline name is TestParent)  in data factory:</p>

<p><a href=""https://i.stack.imgur.com/lIFYQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lIFYQ.png"" alt=""enter image description here""></a>
This parent pipeline invokes a child pipeline. I want to reference this parameter in the child pipeline. What is the syntax to get the value of this parameter from the parent in the child?</p>
","<azure><azure-data-factory>","2019-03-24 03:11:42","4418","0","1","55320905","<p>OK I finally got this to work:</p>

<p>I completely removed the parameter from the parent pipeline. In the child pipeline (which is called HubMaster) we create a parameter called MasterBatchId:</p>

<p><a href=""https://i.stack.imgur.com/P2qyD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P2qyD.png"" alt=""enter image description here""></a></p>

<p>In the parent pipeline I created an <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-pipeline-activity"" rel=""nofollow noreferrer"">Execute pipeline</a> node called EP_HubMaster that calls a child pipeline called HubMaster. In order to populate the child pipeline parameter MasterBatchId at run time, we need to edit the JSON of the parent pipeline to look like this:</p>

<pre><code>{
""name"": ""TestParent"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""EP_HubMaster"",
            ""type"": ""ExecutePipeline"",
            ""typeProperties"": {
                ""pipeline"": {
                    ""referenceName"": ""HubMaster"",
                    ""type"": ""PipelineReference""
                },
                ""parameters"": {
                    ""MasterBatchId"": {
                        ""value"": ""@pipeline().RunId"",
                        ""type"": ""Expression""
                    }
                }
            }
        }
    ],
    ""folder"": {
        ""name"": ""Master""
    }
},
""type"": ""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>

<p>You can see that we pass the <code>@pipeline().RunId</code> from the parent (which was the original intent) to the input parameter of <code>MasterBatchId</code> in the child pipeline.</p>
"
"55315301","How to set SAP VARIABLES on MDX Query during COPY DATA on Data Factory?","<p>I'm trying to get copy data from a SAP BW on-premisses to a CSV file on Blob Storage using MDX connection.
When I run this query on BEX Analyzer, the SAP popup a screen where insert a date range, this isn't an optional variable.
The point is, I have no idea on how to do this during the Copy Data on Azure Data Factory.</p>

<p>I've unsuccessfully tested Where Clause, SAP VARIABLES and FILTER() </p>

<p><strong>QUERY</strong>:</p>

<pre><code>SELECT
    { [Measures].[9M8SVJVCT2BZ52DZQ58MOL7TJ],
    [Measures].[9M8SVJVCT2BZ52DZQ58MOLKGN] } ON COLUMNS
FROM ZSD_I28/Y_CBA_ZSD_I28_TM1_001
WHERE  ([ZSD_I28___F38].[2019/02]:[ZSD_I28___F38].[2019/02]) 
</code></pre>

<p><strong>MESSAGE ERROR:</strong></p>

<pre><code>'Type=Microsoft.Data.Mashup.MashupValueException,Message=SAP Business Warehouse: Entrar um valor para a variável Ano civil/mês (Intervalo de valores).,Source=Microsoft.Data.Mashup.ProviderCommon,'. Activity ID:93e3ab50-92e0-4883-ae02-88e28b9a69ce
</code></pre>

<p><strong>TRANSLATION:</strong>
Entrar um valor para a variável Ano civil/mês (Intervalo de valores)Enter a variable value for Year/Month (Range Value)</p>

<p><strong>[ZSD_I28___F38 -]</strong> This is the Year/Month column that should receive the param.</p>

<p><strong><a href=""https://i.stack.imgur.com/NRUqn.png"" rel=""nofollow noreferrer"">Check this print</a></strong></p>

<p>Thanks</p>
","<mdx><azure-data-factory><sap-bw>","2019-03-23 15:24:22","976","0","1","55330342","<p>currently we don't have support on parameters via MDX in our SAP BW connector. We did have an alternative connector on Open Hub which should have better performance and support on filtering. do you think that can work for your scenario?
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sap-business-warehouse-open-hub"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-sap-business-warehouse-open-hub</a> </p>
"
"55301229","several Azure DevOps project GITs vs single Azure Databricks repository","<p>We have several projects running on Azure. </p>

<p>As they need some separation from each other, we need to establish an Azure Data Factory per each project (as ADF doesn’t have a ACL within itself). 
Each project will have its own GIT repository via DevOps (each project has it’s own DevOps project, so separate GITs) , we end up with each project’s ADF being connected to their own GIT. 
So we have:</p>

<p>Project1.ADF &lt;-> Project1.DevOpsProject1.GIT</p>

<p>Project2.ADF &lt;-> Project2.DevOpsProject2.GIT</p>

<p>We want to be able to connect to Azure Databricks from each ADF. We want to avoid multiplying Azure Databricks due to cost (plus databricks has ACL within that we can use). However, then the databricks workspace can only be connected to a single GIT repository. So if each project is to work on the same databricks then we need a databricks repository shared between the different projects.</p>

<p>Apart from the repository being in Project1.DevOps1.GIT and just that repository shared to Project2 (or vice versa), is there any better way?</p>
","<azure><azure-devops><azure-data-factory><azure-databricks>","2019-03-22 13:58:13","304","0","1","55339312","<p>This was actually much simpler than I thought - you can specify on each notebook the GIT repository you want to connect to.
So we can still have</p>

<p>Project1.ADF &lt;-> Project1.DevOpsProject1.GIT</p>

<p>Project1.Databricks &lt;-> Project1.DevOpsProject1.GIT</p>

<p>Project2.ADF &lt;-> Project2.DevOpsProject2.GIT</p>

<p>Project2.Databricks &lt;-> Project2.DevOpsProject2.GIT</p>

<p><a href=""https://i.stack.imgur.com/oMV8r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oMV8r.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/k8uPp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/k8uPp.png"" alt=""enter image description here""></a></p>
"
"55293198","Azure Data Factory - Trouble authenticating invalid_grant","<p>In attempting to authenticate with Active Directory to retrieve an access token, I followed the following steps:</p>

<p>1 - Created an application in Azure using the following directions (step 3):
<a href=""https://github.com/Azure-Samples/active-directory-java-native-headless#step-3--register-the-sample-with-your-azure-active-directory-tenant"" rel=""nofollow noreferrer"">https://github.com/Azure-Samples/active-directory-java-native-headless#step-3--register-the-sample-with-your-azure-active-directory-tenant</a></p>

<p>2 - Created a ""Azure Active Directory"" user account with the ""Data Factory Contributor"" permission.</p>

<p>3 - Used the steps from (url below), I followed the example code to attempt to retrieve an access token:
<a href=""https://github.com/Azure-Samples/active-directory-java-native-headless/blob/master/src/main/java/PublicClient.java"" rel=""nofollow noreferrer"">https://github.com/Azure-Samples/active-directory-java-native-headless/blob/master/src/main/java/PublicClient.java</a></p>

<p>Note: the resource I am using is...
<a href=""https://management.core.windows.net/"" rel=""nofollow noreferrer"">https://management.core.windows.net/</a></p>

<p>... using the following as an example:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-rest-api#authenticate-with-azure-ad"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-rest-api#authenticate-with-azure-ad</a></p>

<p>4 - Server is responding with the following error:</p>

<pre><code>Exception in thread ""main"" java.util.concurrent.ExecutionException:
com.microsoft.aad.adal4j.AuthenticationException:
 {
    ""error_description"": ""AADSTS65001: The user or administrator has not
    consented to use the application with ID '&lt;my-app-id&gt;' named
    '&lt;my-app-name&gt;'. Send an interactive authorization request for this
    user and resource.\r\nTrace ID: d0af56e6-aaa3-4d25-b23b-
    2984ed2b4400\r\nCorrelation ID: 2422cc2f-1cdd-45c5-8b7c-
    46b1eee4ffae\r\nTimestamp: 2019-03-22 04:58:16Z"",

    ""error"": ""invalid_grant""
 }
</code></pre>

<p>What grants are required in order to get the authentication to work?</p>
","<java><azure><azure-active-directory><azure-data-factory>","2019-03-22 05:03:48","810","2","1","55293412","<p>You should make an authorization request to Azure AD that includes the parameter prompt=admin_consent.</p>

<p>Go to a URL such as <a href=""https://login.microsoftonline.com/tenant-id/oauth2/authorize?client_id=app-client-id&amp;redirect_uri=encoded-reply-url&amp;response_type=code&amp;prompt=admin_consent"" rel=""nofollow noreferrer"">https://login.microsoftonline.com/tenant-id/oauth2/authorize?client_id=app-client-id&amp;redirect_uri=encoded-reply-url&amp;response_type=code&amp;prompt=admin_consent</a>.</p>

<p>Use your admin account to consent the permissions.</p>

<p><a href=""https://i.stack.imgur.com/csm0x.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/csm0x.png"" alt=""enter image description here""></a></p>

<p>Then we can get the access token successfully.</p>

<p><a href=""https://i.stack.imgur.com/z5N4v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z5N4v.png"" alt=""enter image description here""></a></p>
"
"55288437","Iterate in Activity within ForEach activity Azure Data Factory","<p>I have a MetaData activity and a foreach activity connected to it.</p>

<p><a href=""https://i.stack.imgur.com/fNhXy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fNhXy.png"" alt=""enter image description here""></a></p>

<p>I run this ForEach activity sequentially and here is the item it iterates over:</p>

<p><a href=""https://i.stack.imgur.com/eUhcW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eUhcW.png"" alt=""enter image description here""></a></p>

<p>I have a copy activity within this foreach activity:</p>

<p><a href=""https://i.stack.imgur.com/x3g63.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x3g63.png"" alt=""enter image description here""></a></p>

<p>So I have 4 childItems that I get from my meta data activity.  And I need to set the folder name in data lake based on the childItem value.  In order to access the Child Item I have to use the zero based index.  Can I set it dynamically? I assumed there would be something since it is already in a loop and is running sequentially. So I would not have to do </p>

<pre><code>@activity('GetMetaData').output.ChildItems[3].name
</code></pre>

<p>but use the index</p>

<pre><code>@activity('GetMetaData').output.ChildItems[index].name
</code></pre>
","<azure><foreach><azure-data-factory>","2019-03-21 20:02:04","8812","3","1","55291814","<p>Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity#iteration-expression-language"" rel=""nofollow noreferrer"">document</a>,you could refer to the properties inside the for-each activity by using <code>@item().XXX</code>,instead <code>@activity('GetMetaData').output.ChildItems[index].XXX</code>. The <code>items</code> property is the collection and each item in the collection is referred to by using the <code>@item()</code>.</p>

<blockquote>
  <p>In the ForEach activity, provide an array to be iterated over for the
  property items."" Use @item() to iterate over a single enumeration in
  ForEach activity. For example, if items is an array: [1, 2, 3],
  @item() returns 1 in the first iteration, 2 in the second iteration,
  and 3 in the third iteration.</p>
</blockquote>

<p>Also,please see this marked answer :<a href=""https://stackoverflow.com/questions/51608625/azure-data-factory-get-data-for-for-eachcomponent-from-query"">Azure Data Factory get data for &quot;For Each&quot;component from query</a></p>
"
"55279329","How to add dynamic connection string in Azure Data Factory","<p>Can someone help me on achieving dynamic connections to databases using azure data factory?</p>
","<azure><azure-sql-database><azure-cosmosdb><azure-data-factory><azure-databricks>","2019-03-21 11:21:54","1901","-1","1","55291518","<blockquote>
  <p>achieving dynamic connections to databases using azure data factory</p>
</blockquote>

<p>Based on this document <a href=""https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services"" rel=""nofollow noreferrer"">Parameterize linked services in Azure Data Factory</a>,so far,you can now parameterize a linked service and pass dynamic values at run time.</p>

<p><a href=""https://i.stack.imgur.com/sZm2N.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sZm2N.png"" alt=""enter image description here""></a> </p>

<p>It supports Cosmos DB:</p>

<p><a href=""https://i.stack.imgur.com/onTUI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/onTUI.png"" alt=""enter image description here""></a></p>

<p>BTW,MS recommends not to parameterize passwords or secrets. Store all connection strings in Azure Key Vault instead, and parameterize the Secret Name.About details,please see this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault"" rel=""nofollow noreferrer"">link</a>.</p>

<hr>

<p><strong><em>Update Answer:</em></strong></p>

<p>As for db name,of course you can parameterize it.</p>

<p>You could configure dynamic db name when you create cosmos db linked service.</p>

<p><a href=""https://i.stack.imgur.com/vsB0C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vsB0C.png"" alt=""enter image description here""></a></p>

<p>Click dynamic content and create new parameter.</p>

<p><a href=""https://i.stack.imgur.com/RSsWB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RSsWB.png"" alt=""enter image description here""></a></p>

<hr>

<p><strong><em>Update Answer2:</em></strong></p>

<p>Please refer to this <a href=""https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.operations.linked_services_operations.linkedservicesoperations?view=azure-python#create-or-update-resource-group-name--factory-name--linked-service-name--properties--if-match-none--custom-headers-none--raw-false----operation-config-"" rel=""nofollow noreferrer"">sdk function</a> and my working code as below:</p>

<pre><code>from azure.common.credentials import ServicePrincipalCredentials
from azure.mgmt.resource import ResourceManagementClient
from azure.mgmt.datafactory import DataFactoryManagementClient
from azure.mgmt.datafactory.models import *

# Azure subscription ID
subscription_id = '***'

# This program creates this resource group. If it's an existing resource group, comment out the code that creates the resource group
rg_name = '***'

# The data factory name. It must be globally unique.
df_name = '***'

# Specify your Active Directory client ID, client secret, and tenant ID
credentials = ServicePrincipalCredentials(client_id='***',
                                          secret='***',
                                          tenant='***')
resource_client = ResourceManagementClient(credentials, subscription_id)
adf_client = DataFactoryManagementClient(credentials, subscription_id)

resource_client.resource_groups.get(rg_name)

# Create a data factory
df_resource = Factory(location='eastus')
df = adf_client.factories.get(rg_name, df_name, df_resource)
print(df)

ls_name = 'testlink1'
dbName = ""&lt;your db name&gt;""

connection_string = 'AccountEndpoint=https://***.documents.azure.com:443/;AccountKey=***;Database='+dbName+';';

ls_cosmos_db = CosmosDbLinkedService(connection_string=connection_string)
ls = adf_client.linked_services.create_or_update(rg_name, df_name, ls_name, ls_cosmos_db)
print(ls)
</code></pre>

<p>More details, you could refer to official <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-python#create-a-linked-service"" rel=""nofollow noreferrer"">create linked service example code</a> and python sdk for <a href=""https://learn.microsoft.com/en-us/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.models.linkedservice?view=azure-python"" rel=""nofollow noreferrer"">ADF management</a>.</p>

<p><a href=""https://i.stack.imgur.com/Nmo6S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Nmo6S.png"" alt=""enter image description here""></a></p>
"
"55278875","Run Powershell script in Azure Automation or ADF Custom Activity?","<p>What could be the best way to automatize the following process in Azure:</p>

<ol>
<li>Run PowerShell script</li>
<li>Store the output (.csv or .tsv) directly in ADLS (or in a StorageAccount and copy them to ADLS)</li>
<li>Move data from ADLS to one Azure SQL DB</li>
</ol>

<p>For points 2 and 3 think that the best way is to implement them in ADF with copy activity and some U-SQL scripts.</p>

<p>For point 1 I don’t have a clear idea how to implement it. I’m considering a Custom Activity in ADF or implement it on Azure Automation.
Which method is more appropriate for this purpose?</p>
","<azure><powershell><azure-data-factory><azure-automation>","2019-03-21 10:57:38","512","0","1","55291682","<p>Agree with the comment, you could use <strong>Azure Automation runbook</strong> to run the powershell script. There are two runbook types to run the powershell, for the advantages and limitations, see <a href=""https://learn.microsoft.com/en-us/azure/automation/automation-runbook-types#powershell-runbooks"" rel=""nofollow noreferrer"">PowerShell runbooks</a> and <a href=""https://learn.microsoft.com/en-us/azure/automation/automation-runbook-types#powershell-workflow-runbooks"" rel=""nofollow noreferrer"">PowerShell Workflow runbooks</a>.</p>

<p>For more details to use them, see <a href=""https://learn.microsoft.com/en-us/azure/automation/automation-first-runbook-textual-powershell"" rel=""nofollow noreferrer"">My first PowerShell runbook</a> and <a href=""https://learn.microsoft.com/en-us/azure/automation/automation-first-runbook-textual"" rel=""nofollow noreferrer"">My first PowerShell Workflow runbook</a>.</p>
"
"55267485","Getting files and folders in the datalake while reading from datafactory","<p>While reading azure sql table data (which actually consists of path of the directories) from azure data factory by using the paths how to dynamically get the files from the datalake.</p>

<p>Can any one tell me what should I give in the dataset</p>

<p><a href=""https://i.stack.imgur.com/MTO3v.png"" rel=""nofollow noreferrer"">Screenshot</a></p>
","<azure-sql-database><azure-data-factory><azure-data-lake>","2019-03-20 18:06:21","40","0","1","55325097","<p>You could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">lookup activity</a> to read data from azure sql, and then following it by an foreach activity. And then, pass @item(). to your dataset parameter k1.</p>

<p><a href=""https://i.stack.imgur.com/gjD2I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gjD2I.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/Qmvr1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qmvr1.png"" alt=""enter image description here""></a></p>
"
"55263594","Execute python script from azure data factory","<p>Can someone help me with executing python function from azure data factory.
I have stored python function in blob and i'm trying to trigger the same.
However i'm not able to do it. Please assist.</p>

<p>Second, Can i parameterize python function call from ADF?</p>
","<python><azure><azure-data-factory><azure-databricks><azure-triggers>","2019-03-20 14:46:43","3122","1","1","55272638","<p>You could get an idea of <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure Function Activity</a> in ADF which allows you to run Azure Functions in a Data Factory pipeline.</p>

<p>And you could duplicate your python function into <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-first-function-python"" rel=""nofollow noreferrer"">Python Azure Function</a>.</p>

<p>Also,it want to pass parameters into python function,you could set them into body properties.</p>

<p><a href=""https://i.stack.imgur.com/QasUP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QasUP.png"" alt=""enter image description here""></a></p>

<p>The Azure Function Activity supports routing. For example, if your app uses the following routing - <code>https://functionAPP.azurewebsites.net/api/functionName/{value}?code=&lt;secret&gt;</code> - then the functionName is <code>functionName/{value}</code>, which you can parameterize to provide the desired <code>functionName</code> at runtime.</p>
"
"55251654","Azure: create O365 events from MySQL database","<p>I want to create Office 365 calendar events which are in a MySQL table (on-premise server).</p>

<p>Currently, I'm planning to do it with Azure Data Factory and a Functions App.
I copying the MySQL Data from MySQL table to Azure Table Storage (this is working great).</p>

<p>After that, I want to create the event entries via Azure Function (HTTP Trigger, Looping all storage entities and creating calendar event via Graph API) but there are over 10,000 events in that table. The function would probably run too long.</p>

<p>Is there a better way to create these events? O365 can only be used as a source in Azure Data Factory. Maybe a batch in Azure Data Factory is better than a function? Should I run the Function, not for all events? For every single event (table insert trigger)? Are there other/better options to do that in Azure?</p>
","<azure><microsoft-graph-api><azure-functions><azure-data-factory>","2019-03-19 23:55:40","140","2","1","55251800","<p>If you are already writing to table storage then make a <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-table"" rel=""nofollow noreferrer"">Table Trigger</a> and process one record at a time. So then all your 10000 event will be processed in multiple threads. But then you probably will need to think about write limits to calendar. </p>
"
"55248215","Migrating to v2 - Setting up Linked Services - ODBC Cisco Information Server","<p>I am trying to duplicate the services from Data Factory v1 to Data Factory v2.  I have a working linked service in Data Factory v1.  I set up a new Runtime Integration in v2 and tried to create a new ODBC linked service with the following connection string:</p>

<p>""DRIVER={Cisco Information Server 7.0};HOST=an.example.com;PORT=9401; DOMAIN=US;DATASOURCE=company;""</p>

<p>I get the following Error: Connection failed
ERROR [IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified</p>

<p>It works in DFv1, what am I doing wrong?</p>
","<azure-data-factory>","2019-03-19 18:55:40","83","0","1","55269081","<p>I solved the problem. The Cisco Information Server 7.0 ODBC driver was not installed on the computer hosting the Runtime Integration.</p>
"
"55247914","Azure Data factory parameters","<p>I have a data factory pipeline which takes the following parameters</p>

<ul>
<li>Param1</li>
<li>Database Server 1 Name </li>
<li>Database Server 2 Name </li>
<li>Database Server 1 Username</li>
<li>Database Server 2 Username</li>
<li>etc</li>
</ul>

<p>My pipeline decides via some logic which database server to do an import from.</p>

<p>Essentially I want to deploy 2 versions of my pipeline. 1 Runs in dev and the other in prod. </p>

<p>I want to release a dev and prod version of my pipeline via Azure Devops. Each environment release should provide (via key vault) the values of:</p>

<ul>
<li>Database Server 1 Name  </li>
<li>Database Server 2 Name</li>
<li>Database Server 1 Username</li>
<li>Database Server 2 Username</li>
</ul>

<p>First prize would be if those values did not even show up any more as parameters in the pipeline. So that triggers would just have to provide Param1. In addition if you manually run the pipeline I also just want to provide Param1.</p>

<p><strong>EDIT:</strong> Note that I use the parameters eventually in a paramaterized linked service if that makes a difference (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services</a>). </p>
","<azure-keyvault><azure-data-factory>","2019-03-19 18:36:19","460","1","1","55252420","<p>I think the key idea to resolve your problem is to use two separate instances of data factory.</p>

<p>In the DEV enironment you have your parameterized connection as you stated above. When taking the code to PROD, you export the template and import it again into the other instance. There you have an additional config file that fills up the values needed to set up the connection properly.</p>

<p>If you want to avoid having the credentials stored in the config file then just add an azure key vault linked service and parameterize the secret identifier accordingly. When you import the template into PROD you even do not need to provide any parameter but the identifier for which secret to grab from key vault.</p>

<p>See here for more info:</p>

<p><a href=""https://azure.microsoft.com/en-us/blog/continuous-integration-and-deployment-using-data-factory/"" rel=""nofollow noreferrer"">devops integration</a></p>

<p><a href=""https://azure.microsoft.com/en-us/blog/secure-credential-management-for-etl-workloads-using-azure-data-factory-and-azure-key-vault/"" rel=""nofollow noreferrer"">key vault integration</a></p>
"
"55243836","Data from HTTP endpoint to be loaded into Azure Data Lake using Azure Data Factory","<p>I am trying to build a so called ""modern data warehouse"" using Azure services.</p>

<p>First step is to gather all the data in its native raw format into Azure Data Lake store. For some of the data sources we have no other choice than to use API for consuming the data. There's not much information when searching, therefore I am asking.</p>

<p>Is it possible to define 2 Web Activities in my pipeline that will handle below scenario?</p>

<ol>
<li>Web1 activity gets an API URL generated from C# (Azure Function). It returns data in JSON format and saves it to Web1.Output - this is working fine.</li>
<li>Web2 activity consumes Web1.Output and saves it into Azure Data Lake as a plain txt file (PUT or POST) - this is needed.</li>
</ol>

<p>Above scenario is achievable by using Copy activity, but then I am not able to pass dynamic URL generated by Azure Functions. How do I save the JSON output to ADL? Is there any other way?</p>

<p>Thanks!</p>
","<azure><azure-data-factory><azure-data-lake>","2019-03-19 14:52:19","219","0","1","55270340","<p>Since you are using blob storage as an intermediary, and want to consume the blob upon creation, you could take advantage of <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger"" rel=""nofollow noreferrer"">Event Triggers</a>.  You can set up the Event trigger to run a pipeline containing Web2 activity. Which kicks off when the Web1 activity completes (separate pipeline).</p>

<p>By separating the two activities into separate pipelines, the workflow becomes asynchronous.  This means you will not need to wait for both activities to complete before doing the next URL.  There are many other benefits as well.</p>
"
"55239373","Parquet troubles with decimal in Azure Data Factory V2","<p>Since 3 or 4 days i'm experiencing troubles in writing decimal values in parquet file format with Azure Data Factory V2.</p>

<p>The repro steps are quite simple, from an SQL source containing a numeric value i map it to a parquet file using the copy activity.</p>

<p>At runtime the following exception is thrown:</p>

<pre><code>{
    ""errorCode"": ""2200"",
    ""message"": ""Failure happened on 'Source' side. ErrorCode=UserErrorParquetTypeNotSupported,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Decimal Precision or Scale information is not found in schema for column: ADDRESSLONGITUDE,Source=Microsoft.DataTransfer.Richfile.ParquetTransferPlugin,''Type=System.InvalidCastException,Message=Object cannot be cast from DBNull to other types.,Source=mscorlib,'"",
    ""failureType"": ""UserError"",
    ""target"": ""Copy Data""
}
</code></pre>

<p>In the source the complaining column is defined as numeric(32,6) type.</p>

<p>I think the problem is circumscribed to the parquet sink because changing the destination format to csv result in a succeeded pipeline.</p>

<p>Any suggestions?</p>

<p>Based on Jay's answer, here is the whole dataset :</p>

<pre><code>SELECT 
    [ADDRESSLATITUDE]
FROM 
    [dbo].[MyTable]
</code></pre>

<p><a href=""https://i.stack.imgur.com/O020x.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/O020x.png"" alt=""query result""></a></p>
","<azure-data-factory>","2019-03-19 10:58:29","3826","0","1","55255750","<p>Based on the <a href=""https://drill.apache.org/docs/parquet-format/#sql-types-to-parquet-logical-types"" rel=""nofollow noreferrer"">SQL Types to Parquet Logical Types</a> and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#data-type-mapping-for-parquet-files"" rel=""nofollow noreferrer"">Data type mapping for Parquet files</a> in data factory copy activity,it supports <code>Decimal</code> data type.Decimal data is converted into binary data type.</p>

<p><a href=""https://i.stack.imgur.com/G8v38.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G8v38.png"" alt=""enter image description here""></a></p>

<p>Back to your error message:</p>

<blockquote>
  <p>Failure happened on 'Source' side.
  ErrorCode=UserErrorParquetTypeNotSupported,
    'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,
    Message=Decimal Precision or Scale information is not found in schema
  for column: 
    ADDRESSLONGITUDE,Source=Microsoft.DataTransfer.Richfile.ParquetTransferPlugin,''
    Type=System.InvalidCastException,Message=Object cannot be cast from
  DBNull to other types.,Source=mscorlib,'</p>
</blockquote>

<p>If your numeric data has <code>null</code> value, it will be converted into <code>Int</code> data type without any 
Decimal precision or scale information. </p>

<p><a href=""https://i.stack.imgur.com/ZfjQl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZfjQl.png"" alt=""enter image description here""></a></p>

<p>Csv format does not have this transformation process so you could set default value for your numeric data.</p>
"
"55238971","Read JSON from rest API as is with Azure Data Factory","<p>I'm trying to get Azure Data Factory to read my REST API and put it in SQL Server. The source is a REST API and the sink is a SQL Server table.</p>

<p>I tried to do something like: </p>

<pre><code>""translator"": {
    ""type"": ""TabularTranslator"",
    ""schemaMapping"": {
        ""$"": ""json""
    },
    ""collectionReference"": ""$.tickets""
}
</code></pre>

<p>The source looks like:</p>

<pre><code>{ ""tickets"": [ {... }, {...} ] }
</code></pre>

<p>Because of the poor mapping capabilities I'm choosing this path. I'll then split the data with a query. Preferbly I'd like to store each object inside tickets as a row with JSON of that object. </p>

<p>In short, how can I get the JSON output from the RestSource to a SqlSink single column text/nvarchar(max) column?</p>
","<sql-server><rest><azure><azure-data-factory>","2019-03-19 10:38:11","1918","0","1","61238454","<p>I managed to solve the same issue by modifying mapping manually.
ADF anyway tries to parse json, but from the Advanced mode you can edit json paths. Ex., this is the original schema parsed automatically by ADF
<a href=""https://imgur.com/Y7QhcDI"" rel=""nofollow noreferrer"">https://imgur.com/Y7QhcDI</a>
Once opened in Advanced mode it will show full paths by adding indexes of the elements, something similar to $tickets[0][] etc
Try to delete all other columns and keep the only one $tickets (the highest level one), in my case it was $value <a href=""https://i.stack.imgur.com/WnAzC.jpg"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/WnAzC.jpg</a>. As the result the entire json will be written into the destination column.</p>

<p>If there are pagination rules in place, each page will be written as a single row.</p>
"
"55222451","Azure SQL Database: Transfer/Migrate Table from one Azure SQL Database to Another Azure SQL Database using Copy Activity","<p>I am trying to Migrate a table from Database A to Database B in same Azure SQL Server. I was successful in migrating data from Azure blob storage to Azure database using Azure Data Factory, But I don't seem to understand how i can modify my ADF code to migrate data across database.</p>

<p>I've learned to migrate data by help of <a href=""https://stackoverflow.com/questions/6572754/sql-azure-copy-table-between-databases"">This StackOverflow Link</a> But i am looking for migrating table using Copy Activity.</p>

<p>My Pipeline for Azure Blob to Azure SQL is below. Please suggest the modifications that would result in migration of data from one Azure SQL DB to another. Here is the complete Data Factory code. I am looking for a workaround or at least some resource that would guide me. Thanks in advance.</p>

<p><strong>Azure SQL Linked Service</strong></p>

<pre><code>{
""name"": ""AzureSqlLinkedService"",
""properties"": {
    ""description"": """",
    ""hubName"": ""dalete_hub"",
    ""type"": ""AzureSqlDatabase"",
    ""typeProperties"": {
        ""connectionString"": ""Data Source=tcp:server.database.windows.net,1433;Initial Catalog=DB;Integrated Security=False;User ID=login@server.database.windows.net;Password=**********;Connect Timeout=30;Encrypt=True""
    }
}
</code></pre>

<p>}</p>

<p><strong>Azure Storage Linked Service</strong></p>

<pre><code>{
""name"": ""AzureStorageLinkedService"",
""properties"": {
    ""description"": """",
    ""hubName"": ""dalete_hub"",
    ""type"": ""AzureStorage"",
    ""typeProperties"": {
        ""connectionString"": ""DefaultEndpointsProtocol=https;AccountName=newstorageaccount;AccountKey=**********""
    }
}
</code></pre>

<p>}</p>

<p><strong>Input Dataset</strong></p>

<pre><code>   {
    ""name"": ""InputDataset"",
    ""properties"": {
        ""structure"": [
            {
                ""name"": ""Region"",
                ""type"": ""String""
            },
            {
                ""name"": ""Sales"",
""type"": ""String""
            }
        ],
        ""published"": false,
        ""type"": ""AzureBlob"",
        ""linkedServiceName"": ""AzureStorageLinkedService"",
        ""typeProperties"": {
            ""fileName"": ""data.txt"",
            ""folderPath"": ""adfpoc/"",
            ""format"": {""type"": ""TextFormat"",
                ""columnDelimiter"": "",""
            }
        },
        ""availability"": {
            ""frequency"": ""Hour"",
            ""interval"": 1
        },
        ""external"": true,
        ""policy"": {}
    }
}
</code></pre>

<p><strong>Output Dataset</strong></p>

<pre><code>    {
    ""name"": ""OutputDataset"",
    ""properties"": {
        ""structure"": [
            {
                ""name"": ""Region"",
                ""type"": ""String""
            },
            {
                ""name"": ""Sales"",
                ""type"": ""String""
            }
        ],
        ""published"": false,
        ""type"": ""AzureSqlTable"",
        ""linkedServiceName"": ""AzureSqlLinkedService"",
        ""typeProperties"": {
            ""tableName"": ""data""
        },
        ""availability"": {
            ""frequency"": ""Hour"",
            ""interval"": 1
        }
    }
}
</code></pre>

<p><strong>ADFPipeline</strong></p>

<pre><code>  {
    ""name"": ""ADFTutorialPipeline"",
    ""properties"": {
        ""description"": ""Copy data from a blob to Azure SQL table"",
        ""activities"": [
            {
                ""type"": ""Copy"",
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""BlobSource"" },
                    ""sink"": {
                        ""type"": ""SqlSink"",
                        ""writeBatchSize"": 10000,
                        ""writeBatchTimeout"": ""60.00:00:00""
                    }
                },
                ""inputs"": [
                    {
                        ""name"": ""InputDataset""}
                ],
                ""outputs"": [
                    {
                        ""name"": ""OutputDataset""
                    }
                ],
                ""policy"": {
                    ""timeout"": ""01:00:00"",
                    ""concurrency"": 1,
                    ""executionPriorityOrder"": ""NewestFirst""
                },
                ""scheduler"": {
                    ""frequency"": ""Hour"",
                    ""interval"": 1
                },
                ""name"": ""CopyFromBlobToSQL""
            }
        ],
        ""start"": ""2019-03-11T00:00:00Z"",
        ""end"": ""2019-03-12T00:00:00Z"",
        ""isPaused"": false,
        ""hubName"": ""dalete_hub"",
        ""pipelineMode"": ""Scheduled""
    }
}
</code></pre>
","<azure><azure-sql-database><azure-storage><azure-data-factory>","2019-03-18 13:24:57","773","0","2","55230764","<p>Moving from a sql database to another will be similar to what you have already done, but this time the source will be a sql table, like your previous sink (or output).</p>

<p>So what you should do is create a new linked service for the new database, then create a dataset to be the input (with the same format as your previous output, but changing the linked service name so it uses the new linked service).</p>

<p>Last, create a pipeline and configure properly the input and output datasets.</p>

<p>Hope this helped!</p>
"
"55222451","Azure SQL Database: Transfer/Migrate Table from one Azure SQL Database to Another Azure SQL Database using Copy Activity","<p>I am trying to Migrate a table from Database A to Database B in same Azure SQL Server. I was successful in migrating data from Azure blob storage to Azure database using Azure Data Factory, But I don't seem to understand how i can modify my ADF code to migrate data across database.</p>

<p>I've learned to migrate data by help of <a href=""https://stackoverflow.com/questions/6572754/sql-azure-copy-table-between-databases"">This StackOverflow Link</a> But i am looking for migrating table using Copy Activity.</p>

<p>My Pipeline for Azure Blob to Azure SQL is below. Please suggest the modifications that would result in migration of data from one Azure SQL DB to another. Here is the complete Data Factory code. I am looking for a workaround or at least some resource that would guide me. Thanks in advance.</p>

<p><strong>Azure SQL Linked Service</strong></p>

<pre><code>{
""name"": ""AzureSqlLinkedService"",
""properties"": {
    ""description"": """",
    ""hubName"": ""dalete_hub"",
    ""type"": ""AzureSqlDatabase"",
    ""typeProperties"": {
        ""connectionString"": ""Data Source=tcp:server.database.windows.net,1433;Initial Catalog=DB;Integrated Security=False;User ID=login@server.database.windows.net;Password=**********;Connect Timeout=30;Encrypt=True""
    }
}
</code></pre>

<p>}</p>

<p><strong>Azure Storage Linked Service</strong></p>

<pre><code>{
""name"": ""AzureStorageLinkedService"",
""properties"": {
    ""description"": """",
    ""hubName"": ""dalete_hub"",
    ""type"": ""AzureStorage"",
    ""typeProperties"": {
        ""connectionString"": ""DefaultEndpointsProtocol=https;AccountName=newstorageaccount;AccountKey=**********""
    }
}
</code></pre>

<p>}</p>

<p><strong>Input Dataset</strong></p>

<pre><code>   {
    ""name"": ""InputDataset"",
    ""properties"": {
        ""structure"": [
            {
                ""name"": ""Region"",
                ""type"": ""String""
            },
            {
                ""name"": ""Sales"",
""type"": ""String""
            }
        ],
        ""published"": false,
        ""type"": ""AzureBlob"",
        ""linkedServiceName"": ""AzureStorageLinkedService"",
        ""typeProperties"": {
            ""fileName"": ""data.txt"",
            ""folderPath"": ""adfpoc/"",
            ""format"": {""type"": ""TextFormat"",
                ""columnDelimiter"": "",""
            }
        },
        ""availability"": {
            ""frequency"": ""Hour"",
            ""interval"": 1
        },
        ""external"": true,
        ""policy"": {}
    }
}
</code></pre>

<p><strong>Output Dataset</strong></p>

<pre><code>    {
    ""name"": ""OutputDataset"",
    ""properties"": {
        ""structure"": [
            {
                ""name"": ""Region"",
                ""type"": ""String""
            },
            {
                ""name"": ""Sales"",
                ""type"": ""String""
            }
        ],
        ""published"": false,
        ""type"": ""AzureSqlTable"",
        ""linkedServiceName"": ""AzureSqlLinkedService"",
        ""typeProperties"": {
            ""tableName"": ""data""
        },
        ""availability"": {
            ""frequency"": ""Hour"",
            ""interval"": 1
        }
    }
}
</code></pre>

<p><strong>ADFPipeline</strong></p>

<pre><code>  {
    ""name"": ""ADFTutorialPipeline"",
    ""properties"": {
        ""description"": ""Copy data from a blob to Azure SQL table"",
        ""activities"": [
            {
                ""type"": ""Copy"",
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""BlobSource"" },
                    ""sink"": {
                        ""type"": ""SqlSink"",
                        ""writeBatchSize"": 10000,
                        ""writeBatchTimeout"": ""60.00:00:00""
                    }
                },
                ""inputs"": [
                    {
                        ""name"": ""InputDataset""}
                ],
                ""outputs"": [
                    {
                        ""name"": ""OutputDataset""
                    }
                ],
                ""policy"": {
                    ""timeout"": ""01:00:00"",
                    ""concurrency"": 1,
                    ""executionPriorityOrder"": ""NewestFirst""
                },
                ""scheduler"": {
                    ""frequency"": ""Hour"",
                    ""interval"": 1
                },
                ""name"": ""CopyFromBlobToSQL""
            }
        ],
        ""start"": ""2019-03-11T00:00:00Z"",
        ""end"": ""2019-03-12T00:00:00Z"",
        ""isPaused"": false,
        ""hubName"": ""dalete_hub"",
        ""pipelineMode"": ""Scheduled""
    }
}
</code></pre>
","<azure><azure-sql-database><azure-storage><azure-data-factory>","2019-03-18 13:24:57","773","0","2","55273814","<p>I tried the same operation with you and I migrated my table successfully in Data Factory.</p>

<p>For example, I have a table <code>table1</code> in database <code>dbleon</code>, I want to migrate <code>table1</code> to another database <code>dbleon1</code> with Copy Activity.</p>

<p>I create a new table <code>table1</code> in <code>dbmeon1</code> which has the same schema with <code>table1</code> in <code>dbleon</code>.
<a href=""https://i.stack.imgur.com/144ZP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/144ZP.png"" alt=""enter image description here""></a></p>

<p>Here is my ADF code:</p>

<pre><code>{
    ""name"": ""CopyPipeline_0oh"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""Copy_0oh"",
                ""type"": ""Copy"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""userProperties"": [
                    {
                        ""name"": ""Source"",
                        ""value"": ""[dbo].[table1]""
                    },
                    {
                        ""name"": ""Destination"",
                        ""value"": ""[dbo].[table1]""
                    }
                ],
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""SqlSource""
                    },
                    ""sink"": {
                        ""type"": ""SqlSink"",
                        ""writeBatchSize"": 10000
                    },
                    ""enableStaging"": false,
                    ""translator"": {
                        ""type"": ""TabularTranslator"",
                        ""columnMappings"": {
                            ""id"": ""id"",
                            ""name"": ""name""
                        }
                    }
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""SourceDataset_0oh"",
                        ""type"": ""DatasetReference""
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""DestinationDataset_0oh"",
                        ""type"": ""DatasetReference""
                    }
                ]
            }
        ]
    },
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>

<p><strong>Source Dataset:</strong></p>

<pre><code>{
    ""name"": ""SourceDataset_0oh"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureSqlDatabase1"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""AzureSqlTable"",
        ""structure"": [
            {
                ""name"": ""id"",
                ""type"": ""Int32""
            },
            {
                ""name"": ""name"",
                ""type"": ""String""
            }
        ],
        ""typeProperties"": {
            ""tableName"": ""[dbo].[table1]""
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p><strong>Destination Dataset:</strong></p>

<pre><code>{
    ""name"": ""DestinationDataset_0oh"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureSqlDatabase2"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""AzureSqlTable"",
        ""structure"": [
            {
                ""name"": ""id"",
                ""type"": ""Int32"",
                ""precision"": 10
            },
            {
                ""name"": ""name"",
                ""type"": ""String""
            }
        ],
        ""typeProperties"": {
            ""tableName"": ""[dbo].[table1]""
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p>Hope this helps.</p>
"
"55217185","Dynamic content for additional column with copy activity","<p>I'm trying to create a simple copy activity to copy data from a source Azure Table to a sink Mongo Cosmos DB but want to also output an extra column to the sink data where the content of the additional column is the run id (or something else that is dynamically set per run).</p>

<p>I can add the extra column easily by defining an additional column in the source schema but can't work out how to set the content (presumably it should be set in the activity), hence the value for the added field is always NULL in the output DB </p>

<p>Thanks</p>
","<azure><azure-data-factory>","2019-03-18 08:25:10","2137","1","1","55218829","<p>You can do the same, or something similar, and create a Dynamic select statement in your copy activity.  So something like SELECT @{item().sourceTableCustomColumnList}, @pipeline().RunId FROM @{item().sourceTableName} </p>

<p>You may refer the <a href=""https://social.msdn.microsoft.com/Forums/azure/en-US/09ff6f2d-7e33-4891-a601-234d5970bd16/adding-additional-column-in-data-being-saved-in-cosmosdb-by-azure-data-factorys-copy-activity?forum=AzureDataFactory"" rel=""nofollow noreferrer"">MSDN</a> thread which addressing similar issue.</p>

<p>Hope this helps.</p>
"
"55202447","How to drive a Azure Data Factory based off of SQL Server values","<p>I have one Pipeline setup in Azure Data Factory that has a pipeline variable setup called MonthsBack, lets say 12 as a default.  When in an Until loop I build a SQL statement that uses this MonthsBack variable to generate a SQL statement and in the loop it also decrements the value by 1 and appends these SQL statements into another Collection Variable.  After the Until block runs I have an array with 12 SQL statements, which is then used in a ForEach block to execute those 12 statements and dynamically generate 12 files that are Store in a Data Lake.  This all works fine and does exactly what I want to do.</p>

<p>Now I have a new requirement where instead of having 12 SQL statements, what I want to be able to do is set the MonthsBack variable equal to some value in the SQL database, so ideally I would create a new DataSet that would query and retrieve this value from the database and then use that to set default value for the MonthsBack database.  I can't seem to figure out how to accomplish this with the given tasks.  </p>

<p>I know that I have seen in the Dynamic Content pane a way to reference the result of a previous task, but I can't remember how I got to this before.  Can someone either tell me or point me to the correct documentation on how to do this.  Thanks</p>
","<azure-data-factory>","2019-03-16 23:15:50","1271","1","1","55231161","<p>What you want to do is achievable using a Lookup activity and capturing its output with a Set Variable activity.</p>

<p>On the lookup activity select any dataset from the database you want, then click on ""query"" and write the query that brings the value you want, I'll give an example:</p>

<pre><code>select Max(Id) as Var1 from dbo.YourTable
</code></pre>

<p>Then, on the Set Variable activity you can capture the Lookup Activity's output with something like:</p>

<pre><code>@activity('LookupActivityName').output.firstRow.Var1
</code></pre>

<p>Note that I used Var1 on both the query (on the lookup) and the dynamic content (on the set variable).</p>

<p>After this, the variable will be set with the value you want from the database.</p>

<p><a href=""https://i.stack.imgur.com/NJVus.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/NJVus.png"" alt=""Link the set variable after the Lookup""></a></p>

<p>Hope this helped!</p>

<p>Link to documentation on data factory functions and expressions: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions</a></p>
"
"55168479","Capture metadata using Azure Data Factory and storing in SQL Database?","<p>The sort of metadata that I am after includes file sizes, number of rows, file names, if the file has already been processed etc. and I want to capture the flow of data from source to target including capturing data from Azure data lake and SQL DB.</p>

<p>I also want to store this metadata into SQL tables as a control table and a test of how the files/tables/data has changed over the entire ETL/ELT process.</p>

<p>The only way I could think of doing this was by using stored procedures in ADF that collect the metadata for each part and store in SQL tables but I wasn't sure how I could read the metadata from the files in the data lake.</p>

<p>Has anyone come up with an approach on how to do this or even a better solution.</p>
","<azure><azure-sql-database><azure-data-lake><azure-data-factory>","2019-03-14 17:14:16","697","0","1","55179320","<p>You could use get metadata of data lake files via GetMetaData Activity.Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">official document</a>, the output from GetMetadata Activity can be used in conditional expressions to perform validation.</p>

<p>It supports Azure data lake connectors:</p>

<p><a href=""https://i.stack.imgur.com/6Oz5J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6Oz5J.png"" alt=""enter image description here""></a></p>
"
"55167911","How to set header in Azure Data Factory web activity by azure SDK for .net","<p>I am just wondering how can i add header to the azure data factory web activity. I tried code below:</p>

<pre><code>                new WebActivity()
                {
                    Name = ""Authenticating"",
                    Url = ""xxxxxxxxxxxxx"",
                    Method = ""POST"",
                    Headers =  "" \""Content-Type\"": \""application/x-www-form-urlencoded\""""
                },
</code></pre>

<p>However, the code will result in </p>

<p><code>""headers"": {
            ""0"": "" "",
            ""1"": ""\"""",
            ""2"": ""C"",
            ""3"": ""o"",
            ""4"": ""n"",
            ""5"": ""t"",
            ""6"": ""e"",
            ""7"": ""n"",
            ""8"": ""t"",
            ""9"": ""-"",
            ""10"": ""T"",
            ""11"": ""y"",
            ""12"": ""p"",
            ""13"": ""e"",
            ""51"": ""\"""",
            ""seed"": 1840253349
        },</code></p>

<p>I went to their source code, they said </p>

<pre><code>/// Gets or sets represents the headers that will be sent to the
/// request. For example, to set the language and type on a request:
/// ""headers"" : { ""Accept-Language"": ""en-us"", ""Content-Type"":
/// ""application/json"" }. Type: string (or Expression with resultType
/// string).
</code></pre>

<p>I also tried with {} within the string, like: </p>

<pre><code>Headers =  "" { \""Accept-Language\"": \""en-us\"", \""Content-Type\"":\r\n/// \""application/json\"" }"",
</code></pre>

<p>Any idea?</p>
","<azure><azure-data-factory>","2019-03-14 16:44:23","1347","0","1","55189975","<p>We should pass in a Json object instead of string. Just changed the code to below and it worked.</p>

<pre><code>Headers =JObject.Parse(@""{ 'Content-Type': 'application/x-www-form-urlencoded'}"") 
</code></pre>
"
"55166091","Azure Data Factory with long duration Azure Functions","<p>I would like to create a pipeline in Data Factory and I would like to use Azure Function for some C# code which will download files from some web services, etc. The problem is that only HTTP Triggered function is supported in Azure Factory and HTTP Triggered function has duration limited for 230 seconds.
<a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-scale"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-functions/functions-scale</a></p>

<p>Is there any workaround how to trigger azure function with longer duration from data factory? I need to do it synchronously, because another processes will work with downloaded data.</p>

<p>Thank you very much for any ideas.</p>
","<azure><azure-functions><azure-data-factory><azure-functions-runtime>","2019-03-14 15:15:58","523","0","1","55363684","<p>The simplest approach would be to just deploy your function app in an <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-scale#app-service-plan"" rel=""nofollow noreferrer"">App Service Plan</a> instead.</p>

<p>But if you require true serverless experience of the consumption plan, you could try using <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-overview"" rel=""nofollow noreferrer"">Durable Functions</a> along with a pipeline setup that polls the status endpoint returned (as shown <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-create-first-csharp#test-the-function-locally"" rel=""nofollow noreferrer"">here</a>) by the durable functions orchestrator.</p>

<p>I believe you would have to use </p>

<ul>
<li>Until</li>
<li>Web</li>
<li>Set Variable</li>
<li>Wait</li>
</ul>

<p>Basically, you would be polling the status endpoint until the runtimeStatus is set to ""Completed"".</p>

<p>Depending on your use case, <a href=""https://learn.microsoft.com/en-us/azure/batch/batch-technical-overview"" rel=""nofollow noreferrer"">Azure Batch Service</a> might be something you'd want to look into too which also has a connector to Azure Data Factory.</p>
"
"55165561","Copying data from a single spreadsheet into multiple tables in Azure Data Factory","<p>The Copy Data activity in Azure Data Factory appears to be limited to copying to only a single destination table. I have a spreadsheet containing rows that should be expanded out to multiple tables which reference each other - what would be the most appropriate way to achieve that in Data Factory? </p>

<p>Would multiple copy tasks running sequentially be able to perform this task, or does it require calling a custom stored procedure that would perform the inserts? Are there other options in Data Factory for transforming the data as described above?</p>
","<azure-data-factory>","2019-03-14 14:50:43","549","1","1","55214083","<p>If the columnMappings in your source and sink dataset don't against the error conditions mentioned in this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#when-to-specify-dataset-structure"" rel=""nofollow noreferrer"">link</a>,</p>

<blockquote>
  <p>1.Source data store query result does not have a column name that is specified in the input dataset ""structure"" section.</p>
  
  <p>2.Sink data store (if with pre-defined schema) does not have a column name that is specified in the output dataset ""structure"" section.</p>
  
  <p>3.Either fewer columns or more columns in the ""structure"" of sink dataset than specified in the mapping.</p>
  
  <p>4.Duplicate mapping.</p>
</blockquote>

<p>you could connect the copy activities in series and execute them sequentially.</p>

<p>Another solution is Stored Procedure which could meet your custom requirements.About configuration,please refer to my previous detailed case:<a href=""https://stackoverflow.com/questions/51352725/azure-data-factory-mapping-2-columns-in-one-column/51354266#51354266"">Azure Data Factory mapping 2 columns in one column</a></p>
"
"55160947","I wanted to upsert the incremental data from sql db to sql warehouse using Azure data factory","<p>I wanted to update and insert (upsert) the incremental data from the azure SQL database to Azure data warehouse using azure data factory</p>

<p>:-> The DB is having the multiple tables.</p>
","<azure><azure-sql-database><azure-data-factory><cdc><azure-synapse>","2019-03-14 11:07:36","1275","0","1","55166676","<p>You to define a watermark in your source database. A watermark is a column that has the last updated time stamp or an incrementing key. The delta loading solution loads the changed data between an old watermark and a new watermark. The workflow for this approach is depicted in the following diagram.</p>
<p><a href=""https://i.stack.imgur.com/WJN6A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WJN6A.png"" alt=""enter image description here"" /></a></p>
<p>For step-by-step instructions, see <a href=""https://sqlofthenorth.blog/2017/10/17/incremental-etl-loading-with-azure-data-factory-v2/"" rel=""nofollow noreferrer"">this</a> article. On this article adjust the sink to be Azure SQL Data Warehouse and you kind of have all you need on this article.</p>
<p>Another useful example can be found on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-multiple-tables-portal"" rel=""nofollow noreferrer"">this</a> Microsoft documentation.</p>
"
"55159740","Unable to copy file from SFTP in Azure Data Factory when using wildcard(*) in the filename","<p>I am unable to copy csv files from an SFTP connection to blob storage when using the wildcard(*) in the filename.</p>

<p>More specifically, I receive csv files in the SFTP on a daily basis, and they are of the format: ""ddMMyyyyxxxxxx.csv"", where ""xxxxxx"" is the timestamp. More concretely, my csv file for the 13th of March is: ""13032019083647.csv"", while for the 14th of March: ""14032019083556.csv"". Obviously, the timestamp is different for every day, thus I want to copy the file independently of whatever strings exists between the date and the the file extenstion.</p>

<p>In the ""File"" subfield of the ""File path"" of the ""Connection"" tab of my subset, I give as input: ""13032019*.csv"", as instructed by the help icon next to the field:</p>

<p><a href=""https://i.stack.imgur.com/2DKRM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2DKRM.png"" alt=""File name or file filter""></a></p>

<p>When I do so, my Debug run fails with: </p>

<blockquote>
  <p>{""errorCode"": ""2200"", ""message"":
  ""ErrorCode=UserErrorInvalidCopyBehaviorBlobNameNotAllowedWithPreserveOrFlattenHierarchy,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Cannot
  adopt copy behavior PreserveHierarchy when copying from folder to a
  single file.,Source=Microsoft.DataTransfer.ClientLibrary}</p>
</blockquote>

<p>I receive a similar error no matter which type of copy behaviour I choose. I have also tried experimenting with the fileFilter parameter (even though ADF warns that the same behaviour can be achieved with the fileName option), but I still end up getting the same error.</p>

<p>For further clarification, I am attaching the Code segment that ADF produces for this configuration:</p>

<p><a href=""https://i.stack.imgur.com/2ZxzF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2ZxzF.png"" alt=""Code segment with failing fileName""></a></p>

<p>I should also mention, that when using the full fileName in the corresponding field, namely the value: ""13032019083647.csv"", copying works normally.</p>

<p>Any help would be greatly appreciated!</p>
","<azure><ftp><sftp><azure-data-factory>","2019-03-14 10:07:07","2928","0","2","55181672","<p>My guess it might get two files with wildcard operation.
In such cases we need to use metadata activity, filter activity and for-each activity to copy these files.</p>

<p>1.Metadata activity : Use data-set in these activity to point the particular location of the files and pass the child Items as the parameter.</p>

<p>2.Filter activity : Use filter to filter the files based on your needs.</p>

<p>3.For-each activity : In the For-each activity get Items from the previous activity and add copy activity inside the for-each.
In copy activity the source data set should be @item().name. </p>

<p>I hope this will solve your issue.</p>
"
"55159740","Unable to copy file from SFTP in Azure Data Factory when using wildcard(*) in the filename","<p>I am unable to copy csv files from an SFTP connection to blob storage when using the wildcard(*) in the filename.</p>

<p>More specifically, I receive csv files in the SFTP on a daily basis, and they are of the format: ""ddMMyyyyxxxxxx.csv"", where ""xxxxxx"" is the timestamp. More concretely, my csv file for the 13th of March is: ""13032019083647.csv"", while for the 14th of March: ""14032019083556.csv"". Obviously, the timestamp is different for every day, thus I want to copy the file independently of whatever strings exists between the date and the the file extenstion.</p>

<p>In the ""File"" subfield of the ""File path"" of the ""Connection"" tab of my subset, I give as input: ""13032019*.csv"", as instructed by the help icon next to the field:</p>

<p><a href=""https://i.stack.imgur.com/2DKRM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2DKRM.png"" alt=""File name or file filter""></a></p>

<p>When I do so, my Debug run fails with: </p>

<blockquote>
  <p>{""errorCode"": ""2200"", ""message"":
  ""ErrorCode=UserErrorInvalidCopyBehaviorBlobNameNotAllowedWithPreserveOrFlattenHierarchy,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Cannot
  adopt copy behavior PreserveHierarchy when copying from folder to a
  single file.,Source=Microsoft.DataTransfer.ClientLibrary}</p>
</blockquote>

<p>I receive a similar error no matter which type of copy behaviour I choose. I have also tried experimenting with the fileFilter parameter (even though ADF warns that the same behaviour can be achieved with the fileName option), but I still end up getting the same error.</p>

<p>For further clarification, I am attaching the Code segment that ADF produces for this configuration:</p>

<p><a href=""https://i.stack.imgur.com/2ZxzF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2ZxzF.png"" alt=""Code segment with failing fileName""></a></p>

<p>I should also mention, that when using the full fileName in the corresponding field, namely the value: ""13032019083647.csv"", copying works normally.</p>

<p>Any help would be greatly appreciated!</p>
","<azure><ftp><sftp><azure-data-factory>","2019-03-14 10:07:07","2928","0","2","55265260","<p>What worked for me was the following: I kept the same regex for the input file, but I defined as ""Copy behaviour: Merge Files"". Since as mentioned, there is only 1 file that satisfies the regex condition, only 1 file was created as output. I am aware that this is a sort of ""dirty"" solution, but it did the trick for me.</p>
"
"55157552","ADF v2 Ensure that daily job will run after hourly (24 hours) job is successful","<p>Is there a way, where we can ensure in ADF v2 that my daily jobs will only run if the dependent hourly (24 hours) job is successful.</p>
","<azure><azure-data-factory>","2019-03-14 08:02:34","34","0","1","55158156","<p>To achieve your requirements,you could consider below solution:</p>

<p>1.Run the hourly pipeline regularly and get the execution result of the pipeline output.</p>

<p>2.Log the results into a blob file,excluding the timestamp and execution status(success or fail)</p>

<p>3.Before your daily activity,please add a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">look-up activity</a> to retrieve the log info.Then use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">For-Each activity</a> to loop the info.Inside the For-Each activity,use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-if-condition-activity"" rel=""nofollow noreferrer"">if-condition activity</a> to check the hourly pipeline execution result.From this, you can judge the following logical operations.</p>
"
"55156959","New Linked Service (Azure SQL Database) Test connection failed","<p>I'm creating New Linked Service that would allow connecting to PaaS DB but the Test Connection is failing. Things I've checked are:</p>

<p>-Firewall on Logical VM for PaaS DB(Relevant IP are in place)</p>

<p>-Connect to PaaS DB via SSMS(It Works)</p>

<p>-Typos</p>

<p>I've used AutoResolveIntegrationRuntime since DB is in the same RG as the Azure DataFactory. Google didn't help much.</p>

<p>Regards</p>

<p>PS: More info can be provided if needed.<a href=""https://i.stack.imgur.com/xCJZ5.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xCJZ5.jpg"" alt=""enter image description here""></a></p>
","<azure-sql-database><azure-data-factory>","2019-03-14 07:23:06","11339","1","1","55157972","<p>Firstly, make sure you have opened the ""Allow access to Azure Service"" in firewall setting.
<a href=""https://i.stack.imgur.com/ESl44.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ESl44.png"" alt=""enter image description here""></a></p>

<p>What's your PSSA DB, single database or managed instance?</p>

<p><strong>For single database</strong>:</p>

<p>The important thing you need to know that:
<a href=""https://i.stack.imgur.com/jexlK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jexlK.png"" alt=""enter image description here""></a></p>

<p><strong>For managed instance</strong>:</p>

<p>To use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database-managed-insance"" rel=""nofollow noreferrer"">copy data from an Azure SQL Database Managed Instance</a> that's located in a virtual network, set up a self-hosted integration runtime that can access the database. For more information, <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">see Self-hosted integration runtime</a>.</p>

<p>If you provision your self-hosted integration runtime in the same virtual network as your managed instance, make sure that your integration runtime machine is in a different subnet than your managed instance. If you provision your self-hosted integration runtime in a different virtual network than your managed instance, you can use either a virtual network peering or virtual network to virtual network connection. For more information, see <a href=""https://learn.microsoft.com/en-us/azure/sql-database/sql-database-managed-instance-connect-app"" rel=""nofollow noreferrer"">Connect your application to Azure SQL Database Managed Instance</a>.</p>

<p>Here an tutorial can helps you: <a href=""https://medium.com/@mauridb/azure-sql-managed-instances-and-azure-data-factory-a-walk-through-bfb93e79ac0c"" rel=""nofollow noreferrer"">Azure SQL Managed Instances and Azure Data Factory: a walk-through</a>. </p>

<p>Hope this helps.</p>
"
"55147053","How to perform Incremental Load with date or key column using Azure data factory","<p>I wanted to achieve an incremental load from oracle to Azure SQL data warehouse using azure data factory. The Issue that I am facing is I don't have any date column or any key column to perform Incremental load Is there any other way to achieve this.</p>
","<azure><etl><azure-data-factory>","2019-03-13 16:41:31","496","-1","1","55190382","<p>You will either have to:</p>

<p>A. Identify a field in each table you want to use to determine if the row has changed
 B. Implement some kind of change capture feature on the source data</p>

<p>Those are really the only the only two ways to limit the amount of data you pull from the source. </p>

<p>It wouldn't be very efficient, but if you are just trying not to update rows that haven't changed in your destination, you can hash your source values and hash the values in the destination, and only insert/update rows where the hashes don't match. Here's <a href=""http://blogs.adatis.co.uk/nigelmeakins/post/Hashing-for-Change-Detection-in-SQL-Server"" rel=""nofollow noreferrer"">an example</a> of how this works in T-SQL.   </p>

<p>There is a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-overview"" rel=""nofollow noreferrer"">section of the Data Factory documentation</a> dedicated to incrementally loading data. Please check it out if you haven't. </p>
"
"55132660","Azure Data Factory V2 Set-AzureRmDataFactoryV2Dataset : 'TableName' cannot be null","<p>I use following powershell command to create a dataset. This is dataset connects to Azure SQL DB. I will call the stored proc on pipeline based on this dataset connection. However, I got error message ""TableName' cannot be null"".</p>

<pre><code>Set-AzureRmDataFactoryV2Dataset -DataFactoryName DFName1 -DefinitionFile dataset1.json -Name dataset1 -ResourceGroupName rg1
</code></pre>
","<azure-data-factory>","2019-03-13 00:32:52","260","0","1","55133711","<ol>
<li><p>Please check whether you are using the latest powershell. You could use Update-Module -Name AzureRm.DataFactoryV2 to update your module. As my understanding, the lasted SDK should make table name as optional.</p></li>
<li><p>If #1 doesn't resolve your problem, you could just put a dummy table name into your tableName field. For example, tableName: ""dummy"". If you are using query or stored procedure in your copy source, tableName in dataset will be ignored. 
But if you are using this dataset as sink and invoke write storedprocedure, tableName field is required, which is the parameter name of your table type.</p></li>
</ol>
"
"55116637","data factory copy data without explicitly creating a target table","<p>I am working on copying data from a source Oracle database to a Target SQL data warehouse using the Data factory.</p>

<p>When using the copy function in data factory, we are asked to specify the destination location and a table to copy the data to. There are multiple tables that needs to be copied, and therefore making a table for each in the destination is time consuming.</p>

<p>How can I setup data factory to copy data from the source to a destination, where it will automatically create a table at the destination, without having to explicitly create them manually?</p>

<p>TIA</p>
","<azure><azure-data-factory>","2019-03-12 08:01:11","1067","1","1","55157473","<p>Came across the same issue last year, used pipeline.parameters() for dynamic naming and a Data Factory stored procedure activity before the copy activity to first create the empty table from a template before copying <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure</a>.</p>

<pre><code>
CREATE PROCEDURE create_sql_table_proc @WindowStartYear NVARCHAR(30), @WindowStartMonth NVARCHAR(30), @WindowStartDay NVARCHAR(30)
AS

BEGIN


declare @strsqlcreatetable as [NVARCHAR](255)
declare @strsqldroptable as [NVARCHAR](255)
declare @tablename as [NVARCHAR](255)
declare @strsqlsetpk as [NVARCHAR](255)

select @tablename = 'TABLE_NAME_' + @WindowStartYear + @WindowStartMonth + @WindowStartDay

select @strsqldroptable = 'DROP TABLE IF EXISTS ' +  @tablename

select @strsqlcreatetable = 'SELECT * INTO ' + @tablename + ' FROM  OUTPUT_TEMPLATE'

select @strsqlsetpk = 'ALTER TABLE ' + @tablename + ' ADD PRIMARY KEY (CustID)'

exec (@strsqldroptable)
exec (@strsqlcreatetable)
exec (@strsqlsetpk)


END
</code></pre>

<p>Since have started pushing the table to SQL from our Pyspark scripts running on a cluster, where it is not necessary to first create the empty table <a href=""https://medium.com/@radek.strnad/tips-for-using-jdbc-in-apache-spark-sql-396ea7b2e3d3"" rel=""nofollow noreferrer"">https://medium.com/@radek.strnad/tips-for-using-jdbc-in-apache-spark-sql-396ea7b2e3d3</a>.</p>
"
"55115384","Trigger SSIS package when a file is uploaded to Azure blob storage","<p>I want to detect and trigger a SSIS package deployed on the Azure data factory when a file is uploaded to Azure Blob storage. I know it's possible to trigger an SSIS package when a file is dropped to any folder but is it possible to trigger the SSIS package the same way when a file is uploaded to the Azure blob storage? </p>
","<sql-server><azure><ssis><azure-blob-storage><azure-data-factory>","2019-03-12 06:29:31","260","2","1","55116141","<p>try using event triggers . </p>

<p>Data Factory is integrated with Azure Event Grid, which lets you trigger pipelines on an event.
An event-based trigger runs pipelines in response to an event, such as the arrival of a file, or the deletion of a file, in Azure Blob Storage.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger"" rel=""nofollow noreferrer"">how-to-create-event-trigger</a></p>
"
"55097260","Invoke ADF pipeline using Azure EventHubs","<p>Would like to know whether we can invoke ADF pipeline using Azure EventHub?</p>

<p>Thanks.</p>
","<azure><azure-eventhub><azure-data-factory>","2019-03-11 07:46:30","1320","4","1","55097996","<p>You could use <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-event-hubs"" rel=""nofollow noreferrer"">Event Hub Trigger Azure Function</a> to listen your event hub service. It 
 will be run whenever an event hub receives a new event.</p>

<p><a href=""https://i.stack.imgur.com/x2Z5G.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x2Z5G.png"" alt=""enter image description here""></a></p>

<p>In the function method, please use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-dot-net#create-a-pipeline-run"" rel=""nofollow noreferrer"">ADF sdk</a> to invoke your ADF pipeline.</p>
"
"55082120","Query by date in Azure Data Factory Pipeline","<p>I want to use a query in a <strong>copy job</strong> for my source in an Azure Data Factory pipeline <strong>together with a date function</strong> - here is the dummy query:</p>

<p>SELECT * FROM public.report_campaign_leaflet WHERE day=""<strong><em>{today - 1d}</em></strong>""</p>

<p>I´ve found some documentation about dynamic content and some other stuff but no information on how to use date functions directly in a sql query.</p>

<p>Maybe someone has a hint for me?</p>

<p>Thanks &amp; best,
Michael</p>
","<azure-data-factory>","2019-03-09 21:26:42","3894","2","3","55088703","<p>Here is the possible solution for your problem. </p>

<p><a href=""https://i.stack.imgur.com/LdjlD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LdjlD.png"" alt=""enter image description here""></a>In your copy activity, at the source side, you choose query in Use Query option, and then in the query box you write an expression</p>

<p>Here is the expression <code>@concat('SELECT * FROM public.report_campaign_leaflet WHERE day=','""',formatDateTime(adddays(utcnow(),-1), 'yyyy-MM-dd'),'""')</code></p>

<p><strong>formatDateTime</strong> function will just format the output of addDays(utcnow(),-1) into yyyy-MM-dd format</p>

<p>Again, you can have a parameter in your pipeline processDate for example, and to set this value from expression in trigger definition, and then just to call that parameter in the query. (suggestion)</p>
"
"55082120","Query by date in Azure Data Factory Pipeline","<p>I want to use a query in a <strong>copy job</strong> for my source in an Azure Data Factory pipeline <strong>together with a date function</strong> - here is the dummy query:</p>

<p>SELECT * FROM public.report_campaign_leaflet WHERE day=""<strong><em>{today - 1d}</em></strong>""</p>

<p>I´ve found some documentation about dynamic content and some other stuff but no information on how to use date functions directly in a sql query.</p>

<p>Maybe someone has a hint for me?</p>

<p>Thanks &amp; best,
Michael</p>
","<azure-data-factory>","2019-03-09 21:26:42","3894","2","3","56242019","<p>You need to replace the double quote (<code>""</code>) with two single quotes (<code>''</code>):</p>

<pre><code>@concat('SELECT * FROM public.report_campaign_leaflet WHERE day=','''',formatDateTime(adddays(utcnow(),-1), 'yyyy-MM-dd'),'''')
</code></pre>
"
"55082120","Query by date in Azure Data Factory Pipeline","<p>I want to use a query in a <strong>copy job</strong> for my source in an Azure Data Factory pipeline <strong>together with a date function</strong> - here is the dummy query:</p>

<p>SELECT * FROM public.report_campaign_leaflet WHERE day=""<strong><em>{today - 1d}</em></strong>""</p>

<p>I´ve found some documentation about dynamic content and some other stuff but no information on how to use date functions directly in a sql query.</p>

<p>Maybe someone has a hint for me?</p>

<p>Thanks &amp; best,
Michael</p>
","<azure-data-factory>","2019-03-09 21:26:42","3894","2","3","76063443","<p>I had the same issue I have sorted it out via the following method:</p>
<ol>
<li>Create a Set variable activity (Let's say the name - X)</li>
</ol>
<p><a href=""https://i.stack.imgur.com/gxvP6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gxvP6.png"" alt=""enter image description here"" /></a></p>
<pre><code>@formatDateTime('2022-01-01 00:00:00', 'yyyy-MM-dd HH:mm:ss')
</code></pre>
<ol start=""2"">
<li><p>Write the following Code to the pipeline expression builder:</p>
<p>@{concat('select * from Table where Column1=''','A',''' AND TO_CHAR(Date, ''', 'YYYY-MM-DD HH24:MI:SS',''') &gt;= ''', variables('X'),'''')}</p>
</li>
</ol>
<p>This will do the trick!</p>
"
"55080231","Azure data factory not loading","<p>I am not able to open Azure data factory in any browser, it just keep loading from past 1 hour. I have tried refreshing and using other browser, is there any specific reason why it happens? All other services on azure portal is working fine.</p>

<p>Browsers which I have tried:</p>

<ul>
<li>Mozilla Firefox 65.0.2</li>
<li>Internet explorer 9</li>
<li>Microsoft Edge 42</li>
<li>Opera Latest</li>
</ul>

<p>For all of the above browsers all services are fine but when I click on ""Author &amp; Monitor"", it opens up a new tab and keep loading.</p>

<p><a href=""https://i.stack.imgur.com/bdNDn.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/bdNDn.png"" alt=""enter image description here""></a></p>
","<azure><browser><azure-data-factory>","2019-03-09 17:46:37","6568","6","5","55098245","<p>I often experience this issue but not always.The azure data factory UI keep loading or  keep asking me to re-enter my account information. I fixed it by using <code>incognito</code> mode in the browser.</p>

<p>You could find many clues from this <a href=""https://social.msdn.microsoft.com/Forums/en-US/26dd2244-efa2-4225-a32f-2f68d3a60139/data-factory-ui-is-not-loading?forum=AzureDataFactory"" rel=""noreferrer"">MSDN case</a>. Also,try Chrome browser according to this case:<a href=""https://stackoverflow.com/questions/50988754/azure-data-factory-v2-portal-is-slow"">Azure Data Factory v2 portal is slow</a></p>
"
"55080231","Azure data factory not loading","<p>I am not able to open Azure data factory in any browser, it just keep loading from past 1 hour. I have tried refreshing and using other browser, is there any specific reason why it happens? All other services on azure portal is working fine.</p>

<p>Browsers which I have tried:</p>

<ul>
<li>Mozilla Firefox 65.0.2</li>
<li>Internet explorer 9</li>
<li>Microsoft Edge 42</li>
<li>Opera Latest</li>
</ul>

<p>For all of the above browsers all services are fine but when I click on ""Author &amp; Monitor"", it opens up a new tab and keep loading.</p>

<p><a href=""https://i.stack.imgur.com/bdNDn.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/bdNDn.png"" alt=""enter image description here""></a></p>
","<azure><browser><azure-data-factory>","2019-03-09 17:46:37","6568","6","5","55104418","<p>In order to access ADF portal you should use Google Chrome as at this moment other browsers does not support this portal.</p>

<p>See the response for similar issue: <a href=""https://social.msdn.microsoft.com/Forums/en-US/26dd2244-efa2-4225-a32f-2f68d3a60139/data-factory-ui-is-not-loading"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/en-US/26dd2244-efa2-4225-a32f-2f68d3a60139/data-factory-ui-is-not-loading</a> .</p>
"
"55080231","Azure data factory not loading","<p>I am not able to open Azure data factory in any browser, it just keep loading from past 1 hour. I have tried refreshing and using other browser, is there any specific reason why it happens? All other services on azure portal is working fine.</p>

<p>Browsers which I have tried:</p>

<ul>
<li>Mozilla Firefox 65.0.2</li>
<li>Internet explorer 9</li>
<li>Microsoft Edge 42</li>
<li>Opera Latest</li>
</ul>

<p>For all of the above browsers all services are fine but when I click on ""Author &amp; Monitor"", it opens up a new tab and keep loading.</p>

<p><a href=""https://i.stack.imgur.com/bdNDn.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/bdNDn.png"" alt=""enter image description here""></a></p>
","<azure><browser><azure-data-factory>","2019-03-09 17:46:37","6568","6","5","55131253","<p>Thank you that worked - using firefox vs IE solved it.  I don't wish to use Chrome.</p>
"
"55080231","Azure data factory not loading","<p>I am not able to open Azure data factory in any browser, it just keep loading from past 1 hour. I have tried refreshing and using other browser, is there any specific reason why it happens? All other services on azure portal is working fine.</p>

<p>Browsers which I have tried:</p>

<ul>
<li>Mozilla Firefox 65.0.2</li>
<li>Internet explorer 9</li>
<li>Microsoft Edge 42</li>
<li>Opera Latest</li>
</ul>

<p>For all of the above browsers all services are fine but when I click on ""Author &amp; Monitor"", it opens up a new tab and keep loading.</p>

<p><a href=""https://i.stack.imgur.com/bdNDn.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/bdNDn.png"" alt=""enter image description here""></a></p>
","<azure><browser><azure-data-factory>","2019-03-09 17:46:37","6568","6","5","57009199","<p>I had this problem today too, I fixed it just by logging out from another account that I had log in moments before</p>
"
"55080231","Azure data factory not loading","<p>I am not able to open Azure data factory in any browser, it just keep loading from past 1 hour. I have tried refreshing and using other browser, is there any specific reason why it happens? All other services on azure portal is working fine.</p>

<p>Browsers which I have tried:</p>

<ul>
<li>Mozilla Firefox 65.0.2</li>
<li>Internet explorer 9</li>
<li>Microsoft Edge 42</li>
<li>Opera Latest</li>
</ul>

<p>For all of the above browsers all services are fine but when I click on ""Author &amp; Monitor"", it opens up a new tab and keep loading.</p>

<p><a href=""https://i.stack.imgur.com/bdNDn.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/bdNDn.png"" alt=""enter image description here""></a></p>
","<azure><browser><azure-data-factory>","2019-03-09 17:46:37","6568","6","5","72338913","<p>This is the official solution: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-factory-ux-troubleshoot-guide?tabs=edge#azure-data-factory-studio-fails-to-load"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-factory-ux-troubleshoot-guide?tabs=edge#azure-data-factory-studio-fails-to-load</a></p>
<p>Pay attention to the third party cookie settings.</p>
<p>ADF officially supports both Chrome and Edge. So you don't have to use Chrome. Firefox actually also works.</p>
"
"55059751","Data Factory V2 Query Azure Table Storage but use a lookup Value","<p>I have a SQL watermark table which contains the last date in my destination table</p>

<p>My source data is coming from an Azure Storage Table and the date time is a string</p>

<p>I set up the date time in the watermark table to match the format in the Azure table storage</p>

<p>I create a lookup and a copy task</p>

<p><img src=""https://i.stack.imgur.com/NGsTC.jpg"" alt=""enter image description here""></p>

<p>If I hard code the date into the Query for source and run this works fine CreatedAt ge '2019-03-06T14:03:11.000Z'</p>

<p>But obviously I dont want to hard code this value. I want to use the date from the lookup</p>

<p>But when I replace the hardcoded date with the lookup value </p>

<p>CreatedAt ge 'activity('LookupWatermarkOld').output'</p>

<p>I get an error</p>

<blockquote>
  <p>{<br>
  ""errorCode"": ""2200"",<br>
  ""message"":""ErrorCode=FailedStorageOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A
   storage operation failed with the following error 'The remote server returned an error: (400) Bad Request.'.,Source=,
    ''Type=Microsoft.WindowsAzure.Storage.StorageException,Message=The remote server returned an error: (400) Bad Request.,
    Source=Microsoft.WindowsAzure.Storage,StorageExtendedMessage=Syntax
   error at position 42 in 'CreatedAt ge 'activity('LookupWatermarkOld').output''.\nRequestId:8c65ced9-b002-0051-79d9-d41d49000000\nTime:2019-03-07T11:35:39.0640233Z,,''Type=System.Net.WebException,Message=The remote server returned an error: (400) Bad Request.,Source=Microsoft.WindowsAzure.Storage,'"",<br>
  ""failureType"": ""UserError"",<br>
  ""target"": ""CopyMentions""<br>
  }</p>
</blockquote>

<p>Can anyone help me with this? How do you use the Lookup value in a Azure Table query?</p>
","<azure><nosql><azure-table-storage><lookup-tables><azure-data-factory>","2019-03-08 09:00:14","1686","0","3","55063437","<p>check this out:</p>

<p>1) Lookup activity. Query field:</p>

<blockquote>
  <p>SELECT MAX(WatermarkColumnName) as LastId FROM TableName;</p>
</blockquote>

<p>Also, make sure that you checked ""First row only"" option.</p>

<p>2) In Copy Data activity use query. Query field:</p>

<blockquote>
  <p>@concat('SELECT * FROM TableName as s WHERE s.WatermarkColumnName > ''', activity('LookupActivity').output.firstRow.LastID, '''')</p>
</blockquote>
"
"55059751","Data Factory V2 Query Azure Table Storage but use a lookup Value","<p>I have a SQL watermark table which contains the last date in my destination table</p>

<p>My source data is coming from an Azure Storage Table and the date time is a string</p>

<p>I set up the date time in the watermark table to match the format in the Azure table storage</p>

<p>I create a lookup and a copy task</p>

<p><img src=""https://i.stack.imgur.com/NGsTC.jpg"" alt=""enter image description here""></p>

<p>If I hard code the date into the Query for source and run this works fine CreatedAt ge '2019-03-06T14:03:11.000Z'</p>

<p>But obviously I dont want to hard code this value. I want to use the date from the lookup</p>

<p>But when I replace the hardcoded date with the lookup value </p>

<p>CreatedAt ge 'activity('LookupWatermarkOld').output'</p>

<p>I get an error</p>

<blockquote>
  <p>{<br>
  ""errorCode"": ""2200"",<br>
  ""message"":""ErrorCode=FailedStorageOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A
   storage operation failed with the following error 'The remote server returned an error: (400) Bad Request.'.,Source=,
    ''Type=Microsoft.WindowsAzure.Storage.StorageException,Message=The remote server returned an error: (400) Bad Request.,
    Source=Microsoft.WindowsAzure.Storage,StorageExtendedMessage=Syntax
   error at position 42 in 'CreatedAt ge 'activity('LookupWatermarkOld').output''.\nRequestId:8c65ced9-b002-0051-79d9-d41d49000000\nTime:2019-03-07T11:35:39.0640233Z,,''Type=System.Net.WebException,Message=The remote server returned an error: (400) Bad Request.,Source=Microsoft.WindowsAzure.Storage,'"",<br>
  ""failureType"": ""UserError"",<br>
  ""target"": ""CopyMentions""<br>
  }</p>
</blockquote>

<p>Can anyone help me with this? How do you use the Lookup value in a Azure Table query?</p>
","<azure><nosql><azure-table-storage><lookup-tables><azure-data-factory>","2019-03-08 09:00:14","1686","0","3","55217611","<p>Finally I got some help on this and it works with</p>

<p>CreatedAt gt '@{activity('LookupWatermarkOld').output.firstRow.WaterMarkValue}'</p>

<p>the WaterarkValue is the column name from the SQL Lookup table</p>

<p>The Lookup creates an array so you have to specify the FirstRow from this array</p>

<p>And wrap in '' so its used as a string value</p>
"
"55059751","Data Factory V2 Query Azure Table Storage but use a lookup Value","<p>I have a SQL watermark table which contains the last date in my destination table</p>

<p>My source data is coming from an Azure Storage Table and the date time is a string</p>

<p>I set up the date time in the watermark table to match the format in the Azure table storage</p>

<p>I create a lookup and a copy task</p>

<p><img src=""https://i.stack.imgur.com/NGsTC.jpg"" alt=""enter image description here""></p>

<p>If I hard code the date into the Query for source and run this works fine CreatedAt ge '2019-03-06T14:03:11.000Z'</p>

<p>But obviously I dont want to hard code this value. I want to use the date from the lookup</p>

<p>But when I replace the hardcoded date with the lookup value </p>

<p>CreatedAt ge 'activity('LookupWatermarkOld').output'</p>

<p>I get an error</p>

<blockquote>
  <p>{<br>
  ""errorCode"": ""2200"",<br>
  ""message"":""ErrorCode=FailedStorageOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A
   storage operation failed with the following error 'The remote server returned an error: (400) Bad Request.'.,Source=,
    ''Type=Microsoft.WindowsAzure.Storage.StorageException,Message=The remote server returned an error: (400) Bad Request.,
    Source=Microsoft.WindowsAzure.Storage,StorageExtendedMessage=Syntax
   error at position 42 in 'CreatedAt ge 'activity('LookupWatermarkOld').output''.\nRequestId:8c65ced9-b002-0051-79d9-d41d49000000\nTime:2019-03-07T11:35:39.0640233Z,,''Type=System.Net.WebException,Message=The remote server returned an error: (400) Bad Request.,Source=Microsoft.WindowsAzure.Storage,'"",<br>
  ""failureType"": ""UserError"",<br>
  ""target"": ""CopyMentions""<br>
  }</p>
</blockquote>

<p>Can anyone help me with this? How do you use the Lookup value in a Azure Table query?</p>
","<azure><nosql><azure-table-storage><lookup-tables><azure-data-factory>","2019-03-08 09:00:14","1686","0","3","68466443","<p><strong>--For recent ADFv2</strong></p>
<p>Use the watermark/lookup/output  value in parameter.</p>
<p><strong>Example:</strong> <code>ParamUserCount = @{activity('LookupActivity').output.count}</code></p>
<p>or for <strong>output</strong> function</p>
<p><a href=""https://i.stack.imgur.com/opFWS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/opFWS.png"" alt=""https://i.stack.imgur.com/opFWS.png"" /></a></p>
<p>and you can use it in query as</p>
<p><strong>Example:</strong> <code>&quot;select * from userDetails where usercount = {$ParamUserCount}&quot;</code></p>
<p>make sure you enclose the query in &quot; &quot; to set as string and parameter in query should be enclosed in { }</p>
<p><a href=""https://i.stack.imgur.com/K1cJo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K1cJo.png"" alt=""enter image description here"" /></a></p>
"
"55058572","Azure SSIS IR - working with files in the temp folder of the IR node","<p>I have setup a custom SSIS IR, however I'm having problems reading files from the current working directory or temp folder on the IR node</p>

<p><a href=""https://learn.microsoft.com/en-us/sql/integration-services/lift-shift/ssis-azure-files-file-shares?view=sql-server-2017"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sql/integration-services/lift-shift/ssis-azure-files-file-shares?view=sql-server-2017</a></p>

<p>The work flow of my test package is </p>

<ul>
<li>Load compressed file to Azure file share</li>
<li>Unzip file</li>
<li>Modify file, saving it the current working group folder on the IR node (this path .\testfile.json)</li>
<li>Load file to Azure SQL DB</li>
</ul>

<p>The last step is where I'm having issues, I receive the below error message. Maybe looks to be related to security, but no idea how to access the SSIS IR node to check this.</p>

<blockquote>
  <p>Execute SQL Task:Error: Executing the query ""DECLARE @request
  VARCHAR(MAX)  SELECT @request =..."" failed with the following error:
  ""Cannot bulk load because the file "".\testfile.json"" could not be
  opened. Operating system error code (null)."". Possible failure
  reasons: Problems with the query, ""ResultSet"" property not set
  correctly, parameters not set correctly, or connection not established
  correctly.</p>
</blockquote>

<p>How can I fix this issue?</p>
","<azure><azure-sql-database><azure-data-factory>","2019-03-08 07:27:15","375","1","1","55117059","<p>From just the error message, looks like you're using BULK INSERT in Execute SQL Task to load data into Azure SQL DB.  BULK INSERT into Azure SQL DB can only work from Azure Storage Blob, but not from file systems/SSIS IR nodes.  To load data from the current working directory of SSIS IR nodes into Azure SQL DB, you can use a Data Flow with Flat File Source and ADO.NET Destination.</p>
"
"55050376","How can I exclude rows in a Copy Data Activity in Azure Data Factory?","<p>I have built an Pipeline with one Copy Data activity which copies data from an <code>Azure Data Lake</code> and output it to an <code>Azure Blob Storage</code>. </p>

<p>In the output, I can see that some of my rows do not have data and I would like to exclude them from the copy. In the following example, the 2nd row does not have useful data:</p>

<pre><code>{""TenantId"":""qa"",""Timestamp"":""2019-03-06T10:53:51.634Z"",""PrincipalId"":2,""ControlId"":""729c3b6e-0442-4884-936c-c36c9b466e9d"",""ZoneInternalId"":0,""IsAuthorized"":true,""PrincipalName"":""John"",""StreetName"":""Rue 1"",""ExemptionId"":8}
{""TenantId"":""qa"",""Timestamp"":""2019-03-06T10:59:09.74Z"",""PrincipalId"":null,""ControlId"":null,""ZoneInternalId"":null,""IsAuthorized"":null,""PrincipalName"":null,""StreetName"":null,""ExemptionId"":null}
</code></pre>

<p><strong>Question</strong></p>

<p>In the Copy Data activity, how can I put a rule to exclude rows that miss certain values?</p>

<p>Here is the code of my pipeline :</p>

<pre><code>{
    ""name"": ""pipeline1"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""Copy from Data Lake to Blob"",
                ""type"": ""Copy"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""userProperties"": [
                    {
                        ""name"": ""Source"",
                        ""value"": ""tenantdata/events/""
                    },
                    {
                        ""name"": ""Destination"",
                        ""value"": ""controls/""
                    }
                ],
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""AzureDataLakeStoreSource"",
                        ""recursive"": true
                    },
                    ""sink"": {
                        ""type"": ""BlobSink"",
                        ""copyBehavior"": ""MergeFiles""
                    },
                    ""enableStaging"": false,
                    ""translator"": {
                        ""type"": ""TabularTranslator"",
                        ""columnMappings"": {
                            ""Body.TenantId"": ""TenantId"",
                            ""Timestamp"": ""Timestamp"",
                            ""Body.PrincipalId"": ""PrincipalId"",
                            ""Body.ControlId"": ""ControlId"",
                            ""Body.ZoneId"": ""ZoneInternalId"",
                            ""Body.IsAuthorized"": ""IsAuthorized"",
                            ""Body.PrincipalName"": ""PrincipalName"",
                            ""Body.StreetName"": ""StreetName"",
                            ""Body.Exemption.Kind"": ""ExemptionId""
                        }
                    }
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""qadl"",
                        ""type"": ""DatasetReference""
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""datalakestaging"",
                        ""type"": ""DatasetReference""
                    }
                ]
            }
        ]
    }
}
</code></pre>
","<azure><azure-data-factory>","2019-03-07 18:17:20","3769","3","1","55105313","<p>This is a very good question (+1 for that), I had the same question months back and I was surprised that I could not find anything within the Copy Activity to handle this (I even tried with the fault tolerance feature but no luck).</p>

<p>And given that I had other transformations going on in my pipelines with <a href=""https://learn.microsoft.com/en-us/u-sql/"" rel=""nofollow noreferrer"">U-SQL</a>, I ended up using it to accomplish this. So, instead of a Copy Activity I have a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-data-lake-analytics"" rel=""nofollow noreferrer"">U-SQL Activity</a> in ADF using the <a href=""https://learn.microsoft.com/en-us/u-sql/operators/comparison/is-null"" rel=""nofollow noreferrer"">IS NOT NULL</a> operator, it depends on your data but you can play with that, maybe your string contains the ""NULL"" or empty strings """", this is how it looks :</p>

<pre><code>DECLARE @file_set_path string = ""adl://myadl.azuredatalake.net/Samples/Data/{date_utc:yyyy}{date_utc:MM}{date_utc:dd}T{date_utc:HH}{date_utc:mm}{date_utc:ss}Z.txt"";

@data =
    EXTRACT 
            [id] string,
            date_utc DateTime
    FROM @file_set_path
    USING Extractors.Text(delimiter: '\u0001', skipFirstNRows : 1, quoting:false);

@result =
    SELECT 

            [id] ,
            date_utc.ToString(""yyyy-MM-ddTHH:mm:ss"") AS SourceExtractDateUTC
    FROM @data
    WHERE id IS NOT NULL -- you can also use WHERE id &lt;&gt; """" or &lt;&gt; ""NULL"";

OUTPUT @result TO ""wasb://samples@mywasb/Samples/Data/searchlog.tsv"" USING Outputters.Text(delimiter: '\u0001', outputHeader:true);
</code></pre>

<p>Notes: ADLS and Blob storage are supported <a href=""https://learn.microsoft.com/en-us/u-sql/Concepts/files/input-files#registerWABS"" rel=""nofollow noreferrer"">INPUT/OUTPUT files</a></p>

<p>Let me know if that helps or if the example above does not work for your data. 
Hopefully somebody will post an answer using Copy Activity and that'd be awesome but this is one possibility so far.</p>
"
"55046031","Data From REST API In Azure","<p>I have implemented REST API calls using a standalone c# console application. The API returns JSON which i'm deserializing and then storing it in the database. 
Now i want to implement the entire logic in Azure platform so that it can invoked by passing start date and an end date and store location (it should run for three location) Below is the code:</p>

<pre><code>static void Main()
    {


        MakeInventoryRequest();

    }

    static async void MakeInventoryRequest()
    {
        using (var client = new HttpClient())
        {
            var queryString = HttpUtility.ParseQueryString(string.Empty);

            // Request headers
            client.DefaultRequestHeaders.Add(""Ocp-Apim-Subscription-Key"", ""5051fx6yyy124hhfyuscf34f57ce9"");


            // Request parameters

            queryString[""query.locationNumbers""] = ""4638"";
            queryString[""availableFromDate""] = ""2019-01-01"";
            queryString[""availableToDate""] = ""2019-03-07"";


            var uri = ""https://api-test.location.cloud/api/v1/inventory?"" + queryString;

            using (var request = new HttpRequestMessage(HttpMethod.Get, uri))
            using (var response = await client.SendAsync(request))
            {

                var stream = await response.Content.ReadAsStreamAsync();

                if (response.IsSuccessStatusCode == true)
                {
                    List&lt;Inventory&gt; l1 = DeserializeJsonFromStream&lt;List&lt;Inventory&gt;&gt;(stream);

                    InsertInventoryRecords(l1);
                }


                if (response.IsSuccessStatusCode == false)
                {
                    throw new Exception(""Error Response Code: "" + response.StatusCode.ToString() + ""Content is: "" + response.Content.ReadAsStringAsync().Result.ToString());
                }
            }
        }
    }
</code></pre>

<p>Please suggest the best possible design using Azure components</p>
","<azure><azure-functions><azure-data-factory>","2019-03-07 14:20:42","86","0","1","55115773","<p>With the information in hand I think you have multiple options , you need to find out which works for you the best . You can use Cloud service to  host the console app ( you will have to change it to worker role , Visual studio will help you to convert that ) . I am not sure about the load which you are expecting but you can always increase and decrease the instance and these can be deployed to different geographies . </p>

<p>I see that you are persisting the data , if you want to do that you can use many of the SQL offerings . For invoking the REST API you can also azure functions and ADF.</p>

<p>Please feel free to comment if you want any more details on the same.</p>
"
"55045246","How to use Azure Data Factory IF Activity?","<p><a href=""https://i.stack.imgur.com/5nmRD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5nmRD.png"" alt=""enter image description here""></a></p>

<p>I am getting value cnt=1 from my query in LookUp Activity.</p>

<p>Now I want to check if cnt value is 1 in IF activity then run another activity.</p>
","<azure><azure-data-factory>","2019-03-07 13:41:07","36","0","1","55045902","<p>Finally, I found the solution.</p>

<p>Below is the working code for my problem:</p>

<pre><code>@equals(activity('Lookup1').output.firstRow.cnt,1)
</code></pre>
"
"55040944","How to skip already copied files in Azure data factory, copy data tool?","<p>I want to copy data from blob storage(parquet format) to cosmos db. Scheduled the trigger for every one hour. But all the files/data getting copied in every run. how to skip the files that are already copied? </p>

<p>There is no unique key with the data. We should not copy the same file content again.</p>
","<azure><azure-cosmosdb><azure-blob-storage><azure-data-factory>","2019-03-07 10:02:10","1133","1","1","55055877","<p>Based on your requirements, you could get an idea of <code>modifiedDatetimeStart</code> and <code>modifiedDatetimeEnd</code> properties in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage#dataset-properties"" rel=""nofollow noreferrer"">Blob Storage DataSet properties</a>.</p>

<p><a href=""https://i.stack.imgur.com/wKsQ2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wKsQ2.png"" alt=""enter image description here""></a></p>

<p>But you need to modify the configuration of dataset every period of time via <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-dot-net#create-a-dataset"" rel=""nofollow noreferrer"">sdk</a> to push the value of the properties move on.</p>

<p>Another two solutions you could consider :</p>

<p>1.Using <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-triggered-function"" rel=""nofollow noreferrer"">Blob Trigger Azure Function</a>.It could be triggered if any modifications on the blob files then you could transfer data from blob to cosmos db by sdk code.</p>

<p>2.Using <a href=""https://azure.microsoft.com/en-us/services/stream-analytics/"" rel=""nofollow noreferrer"">Azure Stream Analytics</a>.You could configure the <a href=""https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-inputs#stream-data-from-blob-storage"" rel=""nofollow noreferrer"">input as Blob Storage</a> and <a href=""https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-documentdb-output"" rel=""nofollow noreferrer"">output as Cosmos DB</a>.</p>
"
"55031758","Azure Data Factory Compress Several Files to one single zip file","<p>Currently, we are working on a project that needs compress several files from Azure Blob. </p>

<p>For example,</p>

<pre><code>ParentFolder
   File1
   File2
   ChildFolder
       File3
       File4
</code></pre>

<p>and the result we want is</p>

<pre><code>ParentFolder.zip
</code></pre>

<p>Now, we noticed that Azure Data Factory can copy the data and compress it. But no matter what setting we chose, the best result we can get is to get all the file compressed seperately. And the compression type does not contain zip.</p>

<p>Like:</p>

<pre><code>ParentFolder
     File1.gz
     File2.gz
     SubFolder
        File3.gz
        File4.gz
</code></pre>

<p>Is there any way to compress only the parent folder? Or is there any other service that we can give it a try?</p>
","<azure><azure-data-factory>","2019-03-06 20:37:16","2168","0","1","55059994","<p>It seems that you are using the GZip compression type, so that all files are compressed as *.gz. Please use the ZipDeflate compression type instead. Refer to this doc: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#compression-support"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#compression-support</a>.</p>
"
"55025109","Bad Request error while creating Alerts for PipelineFailedRuns using Powershell on Azure","<p>I am using the inline scripting option to create an Alert for AzureDataFactory V2. </p>

<p>The objective is to send an automated mail when the number of pipelines failed is greater than or equal to 1.  </p>

<pre><code>Add-AzureRmMetricAlertRule 
-Name ""SS Rule"" 
-Location ""East US""
-ResourceGroup ""RGname"" 
-Operator GreaterThanOrEqual 
-Threshold 1 
-TargetResourceId ""/subscriptions/subid/resourceGroups/RGname/providers/Microsoft.DataFactory/factories/DFname"" 
-MetricName ""PipelineFailedRuns"" 
-TimeAggregationOperator Total
</code></pre>

<p>I get the following error when I attempt to run this create alert command-</p>

<blockquote>
  <p>[error]Exception type: ErrorResponseException, Message: The target
  resource id
  '/subscriptions/subid/resourceGroups/rgname/providers/Microsoft.DataFactory/factories/'
  is not supported., Code: ResourceNotSupported, Status code:BadRequest,
  Reason phrase: Bad Request</p>
</blockquote>
","<powershell><alert><monitor><azure-data-factory>","2019-03-06 14:11:55","453","0","1","55032808","<p>That error is pretty specific. It's not a PowerShell issue.</p>

<p>What you are doing is not valid as far as Azure is concerned. <a href=""https://learn.microsoft.com/en-us/powershell/module/azurerm.insights/add-azurermmetricalertrule?view=azurermps-6.13.0"" rel=""nofollow noreferrer"">The help file examples shows...</a></p>

<pre><code>Example 1: Add a metric alert rule to a website
    PS C:\&gt;Add-AzureRMMetricAlertRule -Name ""metricRule5"" -Location ""East US"" -ResourceGroup ""Default-Web-EastUS"" -Operator GreaterThan -Threshold 2 -WindowSize 00:05:00 -MetricName ""Requests"" -Description ""Pura Vida"" -TimeAggregationOperator Total
    RequestId                                                                                                    StatusCode
    ---------                                                                                                    ----------
    33574ccf-0b01-43b4-aa97-87e6bbcf1c11 



Example 3: Add a rule with actions

PS C:\&gt;Add-AzureRmMetricAlertRule -Name ""metricRule5"" -Location ""East US"" -ResourceGroup ""Default-Web-EastUS"" -Operator GreaterThan -Threshold 1 -TargetResourceId ""/subscriptions/b93fb07a-6f93-30be-bf3e-4f0deca15f4f/resourceGroups/Default-Web-EastUS/providers/microsoft.web/sites/mywebsite"" -MetricName ""Requests"" -TimeAggregationOperator Total
RequestId                                                                                                    StatusCode
---------                                                                                                    ----------
9a5bc388-c7ac-4dc6-aa70-f4bc29c2c712                                                                                 OK
</code></pre>

<p>So, though these are all on one line you can format them so that they are more readable.</p>

<p>Yet, your post format, you may have done that to make it readable for us. If you did that in you script, then that is wrong. Hence the errors. With your post, that can't be done without using the backtick mark in the formatting ---</p>

<pre><code>Add-AzureRmMetricAlertRule -Name ""SS Rule"" `
-Location ""East US"" `
-ResourceGroup ""RGname"" `
-Operator GreaterThanOrEqual `
-Threshold 1 `
-TargetResourceId ""/subscriptions/subid/resourceGroups/RGname/providers/Microsoft.DataFactory/factories/DFname"" `
-MetricName ""PipelineFailedRuns"" `
-TimeAggregationOperator Total
</code></pre>

<p>--- (many frown on that, I don't have any issues with it) or use splatting.</p>

<pre><code>$addAzureRmMetricAlertRuleSplat = @{
    MetricName = ""PipelineFailedRuns""
    TimeAggregationOperator = 'Total'
    ResourceGroupName = ""RGname""
    Operator = 'GreaterThanOrEqual'
    Name = ""SS Rule""
    Threshold = 1
    Location = ""East US""
    TargetResourceId = ""/subscriptions/subid/resourceGroups/RGname/providers/Microsoft.DataFactory/factories/DFname""
}
Add-AzureRmMetricAlertRule @addAzureRmMetricAlertRuleSplat
</code></pre>
"
"55019457","How to perform Event based data ingestion using Azure Data Lake Storage Gen2 and Azure Data factory V2?","<p>Recently we came across a scenario where our source and sink location are of ADLS Gen2 type. Now we got one interesting use case wherein we have to push data from source to sink with the help of ADF V2. Having said that, its not just normal copy activity we are expecting but we need to perform this activity on an event basis.</p>

<p>While going through the ADLS Gen2 documents found that ADLS Gen2 yet to support ""Azure Event Grids"" and that's the reason though we are able to configure ADF's event-based triggers they did not work.</p>

<p>Can anyone suggest me to tackle this situation, since Azure Event Gird is not supported at this instance of time we don't believe we can achieve this with Azure Event Hubs and their integration with ADF?</p>

<p>Thanks.</p>
","<azure><azure-storage><azure-data-lake><azure-data-factory>","2019-03-06 09:18:20","821","1","1","55039376","<p>From my repro, currently event based trigger are supported only on v2 storage accounts.</p>

<p>Data Factory is now integrated with Azure Event Grid, which lets you trigger pipelines on an event.</p>

<p><strong>Note:</strong> This integration supports only version 2 Storage accounts (General purpose).</p>

<p>Azure Event Grid doesn't receive events from Azure Data Lake Gen2 accounts because those accounts don't yet generate them.</p>

<p>For more details, refer “<a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-known-issues"" rel=""nofollow noreferrer"">Known issues with Azure Data Lake Storage Gen2</a>”.</p>
"
"55015415","Run ADF pipeline without assigning 'Contributor' role at subscription level","<p>Appreciate for your advice if there is other approach i could try out..</p>

<p>I would like to run the ADF pipeline via RestFul API / .Net SDK and I have followed through the Microsoft tutorial for this.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-rest-api#authenticate-with-azure-ad"" rel=""nofollow noreferrer"">Microsoft Run ADF Pipeline link via Restful / SDK</a></p>

<p>Understand that Contributor role has to be assigned to the application (Subscription level).
However, assignment of contributor at subscription level is not allowed at my Client environment due to the security policy. Contributor role entitlement only allowed at resource group level.</p>

<p>Hence, may i have your advice if there is any other approach i could run ADF pipeline without assignment of contributor role at subscription level?</p>

<p>Below is the error message i gotten from my c# program:</p>

<pre><code>System.AggregateException
HResult=0x80131500
Message=One or more errors occurred.
Source=mscorlib
StackTrace:
at System.Threading.Tasks.Task.ThrowIfExceptional(Boolean 
includeTaskCanceledExceptions)
at System.Threading.Tasks.Task`1.GetResultCore(Boolean 
waitCompletionNotification)
at System.Threading.Tasks.Task`1.get_Result()
at ADFv2QuickStart.Program.Main(String[] args) in 
C:\Users\ADFv2QuickStart\Program.cs:line 59
Inner Exception 1:
CloudException: The client 'xxxxxx' with 
object id 'xxxxxx' does not have authorization 
to perform action 
'Microsoft.DataFactory/factories/pipelines/createRun/action' over scope 
'/subscriptions/xxxxxxxx/resourceGroups/rg-xxx/providers/Microsoft.DataFactory/factories/adf-xxxx/pipelines/pipeline12'.
</code></pre>
","<azure><azure-data-factory><azure-rbac>","2019-03-06 04:08:45","876","1","1","55017017","<p>The Contributor role at the resource group level is enough, I start a run of a pipeline via powershell, it works fine. The command essentially calls the REST API : <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/createrun"" rel=""nofollow noreferrer"">Pipelines - Create Run</a>, so you will also be able to invoke the REST API directly.</p>

<pre><code>Invoke-AzDataFactoryV2Pipeline -ResourceGroupName joywebapp -DataFactoryName joyfactoryv2 -PipelineName pipeline1
Get-AzDataFactoryV2PipelineRun -ResourceGroupName joywebapp -DataFactoryName joyfactoryv2 -PipelineRunId ""xxxxxxx""
</code></pre>

<p><a href=""https://i.stack.imgur.com/5zBP2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5zBP2.png"" alt=""enter image description here""></a></p>

<p>In the screenshot, you will find if I get the web app in another resource group, it will give the error, so we can make sure the role is working. <code>Invoke-AzDataFactoryV2Pipeline</code> will return the <code>RunId</code>, just pass it in the <code>Get-AzDataFactoryV2PipelineRun</code>, you can get the details of the PipelineRun.</p>

<p>Catch the request, we could find what I mentioned above, it calls the REST API. It is the same with the one in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-rest-api#create-pipeline-run"" rel=""nofollow noreferrer"">link</a> in your question.</p>

<p><a href=""https://i.stack.imgur.com/bdesY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bdesY.png"" alt=""enter image description here""></a></p>
"
"55006050","Create Blobs with Filename and Data from SQL Rows","<p>There is plenty of documentation on how to use Azure Data Factory to read data from blobs into SQL, and even documentation on how to dump the output of a query to a single blob. I'm trying to create one blob for each row in a table (on Azure SQL Server), named by one field and containing the data in another.</p>

<p>My table has a GUID <code>id</code> field and a nvarchar(max) <code>data</code> field (which contains JSON, though that's largely irrelevant). Suppose I have the following rows:</p>

<pre><code>                 id                    |  data
---------------------------------------+----------
38b2f551-5f13-40ce-8512-c108a05ecd44   |  foo
4db5b25b-1194-44e9-a7b2-bc8889c32979   |  bar
2a3bd653-ce14-4bd2-9243-6923e97224c6   |  baz
</code></pre>

<p>I want the following blobs to be created:</p>

<pre><code>https://mycontainer.blob.core.windows.net/myblobs/38b2f551-5f13-40ce-8512-c108a05ecd44
https://mycontainer.blob.core.windows.net/myblobs/4db5b25b-1194-44e9-a7b2-bc8889c32979
https://mycontainer.blob.core.windows.net/myblobs/2a3bd653-ce14-4bd2-9243-6923e97224c6
</code></pre>

<p>Their contents should be the associated data field, i.e. <code>foo</code>, <code>bar</code>, and <code>baz</code>, respectively.</p>

<p><a href=""https://stackoverflow.com/questions/49554806/data-factory-v2-generate-a-json-file-per-row"">Data Factory v2 - Generate a json file per row</a> has an answer that will work, but it involves querying the database once to get all the <code>id</code>s, then <strong>N</strong> more times to get the data from each row. It seems like it should be possible to query just once for both fields and use one for filename and one for contents, but I haven't been able to figure out how.</p>
","<azure-blob-storage><azure-data-factory>","2019-03-05 15:18:00","729","6","3","55116851","<p>After studying the case <a href=""https://stackoverflow.com/questions/49554806/data-factory-v2-generate-a-json-file-per-row"">Data Factory v2 - Generate a json file per row</a>, i suggestion you don't stick to copy activity.Based on your description,you could consider below solution:</p>

<p>1.Use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">GetMetaData Avtivity</a> which can be used to retrieve metadata of any data in Azure Data Factory.You could get all your blob files' metadata by using this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity#get-a-folders-metadata"" rel=""nofollow noreferrer"">code</a>.</p>

<p>2.Use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">ForEach Activity</a> to loop the file names.</p>

<p>3.In ForEach Activity,run <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure Function Activity</a> to call <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook"" rel=""nofollow noreferrer"">Http Trigger Azure Function</a>. Pass the file name to the Azure Function as parameter. In the function method, please use <a href=""https://learn.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-dotnet?tabs=windows"" rel=""nofollow noreferrer"">azure blob storage sdk</a> to retrieve content by the file name. Then insert the total row into sql database by <a href=""https://learn.microsoft.com/en-us/dotnet/api/overview/azure/sql?view=azure-dotnet"" rel=""nofollow noreferrer"">API</a>.</p>
"
"55006050","Create Blobs with Filename and Data from SQL Rows","<p>There is plenty of documentation on how to use Azure Data Factory to read data from blobs into SQL, and even documentation on how to dump the output of a query to a single blob. I'm trying to create one blob for each row in a table (on Azure SQL Server), named by one field and containing the data in another.</p>

<p>My table has a GUID <code>id</code> field and a nvarchar(max) <code>data</code> field (which contains JSON, though that's largely irrelevant). Suppose I have the following rows:</p>

<pre><code>                 id                    |  data
---------------------------------------+----------
38b2f551-5f13-40ce-8512-c108a05ecd44   |  foo
4db5b25b-1194-44e9-a7b2-bc8889c32979   |  bar
2a3bd653-ce14-4bd2-9243-6923e97224c6   |  baz
</code></pre>

<p>I want the following blobs to be created:</p>

<pre><code>https://mycontainer.blob.core.windows.net/myblobs/38b2f551-5f13-40ce-8512-c108a05ecd44
https://mycontainer.blob.core.windows.net/myblobs/4db5b25b-1194-44e9-a7b2-bc8889c32979
https://mycontainer.blob.core.windows.net/myblobs/2a3bd653-ce14-4bd2-9243-6923e97224c6
</code></pre>

<p>Their contents should be the associated data field, i.e. <code>foo</code>, <code>bar</code>, and <code>baz</code>, respectively.</p>

<p><a href=""https://stackoverflow.com/questions/49554806/data-factory-v2-generate-a-json-file-per-row"">Data Factory v2 - Generate a json file per row</a> has an answer that will work, but it involves querying the database once to get all the <code>id</code>s, then <strong>N</strong> more times to get the data from each row. It seems like it should be possible to query just once for both fields and use one for filename and one for contents, but I haven't been able to figure out how.</p>
","<azure-blob-storage><azure-data-factory>","2019-03-05 15:18:00","729","6","3","55132074","<p>Instead of using the copy activity directly, you could  use a for-each activity to route each row of your sql lookup to a rest api call for the blob storage (<a href=""https://learn.microsoft.com/en-us/rest/api/storageservices/put-blob"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/storageservices/put-blob</a>).  Or some other way of writing.</p>
"
"55006050","Create Blobs with Filename and Data from SQL Rows","<p>There is plenty of documentation on how to use Azure Data Factory to read data from blobs into SQL, and even documentation on how to dump the output of a query to a single blob. I'm trying to create one blob for each row in a table (on Azure SQL Server), named by one field and containing the data in another.</p>

<p>My table has a GUID <code>id</code> field and a nvarchar(max) <code>data</code> field (which contains JSON, though that's largely irrelevant). Suppose I have the following rows:</p>

<pre><code>                 id                    |  data
---------------------------------------+----------
38b2f551-5f13-40ce-8512-c108a05ecd44   |  foo
4db5b25b-1194-44e9-a7b2-bc8889c32979   |  bar
2a3bd653-ce14-4bd2-9243-6923e97224c6   |  baz
</code></pre>

<p>I want the following blobs to be created:</p>

<pre><code>https://mycontainer.blob.core.windows.net/myblobs/38b2f551-5f13-40ce-8512-c108a05ecd44
https://mycontainer.blob.core.windows.net/myblobs/4db5b25b-1194-44e9-a7b2-bc8889c32979
https://mycontainer.blob.core.windows.net/myblobs/2a3bd653-ce14-4bd2-9243-6923e97224c6
</code></pre>

<p>Their contents should be the associated data field, i.e. <code>foo</code>, <code>bar</code>, and <code>baz</code>, respectively.</p>

<p><a href=""https://stackoverflow.com/questions/49554806/data-factory-v2-generate-a-json-file-per-row"">Data Factory v2 - Generate a json file per row</a> has an answer that will work, but it involves querying the database once to get all the <code>id</code>s, then <strong>N</strong> more times to get the data from each row. It seems like it should be possible to query just once for both fields and use one for filename and one for contents, but I haven't been able to figure out how.</p>
","<azure-blob-storage><azure-data-factory>","2019-03-05 15:18:00","729","6","3","68369514","<p>This is a pretty simple feat natively in ADF:</p>
<ol>
<li>Use a Lookup Activity to get the row results of your Database Table</li>
<li>Use a For Each to Iterate over each row from the Lookup</li>
<li>Within the For Each have a Copy Data activity that copies from DB (as a passthrough really) to Blob Storage</li>
</ol>
<p>Here are screenshots with comments to illustrate the exact steps and proof that it works:</p>
<p><strong>1st, here is my mock table that is similar to your example, I have a pretend FileID, and Data field</strong></p>
<p><a href=""https://i.stack.imgur.com/1kja5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1kja5.png"" alt=""Mock Table"" /></a></p>
<p><strong>2nd, I do a lookup on that table, see the details and output below</strong>
<a href=""https://i.stack.imgur.com/pOCuR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pOCuR.png"" alt=""Lookup Activity Example"" /></a></p>
<p><strong>3rd, we put in the For Each activity so that we can perform an action or group of actions per item in the array object</strong></p>
<p><a href=""https://i.stack.imgur.com/wbsmw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wbsmw.png"" alt=""For each Example"" /></a></p>
<p><strong>4th, we go into the For each and create a Copy Data activity with a Source of Azure SQLDB and a Sink of Azure Blob</strong></p>
<p>Source Details:
<a href=""https://i.stack.imgur.com/ZmF39.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZmF39.png"" alt=""Copy Data Source"" /></a></p>
<p>Sink Details:
<a href=""https://i.stack.imgur.com/Wr3dK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Wr3dK.png"" alt=""Copy Data Sink"" /></a></p>
<p><strong>And here is what happens in my Blob Storage when I run this pipeline:</strong></p>
<p><a href=""https://i.stack.imgur.com/skjGw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/skjGw.png"" alt=""Proof it works"" /></a></p>
<p>Let me know if you need anything else or have any questions, hope this helps! Remember to like, love, and follow :)</p>
"
"55005315","update and insert into Azure data warehouse using Azure data factory pipelines","<p>I'm trying to run an adf copy pipeline with and update and insert statements that is supposed to replace merge statement. basically a statement like:</p>

<pre><code>UPDATE TARGET 
SET ProductName = SOURCE.ProductName, 
TARGET.Rate = SOURCE.Rate 
FROM  Products AS TARGET
INNER JOIN UpdatedProducts AS SOURCE 
ON TARGET.ProductID = SOURCE.ProductID
WHERE TARGET.ProductName &lt;&gt; SOURCE.ProductName 
OR TARGET.Rate &lt;&gt; SOURCE.Rate

INSERT Products (ProductID, ProductName, Rate) 
SELECT SOURCE.ProductID, SOURCE.ProductName, SOURCE.Rate
FROM UpdatedProducts AS SOURCE 
WHERE NOT EXISTS
(
SELECT 1
FROM Products 
WHERE ProductID = SOURCE.ProductID
)
</code></pre>

<p>If the target is an azure sql db I would use this way: <a href=""https://www.taygan.co/blog/2018/04/20/upsert-to-azure-sql-db-with-azure-data-factory"" rel=""nofollow noreferrer"">https://www.taygan.co/blog/2018/04/20/upsert-to-azure-sql-db-with-azure-data-factory</a>
but if the target is an adw a stored procedure option doesn't exist! any suggestion? do I have to have a staging table first then I run the update and insert statements from stg_table to target_table? or maybe there is any possibility to do it directly from adf? </p>
","<azure><azure-data-factory><azure-synapse>","2019-03-05 14:40:04","1124","1","1","58515009","<p>If you can't use a stored procedure, my suggestion would be to create a second copy data transform. Run the pre-script on the second transform and drop the table since its a temp table that you created on the first.</p>

<pre><code>BEGIN

MERGE Target AS target_sqldb
USING TempTable AS source_tblstg
ON (target_sqldb.Id= source_tblstg.Id)

WHEN MATCHED THEN
    UPDATE SET
    [Name] = source_tblstg.Name,
    [State] = source_tblstg.State

WHEN NOT MATCHED THEN
    INSERT([Name], [State])
    VALUES (source_tblstg.Name, source_tblstg.State);

DROP TABLE TempTable;

END
</code></pre>
"
"54999689","ADF Copy into SQL table without creating source file","<p>I have a scenario to copy output of GET Metadata activity into a SQL table. Can I do this directly without using Databricks notebook?</p>
","<azure-data-factory>","2019-03-05 09:40:58","86","0","1","55018676","<p>You can make use of look up activity.</p>

<p>GetMetadata -> Lookup</p>

<p>And write insert SQL statement in Query, or use stored procedure.</p>

<p><a href=""https://i.stack.imgur.com/sa07T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sa07T.png"" alt=""enter image description here""></a></p>
"
"54994623","Trying to install Microsoft integration run time on windows 2010","<p>Trying to install Microsoft integration run time on windows 2010  but getting an error </p>

<blockquote>
  <p>service integration runtime service (DIAHostService )failed to start .verify that you have sufficient privilege to start the system services</p>
</blockquote>
","<azure><azure-data-factory>","2019-03-05 02:38:05","629","0","1","55353577","<p><strong>Note:</strong> You should have an entry in “log on as service” that grants the rights to the default service account.</p>

<p>For more details, refer <a href=""https://github.com/MicrosoftDocs/azure-docs/issues/23145"" rel=""nofollow noreferrer"">GitHub</a> issue which addressing similar issue.</p>

<p>Hope this helps.</p>
"
"54991619","How to access the filename from a blob while copying into Azure Data Factory V2","<p>I'm trying to copy a bunch of files from a folder in Azure Data Lake Storage Gen 2 using Data Factory.  I need to access the filenames of the blobs that I am copying at runtime so that I can inspect it and parse the name to retrieve important metadata.  Unfortunately, I do not see a way to do this using Data Factory V2 on Azure.  If anyone knows how to do this it would be greatly appreciated if you shared this with me.</p>
","<azure><azure-data-factory>","2019-03-04 21:11:36","1687","1","1","55037998","<p>You could get an idea of <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">Get Metadata Activity</a> in Azure Data Factory which can be used to retrieve metadata of any data in Azure Data Factory.</p>

<p>However,only below connectors are supported so far:</p>

<p><a href=""https://i.stack.imgur.com/JMkdD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JMkdD.png"" alt=""enter image description here""></a></p>

<p>Since your data is stored in ADL Gen2,you could try to transfer the data from ADL Gen2 to Azure Blob Storage.Then use Metadata Activity to access the file name in folder:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity#get-a-folders-metadata"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity#get-a-folders-metadata</a></p>
"
"54990615","Azure Data Factory v2 - having trouble iterating through list of servers in table","<p>I have a pipeline that includes a simple copy task that reads data from an SFTP source and writes to a table within a server.  I have successfully parameterized the pipeline to prompt for which server and table I want to use at runtime but I want to specify a list of server/table pairs in a table that is accessed by a lookup task for use as parameters instead of needing to manually enter the server/table each time.  For now it's only three combinations of servers and tables but that number should be able to flex as needed.</p>

<p>The issue I'm running into as that when I try to specify the array variable as my parameter in the lookup task within a For Each loop the pipeline fails telling me I need to specify an integer in the value array. I understand what it's telling me but it doesn't seem logical to me that I'd have to specify '0', '1','2' and so on each time.</p>

<p>How do I just let it iterate through the server and table pairs until there aren't any more to process?  I'm not sure of the exact syntax but there has to be a way to tell it run the pipeline once with this server and table, again with a different server and table, then again and again until no more pairs are found in the table.</p>

<p>Not sure if it matters but I am on the data flow preview and using ADFv2</p>
","<azure><azure-data-factory>","2019-03-04 19:55:42","426","0","1","55018870","<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity#iteration-expression-language"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity#iteration-expression-language</a> </p>

<p>I guess you want to access the iterate item, which is <code>item()</code> in adf expression language.</p>

<p>If you append a foreach activity after a look up activity, and put the output of lookup activity in <code>items</code> field in foreach activity, then <code>item()</code> means the iterate item in the lookup output.</p>
"
"54955571","unable to load data in azure sql using azure databricks","<p>I am new to azure databricks . I have written a sample spark program in scala to load in azure sql via below query . I am getting an error . can someone please help me in this </p>

<p>Error Message ----<br>
com.microsoft.sqlserver.jdbc.SQLServerException: The TCP/IP connection to the host b63da5ce2d2d.tr27.northeurope1-a.worker.database.windows.net, port 65535 has failed. Error: ""connect timed out. Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.""</p>

<p>Scala code - </p>

<p>import com.microsoft.azure.sqldb.spark.config.Configimportcom.microsoft.azure.sqldb.spark.connect._// Aquire a DataFrame collection (val collection)valconfig=Config(Map(""url""->""mysqlserver.database.windows.net"",""databaseName""->""MyDatabase"",""dbTable""->""dbo.Clients""""user""->""username"",""password""->""xxxxxxxx""))importorg.apache.spark.sql.SaveModecollection.write.mode(SaveMode.Append).sqlDB(config)</p>
","<azure-sql-database><azure-data-factory><azure-databricks>","2019-03-02 05:37:20","2274","1","2","54955637","<p>It sounds like you have the firewall enabled on the SQL DB. Try disabling that first to confirm the problem. </p>

<p>If it works then you have two choices if you want the firewall enabled:</p>

<p>1) vnet attach your databricks and sqldb to the same vnet. <a href=""https://docs.azuredatabricks.net/administration-guide/cloud-configurations/azure/vnet-inject.html"" rel=""nofollow noreferrer"">https://docs.azuredatabricks.net/administration-guide/cloud-configurations/azure/vnet-inject.html</a></p>

<p>2) on the sqldb whitelist all the azure ips for the location you are. These are available for download. Note that some locations have more than 128 which is the maximum number of firewall rules available. <a href=""https://www.microsoft.com/en-gb/download/details.aspx?id=41653"" rel=""nofollow noreferrer"">https://www.microsoft.com/en-gb/download/details.aspx?id=41653</a></p>
"
"54955571","unable to load data in azure sql using azure databricks","<p>I am new to azure databricks . I have written a sample spark program in scala to load in azure sql via below query . I am getting an error . can someone please help me in this </p>

<p>Error Message ----<br>
com.microsoft.sqlserver.jdbc.SQLServerException: The TCP/IP connection to the host b63da5ce2d2d.tr27.northeurope1-a.worker.database.windows.net, port 65535 has failed. Error: ""connect timed out. Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.""</p>

<p>Scala code - </p>

<p>import com.microsoft.azure.sqldb.spark.config.Configimportcom.microsoft.azure.sqldb.spark.connect._// Aquire a DataFrame collection (val collection)valconfig=Config(Map(""url""->""mysqlserver.database.windows.net"",""databaseName""->""MyDatabase"",""dbTable""->""dbo.Clients""""user""->""username"",""password""->""xxxxxxxx""))importorg.apache.spark.sql.SaveModecollection.write.mode(SaveMode.Append).sqlDB(config)</p>
","<azure-sql-database><azure-data-factory><azure-databricks>","2019-03-02 05:37:20","2274","1","2","54976252","<p>Why your port number is ""65535"" not 1433?</p>

<p>SQL Database communicates over port 1433. I think this is the reason why you can not connect to the Azure SQL DB.
<a href=""https://i.stack.imgur.com/RLXwf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RLXwf.png"" alt=""enter image description here""></a></p>

<p>Please reference: <a href=""https://learn.microsoft.com/en-us/azure/sql-database/sql-database-server-level-firewall-rule"" rel=""nofollow noreferrer"">Server-level IP firewall rules</a>.</p>

<p>We cannot change the port in Azure SQL Server. Your database is hosted on load balanced cloud server instances which are out of 'customer' control.</p>

<p>Reference: <a href=""https://stackoverflow.com/questions/31694129/change-port-in-azure-sql-server"">Change port in azure sql server</a>.</p>

<p>Port 1433 is the only port that must be open on your desktop computer that hosts your SQL Database client application.</p>

<p>If you don't how to open the port 1433 on your computer,</p>

<p>please see: <a href=""https://blogs.technet.microsoft.com/danstolts/2011/06/how-to-open-firewall-port-1433-for-sql-server-database-engine-for-use-with-scom-or-anything-else/"" rel=""nofollow noreferrer"">How To Open Firewall Port 1433 for SQL Server Database Engine</a>.</p>

<p>About how to load data in Azure SQL Database, Azure provides many methods.</p>

<ol>
<li><p><a href=""https://learn.microsoft.com/en-us/azure/sql-database/sql-database-load-from-csv-with-bcp"" rel=""nofollow noreferrer"">Load data from CSV into Azure SQL Database (flat files)</a>.</p></li>
<li><p><a href=""https://azure.microsoft.com/en-us/updates/preview-loading-files-from-azure-blob-storage-into-sql-database/"" rel=""nofollow noreferrer"">Loading files from Azure Blob storage into Azure SQL Database</a>.</p></li>
</ol>

<p>For more details, you can see <a href=""https://learn.microsoft.com/en-us/sql/integration-services/load-data-to-sql-database-with-ssis?view=sql-server-2017"" rel=""nofollow noreferrer"">Load data into SQL Server or Azure SQL Database with SQL Server Integration Services</a>.</p>

<p>Both of them can help you load data in Azure SQL Database.</p>

<p>Hope this helps.</p>
"
"54944443","Azure DataFactory V2 - Salesforce Linkedservice with OAuth token","<p>I am looking to connect to salesforce in ADF v2 using OAuth rather than username, password, secret. </p>

<p>Does anyone have any ideas how to do this? Or alternatives. </p>
","<oauth><salesforce><azure-data-factory>","2019-03-01 12:13:07","603","2","1","54986076","<p>Oauth isn’t supported.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-salesforce#linked-service-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-salesforce#linked-service-properties</a></p>

<p>Maybe you could write your own
Code and then call ADF custom activity?</p>
"
"54941219","Issue connecting to Databricks table from Azure Data Factory using the Spark odbc connector","<p>​We have managed to get a valid connection from Azure Data Factory towards our Azure Databricks cluster using the Spark (odbc) connector. In the list of tables we do get the expected list, but when querying a specific table we get an exception.</p>

<blockquote>
  <p>ERROR [HY000] [Microsoft][Hardy] (35) Error from server: error code:
  '0' error message:
  'com.databricks.backend.daemon.data.common.InvalidMountException:
  Error while using path xxxx for resolving path xxxx within mount at
  '/mnt/xxxx'.'.. Activity ID:050ac7b5-3e3f-4c8f-bcd1-106b158231f3</p>
</blockquote>

<p>In our case the Databrick tables and mounted parquet files stored in Azure Data Lake 2, this is related to the above exception. Any suggestions how to solve this issue?</p>

<p>Ps. the same error appaers when connectin from Power BI desktop.</p>

<p>Thanks
Bart</p>
","<powerbi><parquet><azure-data-factory><azure-data-lake><azure-databricks>","2019-03-01 09:05:40","1454","1","1","55118932","<p>In your configuration to mount the lake can you add this setting:</p>

<pre><code>""fs.azure.createRemoteFileSystemDuringInitialization"": ""true""
</code></pre>

<p>I haven't tried your exact scenario - however this solved a similar problem for me using <a href=""https://datathirst.net/blog/2019/3/7/databricks-connect-limitations"" rel=""nofollow noreferrer"">Databricks-Connect</a>. </p>
"
"54934549","Referencing JSON payload value in Azure Data Factory for If condition","<p>I have a Json file like so as a payload returned from an API call - which is an http dataset type in data factory.</p>

<pre><code>{
    ""count"": 2,
    ""name"": ""DatasetABC"",
    ""columnNames"": [
        ""Column_1"",
        ""Column_2""

    ],
    ""rows"": [""1234"",
             ""5678""

    ]

}
</code></pre>

<p>I would like to be able to use the count records returned in an If condition. Im wondering what I need to use to get the value of ""count"" which is 2.</p>

<p>Any help appreciated.</p>
","<azure-data-factory>","2019-02-28 21:27:51","1235","3","1","54939839","<p>Based on your description, i suppose you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">LookUp Activity</a> in Azure Data Factory.</p>

<p>Lookup activity can retrieve a dataset from any of the Azure Data Factory-supported data sources. Use it in the following scenario:</p>

<blockquote>
  <p>Dynamically determine which objects to operate on in a subsequent
  activity, instead of hard coding the object name. Some object examples
  are files and tables. Lookup activity reads and returns the content of
  a configuration file or table. It also returns the result of executing
  a query or stored procedure. The output from Lookup activity can be
  used in a subsequent copy or transformation activity if it's a
  singleton value. The output can be used in a ForEach activity if it's
  an array of attributes.</p>
</blockquote>

<p>For example,maybe you could access the count value by using <code>@{activity('MyLookupActivity').output.firstRow.count}</code> in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-if-condition-activity"" rel=""nofollow noreferrer"">IF activity</a>.</p>
"
"54929262","How can I use Azure Data Factory to split results from a Database query into multiple blob files for Snowflake ingestion?","<p>We've used ADF's copy function to extract single files from Database tables to individual blob files. We regularly have files that are 8-12 GB with some occasionally much larger. When loading into Snowflake, it appears that Snowflake loading uses 1 thread per file - meaning it can take over an hour to load a single file. 
In contrast, when we look at 9 GB of blobs from an API source that forces us to paginate (resulting in hundreds of files that are ~50 MB), we see load times of around 2 minutes because of parallel processing. </p>

<p>What we really want to do is:
1. Perform a full extract so all data is loaded (we looked into partitioning but it looks like it will hit the table multiple times for each run - we want all of the data as of a specific time and do not want the data changing between runs).
2. Write multiple files from that extract, preferably by file size - but if we could choose the number of records we could use trial and error to pinpoint the file sizes we're looking for. </p>

<p>We've tried Azure Blob Tables as a staging area sink, but they have limited datatypes that conflict with our source datatypes. 
Our sources are varied and on-prem: Oracle, MySql, and SQL Server (so far). We looked at the new Data Flows in ADF, but they are in preview and only work on Azure SQL sources. </p>

<p>Is there any way to paginate your writes in ADF?</p>

<p>Thanks!</p>
","<azure-blob-storage><azure-data-factory><snowflake-cloud-data-platform>","2019-02-28 15:37:00","1030","1","1","55112238","<p>There is an alternative to Data Factory.  There is a product by Matilion which is built specifically for Snowflake and has most of the connectors and capabilities of Data Factory.
You can find ""Matilion ETL for Snowflake"" in the Azure Marketplace (""Create a resource"" in the Azure portal).</p>
"
"54928373","Azure Data Factory Pipeline deployment with PowerShell","<p>I've create an ADFV2 pipeline that use a variable in it's process. Now I want to export this pipeline to a JSON file in order to use it for a futur deployment.</p>

<p>Unfortunately, when I'm trying to deploy the pipeline in another environment with the powershell command ""Set-AzureRmDataFactoryV2Pipeline"", it's not working.</p>

<p>The powershell command run normally, but when I'm going to the ADF Interface and select my pipeline, there is an error and the generated JSON does not contains my variable declaration... So the pipeline cannot run...</p>

<p>Do you have any idea why this problem appears?</p>

<p><strong>Here is the JSON content for my testing pipeline (this pipeline do nothing, it's just for the example):</strong></p>

<pre><code>{
""name"": ""01_test"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""Web1"",
            ""type"": ""WebActivity"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false,
                ""secureInput"": false
            },
            ""typeProperties"": {
                ""url"": {
                    ""value"": ""@variables('test')"",
                    ""type"": ""Expression""
                },
                ""method"": ""GET""
            }
        }
    ],
    ""variables"": {
        ""test"": {
            ""type"": ""String"",
            ""defaultValue"": ""10""
        }
    }
}
</code></pre>

<p>}</p>

<p><strong>And there, the JSON generated on my new environment after the deployment with the powershell command :</strong></p>

<pre><code>{
""name"": ""01_test"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""Web1"",
            ""type"": ""WebActivity"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false,
                ""secureInput"": false
            },
            ""typeProperties"": {
                ""url"": {
                    ""value"": ""@variables('test')"",
                    ""type"": ""Expression""
                },
                ""method"": ""GET""
            }
        }
    ]
},
""type"": ""Microsoft.DataFactory/factories/pipelines""
</code></pre>

<p>}</p>

<p>Thank you for your help.</p>
","<azure><powershell><pipeline><azure-data-factory>","2019-02-28 14:52:07","851","1","1","54940861","<p>I've solved the problem today.</p>

<p>It was a my version of the AzureRM.DataFactoryV2 module.</p>

<p>After I've updated the module, everything works.</p>
"
"54921828","How to use Azure Data Factory Schedule property?","<p>I'm trying to create an advanced schedule trigger, following this: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#schedule-property"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#schedule-property</a></p>

<ol>
<li><p>I Created a very simple pipeline:</p>

<p><a href=""https://i.stack.imgur.com/lMPXj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lMPXj.png"" alt=""enter image description here""></a></p></li>
<li><p>tried a manual run and it was OK:
<a href=""https://i.stack.imgur.com/nRjQ4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nRjQ4.png"" alt=""enter image description here""></a></p></li>
<li><p>Created a schedule (each 1 minute) trigger</p></li>
<li><p>Opened the JSON for that trigger and modified it to:</p></li>
</ol>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>{
    ""name"": ""test"",
    ""properties"": {
        ""runtimeState"": ""Started"",
        ""pipelines"": [
            {
                ""pipelineReference"": {
                    ""referenceName"": ""ForEachMetadata"",
                    ""type"": ""PipelineReference""
                }
            }
        ],
        ""type"": ""ScheduleTrigger"",
        ""typeProperties"": {
            ""recurrence"": {
                ""frequency"": ""Minute"",
                ""interval"": 1,
                ""startTime"": ""2019-02-25T16:23:00.000Z"",
                ""timeZone"": ""UTC"",
                ""schedule"": {
                    ""minutes"": [
                        0,
                        30
                    ],
                    ""hours"": [
                        10,
                        11
                    ],
                    ""weekDays"": [
                        ""monday"",
                        ""tuesday"",
                        ""wednesday"",
                        ""thursday"",
                        ""friday""
                    ]
                }
            }
        }
    }
}</code></pre>
</div>
</div>
</p>

<ol start=""5"">
<li>As soon as it's published I get that the trigger failes:
<a href=""https://i.stack.imgur.com/bVSd2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bVSd2.png"" alt=""enter image description here""></a></li>
</ol>

<p>What am I doing wrong?</p>
","<azure-data-factory>","2019-02-28 09:01:57","346","1","1","54951180","<p>Please revisit the JSON and update  </p>

<pre><code>""frequency"": ""Minute"",
</code></pre>

<p>with </p>

<pre><code>""frequency"": ""Week"",
</code></pre>

<p>and that should help .</p>
"
"54920016","How to create a kusto activity in data factory pipeline using c# .net sdk","<p>I am trying to create a pipeline with Kusto activity using .net code. I am not able to find any KustoActivity class in ADF's DLL. I am also not able to find anywhere in the Kusto's .net reference documentation here: <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models?view=azure-dotnet"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models?view=azure-dotnet</a> .</p>
","<c#><azure-data-factory>","2019-02-28 06:54:20","387","0","1","54981229","<p>Here you can find the library and method related to Azure Data explorer(Including Kusto)</p>

<pre><code>Install-Package Microsoft.Azure.Management.DataFactory -Version 3.0.1
</code></pre>

<p>I would recommend you to install below packages in your [project for pipeline.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>Install-Package Microsoft.Azure.Management.DataFactory
Install-Package Microsoft.Azure.Management.ResourceManager
Install-Package Microsoft.IdentityModel.Clients.ActiveDirectory</code></pre>
</div>
</div>
</p>

<p>Here you can find all the module available in Azure.management.DataFactory</p>

<p><a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models?view=azure-dotnet"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models?view=azure-dotnet</a></p>

<p><a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models.azuredataexplorerlinkedservice?view=azure-dotnet"" rel=""nofollow noreferrer"">AzureDataExplorerLinkedService</a> is the class you would be interested in by which you can explore Kusto in dot net pipeline.</p>

<p>Hope it helps.</p>
"
"54915270","Why am I getting a missing header error when calling the put file api for azure data lake gen2?","<p>I am trying to call the gen2 rest endpoint directly and keep getting an error that I am missing a required header (MissingRequiredHeader message An HTTP header that's mandatory for this request is not specified. I fail to see what header is missing. I'm using the following code to send the request.</p>

<pre><code>var client = new HttpClient();

client.BaseAddress = new Uri($""https://{account}.dfs.core.windows.net/"");
client.DefaultRequestHeaders.Accept.Clear();
client.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(""application/json""));
client.DefaultRequestHeaders.Add(""api_version"", ""2018-11-09"");
client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(""Bearer"", _authResult.AccessToken);

string azPath = $""{baseRawSystemPath}/{path.Replace(""\\"", ""/"")}"";

byte[] bytes = Encoding.UTF8.GetBytes(content);
HttpContent body = new StringContent(content, Encoding.UTF8, ""application/json"");

HttpResponseMessage response = await client.PutAsync(azPath, body);
if (response.IsSuccessStatusCode)
{
    var responseListingJson = await response.Content.ReadAsStringAsync();
    return;
}
else
{
    var error = await response.Content.ReadAsStringAsync();
    return;
}
</code></pre>
","<azure><azure-data-lake><azure-data-factory>","2019-02-27 22:08:18","689","0","1","55224365","<p>Just to answer my own question...  I was missing the resource=file query string parameter.  That allowed the request to succeed and the file to be created.</p>
"
"54906081","How to parse this json in azure data factory in copy operation","<p>{""Properties""::\""{\""receivedPushNotificationMessage\""""Received push notification: {\n    aps =     {\n        alert = \\""Network programmer push\\"";\n    };\n    channelToken = \\""eyJ0eXAiO\"",\""time\"":\""2019-02-26_11-32-03-AM\""}""},{""AppBuild"":""321""},</p>
","<json><azure><blob><azure-data-factory><jsonpath>","2019-02-27 12:58:38","3808","0","1","54917361","<p>JSON format is supported in file type dataset.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#json-format"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#json-format</a></p>
"
"54904098","How to modify the structure of incoming blobs from App center to Application insights in azure?","<p>I want to parse the incoming blobs in application insights to Sql database by using Azure data factory. I am able to parse most part of the blob as it is in json format(even the nested ones) but there are some nested unstructured elements in the same JSON which i need to parse, which is unable right now.
So how to fix this or maybe is there any possibility to change the pattern of incoming blobs inside app insights?</p>
","<azure><blob><azure-application-insights><azure-data-factory>","2019-02-27 11:08:22","45","0","1","54985947","<p>one way could be store the jason data as string in azure sql and trying extracting the information from it store in your table . if your jason files are too big then you can also process it using spark sql using databricks .</p>
"
"54881313","Azure Data Factory: How to trigger a pipeline after another pipeline completed successfully","<p>In Azure Data Factory, how do I trigger a pipeline after other pipelines completed successfully?</p>
<h3>In detail:</h3>
<p>I seek to trigger an SSIS package after other pipelines completed successfully. I already know I can save my SSIS package as a pipeline and run it using a trigger like the other pipelines. But how do I make sure the SSIS package pipeline starts only after the other pipelines are finished? Is there a feature for this in Azure or do I need some kind of workaround for this?</p>
<p>Thanks in advance~</p>
","<ssis><azure-data-factory>","2019-02-26 08:35:13","6302","4","2","54882309","<p>Based on your descriptions,i think you could monitor azure data factory pipelines execution status programmatically.</p>

<p>Please add the following code to continuously check the status of the pipeline run until it finishes by it's <code>RunId</code>.</p>

<pre><code>PipelineRun pipelineRun;
while (true)
{
    pipelineRun = client.PipelineRuns.Get(resourceGroup, dataFactoryName, runResponse.RunId);
    Console.WriteLine(""Status: "" + pipelineRun.Status);
    if (pipelineRun.Status == ""InProgress"")
        System.Threading.Thread.Sleep(15000);
    else
        break;
}
</code></pre>

<p>And starts your SSIS package if the pipeline runs successfully.</p>

<pre><code>if (pipelineRun.Status == ""Succeeded"")
   //..do your business
</code></pre>

<p>More details,please refer to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-programmatically"" rel=""nofollow noreferrer"">document</a>.</p>
"
"54881313","Azure Data Factory: How to trigger a pipeline after another pipeline completed successfully","<p>In Azure Data Factory, how do I trigger a pipeline after other pipelines completed successfully?</p>
<h3>In detail:</h3>
<p>I seek to trigger an SSIS package after other pipelines completed successfully. I already know I can save my SSIS package as a pipeline and run it using a trigger like the other pipelines. But how do I make sure the SSIS package pipeline starts only after the other pipelines are finished? Is there a feature for this in Azure or do I need some kind of workaround for this?</p>
<p>Thanks in advance~</p>
","<ssis><azure-data-factory>","2019-02-26 08:35:13","6302","4","2","54893716","<p>You could always create a parent pipeline that uses <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-pipeline-activity"" rel=""nofollow noreferrer"">execute pipeline</a> and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-invoke-ssis-package-ssis-activity"" rel=""nofollow noreferrer"">execute SSIS package</a> activities. ADF V2 has the concept of <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities#activity-dependency"" rel=""nofollow noreferrer"">dependencies</a>, so have a dependency between the execute pipeline activity and the execute SSIS package activity. Make sure to check the Wait on Completion box for the execute pipeline activity so that they run in sequence rather than in parallel. You can have multiple dependencies for an activity, so if you need SSIS to wait on 3 packages instead of just one, that should still work.</p>
<p><a href=""https://i.stack.imgur.com/JjLNv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JjLNv.png"" alt=""enter image description here"" /></a></p>
<p>Then instead of triggering the other pipeline(s) and SSIS package separately, you can just trigger the parent pipeline instead.</p>
"
"54878136","Copy Data from MySQL (on-premises) to Cosmos DB","<p>I have several questions as follows:</p>

<ol>
<li>I was wondering how I could transfer data from MySQL to Cosmos DB
using either Python or Data Azure Factory, or anything else.</li>
<li>If I understand correctly, a row from the table will be transformed
into a document, is it correct?</li>
<li>Is there any way to create one more row for a doc during the copy activity?</li>
<li>If data in MySQL are changed, will the copied data in Cosmos DB be automatically changed too? If not, how to do such triggers?</li>
</ol>

<p>I do understand that some questions can be simply done; however, I'm new to this. Please bear with me.</p>
","<mysql><azure><azure-cosmosdb><azure-data-factory>","2019-02-26 03:41:04","917","2","1","54878250","<blockquote>
  <p>1.I was wondering how I could transfer data from MySQL to Cosmos DB using either Python or Data Azure Factory, or anything else.</p>
</blockquote>

<p>Yes, you could transfer data from mysql to cosmos db by using Azure Data Factory Copy Activity.</p>

<blockquote>
  <p>If I understand correctly, a row from the table will be transformed
  into a document, is it correct?</p>
</blockquote>

<p>Yes.</p>

<blockquote>
  <p>Is there any way to create one more row for a doc during the copy
  activity?</p>
</blockquote>

<p>If you want to merge multiple rows for one document,then the copy activity maybe can't be used directly. You could make your own logical code(e.g. Python code) in the <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook"" rel=""nofollow noreferrer"">Azure Function Http Trigger</a>. </p>

<blockquote>
  <p>If data in MySQL are changed, will the copied data in Cosmos DB be
  automatically changed too? If not, how to do such triggers?</p>
</blockquote>

<p>So,you could tolerate delay sync,you could sync the data using Copy Activity between sql and cosmos db in the schedule. If you need to timely sync,as i know, azure function does support sql server trigger.But you could get some solutions from this <a href=""https://blogs.msdn.microsoft.com/amitagarwal/2018/01/11/azure-function-apps-trigger-in-azure-sql-sql-server-to-execute-azure-function/"" rel=""nofollow noreferrer"">document</a>.</p>

<ol>
<li><p>Defining Custom Binding in Azure functions</p></li>
<li><p>If not the binding on Azure Functions side, then it can be a SQL trigger invoking an Azure Functions HTTP trigger.</p></li>
</ol>
"
"54868998","Azure Data Factory and SharePoint","<p>I have some Excel files stored in SharePoint online. I want copy files stored in SharePoint folders to Azure Blob storage. </p>

<p>To achieve this, I am creating a new pipeline in Azure Data factory using Azure Portal. What are possible ways to copy files from SharePoint to Azure blob store using Azure Data Factory pipelines? </p>

<p>I have looked at all linked services types in Azure data factory pipeline but couldn't find any suitable type to connect to SharePoint.</p>
","<azure><sharepoint><onedrive><azure-data-factory>","2019-02-25 15:00:07","12637","10","5","54875928","<p>Rather than directly accessing the file in SharePoint from Data Factory, you might have to use an intermediate technology and have Data Factory call that. You have a few of options: </p>

<ol>
<li><a href=""https://www.mssqltips.com/sqlservertip/5893/transfer-files-from-sharepoint-to-blob-storage-with-azure-logic-apps/"" rel=""nofollow noreferrer"">Use a Logic App to move the file</a></li>
<li><a href=""https://learn.microsoft.com/en-us/sharepoint/dev/apis/webhooks/sharepoint-webhooks-using-azure-functions"" rel=""nofollow noreferrer"">Use an Azure Function</a></li>
<li>Use a custom activity and write your own C# to copy the file. </li>
</ol>

<p>To call a Logic App from ADF, you <a href=""http://microsoft-bitools.blogspot.com/2018/06/execute-logic-apps-in-azure-data.html"" rel=""nofollow noreferrer"">use a web activity</a>. 
You can <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">directly call an Azure Function</a> now. </p>
"
"54868998","Azure Data Factory and SharePoint","<p>I have some Excel files stored in SharePoint online. I want copy files stored in SharePoint folders to Azure Blob storage. </p>

<p>To achieve this, I am creating a new pipeline in Azure Data factory using Azure Portal. What are possible ways to copy files from SharePoint to Azure blob store using Azure Data Factory pipelines? </p>

<p>I have looked at all linked services types in Azure data factory pipeline but couldn't find any suitable type to connect to SharePoint.</p>
","<azure><sharepoint><onedrive><azure-data-factory>","2019-02-25 15:00:07","12637","10","5","59248842","<p>We can create a linked service of type '<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system"" rel=""nofollow noreferrer"">File system</a>' by providing the directory URL as 'Host' value. To authenticate the user, provide username and password/AKV details. 
Note: Use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">Self-hosted IR</a> </p>
"
"54868998","Azure Data Factory and SharePoint","<p>I have some Excel files stored in SharePoint online. I want copy files stored in SharePoint folders to Azure Blob storage. </p>

<p>To achieve this, I am creating a new pipeline in Azure Data factory using Azure Portal. What are possible ways to copy files from SharePoint to Azure blob store using Azure Data Factory pipelines? </p>

<p>I have looked at all linked services types in Azure data factory pipeline but couldn't find any suitable type to connect to SharePoint.</p>
","<azure><sharepoint><onedrive><azure-data-factory>","2019-02-25 15:00:07","12637","10","5","60604459","<p>You can use the logic app to fetch data from Sharepoint and load it to azure blob storage and now you can use azure data factory to fetch data from blob even we can set an event trigger so that if any file comes into blob container the azure pipeline will automatically trigger.</p>
"
"54868998","Azure Data Factory and SharePoint","<p>I have some Excel files stored in SharePoint online. I want copy files stored in SharePoint folders to Azure Blob storage. </p>

<p>To achieve this, I am creating a new pipeline in Azure Data factory using Azure Portal. What are possible ways to copy files from SharePoint to Azure blob store using Azure Data Factory pipelines? </p>

<p>I have looked at all linked services types in Azure data factory pipeline but couldn't find any suitable type to connect to SharePoint.</p>
","<azure><sharepoint><onedrive><azure-data-factory>","2019-02-25 15:00:07","12637","10","5","74612022","<p>You can use Power Automate (<a href=""https://make.powerautomate.com/"" rel=""nofollow noreferrer"">https://make.powerautomate.com/</a>) to do this task automatically:</p>
<ol>
<li>Create an Automated cloud flow trigger whenever a new file is dropped in a SharePoint</li>
</ol>
<p><a href=""https://i.stack.imgur.com/b5Q6w.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/b5Q6w.png"" alt=""Automated cloud flow"" /></a></p>
<ol start=""2"">
<li>Use any mentioned trigger as per your requirement and fill in the SharePoint details</li>
</ol>
<p><a href=""https://i.stack.imgur.com/Upg7I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Upg7I.png"" alt=""SharePoint Trigger"" /></a></p>
<ol start=""3"">
<li>Add an action to create a blob and fill in the details as per your use case</li>
</ol>
<p><a href=""https://i.stack.imgur.com/ZqQ89.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZqQ89.png"" alt=""Create Blob action"" /></a></p>
<p><em><strong>By using this you will be pasting all the SharePoint details to the BLOB without even using ADF.</strong></em></p>
"
"54868998","Azure Data Factory and SharePoint","<p>I have some Excel files stored in SharePoint online. I want copy files stored in SharePoint folders to Azure Blob storage. </p>

<p>To achieve this, I am creating a new pipeline in Azure Data factory using Azure Portal. What are possible ways to copy files from SharePoint to Azure blob store using Azure Data Factory pipelines? </p>

<p>I have looked at all linked services types in Azure data factory pipeline but couldn't find any suitable type to connect to SharePoint.</p>
","<azure><sharepoint><onedrive><azure-data-factory>","2019-02-25 15:00:07","12637","10","5","74806475","<p>My previous answer was true at the time, but in the last few years, Microsoft has published guidance on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sharepoint-online-list?tabs=data-factory#copy-file-from-sharepoint-online"" rel=""nofollow noreferrer"">how to copy documents from a SharePoint library</a>. You can copy file from SharePoint Online by using Web activity to authenticate and grab access token from SPO, then passing to subsequent Copy activity to copy data with HTTP connector as source.</p>
<p>I ran into some issues with  large files and Logic Apps. It turned out there were some extremely large files to be copied from that SharePoint library. SharePoint has a default limit of 100 MB buffer size, and the Get File Content action doesn’t natively support chunking.</p>
<p>I successfully pulled the files with the web activity and copy activity. But I found the SharePoint permissions configuration to be a bit tricky. I blogged my  process <a href=""https://datasavvy.me/2021/12/07/copying-large-files-from-sharepoint-online/"" rel=""nofollow noreferrer"">here</a>.
You can use a binary dataset if you just want to copy the full file rather than read the data.</p>
<p>If my file is located at <code>https://mytenant.sharepoint.com/sites/site1/libraryname/folder1/folder2/folder3/myfile.CSV</code>, the URL I need to retrieve the file is <code>https://mytenant.sharepoint.com/sites/site1/libraryname/folder1/folder2/folder3/myfile.CSV')/$value</code>.</p>
<p>Be careful about when you get your auth token. Your auth token is valid for 1 hour. If you copy a bunch of files sequentially, and it takes longer than that, you might get a timeout error.</p>
"
"54866849","How to copy files from sharepoint into blob storage azure data factory v2 using Odata linked service","<p>Can any one help in understanding procedure of copying excel  file from  share point  to azure Blob storage through  Azure Data Factory pipelines. I am struggling while creating Odata linked service. What is service url in odata linked service? 
I am using rest apis provided in link <a href=""https://learn.microsoft.com/en-us/sharepoint/dev/sp-add-ins/working-with-folders-and-files-with-rest"" rel=""nofollow noreferrer"">here</a> as a service url :<a href=""https://xxxxx.sharepoint.com/sites/xxx/_api/web/"" rel=""nofollow noreferrer"">https://xxxxx.sharepoint.com/sites/xxx/_api/web/</a></p>

<p>authentication type: basic </p>

<p>when I test connection I outputs a weird error: <a href=""https://i.stack.imgur.com/DFYuS.png"" rel=""nofollow noreferrer"">here</a></p>

<p>I have tried the following articles so far.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-odata"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-odata</a></p>

<p><a href=""https://learn.microsoft.com/en-us/sharepoint/dev/sp-add-ins/working-with-folders-and-files-with-rest"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sharepoint/dev/sp-add-ins/working-with-folders-and-files-with-rest</a></p>

<p>Note: when I try these rest api urls from browser they work fine I can download files and so on.
is there any other way of achieving this goal??</p>
","<azure><sharepoint><azure-data-factory><azure-blob-storage>","2019-02-25 13:02:29","3475","1","1","54917401","<p>Sharepoint online is supported by OData linked service through AAD SPN with Cert authentication type. </p>

<p>A cert using by OData connector can be created and configured by following the below article, which will guide you to connect the cert to an AAD app for invoking SPO Online with an app only access token.</p>

<p><a href=""https://learn.microsoft.com/en-us/sharepoint/dev/solution-guidance/security-apponly-azuread"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sharepoint/dev/solution-guidance/security-apponly-azuread</a></p>
"
"54860060","Azure Data Factory Error code"":9056,""message"":""Cannot connect to SQL Server""","<p>I am trying to copy a file data from Azure Blob storage to Azure SQL DB just for my learning. I cannot able to create the linked service for Azure SQL db destination as it is giving the error. I can able to connect fine from my local SSMS to the Azure SQL server but not from AZURE data factory. I turned on Allow access to Azure services. I am using the default integration runtime (AutoResolveIntegrationRuntime). I also did Add client IP by adding my current IP address to the rule list</p>
","<azure><azure-data-factory>","2019-02-25 05:35:19","922","1","1","54865902","<p>Try using an Azure Integration Runtime with the same region as the SQL server. Sometimes the auto resolve cannot reach the sql server.</p>

<p>Hope this helped!</p>
"
"54851574","Delete temporary files from Azure Datalake Storage in a Azure DataFactory Pipeline (USQL preferred)","<p>We are using AdLS (Azure data lake storage)as a temporary storage in our ADF (Azure data factory - V2) pipeline. What is the best way to delete the data that is stored temporarily in ADLS?</p>

<p>U-SQL only supports DDL and not DML, so can’t delete the temporary data(files) stored in ADLS using ADLA(Azure data lake analytics)</p>

<p>I plan on using ADF's ""Web Activity"" DELETE method, but that uses tokens that expire and I have to keep updating them.</p>

<p>Can any one please let me know what other options do we have?</p>
","<azure><azure-data-lake><u-sql><azure-data-factory>","2019-02-24 11:49:02","388","0","1","54855993","<p>the best way is to use new Delete Activity in ADF. 
In the right top corner of your ADF UI, you can find code section, click here and write JSON syntax for delete activity (i didn't find delete activity widget/icon so I needed to write directly JSON code)</p>

<p>You can check syntax <a href=""https://learn.microsoft.com/en-us/azure/data-factory/delete-activity"" rel=""nofollow noreferrer"">here</a></p>

<p>Example pipeline with only delete activity </p>

<pre><code>{
""name"": ""DeleteFilePipeline"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""DeleteActivity"",
            ""type"": ""Delete"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false,
                ""secureInput"": false
            },
            ""typeProperties"": {
                ""dataset"": {
                    ""referenceName"": ""deleteTest"",
                    ""type"": ""DatasetReference""
                },
                ""enableLogging"": false,
                ""maxConcurrentConnections"": 1
            }
        }
    ]
}
</code></pre>

<p>}</p>

<p>Dataset deleteTest is Azure Data Lake Gen1 dataset.</p>
"
"54824323","Databricks job cluster per pipeline not per notebook activity","<p>I'm transforming data within different Databricks notebooks (reading, transforming and writing to/from ADLS). I conected these notebooks within a DataFactory pipeline:</p>

<pre><code>Notebook 1 --&gt; Notebook 2 --&gt; Notebook 3 --&gt; Notebook
</code></pre>

<p>I've than created a connection to my Databricks from the DataFactory and added it to my notebook activities. I would like to start a Databricks cluster whenever the pipeline has been triggered. Overall all of this it working fine. But Databricks starts a job cluster for each notebook activity which takes too long and seems unnecessary to me.</p>

<p>Is it possible to start a cluster at the beginning of a pipeline and then shut it down after all notebooks has been completed? Or are there any arguments that it's good to have a job cluster for each activity?</p>
","<azure><azure-data-factory><databricks><azure-databricks>","2019-02-22 09:49:58","1529","1","2","54826342","<p>Currently using same job cluster for multiple notebook activities is not possible.</p>

<p>Two alternative options:</p>

<ol>
<li>Use interactive cluster </li>
<li>Use interactive cluster and (if cost conscious) have a web activity at the beginning to START the cluster via azure databricks REST endpoint and another web activity at the end after notebook activities to <a href=""https://docs.databricks.com/api/latest/clusters.html#delete-terminate"" rel=""nofollow noreferrer"">DELETE(TERMINATE)</a> the cluster via REST endpoint</li>
</ol>

<p>Unfortunately both options use <strong>interactive clusters</strong> - which are bit expensive compared to <strong>job clusters</strong>.</p>
"
"54824323","Databricks job cluster per pipeline not per notebook activity","<p>I'm transforming data within different Databricks notebooks (reading, transforming and writing to/from ADLS). I conected these notebooks within a DataFactory pipeline:</p>

<pre><code>Notebook 1 --&gt; Notebook 2 --&gt; Notebook 3 --&gt; Notebook
</code></pre>

<p>I've than created a connection to my Databricks from the DataFactory and added it to my notebook activities. I would like to start a Databricks cluster whenever the pipeline has been triggered. Overall all of this it working fine. But Databricks starts a job cluster for each notebook activity which takes too long and seems unnecessary to me.</p>

<p>Is it possible to start a cluster at the beginning of a pipeline and then shut it down after all notebooks has been completed? Or are there any arguments that it's good to have a job cluster for each activity?</p>
","<azure><azure-data-factory><databricks><azure-databricks>","2019-02-22 09:49:58","1529","1","2","64305896","<p>There is a possible workaround also. You can create and trigger &quot;master&quot; Databricks notebook with job cluster from ADF and it will call your notebooks with appropriate parameters one by one with dbutils.notebook.run() command.</p>
<p>In this way, you will achieve cost savings from job cluster and it will also terminate immediately.</p>
<p>See section &quot;https://towardsdatascience.com/building-a-dynamic-data-pipeline-with-databricks-and-azure-data-factory-5460ce423df5&quot; in this article -&gt; <a href=""https://towardsdatascience.com/building-a-dynamic-data-pipeline-with-databricks-and-azure-data-factory-5460ce423df5"" rel=""nofollow noreferrer"">https://towardsdatascience.com/building-a-dynamic-data-pipeline-with-databricks-and-azure-data-factory-5460ce423df5</a></p>
"
"54812089","Bringing incremental data in from REST APIs into SQL azure","<p>My needs are following:
- Need to fetch data from a 3rd party API into SQL azure.</p>

<ul>
<li><p>The API's will be queried everyday for incremental data and may require pagination as by default any API response will give only Top N records.</p></li>
<li><p>The API also needs an auth token to work, which is the first call before we start downloading data from endpoints.</p></li>
</ul>

<p>Due to last two reasons, I've opted for Function App which will be triggered daily rather than data factory which can query web APIs.</p>

<p>Is there a better way to do this?
Also I am thinking of pushing all JSON into Blob store and then parsing data from the JSON into SQL Azure. Any recommendations?</p>
","<azure><azure-sql-database><azure-functions><azure-data-factory>","2019-02-21 16:38:52","2594","3","4","54819337","<p>Maybe you can create a time task by SQL server Agent.</p>

<p><code>SQL server Agent</code>--<code>new job</code>--<code>Steps</code>--<code>new step</code>:
<a href=""https://i.stack.imgur.com/xHvPT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xHvPT.png"" alt=""enter image description here""></a></p>

<p>In the Command, put in your <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/json/import-json-documents-into-sql-server?view=sql-server-2017#import-json-documents-from-azure-blob-storage"" rel=""nofollow noreferrer"">Import JSON documents from Azure Blob Storage</a> sql statemanets for example.</p>

<p><code>Schedules</code>--<code>new schedule</code>:
<a href=""https://i.stack.imgur.com/L3AGD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L3AGD.png"" alt=""enter image description here""></a>
Set Execution time.</p>

<p>But I think Azure function is better for you to do this.Azure Functions is a solution for easily running small pieces of code, or ""functions,"" in the cloud. You can write just the code you need for the problem at hand, without worrying about a whole application or the infrastructure to run it. Functions can make development even more productive, and you can use your development language of choice, such as C#, F#, Node.js, Java, or PHP. </p>

<p>It is more intuitive and efficient.</p>

<p>Hope this helps.</p>
"
"54812089","Bringing incremental data in from REST APIs into SQL azure","<p>My needs are following:
- Need to fetch data from a 3rd party API into SQL azure.</p>

<ul>
<li><p>The API's will be queried everyday for incremental data and may require pagination as by default any API response will give only Top N records.</p></li>
<li><p>The API also needs an auth token to work, which is the first call before we start downloading data from endpoints.</p></li>
</ul>

<p>Due to last two reasons, I've opted for Function App which will be triggered daily rather than data factory which can query web APIs.</p>

<p>Is there a better way to do this?
Also I am thinking of pushing all JSON into Blob store and then parsing data from the JSON into SQL Azure. Any recommendations?</p>
","<azure><azure-sql-database><azure-functions><azure-data-factory>","2019-02-21 16:38:52","2594","3","4","54819424","<p>If you could set the default top N values in your api, then you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">web activity</a> in azure data factory to call your rest api to get the response data.Then configure the response data as input of copy activity(<code>@activity('ActivityName').output</code>) and the sql database as output. Please see this thread :<a href=""https://stackoverflow.com/questions/49693338/use-output-from-web-activity-call-as-variable"">Use output from Web Activity call as variable</a>.</p>

<p>The web activity support <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity#type-properties"" rel=""nofollow noreferrer"">authentication</a> properties for your access token.</p>

<blockquote>
  <p>Also I am thinking of pushing all JSON into Blob store and then
  parsing data from the JSON into SQL Azure. Any recommendations?</p>
</blockquote>

<p>Well,if you could dump the data into blob storage,then <a href=""https://azure.microsoft.com/en-us/services/stream-analytics/"" rel=""nofollow noreferrer"">azure stream analytics</a> is the perfect choice for you.</p>

<p>You could run the daily job to select or parse the json data with <a href=""https://learn.microsoft.com/en-us/stream-analytics-query/stream-analytics-query-language-reference?toc=%2Fen-us%2Fazure%2Fstream-analytics%2FTOC.json&amp;bc=%2Fen-us%2Fazure%2Fbread%2Ftoc.json"" rel=""nofollow noreferrer"">asa sql</a> ,then dump the data into sql database.Please see this <a href=""https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal"" rel=""nofollow noreferrer"">official sample.</a></p>
"
"54812089","Bringing incremental data in from REST APIs into SQL azure","<p>My needs are following:
- Need to fetch data from a 3rd party API into SQL azure.</p>

<ul>
<li><p>The API's will be queried everyday for incremental data and may require pagination as by default any API response will give only Top N records.</p></li>
<li><p>The API also needs an auth token to work, which is the first call before we start downloading data from endpoints.</p></li>
</ul>

<p>Due to last two reasons, I've opted for Function App which will be triggered daily rather than data factory which can query web APIs.</p>

<p>Is there a better way to do this?
Also I am thinking of pushing all JSON into Blob store and then parsing data from the JSON into SQL Azure. Any recommendations?</p>
","<azure><azure-sql-database><azure-functions><azure-data-factory>","2019-02-21 16:38:52","2594","3","4","54977384","<p>How long does it take to call all of the pages?  If it is <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-scale#consumption-plan"" rel=""nofollow noreferrer"">under ten minutes</a>, then my recommendation would be to build an Azure Function that queries the API and inserts the json data directly into a SQL database.  </p>

<h2>Azure Function</h2>

<p>Azure functions are very <a href=""https://www.troyhunt.com/serverless-to-the-max-doing-big-things-for-small-dollars-with-cloudflare-workers-and-azure-functions/"" rel=""nofollow noreferrer"">cost effective</a>.  The first million execution are free.  If it takes longer than ten, then have a look at durable functions.  For handling pagination, we have plenty of examples.  Your <em>exact</em> solution will depend on the API you are calling and the language you are using. Here is an example in <a href=""https://stackoverflow.com/questions/43445615/how-to-loop-calls-to-pagination-url-in-c-sharp-httpclient-to-download-all-pages/43447577"">C# using HttpClient</a>.  Here is one for <a href=""https://stackoverflow.com/questions/17777845/python-requests-arguments-dealing-with-api-pagination"">Python using Requests</a>. For both, the pattern is similar.  Get the total number of pages from the API, set a variable to that value, and loop over the pages;  Getting and saving your data in each iteration. If the API won't provide the max number of pages, then loop until you get an error.  Protip: Make sure specify an upper bound for those loops. Also, if your API is flakey or has intermittent failures, consider using a graceful retry pattern such as <a href=""https://learn.microsoft.com/en-us/dotnet/standard/microservices-architecture/implement-resilient-applications/explore-custom-http-call-retries-exponential-backoff"" rel=""nofollow noreferrer"">exponential backoff</a>. </p>

<h2>Azure SQL Json Indexed Calculated Columns</h2>

<p>You mentioned storing your data as json files into a storage container.  Are you sure you need that?  If so, then you could <a href=""https://medium.com/@mauridb/work-with-json-files-with-azure-sql-8946f066ddd4"" rel=""nofollow noreferrer"">create an external table link between the storage container and the database</a>.  That has the advantage of not having the data take up any space in the database.  However, if the json will fit in the database, I would <strong>highly recommend</strong> dropping that json right into the SQL database and leveraging <a href=""https://hackernoon.com/one-sql-cheat-code-for-blazing-fast-json-queries-d0cb6160d380"" rel=""nofollow noreferrer"">indexed calculated columns</a> to make querying the json extremely quick.  </p>

<p>Using this pairing should provide incredible performance per penny value!  Let us know what you end up using.</p>
"
"54812089","Bringing incremental data in from REST APIs into SQL azure","<p>My needs are following:
- Need to fetch data from a 3rd party API into SQL azure.</p>

<ul>
<li><p>The API's will be queried everyday for incremental data and may require pagination as by default any API response will give only Top N records.</p></li>
<li><p>The API also needs an auth token to work, which is the first call before we start downloading data from endpoints.</p></li>
</ul>

<p>Due to last two reasons, I've opted for Function App which will be triggered daily rather than data factory which can query web APIs.</p>

<p>Is there a better way to do this?
Also I am thinking of pushing all JSON into Blob store and then parsing data from the JSON into SQL Azure. Any recommendations?</p>
","<azure><azure-sql-database><azure-functions><azure-data-factory>","2019-02-21 16:38:52","2594","3","4","55009484","<p>One thing to consider for scale would be to parallelize both the query and the processing. If there is no ordering requirement, or if processing <em>all</em> records would take longer than the 10 minute function timeout. Or if you want to do some tweaking/transformation of the data in-flight, or if you have different destinations for different types of data. Or if you want to be insulated from a failure - e.g., your function fails halfway through processing and you don't want to re-query the API. Or you get data a different way and want to start processing at a specific step in the process (rather than running from the entry point). All sorts of reasons. </p>

<p>I'll caveat here to say that the best degree of parallelism vs complexity is largely up to your comfort level and requirements. The example below is somewhat of an 'extreme' example of decomposing the process into discrete steps and using a function for each one; in some cases it may not make sense to split specific steps and combine them into a single one. <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-overview"" rel=""nofollow noreferrer"">Durable Functions</a> also help make orchestration of this potentially easier. </p>

<ul>
<li>A timer-driven function that queries the API to understand the depth of pages required, or queues up additional pages to a second function that actually makes the paged API call</li>
<li>That function then queries the API, and writes to a scratch area (like Blob) or drops each row into a queue to be written/processed (e.g., something like a storage queue, since they're cheap and fast, or a Service Bus queue if multiple parties are interested (e.g., pub/sub)</li>
<li>If writing to scratch blob, a blob-triggered function reads the blob and queues up individual writes to a queue (e.g., a storage queue, since a storage queue would be cheap and fast for something like this)</li>
<li>Another queue-triggered function actually handles writing the individual rows to the next system in line, SQL or whatever.</li>
</ul>

<p>You'll get some parallelization out of that, plus the ability to start from any step in the process, with a correctly-formatted message. If your processors encounter bad data, things like poison queues/dead letter queues would help with exception cases, so instead of your entire process dying, you can manually remediate the bad data.</p>
"
"54791465","Does the per activity cost count each data factory individually","<p>We are setting up a data factory to help with our global failover scenario. The Pipeline copies data from our SQL server located on premise into Azure Table Storage.</p>

<p>We are using Data Factory V2 and have set up the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">CI/CD pipeline</a> as described in the ADF documentation. </p>

<p>Therefore, our dev and test instances only copy data from the Sql to one region, but our production needs to copy data to multiple regions.  My thought to simplify things would be to have one Factory per region that will only copy data to that region (so that production and dev can share the exact same pipelines).</p>

<p>However, this will mean that we will have multiple pipelines and all of them will have a rather low usage. There are only 3 activities that run once a day, so we will only have 90 activities per month. Looking at the <a href=""https://azure.microsoft.com/en-us/pricing/details/data-factory/data-pipeline/"" rel=""nofollow noreferrer"">data factory pricing</a>, you are charged for every 1,000 activities. </p>

<p>My question is, since each of these factories will have less than 1,000 activities, will we be charged the minimum of $1.50 for each factory or will the pricing just charge us once since all of them together will still be less than 1,000 activities?</p>
","<azure><azure-data-factory>","2019-02-20 16:48:24","705","0","1","54799480","<p>Great question! The pricing is calculated per Data Factory instance and not per pipeline. You can have as many pipelines in a single Data Factory instance. You will be charged based on the number of activity runs within a Data Factory instance.</p>

<p>In your case, since you are planning on having multiple Data Factory instances, you will be billed multiple times. Eg- If you have 3 data factories(may or may not be across different regions) and each ADF has 90 activity runs a month, <strong>you will be charged 3x$1.5 = $4.5.</strong></p>

<p>For an accurate estimation of pricing, please refer :</p>

<p><a href=""https://azure.microsoft.com/en-in/pricing/calculator/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-in/pricing/calculator/</a></p>

<p>Hope this helps!</p>
"
"54780664","Azure Data lake File Name mapping with target","<p>I am trying to load the files from ADL using metadata in Azure Data Factory, I need to pass FileName of to the target table. Please suggest what would be the approach.</p>
","<azure><azure-data-factory>","2019-02-20 07:07:39","58","0","1","54823202","<p>I suggest you using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity#metadata-options"" rel=""nofollow noreferrer"">GetMetadata Activity</a> in Azure Data Factory, from this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity#supported-connectors"" rel=""nofollow noreferrer"">supported list</a>, it supports ADL connector.</p>

<p><a href=""https://i.stack.imgur.com/TUExw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TUExw.png"" alt=""enter image description here""></a></p>

<p>The GetMetadata result is shown in activity output. To use the result in subsequent activity, use the pattern of <code>@{activity('MyGetMetadataActivity').output.itemName}</code>.</p>
"
"54775456","Azure Data Factory Compression Type","<p>I am working on a project trying to zip the files in Azure Blob Storage. I know the Azure Data Factory support compression type option, but I cannot find any reference to how this compression process behaves.</p>

<p>If I want to generate a *.zip file:</p>

<p>Origin Files:</p>

<pre><code>ParentFolder
    Image1.jpeg
    Txt1.txt
    ChildFolder
         Image2.jpeg
         Txt.txt
</code></pre>

<p>Is it going to zip only the ParentFolder? Or it is going to zip every single file recursively?</p>
","<azure><azure-data-factory>","2019-02-19 21:47:44","3214","0","1","54778774","<p>Compression type seems does not support <code>.zip</code>, it just supports <code>GZIP, Deflate, BZIP2, ZipDeflate</code>, see this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#compression-support"" rel=""nofollow noreferrer"">link</a>.</p>

<p>I test to copy the files like the sample you mentioned from one storage account to another one, use <code>GZIP</code>.</p>

<p>The files after being copied, it will be like as below(choose the <code>Copy file recursively</code> option).</p>

<pre><code>ParentFolder
    Image1.jpeg.gz
    Txt1.txt.gz
    ChildFolder
         Image2.jpeg.gz
         Txt.txt.gz
</code></pre>

<p>If not choose the <code>Copy file recursively</code> option, it will be like as below.</p>

<pre><code>ParentFolder
        Image1.jpeg.gz
        Txt1.txt.gz
</code></pre>
"
"54755940","How to get around Azure Data Factory ARM Template parameter limits?","<p>I have created an Data Factory in Azure with a about 10-15 pipelines in it and they all work fine and I can debug and run them without issue on the Test Data Factory that I have setup.  I have also setup Azure DevOps integration with the Data Factory with a CI/CD pipeline.  </p>

<p>Initially this pipeline worked fine as well and I was able to use the generated ARM template to recreate the Pipelines/Triggers/etc on a production Data Factory instance.</p>

<p>Now I have added a few more things to the Data Factory and when it generates the ARM template now it creates a template that with 293 parameters, which when it is run through the CI/CD pipeline fails because only 256 parameters are allowed with an ARM template.</p>

<p>A lot of the items that it put in the parameter file I do not need to be parameterized, like filename and file paths.  All I really want to have parameterized is the various connection information.  </p>

<p>I tried to create a second ARM Template parameter file and remove the parameters that I didn't want because I read some documentation about being able to do this to add some additional parameters, but this doesn't work because it doesn't remove the parameters out of the ARM Template itself.</p>

<p>So my question is, is there a way to handle this?  The things that I can think of doing are creating multiple Data Factories and only having a couple of pipelines in each one.  I don't really like this idea because it would become very large and cumbersome over time.  I could manually remove the templates out of the parameter file and also out of the template itself, but I really don't like this solution as it is manual and really error prone.</p>

<p>What I would ideally like to be able to do is to define which items I want to be parameterized in the Pipeline, like I mentioned 90% of the things that it parameterized I don't need to be parameterized, but I can't find any way to do this (short of manually doing it).</p>
","<azure><azure-data-factory><azure-rm-template>","2019-02-18 21:54:12","1867","1","2","54760169","<p>2 ways:</p>

<ol>
<li>obvious - remove unwanted parameters</li>
<li>less obvious - use objects instead of strings.</li>
</ol>

<p>let me explain, imagine you have 5 input parameters called param1,2,3,4,5. you can ""compress"" them into a single parameter like this:</p>

<pre><code>""param"": {
    ""type"": object,
    ""defaultValue"": {
        ""param1"": ""something"",
        xxx
        ""param5"": ""otherthing""
    }
}
</code></pre>

<p>seeing you have 290 parameters, I'd probably go the object route, because that way you can logically group them.</p>

<p>as for not parameterizing thing - you can define variable of default values for them (but if you define default values they still count for parameters), so variables allow you to do that.</p>
"
"54755940","How to get around Azure Data Factory ARM Template parameter limits?","<p>I have created an Data Factory in Azure with a about 10-15 pipelines in it and they all work fine and I can debug and run them without issue on the Test Data Factory that I have setup.  I have also setup Azure DevOps integration with the Data Factory with a CI/CD pipeline.  </p>

<p>Initially this pipeline worked fine as well and I was able to use the generated ARM template to recreate the Pipelines/Triggers/etc on a production Data Factory instance.</p>

<p>Now I have added a few more things to the Data Factory and when it generates the ARM template now it creates a template that with 293 parameters, which when it is run through the CI/CD pipeline fails because only 256 parameters are allowed with an ARM template.</p>

<p>A lot of the items that it put in the parameter file I do not need to be parameterized, like filename and file paths.  All I really want to have parameterized is the various connection information.  </p>

<p>I tried to create a second ARM Template parameter file and remove the parameters that I didn't want because I read some documentation about being able to do this to add some additional parameters, but this doesn't work because it doesn't remove the parameters out of the ARM Template itself.</p>

<p>So my question is, is there a way to handle this?  The things that I can think of doing are creating multiple Data Factories and only having a couple of pipelines in each one.  I don't really like this idea because it would become very large and cumbersome over time.  I could manually remove the templates out of the parameter file and also out of the template itself, but I really don't like this solution as it is manual and really error prone.</p>

<p>What I would ideally like to be able to do is to define which items I want to be parameterized in the Pipeline, like I mentioned 90% of the things that it parameterized I don't need to be parameterized, but I can't find any way to do this (short of manually doing it).</p>
","<azure><azure-data-factory><azure-rm-template>","2019-02-18 21:54:12","1867","1","2","65247882","<p>Use a custom parameter template as described in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template"" rel=""nofollow noreferrer"">MS Documentation: Use custom parameters with the Resource Manager template</a>.</p>
"
"54744597","Upgrade Data Factory to GA API","<p>This morning, when I logged in to the Data Factory UI, I was presented with the following message:</p>

<p><a href=""https://i.stack.imgur.com/u2ZuM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u2ZuM.png"" alt=""enter image description here""></a>
I clicked ""Cancel"" as we weren't quite ready to upgrade at the time, but now that we have ensured that all our PowerShell scripts, etc. are unaffected, we want to perform the upgrade. Unfortunately, I can't find anywhere in the ADF UI nor in the Azure portal, where we can initiate the upgrade, and the message box no longer shows up when logging into ADF. How do we proceed?</p>
","<azure-data-factory>","2019-02-18 09:59:12","171","1","1","54950103","<p>There should be a banner on top of your authoring screen that has the upgrade button. If that's not showing please refresh your browser and the banner should show up again :)
Let me know if this solves it.</p>
"
"54707996","How to set the path of a CSV file that is in account storage in azure data factory pipeline","<p>I have created a SSIS package that reads from a CSV file (using the <code>Flat file connection manager</code>) and loads records into a database. I have deployed it on Azure data factory pipeline and I need to give the path of the CSV file as a parameter. I have created a azure storage account and uploaded the source file there as shown below. </p>

<p>Can I just give the URL of the source file for the Import file in the SSIS package settings as shown below? I tried it but it is currently throwing 2906 error. I am new to Azure - appreciate any help here. </p>

<p><a href=""https://i.stack.imgur.com/afxqP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/afxqP.png"" alt=""Azure account storage - Source file:""></a></p>

<p><a href=""https://i.stack.imgur.com/SQCFk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SQCFk.png"" alt=""Azure DF - Execute SSIS Package""></a></p>
","<ssis><azure-storage><azure-pipelines><azure-data-factory>","2019-02-15 11:07:25","433","3","1","54716326","<p>First, you said Excel and then you said CSV. Those are two different formats. But since you mention the flat file connection manager, I'm going to assume you meant CSV. If not, let me know and I'll update my answer. </p>

<p>I think you will need to install the <a href=""https://www.microsoft.com/en-us/download/details.aspx?id=54798"" rel=""nofollow noreferrer"">SSIS Feature Pack for Azure</a> and use the <a href=""https://learn.microsoft.com/en-us/sql/integration-services/connection-manager/azure-storage-connection-manager?view=sql-server-2017"" rel=""nofollow noreferrer"">Azure Storage Connection Manager</a>. You can then use the <a href=""https://learn.microsoft.com/en-us/sql/integration-services/data-flow/azure-blob-source?view=sql-server-2017"" rel=""nofollow noreferrer"">Azure Blob Source</a> in your data flow task (it supports CSV files). When you add the blob source, the GUI should help you create the new connection manager. There is a <a href=""https://www.mssqltips.com/sqlservertip/4447/how-to-import-data-from-azure-blob-storage-to-a-local-file-with-sql-server-integration-services/"" rel=""nofollow noreferrer"">tutorial on MS SQL Tips</a> that shows each step. It's a couple years old, but I don't think much has changed. </p>

<p>As a side thought, is there a reason you chose to use SSIS over native ADF V2? It does a nice job of <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">copying data</a> from blob storage to a database. </p>
"
"54706153","Sending parameter from web activity in Data Factory to logic apps","<p>I can successfully trigger Logic App from my Pipeline in ADFv2 via web activity. But now I would like to send also some user-defined parameters to logic app.
My question is now:</p>

<ul>
<li><p>How can I send parameter from web Activity to logic app</p></li>
<li><p>How can I extract this parameter in logic app</p></li>
</ul>
","<azure><azure-data-factory><azure-logic-apps>","2019-02-15 09:24:04","2587","1","1","54707592","<p><strong>On the Azure Data Factory-v2 side:</strong></p>

<ol>
<li>Click on the web activity. Go to the settings tab of the activity.</li>
</ol>

<p><a href=""https://i.stack.imgur.com/jk5MB.png"" rel=""nofollow noreferrer"">See this image for how to fill fields in settings tab</a></p>

<ol start=""2"">
<li><p>You have already figured what goes into URL and Method field in settings tab as you have successfully triggered logic app.</p></li>
<li><p>Let's suppose we want to send parameters in JSON (preferred way). Set 'NAME' Headers field to 'Content-Type' and 'VALUE' to 'application/json'.</p></li>
<li><p>In the body send you parameters in the form of JSON. Let's send following dummy parameters</p></li>
</ol>

<pre><code>{""Location"":""northeurope"",""Model"":""dummy_model"",""Server_name"":""dummy_service"",""Onwer_email"":""dummy@dummy.com""}
</code></pre>

<p><strong>On the Logic App side:</strong></p>

<ol>
<li><p>You have already used 'When a HTTP request is received' trigger for logic app.</p></li>
<li><p>In the 'Request Body JSON Schema' field, enter the following schema to catch parameters send from ADFv2 web activity:</p></li>
</ol>

<pre><code>{
    ""properties"": {
        ""Location"": {
            ""type"": ""string""
        },
        ""Model"": {
            ""type"": ""string""
        },
        ""Onwer_email"": {
            ""type"": ""string""
        },
        ""Server_name"": {
            ""type"": ""string""
        }
    },
    ""type"": ""object""
}
</code></pre>

<p><a href=""https://i.stack.imgur.com/xrzof.png"" rel=""nofollow noreferrer"">See this image for help</a></p>

<ol start=""3"">
<li><p>You can also use 'Use sample payload to generate schema' instead of doing step 2 above. When using this option simply paste the json that you passed in body of ADFv2 web activity. It will automatically generate JSON schema to catch parameter.</p></li>
<li><p>Set the 'Method' field to the same method that you selected in ADFv2 web activity 'Method' field. </p></li>
<li><p>In subsequent steps in logic apps (for example initialize variable step) you can now use parameters set above (Location, Model, Onwer_email and Server_name) as Dynamic content using 'Add dynamic content' option. <a href=""https://i.stack.imgur.com/Du37C.png"" rel=""nofollow noreferrer"">See this image for help.</a></p></li>
</ol>
"
"54688908","Azure data factory: Using output of Rest in copy data activity in next activity","<p>I have copy data activity which copies data from Rest to SQL Server. Rest returns a json response. I need to have another Web activity after success of copy data. This activity needs data from previous rest api response (which is part of copy data). Any idea how we can achieve this. </p>

<p>I have tried using </p>

<pre><code>@{activity('ACTIVITY_NAME').output.&lt;json_field_from_response&gt;
</code></pre>

<p>I get following error. </p>

<pre><code>{
    ""errorCode"": ""InvalidTemplate"",
    ""message"": ""The expression 'activity('ACTIVITY_NAME').output.batch_id' cannot be evaluated because property 'batch_id' doesn't exist, available properties are 'dataRead, dataWritten, rowsRead, rowsCopied, copyDuration, throughput, errors, effectiveIntegrationRuntime, usedDataIntegrationUnits, usedParallelCopies, executionDetails'."",
    ""failureType"": ""UserError"",
    ""target"": ""Web1""
}
</code></pre>

<p>I am hoping there will be some way in dataset or pipeline to set variable to be used later. But I am not able to find it. Thanks.</p>
","<azure-data-factory>","2019-02-14 11:02:31","1084","0","1","54690431","<p>You could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">lookup activity</a> and get the data from <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity#use-the-lookup-activity-result-in-a-subsequent-activity"" rel=""nofollow noreferrer"">lookup output</a>.</p>
"
"54684909","Azure Data Lake Gen 2 Integration with DataFactory","<p>I am trying to connect data lake gen2 with data factory v2 where we need to add the user through Add user Wizard in data lake . But we couldnt see that option in this and we are not able to connect to data lake gen 2 from data factory . Please help .Let us know what is different approach than Gen1 for Gen2 in connecting to ADF V2.</p>

<p>THank you,
Sashank Pappu</p>
","<azure><azure-data-lake><azure-data-factory>","2019-02-14 07:09:20","1266","0","1","54833026","<p>I'm not exactly sure what you mean by ""where we need to add the user through Add user Wizard in data lake"". Currently, Data Factory V2 supports connecting to Azure Data Lake Storage Gen2 via: </p>

<ul>
<li>account key</li>
<li>service principal</li>
<li>managed identity</li>
</ul>

<p>To create a linked service in ADF, create a new dataset and choose Azure Data Lake Storage Gen2.<br>
<a href=""https://i.stack.imgur.com/xI4Dx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xI4Dx.png"" alt=""enter image description here""></a></p>

<p>When creating the linked service, choose your authentication method. If you go with account key, your url will look like <code>https://&lt;accountname&gt;.dfs.core.windows.net</code>. It's easiest to use the From Azure subscription to find your account rather than typing it in. </p>

<p>If you choose Service Principal, you will need to have <a href=""https://learn.microsoft.com/en-us/azure/storage/common/storage-auth-aad-app#register-your-application-with-an-azure-ad-tenant"" rel=""nofollow noreferrer"">registered your app in AD</a> and granted the service princpal Storage Blob Data Reader and/or Storage Blob Data Contributor role. </p>

<p>If you use Managed Service Identity, grab the service identity application ID from the window right underneath where you chose your storage account while creating your linked service. 
<a href=""https://i.stack.imgur.com/oNUeX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oNUeX.png"" alt=""enter image description here""></a></p>

<p>Then give it appropriate permissions. Go to the Azure Portal and open your storage account. Choose Access Control (IAM). Click on Role assignments and then click the Add button. Select Storage Blob Data Reader (or Storage Blob Data Writer if necessary). Leave Assign access to set on Azure AD user, group, or service principal. Paste in the service identity (for MSI, for Service Principal, paste in the application ID) in the Select box. It will search and return an identity with the name of your data factory. Select it and click save.  </p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage"" rel=""nofollow noreferrer"">Here's a good link that explains all the details.</a></p>
"
"54664520","Oracle Integration Run Time Sharing Issue","<p>I'm using Oracle Self hosted IR in my Dev data factory. Now I want to share this IR to another Data Factory  and I've followed the below instructions to create the linked IR.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime</a></p>

<p>But I'm receiving the below permission issue.</p>

<pre><code>Failed to list the data factories. 
Detail:{
    ""odata.error"":{
        ""code"":""Authorization_RequestDenied"",
        ""message"":{
            ""lang"":""en"",
            ""value"":""Guest users are not allowed to perform this action.""
        },
        ""requestId"":""841a68c6-645d-43c0-bd6c-e592208880c1"",
        ""date"":""2019-02-08T08:03:19""
    }
}
</code></pre>

<p>The logged in user into Azure Portal is having Guest but at the data factory (both) level he is the Owner for two Data Factories.</p>

<p>I'm searching the data factory by name and the ID for grating the permissions.</p>

<p>How can we identify the MSI of data factoy?</p>

<p>Thanks in Advance.</p>
","<azure-data-factory>","2019-02-13 07:14:24","26","0","1","54706111","<p>You can get the MSI of data factory from the ""Properties"" of your data factory on the portal. Here is an example:
<a href=""https://i.stack.imgur.com/EbSrZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EbSrZ.png"" alt=""enter image description here""></a></p>
"
"54648899","How send a notification once DataFactory Job runs or failed?","<p>I wanted to send a notification once DataFactory Job runs or failed.</p>

<p>I don't want to send it through Logic apps and Azure alters.</p>

<p>Please suggest some way to manage this situation. 
<a href=""https://i.stack.imgur.com/vYZcn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vYZcn.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2019-02-12 11:16:32","6168","2","3","54661539","<p>As I know,ADF has own <strong>Alert/Monitor</strong> mechanism.Please see the button in the ADF portal as below.</p>

<p><a href=""https://i.stack.imgur.com/04W0l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/04W0l.png"" alt=""enter image description here""></a></p>

<p>You could create new Alert Rule.</p>

<p><a href=""https://i.stack.imgur.com/Yo7Qi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Yo7Qi.png"" alt=""enter image description here""></a></p>

<p>Configure alert Criteria.</p>

<p><a href=""https://i.stack.imgur.com/txHvK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/txHvK.png"" alt=""enter image description here""></a></p>

<p>Configure alert channel.</p>

<p><a href=""https://i.stack.imgur.com/4t3uS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4t3uS.png"" alt=""enter image description here""></a></p>
"
"54648899","How send a notification once DataFactory Job runs or failed?","<p>I wanted to send a notification once DataFactory Job runs or failed.</p>

<p>I don't want to send it through Logic apps and Azure alters.</p>

<p>Please suggest some way to manage this situation. 
<a href=""https://i.stack.imgur.com/vYZcn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vYZcn.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2019-02-12 11:16:32","6168","2","3","54698188","<p>I would suggest a combination of Azure Function + SendGrid. </p>

<p>SendGrid is a cloud-based email service, and on a free pricing tier, you can send 25k emails. Support .NET,Java and Python (i think).</p>

<p>So write an azure function that will send an email via SendGrid. 
On pipeline failure, you will just call AF.</p>

<p>I would suggest you in this scenario to pay as you go for AF, and as I said free tier for SendGrid. Minimum cost. </p>

<p><a href=""https://learn.microsoft.com/en-us/azure/sendgrid-dotnet-how-to-send-email"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/sendgrid-dotnet-how-to-send-email</a> here you can find how to use SendGrid (code example) </p>

<p><a href=""https://azure.microsoft.com/en-us/blog/azure-functions-now-supported-as-a-step-in-azure-data-factory-pipelines/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/blog/azure-functions-now-supported-as-a-step-in-azure-data-factory-pipelines/</a></p>
"
"54648899","How send a notification once DataFactory Job runs or failed?","<p>I wanted to send a notification once DataFactory Job runs or failed.</p>

<p>I don't want to send it through Logic apps and Azure alters.</p>

<p>Please suggest some way to manage this situation. 
<a href=""https://i.stack.imgur.com/vYZcn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vYZcn.png"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2019-02-12 11:16:32","6168","2","3","69127313","<p>I use webhook to send messages to Team. Here is the official template.</p>
<p><a href=""https://i.stack.imgur.com/jtHbb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jtHbb.png"" alt=""enter image description here"" /></a></p>
<h2>failed</h2>
<p>here is my UI. I add a Lookup Activity to search a non-existent DB table to raise an error.</p>
<p><a href=""https://i.stack.imgur.com/fBqjv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fBqjv.png"" alt=""enter image description here"" /></a></p>
<p>And then I connect it to copy activity.</p>
<p><a href=""https://i.stack.imgur.com/I8E52.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/I8E52.png"" alt=""enter image description here"" /></a></p>
<h2>succeeded</h2>
<p>It's the same way, but not need to raise an error.</p>
<p><a href=""https://i.stack.imgur.com/sZraI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sZraI.png"" alt=""enter image description here"" /></a></p>
"
"54635420","Azure Data Factory from On-Premises PostgreSQL","<p>I'm trying to create a Linked Service connection in ADF from an on-premises PostgreSQL to Azure DW.  I've selected the PostgreSQL connection type, but am unable to establish an ODBC connection - </p>

<blockquote>
  <p>""Connection refused.  Cannot connect to host""</p>
</blockquote>

<p>which I think is expected considering the PostgreSQL server is on a different cloud network. </p>

<p>What steps do I need to take in order to create a connection between the two?  Can I create ADF Pipelines directly thereafter, or do I need to create an SSIS Runtime and work with SSIS packages only? </p>
","<postgresql><azure><azure-data-factory>","2019-02-11 16:54:13","885","0","1","54922688","<p>Since you want to transfer data from on-premises PostgreSQL to Azure DW, i think you need to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">create and configure a self-hosted integration runtime</a>.As the statements you could find in this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#self-hosted-integration-runtime"" rel=""nofollow noreferrer"">document</a>:</p>

<blockquote>
  <p>A self-hosted IR is capable of running copy activity between a cloud
  data stores and a data store in private network. </p>
</blockquote>

<p>Please follow the steps to create Self-hosted IR:<a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#install-and-register-self-hosted-ir-from-the-download-center"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#install-and-register-self-hosted-ir-from-the-download-center</a></p>

<p>BTW,according to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats"" rel=""nofollow noreferrer"">document</a>,PostgreSQL only supports source link,not for sink, which is suitable for you.</p>
"
"54620090","Copy only the latest file from azure data lake store with Azure Data Factory (ADF)","<p>I'm trying to copy data from azure data lake store, perform some processing and move it into a different folder in the same data lake using azure data factory. The source data is organized by year, month and date. I only want to copy the latest file every day and keep replacing it. How can I do this using ADF? I see some answers about using slice start and end dates but that would still mean giving the dates in the JSON file. How do I change those dates automatically?</p>
","<azure><copy><azure-data-lake><azure-data-factory>","2019-02-10 19:21:00","819","0","1","54671702","<p>Assuming you are using ADFv2.</p>

<blockquote>
  <p>I see some answers about using slice start and end dates but that
  would still mean giving the dates in the JSON file. How do I change
  those dates automatically?</p>
</blockquote>

<p>Thats the way to go, it is automatic. You do not have to give dates in pipeline. You parameterize the date and attach the pipeline to a <strong>Tumbling window trigger</strong> and use trigger system variables @trigger().outputs.windowStartTime. So now the trigger will give the dates.</p>

<p>Then you can schedule the trigger every 24 hours so that your tumbling window trigger will pass the @trigger().outputs.windowStartTime e.g. 2019/02/13 (you can format the date as you need based on your datalake structure - format options available in ADF) to the pipeline activity and asks the activity to read from azuredatalake/2019/02/13/file.txt</p>

<p>Follow this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-read-write-partitioned-data"" rel=""nofollow noreferrer"">doc</a> to get an idea.</p>
"
"54616788","jsonNodeReference and jsonPathDefinition for multiple child nodes","<p>I want to know how to apply jsonNodeReference and jsonPathDefinition for multiple child node in azure data factory</p>

<p>Example json data:</p>

<pre><code>{
 ""success"": true,
""profiles"": [
    {
        ""id"": ""123"",
        ""data"": [
            {
             ""date"": ""2018-11-08T00:00:00"",
             ""name"": ""sree"",
             ""Quantity"": ""10""
            },
            {
                ""date"": ""2018-11-09T00:00:00"",
                ""name"": ""sree"",
                ""Quantity"": ""20""
            }
               ]
      },
      {
        ""id"": ""154"",
        ""data"": [
            {
             ""date"": ""2018-11-08T00:00:00"",
             ""name"": ""Dhanu"",
             ""Quantity"": ""25""
            },
            {
             ""date"": ""2018-11-09T00:00:00"",
             ""name"": ""Dhanu"",
             ""Quantity"": ""29""
            }
              ]
        }
   ]
</code></pre>

<p>}</p>

<p>I have tried with ""jsonNodeReference"":""$.['profiles']"" and 
""jsonNodeReference"":""$.['profiles']['data']"" in the input dataset</p>

<p>The expected result is</p>

<pre><code>Success Id  Date                name    Quantity
TRUE    123 2018-11-08T00:00:00 Sree    10
TRUE    123 2018-11-09T00:00:00 Sree    20
TRUE    154 2018-11-08T00:00:00 Dhanu   25
TRUE    154 2018-11-09T00:00:00 Dhanu   29
</code></pre>
","<azure><azure-data-factory>","2019-02-10 13:17:50","140","0","1","54631187","<p>ADF only supports expand one array at a time. For example, you could example profiles[0] like the following. 
$.['profiles'][0]['data']</p>

<p>For your scenario, I guess you could use ADF custom activity to parse it yourself.</p>

<p><a href=""https://i.stack.imgur.com/0IpHi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0IpHi.png"" alt=""enter image description here""></a></p>
"
"54607459","Azure Data Factory Dynamic content parameter","<p>I am trying to load the data from the last runtime to lastmodifieddate from the source tables using Azure Data Factory.</p>

<p>this is working fine :</p>

<pre><code>@concat(' SELECT  * FROM dbo. ',
        item().TABLE_list ,
       ' WHERE   modifieddate &gt; DATEADD(day, -1, GETDATE())')""
</code></pre>

<p>when i use:</p>

<pre><code>@concat(' SELECT  * FROM dbo.  ',
         item().TABLE_list ,
        '  WHERE   modifieddate &gt; @{formatDateTime(
                  addhours(pipeline().TriggerTime-24)),
                  ''yyyy','-','MM','-','ddTHH',':','mm',':','ssZ''}')
</code></pre>

<p>getting error as """"errorCode"": ""2200"",</p>

<pre><code>""message"": ""Failure happened on 'Source' side. 'Type=System.Data.SqlClient.SqlException,Message=Must declare the scalar variable \""@\"".,Source=.Net SqlClient Data Provider,SqlErrorNumber=137,Class=15,ErrorCode=-2146232060,State=2,Errors=[{Class=15,Number=137,State=2,Message=Must declare the scalar variable \""@\"".,},],'"",
    ""failureType"": ""UserError"",
    ""target"": ""Copy Data1""
}
</code></pre>

<ul>
<li><p>what mistake am I doing?  </p></li>
<li><p>I need to pass dynamically last run time date of pipeline after > in where condition.</p></li>
</ul>
","<azure><azure-data-factory>","2019-02-09 15:01:02","5055","0","1","54665522","<pre><code>    SELECT * 
    FROM dbo.@{item().TABLE_LIST} 
    WHERE modifieddate &gt; 
    @{formatDateTime(addhours(pipeline().TriggerTime, -24), 'yyyy-MM-ddTHH:mm:ssZ')}
</code></pre>
<p>You could use string interpolation expression. Concat makes things complicated.</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#expressions"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#expressions</a></p>
"
"54600618","Delete File from ADL after processing via ADF","<p>This is a 2 part issue:</p>

<ol>
<li>How to move/delete a file after processing.  Currently we can only copy, but the original file remains in the source.  Ideally we don't want to add a separate process to get metadata and compare if processed or not.  I have been following one blog that asks to use a WEB activity and the Delete Rest API.  I have gotten to that point, but I am not able to understand/follow instructions on to obtain the ACCESS TOKEN.   It shows Curl steps, if someone can help on this would be great. </li>
<li>I have created a pipeline, that has 4 Main activities.  As part of this pipeline I want to be able to send email notifications for Success and failure.</li>
</ol>

<p>Each success of an activity moves to the next activity, but if any activity fails it will send email.  I Want to be able to have one SUCCESS or FAILURE Notification and dynamically add content to email instead of sending email on previous activity.   For Failure it seems like I have to create a separate web activity for each activity to align a Failure, which is not elegant. </p>

<p>Based on how the WEB activity is set to use the logical apps for email, we have to define the activity name that the email is associated with (at least based on my understanding). So i created a separate WEB activity for each failure email, not very elegant.</p>

<p>Does anyone have a better way to handle.</p>

<p>enter image description here</p>

<p><a href=""https://i.stack.imgur.com/ZGi4w.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<azure-data-factory>","2019-02-08 21:40:12","398","0","1","54603940","<p>The only alternative is using a logic app to delete blobs, you can trigger the logic app also via a Web activity. Can you share the blog URL  for the delete via web? </p>

<p>As for the email on failure, the issue is that you cannot have 'or' dependencies in the pipeline. A workaround that I'm using is to introduce a variable, when an activity fails or when the pipeline is finished, it sets the variable. In parallel in the pipeline I have a while activity that checks this variable. When it gets a certain value an action will be triggered. Also not elegant, but you don't need a lot of Web activities this way. </p>
"
"54599813","How to enable the access to Azure services in my AZURE SQL database server?","<p>I'm trying to create a ""Integration Runtime"" (AZURE-SSIS type) in my data factory. (To run my SSIS packages in cloud) I already have a virtual machine (Azure SQL data base server Up and running). </p>

<p>While during the IR creation process I'm not able to link my AZURE SQL database SSISDB catalog to ""Catalog database server endpoint""  (OR) the server is not showing up in the drop down box.</p>

<p>In the MSDN blog it says  </p>

<pre><code>""Confirm that the **Allow access to Azure services** setting is enabled for the database server. This is not applicable when you use Azure SQL Database with virtual network service endpoints/Managed Instance to host SSISDB.""
</code></pre>

<p>Could anyone know how to enable this feature ? I hope by enabling this feature I can link my server in the IR and run the packages in the cloud.</p>

<p>Thanks </p>
","<azure><azure-sql-database><azure-data-factory>","2019-02-08 20:35:13","949","0","2","54600004","<blockquote>
  <p>To allow applications from Azure to connect to your Azure SQL server,
  Azure connections must be enabled. When an application from Azure
  attempts to connect to your database server, the firewall verifies
  that Azure connections are allowed. A firewall setting with starting
  and ending address equal to 0.0.0.0 indicates Azure connections are
  allowed. If the connection attempt is not allowed, the request does
  not reach the Azure SQL Database server.</p>
</blockquote>

<p>you can do it any way, powershell, az cli, arm templates. if you go to the portal to the firewall blade, there would be a button to do that.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/sql-database/sql-database-firewall-configure"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/sql-database/sql-database-firewall-configure</a></p>
"
"54599813","How to enable the access to Azure services in my AZURE SQL database server?","<p>I'm trying to create a ""Integration Runtime"" (AZURE-SSIS type) in my data factory. (To run my SSIS packages in cloud) I already have a virtual machine (Azure SQL data base server Up and running). </p>

<p>While during the IR creation process I'm not able to link my AZURE SQL database SSISDB catalog to ""Catalog database server endpoint""  (OR) the server is not showing up in the drop down box.</p>

<p>In the MSDN blog it says  </p>

<pre><code>""Confirm that the **Allow access to Azure services** setting is enabled for the database server. This is not applicable when you use Azure SQL Database with virtual network service endpoints/Managed Instance to host SSISDB.""
</code></pre>

<p>Could anyone know how to enable this feature ? I hope by enabling this feature I can link my server in the IR and run the packages in the cloud.</p>

<p>Thanks </p>
","<azure><azure-sql-database><azure-data-factory>","2019-02-08 20:35:13","949","0","2","54600070","<p>Please take a look where is located on below image.</p>
<p><a href=""https://i.stack.imgur.com/73fA2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/73fA2.png"" alt=""enter image description here"" /></a></p>
<p>On <a href=""https://learn.microsoft.com/en-us/azure/sql-database/sql-database-server-level-firewall-rule#create-a-server-level-ip-firewall-rule"" rel=""nofollow noreferrer"">this</a> documentation you will have instructions how to reach that screen.</p>
"
"54594226","Data ingestion from Rest API using Azure","<p>As part of a POC i need to get data from REST API end point that return JSON data as Response and then save it AS-IS into Azure SQL database.</p>

<p>This REST API will be used for both Historical (Around 10000 records expected) and incremental load that will be executing at some regular intervals. Rest API use OAuth 2.0 access token for Authorization</p>

<p>Please let me know the best options available for doing data ingestion in Azure , should i use ADF or Azure functions and what are the best practices for configuring REST API in ADF in particular to error handling.</p>

<p>If this POC works successfully i need to integrate few more API calls into the design as well.</p>
","<azure><azure-sql-database><azure-functions><azure-data-factory>","2019-02-08 14:12:07","1869","2","1","54598659","<p>I saw this post yesterday on this topic, using DataFactory to pull data from a rest API.</p>

<p><a href=""https://www.linkedin.com/pulse/azure-data-factory-copy-from-rest-api-sql-database-rohit-vangala"" rel=""nofollow noreferrer"">https://www.linkedin.com/pulse/azure-data-factory-copy-from-rest-api-sql-database-rohit-vangala</a></p>
"
"54589413","Azure Data Factory REST API to Service Now - Pagination Issue","<p>We are trying to use Azure Data Factory to pull data from ServiceNow using the REST API Connector. </p>

<p>We have tried working with the specific ServiceNow connector in ADF but this does not perform adequately well for our purposes - we need to use actuals and display values and take frequent small loads, you can't filter display in place so it's too slow.</p>

<p>In simple tests the REST API works fine, the challenge is specifically around pagination. After the initial GET request, the ServiceNow API returns relative links in the response header in the following format;</p>

<pre><code>Link →&lt;url&gt;;rel=""first"",&lt;url&gt;;rel=""next"",&lt;url&gt;;rel=""last""
</code></pre>

<p>The REST Resource dataset has settings for Pagination Rules and the documentation suggests that this can be handled - <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest#pagination-support"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-rest#pagination-support</a></p>

<blockquote>
  <p>This generic REST connector supports the following pagination patterns:</p>
  
  <p><em>Next request’s header = header value in current response headers</em></p>
</blockquote>

<p>We can't figure out what to put in the Pagination Rules Key/Value in order have it navigate to the rel=""next"" URL each time it receives a page of data.</p>

<p>We have tried most of the options described in the documentation - something like this seems close:</p>

<pre><code>            {""paginationRules"": {
            ""AbsoluteUrl"": ""Headers.['Link']""}
</code></pre>

<p>It seems like the Headers.['Link'] part is formed correctly but isn't specific enough.</p>

<p>Has anyone advise what the setting should be to make this work?</p>
","<pagination><azure-data-factory><servicenow-rest-api>","2019-02-08 09:33:11","2728","1","2","54664082","<p>With the REST api you can do pagination by adding this parameter to the link</p>

<pre><code>&amp;sysparm_offset=10000
</code></pre>

<p>Limit is default 10000 if you want a lower limit add </p>

<pre><code>&amp;sysparm_limit=300
</code></pre>

<p>and adjust offset accordingly.</p>
"
"54589413","Azure Data Factory REST API to Service Now - Pagination Issue","<p>We are trying to use Azure Data Factory to pull data from ServiceNow using the REST API Connector. </p>

<p>We have tried working with the specific ServiceNow connector in ADF but this does not perform adequately well for our purposes - we need to use actuals and display values and take frequent small loads, you can't filter display in place so it's too slow.</p>

<p>In simple tests the REST API works fine, the challenge is specifically around pagination. After the initial GET request, the ServiceNow API returns relative links in the response header in the following format;</p>

<pre><code>Link →&lt;url&gt;;rel=""first"",&lt;url&gt;;rel=""next"",&lt;url&gt;;rel=""last""
</code></pre>

<p>The REST Resource dataset has settings for Pagination Rules and the documentation suggests that this can be handled - <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-rest#pagination-support"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-rest#pagination-support</a></p>

<blockquote>
  <p>This generic REST connector supports the following pagination patterns:</p>
  
  <p><em>Next request’s header = header value in current response headers</em></p>
</blockquote>

<p>We can't figure out what to put in the Pagination Rules Key/Value in order have it navigate to the rel=""next"" URL each time it receives a page of data.</p>

<p>We have tried most of the options described in the documentation - something like this seems close:</p>

<pre><code>            {""paginationRules"": {
            ""AbsoluteUrl"": ""Headers.['Link']""}
</code></pre>

<p>It seems like the Headers.['Link'] part is formed correctly but isn't specific enough.</p>

<p>Has anyone advise what the setting should be to make this work?</p>
","<pagination><azure-data-factory><servicenow-rest-api>","2019-02-08 09:33:11","2728","1","2","54923985","<p>if my understanding is correct, you want to extract the 3rd &lt;url&gt; from the Link header as the URL of next page.
    Link →&lt;url&gt;;rel=""first"",&lt;url&gt;;rel=""next"",&lt;url&gt;;rel=""last""</p>

<p>Pagination rule currently doesn't support expressions such as string extraction in this case.</p>
"
"54585497","Move Data from SQL Server VM to SQL Server VM on a daily basis","<p>I would like to know what is the fastest way to export the data of 40 tables around 500 MB each one (Biggest one 17 GB) of a SQL Server instance in Azure to a different SQL Server instance in Azure same data center on a daily basis so we can schedule the process very early and get the latest data available once we arrive to the work. Some options are generating raw files and FTP the files to the other server using SSIS, using DTS or use the copy tool of Azure Data Factory.</p>

<p>Thanks,
Vlad</p>
","<sql-server><azure><ssis><azure-data-factory>","2019-02-08 03:04:03","65","0","1","54589620","<p><strong>Disclaimer:</strong> this post is based pretty much  on a personal opinion</p>

<p>Because you need to do it on a scheduled basis and performance is important perhaps such approach is a way to go:</p>

<ol>
<li>Create a staging database [ExportDB] with simple recovery</li>
<li>Perform series of <code>SELECT * INTO [ExportDB].dbo.Table1 FROM YourDB.dbo.Table1</code>. Such approach involves BULK LOAD during the data load</li>
<li>Backup database with compression directly to azure blob storage </li>
<li>Restore ExportDB on another server from azure blob storage</li>
</ol>

<hr>

<p>References:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/virtual-machines/windows/sql/virtual-machines-windows-use-storage-sql-server-backup-restore"" rel=""nofollow noreferrer"">Use Azure Storage for SQL Server Backup and Restore</a></p>
"
"54583752","Do you have to use Azure Data Factory or can you just Databricks as your ETL tool from your multiple sources?","<p>...Or do i need to add the data into a data lake using data factory first and then use databricks as an ELT?</p>
","<azure><azure-data-factory>","2019-02-07 23:11:31","206","0","2","54671410","<p>Depends.</p>

<p>Databricks can connect to <a href=""https://docs.databricks.com/spark/latest/data-sources/index.html"" rel=""nofollow noreferrer"">datasources</a> and ingest data. However Azure Data Factory(ADF) have more <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">connectors</a> than databricks. So it depends on what you need. If using ADF, you need to land the data somewhere (i.e. Azure storage) so that databricks can pick it up.</p>

<p>Moreover, another main feature of ADF is to orchestrate data movement or activity. Databricks do have Job feature to schedule notebooks or JAR, however it is limited within databricks. If you want to schedule anything outside of databricks (e.g. drop file to SFTP or email on completion or terminate databricks cluster etc...) then ADF is the way to go.</p>
"
"54583752","Do you have to use Azure Data Factory or can you just Databricks as your ETL tool from your multiple sources?","<p>...Or do i need to add the data into a data lake using data factory first and then use databricks as an ELT?</p>
","<azure><azure-data-factory>","2019-02-07 23:11:31","206","0","2","54722069","<p>Indeed it depends to the scenario I think. If you have a wide variety of datascources you need to connect to then adf is probably the better option. </p>

<p>If your sources are datafiles (in any format) you could consider using databricks for etl.</p>

<p>I use databricks as a pure etl tool (without adf) by mounting a notebook to a storage container in a blobstorage, take huge xml data from there and write the data to a dataframe in databricks. Then I parse the shape of the dataframe and then writing the data into an azure sql database. Fair to say I’m not really using it for the “e” in etl, as the data has already been extracted from the real source system.</p>

<p>Big advantage is the power you have at your disposal to parse the files.</p>

<p>Best regards.</p>
"
"54563556","Azure Data Factory with Integration Runtime - Delete (or move) file after copy","<p>I have an on premise server with the Microsoft Integration Runtime installed.
In Azure Data Factory V2 I created a pipeline that copies files from the on premise server to a blob storage.</p>

<p>After a successful transfer I need to delete the files on the on premise server. I am not able to find a solution for this in the documentation. How can this be achieved?</p>
","<azure-data-factory>","2019-02-06 22:30:09","1199","1","3","54563968","<p>You have the option to call Azure Automation using webhooks, with the web activity. In Azure Automation you can program a powershell or python script with a Hybrid Runbook Worker to delete the file from the on premise server. You can read more on this here: <a href=""https://learn.microsoft.com/en-us/azure/automation/automation-hybrid-runbook-worker"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/automation/automation-hybrid-runbook-worker</a></p>

<p>Another easier option would be to program a script to be run on the server with the windows task scheduler where you run a script to delete the file. Make sure you program the script to be run after data factory has copied the files to the blob, and that's it!</p>

<p>Hope this helped!</p>
"
"54563556","Azure Data Factory with Integration Runtime - Delete (or move) file after copy","<p>I have an on premise server with the Microsoft Integration Runtime installed.
In Azure Data Factory V2 I created a pipeline that copies files from the on premise server to a blob storage.</p>

<p>After a successful transfer I need to delete the files on the on premise server. I am not able to find a solution for this in the documentation. How can this be achieved?</p>
","<azure-data-factory>","2019-02-06 22:30:09","1199","1","3","55178301","<p>Recently Azure Data Factory introduced a <a href=""https://learn.microsoft.com/de-de/azure/data-factory/delete-activity"" rel=""nofollow noreferrer"">Delete Activity</a> to delete files or folders from on-premise storage stores or cloud storage stores. </p>
"
"54563556","Azure Data Factory with Integration Runtime - Delete (or move) file after copy","<p>I have an on premise server with the Microsoft Integration Runtime installed.
In Azure Data Factory V2 I created a pipeline that copies files from the on premise server to a blob storage.</p>

<p>After a successful transfer I need to delete the files on the on premise server. I am not able to find a solution for this in the documentation. How can this be achieved?</p>
","<azure-data-factory>","2019-02-06 22:30:09","1199","1","3","67003924","<p>If you are simply moving the file then you can use a Binary dataset in a copy activity. This combination makes a checkbox setting visible that when enabled will automatically delete the file once the copy operation completes. This is a little nicer as you do not need the extra delete activity and the file is &quot;moved&quot; only if the copy operation is a success.</p>
<p><a href=""https://i.stack.imgur.com/Um8pC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Um8pC.png"" alt=""enter image description here"" /></a></p>
"
"54540491","Get/set user property on data lake blob in azure data factory","<p>I'm looking for a way to access user properties on blobs in my data lake as part of an ADFv2 pipeline.  For example, say I have a blob trigger associated with my ADLSv2 linked service, and I want to get the value of a user property on the blob that triggered my pipeline called ""dataVersion"".  </p>

<p>I imagined that I could use a Web activity, and build the URL using an expression that references the linked ADLSv2 service and the blob that triggered the pipeline to call the ADLS REST API to get the properties, but I'm struggling to find comprehensive documentation on what can be done with 'dynamic content'.</p>

<p>Other ideas for how you might access blob properties, or alternative ways to pass metadata along from pipeline to pipeline, would be welcome.</p>
","<azure-data-factory>","2019-02-05 18:04:18","1094","2","1","54567697","<p>1.If you want to get the properties in the blob file, you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">Look Up Activity</a>. It can retrieve a dataset from any of the Azure Data Factory-supported data sources.</p>

<blockquote>
  <p>Lookup activity reads and returns the content of a configuration file
  or table. It also returns the result of executing a query or stored
  procedure. The output from Lookup activity can be used in a subsequent
  copy or transformation activity if it's a singleton value. The output
  can be used in a ForEach activity if it's an array of attributes.</p>
</blockquote>

<p>So you could pass the value of the desired property to subsequent activity with the expression <code>@{activity('MyLookupActivity').output.firstRow.TableName}</code></p>

<p>2.If you want to get the metadata properties of blob itself,please use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">GetMetadata activity</a>. The value could be targeted by the expression ,such as <code>@{activity('MyGetMetadataActivity').output.itemName}</code></p>
"
"54540405","copying 14gb file from ftp to azure data lake store using ADF","<p>I am trying to copy 14gb file from FTP to my azure data lake store using Azure data factory. When I executed the pipeline it started copying the file and copied almost 13.9 gb within half hour. </p>

<p>Remaining data is not copied even after running the pipeline for 8 hours and finally failed by providing message that file not available. Reason for file not available is the source team removed the file for next file.</p>

<p>Increased the integration unit to 250</p>

<pre><code>{
    ""name"": ""job_fa"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""set_parameters_adh_or_sch"",
                ""description"": ""validate and set the parameter values based on the runtype sch or adh"",
                ""type"": ""Lookup"",
                ""dependsOn"": [
                    {
                        ""activity"": ""br_bs_loggin"",
                        ""dependencyConditions"": [
                            ""Succeeded""
                        ]
                    }
                ],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""userProperties"": [
                    {
                        ""name"": ""CheckLookup1"",
                        ""value"": ""1""
                    }
                ],
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""SqlSource"",
                        ""sqlReaderStoredProcedureName"": ""[dbo].[usp_FeedParameters_main]"",
                        ""storedProcedureParameters"": {
                            ""FeedName_in"": {
                                ""type"": ""String"",
                                ""value"": {
                                    ""value"": ""@pipeline().parameters.p_FeedName"",
                                    ""type"": ""Expression""
                                }
                            },
                            ""RunType_in"": {
                                ""type"": ""String"",
                                ""value"": {
                                    ""value"": ""@pipeline().parameters.p_RunType"",
                                    ""type"": ""Expression""
                                }
                            },
                            ""SrcEnddate_in"": {
                                ""type"": ""String"",
                                ""value"": {
                                    ""value"": ""@pipeline().parameters.p_SrcEndDate"",
                                    ""type"": ""Expression""
                                }
                            },
                            ""SrcStartdate_in"": {
                                ""type"": ""String"",
                                ""value"": {
                                    ""value"": ""@pipeline().parameters.p_SrcStartDate"",
                                    ""type"": ""Expression""
                                }
                            },
                            ""TgtDate_in"": {
                                ""type"": ""String"",
                                ""value"": {
                                    ""value"": ""@pipeline().parameters.p_TargetDate"",
                                    ""type"": ""Expression""
                                }
                            },
                            ""SrcHour_in"": {
                                ""type"": ""String"",
                                ""value"": {
                                    ""value"": ""@pipeline().parameters.p_SrcHour"",
                                    ""type"": ""Expression""
                                }
                            },
                            ""TgtHour_in"": {
                                ""type"": ""String"",
                                ""value"": {
                                    ""value"": ""@pipeline().parameters.p_TgtHour"",
                                    ""type"": ""Expression""
                                }
                            }
                        }
                    },
                    ""dataset"": {
                        ""referenceName"": ""AzureSql_cdpconfiguser"",
                        ""type"": ""DatasetReference""
                    },
                    ""firstRowOnly"": true
                }
            },
            {
                ""name"": ""br_bs_loggin"",
                ""description"": ""insert into the batch run and update the batch scheduler to started in case of sch run"",
                ""type"": ""Lookup"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""SqlSource"",
                        ""sqlReaderStoredProcedureName"": ""[dbo].[usp_BatchRun]"",
                        ""storedProcedureParameters"": {
                            ""FeedName_in"": {
                                ""type"": ""String"",
                                ""value"": {
                                    ""value"": ""@pipeline().parameters.p_FeedName"",
                                    ""type"": ""Expression""
                                }
                            },
                            ""RunType_in"": {
                                ""type"": ""String"",
                                ""value"": {
                                    ""value"": ""@pipeline().parameters.p_RunType"",
                                    ""type"": ""Expression""
                                }
                            },
                            ""Status_in"": {
                                ""type"": ""String"",
                                ""value"": ""Started""
                            }
                        }
                    },
                    ""dataset"": {
                        ""referenceName"": ""AzureSql_cdpconfiguser"",
                        ""type"": ""DatasetReference""
                    },
                    ""firstRowOnly"": true
                }
            },
            {
                ""name"": ""Check if file exists in target"",
                ""type"": ""GetMetadata"",
                ""dependsOn"": [
                    {
                        ""activity"": ""Copy Data WT to ADLS"",
                        ""dependencyConditions"": [
                            ""Succeeded""
                        ]
                    }
                ],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""typeProperties"": {
                    ""dataset"": {
                        ""referenceName"": ""AzureDataLakeStoreFile_wt_tgt_path_and_name"",
                        ""type"": ""DatasetReference"",
                        ""parameters"": {
                            ""TgtFilePath"": ""@activity('set_parameters_adh_or_sch').output.firstrow.TgtFileName_wt_dt_out"",
                            ""TgtFileName"": {
                                ""value"": ""@activity('set_parameters_adh_or_sch').output.firstrow.TgtFileName_wt_dt_out"",
                                ""type"": ""Expression""
                            }
                        }
                    },
                    ""fieldList"": [
                        ""exists"",
                        ""size""
                    ]
                }
            },
            {
                ""name"": ""Copy Data WT to ADLS"",
                ""type"": ""Copy"",
                ""dependsOn"": [
                    {
                        ""activity"": ""set_parameters_adh_or_sch"",
                        ""dependencyConditions"": [
                            ""Succeeded""
                        ]
                    }
                ],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""userProperties"": [
                    {
                        ""name"": ""Source"",
                        ""value"": ""@{activity('set_parameters_adh_or_sch').output.firstrow.SrcFilePath_wo_dt_out}/@{activity('set_parameters_adh_or_sch').output.firstrow.SrcFileName_wt_dt_out}""
                    },
                    {
                        ""name"": ""Destination"",
                        ""value"": ""@{activity('set_parameters_adh_or_sch').output.firstrow.TgtFilePath_wt_dt_out}/@{activity('set_parameters_adh_or_sch').output.firstrow.TgtFilePath_wt_dt_out}""
                    }
                ],
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""FileSystemSource"",
                        ""recursive"": true
                    },
                    ""sink"": {
                        ""type"": ""AzureDataLakeStoreSink""
                    },
                    ""enableStaging"": false,
                    ""dataIntegrationUnits"": 0
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""FTP_SRC_FA"",
                        ""type"": ""DatasetReference"",
                        ""parameters"": {
                            ""SrcFileName"": ""@activity('set_parameters_adh_or_sch').output.firstrow.SrcFileName_wt_dt_out"",
                            ""SrcFilePath"": ""@activity('set_parameters_adh_or_sch').output.firstrow.SrcFilePath_wo_dt_out""
                        }
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""AzureDataLakeStoreFile_wt_tgt_path_and_name"",
                        ""type"": ""DatasetReference"",
                        ""parameters"": {
                            ""TgtFilePath"": ""@activity('set_parameters_adh_or_sch').output.firstrow.TgtFileName_wt_dt_out"",
                            ""TgtFileName"": {
                                ""value"": ""@activity('set_parameters_adh_or_sch').output.firstrow.TgtFileName_wt_dt_out"",
                                ""type"": ""Expression""
                            }
                        }
                    }
                ]
            },
            {
                ""name"": ""br_bs_update_failed"",
                ""type"": ""SqlServerStoredProcedure"",
                ""dependsOn"": [
                    {
                        ""activity"": ""Copy Data WT to ADLS"",
                        ""dependencyConditions"": [
                            ""Failed""
                        ]
                    }
                ],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""typeProperties"": {
                    ""storedProcedureName"": ""[dbo].[usp_BatchRunUpdate]"",
                    ""storedProcedureParameters"": {
                        ""BatchId"": {
                            ""value"": {
                                ""value"": ""@activity('br_bs_loggin').output.firstrow.Batchid_out"",
                                ""type"": ""Expression""
                            },
                            ""type"": ""String""
                        },
                        ""FeedID"": {
                            ""value"": {
                                ""value"": ""@activity('br_bs_loggin').output.firstrow.FeedId_out"",
                                ""type"": ""Expression""
                            },
                            ""type"": ""Int32""
                        },
                        ""FeedRunId"": {
                            ""value"": {
                                ""value"": ""@activity('br_bs_loggin').output.firstrow.BatchRunId_out"",
                                ""type"": ""Expression""
                            },
                            ""type"": ""Int32""
                        },
                        ""Status"": {
                            ""value"": ""Failed"",
                            ""type"": ""String""
                        }
                    }
                },
                ""linkedServiceName"": {
                    ""referenceName"": ""AzureSqlDatabase1_cdp_dev_sql_db_appconfig"",
                    ""type"": ""LinkedServiceReference""
                }
            },
            {
                ""name"": ""If Condition1"",
                ""type"": ""IfCondition"",
                ""dependsOn"": [
                    {
                        ""activity"": ""Check if file exists in target"",
                        ""dependencyConditions"": [
                            ""Succeeded""
                        ]
                    }
                ],
                ""typeProperties"": {
                    ""expression"": {
                        ""value"": ""@equals(activity('Check if file exists in target').output.Exists,true)"",
                        ""type"": ""Expression""
                    },
                    ""ifFalseActivities"": [
                        {
                            ""name"": ""Stored Procedure_failed"",
                            ""type"": ""SqlServerStoredProcedure"",
                            ""policy"": {
                                ""timeout"": ""7.00:00:00"",
                                ""retry"": 0,
                                ""retryIntervalInSeconds"": 30,
                                ""secureOutput"": false,
                                ""secureInput"": false
                            },
                            ""typeProperties"": {
                                ""storedProcedureName"": ""[dbo].[usp_BatchRunUpdate]"",
                                ""storedProcedureParameters"": {
                                    ""BatchId"": {
                                        ""value"": {
                                            ""value"": ""@activity('br_bs_loggin').output.firstrow.Batchid_out"",
                                            ""type"": ""Expression""
                                        },
                                        ""type"": ""String""
                                    },
                                    ""FeedID"": {
                                        ""value"": {
                                            ""value"": ""@activity('br_bs_loggin').output.firstrow.FeedId_out"",
                                            ""type"": ""Expression""
                                        },
                                        ""type"": ""Int32""
                                    },
                                    ""FeedRunId"": {
                                        ""value"": {
                                            ""value"": ""@activity('br_bs_loggin').output.firstrow.BatchRunId_out"",
                                            ""type"": ""Expression""
                                        },
                                        ""type"": ""Int32""
                                    },
                                    ""Status"": {
                                        ""value"": ""Failed"",
                                        ""type"": ""String""
                                    }
                                }
                            },
                            ""linkedServiceName"": {
                                ""referenceName"": ""AzureSqlDatabase1_cdp_dev_sql_db_appconfig"",
                                ""type"": ""LinkedServiceReference""
                            }
                        }
                    ],
                    ""ifTrueActivities"": [
                        {
                            ""name"": ""Stored Procedure1"",
                            ""type"": ""SqlServerStoredProcedure"",
                            ""policy"": {
                                ""timeout"": ""7.00:00:00"",
                                ""retry"": 0,
                                ""retryIntervalInSeconds"": 30,
                                ""secureOutput"": false,
                                ""secureInput"": false
                            },
                            ""typeProperties"": {
                                ""storedProcedureName"": ""[dbo].[usp_BatchRunUpdate]"",
                                ""storedProcedureParameters"": {
                                    ""BatchId"": {
                                        ""value"": {
                                            ""value"": ""@activity('br_bs_loggin').output.firstrow.Batchid_out"",
                                            ""type"": ""Expression""
                                        },
                                        ""type"": ""String""
                                    },
                                    ""FeedID"": {
                                        ""value"": {
                                            ""value"": ""@activity('br_bs_loggin').output.firstrow.FeedId_out"",
                                            ""type"": ""Expression""
                                        },
                                        ""type"": ""Int32""
                                    },
                                    ""FeedRunId"": {
                                        ""value"": {
                                            ""value"": ""@activity('br_bs_loggin').output.firstrow.BatchRunId_out"",
                                            ""type"": ""Expression""
                                        },
                                        ""type"": ""Int32""
                                    },
                                    ""Status"": {
                                        ""value"": ""Succeeded"",
                                        ""type"": ""String""
                                    }
                                }
                            },
                            ""linkedServiceName"": {
                                ""referenceName"": ""AzureSqlDatabase1_cdp_dev_sql_db_appconfig"",
                                ""type"": ""LinkedServiceReference""
                            }
                        }
                    ]
                }
            }
        ],
        ""parameters"": {
            ""p_FeedName"": {
                ""type"": ""String"",
                ""defaultValue"": ""fa_cpsmyid_vdumcap1""
            },
            ""p_BatchType"": {
                ""type"": ""String"",
                ""defaultValue"": ""RAW""
            },
            ""p_RunType"": {
                ""type"": ""String"",
                ""defaultValue"": ""sch""
            },
            ""p_SrcStartDate"": {
                ""type"": ""String""
            },
            ""p_SrcEndDate"": {
                ""type"": ""String""
            },
            ""p_TargetDate"": {
                ""type"": ""String""
            },
            ""p_SrcHour"": {
                ""type"": ""String""
            },
            ""p_TgtHour"": {
                ""type"": ""String""
            }
        },
        ""variables"": {
            ""v_StartDate"": {
                ""type"": ""String""
            },
            ""v_EndDate"": {
                ""type"": ""String""
            }
        },
        ""folder"": {
            ""name"": ""Batch_load""
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>
","<azure><ftp><azure-data-factory>","2019-02-05 17:59:10","200","-1","1","54546182","<p>Based on your description,all concern is about improving transfer performance,i think.</p>

<p>Firstly,referring to the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance#data-integration-units"" rel=""nofollow noreferrer"">Data integration units statements</a>, <code>DIU</code> only applies to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#azure-integration-runtime"" rel=""nofollow noreferrer"">Azure Integration Runtime</a>, but not <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#self-hosted-integration-runtime"" rel=""nofollow noreferrer"">Self-hosted Integration Runtime</a>.Your source data is from FTP,so i think it is not affected by the <code>DIU</code> even though you already set the largest number.(Of course,it is referred by the official document,you still could get the verification from ADF team)</p>

<p>Then maybe you could get some clues to improve the copy performance from this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance"" rel=""nofollow noreferrer"">document</a>.</p>

<p>Such as:
1. Try to use the <code>parallelCopies</code> property to indicate the parallelism that you want Copy Activity to use. But it also has some restrictions from the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance#parallel-copy"" rel=""nofollow noreferrer"">statements</a>.</p>

<p>2.Try to set the sink dataset as <code>Azure SQL Data Warehouse</code>,because it seems that it has better performance than ADL.</p>

<p><a href=""https://i.stack.imgur.com/vDsJ5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vDsJ5.png"" alt=""enter image description here""></a></p>

<p>3.Try to compression the file from the source dataset to reduce the file size.</p>

<p>4.Consider to use Azure Cloud Service as source dataset such as Azure Blob Storage,as i know, the performance of copy activity between azure services is better commonly.</p>
"
"54533064","Converting d MMMM yyyy h:mm:ss + CET OR CEST in Azure Data Factory","<p>I am moving data from Blob to SQL, for the Blob I need to provide the correct schema including culture/format for datetime values (culture = 'nl-nl'):</p>

<ul>
<li>17 mei 2018 10:08:44 CEST  </li>
<li>10 december 2018 11:44:54 CET</li>
</ul>

<p>I have the first part of the format string: d MMMM yyyy h:mm:ss</p>

<p>I am stuck with the last part of the format string. </p>

<p>In ADF I am not able to provide an array of formats like in C#. I am not interested in the timezone, I only need dates. How can I provide a valid format string for ADF? Are there wildcard characters available to just ignore the timezone?</p>
","<c#><azure><azure-data-factory>","2019-02-05 11:07:15","485","1","1","54547076","<p>Searched the supported <a href=""http://%20https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">expression and functions</a> in ADF,no such features could convert <code>culture/format</code> to <code>datetime</code> format directly. </p>

<p>I provide two workarounds for you:</p>

<p>1.Create a Http Trigger Azure Function and set the convert method with c# code in it. Then use Azure Function Activity in ADF to execute the Azure Function, pass the blob storage data as input parameters.Then access the output data to the sql server sink.</p>

<p>2.Follow these steps to create a tempory table to receive data from blob storage and convert the format in a sql server stored procedure. You could refer to my previous case:<a href=""https://stackoverflow.com/questions/51352725/azure-data-factory-mapping-2-columns-in-one-column/51354266#51354266"">Azure Data Factory mapping 2 columns in one column</a></p>
"
"54524452","How do I programatically track error messages/","<p>I have a Lookup activity with a Failure output which executes a Stored Procedure activity. The Stored Procedure activity logs the failure. How do I programmatically get the name of the erroring Lookup activity and also the error message as in input parameters to the Stored Procedure activity? Thanks.</p>
","<azure-data-factory>","2019-02-04 21:19:04","50","0","1","54529097","<p>You could follow the example sdk code <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-programmatically#net"" rel=""nofollow noreferrer"">here</a> to get the error messages from the activity which runs in the pipeline.</p>

<p>1.Run the Lookup activity pipeline.</p>

<pre><code>CreateRunResponse runResponse = client.Pipelines.CreateRunWithHttpMessagesAsync(resourceGroup, dataFactoryName, pipelineName).Result.Body;
Console.WriteLine(""Pipeline run ID: "" + runResponse.RunId);
</code></pre>

<p>2.Get the error message if it crashed into some issues.</p>

<pre><code>List&lt;ActivityRun&gt; activityRuns = client.ActivityRuns.ListByPipelineRun(
resourceGroup, dataFactoryName, runResponse.RunId, DateTime.UtcNow.AddMinutes(-10), DateTime.UtcNow.AddMinutes(10)).ToList(); 
if (pipelineRun.Status == ""Succeeded"")
Console.WriteLine(activityRuns.First().Output);
else
Console.WriteLine(activityRuns.First().Error);
</code></pre>

<p>3.Then run another sp activity pipeline with the above messages as parameters.</p>

<pre><code>Dictionary&lt;string, object&gt; parameters = new Dictionary&lt;string, object&gt;
{
    { ""errorMessage"", activityRuns.First().Error}
};
CreateRunResponse runResponse = client.Pipelines.CreateRunWithHttpMessagesAsync(resourceGroup, dataFactoryName, pipelineName, parameters: parameters).Result.Body;
Console.WriteLine(""Pipeline run ID: "" + runResponse.RunId);
</code></pre>
"
"54510684","Why is password type of AzureKeyVaultSecret dropped when creating LinkedService via powershell,","<p>I'm attemping to create a LinkedService via the powershell command</p>

<pre><code>New-AzureRmDataFactoryV2LinkedService -ResourceGroupName rg -DataFactoryName df -Name n -DefinitionFile n.json 
</code></pre>

<p>the result is that the LinkedService is created, however the reference to the password type of AzureKeyVaultSecret is removed rendering it non-operational</p>

<p>The config file n.json was extracted from the DataFactory code tab and has the syntax below...</p>

<pre><code>{
""name"": ""&lt;name&gt;"",
""type"": ""Microsoft.DataFactory/factories/linkedservices"",
""properties"": {
    ""type"": ""Oracle"",
    ""typeProperties"": {
        ""connectionString"": ""host=&lt;host&gt;;port=&lt;port&gt;;serviceName=&lt;serviceName&gt;;user id=&lt;user_id&gt;"",
        ""password"": {
            ""type"": ""AzureKeyVaultSecret"",
            ""store"": {
                ""referenceName"": ""Prod_KeyVault"",
                ""type"": ""LinkedServiceReference""
            },
            ""secretName"": ""&lt;secretname&gt;""
        }
    },
    ""connectVia"": {
        ""referenceName"": ""&lt;runtimename&gt;"",
        ""type"": ""IntegrationRuntimeReference""
    }
}
</code></pre>

<p>}</p>

<p>When the new LinkedService is created, the code looks exactly the same except properties->typeProperties->password is removed and requires manual configuration - which I'm trying to avoid if possible.</p>

<p>Any thoughts?</p>
","<powershell><azure-devops><azure-data-factory>","2019-02-04 05:42:31","46","0","1","54511064","<p>If you have tried using ""Update-Module -Name AzureRm.DataFactoryV2"" to update your powershell to the latest version, and it is still the same behavior, then the possible root cause is that password is not support as Azure Key Value yet in Powershell. As far as I know, it is a new feature added recently. So it may take some time to rollout it to Powershell.</p>

<p>In that case, the workaround is to use UI to create linked service for now.</p>
"
"54509101","How to populate a star schema using Azure Datafactory v2","<p>I have a few source feeds from which I want to extract the data into a traditional star schema database (Azure SQL Database) for OLAP purposes, using Azure Data Factory v2.</p>

<p>Most of the source feeds corresponds to a dimension, so they map to the schema in my data warehouse database, apart from the internal surrogate key.</p>

<p>Then I have another feed which corresponds pretty much to the fact table. A lot of the identifiers there correspond to business keys of dimension tables.</p>

<p>The Azure Data Factory v2 documentation for the Copy Activity recommends that as much as possible, it uses the (default) <em><a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance#considerations-for-the-sink"" rel=""nofollow noreferrer"">Bulk Copy API to copy the data in append mode</a></em>.</p>

<p>So my questions are: </p>

<ol>
<li><p>What is the best approach to sync my dimension tables. I do not want to delete my existent data and repopulate it, because it will ruin my surragate keys, which will be referred to by my fact table. I was thinking to use an <em>upsert</em> stored procedure, which checks if the row already exists, and does an update instead of an insert. Is this something that will be very slow? Is there a better approach for this (maybe using the preCopy Script in some way?).</p></li>
<li><p>When populating the fact table, I want to lookup all the respective IDs (surragate keys) of the dimensions, and convert all the business keys to those IDs prior to inserting the data. What is the recommended approach to achieve this? Again a stored procedure would probably do, but I am afraid it is slow. Is there a better approach to this?</p></li>
</ol>
","<azure-sql-database><azure-data-factory>","2019-02-04 00:59:50","1361","3","1","54555734","<p>The best approach I (and some of my friends) know is to MERGE the data.</p>

<p>To achieve that - create a staging table (the same or different database on the same target server) which has the same structure as source table + PK only.
Hence, the process in ADF should be split into 2 steps:</p>

<ol>
<li>Truncate target (staging) table</li>
<li>Insert all data from the source into staging</li>
<li>Run stored procedure which contains MERGE statement and all required logic in there.</li>
</ol>

<p>Having that approach you will keep:</p>

<ul>
<li>best performance while putting the data into a staging table (bulk)</li>
<li>the fastest method to upsert (update/insert) your target table </li>
<li>capability to implement sophisticated business logic </li>
<li>apply SCD (Slowly Changing Dimension) when required </li>
<li>reduce the amount of disk operation</li>
</ul>

<p>The following posts could help you understand the process and build whole MERGE statement: [<a href=""https://sqlplayer.net/category/bi/data-warehousing/"" rel=""nofollow noreferrer"">https://sqlplayer.net/category/bi/data-warehousing/</a> ]</p>

<p>It's a pretty common scenario, so you gave me an idea for a new blog post. Thx.</p>

<p>I hope that helps you.</p>
"
"54488668","Is creating linked service in data factory has been changed? There are not two options of connection string and keyvault anymore","<p>I created a <strong>linked service in data factory using keyvault option</strong> about some months ago. I wanted to create a new linked service some days ago and I understood the UI for linked service creation has been changed! 
Previously based on this article <em><a href=""https://learn.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault#azure-key-vault-linked-service"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault#azure-key-vault-linked-service</a></em> there were two option <strong>1. connection string (needed DB name, server name and username and password for DB)2.KeyVault(Just needed secret name and keyvault connection)</strong>.
While now those two options has been changed to <strong>1.password 2. Keyvault</strong>. and the weird part is that in both two options DB name, username and password are mandatory! which is not acceptable because the point of using keyvault is not to share DB properties with developers and just sharing the secret name.
Does someone have any opinion about it??</p>
","<azure><azure-data-factory>","2019-02-02 00:01:54","475","-1","2","54499475","<p>Yes. It has been changed. Now you only need put your password into your azure Keyvault.<br>
You old linked service will still work. But the new UI will only support password only azure Keyvault.</p>
"
"54488668","Is creating linked service in data factory has been changed? There are not two options of connection string and keyvault anymore","<p>I created a <strong>linked service in data factory using keyvault option</strong> about some months ago. I wanted to create a new linked service some days ago and I understood the UI for linked service creation has been changed! 
Previously based on this article <em><a href=""https://learn.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault#azure-key-vault-linked-service"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault#azure-key-vault-linked-service</a></em> there were two option <strong>1. connection string (needed DB name, server name and username and password for DB)2.KeyVault(Just needed secret name and keyvault connection)</strong>.
While now those two options has been changed to <strong>1.password 2. Keyvault</strong>. and the weird part is that in both two options DB name, username and password are mandatory! which is not acceptable because the point of using keyvault is not to share DB properties with developers and just sharing the secret name.
Does someone have any opinion about it??</p>
","<azure><azure-data-factory>","2019-02-02 00:01:54","475","-1","2","54649062","<p>You can edit the json code of linked service to make it reference connection string.</p>

<p><a href=""https://i.stack.imgur.com/LDNuy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LDNuy.png"" alt=""enter image description here""></a></p>

<p>Here's the format, then click finish button, it will be published.</p>

<p><a href=""https://i.stack.imgur.com/wV0L4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wV0L4.png"" alt=""enter image description here""></a></p>
"
"54483295","Managing complex SQL queries in ARM template","<p><strong>Context</strong></p>

<p>I am using ARM templates to deploy my resources to Azure and as part of this, I have a DataFactory with several SQL queries which are quite long. </p>

<p><strong>Problem</strong></p>

<p>It's quite annoying to always have to scroll-across/copy/paste/format when I need to change the queries. It's hard to maintain. </p>

<p><strong>Question</strong></p>

<p>Does anyone know a more efficient way of managing these queries in ARM templates? Is there a way of loading these from sql files upon deployment? </p>
","<azure-devops><azure-resource-manager><azure-data-factory><azure-deployment>","2019-02-01 16:19:53","200","0","1","54483844","<p>I'd suggest storing them externally and passing in as a base64 encoded string (to prevent escape hell) with powershell\python\whathaveyou and decode directly in the template with <code>base64ToString()</code>.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/azure-resource-manager/resource-group-template-functions-string#base64tostring"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-resource-manager/resource-group-template-functions-string#base64tostring</a></p>
"
"54477345","Azure DataFactory v2 Linked Service with Integration Runtime","<p>Self Hosted IR Setup on Client Server, shows connected to my ADFv2.
Creating Linked Service times out. Only difference i can see between this setup and previous successful setup is that SQL Connection requires trust server certificate tick enabled. </p>

<p>I dont know how to add trust server certificate to my json sqlconnection file.</p>

<p>Removed Servername and password below for SqlServerLinkedService.json</p>

<pre><code>{
""properties"": {
    ""type"": ""SqlServer"",
    ""typeProperties"": {
        ""connectionString"": {
            ""type"": ""SecureString"",
            ""value"": ""Server=&lt;servername&gt;;Database=Master;User 
ID=admin;Password=&lt;password&gt;;Timeout=60""
        }
    },
    ""connectVia"": {
        ""type"": ""integrationRuntimeReference"",
        ""referenceName"": ""Test-IR""
    }
},
""name"": ""SqlServerLinkedService""
}


New-AzureRmDataFactoryV2LinkedServiceEncryptedCredential -DataFactoryName 
$dataFactoryName -ResourceGroupName $ResourceGroupName - 
IntegrationRuntimeName ""TEST-IR"" -File "".\SQLServerLinkedService.json
"" &gt; encryptedSQLServerLinkedService.json   
</code></pre>

<p>Gives me the error:</p>

<pre><code> New-AzureRmDataFactoryV2LinkedServiceEncryptedCredential : Unable to 
connect to the remote server caused by A connection attempt failed because 
the connected party did not properly respond after a period of time, or
established connection failed because connected host has failed to respond 
&lt;ip address removed&gt;:8050 At line:1 char:1
+ New-AzureRmDataFactoryV2LinkedServiceEncryptedCredential -DataFactory ...
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ CategoryInfo          : CloseError: (:) [New- 
AzureRmData...yptedCredential], GeneralException
+ FullyQualifiedErrorId : 
</code></pre>

<p>Microsoft.Azure.Commands.DataFactoryV2.NewAzureDataFactoryLinkedServiceEncryptedCredentialCommand </p>
","<azure><azure-data-factory>","2019-02-01 10:16:18","1131","0","1","54501395","<p>You could use TrustServerCertificate in your connection string. <a href=""https://learn.microsoft.com/en-us/dotnet/framework/data/adonet/connection-string-syntax#using-trustservercertificate"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/dotnet/framework/data/adonet/connection-string-syntax#using-trustservercertificate</a></p>
"
"54469214","How to use pre-copy script in Azure Data Factory to remove null/special character rows?","<p>I am moving data within folder from Azure Data Lake to a SQL Server using Azure Data Factory (ADF).  </p>

<p>The folder contains hundreds of .csv files. However, one inconsistent problem with these csv's is that some (not all) have a final row that contains a special character, which when trying to load to a sql table with datatypes other than NVARCHAR(MAX) will fail. To get around this, I have to first use ADF to load the data into staging tables where all columns are set to NVARCHAR(MAX), then I insert those rows that do not contain a special character into tables that have the appropriate data type. </p>

<p>This is a weekly process, and is over a terabyte of data and it takes forever to move the data so I am looking into ways to import into my final tables rather than having a staging component.  </p>

<p>I notice that there is a 'pre-copy script' field that can execute before the load to sql server. I want to add code that will allow me to parse out special characters OR null rows before loading to sql server. </p>

<p>I am unsure of how to approach this since the csv's would not be stored in a table, so SQL code wouldn't work. Any guidance on how I can utilize the pre-copy script to clean my data before loading it into sql server?</p>
","<sql-server><azure><azure-sql-database><azure-data-factory><azure-data-lake>","2019-01-31 21:03:53","6308","1","2","54493177","<p>The pre-copy script is a script that you run against the database before copying new data in, not to modify the data you are ingesting.</p>

<p>I already answered this on another question, providing a possible solution using an intermediate table: <a href=""https://stackoverflow.com/questions/50402950/pre-copy-script-in-data-factory-or-on-the-fly-data-processing/50411527"">Pre-copy script in data factory or on the fly data processing</a></p>

<p>Hope this helped!</p>
"
"54469214","How to use pre-copy script in Azure Data Factory to remove null/special character rows?","<p>I am moving data within folder from Azure Data Lake to a SQL Server using Azure Data Factory (ADF).  </p>

<p>The folder contains hundreds of .csv files. However, one inconsistent problem with these csv's is that some (not all) have a final row that contains a special character, which when trying to load to a sql table with datatypes other than NVARCHAR(MAX) will fail. To get around this, I have to first use ADF to load the data into staging tables where all columns are set to NVARCHAR(MAX), then I insert those rows that do not contain a special character into tables that have the appropriate data type. </p>

<p>This is a weekly process, and is over a terabyte of data and it takes forever to move the data so I am looking into ways to import into my final tables rather than having a staging component.  </p>

<p>I notice that there is a 'pre-copy script' field that can execute before the load to sql server. I want to add code that will allow me to parse out special characters OR null rows before loading to sql server. </p>

<p>I am unsure of how to approach this since the csv's would not be stored in a table, so SQL code wouldn't work. Any guidance on how I can utilize the pre-copy script to clean my data before loading it into sql server?</p>
","<sql-server><azure><azure-sql-database><azure-data-factory><azure-data-lake>","2019-01-31 21:03:53","6308","1","2","54499504","<p>You could consider stored procedure.  <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#invoking-stored-procedure-for-sql-sink"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#invoking-stored-procedure-for-sql-sink</a></p>
"
"54468413","Add Dynamic Content - Azure Data Factory ADF","<p>I need to add a Dynamic Content in an ADF. In such a way that it needs to get last month date as below formate. </p>

<p>Result: 2018-12.csv.zip </p>
","<azure><expression><azure-data-factory>","2019-01-31 20:02:22","13823","1","4","54469893","<p>This can be done by using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions</a>.  </p>

<p>Possible solution: 
I will suggest you to use 3 parameters here. 
 This solution use trigger().startTime, but you can use utcnow() aswell.</p>

<p>Note: adddays(trigger().startTime,-31,'MM') will return 12 as month, as that month was 31 days ago. If your trigger is on 5th of the month, or on the first day in the month than you can use -2.</p>

<pre><code>processYear = @formatDateTime(adddays(trigger().startTime,-31), 'yyyy')

processMonth = @formatDateTime(adddays(trigger().startTime,-31), 'MM')

result = @concat(pipeline().parameters.processYear,'-',pipeline().parameters.processMonth,'.csv.zip').
</code></pre>

<p>Hope that this helps. </p>
"
"54468413","Add Dynamic Content - Azure Data Factory ADF","<p>I need to add a Dynamic Content in an ADF. In such a way that it needs to get last month date as below formate. </p>

<p>Result: 2018-12.csv.zip </p>
","<azure><expression><azure-data-factory>","2019-01-31 20:02:22","13823","1","4","54538949","<p>Above given case works, if I know the date but I am running my Data Factory every day. So below works better. </p>

<p>@concat(substring(string(if(equals(int(formatDateTime(utcnow(), 'MM')),01), sub(int(formatDateTime(utcnow(), 'yyyyMM')),89),sub(int(formatDateTime(utcnow(),'yyyyMM')),1))),0,4),'-',substring(string(if(equals(int(formatDateTime(utcnow(), 'MM')),01), sub(int(formatDateTime(utcnow(), 'yyyyMM')),89),sub(int(formatDateTime(utcnow(),'yyyyMM')),1))),4,2),'.csv.zip')</p>

<p>Any changes appreciated. </p>
"
"54468413","Add Dynamic Content - Azure Data Factory ADF","<p>I need to add a Dynamic Content in an ADF. In such a way that it needs to get last month date as below formate. </p>

<p>Result: 2018-12.csv.zip </p>
","<azure><expression><azure-data-factory>","2019-01-31 20:02:22","13823","1","4","57718163","<p>If you set the date to the 28th and then subtract 30 days, that should get you into the previous month from any date.  Then simply format as <code>yyyy-MM</code>, eg</p>

<p><code>@concat(formatDateTime(adddays(formatDateTime(utcnow(), 'yyyy-MM-28'),-30),'yyyy-MM'),'.csv.zip')</code></p>
"
"54468413","Add Dynamic Content - Azure Data Factory ADF","<p>I need to add a Dynamic Content in an ADF. In such a way that it needs to get last month date as below formate. </p>

<p>Result: 2018-12.csv.zip </p>
","<azure><expression><azure-data-factory>","2019-01-31 20:02:22","13823","1","4","67323390","<p>@concat(formatDateTime(subtractFromTime(utcNow(),1,'Month'), 'yyyy-MM'), '.csv.zip')</p>
"
"54457963","Data Copy From REST API Working Very Slow-ADF","<p>I am trying to copy a data from a REST client to the azure data lake using ADF. I am using the Copy activity and HTTP linked service. It takes aroung 15 sec for a file to download. Which is very slow.</p>

<p>I had increased the DTU but had no impact on the data transfer.</p>
","<azure><azure-data-factory>","2019-01-31 10:01:55","369","0","1","54472166","<p>Each activity in data factory has an overhead of around 10 seconds. This is not &quot;very slow&quot; as it is meant to move any amount of data from source to sink. The main question is, those 15 seconds have an impact on your solution?</p>
<p>Also consider that its not a streaming or a real time tool, if you are looking for quick response times you might find stream analytics useful.</p>
<p>Hope this helped!</p>
"
"54445138","How to import an existing data factory ARM template?","<p>I went to an existing data factory, and chose to <code>export template</code>:</p>

<p><a href=""https://i.stack.imgur.com/Cjk1y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Cjk1y.png"" alt=""enter image description here""></a></p>

<p>I then manually created a new data factory, and now I am trying to import the existing data factory to this new one, by going to the new one, and selecting <code>import template</code>:</p>

<p><a href=""https://i.stack.imgur.com/zetkp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zetkp.png"" alt=""enter image description here""></a></p>

<p>However, this leads us to a generic page:</p>

<p><a href=""https://i.stack.imgur.com/kY5Vc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kY5Vc.png"" alt=""enter image description here""></a></p>

<p><strong>How do we publish or import a data factory using the same templates as were generated when we exported the data factory?</strong></p>
","<azure><azure-data-factory><azure-rm-template>","2019-01-30 16:26:57","12053","3","3","54455489","<p>Click on <code>Build your own template in the editor</code> and paste the entire content of the exported <code>armtemplate.json</code> file and that should work</p>
"
"54445138","How to import an existing data factory ARM template?","<p>I went to an existing data factory, and chose to <code>export template</code>:</p>

<p><a href=""https://i.stack.imgur.com/Cjk1y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Cjk1y.png"" alt=""enter image description here""></a></p>

<p>I then manually created a new data factory, and now I am trying to import the existing data factory to this new one, by going to the new one, and selecting <code>import template</code>:</p>

<p><a href=""https://i.stack.imgur.com/zetkp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zetkp.png"" alt=""enter image description here""></a></p>

<p>However, this leads us to a generic page:</p>

<p><a href=""https://i.stack.imgur.com/kY5Vc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kY5Vc.png"" alt=""enter image description here""></a></p>

<p><strong>How do we publish or import a data factory using the same templates as were generated when we exported the data factory?</strong></p>
","<azure><azure-data-factory><azure-rm-template>","2019-01-30 16:26:57","12053","3","3","54917540","<p>This doc shows how to do continuous integration and delivery in Azure Data Factory <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a></p>
"
"54445138","How to import an existing data factory ARM template?","<p>I went to an existing data factory, and chose to <code>export template</code>:</p>

<p><a href=""https://i.stack.imgur.com/Cjk1y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Cjk1y.png"" alt=""enter image description here""></a></p>

<p>I then manually created a new data factory, and now I am trying to import the existing data factory to this new one, by going to the new one, and selecting <code>import template</code>:</p>

<p><a href=""https://i.stack.imgur.com/zetkp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zetkp.png"" alt=""enter image description here""></a></p>

<p>However, this leads us to a generic page:</p>

<p><a href=""https://i.stack.imgur.com/kY5Vc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kY5Vc.png"" alt=""enter image description here""></a></p>

<p><strong>How do we publish or import a data factory using the same templates as were generated when we exported the data factory?</strong></p>
","<azure><azure-data-factory><azure-rm-template>","2019-01-30 16:26:57","12053","3","3","62139778","<p>Unzip the file you got when you hit ""Export ARM Template"" in ADF.</p>

<p>If you hit ""Build your own template in the editor"" you can then click ""Load file"" to upload the file named ""arm_template.json"" from the zip, then hit ""Save"" at the bottom. </p>

<p>Then click the ""Edit Parameters"" button and upload the ""arm_template_parameters.json"" file the same way as above, change the ""factoryName"" field' value to the desired factory and hit ""Save"" again.</p>

<p>Note: If you have any passwords or other hidden credentials you will have to re-enter them.</p>

<p>You should be all set to hit purchase and you will be able to create your ADF Factory from the ARM templates.</p>
"
"54442846","How do I use ADF copy activity with multiple rows in source?","<p>I have source which is JSON array, sink is SQL server. When I use column mapping and see the code I can see mapping is done to first element of array so each run produces single record despite the fact that source has multiple records. How do I use copy activity to import ALL the rows?</p>

<pre><code> ""enableStaging"": false,
                    ""translator"": {
                        ""type"": ""TabularTranslator"",
                        ""schemaMapping"": {
                            ""['@odata.context']"": ""BuyerFinancing"",
                            ""['@odata.nextLink']"": ""PropertyCondition"",
                            ""value[0].AssociationFee"": ""AssociationFee"",
                            ""value[0].AssociationFeeFrequency"": ""AssociationFeeFrequency"",
                            ""value[0].AssociationName"": ""AssociationName"",
</code></pre>
","<azure-data-factory>","2019-01-30 14:27:53","2156","0","2","54444115","<p>ADF support cross apply for json array. Please check the example in this doc. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#jsonformat-example"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#jsonformat-example</a></p>

<p>For schema mapping: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#schema-mapping"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#schema-mapping</a></p>
"
"54442846","How do I use ADF copy activity with multiple rows in source?","<p>I have source which is JSON array, sink is SQL server. When I use column mapping and see the code I can see mapping is done to first element of array so each run produces single record despite the fact that source has multiple records. How do I use copy activity to import ALL the rows?</p>

<pre><code> ""enableStaging"": false,
                    ""translator"": {
                        ""type"": ""TabularTranslator"",
                        ""schemaMapping"": {
                            ""['@odata.context']"": ""BuyerFinancing"",
                            ""['@odata.nextLink']"": ""PropertyCondition"",
                            ""value[0].AssociationFee"": ""AssociationFee"",
                            ""value[0].AssociationFeeFrequency"": ""AssociationFeeFrequency"",
                            ""value[0].AssociationName"": ""AssociationName"",
</code></pre>
","<azure-data-factory>","2019-01-30 14:27:53","2156","0","2","67347958","<p>Use * as the source field to indicate all elements in json format.  For example, with json:</p>
<pre><code>{
&quot;results&quot;: [
     {&quot;field1&quot;: &quot;valuea&quot;, &quot;field2&quot;: &quot;valueb&quot;},
     {&quot;field1&quot;: &quot;valuex&quot;, &quot;field2&quot;: &quot;valuey&quot;}
    ]
}
</code></pre>
<p>and a database table with a column result to store the json.  The mapping with results as the collection and * and the sub element will create two records with:</p>
<pre><code>{&quot;field1&quot;: &quot;valuea&quot;, &quot;field2&quot;: &quot;valueb&quot;}
{&quot;field1&quot;: &quot;valuex&quot;, &quot;field2&quot;: &quot;valuey&quot;}
</code></pre>
<p>in the result field.</p>
<p><a href=""https://i.stack.imgur.com/UbSo7.png"" rel=""nofollow noreferrer"">Copy Data Field Mapping</a></p>
"
"54441426","Error calling the azure function endpoint from azure data factory","<p>I have linked azure function in data factory pipeline which writes the text file to blob storage 
The azure function works fine when executed independently and writes the file to blob storage
But i am facing below mentioned error when i run the azure function from data factory</p>

<pre><code>{
    ""errorCode"": ""3600"",
    ""message"": ""Error calling the endpoint."",
    ""failureType"": ""UserError"",
    ""target"": ""Azure Function1""
}
</code></pre>

<p>I have configured the azure fucntion to access the blob with blobendpoint and  shared access signature as mentioned below</p>

<pre><code>""AzureWebJobsStorage"": ""DefaultEndpointsProtocol=https;AccountName=XYZ;AccountKey=XYZ;BlobEndpoint=ABC;SharedAccessSignature=AAA""
</code></pre>

<p>Please let me know if i need to make some additional properties changes in blob storage to access azure function successfully  from data factory</p>
","<azure><azure-functions><azure-data-factory>","2019-01-30 13:11:10","6129","1","2","54441980","<p>What is the trigger in your azure function? http trigger?
Also how is your azure function protected? 
if protected using AAD you need Bearer token.
if you are using keys you need x-function key.
<a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook#authorization-keys"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook#authorization-keys</a></p>

<p>Here is a video from channel9 that might help:</p>

<p>Run Azure Functions from Azure Data Factory pipelines</p>

<p><a href=""https://channel9.msdn.com/Shows/Azure-Friday/Run-Azure-Functions-from-Azure-Data-Factory-pipelines"" rel=""nofollow noreferrer"">https://channel9.msdn.com/Shows/Azure-Friday/Run-Azure-Functions-from-Azure-Data-Factory-pipelines</a></p>
"
"54441426","Error calling the azure function endpoint from azure data factory","<p>I have linked azure function in data factory pipeline which writes the text file to blob storage 
The azure function works fine when executed independently and writes the file to blob storage
But i am facing below mentioned error when i run the azure function from data factory</p>

<pre><code>{
    ""errorCode"": ""3600"",
    ""message"": ""Error calling the endpoint."",
    ""failureType"": ""UserError"",
    ""target"": ""Azure Function1""
}
</code></pre>

<p>I have configured the azure fucntion to access the blob with blobendpoint and  shared access signature as mentioned below</p>

<pre><code>""AzureWebJobsStorage"": ""DefaultEndpointsProtocol=https;AccountName=XYZ;AccountKey=XYZ;BlobEndpoint=ABC;SharedAccessSignature=AAA""
</code></pre>

<p>Please let me know if i need to make some additional properties changes in blob storage to access azure function successfully  from data factory</p>
","<azure><azure-functions><azure-data-factory>","2019-01-30 13:11:10","6129","1","2","54497119","<p>The Azure Function Activity in the ADF pipeline expects the Azure Function to return a JSON object instead of an HttpResponseMessage.
Here is how we solved it:
<a href=""https://microsoft-bitools.blogspot.com/2019/01/introducing-azure-function-activity-to.html"" rel=""nofollow noreferrer"">https://microsoft-bitools.blogspot.com/2019/01/introducing-azure-function-activity-to.html</a></p>
"
"54441225","Azure IR Copy from Onprem directly to Azure SQL","<p>I have setup an IR on a local sql server, And connected it to my Data factory v2, test run successfully copied a simple table dbo.employee into Blob storage.</p>

<p>Because i am copying small amounts of data i would like to copy data directly from on prem SQL, to SQL Azure database using a select.</p>

<p>I followed this tutorial - <strong><a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-hybrid-copy-powershell"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-hybrid-copy-powershell</a></strong></p>

<p>I then created a pipeline with a copy task, using my sqlserverdataset as a source - Simple Employee Table with 3 columns and 3 rows.</p>

<p>I created a similar table on my azure sql server testemployee with the same columns and set that for as the sink dataset. </p>

<p>Im not sure how to structure the stored proc its asking for in the sink.</p>

<p>for test purpose i created a sp on my azure sql as </p>

<blockquote>
  <p>CREATE PROCEDURE spCopyFromOnPremToAzure AS
      BEGIN
         INSERT [dbo].[TestEmployee]
         SELECT * FROM [dbo].[Employee] 
      END</p>
</blockquote>

<p>and used that as the proc in sink.
for table type i used TABLE</p>

<p>under mapping i can see the source and destination columns have mapped.</p>

<p>I triggered the pipeline which then failed,</p>

<p><strong>SOURCE</strong></p>

<pre><code>{
    ""source"": {
        ""type"": ""SqlSource""
    },
    ""sink"": {
        ""type"": ""SqlSink"",
        ""writeBatchSize"": 10000,
        ""sqlWriterStoredProcedureName"": ""[dbo].[spCopyFromOnPremToAzure]"",
        ""sqlWriterTableType"": ""TABLE""
    },
    ""enableStaging"": false,
    ""translator"": {
        ""type"": ""TabularTranslator"",
        ""columnMappings"": {
            ""ID"": ""ID"",
            ""FirstName"": ""FirstName"",
            ""LastName"": ""LastName""
        }
    }
}
</code></pre>

<p><strong>OUTPUT</strong></p>

<pre><code>{
    ""dataRead"": 62,
    ""dataWritten"": 0,
    ""rowsRead"": 3,
    ""rowsCopied"": 0,
    ""copyDuration"": 8,
    ""throughput"": 0.01,
    ""errors"": [
        {
            ""Code"": 11000,
            ""Message"": ""'Type=System.Data.SqlClient.SqlException,Message=The procedure \""spCopyFromOnPremToAzure\"" has no parameter named \""@[dbo].[TestEmployee]\"".,Source=.Net SqlClient Data Provider,SqlErrorNumber=349,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=349,State=1,Message=The procedure \""spCopyFromOnPremToAzure\"" has no parameter named \""@[dbo].[TestEmployee]\"".,},],'"",
            ""EventType"": 0,
            ""Category"": 5,
            ""Data"": {},
            ""MsgId"": null,
            ""ExceptionType"": null,
            ""Source"": null,
            ""StackTrace"": null,
            ""InnerEventInfos"": []
        }
    ],
    ""effectiveIntegrationRuntime"": ""TestIR"",
    ""usedParallelCopies"": 1,
    ""executionDetails"": [
        {
            ""source"": {
                ""type"": ""SqlServer""
            },
            ""sink"": {
                ""type"": ""AzureSqlDatabase""
            },
            ""status"": ""Failed"",
            ""start"": ""2019-01-30T13:45:19.4402274Z"",
            ""duration"": 8,
            ""usedParallelCopies"": 1,
            ""detailedDurations"": {
                ""queuingDuration"": 1,
                ""timeToFirstByte"": 0,
                ""transferDuration"": 7
            }
        }
    ]
}
</code></pre>

<p><strong>ERROR</strong></p>

<pre><code>{
    ""errorCode"": ""2200"",
    ""message"": ""'Type=System.Data.SqlClient.SqlException,Message=The procedure \""spCopyFromOnPremToAzure\"" has no parameter named \""@[dbo].[TestEmployee]\"".,Source=.Net SqlClient Data Provider,SqlErrorNumber=349,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=349,State=1,Message=The procedure \""spCopyFromOnPremToAzure\"" has no parameter named \""@[dbo].[TestEmployee]\"".,},],'"",
    ""failureType"": ""UserError"",
    ""target"": ""Copy Data1""
}
</code></pre>

<p>Im assuming this has to do with the stored proc, How would i structure it to do a basic select from Source and copy to sink ?</p>
","<azure><azure-data-factory>","2019-01-30 13:00:41","232","0","2","54444246","<p>In your case, seems you don’t need stored procedure. Just set table name in dataset is enough.</p>

<p>Please go through this doc. It also has an example for stored procedure.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#invoking-stored-procedure-for-sql-sink"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#invoking-stored-procedure-for-sql-sink</a></p>
"
"54441225","Azure IR Copy from Onprem directly to Azure SQL","<p>I have setup an IR on a local sql server, And connected it to my Data factory v2, test run successfully copied a simple table dbo.employee into Blob storage.</p>

<p>Because i am copying small amounts of data i would like to copy data directly from on prem SQL, to SQL Azure database using a select.</p>

<p>I followed this tutorial - <strong><a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-hybrid-copy-powershell"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-hybrid-copy-powershell</a></strong></p>

<p>I then created a pipeline with a copy task, using my sqlserverdataset as a source - Simple Employee Table with 3 columns and 3 rows.</p>

<p>I created a similar table on my azure sql server testemployee with the same columns and set that for as the sink dataset. </p>

<p>Im not sure how to structure the stored proc its asking for in the sink.</p>

<p>for test purpose i created a sp on my azure sql as </p>

<blockquote>
  <p>CREATE PROCEDURE spCopyFromOnPremToAzure AS
      BEGIN
         INSERT [dbo].[TestEmployee]
         SELECT * FROM [dbo].[Employee] 
      END</p>
</blockquote>

<p>and used that as the proc in sink.
for table type i used TABLE</p>

<p>under mapping i can see the source and destination columns have mapped.</p>

<p>I triggered the pipeline which then failed,</p>

<p><strong>SOURCE</strong></p>

<pre><code>{
    ""source"": {
        ""type"": ""SqlSource""
    },
    ""sink"": {
        ""type"": ""SqlSink"",
        ""writeBatchSize"": 10000,
        ""sqlWriterStoredProcedureName"": ""[dbo].[spCopyFromOnPremToAzure]"",
        ""sqlWriterTableType"": ""TABLE""
    },
    ""enableStaging"": false,
    ""translator"": {
        ""type"": ""TabularTranslator"",
        ""columnMappings"": {
            ""ID"": ""ID"",
            ""FirstName"": ""FirstName"",
            ""LastName"": ""LastName""
        }
    }
}
</code></pre>

<p><strong>OUTPUT</strong></p>

<pre><code>{
    ""dataRead"": 62,
    ""dataWritten"": 0,
    ""rowsRead"": 3,
    ""rowsCopied"": 0,
    ""copyDuration"": 8,
    ""throughput"": 0.01,
    ""errors"": [
        {
            ""Code"": 11000,
            ""Message"": ""'Type=System.Data.SqlClient.SqlException,Message=The procedure \""spCopyFromOnPremToAzure\"" has no parameter named \""@[dbo].[TestEmployee]\"".,Source=.Net SqlClient Data Provider,SqlErrorNumber=349,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=349,State=1,Message=The procedure \""spCopyFromOnPremToAzure\"" has no parameter named \""@[dbo].[TestEmployee]\"".,},],'"",
            ""EventType"": 0,
            ""Category"": 5,
            ""Data"": {},
            ""MsgId"": null,
            ""ExceptionType"": null,
            ""Source"": null,
            ""StackTrace"": null,
            ""InnerEventInfos"": []
        }
    ],
    ""effectiveIntegrationRuntime"": ""TestIR"",
    ""usedParallelCopies"": 1,
    ""executionDetails"": [
        {
            ""source"": {
                ""type"": ""SqlServer""
            },
            ""sink"": {
                ""type"": ""AzureSqlDatabase""
            },
            ""status"": ""Failed"",
            ""start"": ""2019-01-30T13:45:19.4402274Z"",
            ""duration"": 8,
            ""usedParallelCopies"": 1,
            ""detailedDurations"": {
                ""queuingDuration"": 1,
                ""timeToFirstByte"": 0,
                ""transferDuration"": 7
            }
        }
    ]
}
</code></pre>

<p><strong>ERROR</strong></p>

<pre><code>{
    ""errorCode"": ""2200"",
    ""message"": ""'Type=System.Data.SqlClient.SqlException,Message=The procedure \""spCopyFromOnPremToAzure\"" has no parameter named \""@[dbo].[TestEmployee]\"".,Source=.Net SqlClient Data Provider,SqlErrorNumber=349,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=349,State=1,Message=The procedure \""spCopyFromOnPremToAzure\"" has no parameter named \""@[dbo].[TestEmployee]\"".,},],'"",
    ""failureType"": ""UserError"",
    ""target"": ""Copy Data1""
}
</code></pre>

<p>Im assuming this has to do with the stored proc, How would i structure it to do a basic select from Source and copy to sink ?</p>
","<azure><azure-data-factory>","2019-01-30 13:00:41","232","0","2","54458535","<p>I managed to get this working - Took some trial and error but now i know. 
When using the Wizard copy data pipeline, i simply had to select my source and destination datasets, each with their own LinkedService - and that worked 100%.</p>

<p>I can also specify a query that way. I just needed to understand that each dataset requires a linked service. Thanks so much</p>
"
"54412212","How do I convert string to datetime in ADF?","<p>I'm transforming Rest data results and sink into Azure SQL and it fails with error below when translating string into DateTime2. </p>

<blockquote>
  <p>""ErrorCode=UserErrorInvalidDataValue,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Column
  'ABOR_CDOM_Seed_Datetime' contains an invalid value
  '2017-01-02T16:33:43.223Z'. Cannot convert '2017-01-02T16:33:43.223Z'
  to type 'DateTime' with format 'yyyy-MM-dd
  HH:mm:ss.fffffff'.,Source=Microsoft.DataTransfer.Common,''Type=System.FormatException,Message=String
  was not recognized as a valid DateTime.,Source=mscorlib,'""</p>
</blockquote>
","<azure-data-factory>","2019-01-29 00:26:36","11462","4","1","54412778","<p>One has to go to <code>Schema</code> of sink and add <code>Format</code> as below to accommodate for this custom format</p>

<p><a href=""https://i.stack.imgur.com/8b7hj.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/8b7hj.png"" alt=""enter image description here""></a></p>
"
"54401745","Azure Data Factory passing a parameter into a function (string replace)","<p>I'm trying to use ADF to create azure table storage tables from one source SQL table.  </p>

<ul>
<li><p>Within my pipeline..</p>

<ul>
<li>I can query a distinct list of customers pass this into a for-each
task.</li>
<li>Inside the for each select the data for each customer</li>
</ul></li>
</ul>

<p>But when I try to to create an Azure table for each customers data with the table name based on the customer ID I hit errors.
The customer ID is a GUID so I'm trying to format this to remove the dashes which are invalid in a table name...</p>

<p>Something along the lines of 
@replace('@{item().orgid}','-','')</p>

<p>So  7fb6d90f-2cc0-40f5-b4d0-00c82b9935c4 becomes 7fb6d90f2cc040f5b4d000c82b9935c4</p>

<p>I can't seem to get the syntax right
Any ideas?</p>
","<azure><azure-data-lake><azure-data-factory>","2019-01-28 12:10:37","2166","0","1","54410192","<p>try this: <code>@replace(item().orgid,'-','').</code></p>
"
"54400579","Get exceptions from Databricks Notebook in Azure Datafactory pipeline","<p>I've added a Databricks Notebook to a Datafactory pipeline. If the Python script inside the notebook throws an exception, this exception will not be mentioned by the pipeline. I know there is a runPageUrl where I can see the results. But I want the pipeline to know if an error occurred in the Python script. Is there a way to pass exception information to the pipeline?</p>
","<python><azure><exception><azure-data-factory><azure-databricks>","2019-01-28 11:04:13","2429","3","1","54420548","<p>Found it. I just had to add assertion to the exceptions. Next step is to find out how to get the error message in the outcome of the activity in the pipeline...</p>
"
"54383191","How to include blob metadata in copy data mapping","<p>I'm working on a ADF v2 pipeline, which copies data from csv blob to Azure SQL database table. For each load I would like to collect source metadata, like source blob name, and save it to a target table as a part of data lineage framework.</p>

<p>My blob source run the following schema:</p>

<ol>
<li>StoreName,</li>
<li>StoreLocation,</li>
<li>StoreTaxId.</li>
</ol>

<p>My destination table run the following schema:</p>

<ol>
<li>StoreName,</li>
<li>StoreLocation,</li>
<li>DwhProcessDate,</li>
<li>DwhSourceName.</li>
</ol>

<p>I do not know, how to properly include name of the source in the mapping section of Copy Data activity.</p>

<p>For the moment I have:</p>

<ol>
<li>defined a [Get Metadata1] activity to get references to all blobs that are available from Azure Blob Storage</li>
<li>defined a [ForEach1] activity, iterating through the output of an expression @activity('Get Metadata1').output.childitems</li>
<li>inside the [ForEach1] activity, I have placed [Copy Data1] activity, where I have source and sink sections defined. </li>
</ol>

<p>What I'm looking for is a way to add extra line to the mapping section, which will samehow bind @item().name to destination column [DwhSourceName]</p>

<p>Thanks for all suggestion on how to achieve this.</p>
","<azure><azure-data-factory>","2019-01-26 21:58:27","991","2","1","54418121","<p>Actually,based on my test,you can specify the dymatic content of column key,but you can't set blob metadata as value of columns in copy data mapping at the pipeline run time. Please see the rules mentioned in this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#explicit-column-mapping"" rel=""nofollow noreferrer"">document</a>.</p>

<p><a href=""https://i.stack.imgur.com/p7x96.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p7x96.png"" alt=""enter image description here""></a></p>

<p>You still need to add the <code>FileName</code> column in your source data before the copy activity.Maybe you could use <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-triggered-function"" rel=""nofollow noreferrer"">Azure Blob Trigger Function</a> to get the blob file name so hat you could add the <code>FileName</code> column when any data stream into the blob.(Please refer to this case:<a href=""https://stackoverflow.com/questions/45196303/how-do-i-get-the-name-of-the-inputblob-that-triggered-my-azure-function-with-pyt"">How Do I get the Name of The inputBlob That Triggered My Azure Function With Python</a>)</p>
"
"54381014","Azure ADF @replace expression how to escape the \ character","<p>Azure ADF expressions can't parse this for some reason.</p>

<p>This is the expression used to set variable activity of a string variable named ""out"".
<code>@replace('whats\up','\','/')</code></p>

<p>The error is:
<code>Invalid Position 25 Syntax error: Missing comma between arguments</code> which actually points to the <code>/</code> character</p>

<p>@'\' &amp;/ @'/' doesn't make a difference.
Neither does any combination of '\\', '/', ....</p>

<p>Actually my value <code>what\up</code> is assigned a variable named 'in'.
So the real expression is: <code>@replace(variables('in'),'\','/')</code>
It yields the same error but a <code>Position 30</code></p>

<p>There isn't anything I can find that comes close to explaining the expression engine/compilation/execution in ADF.</p>

<p>Sorry for the crappy screen camera shots.  But the bubbles don't screen capture
<a href=""https://i.stack.imgur.com/uf2XQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uf2XQ.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/p42xY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p42xY.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2019-01-26 17:47:09","16575","1","1","54432881","<p>enter image description here@replace('whats\up, '\', '/') You missed a single quote ' after up. </p>

<pre><code>{
""name"": ""pipeline14"",[![enter image description here][1]][1]
""properties"": {
    ""activities"": [
        {
            ""name"": ""Set Variable1"",
            ""type"": ""SetVariable"",
            ""typeProperties"": {
                ""variableName"": ""a"",
                ""value"": ""@replace('whats\\up','\\','/')""
            }
        }
    ],
    ""variables"": {
        ""a"": {
            ""type"": ""String""
        }
    }
}
</code></pre>

<p>}
<a href=""https://i.stack.imgur.com/IrelA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IrelA.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/ojfWt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ojfWt.png"" alt=""enter image description here""></a></p>
"
"54367610","Does Azure Batch support .Net Core in Azure Data Factory v2?","<p>I've been trying to get even a simple ""Hello World"" .Net Core 2.x Console App running via ADF v2 Custom Activity/Batch Service but with no luck. It does seem to work when I provision a WindowsServer and run a .Net Framework 4.5.x Console App though. Can't find any examples where this would work but also can't imagine why it wouldn't. </p>

<p>I've tried calling:</p>

<ol>
<li>ConsoleApp.dll</li>
<li>dotnet run ConsoleApp</li>
<li>and dotnet publish -r win-x64, then copying those files over and calling ConsoleApp.exe</li>
</ol>

<p>Any suggestions on what else I could try?</p>
","<azure-data-factory><azure-batch>","2019-01-25 14:48:15","296","2","1","56026951","<p>In our case we had our dotnet core application that was parsing the files, so we ended up scaling up Azure Function to P1V2, and that did the job for us both CPU and RAM wise. So, if it is your code that you need to run, try Functions. In addition, Functions are serverless which eliminates a lot of pluming. That is unless you have a specific need to run it as Batch.</p>
"
"54362686","Dynamically Assign Source column in import schema ""expression"" in Azure Data Factory using Lookup activity","<p>i am new in azure data factory V2</p>

<p>i have schema file- </p>

<blockquote>
  <p>CustomerSchema.csv</p>
</blockquote>

<p>Data-in CSV File</p>

<pre><code>CustomerId, CustomerName, CustomerAddress
</code></pre>

<p>Actual Data File in CSV Formatoe
FileName-CustomerMaster.CSV, data below</p>

<pre><code>101,Aphilps,R-z1 west loan usa
102,Jons s,202-3 sest road  london
103,Ravi,Old Madaras banglore
104,n Har,hi road -203 banglore
105,Jons K,West-23-34 new york usa
</code></pre>

<p>Target File with Schema
CustomerTraget.CSV</p>

<p>CustomerId, CustomerName, CustomerAddress
----Blank no Data</p>

<p>i have pipline where first activity is lookup activity that bring schema from file CustomerSchema.CSV</p>

<p>output of lookup activity is correct, it's fetch 3 Columns name</p>

<p><a href=""https://i.stack.imgur.com/NrHrd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NrHrd.png"" alt=""enter image description here""></a> </p>

<p>Now how i passed output of lookup activity in my copy activity import schema expression</p>

<p>i have try below but not work</p>

<p>@activity('ColumnLookup').output.firstRow in schema expression but it's not working</p>

<p>please help me</p>
","<azure-data-factory>","2019-01-25 09:48:54","1991","1","1","54412855","<p>Parameterized schema or mapping should follow this pattern:</p>

<p>column mapping:</p>

<pre><code>{
    ""type"": ""TabularTranslator"",
    ""columnMappings"": 
    {
        ""UserId"": ""MyUserId"",
        ""Group"": ""MyGroup"",
        ""Name"": ""MyName""
     }
}
</code></pre>

<p>schema:</p>

<pre><code>[
    { ""name"": ""UserId""},
    { ""name"": ""Name""},
    { ""name"": ""Group""}
]
</code></pre>

<p>ref : <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping</a></p>
"
"54311278","How to get alert for multiple pipelines success instead of 1 activity inside a pipeline in Azure Data Factory","<p>I am creating a new email notification alert for my azure data factory. So here i have 4 different pipelines with multiple data_copy activities each. As of now using Logic App i am able to send mail for each copy_activity of a pipeline. But my requirement is to get a single mail after successful completion of each pipeline.</p>

<p>Below is a basic understanding of current setup.</p>

<pre><code>Pipeline A
    * Copy_Data Activity A1
    * Copy_Data Activity A2
    * Copy_Data Activity A3
Pipeline B
    * Copy_Data Activity B1
    * Copy_Data Activity B2
    * Copy_Data Activity B3
Pipeline C
    * Copy_Data Activity C1
    * Copy_Data Activity C2
    * Copy_Data Activity C3
</code></pre>

<p>Now my present work is that using logic App i can only send a mail for any Copy_Data activity e.g. Copy_Data Activity A1,A2,A3,C1, etc.</p>

<p>But my <strong>expected result</strong> in a mail using this Logic app is like below</p>

<pre><code>Pipeline_Name       Status     
Pipeline A          Success
Pipeline B          Success
Pipeline C          Success
</code></pre>

<p>Basically i want the overall status of each pipeline as SUCCESS in a single mail if each and every activity of that pipeline is successful. </p>

<p>Any suggestions how to use logic app in fetching records on all pipeline levels instead of activity level. </p>

<p>I have successfully triggered the mail on copy_data activity level</p>

<blockquote>
  <p><em>Can't share any code here. Just to secure privacy</em></p>
</blockquote>

<p>I expect the results as SUCCESS/FAILED on pipeline level only.</p>
","<azure><azure-devops><azure-data-factory><sql-azure-alerts>","2019-01-22 15:16:05","216","1","1","55111600","<p>When you set up the signal logic in the Data Factory's Monitoring Alerts, there are options for both pipeline level and activity level metrics.  Try changing from activity to pipeline.
<a href=""https://i.stack.imgur.com/8B0RB.png"" rel=""nofollow noreferrer"">picture of Configure signal logic blade</a></p>
"
"54311024","parsing html table content using azure data factory","<p>I am trying to parse the html table content from the URL mentioned below </p>

<p><a href=""https://www.w3schools.com/html/html_tables.asp"" rel=""nofollow noreferrer"">https://www.w3schools.com/html/html_tables.asp</a></p>

<p>Just wanted to check if there are any activities available in azure data factory to download the html content , parse the html table content and store the parsed data in azure blob </p>
","<azure><azure-storage><azure-functions><azure-data-factory>","2019-01-22 15:01:14","1345","0","1","54320135","<p>You can use Web table connector available in the Azure Data Factory as per your requirement. Web table connector extracts table content from an HTML webpage. For more details, refer <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-web-table"" rel=""nofollow noreferrer"">Web table connector</a>.</p>

<p>You can copy data from Web table database to any supported sink data store. For a list of data stores that are supported as sources/sinks by the copy activity, see the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats"" rel=""nofollow noreferrer"">Supported data stores table</a>.</p>
"
"54303899","Does azure data factory support real time copy activity?","<p>I want to copy data from an on-premise oracle db to sql server on a real time basis.</p>
","<azure><android-activity><copy><real-time><azure-data-factory>","2019-01-22 08:15:47","2618","0","3","54319922","<p>Welcome to Stack Overflow!</p>

<p>You can copy data from an Oracle database to any supported sink data store. For a list of data stores that are supported as sources or sinks by the copy activity, see the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats"" rel=""nofollow noreferrer"">Supported data stores</a> table.</p>

<p>To copy data from and to an Oracle database that isn't publicly accessible, you need to set up a Self-hosted Integration Runtime. The integration runtime provides a built-in Oracle driver. Therefore, you don't need to manually install a driver when you copy data from and to Oracle.</p>

<p>For more details and step by step procedure, refer <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-oracle"" rel=""nofollow noreferrer"">Copy data from and to Oracle by using Azure Data Factory</a>.</p>

<p>You can also use custom activity as per your needs. Refer <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">custom activities</a>.</p>

<p>Hope this helps.</p>
"
"54303899","Does azure data factory support real time copy activity?","<p>I want to copy data from an on-premise oracle db to sql server on a real time basis.</p>
","<azure><android-activity><copy><real-time><azure-data-factory>","2019-01-22 08:15:47","2618","0","3","54320849","<p>You could copy data incrementally. But there is frequency limitation. Please reference this post. <a href=""https://social.msdn.microsoft.com/Forums/en-US/54380f98-716b-4a95-88af-cad2ab7e47b5/what-type-of-data-ingestion-does-azure-data-factory-use?forum=AzureDataFactory"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/en-US/54380f98-716b-4a95-88af-cad2ab7e47b5/what-type-of-data-ingestion-does-azure-data-factory-use?forum=AzureDataFactory</a></p>
"
"54303899","Does azure data factory support real time copy activity?","<p>I want to copy data from an on-premise oracle db to sql server on a real time basis.</p>
","<azure><android-activity><copy><real-time><azure-data-factory>","2019-01-22 08:15:47","2618","0","3","59660555","<p>Data Factory has many strengths, but frequency is not one of them. Have you considered a different approach?</p>

<p>For real time integrations I would recommend Functional App, Logic App and/or Service Bus. I would call this App whenever there is a relevant change in the oracle DB. Alternatively you could have an API on top of this on-premise oracle DB that you call from a scheduled APP.</p>

<p>If you expect heavy traffic you might want to consider using a Service Bus. The illustration below shows how Azure Service Bus sends data from a Publisher(on-premise) to a Subscriber(Azure sql DB) using Topic Message.</p>

<p><a href=""https://i.stack.imgur.com/JedWv.png"" rel=""nofollow noreferrer"">Illustration of large scale service bus dataflow</a></p>

<p>Ref. <a href=""https://learn.microsoft.com/en-us/azure/service-bus-messaging/"" rel=""nofollow noreferrer"">Azure Service Bus</a></p>
"
"54288928","Staging tables in DB vs storage area","<p>Typically on an on-premise SQL server ETL workflow via SSIS, we load data from anywhere into staging tables and then apply validation and transformations to load/merge them into downstream data warehouse tables.</p>

<p>My question is if we should do something similar on Azure, where we have set of staging tables and downstream tables in azure SQL database or use azure storage area as staging and move data from there into final downstream tables via ADF.</p>

<p>As wild is it may seem, we also have a proposal to have separate staging database and downstream database, between which we move using ADF.</p>
","<sql><sql-server><azure-sql-database><azure-storage><azure-data-factory>","2019-01-21 11:23:05","1816","1","1","54293841","<p>There are different models for doing data movement pipelines and no single one is perfect.  I'll make a few comments on the common patterns I see in case that will help you make decisions on your application.</p>

<p>For many data warehouses where you are trying to stage in data and create dimensions, there is often a process where you load the raw source data into some other database/tables as raw data and then process it into the format you want to insert into your fact and dimension tables.  That process is complicated by the fact that you may have data arrive late or data that is corrected on a later day, so often these systems are designed using partitioned tables on the target fact tables to allow re-processing of a partition worth of data (e.g. a day) without having to reprocess the whole fact table.  Furthermore, the transformation process on that staging table may be intensive if the data itself is coming in a form far away from how you want to represent it in your DW.  Often in on-premises systems, these are handled in a separate database (potentially on the same SQL Server) to isolate it from the production system.  Furthermore, it is sometimes the case that these staging tables are re-creatable from original source data (CSV files or similar), so it is not the store of record for that source material.  This allows you to consider using simple recovery mode on that database (which reduces the Log IO requirements and recovery time compared to full recovery).  While not every DW uses full recovery mode for the processed DW data (some do dual load to a second machine instead since the pipeline is there), the ability to use full recovery plus physical log replication (AlwaysOn Availability Groups) in SQL Server gives you the flexibility to create a disaster recovery copy of the database in a different region of the world.  (You can also do query read scale-out on that server if you would like).  There are variations on this basic model, but a lot of on-premises systems have something like this.</p>

<p>When you look at SQL Azure, there are some similarities and some differences that matter when considering how to set up an equivalent model:</p>

<ol>
<li>You have full recovery on all user databases (but tempdb is in simple recovery).  You also have quorum-commit of your changes to N replicas (like in Availability Groups) when using v-core or premium dbs which matters a fair amount because you often have a more generic network topology in public cloud systems vs. a custom system you build yourself.  In other words, log commit times may be slower than your current system.  For batch systems it does not necessarily matter too much, but you need to be careful to use large enough batch sizes so that you are not waiting on the network all the time in your application.  Given that your staging table may also be a SQL Azure database, you need to be aware that it also has quorum commit so you may want to consider which data is going to stay around day-over-day (stays in SQL Azure DB) vs. which can go into tempdb for lower latencies and be re-created if lost.</li>
<li>There is no intra-db resource governance model today in SQL Azure (other than elastic pools which is partial and is targeting a different use case than DW).  So, having a separate staging database is a good idea since it isolates your production workload from the processing in the staging database.  You avoid noisy neighbor issues with your primary production workload being impacted by the processing of the day's data you want to load.</li>
<li>When you provision machines for on-premises DW, you often buy a sufficiently large storage array/SAN that you can host your workload and potentially many others (consolidation scenarios).  The premium/v-core DBs in SQL Azure are set up with local SSDs (with Hyperscale being the new addition where it gives you some cross-machine scale-out model that is a bit like a SAN in some regards).  So, you would want to think through the IOPS required for your production system and your staging/loading process.  You have the ability to choose to scale up/down each of these to better manage your workload and costs (unlike a CAPEX purchase of a large storage array which is made up front and then you tune workloads to fit into it).</li>
<li>Finally, there is also a SQL DW offering that works a bit differently than SQL Azure - it is optimized for larger DW workloads and has scale-out compute with the ability to scale that up/down as well.  Depending on your workload needs, you may want to consider that as your eventual DW target if that is a better fit.</li>
</ol>

<p>To get to your original question - can you run a data load pipeline on SQL Azure?  Yes you can.  There are a few caveats compared to your existing experiences on-premises, but it will work.  To be fair, there are also people who just load from CSV files or similar directly without using a staging table.  Often they don't do as many transformations, so YMMV based on your needs.</p>

<p>Hope that helps.</p>
"
"54287652","can we pass pipeline variable inside the for each loop or other iterative activities","<p>I am using azure data factory to download a file using the copy activity. The copy activity is called inside the <code>for each</code> activity. I need to pass a variable that can be used in the copy activity . </p>

<p>But I get an error saying <code>""The output of variable &lt;variable name&gt; can't be referenced since it is not a variable of the current pipeline""</code>.</p>
","<azure><azure-data-factory>","2019-01-21 10:11:43","1751","1","2","54303426","<p>You could try to use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-set-variable-activity"" rel=""nofollow noreferrer"">Set Variable Activity</a> with Copy Activity in the ForEach Activity.</p>

<p>Set Variable Activity:</p>

<blockquote>
  <p>Use the Set Variable activity to set the value of an existing variable
  of type String, Bool, or Array defined in a Data Factory pipeline.</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/ZHOpr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZHOpr.png"" alt=""enter image description here""></a></p>

<p>Then you could use it with dynamic content,such as <code>@activity('Set Variable1').value</code></p>
"
"54287652","can we pass pipeline variable inside the for each loop or other iterative activities","<p>I am using azure data factory to download a file using the copy activity. The copy activity is called inside the <code>for each</code> activity. I need to pass a variable that can be used in the copy activity . </p>

<p>But I get an error saying <code>""The output of variable &lt;variable name&gt; can't be referenced since it is not a variable of the current pipeline""</code>.</p>
","<azure><azure-data-factory>","2019-01-21 10:11:43","1751","1","2","54836109","<p>Before you can use a variable, you must first declare it in the scopt of the pipeline.</p>

<ol>
<li>Go to pipeline view</li>
<li>Click in empty whitespace, so you are not focused on any activity</li>
<li>Select the Variable tab</li>
<li>Make you variable</li>
</ol>

<p>Please be aware that when setting the value of a variable, it cannot reference itself (X=X+1).  I.E. You cannot use a variable as a counter during a for-each loop.
<a href=""https://i.stack.imgur.com/Ri3RM.png"" rel=""nofollow noreferrer"">Pipeline variable declaration</a></p>
"
"54271111","UTC and daylight savings - job schedule","<p>For daylight savings time will an ADFV2 schedule update itself or do we need to go in there and update it when the time comes? I'm guessing the latter bc UTC doesn't change but I don't want to guess.</p>
","<azure-data-factory>","2019-01-19 20:29:33","203","0","1","54321255","<p>ADF won’t handle daylight savings time automatically.</p>

<p>Here's a post using azure function and web activity to make it work. Maybe you could take a look.</p>

<p><a href=""http://www.aussierobsql.com/azure-data-factory-v2-handling-daylight-savings-p1/"" rel=""nofollow noreferrer"">http://www.aussierobsql.com/azure-data-factory-v2-handling-daylight-savings-p1/</a>
<a href=""http://www.aussierobsql.com/azure-data-factory-v2-handling-daylight-savings-p2/"" rel=""nofollow noreferrer"">http://www.aussierobsql.com/azure-data-factory-v2-handling-daylight-savings-p2/</a></p>
"
"54256158","The Format of Body for a POST request in Web Activity in Azure Data Factory","<p>I have created a web activity in azure data factory pipeline which have only one header and I have to pass body for a POST request. I have tried passing body as JSON and as String also but the request failed with ""Invalid Query"". Can anyone please tell me how can I send a POST request from azure data pipeline with additional header and body. I have multiple key value pairs to pass to body.<a href=""https://i.stack.imgur.com/DRKaG.png"" rel=""nofollow noreferrer"">Check the Screenshot of web activity</a></p>
","<azure><azure-data-factory>","2019-01-18 14:39:43","6319","1","1","54286753","<p>Please try this. First, add one more header with Name of <code>Content-Type</code> and value of <code>application/x-www-form-urlencoded</code></p>

<p>Then change the Body to:</p>

<pre><code>grant_type=password&amp;username={username}&amp;password={password}&amp;scope=customer-api
</code></pre>
"
"54244186","How to receive/access headers of the POST request and 'User Properties' from Data Factory task in Azure Function app","<p>I'm still new to Azure, so please bear with me if this is a newbie question.</p>

<p>I created a task in Azure Data Factory that will invoke a Http-triggered Python function (Consumption plan). The settings and user properties of that task is as shown below:</p>

<p><a href=""https://i.stack.imgur.com/Yedja.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Yedja.png"" alt=""enter image description here""></a></p>

<p>and here</p>

<p><a href=""https://i.stack.imgur.com/Ycos9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ycos9.png"" alt=""enter image description here""></a></p>

<p>The function itself is as shown below:</p>

<p><a href=""https://i.stack.imgur.com/0PmSJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0PmSJ.png"" alt=""enter image description here""></a></p>

<p><strong>Q1:</strong> I'd like to know how to read/access the headers of the POST request in the Python function ('run.py' in the screenshot above). For now, I could only access the body of the HTTP request by using <code>os.environ['req']</code>.</p>

<p><strong>Q2:</strong> I'd also like to know if it's possible to access 'User Properties' in the 'run.py' assuming that I run the task in the Data Factory (the first and second screenshot). If so, how would I do that. </p>

<p>The existing resources (e.g., <a href=""https://stackoverflow.com/q/46094637/1330974"">1</a> and <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook#trigger---python-example"" rel=""nofollow noreferrer"">2</a>) that I could find online don't tell me yet. Any advice/tip would be greatly appreciated. Thank you in advance!</p>
","<azure><azure-functions><azure-data-factory>","2019-01-17 20:58:51","741","1","1","55818142","<p>I finally figured it out and am sharing what I found below so that it helps everyone out there wondering the same thing as i did.</p>

<p>This is the code I wrote in Python Function App to access the body and the request headers.</p>

<pre><code>import os
import json

# This is how I'm currently reading the **body of the POST request**
postreqdata = json.loads(open(os.environ['req']).read())

# This is how we should read **a header of the POST request**;
# here 'excelSourcePath' is one of the header names.
postreqdata['header1'] = os.environ['REQ_HEADERS_EXCELSOURCEPATH']

# 'User Properties' is just for monitoring purpose
# https://social.msdn.microsoft.com/Forums/en-US/8692cd00-307b-4204-a547-bed2030cb762/adfv2-user-property-setting?forum=AzureDataFactory

response = open(os.environ['res'], 'w')
response.write(json.dumps({'This is what I see from POST request now':postreqdata}))
response.close()
</code></pre>

<p>Hope this is helpful.</p>
"
"54241610","Formatting month in Azure Data Factory V2","<p>How do you format the date expression in ADFv2 so that you can get '1' for Jan and '10' for Oct?</p>

<p>I've tried using <code>formatDateTime(dataset().date, '%M')</code>, but I'm not sure this is the correct thing. I'm trying to set the folder paths for example 2019/1/1 and 2018/12/20.</p>
","<azure><expression><azure-data-factory>","2019-01-17 17:52:19","1467","1","1","54245149","<p>You can find all the formatting options <a href=""https://learn.microsoft.com/en-us/dotnet/standard/base-types/custom-date-and-time-format-strings"" rel=""nofollow noreferrer"">here</a> and the function reference <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">here</a>. </p>

<p>I don't think you need the %, the following should work. </p>

<pre><code>formatDateTime(dataset().date, 'M')
</code></pre>
"
"54235429","How to migration existing V1 Json data-factory template to V2 version and use the MS migration tool","<p>Have requirement to migrate the Azure data factory v1 jobs to V2 version.Tried using the existing migration tool by MS its not working giving error.Need help and advice ,how to achieve the migration and what best option or approach can be followed .Also any way to validate the Json template before deployment.</p>

<p>Tried with the MS v1 to v2 tool.Its giving error can't covert also tried directly connecting portal by tool.. same error. Any powershell script or other method where can divide the json and make it v2 format and validate same quickly.</p>

<p>Suggest the best practice or method to be followed.Any existing case study or reference which done this activity will be really helpful.</p>
","<azure-data-factory>","2019-01-17 11:56:36","1020","0","1","54340402","<p>Based on my research and this <a href=""https://social.msdn.microsoft.com/Forums/en-US/4c540366-2ddc-4c00-82a6-ffa1abd632dc/upgrade-data-factory-v1-to-v2?forum=AzureDataFactory"" rel=""nofollow noreferrer"">thread</a>,it seems that only v1 to v2 migration tool could be chosen so far.</p>

<p>See <a href=""https://learn.microsoft.com/en-us/azure/data-factory/compare-versions"" rel=""nofollow noreferrer"">documentation</a> for more on the differences between V1 and V2, and ADF V2 offers richer control flow, authoring, and monitoring capabilities.</p>

<p>If you crashed into error while using the tool, the migration tool allows for direct feedback to be submitted in the tool itself.</p>

<p><a href=""https://i.stack.imgur.com/RD17s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RD17s.png"" alt=""enter image description here""></a></p>

<p>More details, please refer to this <a href=""https://azuredatafactoryv2.github.io/v1tov2migration/"" rel=""nofollow noreferrer"">blog</a>.</p>
"
"54229475","Parallel Tasks in Data Factory Custom Activity (ADF V2)","<p>I am running Custom code activity in ADF v2 using Batch Service. Whenever this runs it only create one CloudTask within my Batch Job although I have more than two dozen parallel.Invoke methods running. Is there a way I can create multiple Tasks from one Custom Activity from ADF so that the processing can spread across all nodes in Batch Pool</p>

<p>I have fixed Pool with two nodes. Max Tasks are also set to 8 per node and Scheduling policy is also set to ""Spread"". I have only one Custom Task on my pipeline with Multiple Parallel.Invoke (Almost two Dozen).I was hoping this will create Multiple CloudTasks and will be spread Across both of my nodes as both nodes are single core. Looks like when each Custom Activity runs in ADF, it creates only one Task (CloudTask) for Batch Service. </p>

<p>My other hope was to use </p>

<p><a href=""https://learn.microsoft.com/en-us/azure/batch/tutorial-parallel-dotnet"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/batch/tutorial-parallel-dotnet</a></p>

<p>and manually create CloudTasks in my console application and create Multiple Tasks Programatically and then run that Console Application with ADF Custom Activity but CloudTask takes JobId and Cmd. Wanted to something like following but instead of passing taskCommandLine, I wanted to pass a C# method name and parameters to execute</p>

<pre><code>string taskId = ""task"" + i.ToString().PadLeft(3, '0');
string taskCommandLine = ""ping -n "" + rand.Next(minPings, maxPings + 
1).ToString() + "" localhost"";
CloudTask task = new CloudTask(taskId, taskCommandLine); 
// Wanted to do CloudTask task = new CloudTask(taskId, 
SomeMethod(args));
tasks.Add(task);
</code></pre>

<p>Also it looks like we can't create CloudTasks by using .NET API for Batch within Custom Activity of ADF</p>

<p>What I wanted to Achieve?</p>

<p>I have data in SQL Server table and I want to run different transformations on it by slicing it Horizontally or Vertically (by picking rows or columns). I want to run those transformations in Parallel (wants to have multiple CloudTask instances so that each one can operate on a specific Column Independently and after transformation load it 
into a different table). But the issue is it looks like we can't use .NET Batch Service API within ADF and the only way seems to be having multiple Custom Activities in my Data Factory pipeline. </p>
","<azure-data-factory><azure-batch>","2019-01-17 05:19:54","990","1","1","54287589","<p>Application needs to deployed on each and every node within Batch pool and CloudTasks needs to be created by calling the application with cmd</p>

<pre><code>CloudTask task =
new CloudTask(
    ""MyTask"",
    ""cmd /c %AZ_BATCH_APP_PACKAGE_MyTask%\\myTask.exe -args -here"");
</code></pre>
"
"54227113","Azure Cosmos DB throttle during copying","<p>I have a copy pipeline set up in Azure Data Factory, which copies everything in Cosmos DB daily to Azure Data Lake.
When copying. there is a spike on RU/s. I donot want to increase Throughput.</p>

<p>Anything I can do to lowering the impact? e.g. can I set a limit to the copying pipeline?</p>

<p><a href=""https://i.stack.imgur.com/SUSCH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SUSCH.png"" alt=""enter image description here""></a></p>
","<azure><azure-cosmosdb><azure-data-factory>","2019-01-16 23:51:48","692","3","1","54229740","<p>As @David said in the comment, any interactions with Cosmos DB requires the consumption of Rus. RUs setting is an important indicator of fees and performance. More details, you could refer to this <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/request-units"" rel=""nofollow noreferrer"">official article</a>.</p>

<p>Basically, RU metrics will be shocked by the adf copy activity and throughput setting will not be automatically adjusted by cosmos db. </p>

<p>If you do want to adjust throughput setting temporally,you could execute <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook"" rel=""nofollow noreferrer"">http trigger azure function</a> with <a href=""https://learn.microsoft.com/bs-latn-ba/azure//data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">azure function activity</a> which is accessed at head and tail of copy activity. In that activity,  adjust throughput settings appropriately with sdk or rest api.(Please refer to the case:<a href=""https://stackoverflow.com/questions/53830016/cosmos-db-throughput/53830754#53830754"">Cosmos Db Throughput</a>)</p>
"
"54201319","Copy nested objects from SQL Server to Azure CosmosDB using a Data Factory","<p>Let's say I have the following data structure:</p>

<pre><code>public class Account
{
    public int AccountID { get; set; }
    public string Name { get; set; }
}

public class Person
{
    public int PersonID { get; set; }
    public string Name { get; set; }
    public List&lt;Account&gt; Accounts { get; set; }
}
</code></pre>

<p>I want to move my data from an SQL Server database to Azure Cosmos DB using a Data Factory. For each person, I want to create a json file containing the accounts as nested objects like this:</p>

<pre><code>""PersonID"": 1,
""Name"": ""Jim"",
""Accounts"": [{
    ""AccountID"": 1,
    ""PersonID"": 1,
    ""Name"": ""Home""
},
{
    ""AccountID"": 2,
    ""PersonID"": 1,
    ""Name"": ""Work""
}]
</code></pre>

<p>I wrote a stored procedure to retrieve my data. In order to include the accounts as nested objects, I convert the SQL query's result to json:</p>

<pre><code>select (select *
from Person p join Account Accounts on Accounts.PersonID = p.PersonID
for json auto) as JsonResult
</code></pre>

<p>Unfortunately, my data gets copied into a single field instead of the proper object structure: </p>

<p><a href=""https://i.stack.imgur.com/7wbEe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7wbEe.png"" alt=""enter image description here""></a></p>

<p>Does anyone know what I should do to fix this?</p>

<p><strong>Edit</strong>
There is a similar question here but I didn't find a good answer:
<a href=""https://stackoverflow.com/questions/51430699/is-there-a-way-to-insert-a-document-with-a-nested-array-in-azure-data-factory"">Is there a way to insert a document with a nested array in Azure Data Factory?</a> </p>
","<json><sql-server><azure-cosmosdb><azure-data-factory>","2019-01-15 14:52:39","577","2","1","54428302","<p>For anyone in the same situation, I ended up writing a .net application to read the entries from the database and import using the SQL API.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/create-sql-api-dotnet"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/cosmos-db/create-sql-api-dotnet</a> </p>

<p>That method is little slow for large imports because it has to serialize the each object and then import them individually. A much faster way I found later is to use the bulk executor library which allows you to import json in bulk without serializing it first:</p>

<p><a href=""https://github.com/Azure/azure-cosmosdb-bulkexecutor-dotnet-getting-started"" rel=""nofollow noreferrer"">https://github.com/Azure/azure-cosmosdb-bulkexecutor-dotnet-getting-started</a> </p>

<p><a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/bulk-executor-overview"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/cosmos-db/bulk-executor-overview</a> </p>

<p><strong>Edit</strong></p>

<p>After installing the NuGet package Microsoft.Azure.CosmosDB.BulkExecutor:</p>

<pre><code>var documentClient = new DocumentClient(new Uri(connectionConfig.Uri), connectionConfig.Key);
var dataCollection = documentClient.CreateDocumentCollectionQuery(UriFactory.CreateDatabaseUri(database))
    .Where(c =&gt; c.Id == collection)
    .AsEnumerable()
    .FirstOrDefault();

var bulkExecutor = new BulkExecutor(documentClient, dataCollection);
await bulkExecutor.InitializeAsync();
</code></pre>

<p>Then import the docs:</p>

<pre><code>var response = await client.BulkIMportAsync(docunemts);
</code></pre>
"
"54192628","How to convert pipeline().parameters.windowStart to epoch in Azure Data Factory copy pipeline query","<p>I have a timestamp column in DocDb, I would like to query that in Azure Data Factory copy pipeline, which copies DocDb to Azure Data Lake</p>

<p>I would like to </p>

<pre><code>select * from c
where c._ts &gt; '@{pipeline().parameters.windowStart}'
</code></pre>

<p>But I got </p>

<pre><code>Errors"":[""An invalid query has been specified with filters against path(s) that are not range-indexed.
</code></pre>

<p>In the DocDb policy, I have</p>

<pre><code>""includedPaths"": [
    {
        ""path"": ""/*"",
        ""indexes"": [
            {
                ""kind"": ""Range"",
                ""dataType"": ""Number"",
                ""precision"": -1
            },
            {
                ""kind"": ""Hash"",
                ""dataType"": ""String"",
                ""precision"": 3
            }
        ]
    }
  ]
</code></pre>

<p>I think this should allow _ts int64 to be queried by range.</p>

<p>Where did I go wrong?</p>

<p>Thanks.</p>
","<azure-cosmosdb><azure-data-factory>","2019-01-15 04:15:51","604","2","2","54195819","<p>I reproduce your issue with your sql and your index policy.</p>

<p><a href=""https://i.stack.imgur.com/D4y8L.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D4y8L.png"" alt=""enter image description here""></a></p>

<p>Based on my observation, it seems that the filter is treated as String,not Int.You could remove the <code>'</code> in your sql and try again,it works for me.</p>

<p>sql:</p>

<pre><code>select * from c
where c._ts &gt; @{pipeline().parameters.windowStart}
</code></pre>

<p>Output:</p>

<p><a href=""https://i.stack.imgur.com/5vDd2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5vDd2.png"" alt=""enter image description here""></a></p>
"
"54192628","How to convert pipeline().parameters.windowStart to epoch in Azure Data Factory copy pipeline query","<p>I have a timestamp column in DocDb, I would like to query that in Azure Data Factory copy pipeline, which copies DocDb to Azure Data Lake</p>

<p>I would like to </p>

<pre><code>select * from c
where c._ts &gt; '@{pipeline().parameters.windowStart}'
</code></pre>

<p>But I got </p>

<pre><code>Errors"":[""An invalid query has been specified with filters against path(s) that are not range-indexed.
</code></pre>

<p>In the DocDb policy, I have</p>

<pre><code>""includedPaths"": [
    {
        ""path"": ""/*"",
        ""indexes"": [
            {
                ""kind"": ""Range"",
                ""dataType"": ""Number"",
                ""precision"": -1
            },
            {
                ""kind"": ""Hash"",
                ""dataType"": ""String"",
                ""precision"": 3
            }
        ]
    }
  ]
</code></pre>

<p>I think this should allow _ts int64 to be queried by range.</p>

<p>Where did I go wrong?</p>

<p>Thanks.</p>
","<azure-cosmosdb><azure-data-factory>","2019-01-15 04:15:51","604","2","2","54206904","<p>Thanks, @Jay.</p>
<p>I ended up using <a href=""https://blog.kloud.com.au/2018/01/12/cosmos-db-server-side-programming-with-typescript-part-2-user-defined-functions/"" rel=""nofollow noreferrer"">UDF</a></p>
<pre><code>function dateTime2Epoch(dateTimeString){
    return Math.trunc(new Date(dateTimeString).getTime()/1000);
}
</code></pre>
<p>in Cosmos db.
Then in Azure Data Factory query</p>
<pre><code>select * from c 
where c._ts &gt;= udf.dateTime2Epoch('@{pipeline().parameters.windowStart}')
  and c._ts &lt; udf.dateTime2Epoch('@{pipeline().parameters.windowEnd}')
</code></pre>
<p>However, the query seems to be very slow. I will update this when I found more.</p>
<p><strong>Update</strong>: Ended up with copying the whole thing.</p>
"
"54187379","Azure Data Factory - Oracle Source Terrible Performance","<p>Working within Azure Data Factory, using the built-in Oracle Connector...</p>

<p>Given a very simple query, such as:</p>

<pre><code>SELECT Col001, Col002, Col003 FROM APPS.WHATEVER_TABLE;
</code></pre>

<p>This type of query, with around 30 columns, can stream 1,000,000 rows to Toad on a tiny VM in less than 60 seconds. From the exact same Oracle server, from within Azure Data Factory's Self-Hosted Integration Runtime, this query takes over 8 minutes, with frequent pauses/hangs.</p>

<p>The CPU in the IR box runs at around 30% during this time.
The free memory on the IR box stays at or above 5GB during this time.
This performs the same, regardless of the DTU level of the Azure SQL Database Sink. Today I tried this between 800 DTU and 3,000 DTU and got the exact same performance, with Log I/O on the Azure SQL Database staying at or under 10%.</p>

<p>The documentation for the ADF Oracle Connector does not help in this at all, as it does not give any guidance for how to tweak connection string parameters, or really whether or not you can even do so.</p>

<p>Thoughts?</p>
","<azure><oracle11g><azure-sql-database><azure-data-factory>","2019-01-14 18:42:17","549","2","1","69119547","<p>Resolution:</p>
<p>We began to suspect that something was amiss with data types, because the problem disappeared if we cast all of our high-precision Oracle NUMBER columns to less precision, or to something like integer.</p>
<p>It got so bad that we opened a case with Microsoft about it, and our worst fears were confirmed.</p>
<p>The Azure Data Factory runtime decimal type has a maximum precision of 28. If a decimal/numeric value from the source has a higher precision, ADF will first cast it to a string. The performance of the string casting code is abysmal.</p>
<p>Check to see if your source has any high-precision numeric data, or if you have not explicitly defined schema, see if you're perhaps accidentally using string.</p>
"
"54185630","Is possible to read an Azure Databricks table from Azure Data Factory?","<p>I have a table into an Azure Databricks Cluster, i would like to replicate this data into an Azure SQL Database, to let another users analyze this data from Metabase.</p>

<p>Is it possible to acess databricks tables through Azure Data factory?</p>
","<azure><azure-data-factory><metabase><azure-databricks>","2019-01-14 16:38:12","1794","0","2","54185786","<p>No, unfortunately not. Databricks tables are typically temporary and last as long as your job/session is running. <a href=""https://docs.databricks.com/spark/latest/spark-sql/language-manual/create-table.html"" rel=""nofollow noreferrer"">See here</a>. </p>

<p>You would need to persist your databricks table to some storage in order to access it. Change your databricks job to dump the table to Blob storage as it's final action. In the next step of your data factory job, you can then read the dumped data from the storage account and process further. </p>

<p>Another option may be <a href=""https://docs.azuredatabricks.net/delta/delta-intro.html"" rel=""nofollow noreferrer"">databricks delta</a> although I have not tried this yet... </p>
"
"54185630","Is possible to read an Azure Databricks table from Azure Data Factory?","<p>I have a table into an Azure Databricks Cluster, i would like to replicate this data into an Azure SQL Database, to let another users analyze this data from Metabase.</p>

<p>Is it possible to acess databricks tables through Azure Data factory?</p>
","<azure><azure-data-factory><metabase><azure-databricks>","2019-01-14 16:38:12","1794","0","2","54189276","<p>If you register the table in the Databricks hive metastore then ADF could read from it using the ODBC source in ADF. Though this would require an IR. 
Alternatively you could write the table to external storage such as blob or lake. ADF can then read that file and push it to your sql database. </p>
"
"54179651","how to setup copy activity for json in Azure Data Factory v2 schema not detected","<p>i have try to read Json file using copy activity and write data in sql server.</p>

<p>my json file available in blob store.</p>

<p>i have set  file fromat-JSON format</p>

<blockquote>
  <p>when i try to import schema i got error-Error occurred when
  deserializing source JSON data. Please check if the data is in valid
  JSON object format.. Activity ID:2f799221-f037-4f72-8e6c-385778929110</p>
</blockquote>

<p>myjsonData</p>

<pre><code>{
    ""id"": ""ed0e4960-d9c5-11e6-85dc-d7996816aad3"",
    ""context"": {
        ""device"": {
            ""type"": ""PC""
        },
        ""custom"": {
            ""dimensions"": [
                {
                    ""TargetResourceType"": ""Microsoft.Compute/virtualMachines""
                },
                {
                    ""ResourceManagementProcessRunId"": ""827f8aaa-ab72-437c-ba48-d8917a7336a3""
                },
                {
                    ""OccurrenceTime"": ""1/13/2017 11:24:37 AM""
                }
            ]
        }
    }
}
</code></pre>

<p>Regards,
Manish</p>
","<azure-data-factory>","2019-01-14 10:26:45","1924","0","1","54196393","<p>Based on your description and your sample source data, you could import the schema directly,however the column is nested.</p>

<p><a href=""https://i.stack.imgur.com/KoIXT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KoIXT.png"" alt=""enter image description here""></a></p>

<p>If you want to flatten the nested json before you store them into sql server database as rows,you could execute <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure Function Activity</a> before the Copy Activity.</p>

<p>Or you could execute the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#invoking-stored-procedure-for-sql-sink"" rel=""nofollow noreferrer"">stored procedure</a> in sql server dataset.</p>
"
"54170433","Azure Data Factory - Clean Up Batch Task Files","<p>I'm working with Azure Data Factory v2, using a Batch Account Pool w/ dedicated nodes to do processing. I'm finding over time the Batch Activity fails due to no more space on the D:/ temp drive on the nodes. For each ADF job, it creates a working directory on the node and after the job completes I'm finding it doesn't clean up the files. Wondering if anybody else has encountered this before and what the best solution to implement is.</p>

<p>EDIT: Seems to be a file retention setting in ADF nowadays that wasn't present when I posed the question. For anybody future coming across the same issue that's a possible solution.</p>
","<azure-data-factory><azure-batch>","2019-01-13 15:40:16","1200","2","4","54174262","<p>Cleanup of tasks is done either when the task is deleted or when the tasks retention time is elapsed (<a href=""https://learn.microsoft.com/en-us/rest/api/batchservice/task/add#taskconstraints"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/batchservice/task/add#taskconstraints</a>). Either of these should solve the issue you are having.</p>

<p>Note: The default retention time has been decreased from infinite to 7 days in the latest REST API(2018-12-01.8.0) to allow task cleanup by default. Tasks created with versions prior to this will not have this new default.</p>
"
"54170433","Azure Data Factory - Clean Up Batch Task Files","<p>I'm working with Azure Data Factory v2, using a Batch Account Pool w/ dedicated nodes to do processing. I'm finding over time the Batch Activity fails due to no more space on the D:/ temp drive on the nodes. For each ADF job, it creates a working directory on the node and after the job completes I'm finding it doesn't clean up the files. Wondering if anybody else has encountered this before and what the best solution to implement is.</p>

<p>EDIT: Seems to be a file retention setting in ADF nowadays that wasn't present when I posed the question. For anybody future coming across the same issue that's a possible solution.</p>
","<azure-data-factory><azure-batch>","2019-01-13 15:40:16","1200","2","4","54202202","<p>Figured out a solution, posting to hopefully help the next person that comes along. </p>

<p>I found the Azure Python SDK for Batch, I created a small script that will iterate through all the pools + nodes on an account and delete any files in the workitems directory that are older than 1 day. </p>

<pre><code>import azure.batch as batch
import azure.batch.operations.file_operations as file_operations
from azure.batch.batch_auth import SharedKeyCredentials
import azure.batch.operations
import msrest.service_client
from datetime import datetime

program_datetime = datetime.utcnow()

batch_account = 'batchaccount001'
batch_url = 'https://batchaccount001.westeurope.batch.azure.com'
batch_key = '&lt;BatchKeyGoesHere&gt;'
batch_credentials = SharedKeyCredentials(batch_account, batch_key)

#Create Batch Client with which to do operations
batch_client = batch.BatchServiceClient(credentials=batch_credentials,
                                        batch_url = batch_url
                                        )

service_client = msrest.service_client.ServiceClient(batch_credentials, batch_client.config)

#List out all the pools
pools = batch_client.pool.list()
pool_list = [p.id for p in pools]

for p in pool_list:
    nodes = batch_client.compute_node.list(p)
    node_list = [n.id for n in nodes]
    for n in node_list:
        pool_id = p
        node_id = n
        print(f'Pool = {pool_id}, Node = {node_id}')
        fo_client = azure.batch.operations.FileOperations(service_client,
                                                          config=batch_client.config,
                                                          serializer=batch_client._serialize,
                                                          deserializer=batch_client._deserialize)
        files = fo_client.list_from_compute_node(pool_id,
                                                 node_id,
                                                 recursive=True,
                                                 file_list_from_compute_node_options=None,
                                                 custom_headers=None,
                                                 raw=False
                                                )

        for file in files:
            # Check to make sure it's not a directory. Directories do not have a last_modified property.
            if not file.is_directory:
                file_datetime = file.properties.last_modified.replace(tzinfo=None)
                file_age_in_seconds = (program_datetime - file_datetime).total_seconds()
                # Delete anything older than a day in the workitems directory.
                if file_age_in_seconds &gt; 86400 and file.name.startswith('workitems'):
                    print(f'{file_age_in_seconds} : {file.name}')
                    fo_client.delete_from_compute_node(pool_id, node_id, file.name)
</code></pre>
"
"54170433","Azure Data Factory - Clean Up Batch Task Files","<p>I'm working with Azure Data Factory v2, using a Batch Account Pool w/ dedicated nodes to do processing. I'm finding over time the Batch Activity fails due to no more space on the D:/ temp drive on the nodes. For each ADF job, it creates a working directory on the node and after the job completes I'm finding it doesn't clean up the files. Wondering if anybody else has encountered this before and what the best solution to implement is.</p>

<p>EDIT: Seems to be a file retention setting in ADF nowadays that wasn't present when I posed the question. For anybody future coming across the same issue that's a possible solution.</p>
","<azure-data-factory><azure-batch>","2019-01-13 15:40:16","1200","2","4","54245756","<p>I'm an engineer with Azure Data Factory.  We used an Azure Batch SDK earlier than 2018-12-01.8.0, thus the Batch tasks created via ADF defaulted to an infinite retention period as mentioned earlier.
We're rolling out a fix to default the retention period for Batch tasks created through ADF to 30 days going forward, and also introducing a property, retentionTimeInDays in the typeProperties of custom activity, that customers can set in their ADF pipelines to override this default.  When this has been rolled out, the documentation at <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity#custom-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity#custom-activity</a> will be updated with more details.  Thank you for your patience.</p>
"
"54170433","Azure Data Factory - Clean Up Batch Task Files","<p>I'm working with Azure Data Factory v2, using a Batch Account Pool w/ dedicated nodes to do processing. I'm finding over time the Batch Activity fails due to no more space on the D:/ temp drive on the nodes. For each ADF job, it creates a working directory on the node and after the job completes I'm finding it doesn't clean up the files. Wondering if anybody else has encountered this before and what the best solution to implement is.</p>

<p>EDIT: Seems to be a file retention setting in ADF nowadays that wasn't present when I posed the question. For anybody future coming across the same issue that's a possible solution.</p>
","<azure-data-factory><azure-batch>","2019-01-13 15:40:16","1200","2","4","55472880","<p>You can use the <code>retentionTimeInDays</code> config in <code>typeProperties</code> when deploying via ARM Template.</p>

<p>Please note that you should provide the config <code>retentionTimeInDays</code> in <code>Double</code> and not <code>String</code>.</p>
"
"54168013","Data Factory Data Copy from REST to Table Storage - PartitionKey null [object/array]","<p>Good day,</p>

<p>We are importing the following JSON format via COPY DATA (REST TO TABLE STORAGE) in Data Factory v2:</p>

<pre><code>[  
   {  
      ""ERROR"":false,
      ""USERNAME"":""DUMMY""
   },
   [  
      {  
         ""A"":1,
         ""B"":2,
         ""C"":3
      },
      {  
         ""A"":1,
         ""B"":0,
         ""C"":3
      }
   ]
]
</code></pre>

<p>Partition key value: <code>USE SOURCE COLUMN</code></p>

<p>Partition key column: <code>C</code></p>

<p>Mapping is properly set for <code>A B and C</code>. ERROR and USERNAME are -- Not Included --.</p>

<p>Gives the following error in debug:</p>

<blockquote>
  <p>""errorCode"": ""2200"", ""message"": ""Failure happened on 'Sink' side.
  ErrorCode=UserErrorAzureTableKeyColumnWithNullValue,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Column
  'C' contains 'NULL' value thus cannot be used as 'PartitionKey' for
  Azure Table.</p>
</blockquote>

<p>I understand why the error happens, the COPY DATA tool tries to create three rows:</p>

<p>1.Fist row is based on ERROR and USERNAME. This row has indeed value null for C</p>

<p>2.Second row is based on A, B and C. This row is correct as there are no null values for C.</p>

<p>3.Third row is based on A, B and C. This row is correct as there are no null values for C.</p>

<p>Fault tolerance is on: Skip incompatible rows, but does not solve the problem.</p>

<p>We know above because it works with default Partition Key value:</p>

<p><a href=""https://i.stack.imgur.com/g5BCO.png"" rel=""nofollow noreferrer"">Screenshot two rows</a></p>

<p>We can't change the format of the input. What can we do in Data Factory to solve this issue? How can we disregard first object with ERROR and USER, or how can we skip rows with Partition Key if null?</p>

<p>Is this something that can handled with ""Dynamic Content"" code?</p>
","<json><azure><azure-table-storage><azure-data-factory>","2019-01-13 10:35:03","194","-1","1","54177885","<p>Sinbad. Based on my test,there is no direct way to skip the specific rows in the <code>REST-TableStorage</code> copy activity.</p>

<p>My idea is that you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure Function activity</a> to process your rest api data first. </p>

<p>Please refer to the below steps:</p>

<p>1.Create <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook"" rel=""nofollow noreferrer"">Http Trigger</a> in Azure Funtion, access your rest api and get the data.</p>

<p>2.In that Azure Function,loop the data and remove the ""<code>ERROR""</code> and <code>""USERNAME""</code> row to return the array which only contains <code>""A"" ""B"" ""C""</code>.</p>

<p>3.Execute the function in azure data factory with Azure Function activity ,then link with copy activity.</p>

<p><a href=""https://i.stack.imgur.com/boxjS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/boxjS.png"" alt=""enter image description here""></a></p>

<p>There will be no more incompatible rows.</p>
"
"54151549","Issue with 24 hour time parameter in Azure ADF V2","<p>I'm trying to use the expression @formatDateTime(dataset().WindowStartTime, 'hh') as a dynamic filename in Azure Data Factory V2. </p>

<p>I'm having the issue that when I'm using the value 2018-12-31T13:00:04.279Z for WindowStartTime, I'm expecting a filename of 13 but I'm getting a filename of 01. </p>

<p>I'm wondering what I need to do in the expression to get the time in 24 hour time?</p>
","<azure><expression><azure-data-factory>","2019-01-11 17:41:31","1087","0","1","54168495","<p>I think that changing <code>hh</code> to <code>HH</code> should solve your problem.</p>

<p>Take a look at <a href=""https://learn.microsoft.com/en-us/dotnet/standard/base-types/custom-date-and-time-format-strings"" rel=""nofollow noreferrer"">Custom Date and Time Format Strings</a> for more information on custom formatting.</p>

<p>Hope it helps!</p>
"
"54150714","how to load file from share point online to Azure Blob using Azure Data Factory","<p>Can any one help me how to load csv file from share point online to azure Blob storage using Azure Data Factory. </p>

<p>I  tried with Logic apps and succeed however logic app will not upload all file unless  we made any change to the file or upload new. </p>

<p>I need to load all the file even there is no changes. </p>
","<azure><azure-logic-apps><azure-data-factory>","2019-01-11 16:43:02","1372","0","2","54160096","<p>You probably can use a Logic App by changing to a Recurrence Trigger.</p>

<p>On that interval, you List the files in the Library then take any action on them you want.</p>
"
"54150714","how to load file from share point online to Azure Blob using Azure Data Factory","<p>Can any one help me how to load csv file from share point online to azure Blob storage using Azure Data Factory. </p>

<p>I  tried with Logic apps and succeed however logic app will not upload all file unless  we made any change to the file or upload new. </p>

<p>I need to load all the file even there is no changes. </p>
","<azure><azure-logic-apps><azure-data-factory>","2019-01-11 16:43:02","1372","0","2","54412984","<p>ADF v2 now supports loading from sharepoint online by OData connector with AAD service principal authentication: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-odata"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-odata</a>  </p>
"
"54137361","ADFv2: creating data source with a dynamic authenticated URL","<p>I have a pipeline that uses a Lookup activity with a POST REST service as the source dataset.  This Lookup successfully retrieves a string from the body that is then passed as an ""access token"" inside the header of GET Web activity.</p>

<p>This GET Web activity then returns a set of 3 values inside the body of the return: fileURL, sasToken, fileaccessURL.  The ""fileaccessURL"" is simply a concatenation of ""fileURL"" and ""sasToken""</p>

<p>The new dynamically created ""fileaccessurl"" points to a ZIP file inside a blob storage housed with my partner company.  This ZIP file contains that actual data I want to use as my data source. I am OK with deflating the file once I get it and using the files within as my actual source.</p>

<p>The issue I'm having is how to specify that the dynamically assigned ""fileaccessurl"" is my data source for getting that initial ZIP file?  I can't parameterize REST or HTTP linked services or figure out any other way to set this.  I have already eliminated the option of connecting directly to the partner blob storage.  I have tried using lookups but I still can't figure out how to specify my destination with a URL that isn't necessarily always the same.</p>
","<azure><azure-sql-database><azure-data-factory>","2019-01-10 21:46:47","500","0","1","54413017","<p>If you are looking for how to parameterize linked service, you can try to use the advanced part and check ""specify dynamic content in JSON format"".</p>

<p><a href=""https://i.stack.imgur.com/VAjYG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VAjYG.png"" alt=""enter image description here""></a></p>

<p>This is the sample JSON code:</p>

<pre><code>{
    ""name"": ""AzureSqlDatabase"",
    ""properties"": {
        ""type"": ""AzureSqlDatabase"",
        ""typeProperties"": {
            ""connectionString"": {
                ""value"": ""Server=tcp:myserver.database.windows.net,1433;Database=@{linkedService().DBName};User ID=user;Password=fake;Trusted_Connection=False;Encrypt=True;Connection Timeout=30"",
                ""type"": ""SecureString""
            }
        },
        ""connectVia"": null,
        ""parameters"": {
            ""DBName"": {
                ""type"": ""String""
            }
        }
    }
}
</code></pre>

<p>You can try to modify the code to align with Rest or HTTP linked service payload.</p>
"
"54135823","insert @trigger().startTime into sql table in Azure DataFactory","<p>I have DataFactory where the source is CSV and the Destination is Azure SQL Data Warehouse</p>

<p>the Table in the Azure SQL Data Warehouse has an extra DateTime column for when the trigger was fired.</p>

<p>how I can have this to work when mapping the schema?</p>

<p>note: in Azure SQL Data Warehouse, it is not possible to have a column with default value GETDATE() as in Azure SQL Database.</p>

<p>the Column in SQL Data warehouse is ""InsertedOn""</p>

<p>My Pipeline looks like this:</p>

<pre><code>{
    ""name"": ""Pipeline01"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""CopyCSVtoDW"",
                ""type"": ""Copy"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""BlobSource"",
                        ""recursive"": true
                    },
                    ""sink"": {
                        ""type"": ""SqlDWSink"",
                        ""allowPolyBase"": false,
                        ""writeBatchSize"": 10000
                    },
                    ""enableStaging"": false,
                    ""enableSkipIncompatibleRow"": false,
                    ""translator"": {
                        ""type"": ""TabularTranslator"",
                        ""columnMappings"": {
                            ""Id"": ""pointconnectnativeid"",
                            ""ValueDate"": ""valuedate"",
                            ""Value"": ""value"",
                            ""InsertedOn"": ""insertedon"",
                            ""forecastDate"": ""forecastDate""
                        }
                    }
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""SourceCSV"",
                        ""type"": ""DatasetReference""
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""DestinationDW"",
                        ""type"": ""DatasetReference""
                    }
                ]
            }
        ]
    },
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>

<p>here is my source:</p>

<pre><code>{
    ""name"": ""SourceCSV"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""skdwstorage"",
            ""type"": ""LinkedServiceReference""
        },
        ""parameters"": {
            ""triggerDateTime"": {
                ""type"": ""Object"",
                ""defaultValue"": ""@trigger().startTime""
            }
        },
        ""type"": ""AzureBlob"",
        ""structure"": [
            {
                ""name"": ""Id"",
                ""type"": ""String""
            },
            {
                ""name"": ""ValueDate"",
                ""type"": ""DateTime"",
                ""format"": ""dd.MM.yyyy HH:mm:ss""
            },
            {
                ""name"": ""Value"",
                ""type"": ""Decimal""
            },
            {
                ""name"": ""InsertedOn"",
                ""type"": ""DateTime"",
                ""description"": ""@trigger().startTime"",
                ""format"": ""dd.MM.yyyy HH:mm:ss""
            },
            {
                ""name"": ""forecastDate"",
                ""type"": ""DateTime"",
                ""format"": ""dd.MM.yyyy HH:mm:ss""
            }
        ],
        ""typeProperties"": {
            ""format"": {
                ""type"": ""TextFormat"",
                ""columnDelimiter"": ""|"",
                ""rowDelimiter"": ""\n"",
                ""quoteChar"": ""\"""",
                ""nullValue"": ""\\N"",
                ""encodingName"": null,
                ""treatEmptyAsNull"": true,
                ""skipLineCount"": 0,
                ""firstRowAsHeader"": true
            },
            ""fileName"": """",
            ""folderPath"": ""csv""
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>
","<azure-data-factory><azure-synapse>","2019-01-10 19:37:05","1382","0","1","54144709","<p>In your .json trigger definition you can define a parameter called TriggerStartTime:</p>

<pre><code>""parameters"": {
"" TriggerStartTime"": ""@trigger().startTime""
}
</code></pre>

<p>In your case for example:</p>

<pre><code>{
    ""name"": ""Pipeline01Trigger"",
    ""properties"": {
        ""runtimeState"": ""Started"",
        ""pipelines"": [
            {
                ""pipelineReference"": {
                    ""referenceName"": ""Pipeline01"",
                    ""type"": ""PipelineReference""
                },
                ""parameters"": {
                    ""TriggerStartTime"": ""@trigger().startTime""
                }
            }
        ],
        ""type"": ""ScheduleTrigger"",
        ""typeProperties"": {
            ""recurrence"": {
                ""frequency"": ""Hour"",
                ""interval"": 1,
                ""startTime"": ""2019-01-01T00:00:00Z"",
                ""timeZone"": ""UTC""
            }
        }
    }
}
</code></pre>

<p>In your Pipeline01 Parameter's section you have to set up a default value for the parameter.</p>

<p>After in your copy activity you can Map this parameter with:</p>

<pre><code>@pipeline().parameters.TriggerStartTime
</code></pre>

<p>In your case something like:</p>

<pre><code>""columnMappings"": {
    ""Id"": ""pointconnectnativeid"",
    ""ValueDate"": ""valuedate"",
    ""Value"": ""value"",
    ""InsertedOn"": ""@pipeline().parameters.TriggerStartTime"",
    ""forecastDate"": ""forecastDate""
}
</code></pre>

<p>Here you can find some information:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger#pass-the-trigger-start-time-to-a-pipeline"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger#pass-the-trigger-start-time-to-a-pipeline</a></p>

<p><a href=""https://learn.microsoft.com/es-es/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">https://learn.microsoft.com/es-es/azure/data-factory/control-flow-expression-language-functions</a></p>
"
"54132618","Its possible to remove files from data lake using data factory?","<p>Its possible remove files from datalake using data factory?
Its very dificult to do that, i m stuck at it.</p>
","<azure-data-factory><azure-data-lake>","2019-01-10 16:06:39","173","1","2","54136204","<p>It's not possible to delete a file using Data Factory. I found that the main purpose of Data Factory is ETL orchestration.</p>

<p>If you are familiar with .NET/Java coding, I would suggest you to create an Azure Function that will be called from ADF and it will delete a file. Azure Functions are now supported in ADF (don't need to use HTTP trigger/web component)</p>

<p>There is an SDK for File operation on Data Lake, and it contains method Delete, that deletes a file from Data Lake Store.
Take a look on this docs: 
<a href=""https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-data-operations-net-sdk#see-also"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-data-operations-net-sdk#see-also</a></p>

<p>Delete method: 
<a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.datalake.store.adlsclient.delete?view=azure-dotnet"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.datalake.store.adlsclient.delete?view=azure-dotnet</a></p>

<p>You can pass path to the file as a request parameter/header whatever you feel comfortable from ADF to Azure Function.
Hope this suggestion can work for you.</p>
"
"54132618","Its possible to remove files from data lake using data factory?","<p>Its possible remove files from datalake using data factory?
Its very dificult to do that, i m stuck at it.</p>
","<azure-data-factory><azure-data-lake>","2019-01-10 16:06:39","173","1","2","54668617","<p>The best option you have is create an Azure Batch to do that.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/batch/"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/batch/</a></p>

<p>You can create an application that remove the files and call it from data factory.
You can pass the files as parameteres too.</p>
"
"54131171","Azure Data Factory - 'variables' is not a recognized function","<p>We are using an Azure SQL Database sink in a Copy Activity.</p>

<p>The requirement is for us to execute a stored procedure here via the ""Pre-Copy Script"" property of the Sink. We are using Dynamic Content, passing in a ""ProcessName"" parameter. As you can see, we have a ProcessName variable, and it is used in a call to @concat() to build the stored procedure string for this Sink property.</p>

<p>However, any time we use the variables collection in Dynamic Content, we get this warning as shown in the image. The warning states:</p>

<blockquote>
  <p>'variables' is not a recognized function</p>
</blockquote>

<p>Is there a way to avoid having this ""Warning"" in the UI? It works fine, but it looks terrible. It appears everywhere we use variables, not just in this case.</p>

<p><a href=""https://i.stack.imgur.com/OL47E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OL47E.png"" alt=""Add Dynamic Content""></a></p>
","<azure-data-factory>","2019-01-10 14:48:17","2269","2","1","54131312","<p>Try using pipeline parameters instead of variables and calling it as explained here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions</a></p>

<p>You are most likely going to have to change your logic to do this. But if it works fine as it is, I wouldn't mind changing logic or code just to stop seeing the warning in the UI.</p>

<p>Hope this helped!</p>
"
"54130889","How can I visually distinguish Activities in ForEach loops in DataFactory Templates?","<p>I am working with Azure <strong>DataFactory Templates</strong> and trying to give each activity a unique <strong>name</strong> in a <strong>ForEach</strong> loop so that I can distinguish them in the pipeline run.</p>

<p>I am copying files from BlobStorage into CosmosDB collections. If I execute my script I only see identical entries for each activity and do not see which collections are being migrated:</p>

<p><a href=""https://i.stack.imgur.com/zZo6J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zZo6J.png"" alt=""enter image description here""></a></p>

<p>This is what I want to use, but it gives me an InvalidTemplate error:</p>

<pre><code>{
    ""name"": ""ForEach_894"",
    ""type"": ""ForEach"",
    ""typeProperties"": {
        ""items"": {
            ""value"": ""@activity('GetBlobStorageFileNames').output.childItems"",
            ""type"": ""Expression""
        },
        ""activities"": [
            {
                ""name"": ""@{item().name}"",
                ...
            }
        ]
    }
}
</code></pre>

<pre><code>ErrorCode=InvalidTemplate, ErrorMessage=The template validation failed: 'The name of template action '@{item().name}Scope' at line '1' and column '20385' is not defined or not valid
</code></pre>

<p>My script works if I use a fixed string:</p>

<pre><code>""activities"": [
    {
        ""name"": ""abc"",
        ...
    }
</code></pre>

<p>Maybe I can add extra columns like in this case:</p>

<p><a href=""https://i.stack.imgur.com/0TseP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0TseP.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2019-01-10 14:31:29","246","1","1","54981931","<p>I ran into the same issue. You cannot see it while the activity is running, but once the activity has finished the columns (Source, Destination) will be added.</p>

<p><a href=""https://i.stack.imgur.com/CtQRY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CtQRY.png"" alt=""enter image description here""></a></p>

<p>Just specify them in your ARM Template, like this:</p>

<pre><code>{
    ""name"": ""ForEach_894"",
    ""type"": ""ForEach"",
    ""typeProperties"": {
        ""items"": {
            ""value"": ""@activity('GetBlobStorageFileNames').output.childItems"",
            ""type"": ""Expression""
        },
        ""activities"": [
            {
                ""name"": ""@{item().name}"",
                ""userProperties"": [
                    {
                        ""name"": ""Source"",
                        ""value"": ""@{item().name}""
                    },
                    {
                        ""name"": ""Destination"",
                        ""value"": ""@{item().name}""
                    }
                ]
                ...
            }
        ]
    }
}
</code></pre>
"
"54128355","Azure Data Factory Date Conversion","<p>I am just starting to learn Data Factory and I have a very simple pipeline that is loading a CSV file from Azure Blob Storage into Azure SQL Data Warehouse.</p>

<p>The source CSV file column 3 includes dates in the format of <code>MM/dd/yyyy</code> and my target SQL DW table has this column typed as date.</p>

<p>My pipeline fails with the following error, presumably because I am not in US and so an implicit type conversion won't work:</p>

<blockquote>
  <p>Activity BlobToSQLDW failed: ErrorCode=FailedDbOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Error happened when loading data into SQL Data Warehouse.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Data.SqlClient.SqlException,Message=HdfsBridge::recordReaderFillBuffer - Unexpected error encountered filling record reader buffer: HadoopSqlException: Error converting data type VARCHAR to DATETIME.,Source=.Net SqlClient Data Provider,SqlErrorNumber=107090,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=107090,State=1,Message=HdfsBridge::recordReaderFillBuffer - Unexpected error encountered filling record reader buffer: HadoopSqlException: Error converting data type VARCHAR to DATETIME.,},],'</p>
</blockquote>

<p>I have tried making edits to the schema for the blob source, including setting format and culture (see screenshot below) but to no avail yet.</p>

<p><strong>Azure Blob Storage Schema screenshot:</strong></p>

<p><img src=""https://i.stack.imgur.com/9tsSI.png"" alt=""Azure Blob Storage Schema screenshot""></p>

<p>I did manage to workaround the issue by creating a staging table in SQL DW with all columns typed as varchar and changing the ADF pipeline to target that instead, and then I complete the task inside SQL DW using a T-SQL Insert (staging-to-target) to make the type conversions.</p>

<p>I'm sure there is a proper way to do this within ADF though. Can anyone please advise?</p>
","<azure-data-factory>","2019-01-10 12:06:27","9123","0","2","54321197","<p>Date is only supported in SQL type data source (Azure SQL, SQL server). in other dataset, only DateTime is available. I didn't find there is a date option in blob dataset schema, can you check again if it is a blob dataset ?</p>
"
"54128355","Azure Data Factory Date Conversion","<p>I am just starting to learn Data Factory and I have a very simple pipeline that is loading a CSV file from Azure Blob Storage into Azure SQL Data Warehouse.</p>

<p>The source CSV file column 3 includes dates in the format of <code>MM/dd/yyyy</code> and my target SQL DW table has this column typed as date.</p>

<p>My pipeline fails with the following error, presumably because I am not in US and so an implicit type conversion won't work:</p>

<blockquote>
  <p>Activity BlobToSQLDW failed: ErrorCode=FailedDbOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Error happened when loading data into SQL Data Warehouse.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Data.SqlClient.SqlException,Message=HdfsBridge::recordReaderFillBuffer - Unexpected error encountered filling record reader buffer: HadoopSqlException: Error converting data type VARCHAR to DATETIME.,Source=.Net SqlClient Data Provider,SqlErrorNumber=107090,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=107090,State=1,Message=HdfsBridge::recordReaderFillBuffer - Unexpected error encountered filling record reader buffer: HadoopSqlException: Error converting data type VARCHAR to DATETIME.,},],'</p>
</blockquote>

<p>I have tried making edits to the schema for the blob source, including setting format and culture (see screenshot below) but to no avail yet.</p>

<p><strong>Azure Blob Storage Schema screenshot:</strong></p>

<p><img src=""https://i.stack.imgur.com/9tsSI.png"" alt=""Azure Blob Storage Schema screenshot""></p>

<p>I did manage to workaround the issue by creating a staging table in SQL DW with all columns typed as varchar and changing the ADF pipeline to target that instead, and then I complete the task inside SQL DW using a T-SQL Insert (staging-to-target) to make the type conversions.</p>

<p>I'm sure there is a proper way to do this within ADF though. Can anyone please advise?</p>
","<azure-data-factory>","2019-01-10 12:06:27","9123","0","2","55404793","<p>It is because ADF is using Polybase for loading file data to SQL DW. According to official documentation Polybase default date format is yyyy-MM-dd. I haven't found any way to specify a date format in ADF Copy Activity properties nor source Dataset properties. The only option is disabling Polybase and It will work. You should uncheck ""Allow Polybase"" in ""Sink"" tab in Copy Activity properties.</p>
"
"54124081","Trigger not running for Stored Procedure Activity in azure Data Factory v2","<p>I have created a Activity Stored Procedure1.
it is running fine when i manually run.
it is running fine with Trigger Now.</p>

<p>but schedule trigger not running. I have only one activity in my pipeline Stored Procedure1</p>

<p>below is trigger code</p>

<pre><code>{
""name"": ""trigger1"",
""properties"": {
    ""runtimeState"": ""Started"",
    ""pipelines"": [
        {
            ""pipelineReference"": {
                ""referenceName"": ""pipeline3"",
                ""type"": ""PipelineReference""
            }
        }
    ],
    ""type"": ""ScheduleTrigger"",
    ""typeProperties"": {
        ""recurrence"": {
            ""frequency"": ""Minute"",
            ""interval"": 6,
            ""startTime"": ""2019-01-10T20:47:00.000Z"",
            ""timeZone"": ""UTC""
        }
    }
}
</code></pre>
","<azure-data-factory>","2019-01-10 07:54:05","889","1","2","54125081","<p>Based on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#schedule-trigger-definition"" rel=""nofollow noreferrer"">official document</a>, I think your trigger did not be triggered because of the limitation of Azure Data Factory.</p>

<p><strong><em>Official statement:</em></strong></p>

<blockquote>
  <p>The parameters property is a mandatory property of the pipelines
  element. If your pipeline doesn't take any parameters, you must
  include an empty JSON definition for the parameters property.</p>
</blockquote>

<p>So, according to your code, you missed the <code>parameters</code> property. Please add it even though you don't have it, and the trigger will be executed normally.</p>
"
"54124081","Trigger not running for Stored Procedure Activity in azure Data Factory v2","<p>I have created a Activity Stored Procedure1.
it is running fine when i manually run.
it is running fine with Trigger Now.</p>

<p>but schedule trigger not running. I have only one activity in my pipeline Stored Procedure1</p>

<p>below is trigger code</p>

<pre><code>{
""name"": ""trigger1"",
""properties"": {
    ""runtimeState"": ""Started"",
    ""pipelines"": [
        {
            ""pipelineReference"": {
                ""referenceName"": ""pipeline3"",
                ""type"": ""PipelineReference""
            }
        }
    ],
    ""type"": ""ScheduleTrigger"",
    ""typeProperties"": {
        ""recurrence"": {
            ""frequency"": ""Minute"",
            ""interval"": 6,
            ""startTime"": ""2019-01-10T20:47:00.000Z"",
            ""timeZone"": ""UTC""
        }
    }
}
</code></pre>
","<azure-data-factory>","2019-01-10 07:54:05","889","1","2","57043330","<p>Just adding this if it helps to someone...
I had the similar issue and tried this option of creating dummy variable in pipeline, but the issue was when I deploy the code to other environment(dev-->test/ppe/prod), i was getting the error like 'instance of an object not set properly' (and I have azure function activity in my adf pipeline, and this is where i was getting this error). So I tried creating dummy header under 'settings' in azure function activity and deployed,trigger worked this time as expected. Hope this helps.</p>
"
"54101142","Connect Azure Data Factory to SAP BW","<p>I have an SSIS package that successfully uses the Microsoft SAP BW connector. The SAP Administrator has set up his side so that it uses a process chain and ProgramId as connection criteria.  I start my SSIS package and it runs in ""Wait"" mode until the SAP job executes.  This all works great.  I now need to replicate this using the Azure data factory's SAP BW connector but the Azure connector does not have the same look and feel so I am attempting to edit the code in the Connections tab for the SAPBW connection to include the Wait mode etc.</p>

<p>The SAP BW connection to the SAP BW system successfully passes the ""Test Connection"" in the Data Factory.
In the SSIS SAP BW connector the advanced properties display these values which I am trying to replicate (hope this image works):
<img src=""https://drive.google.com/open?id=11Hvif2mWUTGA_OCjk6XY9bpmUbkJzuVN"" alt=""SSIS SapBW""></p>

<p>So I added the ""Custom Properties"" to the code in the Connections -> linked Services->SapBw</p>

<pre><code>    {
""name"": ""SapBw"",
""type"": ""Microsoft.DataFactory/factories/linkedservices"",
""properties"": {
    ""type"": ""SapBw"",
    ""typeProperties"": {
        ""server"": ""sapdb.compnme.local"",
        ""systemNumber"": ""00"",
        ""clientId"": ""400"",
        ""userName"": ""myUser"",
        ""encryptedCredential"": ""abc123""
    },
    ""connectVia"": {
        ""referenceName"": ""ARuntime"",
        ""type"": ""IntegrationRuntimeReference""
    }
},
        ""Custom Properties"":{
        ""DbTableName"":""/BIC/OHCSST_OHD"",
        ""DestinationName"":""CSST_OHD"",
        ""ExecutionMode"":""W"",
        ""GatewayHost"":""sapdb.compnme.local"",
        ""GatewayService"":""sapgw00"",
        ""ProcessChain"":""Z_CS_STAT_OHD"",
        ""ProgramId"":""ProgId_P23"",
        ""Timeout"":""1200""

    }
</code></pre>

<p>}</p>

<p>Unfortunately, when I click ""Finish"" the connection is successfully published but when I go to view the code my Custom Properties have disappeared.  Is there a different process to connect to SAP Open Hub iwht the Azure data factory as there does not appear to be anything on the MS website to guide me.</p>
","<azure-data-factory>","2019-01-08 23:21:31","1018","0","2","54301965","<p>Your image attachment could not display correctly. Based upon what I comprehend, I wonder if you confused ADF SSIS-IR and ADF Self-hosted IR. </p>

<p>Because you leveraged the BW connector in SSIS, apparently you were using the SSIS package and deployed it to ADF SSIS-IR stack. This IR has nothing to do with the Self-hosted IR which is required by ADF Copy activity from SAP BW. You mentioned you defined custom properties in the linked services, but the context of linked services is for the ADF native BW MDX connection interface. No matter what you define in the ADF linked services, it would not affect SSIS IR. Also, you may need to realize that ADF native BW interface is for MDX access only to query BW InfoCube and BEx QueryCube data. There is nothing to do with Open Hub.</p>

<p>Tactically, you should apply the custom properties to your BW connection in SSIS package, but I have a feeling that you may not know deeply the pros and cons of SSIS BW connector, ADF BW connector, Open Hub, and MDX. From real project experience, there are major robustness issues with the SSIS BW connector's integration with Open Hub and Process Chain. The DTP jobs inside the process chain could fail frequently, and the ""reset"" of DTP jobs is a frustrating experience. I suggest you describe your requirement before spending too much energy solving a connection property issue.</p>
"
"54101142","Connect Azure Data Factory to SAP BW","<p>I have an SSIS package that successfully uses the Microsoft SAP BW connector. The SAP Administrator has set up his side so that it uses a process chain and ProgramId as connection criteria.  I start my SSIS package and it runs in ""Wait"" mode until the SAP job executes.  This all works great.  I now need to replicate this using the Azure data factory's SAP BW connector but the Azure connector does not have the same look and feel so I am attempting to edit the code in the Connections tab for the SAPBW connection to include the Wait mode etc.</p>

<p>The SAP BW connection to the SAP BW system successfully passes the ""Test Connection"" in the Data Factory.
In the SSIS SAP BW connector the advanced properties display these values which I am trying to replicate (hope this image works):
<img src=""https://drive.google.com/open?id=11Hvif2mWUTGA_OCjk6XY9bpmUbkJzuVN"" alt=""SSIS SapBW""></p>

<p>So I added the ""Custom Properties"" to the code in the Connections -> linked Services->SapBw</p>

<pre><code>    {
""name"": ""SapBw"",
""type"": ""Microsoft.DataFactory/factories/linkedservices"",
""properties"": {
    ""type"": ""SapBw"",
    ""typeProperties"": {
        ""server"": ""sapdb.compnme.local"",
        ""systemNumber"": ""00"",
        ""clientId"": ""400"",
        ""userName"": ""myUser"",
        ""encryptedCredential"": ""abc123""
    },
    ""connectVia"": {
        ""referenceName"": ""ARuntime"",
        ""type"": ""IntegrationRuntimeReference""
    }
},
        ""Custom Properties"":{
        ""DbTableName"":""/BIC/OHCSST_OHD"",
        ""DestinationName"":""CSST_OHD"",
        ""ExecutionMode"":""W"",
        ""GatewayHost"":""sapdb.compnme.local"",
        ""GatewayService"":""sapgw00"",
        ""ProcessChain"":""Z_CS_STAT_OHD"",
        ""ProgramId"":""ProgId_P23"",
        ""Timeout"":""1200""

    }
</code></pre>

<p>}</p>

<p>Unfortunately, when I click ""Finish"" the connection is successfully published but when I go to view the code my Custom Properties have disappeared.  Is there a different process to connect to SAP Open Hub iwht the Azure data factory as there does not appear to be anything on the MS website to guide me.</p>
","<azure-data-factory>","2019-01-08 23:21:31","1018","0","2","54336838","<p>Did some work with a Microsoft person - the process we wanted was to use an OpenHub connection in the Data Factory. This link to the Microsoft Azure Data Factory forum has a document that talks about how to achieve this. 
<a href=""https://social.msdn.microsoft.com/Forums/azure/en-US/dd6a8974-bcb1-4890-bd83-15eedace06f7/sap-bw-no-table-in-connection-tab?forum=AzureDataFactory"" rel=""nofollow noreferrer"">DataFactory Forum</a></p>

<p>Unfortunately this process didn't work for me becasue our SAP Version is 4 when it should work with 7.3 13.</p>
"
"54098691","Number of Nodes in Azure Batch Pool when running executable from Azure Data Factory","<p>We have an ADF pipeline that runs 3 other pipelines which run the same executable using the same Azure Batch pool. Most of the time the executable runs successfully. However, there are instances where the pipeline hangs before it starts running the executable.</p>

<p>Note that all 3 pipelines are using the same Azure Batch pool. </p>

<ul>
<li><p>The executable does not have any parallelism, so does the number of nodes make a difference? Should there be at least 3 nodes?</p></li>
<li><p>Is there an issue with the pipelines using the same Azure Batch pool? Should they be using different pools?</p></li>
</ul>

<p>Thanks</p>

<p>Mehmet</p>
","<azure-data-factory><azure-batch>","2019-01-08 19:43:10","82","0","1","54101199","<p>I might try and attempt cater 2 questions from my <code>Batch</code> hat on: (I hope this gives you idea to put some debugging in place to identify in depth) </p>

<ul>
<li><p>The executable does not have any parallelism, so does the number of nodes make a difference? Should there be at least 3 nodes?</p>

<ul>
<li>No, number of nodes are not restrictive to any parallelism concept in Batch. AFAIK, and I am not sure from where 3 Nodes is mentioned but is it the case that with 3 nodes you have your scenario working?</li>
<li><strong>ErrorHandling</strong>:  As long as the conceptual code is well managed if there is any exception from the <code>BatchException</code> or <code>FailureInfo</code> with in BatchErrors will get pushed for the user. e.g. <a href=""https://learn.microsoft.com/en-us/azure/batch/batch-task-fail-event"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/batch/batch-task-fail-event</a> or <a href=""https://learn.microsoft.com/en-us/rest/api/batchservice/batch-status-and-error-codes"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/batchservice/batch-status-and-error-codes</a></li>
<li>From the nodes you can always get the service logs: <a href=""https://learn.microsoft.com/en-us/rest/api/batchservice/computenode/uploadbatchservicelogs"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/batchservice/computenode/uploadbatchservicelogs</a> </li>
<li>What is the node state when your parallel task is hanging?</li>
</ul></li>
<li><p>Is there an issue with the pipelines using the same Azure Batch pool? Should they be using different pools?</p>

<ul>
<li>No, AFAIK. I would recommend to detail your scenario if there is anything specific you are seeing.</li>
</ul></li>
</ul>

<p>Since its code node is trying to run, I would recommend setting debug point and checking service log to identify user code run behavior.</p>

<p>Thanks,</p>
"
"54084951","How to pass a route to Azure function (C#) http trigger in Data Factory pipeline?","<p>I need to pass header information in Azure Function activity in Data Factory.
As can be seen in the picture, the header is marked in red.</p>

<p>I need to change the following code to receive the header. I also need to capture the value of the header.</p>

<p>public static async Task Run([HttpTrigger(AuthorizationLevel.Function, ""get"", Route = ""{name}"" )] HttpRequestMessage req,
            string name,
            TraceWriter log,
            [Blob(""pk-api-test/{name}"", FileAccess.Read)] Stream myBlob)</p>

<p><a href=""https://i.stack.imgur.com/nTmff.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nTmff.png"" alt=""enter image description here""></a></p>
","<c#><azure><azure-functions><azure-data-factory>","2019-01-08 03:47:36","769","0","2","54085182","<p>Have a try at code below. Use <code>headers</code> to access headers info in http request.</p>

<pre><code>   public static async Task Run(
         [HttpTrigger(AuthorizationLevel.Function, ""get"", Route = null)]HttpRequestMessage req,
         [Blob(""pk-api-test/{headers.name}"", FileAccess.Read)]Stream myBlob, 
         IDictionary&lt;string, string&gt; headers,
         TraceWriter log)
   {
       string name = headers[""name""];
       //...
   }
</code></pre>
"
"54084951","How to pass a route to Azure function (C#) http trigger in Data Factory pipeline?","<p>I need to pass header information in Azure Function activity in Data Factory.
As can be seen in the picture, the header is marked in red.</p>

<p>I need to change the following code to receive the header. I also need to capture the value of the header.</p>

<p>public static async Task Run([HttpTrigger(AuthorizationLevel.Function, ""get"", Route = ""{name}"" )] HttpRequestMessage req,
            string name,
            TraceWriter log,
            [Blob(""pk-api-test/{name}"", FileAccess.Read)] Stream myBlob)</p>

<p><a href=""https://i.stack.imgur.com/nTmff.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nTmff.png"" alt=""enter image description here""></a></p>
","<c#><azure><azure-functions><azure-data-factory>","2019-01-08 03:47:36","769","0","2","54122150","<p>I solved it by using 'Route = TestFunction/{name}' in the code and in the Azure function settings in Data Factory, I used the Function Name = TestFunction/albany</p>
"
"54082369","Dynamically Append datetime to filename during copy activity or when specifying name in blob dataset","<p>I am saving a file to blob storage in Data factory V2, when I specify the location to save to I am calling the file (for example) file1 and it saves in blob as file1, no problem. But can I use the dynamic content feature to append the datetime to the filename so its something like file1_01-07-2019_14-30-00 ?(7th Jan 14:30:00 just in case its awkward to read). Alternatively, can I output the result (the filename) of the webhook activity to the next activity (the function)?</p>

<p>Thank you.</p>
","<blob><azure-data-factory><dynamic-content>","2019-01-07 21:49:36","13136","1","3","54131746","<p>You can add a dataset parameter such an WindowStartTime, which is in the format 2019-01-10T13:50:04.279Z. Then you would have something like below for the dynamic filename:
@concat('file1_', formatDateTime(dataset().WindowStartTime, 'MM-dd-yyyy_hh-mm-ss')).</p>

<p>To use in the copy activity you will also need to add a pipeline parameter.</p>
"
"54082369","Dynamically Append datetime to filename during copy activity or when specifying name in blob dataset","<p>I am saving a file to blob storage in Data factory V2, when I specify the location to save to I am calling the file (for example) file1 and it saves in blob as file1, no problem. But can I use the dynamic content feature to append the datetime to the filename so its something like file1_01-07-2019_14-30-00 ?(7th Jan 14:30:00 just in case its awkward to read). Alternatively, can I output the result (the filename) of the webhook activity to the next activity (the function)?</p>

<p>Thank you.</p>
","<blob><azure-data-factory><dynamic-content>","2019-01-07 21:49:36","13136","1","3","54196012","<p>Once you set up the copy activity and select you blob dataset as the sink, you need to put in a value for the WindowStartTime, this can either just be a timestamp e.g. 1900-01-01T13:00:00Z or you can put in a pipeline parameter into this. 
<a href=""https://i.stack.imgur.com/CoaCr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CoaCr.png"" alt=""enter image description here""></a></p>

<p>Having a parameter would maybe be more helpful if you're setting up a schedule trigger, as you will be able to input this WindowStartTime timestamp by when the trigger runs. For this you would use @trigger().scheduledTime as the value for the trigger parameter WindowStartTime.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#trigger-type-comparison"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#trigger-type-comparison</a></p>
"
"54082369","Dynamically Append datetime to filename during copy activity or when specifying name in blob dataset","<p>I am saving a file to blob storage in Data factory V2, when I specify the location to save to I am calling the file (for example) file1 and it saves in blob as file1, no problem. But can I use the dynamic content feature to append the datetime to the filename so its something like file1_01-07-2019_14-30-00 ?(7th Jan 14:30:00 just in case its awkward to read). Alternatively, can I output the result (the filename) of the webhook activity to the next activity (the function)?</p>

<p>Thank you.</p>
","<blob><azure-data-factory><dynamic-content>","2019-01-07 21:49:36","13136","1","3","54565273","<p>I couldn't get this to work without editing the copy pipeline JSON file directly (late 2018 - may not be needed anymore).  You need dynamic code in the copy pipeline JSON and settings defined in the dataset for setting filename parameters.</p>

<p>In the dataset define 'Parameters' for folder path and/or filename (click '+ New' and give them any name you like) e.g. sourceFolderPath, sourceFileName.</p>

<p>Then in dataset under 'Connection' include the following in the 'File path' definition:
@dataset().sourceFolderPath and @dataset().sourceFileName either side of the '/'
(see screenshot below)</p>

<p>In the copy pipeline click on 'Code' in the upper right corner of pipeline window and look for the following code under the 'blob' object you want defined by a dynamic filename - it the 'parameters' code isn't included add it to the JSON and click the 'Finish' button - this code may be needed in 'inputs', 'outputs' or both depending on the dynamic files you are referencing in your flow - below is an example where the output includes the date parameter in both folder path and file name (the date is set by a Trigger parameter):</p>

<pre><code>  ""inputs"": [
     {
        ""referenceName"": ""tmpDataForImportParticipants"",
        ""type"": ""DatasetReference""
     }
  ],
  ""outputs"": [
      {
         ""referenceName"": ""StgParticipants"",
         ""type"": ""DatasetReference"",
         ""parameters"": {
              ""sourceFolderPath"": {
                   ""value"": &lt;derived value of folder path&gt;,
                   ""type"": ""Expression""
               },
               ""sourceFileName"": {
                    ""value"": &lt;derived file name&gt;,
                    ""type"": ""Expression""
               }
          }
      }
  ]
</code></pre>

<p>Derived value of folder path may be something like the following - this results in a folder path of yyyy/mm/dd within specified blobContainer:</p>

<p>""blobContainer/@{formatDateTime(pipeline().parameters.windowStart,'yyyy')}/@{formatDateTime(pipeline().parameters.windowStart,'MM')}/@{formatDateTime(pipeline().parameters.windowStart,'dd')}""</p>

<p>or it could be hardcoded e.g. ""blobContainer/directoryPath"" - don't include '/' at start or end of definition</p>

<p>Derived file name could be something like the following:</p>

<p>""@concat(string(pipeline().parameters.'_',formatDateTime(dataset().WindowStartTime, 'MM-dd-yyyy_hh-mm-ss'))>,'.txt')""</p>

<p>You can include any parameter set by the Trigger e.g. an ID value, account name, etc. by including pipeline().parameters.</p>

<p><a href=""https://i.stack.imgur.com/3mQIi.png"" rel=""nofollow noreferrer"">Dynamic Dataset Parameters example</a></p>

<p><a href=""https://i.stack.imgur.com/UCslE.png"" rel=""nofollow noreferrer"">Dynamic Dataset Connection example</a></p>
"
"54082191","identifying the proper syntax for using HTTP dataset in ADFv2","<p>I am able to successfully use Postman to authenticate and subsequently get data housed within a sandbox but I cannot figure out how to specify the same data within ADFv2.</p>

<p>I'm expecting to retrieve, temporarily store and later use a bearer token that this API generates.  This token is then used in the second step that actually downloads the data I want in JSON format.</p>

<p>For the Authentication step, Postman generates code that looks this:</p>

<pre><code>POST /v1/oauth/token HTTP/1.1
Host: api.sandbox.COMPANY.com
Ocp-Apim-Subscription-Key: MYKEY
Content-Type: multipart/form-data; boundary=---- 
WebKitFormBoundaryALPHANUM
cache-control: no-cache
Postman-Token: MYTOKEN

Content-Disposition: form-data; name=""key""

MYKEY

Content-Disposition: form-data; name=""grant_type""

vapi_key
------WebKitFormBoundaryALPHANUM--
</code></pre>

<p>I've created a linked HTTP and REST connection in ADFv2 with the base URL of ""<a href=""https://api.sandbox.COMPANY.com"" rel=""nofollow noreferrer"">https://api.sandbox.COMPANY.com</a>"" and using no authentication.</p>

<p>I cannot figure out how to translate the functional Postman connection to a way that ADFv2 will work.  Thoughts?</p>
","<postman><api-design><azure-data-factory>","2019-01-07 21:32:40","67","0","1","54089597","<p>You could check this example.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-http#dataset-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-http#dataset-properties</a></p>
"
"54076004","Setting a Linked Service parameter at runtime in a Data Factory pipeline","<p>How do I pass in parameters at runtime to a Linked Service in Data Factory? </p>

<p>I have a parameter defined and consumed in my Azure Function linked service definition, but am unsure how I set that during invocation in my pipeline action. In this case I would like to set functionAlias in a trigger to set the Azure Function hostname. Currently an exception is returned on (blob create) trigger stating that functionAlias is not specified.</p>

<p>Pipeline</p>

<pre><code>        {
            ""name"": ""myActivity"",
            ""type"": ""AzureFunctionActivity"",
            ""typeProperties"": {
                ""functionName"": ""myFunctionName"",
                ""method"": ""POST""
            }
            ""linkedServiceName"": {
                ""referenceName"": ""myFunctionLinkedService"",
                ""type"": ""LinkedServiceReference""
            }
        }
</code></pre>

<p>Linked service</p>

<pre><code>{
    ""name"": ""myFunctionLinkedService"",
    ""type"": ""Microsoft.DataFactory/factories/linkedservices"",
    ""properties"": {
        ""type"": ""AzureFunction"",
        ""typeProperties"": {
            ""functionAppUrl"": ""https://@{linkedService().functionAlias}.azurewebsites.net"",
            ""encryptedCredential"": """"
        },
        ""parameters"": {
            ""functionAlias"": {
                ""type"": ""String""
            }
        }
    }
}
</code></pre>
","<azure><azure-data-factory>","2019-01-07 14:12:31","1382","1","2","54413320","<p>I tried to parameterize Azure Function Linked Service in the Data Factory UI and get the same error as same as you because the parameter input box did not show up ever.</p>

<p>Then I follow the official document to try to parameterize Azure SQL Database Linked Service.</p>

<p><a href=""https://i.stack.imgur.com/ZCTfu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZCTfu.png"" alt=""enter image description here""></a></p>

<p>Then I test it and the input box showed up just as I expected.</p>

<p><a href=""https://i.stack.imgur.com/OBBQK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OBBQK.png"" alt=""enter image description here""></a></p>

<p>I check the document and find the below statement.</p>

<p><a href=""https://i.stack.imgur.com/EnVQD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EnVQD.png"" alt=""enter image description here""></a></p>

<p>It seems that only data stores are supported by parameterize feature in ADF so far, however Azure Function is belong to <code>Compute</code>,not data stores.(Maybe it will be supported in the furture)</p>

<p><a href=""https://i.stack.imgur.com/pBtCH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pBtCH.png"" alt=""enter image description here""></a></p>

<p>You could submit <a href=""https://feedback.azure.com/forums/270578-data-factory"" rel=""nofollow noreferrer"">feedback</a> or contact ADF team directly to verify that.</p>
"
"54076004","Setting a Linked Service parameter at runtime in a Data Factory pipeline","<p>How do I pass in parameters at runtime to a Linked Service in Data Factory? </p>

<p>I have a parameter defined and consumed in my Azure Function linked service definition, but am unsure how I set that during invocation in my pipeline action. In this case I would like to set functionAlias in a trigger to set the Azure Function hostname. Currently an exception is returned on (blob create) trigger stating that functionAlias is not specified.</p>

<p>Pipeline</p>

<pre><code>        {
            ""name"": ""myActivity"",
            ""type"": ""AzureFunctionActivity"",
            ""typeProperties"": {
                ""functionName"": ""myFunctionName"",
                ""method"": ""POST""
            }
            ""linkedServiceName"": {
                ""referenceName"": ""myFunctionLinkedService"",
                ""type"": ""LinkedServiceReference""
            }
        }
</code></pre>

<p>Linked service</p>

<pre><code>{
    ""name"": ""myFunctionLinkedService"",
    ""type"": ""Microsoft.DataFactory/factories/linkedservices"",
    ""properties"": {
        ""type"": ""AzureFunction"",
        ""typeProperties"": {
            ""functionAppUrl"": ""https://@{linkedService().functionAlias}.azurewebsites.net"",
            ""encryptedCredential"": """"
        },
        ""parameters"": {
            ""functionAlias"": {
                ""type"": ""String""
            }
        }
    }
}
</code></pre>
","<azure><azure-data-factory>","2019-01-07 14:12:31","1382","1","2","68471300","<p>Just UI doesn't support it natively. But you can use advanced JSON. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services</a>
<a href=""https://i.stack.imgur.com/ONl5y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ONl5y.png"" alt=""enter image description here"" /></a></p>
"
"54069497","How to get blob name and use as parameter or variable for next activity in Data Factory V2","<p>I am running a Data Factory v2 pipeline, as part of the process it saves a file to Azure blob storage, and renames it using a powershell runbook that gets triggered by a web(hook) activity in the same pipeline. </p>

<p>I need to read the content of the file, and to do that I run a function as part of the pipeline. </p>

<p>How can I grab the filename in storage (I have a dataset for the storage), and use, or pass the name of that file to the next activity (the function) so it can read the file? </p>

<p>I have tried a metadata lookup and got the first part of the filename based on the name its saved as in storage before its renamed (it has the date appended to it), which is all I need, but how can I reference the output of that metadata lookup and pass onto the function activity? Can I use a 'like' in dynamic content for the function? I'd be happy to get the name of the file in storage based on a <code>'filename like'</code> </p>

<p>I'm open to any ideas. I don't necessarily need the whole name of the file, if I could do something such as a like or contains in ADF dynamic content I could use the output of the metadata lookup, I just don't know how, or if that is allowed or possible. Could I use <code>'startswith'</code>? </p>

<p>Thank you.</p>
","<parameters><blob><azure-data-factory><dynamic-content>","2019-01-07 06:33:54","1642","0","1","54412926","<p>This document shows how to use look up activity result in the next activity: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity#use-the-lookup-activity-result-in-a-subsequent-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity#use-the-lookup-activity-result-in-a-subsequent-activity</a> </p>

<p>You can reference the output in the header or payload body of web activity.</p>

<p>In your case, if you want to filter the filename list you may try filter activity after lookup activity: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-filter-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-filter-activity</a> </p>
"
"54047865","Reboot Node using VSTS task or ARM template?","<p>I have a data factory v2 with a batch account, pool and a single node. It seems the node has gotten stuck in ""waiting for start task state' and thus all my schedules stopped working.</p>

<p>I don't have the ability to reboot the node through the Azure Portal UI. Is there a way I can do it through a VSTS task or arm template?</p>
","<azure><azure-devops><azure-resource-manager><azure-data-factory><azure-batch>","2019-01-05 00:28:04","117","0","1","54048746","<p>You can restart a node only if it is in an idle or running state.</p>

<p>Get the information about the specified compute node.</p>

<pre><code>GET {batchUrl}/pools/{poolId}/nodes/{nodeId}?api-version=2018-12-01.8.0
</code></pre>

<p>You can try with the below REST URI to reboot/restart a node and see if it resolves the issue.</p>

<pre><code>POST {batchUrl}/pools/{poolId}/nodes/{nodeId}/reboot?api-version=2018-12-01.8.0
</code></pre>

<p>For more details, refer to <a href=""https://learn.microsoft.com/en-us/rest/api/batchservice/computenode/reboot"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/batchservice/computenode/reboot</a></p>
"
"54034373","ADF log analytics - how to correlate multiple pipelines","<p>I have been doing some experimentation and for an ADF log in Log Analytics the correlation ID appears to be a copy of the Pipeline RunId.  The TriggerId appears to be blank.</p>

<p>I have a pipelines that call other pipelines. </p>

<pre><code>ScheduledTrigger -&gt; Pipeline 1 -&gt; activity 1 execute pipeline(pipeline 2)
                               -&gt; activity 2 execute pipeline(pipeline 3)
</code></pre>

<p>I'd like a single ID that enables me to find all pipelines and activities that were run from the parent trigger.  I thought that would be in the correlation ID or the Trigger ID.</p>

<p>I know I can solve this by passing the trigger ID from the @trigger() object as a parameter through the stack of pipelines but wondered if I am missing some trick to get this automatically?</p>
","<azure-data-factory>","2019-01-04 06:59:24","517","0","1","55737901","<p>Just closing this - I did what I anticipated I needed to do.
The top most parent copies the pipeline's RunId and passes it as a parameter to all child pipelines.  All child pipelines pass on this same RunId to their children (not their own RunID).  </p>

<p>When I call stored procedures or external resources (like web or function), I pass this same RunID as a required parameter.  This way there is a consistent correlation ID available to all children whether they be pipelines, SPs or external calls.  </p>

<p>I maintain the name ""RunID"" all the way through to log analytics from any component and therefore can easily get a full picture of what went on.  Whiel this is a bit of overhead, it has saved hours of debugging when something goes wrong across multiple ADF, functions, logic apps, web calls, SP calls, etc because they are all tied together by this RunID.</p>

<p>Hope this helps.
Mark.</p>
"
"54017909","Azure Data Factory V2 : Error using the REST connector - ""Failed to get schema from""","<p>I am doing a POC to call restFul API using ADF CopyData. </p>

<p>The restFul API details are available on this link </p>

<p><a href=""http://www.groupkt.com/post/f2129b88/free-restful-web-services-to-consume-and-test.htm"" rel=""nofollow noreferrer"">http://www.groupkt.com/post/f2129b88/free-restful-web-services-to-consume-and-test.htm</a></p>

<p>from this page, i want to rest below service
<a href=""http://services.groupkt.com/country/get/iso3code/IND"" rel=""nofollow noreferrer"">http://services.groupkt.com/country/get/iso3code/IND</a></p>

<p>The linked service to base URL is successfully created in ADF 
<a href=""http://services.groupkt.com/country/get"" rel=""nofollow noreferrer"">http://services.groupkt.com/country/get</a></p>

<p>in the next steps when I try to lookup data or do next steps i get error 
Error using the REST connector - ""Failed to get schema from""</p>

<p>Let me know if you have come across this issue in ADF V2 </p>
","<restful-url><azure-data-factory>","2019-01-03 07:17:39","438","0","1","54033698","<p>I was able to make this POC work by changing the relative URL which i was initially giving ""/iso3code/IND"" </p>

<p>Below is how the URLs should be</p>

<p><strong>Absolute URL</strong> should be  <a href=""http://services.groupkt.com/country/get"" rel=""nofollow noreferrer"">http://services.groupkt.com/country/get</a> : this is used for creating the Linked Service of the CopyData </p>

<p><strong>Relative URL</strong> should be   <a href=""http://services.groupkt.com/country/get/iso3code/IND"" rel=""nofollow noreferrer"">http://services.groupkt.com/country/get/iso3code/IND</a>
This is used for getting the mapping details of the Restful service </p>
"
"54006355","Slow performance when copying from HTTP source to blob sink","<p>I use a copy activity to call an HTTP API and store the json response as a file in Azure blob storage. The copy activity is executed in a ForEach loop and each activity run takes 16 seconds, but when I look at the run details it says the copy duration is only 3 seconds. Then why does the activity take 16 seconds to complete? The source dataset is an Http File with an HttpServer linked service and the sink dataset is a blob storage json file. Both the source and sink datasets are configured with Binary Copy and it's a GET request to an HTTPS URL with anonymous authentication.</p>

<p>I would like to speed up this acticity since it is run multiple times inside the ForEach loop. Is there some way to improve the performance?</p>
","<azure-data-factory>","2019-01-02 12:24:45","229","0","1","54007354","<p>There is always a few seconds of overhead when starting an activity. Also consider that the http server might be also responsible for some of those seconds you are seeing there.</p>

<p>If you are using a for each loop and want to speed up the process, you can uncheck the Sequential check in the settings tab of the foreach activity.</p>

<p>Hope this helped!</p>
"
"54004443","Issue while reading a from container how do i put the data in sql table with this issue file","<p>Error found when processing 'Csv/Tsv Format Text' source 'xyz_File' with row number 7: found more columns than expected column count: 65.. Activity ID:c310daeb-7c87-4176-95c3-47bab56a9b1b</p>

<p>I have tried with ""\u0001"" separator and it's providing only a single column.
Like Below.
col1,col2,col3......Col150</p>

<p>It should appear like.
col1,col2,col3......Col150==>>col1 Col2 col3</p>
","<azure><azure-data-factory>","2019-01-02 10:10:52","239","0","1","54007375","<p>Try defining the schemas in the copy activity. </p>

<p>Also next time you ask try to explain a bit more your case so we can help you in a better way.</p>

<p>Hope this helped!</p>
"
"54003571","SQL Server On Premise to Azure Data Warehouse: Copy Multiple tables","<p>We want to copy 1000+ tables from SQL Server on premise to an Azure Data Warehouse.
Does Azure Data factory have a quick operation to conduct both initial and Daily incremental loading from SQL Server OLTP to Azure Data warehouse? The loading can be a quick 1:1 copy, no transformations needed. We will conduct transformation and denormalization on the Azure DW staging side (roundrobin tables).</p>

<p>We are refraining from SSIS, since the development time is very high to create 1000 packages.</p>

<p>Does Azure Data Warehouse allow:</p>

<ol>
<li><p>Replication from SQL Server on premise to Azure Data Warehouse?</p></li>
<li><p>Log Shipping from SQL Server on premise to Azure Data Warehouse?</p></li>
<li><p>AlwaysOn AG from SQL Server on premise to Azure Data Warehouse?</p></li>
<li><p>Backup (Full, Differential, Transaction log) from SQL Server on premise to Azure Data Warehouse?</p></li>
<li><p>Azure Data Factory from SQL Server on premise to Azure Data Warehouse?</p></li>
</ol>

<p>Seeking straight copies, with focus on quick and very low development time.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server</a></p>
","<sql-server><azure><sql-server-2016><azure-data-factory><azure-synapse>","2019-01-02 09:00:01","772","-1","1","54004477","<p>Azure Data Factory offers the following benefits for loading data into Azure SQL Data Warehouse:</p>

<p><strong>1.</strong> Easy to set up: An intuitive 5-step wizard with no scripting required.
Rich data store support: Built-in support for a rich set of on-premises and cloud-based data stores. For a detailed list, see the table of Supported data stores.</p>

<p><strong>2.</strong> Secure and compliant: Data is transferred over HTTPS or ExpressRoute. The global service presence ensures that your data never leaves the geographical boundary.</p>

<p><strong>3.</strong> Unparalleled performance by using PolyBase: Polybase is the most efficient way to move data into Azure SQL Data Warehouse. Use the staging blob feature to achieve high load speeds from all types of data stores, including Azure Blob storage and Data Lake Store. (Polybase supports Azure Blob storage and Azure Data Lake Store by default.) For details, see Copy activity performance.</p>

<p>You don't want to use SSIS, and i think you also found that Microsoft provides the tutorial about <a href=""https://learn.microsoft.com/en-us/azure/data-factory/load-azure-sql-data-warehouse"" rel=""nofollow noreferrer"">Load data into Azure SQL Data Warehouse by using Azure Data Factory</a>.</p>

<p>Maybe this couldn't answer your five questions directly, still hope this can helps you.</p>
"
"53985412","Azure Data Factory: Output Copied File and Folder Information from Copy Activity","<p>I'm using the Self-Hosted Integration Runtime in Azure Data Factory to copy data from an On-Premises source (normal file system) to an Azure Blob Storage destination. After being transferred, I want to process the files automatically by attaching a Notebook running on a Databricks cluster. The pipeline works fine, but my question concerns the output of the Copy Activity.</p>

<p><strong>Is there a way to get information on the transferred files and folders for each run?</strong> I would pass this information as parameters to the notebook.</p>

<p>Looking at the documentation, it seems only aggregated information is available:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview</a></p>

<p>Which kind of makes sense, if you transfer huge amounts of files. If not possible, I guess an alternate approach would be to just leave the copy process to itself, and create another pipeline based on storage account events? Or maybe store the new file and folder information for each run  in a fixed text file, transfer it also, and read it in the notebook?</p>
","<azure><copy><azure-data-factory>","2018-12-31 08:51:39","2597","3","2","54019563","<p>If you want to get information of files or directories beeing read from data factory this can be done using the Get Metadata Activity, see the following <a href=""https://stackoverflow.com/questions/49923091/get-metadata-activity-adf-v2/50199878#50199878"">answer</a> for an example.</p>

<p>Another approach to detect new files in your notebook would be to use structured streaming with file sources. This works pretty well and you just call the notebook activity after the copy activity. </p>

<p>For this you define a streaming input data frame:</p>

<pre><code>streamingInputDF = (
   spark
     .readStream                     
     .schema(pqtSchema)               
     .parquet(inputPath) 
 )
</code></pre>

<p>with <em>inputPath</em> pointing to the input dir in the Blob Storage. Supported file formats are text, csv, json, orc, parquet, so it depends on your concrete scenario if this will work for you.</p>

<p>Important is that on the target you use the trigger once option, so the notebook does not need to run pemananently, e. g.:</p>

<pre><code>streamingOutputDF \
    .repartition(1) \
    .writeStream \
    .format(""parquet"") \
    .partitionBy('Id') \
    .option(""checkpointLocation"", adlpath +  ""spark/checkpointlocation/data/trusted/sensorreadingsdelta"") \
    .option(""path"", targetPath + ""delta"") \
    .trigger(once=True) \
    .start()
</code></pre>

<p>Another approach could be using Azure Queue Storage (AQS), see the following <a href=""https://docs.azuredatabricks.net/spark/latest/structured-streaming/aqs.html#use-the-abs-aqs-file-source"" rel=""nofollow noreferrer"">documentation</a>.</p>
"
"53985412","Azure Data Factory: Output Copied File and Folder Information from Copy Activity","<p>I'm using the Self-Hosted Integration Runtime in Azure Data Factory to copy data from an On-Premises source (normal file system) to an Azure Blob Storage destination. After being transferred, I want to process the files automatically by attaching a Notebook running on a Databricks cluster. The pipeline works fine, but my question concerns the output of the Copy Activity.</p>

<p><strong>Is there a way to get information on the transferred files and folders for each run?</strong> I would pass this information as parameters to the notebook.</p>

<p>Looking at the documentation, it seems only aggregated information is available:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview</a></p>

<p>Which kind of makes sense, if you transfer huge amounts of files. If not possible, I guess an alternate approach would be to just leave the copy process to itself, and create another pipeline based on storage account events? Or maybe store the new file and folder information for each run  in a fixed text file, transfer it also, and read it in the notebook?</p>
","<azure><copy><azure-data-factory>","2018-12-31 08:51:39","2597","3","2","54030603","<p>The solution was actually quite simple in this case. I just created another pipeline in Azure Data Factory, which was triggered by a <em>Blob Created</em> event, and the <strong>folder</strong> and <strong>filename</strong> passed as parameters to my notebook. Seems to work well, and a minimal amount of configuration or code required. Basic filtering can be done with the event, and the rest is up to the notebook.</p>

<p>For anyone else stumbling across this scenario, details below:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger</a></p>
"
"53931992","Data Lake -U SQL we need to create out Pramater( Outout File Name) with expression of UTC time Error Out","<p>WE have  requirment to  gnerate output file with filename
for example ""filename_utctime""</p>

<p>we run USQL from ADF</p>

<h2>below is USQL</h2>

<pre><code>    @Orders=
EXTRACT
 OrderID int,
    CustomerID int,
    OrderDetailID int,
    OrderTotal double,
    OrderDate DateTime

 FROM @IN
      USING Extractors.Csv();

@GetLatest=
SELECT 
      OrderID,
    CustomerID,
    OrderDetailID,
    OrderTotal,
    OrderDate,
    ROW_NUMBER() OVER(PARTITION BY OrderID,CustomerID,OrderDetailID ORDER BY OrderDate DESC) AS Rid FROM @Orders;
@FinalOrder=
SELECT 
      OrderID,
    CustomerID,
    OrderDetailID,
    OrderTotal,
    OrderDate
    FROM @GetLatest WHERE Rid==1;
OUTPUT  @FinalOrder 
    TO @OUT
    USING Outputters.Csv();
</code></pre>

<hr>

<p>Now in ADF, 
i have Pass two Paramters</p>

<pre><code>  ""parameters"": {
                    ""IN"": ""/OrderDump/Orders.csv"",
                    ""OUT"": {
                        **""value"": ""/OrderDump/formatDateTime(utcnow(), 'yyyyMMdd')"",**
                        ""type"": ""Expression""
                    }
                }
</code></pre>

<p>But it's not working and give errors. i want to when output file gnerated with current date time</p>

<pre><code>{ ""errorCode"": ""2703"", 
  ""message"": ""Error Id: VertexFailedFast, Error Message: Vertex failed with a fail-fast error. "", 
""failureType"": ""UserError"", 
""target"": ""Delata"" }
</code></pre>
","<azure-data-factory><azure-data-lake><u-sql>","2018-12-26 12:15:28","244","1","1","53933007","<p>In your ADF you should use @concat function. 
So your expression should look like </p>

<pre><code>@concat('/OrderDump/',formatDateTime(utcnow(),'yyyyMMdd'),'.csv')
</code></pre>
"
"53891234","Are there release notes for ADFv2?","<p>I noticed new options in an existing pipeline I built which got me wondering what it was and what else was added that I didn't notice.  Most other services have release notes, surely this exists for ADFv2?  How do I get on the list?</p>
","<azure><azure-data-factory>","2018-12-21 22:01:14","559","0","2","53908225","<p>Closest thing to a patch notes we have is the roadmap: <a href=""https://azure.microsoft.com/en-us/updates/?tag=data-factory"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/updates/?tag=data-factory</a></p>

<p>Hope this helped!</p>
"
"53891234","Are there release notes for ADFv2?","<p>I noticed new options in an existing pipeline I built which got me wondering what it was and what else was added that I didn't notice.  Most other services have release notes, surely this exists for ADFv2?  How do I get on the list?</p>
","<azure><azure-data-factory>","2018-12-21 22:01:14","559","0","2","53909392","<p>You can find the ADF V2 release notes from the ADF portal <a href=""https://adf.azure.com/datafactories"" rel=""nofollow noreferrer"">https://adf.azure.com/datafactories</a></p>

<p>at the top right part of the page.
<a href=""https://i.stack.imgur.com/qq9vo.png"" rel=""nofollow noreferrer"">sample release note screenshot</a></p>
"
"53876227","ADFv2 trouble with column mapping (reposting)","<p>I have a source .csv with 21 columns and a destination table with 25 columns.</p>

<p>Not ALL columns within the source have a home in the destination table and not all columns in the destination table come from the source.</p>

<p>I cannot get my CopyData task to let me pick and choose how I want the mapping to be. The only way I can get it to work so far is to load the source data to a ""holding"" table that has a 1:1 mapping and then execute a stored procedure to insert data from that table into the final destination.</p>

<p>I've tried altering the schemas on both the source and destination to match but it still errors out because the ACTUAL source has more columns than the destination or vice versa.</p>

<p>This can't possibly be the most efficient way to accomplish this but I'm at a loss as to how to make it work.</p>

<p>Yes I have tried the user interface, yes I have tried the column schemas, no I can't modify the source file and shouldn't need to.</p>

<p>The error code that is returned is some variation on:</p>

<pre><code>""errorCode"": ""2200"",
    ""message"": ""ErrorCode=UserErrorInvalidColumnMappingColumnCountMismatch,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Invalid column mapping provided to copy activity: '{LONG LIST OF COLUMN MAPPING HERE}', Detailed message: Different column count between target structure and column mapping. Target column count:25, Column mapping count:16. Check column mapping in table definition.,Source=Microsoft.DataTransfer.Common,'"",
    ""failureType"": ""UserError"",
    ""target"": ""LoadPrimaryOwner""
</code></pre>
","<azure><azure-data-factory>","2018-12-20 21:22:20","9379","4","1","53878778","<p>Tim F. Please view the statements in this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-schema-and-type-mapping#explicit-column-mapping"" rel=""nofollow noreferrer"">Schema mapping in copy activity</a>:</p>

<blockquote>
  <p>Column mapping supports mapping all or subset of columns in the source
  dataset ""structure"" to all columns in the sink dataset ""structure"".</p>
  
  <p>The following are error conditions that result in an exception:</p>
  
  <p>1.Source data store query result does not have a column name that is specified in the input dataset ""structure"" section.</p>
  
  <p>2.Sink data store (if with pre-defined schema) does not have a column name that is specified in the output dataset ""structure"" section.</p>
  
  <p>3.Either fewer columns or more columns in the ""structure"" of sink dataset than specified in the mapping.</p>
  
  <p>4.Duplicate mapping.</p>
</blockquote>

<p>So,you could know that all the columns in the sink dataset need to be mapped. Since you can't change the destination,maybe you don't have to struggle in an unsupported feature.</p>

<p>Of course ,you could use <code>stored procedure</code> mentioned in your description.That's a perfect workaround and not very troublesome. About the using details, you could refer to my previous cases:</p>

<p>1.<a href=""https://stackoverflow.com/questions/52553365/azure-data-factory-activity-copy-evaluate-column-in-sink-table-with-pipeline/52585664#52585664"">Azure Data Factory activity copy: Evaluate column in sink table with @pipeline().TriggerTime</a></p>

<p>2.<a href=""https://stackoverflow.com/questions/51412683/azure-data-factory-copy-activity-failed-mapping-strings-from-csv-to-azure-sql/51419543#51419543"">Azure Data factory copy activity failed mapping strings (from csv) to Azure SQL table sink uniqueidentifier field</a></p>

<p>In addition, if you really don't want avoid above solution,you could submit <a href=""https://feedback.azure.com/forums/270578-data-factory"" rel=""nofollow noreferrer"">feedback</a> to ADF team about your desired feature.</p>
"
"53859600","Handling Bearer Tokens in Azure Pipeline for HTTP Objects","<p>So in Azure Data Factory, for a pipeline, I had an HTTP object set up for copying data from an API, it was using a basic Password and Username. Now the API uses a bearer token to authorize calls. I've been able to code up a solution in Python, but I really don't know how to get Azure to handle this authentication process, in the Copy step.</p>

<p>Is there a way to call for the bearer token earlier and then pass it as part of the HTTP link service password?</p>

<p>Python Script:</p>

<pre><code>import http.client

conn = http.client.HTTPSConnection(""www.url.com"")

headers = {
    'authorization': ""Basic [removed]"",
    'cache-control': ""no-cache"",
    }
conn.request(""GET"", ""/v1/oauth2/accesstoken?grant_type=client_credentials"", headers=headers)
res = conn.getresponse()
data = res.read()

import json
datajson = json.loads(data.decode(""utf-8""))
headers = {
    'authorization': ""Bearer "" + datajson[""access_token""],
    'cache-control': ""no-cache"",
    }

conn.request(""GET"", ""/data?data-date=2018-12-09"", headers=headers)

res = conn.getresponse()
data = res.read()

print(data.decode(""utf-8""))
</code></pre>
","<azure><azure-data-factory>","2018-12-19 21:46:31","8397","4","2","53863457","<p>Unfortunately, according to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-http"" rel=""noreferrer"">Copy data from an HTTP endpoint by using Azure Data Factory</a>, the only supported authentication methods are: <strong>Anonymous</strong>, <strong>Basic</strong>, <strong>Digest</strong>, <strong>Windows</strong>, or <strong>ClientCertificate</strong>.</p>

<p>But, you might be able to do a workaround by using the <code>additionalHeaders</code> of the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-http#dataset-properties"" rel=""noreferrer"">Dataset's properties</a> to pass the bearer token to the <strong>HTTP</strong> endpoint.</p>

<p>To get the token (and even you might be able to get data this way), you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""noreferrer"">Web activity in Azure Data Factory</a> to perform the <strong>HTTP</strong> requests.</p>

<p>Hope it helps!</p>
"
"53859600","Handling Bearer Tokens in Azure Pipeline for HTTP Objects","<p>So in Azure Data Factory, for a pipeline, I had an HTTP object set up for copying data from an API, it was using a basic Password and Username. Now the API uses a bearer token to authorize calls. I've been able to code up a solution in Python, but I really don't know how to get Azure to handle this authentication process, in the Copy step.</p>

<p>Is there a way to call for the bearer token earlier and then pass it as part of the HTTP link service password?</p>

<p>Python Script:</p>

<pre><code>import http.client

conn = http.client.HTTPSConnection(""www.url.com"")

headers = {
    'authorization': ""Basic [removed]"",
    'cache-control': ""no-cache"",
    }
conn.request(""GET"", ""/v1/oauth2/accesstoken?grant_type=client_credentials"", headers=headers)
res = conn.getresponse()
data = res.read()

import json
datajson = json.loads(data.decode(""utf-8""))
headers = {
    'authorization': ""Bearer "" + datajson[""access_token""],
    'cache-control': ""no-cache"",
    }

conn.request(""GET"", ""/data?data-date=2018-12-09"", headers=headers)

res = conn.getresponse()
data = res.read()

print(data.decode(""utf-8""))
</code></pre>
","<azure><azure-data-factory>","2018-12-19 21:46:31","8397","4","2","59175122","<p>I added a header in the UI and it works.
<code>NAME: Authorization</code>, <code>VALUE: Bearer [my_token]</code>. Generated this code:</p>

<pre><code>{
    ""name"": ""PostToSlack"",
    ""type"": ""WebActivity"",
    ""dependsOn"": [],
    ""policy"": {
        ""timeout"": ""7.00:00:00"",
        ""retry"": 0,
        ""retryIntervalInSeconds"": 30,
        ""secureOutput"": false,
        ""secureInput"": false
    },
    ""userProperties"": [],
    ""typeProperties"": {
        ""url"": ""https://slack.com/api/chat.postMessage"",
        ""method"": ""POST"",
        ""headers"": {
            ""Authorization"": ""Bearer my_token""
        },
        ""body"": {
            ""channel"": ""#random"",
            ""as_user"": ""True"",
            ""text"": ""Hi from ADF!""
        },
        ""linkedServices"": [],
        ""datasets"": []
    }
}
</code></pre>

<p>From UI:</p>

<p><a href=""https://i.stack.imgur.com/MhsZk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MhsZk.png"" alt=""enter image description here""></a></p>
"
"53855583","How to delete data from table cache using data factory?","<p>I have a data factory pipeline that is taking data from SQL Server and copying it to table storage:</p>

<pre><code>Azure Sql Server View --&gt; Table Storage Cache
</code></pre>

<p>This step is doing a <code>REPLACE</code> of the specific row based on a <code>guid</code> from the SQL Server view. </p>

<p>If a record is deleted from the source view, how do we delete that same record from table storage?</p>
","<sql><sql-server><azure><azure-table-storage><azure-data-factory>","2018-12-19 16:40:07","864","1","2","53864026","<p>During the Copy Active, we couldn't do operation to the table data. Data Factory does not support this operation.</p>

<p>I have created a data pipeline to copy data from my SQL Server to the table storage for the test. Before your Copy Active begin, you can delete or insert your record as you want. And you can preview the data in Data Factory. Once the Copy Active is published and finished, it means that your data have been copied to the table storage. </p>

<p>If your really need to delete the same record in Table Storage, one of the ways is that you can login you Azure portal and using the ""Storage Explorer(preview)"" to delete the record:
<a href=""https://i.stack.imgur.com/TS41D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TS41D.png"" alt=""enter image description here""></a></p>

<p>Hope this helps.</p>
"
"53855583","How to delete data from table cache using data factory?","<p>I have a data factory pipeline that is taking data from SQL Server and copying it to table storage:</p>

<pre><code>Azure Sql Server View --&gt; Table Storage Cache
</code></pre>

<p>This step is doing a <code>REPLACE</code> of the specific row based on a <code>guid</code> from the SQL Server view. </p>

<p>If a record is deleted from the source view, how do we delete that same record from table storage?</p>
","<sql><sql-server><azure><azure-table-storage><azure-data-factory>","2018-12-19 16:40:07","864","1","2","59432872","<p>Here is a hint -- some code I have run in production for years... the idea is to have the source use a timestamp (or simpler flag) on deleted records and then to have your MERGE respect it.</p>

<pre><code>CREATE PROCEDURE [DataFactory].[MergeCompany]
(
  @Company [DataFactory].[CompanyType] READONLY
)
AS
BEGIN

  MERGE INTO [Company] AS TGT
  USING @Company As SRC
  ON TGT.pk = SRC.pk
  --
  WHEN MATCHED AND SRC.aud_delete_date IS NOT NULL
  THEN DELETE
  --
  WHEN MATCHED AND SRC.aud_delete_date IS NULL
  THEN UPDATE SET
     TGT.comp_number     = SRC.comp_number                           
    ,TGT.comp_name       = SRC.comp_name                                
    ,TGT.aud_update_date = SRC.aud_update_date    
    ,TGT.aud_create_date = SRC.aud_create_date                            
  --
  WHEN NOT MATCHED BY TARGET 
  THEN INSERT(
     pk
    ,comp_number    
    ,comp_name      
    ,aud_update_date
    ,aud_create_date                            
  ) 
  VALUES(
     SRC.pk
    ,SRC.comp_number    
    ,SRC.comp_name      
    ,SRC.aud_update_date
    ,SRC.aud_create_date                            
  )
  ; -- Required semicolon

END
GO
</code></pre>
"
"53828830","Error when using CRM as source dataset in ADF v2","<p>I am currently working on an adf v2 project. And on one of my pipelines, I am running a copy activity that retrieves data from an Odata source (connecting to a CRM instance) and transfers it to an Azure SQL Database. When I preview the data in the source, it successfully retrieves all of the data, but when I run the activity it returns an error like this: <em>""Failure happened on 'Source' side. 'Type=System.Collections.Generic.KeyNotFoundException,Message=The given key was not present in the dictionary.,Source=mscorlib adfv2 error""</em> </p>

<p>Is this an ongoing issue with adf v2 or there is actually a missing attribute in the crm source side ? </p>

<p>Thanks for answering everyone</p>
","<azure><dynamics-crm><odata><azure-data-factory>","2018-12-18 08:16:40","204","0","1","53852612","<p>Any reason why you didn't choose the Dynamics CRM connector over OData? The former is more recommended in ADF.</p>
"
"53826927","Azure Data Factory v2 pipeline double quotes","<p>My source file has nvarchar and numeric columns. </p>

<p>Numeric column has thousand separator, do identify the value comes with double quotes. When i use the quoteChar (""\"""") in file format the numeric value works fine. </p>

<p>Same time the nvarchar column (Name) has multiple double quotes between the data, if i use the quoteChar the values are split into further more columns based on the number of double quotes. </p>

<p>Is there any fix/solution for this?</p>
","<azure><escaping><pipeline><azure-data-factory><double-quotes>","2018-12-18 05:33:54","2503","0","2","53829434","<p>Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#text-format"" rel=""nofollow noreferrer"">properties</a> in Text format, only one character is allowed. So you can't separate the different data types by different delimiter.</p>

<p>You could try to use <code>|</code> column delimiter if your nvarchar doesn't have <code>|</code> character. Or maybe you have to parse your source file to remove the thousand separator in an <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-azure-function-activity"" rel=""nofollow noreferrer"">Azure Function activity</a> before the transfer. Then it could be identified by copy activity in adf. </p>
"
"53826927","Azure Data Factory v2 pipeline double quotes","<p>My source file has nvarchar and numeric columns. </p>

<p>Numeric column has thousand separator, do identify the value comes with double quotes. When i use the quoteChar (""\"""") in file format the numeric value works fine. </p>

<p>Same time the nvarchar column (Name) has multiple double quotes between the data, if i use the quoteChar the values are split into further more columns based on the number of double quotes. </p>

<p>Is there any fix/solution for this?</p>
","<azure><escaping><pipeline><azure-data-factory><double-quotes>","2018-12-18 05:33:54","2503","0","2","70808143","<p>ADF parser fails while reading text that is encapsulated with double quotes and also have a comma within the text, like &quot;Hello, World&quot;. To make it work, set Escape Character and Quote Character properties value to double quote. This will save the whole text with double quotes into destination.</p>
"
"53820439","Azure Data Factory - AWS Oracle Database to Azure SQL Server","<p>Is it possible to connect ADF to an Oracle database on AWS as source and migrate data to an Azure SQL Server?</p>

<p>I've made some attempties and the result was always timeout. </p>

<p>The goal was achieved using an Integration Runtime, but didn't wanted to use it. I'd like a direct connection.</p>
","<azure><azure-data-factory>","2018-12-17 17:40:50","645","0","1","53828191","<blockquote>
  <p>I've made some attempties and the result was always timeout.</p>
</blockquote>

<p>Hi,Vinicius. Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-oracle"" rel=""nofollow noreferrer"">Oracle connector</a>,no more special properties you need to configure except host,user,password properties etc for Oracle database on AWS. You could check the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-oracle#supported-capabilities"" rel=""nofollow noreferrer"">supported versions</a> of an Oracle database.</p>

<p>If you still have timeout issue, you could commit <a href=""https://feedback.azure.com/forums/270578-data-factory"" rel=""nofollow noreferrer"">feedback</a> to azure data factory to find an official statement.</p>

<p>Since you want to avoid IR, maybe you could refer to below solutions:</p>

<p>1.Try export data to <code>AWS S3</code> and ADF supports <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-amazon-simple-storage-service"" rel=""nofollow noreferrer"">AWS S3 connector</a> as source.</p>

<p>2.Try to use <a href=""https://community.hitachivantara.com/docs/DOC-1009855-data-integration-kettle"" rel=""nofollow noreferrer"">Data Integration - Kettle</a> tool to transfer data by jdbc driver.</p>
"
"53816347","Check Stored procedure has retured a value","<p>I am a newbie to Datafctory. As part of my pipeline, I execute an sp to fetch the next record to process using Lookup and then use the returned value in a Set Variable. </p>

<p>If the SP returns noting then the Set Variable fails with the following error </p>

<p>Activity SetBatchId failed: The expression 'activity('usp_get_next_archive_batch').output.firstRow.id' cannot be evaluated because property 'firstRow' doesn't exist, available properties are 'effectiveIntegrationRuntime'.</p>

<p>Is there a way in DF to check the property exists before using it</p>

<p>thanks</p>
","<azure-data-factory>","2018-12-17 13:34:28","4235","2","2","53836689","<p>Please add a question mark after ‘output’. Means ‘output?.firstRow’.  </p>

<p>See also this post. 
<a href=""https://stackoverflow.com/questions/51222698/azure-data-factory-for-each-item-value-does-not-exist-for-a-particular-attrib?rq=1"">Azure Data Factory: For each item() value does not exist for a particular attribute</a></p>
"
"53816347","Check Stored procedure has retured a value","<p>I am a newbie to Datafctory. As part of my pipeline, I execute an sp to fetch the next record to process using Lookup and then use the returned value in a Set Variable. </p>

<p>If the SP returns noting then the Set Variable fails with the following error </p>

<p>Activity SetBatchId failed: The expression 'activity('usp_get_next_archive_batch').output.firstRow.id' cannot be evaluated because property 'firstRow' doesn't exist, available properties are 'effectiveIntegrationRuntime'.</p>

<p>Is there a way in DF to check the property exists before using it</p>

<p>thanks</p>
","<azure-data-factory>","2018-12-17 13:34:28","4235","2","2","67186364","<p>The expression should be 'activity('usp_get_next_archive_batch').output['firstRow'].['id']</p>
"
"53808469","Best method to transfer and transfrom large amount of data from a SQL Server to an Azure SQL Server. Azure Data Factory, HDInsight, etc","<p>I like to find the best methods of transferring 20 GB of SQL data from a SQL Server database installed on a customer onsite server, Client, to our Azure SQL Server, Source, on an S4 with 200 <a href=""https://learn.microsoft.com/en-us/azure/sql-database/sql-database-service-tiers-dtu"" rel=""nofollow noreferrer"">DTUs</a> performance for $320 a month. When doing an initial setup, we set up an Azure Data Factory that copies over the 20 GB  via multiple table copies, e.g., Client Table A's content to Source Table A, Client Table B's content to Source Table B, etc. Then we run many Extractors store procedures that insert into Stage tables the data from the Source tables by joining these Source Table together, e.g., Source A join to Source B. After that is incremental copies, but the initial setup do take forever.</p>

<p>Currently the copying time on an S4 is around 12 hours with the extracting time to be 4 hours. Increasing the performance tier to an S9 of 1600 DTUs for $2400 a month will decrease time to 6 hours with the extracting time to be 2 hours, but that bring with it the higher cost. </p>

<p>I was wondering if there was other Azure methods. Does setting up an HDInsight cluster with Hadoop or Spark be more efficient for cost compare to scaling up the Azure SQL DB to an S9 and more? An S9 of $2400 a month of 31 days is $3.28 an hour. <a href=""https://azure.microsoft.com/en-us/pricing/details/hdinsight/"" rel=""nofollow noreferrer"">Azure HDInsight Clusters of Memorized Optimized Nodes of a D14 v2 instance</a> is $1.496 per hour so it would be cheaper than an S9. However, how does it compare in terms of performance. Would the copying process be quicker or will the extraction process be quicker? </p>

<p>I am not used to Big Data methods yet. Thank you for all the help.</p>
","<apache-spark><hadoop><azure-sql-database><azure-hdinsight><azure-data-factory>","2018-12-17 02:59:25","1945","0","1","53817929","<p>Azure Data Factory Copy Activity delivers a first-class secure, reliable, and high-performance data loading solution. It enables you to copy tens of terabytes of data every day across a rich variety of cloud and on-premises data stores.Copy Activity offers a highly optimized data loading experience that is easy to configure and set up.</p>

<p>You can see the performance reference table about Copy Activity:<a href=""https://i.stack.imgur.com/ZvgsZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZvgsZ.png"" alt=""enter image description here""></a>
The table shows the copy throughput number in MBps for the given source and sink pairs in a single copy activity run based on in-house testing.</p>

<p>If you want the data could be transfered quicker by using Azure Data Factory Copy Activity, Azure provides three ways to achieve higher throughput:</p>

<ol>
<li><p>Data integration units. A Data Integration Unit (DIU) (formerly known as Cloud Data Movement Unit or DMU) is a measure that represents the power (a combination of CPU, memory, and network resource allocation) of a single unit in Data Factory. You can achieve higher throughput by using more Data Integration Units (DIU).You are charged based on the total time of the copy operation. The total duration you are billed for data movement is the sum of duration across DIUs.</p></li>
<li><p>Parallel Copy. We can use the parallelCopies property to indicate the parallelism that you want Copy Activity to use.For each Copy Activity run, Data Factory determines the number of parallel copies to use to copy data from the source data store and to the destination data store.</p></li>
<li><p>Staged copy. When you copy data from a source data store to a sink data store, you might choose to use Blob storage as an interim staging store.</p></li>
</ol>

<p>You can take these ways to tune the performance of your Data Factory service with Copy Activity.</p>

<p>For more details about Azure Data Factory Copy Activity performace, please see:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance#data-integration-units"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance#data-integration-units</a></p>
"
"53782396","Copy-activity from Blob Storage-Account to CosmosDb is very slow","<h3>Situation:</h3>
<p>I'm using the <strong>copy-activity</strong> from <strong>azure-data-factory</strong> to copy one json-file with <strong>500 MB</strong> from a <strong>storage-account-blob to CosmosDB</strong> and from <strong>CosmosDb to a storage-Account-blob</strong></p>
<p>The AzureBlobStorageLinkedService is configured with a <strong>SAS-Token</strong>.</p>
<h3>Times:</h3>
<p>CosmosDb to a storage-Account-blob: <strong>4 minutes</strong></p>
<p>Storage-account-blob to CosmosDB: <strong>2 hours - over 7 hours (timeout)</strong></p>
<h3>CosmosDB:</h3>
<p>Before copy-activity will be started, an empty collection with <strong>20.000 RU/s</strong> will be created. I looked at the metrics of CosmosDB and it is really bored. There are only a few 429 errors. We have &quot;default indexing-configuration&quot; and a partitionKey. This means that we have data with several partitionKeys from several partitionKey-ranges (partitions)</p>
<h3>Data:</h3>
<p>In the json-file there are <strong>48.000</strong> json-objects. Some are <strong>small</strong> and some can have <strong>200 KB</strong>.</p>
<h3>Tries:</h3>
<p>I tried with different WriteBatchSizes:</p>
<pre><code>5: 2 hours

100: 2 hours

10.000: 7 hours (timeout)
</code></pre>
<p>I tried it with same/different regions =&gt; no difference</p>
<p>I tried it with smaller files =&gt; they are much faster (500 KB/s instead of 50 KB/s)</p>
<h3>Question:</h3>
<p>Why it is so slowly?
Is the file with 500 MB too large?</p>
","<c#><azure><azure-blob-storage><azure-cosmosdb><azure-data-factory>","2018-12-14 15:15:38","232","2","1","53815979","<p>I tried with very high throughput-values and it worked fine:</p>

<pre><code>1.000.000 RU/s: 9 Minuten ✔
100.000 RU/s: 15 Minuten ✔
</code></pre>

<p>But I have to think on scaling down after data-transfer ist complete, because of costs!!!</p>
"
"53777807","Backup Azure Data Factoryv2 configurations to JSON","<p>This is a bit of a generic question - I've got an ADFv2 instance with a number of fiddly datasets (namely they require custom queries to make them behave) and a number of relatively complex pipelines.</p>

<p>So far, I've been creating all of these via the Web Page - but I've got to the point where I'm looking to backup the entirety of the config to a series of JSON files, so I can store it in somewhere like Git, and if needed, recreate the whole thing from scratch using Powershell.</p>

<p>Now, I'm aware that from the GUI, there is the option of clicking for the ARM code - which will display the relevant JSON - I'm however looking for a way (probably via PS) to auto-magically backup the entire instance (Connections, Datasets, Pipelines etc. etc.) to multiple JSON files.</p>

<p>Is anyone aware if this functionality natively exists? or if there is a tool to do it? or is the only way to build a big Powershell loop, looping through all the contents and outputting each to JSON?</p>

<p>I appreciate any input.</p>

<p>Thanks</p>
","<azure><powershell><azure-data-factory>","2018-12-14 10:23:48","1761","0","1","53779528","<p><a href=""https://i.stack.imgur.com/1nSpb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1nSpb.png"" alt=""enter image description here""></a></p>

<p>Click the Set up Code Repository option.</p>

<p>Then you can configure with Azure DevOps or Github.</p>
"
"53765075","Parse multiple json files in one activity","<p>I would like to use data factory to regularly download 500000 json files from a web API and store them in a blob storage container. Then I need to parse the json files to extract some values from each file and store these values together with an ID (part of filename) in a database. I can do this using a ForEach activity and run a custom activity for each file, but this is very slow, so I would prefer some batch activity which could run the same parsing code on each file. Is there some way to do this?</p>
","<azure-data-factory>","2018-12-13 15:24:41","802","0","1","53771857","<p>If your source json files have same schema, you can leverage the Copy Activity which can parse those files in a single run. But if possible, I would suggest to split those files into different sub folder (e.g. 1000 files per folder), so that each copy run needs less time and ease the management.</p>

<p>Refer to this doc for more details: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview</a></p>
"
"53764072","Retrieve blob file name in Copy Data activity","<p>I download json files from a web API and store them in blob storage using a Copy Data activity and binary copy. Next I would like to use another Copy Data activity to extract a value from each json file in the blob container and store the value together with its ID in a database. The ID is part of the filename, but is there some way to extract the filename?</p>
","<azure-data-factory>","2018-12-13 14:27:47","1695","1","2","53764236","<p>You can do the following set of activities:</p>

<p>1) A GetMetadata activity, configure a dataset pointing to the blob folder, and add the Child Items in the Field List.</p>

<p>2) A forEach activity that takes every item from the GetMetadata activity and iterates over them. To do this you configure the Items to be @activity('NameOfGetMetadataActivity').output.childItems</p>

<p>3) Inside the foreach, you can extract the filename of each file using the following function: item().name</p>

<p>After this continue as you see fit, either adding functions to get the ID or copy the entire name.</p>

<p>Hope this helped!</p>
"
"53764072","Retrieve blob file name in Copy Data activity","<p>I download json files from a web API and store them in blob storage using a Copy Data activity and binary copy. Next I would like to use another Copy Data activity to extract a value from each json file in the blob container and store the value together with its ID in a database. The ID is part of the filename, but is there some way to extract the filename?</p>
","<azure-data-factory>","2018-12-13 14:27:47","1695","1","2","63448908","<p>After Setting up Dataset for source file/file path with wildcard and destination/sink as some table</p>
<ol>
<li>Add Copy Activity setup source, sink</li>
<li>Add Additional Columns</li>
<li>Provide a name to the additional column and value &quot;$$FILEPATH&quot;</li>
<li>Import Mapping and voila - your additional column should be in the list of source columns marked &quot;Additional&quot;</li>
</ol>
<p><a href=""https://i.stack.imgur.com/g4XG1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g4XG1.png"" alt=""Setup"" /></a></p>
"
"53762492","Data Factory V2 Error Code 2200 on Data Lake Sink","<p>Im using the following as a workshop  <a href=""https://www.youtube.com/watch"" rel=""nofollow noreferrer"">https://www.youtube.com/watch</a>?
v=IAqJ6nCDtGc  </p>

<p>I have some sales data in an Azure SQL Database (Rather than an on premise database)  </p>

<p>And I have a data lake Gen Storage 1 I've successfully set up the Connectors and Data sets. Each Connector has <strong>tested ok</strong></p>

<p>Do create the Azure Data Lake created an app registration to get the information I needed for the principal ID and the Principal Key</p>

<p>Ive created the pipeline with a copy activity as per the above video  </p>

<p>When I run it I get the following  </p>

<p>{ ""errorCode"": ""2200"", ""message"": ""Failure happened on 'Sink' side. ErrorCode=UserErrorFailedFileOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The request to 'Unknown' failed and the status code is 'BadRequest', request id is ''. \r\nBad Request\r\n\r\n<h2>Bad Request - Invalid URL</h2>\r\n<hr><p>HTTP Error 400. The request URL is invalid.</p>\r\n\r\n ,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Net.WebException,Message=The remote server returned an error: (400) Bad Request.,Source=System,'"", ""failureType"": ""UserError"", ""target"": ""Copy Sales data to data lake"" }  </p>

<p>Ive checked the URL in the data lake connector and this seems fine</p>

<p>""dataLakeStoreUri"": ""https://.azuredatalakestore.net/webhdfs/v1"",</p>

<p>The only other URL I can think of is the one set up when registering the app on sign on URL</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal</a></p>

<p>If anyone can help it would be greatly appreciated. </p>

<p>Debbie</p>
","<azure><azure-data-lake><azure-data-factory>","2018-12-13 13:01:53","4881","0","2","53858128","<p>A couple of thoughts.</p>

<p>Can you check you have the account uri set to ""https://&lt;<strong><em>your ADL endpoint name</em></strong>>.azuredatalakestore.net/webhdfs/v1""?  Above you have ""<strong><em>.azuredatalakestore.net/webhdfs/v1</em></strong>"" but understand you may have deleted the endpoint in this post for privacy.  </p>

<p>On the ADL permissions, have you assigned permissions through the data explorer in ADL?  The service principal I believe needs execute permissions on the root folder and all the folders in the required path plus read/write on any subfolder that will be written to.  i.e. if I am writing to \foo\bar.txt</p>

<pre><code>\       permissions needed = x
foo     permissions needed = x
bar.txt permissions needed = rw
</code></pre>

<p>See <a href=""https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-access-control#common-scenarios-related-to-permissions"" rel=""nofollow noreferrer"">here</a> for details.</p>

<p>Finally, folders are case sensitive so check that they are being referenced correctly.</p>

<p>Hope this helps.
Mark. </p>
"
"53762492","Data Factory V2 Error Code 2200 on Data Lake Sink","<p>Im using the following as a workshop  <a href=""https://www.youtube.com/watch"" rel=""nofollow noreferrer"">https://www.youtube.com/watch</a>?
v=IAqJ6nCDtGc  </p>

<p>I have some sales data in an Azure SQL Database (Rather than an on premise database)  </p>

<p>And I have a data lake Gen Storage 1 I've successfully set up the Connectors and Data sets. Each Connector has <strong>tested ok</strong></p>

<p>Do create the Azure Data Lake created an app registration to get the information I needed for the principal ID and the Principal Key</p>

<p>Ive created the pipeline with a copy activity as per the above video  </p>

<p>When I run it I get the following  </p>

<p>{ ""errorCode"": ""2200"", ""message"": ""Failure happened on 'Sink' side. ErrorCode=UserErrorFailedFileOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The request to 'Unknown' failed and the status code is 'BadRequest', request id is ''. \r\nBad Request\r\n\r\n<h2>Bad Request - Invalid URL</h2>\r\n<hr><p>HTTP Error 400. The request URL is invalid.</p>\r\n\r\n ,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Net.WebException,Message=The remote server returned an error: (400) Bad Request.,Source=System,'"", ""failureType"": ""UserError"", ""target"": ""Copy Sales data to data lake"" }  </p>

<p>Ive checked the URL in the data lake connector and this seems fine</p>

<p>""dataLakeStoreUri"": ""https://.azuredatalakestore.net/webhdfs/v1"",</p>

<p>The only other URL I can think of is the one set up when registering the app on sign on URL</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal</a></p>

<p>If anyone can help it would be greatly appreciated. </p>

<p>Debbie</p>
","<azure><azure-data-lake><azure-data-factory>","2018-12-13 13:01:53","4881","0","2","54037092","<p>found out what the issue was. </p>

<p>I had set up a dynamic file path for the data Lake so the file would be placed into Year/Month/Day folders but this had caused the above issue. Once I deleted this path and just ran it into the data lake it worked.</p>

<p>The file path I used was from this u-tube how to guide</p>

<p>uhttps://www.youtube.com/watch?v=IAqJ6nCDtGc</p>

<p>I still need to figure out how to do the above but at the least I can get my file into the data lake</p>
"
"53756570","How to pass arguments to ADF pipeline using powershell","<p>I have created a pipeline which has two parameters at pipeline level.
I want to send the values to these parameters using powershell and trigger the pipeline.
Any idea how to do it using Powershell.</p>
","<powershell><azure-data-factory>","2018-12-13 07:01:30","3575","2","2","53764069","<p>I'll leave a script that you can then modify to your needs:</p>

<pre><code>Login-AzureRmAccount
Select-AzureRmSubscription -Subscription ""yourSubId""

$dfname = ""youDataFActoryName""
$rgName = ""yourResourceGroupName""
$pipe = ""pipeName""
$parameters = @{
    ""param1"" = ""asdasd""
    ""param2"" = ""123456""
}

Invoke-AzureRmDataFactoryV2Pipeline -DataFactoryName $dfname -ResourceGroupName $rgName -PipelineName $pipe -Parameter $parameters
</code></pre>

<p>Hope this helped!</p>
"
"53756570","How to pass arguments to ADF pipeline using powershell","<p>I have created a pipeline which has two parameters at pipeline level.
I want to send the values to these parameters using powershell and trigger the pipeline.
Any idea how to do it using Powershell.</p>
","<powershell><azure-data-factory>","2018-12-13 07:01:30","3575","2","2","69400553","<p>Parameter is to be passed as a <code>hashtable</code>. The <code>hashtable</code> is created using <code>@</code></p>
<p>e.g :</p>
<pre><code>$param = @{&quot;year&quot; = &quot;2022&quot;}

Invoke-AzDataFactoryV2Pipeline -ResourceGroupName &quot;UAT&quot; -DataFactoryName &quot;ADF&quot; -PipelineName &quot;Snapshot&quot; -Parameter $param
</code></pre>
"
"53754827","ADF V2 failure when using bool variable","<p>Very simple issue.  I am trying to set up a pipeline that has a  variable of type bool.  As soon as I add it, the pipeline fails with:</p>

<pre><code>{
""code"":""BadRequest"",
""message"":""Invalid value for property 'type'"",
""target"":""pipeline/pipeline2/runid/66b9c7be-9894-494a-abd9-34fd92bbd972"",
""details"":null,
""error"":null
}
</code></pre>

<p>simple pipeline with a string variable and a wait activity succeeds.</p>

<pre><code>{
""name"": ""pipeline2"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""Wait1"",
            ""type"": ""Wait"",
            ""typeProperties"": {
                ""waitTimeInSeconds"": 1
            }
        }
    ],
    ""variables"": {
        ""Test"": {
            ""type"": ""String"",
            ""defaultValue"": ""\""Hello\""""
        }
    }
}
}
</code></pre>

<p>When I add a bool and nothing else, it fails to debug.</p>

<pre><code>{
""name"": ""pipeline2"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""Wait1"",
            ""type"": ""Wait"",
            ""typeProperties"": {
                ""waitTimeInSeconds"": 1
            }
        }
    ],
    ""variables"": {
        ""Test"": {
            ""type"": ""String"",
            ""defaultValue"": ""\""Hello\""""
        },
        ""TestBool"": {
            ""type"": ""Bool"",
            ""defaultValue"": false
        }
    }
  }
}
</code></pre>

<p>Any clue how to get this to work?  I am trying to use this variable as a condition for an Until loop.<br>
Many thanks.</p>
","<azure-data-factory>","2018-12-13 04:00:05","5308","0","1","53754887","<p>ok, I experimented.  </p>

<p>If I go into the code and set the type as boolean rather than Bool, then the above pipeline runs.  </p>

<p>Looks like a UI bug in the designer that sets the type to Bool.  I'll file a bug report.
Mark.</p>

<p><strong>Update</strong> ok it runs but I can't set a default value (it disappears) and anything that references the value causes an Internal Server Error (presumably because it is null which is invalid for a Boolean).  Definitely something for the engineers to look at.</p>

<p><strong>Update 2</strong> It appears you can set the variable with SetVariable without error but it appears not to do anything.  The value is always true in my test case.</p>

<p><strong>Update 3</strong> Microsoft has a fix coming next week.</p>
"
"53754745","How to send the parameter values to azure data factory using powershell","<p>I want to send 2 parameters to data factory pipeline parameters and trigger the pipeline using powershell scripting.
May I know how to do it.</p>
","<azure-data-factory>","2018-12-13 03:47:00","852","2","1","53790520","<p>You can trigger the pipeline through <code>Invoke-AzureRmDataFactoryV2Pipeline</code> command. It should look like:</p>

<pre><code>Invoke-AzureRmDataFactoryV2Pipeline -DataFactory $yourADFv2DataFactory -PipelineName ""YourAdfv2PipelineName"" -ParameterFile .\PipelineParameters.json
</code></pre>

<p>The PipelineParameters.json file should be like:</p>

<pre><code>{
  ""parameter_1_name"": ""parameter_1_value"",
  ""parameter_2_name"": ""parameter_2_value""
}
</code></pre>

<p>You can refer the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#azure-powershell"" rel=""nofollow noreferrer"">Official Documentation here</a></p>
"
"53753118","column mapping trouble within ADFv2","<p>I have a source .csv with 21 columns and a destination table with 25 columns.</p>

<p>Not ALL columns within the source have a home in the destination table and not all columns in the destination table come from the source.</p>

<p>I cannot get my CopyData task to let me pick and choose how I want the mapping to be.  The only way I can get it to work so far is to load the source data to a ""holding"" table that has a 1:1 mapping and then execute a stored procedure to insert data from that table into the final destination.</p>

<p>I've tried altering the schemas on both the source and destination to match but it still errors out because the ACTUAL source has more columns than the destination or vice versa.</p>

<p>This can't possibly be the most efficient way to accomplish this but I'm at a loss as to how to make it work.</p>

<p>The error code that is returned is some variation on:</p>

<pre><code>""errorCode"": ""2200"",
    ""message"": ""ErrorCode=UserErrorInvalidColumnMappingColumnCountMismatch,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Invalid column mapping provided to copy activity: '{LONG LIST OF COLUMN MAPPING HERE}', Detailed message: Different column count between target structure and column mapping. Target column count:25, Column mapping count:16. Check column mapping in table definition.,Source=Microsoft.DataTransfer.Common,'"",
    ""failureType"": ""UserError"",
    ""target"": ""LoadPrimaryOwner""
</code></pre>
","<json><azure><azure-data-factory>","2018-12-12 23:51:13","2079","0","3","53753573","<p>It seems that you were trying to extract the 16 columns from the source table to target table. If your target is Sql Server or Azure Sql DB, you can try the following settings:</p>

<ul>
<li>Set the source structure as 21 columns in csv file.</li>
<li>Set the column mapping with 16 columns mapping as your wanted data.</li>
<li>Set the target structure as 16 columns, which has the same name and order in the column mapping definition.</li>
</ul>
"
"53753118","column mapping trouble within ADFv2","<p>I have a source .csv with 21 columns and a destination table with 25 columns.</p>

<p>Not ALL columns within the source have a home in the destination table and not all columns in the destination table come from the source.</p>

<p>I cannot get my CopyData task to let me pick and choose how I want the mapping to be.  The only way I can get it to work so far is to load the source data to a ""holding"" table that has a 1:1 mapping and then execute a stored procedure to insert data from that table into the final destination.</p>

<p>I've tried altering the schemas on both the source and destination to match but it still errors out because the ACTUAL source has more columns than the destination or vice versa.</p>

<p>This can't possibly be the most efficient way to accomplish this but I'm at a loss as to how to make it work.</p>

<p>The error code that is returned is some variation on:</p>

<pre><code>""errorCode"": ""2200"",
    ""message"": ""ErrorCode=UserErrorInvalidColumnMappingColumnCountMismatch,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Invalid column mapping provided to copy activity: '{LONG LIST OF COLUMN MAPPING HERE}', Detailed message: Different column count between target structure and column mapping. Target column count:25, Column mapping count:16. Check column mapping in table definition.,Source=Microsoft.DataTransfer.Common,'"",
    ""failureType"": ""UserError"",
    ""target"": ""LoadPrimaryOwner""
</code></pre>
","<json><azure><azure-data-factory>","2018-12-12 23:51:13","2079","0","3","53762757","<p>Have you tried mapping the columns in the graphical editor? Just click on the copy activity, then mapping and click the blue button ""Import Schemas"". This will import both schemas and let you pick which column from source maps with which column from sink.</p>

<p>Hope this helped!</p>

<p><a href=""https://i.stack.imgur.com/YYC6z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YYC6z.png"" alt=""Added image to clarify""></a></p>
"
"53753118","column mapping trouble within ADFv2","<p>I have a source .csv with 21 columns and a destination table with 25 columns.</p>

<p>Not ALL columns within the source have a home in the destination table and not all columns in the destination table come from the source.</p>

<p>I cannot get my CopyData task to let me pick and choose how I want the mapping to be.  The only way I can get it to work so far is to load the source data to a ""holding"" table that has a 1:1 mapping and then execute a stored procedure to insert data from that table into the final destination.</p>

<p>I've tried altering the schemas on both the source and destination to match but it still errors out because the ACTUAL source has more columns than the destination or vice versa.</p>

<p>This can't possibly be the most efficient way to accomplish this but I'm at a loss as to how to make it work.</p>

<p>The error code that is returned is some variation on:</p>

<pre><code>""errorCode"": ""2200"",
    ""message"": ""ErrorCode=UserErrorInvalidColumnMappingColumnCountMismatch,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Invalid column mapping provided to copy activity: '{LONG LIST OF COLUMN MAPPING HERE}', Detailed message: Different column count between target structure and column mapping. Target column count:25, Column mapping count:16. Check column mapping in table definition.,Source=Microsoft.DataTransfer.Common,'"",
    ""failureType"": ""UserError"",
    ""target"": ""LoadPrimaryOwner""
</code></pre>
","<json><azure><azure-data-factory>","2018-12-12 23:51:13","2079","0","3","53927509","<p>In the sink dataset delete the columns that you don't want to be mapped.</p>

<p>delete the columns that are not required in the sink by selecting and then click delete button</p>

<p>[<img src=""https://i.stack.imgur.com/HaGTe.png"" alt=""delete the columns that are not required in the sink by selecting and then click delete button[1]""></p>

<p>In order for the copy to work smoothly.</p>

<p>1.The source dataset should have all the columns in the same sequence.<br>
2. All the columns selected in the sink dataset have to be mapped.</p>
"
"53751589","Azure Blob creation not firing Azure Data Factory event trigger","<p>I've Event trigger in Azure Data Factory, it triggers when a new blob is created in Azure Blob storage. But my trigger is not firing on Blob creation.</p>

<p>Followed below link but stuck at below mentioned point:
<a href=""https://stackoverflow.com/questions/51134475/azure-data-factory-event-not-starting-pipeline"">Azure Data Factory: event not starting pipeline</a>.</p>

<p>Environment details:</p>

<p>Event Grid is registered, ADF is v2 and passing parameters to pipeline.
My question is do I need to have Azure storage event subscription activated? If so what should be my event handler(which in my case is ADF pipeline)?</p>

<p>Please suggest:</p>

<ol>
<li>Azure Storage Event subscription is mandatory to fire Blob creation triggers? if yes then Event Handler option.</li>
<li>If it is not mandatory(as per my research only Event Grid has to be registered) what is causing my Event trigger to not fire?</li>
</ol>
","<azure><triggers><azure-blob-storage><azure-data-factory>","2018-12-12 21:24:34","1370","2","1","53758543","<p>Must be using a V2 Storage Account</p>

<p>Trigger name must only contain letters, numbers and the '-' character (this restriction will soon be removed)</p>

<p>Trigger makes the following properties available @triggerBody().folderPath and @triggerBody().fileName. To use these in your pipeline your must map them to pipeline paramaters and use them as such: @pipeline().parameters.paramaetername.</p>

<p>Have you followed above guidelines? If yes, maybe you could consider create a support ticket.</p>
"
"53744836","Azure Data Factory v2 Oracle on-premise to SQL Server (IaaS)","<p>I am trying to create a ADF v2 pipeline that will copy data from on-premise Oracle server to SQL Server VM. <br>
Network Admins have set up Integration Runtime for Oracle. Their idea was that we can simply use SQL Azure as a target. It worked, but for some other reason, we want to use SQL Server on VM instead. <br>
I figured that I need to set up stand-alone IR and set it on VM. Unfortunately, when I tried to run pipeline I got the error that both source and target need to be on the same IR.</p>
","<azure-data-factory>","2018-12-12 14:07:08","1410","0","1","53753506","<p>It is expected that source and target should be on the same self-hosted IR, so that the Copy activity could be executed on that IR. You can learn more details on how Copy activity work in this doc: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview</a>.</p>

<p>For your case: </p>

<ul>
<li>If your <strong>IR for Oracle</strong> can access the target SQL Server VM, you can just use that IR to copy data from Oracle to SQL Server.</li>
<li>Else, you need 2 copy activities: (1) copy from Oracle to a data store that both source Oracle VM and target SQL Server VM can access (e.g. a Blob Storage) on <strong>IR for Oracle</strong>. (2) copy from Blob to SQL Server on <strong>IR for SQL Server</strong></li>
</ul>
"
"53737200","How To Passing Parameters to Data Factory Activities","<p>I have created two parameters at the pipeline level. There are 3 activities in a pipeline to which I want to send these parameters.
The parameter for activity is the file path and that is getting created based on the pipeline level parameters. I have used below code to form a path but getting an error - invalid  - 'concat' does not have an overload that supports the arguments given</p>

<p><code>
@concat('weather/data/',adddays(utcnow(),pipeline().parameters.numdays,'yyyy'),'/',adddays(utcnow(),pipeline().parameters.numdays,'MM'),'/',adddays(utcnow(),pipeline().parameters.numdays,'dd'),'.IND.',pipeline().parameters.instance,'.csv')</code></p>
","<azure-data-factory>","2018-12-12 06:23:18","1805","0","1","53737903","<p>Can it be that one of the arguments isn't a string, e.g. pipeline().parameters.instance? Maybe you need to convert it into a string.</p>
"
"53736044","Send file as attachment using REST API of DB2 in Azure data factory","<p>I need to send a file present in azure data lake to IBM cloud server using the REST API of DB2. </p>

<p>I have used the rest API in command prompt in the below format 
curl -X POST """" -H ""accept: application/json"" -H ""Authorization: Bearer "" -k -F fichier=.csv </p>

<p>I have tried to use a Web activity but I was not successful.</p>

<p>I would like to know how this activity can be done in ADF? </p>
","<azure><ibm-cloud><azure-storage><azure-data-factory>","2018-12-12 04:22:07","828","1","1","53759019","<p>Bharath, based on your description, I want to provide two suggestions:</p>

<p>1.As @Saul Cruz said in the command, you could use <a href=""https://learn.microsoft.com/ga-ie/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">custom activity</a> to run the command to do the transfer mission.</p>

<p>2.I found Azure Data Factory copy activity has <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-store"" rel=""nofollow noreferrer"">Azure Data Lake connector</a> and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-db2"" rel=""nofollow noreferrer"">DB2 connector</a>,you could configure copy activity to transfer the csv file. </p>

<hr>

<p>Update Answer:</p>

<p>Per my research, Web activity is not suitable for you. It's unstable for transfer data.</p>

<p>Since you want to avoid custom activity, maybe I suggest you adopt below solution:</p>

<p>1.Use Copy activity to transfer <code>csv</code> file from Azure Data Lake to Azure Blob Storage.</p>

<p>2.Use <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-triggered-function"" rel=""nofollow noreferrer"">Azure Function Blob Trigger</a> to monitor any change feed,then send rest request in function.</p>
"
"53726624","ADF Copy Activity insert XML column to Azure SQL DB fails","<p>I have some files in Azure blob that contains 10 columns (9 are strings, 1 is XML). The destination table also has 9 strings and 1 xml datatype</p>

<p>I'm receiving the below error when inserting the data. If I remove the XML column then no problem to insert.</p>

<p>Any idea to about this? The XML is well formed because I have no issue inserting to SQL Server on prem</p>

<p>Maybe XML is not supported as I dont see XML data type available in the source or sink data type options...?</p>

<p>Activity Copy Data1 failed: ErrorCode=FailedDbOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A database operation failed with the following error: &apos;XML parsing: line 1&#44; character 55&#44; unable to switch the encoding&apos;,Source=,''Type=System.Data.SqlClient.SqlException,Message=XML parsing: line 1&#44; character 55&#44; unable to switch the encoding,Source=.Net SqlClient Data Provider,SqlErrorNumber=9402,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=9402,State=1,Message=XML parsing: line 1&#44; character 55&#44; unable to switch the encoding,},],'</p>
","<xml><azure><azure-sql-database><azure-data-factory>","2018-12-11 14:48:24","1081","0","1","53737246","<p><strong>Note:</strong> XML file type in copy activity is not supported.</p>

<p>I would suggest you to vote up an idea submitted by another Azure customer.</p>

<p><a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/17508058-xml-file-type-in-copy-activity-along-with-xml-sc"" rel=""nofollow noreferrer"">XML file type in copy activity. along with XML schema validation</a></p>

<p>All of the feedback you share in these forums will be monitored and reviewed by the Microsoft engineering teams responsible for building Azure.</p>
"
"53725481","Data factory SAP BW","<p>I'm trying to use the Data Factory in Azure to export data from a SAP BW. The connection is working, and I'm able to get data. The problem is how I'm getting the data. The picture describes the issue pretty well.</p>

<p>Has anyone encountered something similar? Any tips on how to approach this issue? Any help is greatly appreciated! </p>

<p>Query like:</p>

<pre><code>SELECT
    [Measures].&lt;&lt;Measure&gt;&gt; ON COLUMNS, 
    NON EMPTY 
    {&lt;&lt;Dimension&gt;&gt;.MEMBERS,
    &lt;&lt;Dimension&gt;&gt;.MEMBERS} ON ROWS
FROM &lt;&lt;Cube&gt;&gt;
</code></pre>

<p>Picture:
<a href=""https://i.stack.imgur.com/9Gxfh.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/9Gxfh.png</a></p>

<p>Best regards,</p>
","<azure><ssas><mdx><azure-data-factory><sap-bw>","2018-12-11 13:48:04","336","1","2","53725838","<p>This is how your query should look like.</p>

<pre><code>select Measures.Value on columns,

nonempty
(
DimPlan.Plan.Plan,
DimCategory.Category.Category,
DimProduct.Product.Product
)
on rows 
from YourCube
</code></pre>
"
"53725481","Data factory SAP BW","<p>I'm trying to use the Data Factory in Azure to export data from a SAP BW. The connection is working, and I'm able to get data. The problem is how I'm getting the data. The picture describes the issue pretty well.</p>

<p>Has anyone encountered something similar? Any tips on how to approach this issue? Any help is greatly appreciated! </p>

<p>Query like:</p>

<pre><code>SELECT
    [Measures].&lt;&lt;Measure&gt;&gt; ON COLUMNS, 
    NON EMPTY 
    {&lt;&lt;Dimension&gt;&gt;.MEMBERS,
    &lt;&lt;Dimension&gt;&gt;.MEMBERS} ON ROWS
FROM &lt;&lt;Cube&gt;&gt;
</code></pre>

<p>Picture:
<a href=""https://i.stack.imgur.com/9Gxfh.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/9Gxfh.png</a></p>

<p>Best regards,</p>
","<azure><ssas><mdx><azure-data-factory><sap-bw>","2018-12-11 13:48:04","336","1","2","53805019","<p>Looks like you are getting the <code>ALL</code> members of each hierarchy coming through into the results.</p>

<p>Very similar to MoazRubs answer but avoiding needing to use the <code>NonEmpty</code> function - you can simply cross-join the hierarchies via the <code>*</code> operator:</p>

<pre><code>SELECT
      Measures.Value ON 0,
      DimPlan.Plan.Plan.MEMBERS *
      DimCategory.Category.Category.MEMBERS *
      DimProduct.Product.Product.MEMBERS
         ON 1
FROM YourCube; 
</code></pre>
"
"53720451","ADF - Pipeline with powershell script","<p>I have written a small Powershell script in order to retrieve some information from my organization and export this into a .csv file:</p>

<pre><code>Get-ADUser -Filter {company -Like ""*COMPANYNAME*""} -Properties department | Select sAMAccountName, department, userprincipalname | export-csv -path C:\temp\test.csv
</code></pre>

<p>My aim is to create a pipeline in Azure Data Factory, in order to launch this script and export the csv into a dataset, so that I can get all the data within SQL Azure.</p>

<p>How could I create the pipeline and insert this script?</p>
","<azure><powershell><csv><azure-pipelines><azure-data-factory>","2018-12-11 08:51:33","3920","0","2","53721700","<p>Based on this <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/19119145-powershell-script-support-in-activity"" rel=""nofollow noreferrer"">feedback</a>, <code>Powershell Script</code> can't be run directly in the adf pipeline so far. Maybe you could vote up this thread.</p>

<p>You could configure <code>custom activity</code> from this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity#custom-activity"" rel=""nofollow noreferrer"">document</a> to run your ps script, also you could follow this <a href=""https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/data-factory/v1/data-factory-use-custom-activities.md"" rel=""nofollow noreferrer"">github tutorial</a> to create c# custom activity.</p>
"
"53720451","ADF - Pipeline with powershell script","<p>I have written a small Powershell script in order to retrieve some information from my organization and export this into a .csv file:</p>

<pre><code>Get-ADUser -Filter {company -Like ""*COMPANYNAME*""} -Properties department | Select sAMAccountName, department, userprincipalname | export-csv -path C:\temp\test.csv
</code></pre>

<p>My aim is to create a pipeline in Azure Data Factory, in order to launch this script and export the csv into a dataset, so that I can get all the data within SQL Azure.</p>

<p>How could I create the pipeline and insert this script?</p>
","<azure><powershell><csv><azure-pipelines><azure-data-factory>","2018-12-11 08:51:33","3920","0","2","53739496","<p>You can run a PowerShell script by creating a DataFactory with a Custom Activity that runs the command: <code>powershell .\script.ps1</code>
You must copy script.ps1 in the StorageAccount linked with your custom activity.</p>

<p>To copy Activity Output you can follow this thread <a href=""https://social.msdn.microsoft.com/Forums/en-US/5ceda984-874e-417b-8a28-7a512ede61d0/custom-activity-output-values-for-use-in-sequential-task?forum=AzureDataFactory"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/en-US/5ceda984-874e-417b-8a28-7a512ede61d0/custom-activity-output-values-for-use-in-sequential-task?forum=AzureDataFactory</a>, it worked for me.
And find some extra info here: <a href=""https://stackoverflow.com/questions/53518485/copy-output-data-from-custom-activity-powershell-script"">Copy output data from Custom Activity Powershell script</a></p>
"
"53700712","Find out the user who deployed Azure Data Factory pipeline in ADF v1","<p>We are having a ADF V1 pipeline, which we had paused to avoid data flowing in. Suddenly, we see that it has been deployed in running mode, by some user. How to find out who  had deployed the azure data factory pipeline. </p>

<p>When I see in the azure portal, I only see the deployment time. I don't see the user, who deployed it. I also tried to see the same using azure powershell also. But, not able to find. </p>

<pre><code>Connect-AzureRmAccount

Select-AzureRmSubscription -SubscriptionName ""TestSubscriptionName""

(Get-AzureRmDataFactoryPipeline -DataFactoryName ""TestDataFactory""       
-ResourceGroupName ""TestRG"" -Name ""TestPipeline"").Properties.RuntimeInfo  
</code></pre>

<p>Please let me know, how can I find out the user, who had deployed it. </p>
","<azure><powershell><azure-data-factory>","2018-12-10 06:47:16","1579","1","1","53718303","<p>The activity log contains all write operations (PUT, POST, DELETE) performed on your resources. It does not include read operations (GET).</p>

<p>Through activity logs, you can determine:</p>

<ul>
<li>what operations were taken on the resources in your subscription</li>
<li>who initiated the operation (although operations initiated by a
backend service do not return a user as the caller)</li>
<li>when the operation occurred</li>
<li>the status of the operation</li>
<li>the values of other properties that might help you research the
operation</li>
</ul>

<p>You can also use the below PowerShell cmdlet to retrieve the log entries.</p>

<pre><code>Get-AzureRmLog -ResourceGroup ExampleGroup
</code></pre>

<p>For more details, refer to <a href=""https://learn.microsoft.com/en-us/azure/azure-resource-manager/resource-group-audit"" rel=""nofollow noreferrer"">View activity logs to audit actions on resources</a>.</p>
"
"53671401","Azure datafactory v2 Linkedservice build","<p>I am using the following JSON template </p>

<pre><code>{
     ""properties"": {
         ""type"": ""AzureStorage"",
         ""typeProperties"": {
             ""connectionString"": {
                 ""type"": ""SecureString"",
                 ""value"": ""DefaultEndpointsProtocol=https;AccountName=&lt;accountname&gt;;AccountKey=&lt;accountkey&gt;;EndpointSuffix=core.windows.net""
             }
         }
     },
     ""name"": ""AzureStorageLinkedService""
 }
</code></pre>

<p>from the URL <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-hybrid-copy-powershell#get-storage-account-name-and-account-key"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-hybrid-copy-powershell#get-storage-account-name-and-account-key</a>
 to create an Azure storage Linked service.
Rather than hard-coding the Account name and the key is it possible to pass it as a parameter to the PowerShell cmdlet </p>

<pre><code>Set-AzureRmDataFactoryV2LinkedService
</code></pre>

<p>I am trying to build a PowerShell script to automate the deployment</p>

<p>Thanks</p>
","<azure-storage><azure-data-factory>","2018-12-07 14:20:16","91","0","1","53753161","<p>I'm not sure if this is what you want, but do you want to dynamically extract the azure account name and account key?</p>

<p>are you automating this  for many environments? in my cause i am using azure DevOps and Azure Key Vault to pass the required parameters per environment (Dev, Test, and Production), there is a very nice step-by-step guide that automates the deployment of ADF here</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a></p>

<p>You don't have to hard-code the keys, passwords, etc. but you have to have them specified in Azure Key Vault and link this to ADF</p>

<p><a href=""https://azure.microsoft.com/en-us/updates/secure-credential-management-using-azure-key-vault-and-data-factory/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/updates/secure-credential-management-using-azure-key-vault-and-data-factory/</a></p>

<p>So, for example for an Azure Data Lake Store  you can specify something like this</p>

<pre><code>{
    ""name"": ""linked_service_adls"",
    ""properties"": {
        ""type"": ""AzureDataLakeStore"",
        ""typeProperties"": {
            ""dataLakeStoreUri"": ""https://YOURADLSNAME.azuredatalakestore.net/webhdfs/v1"",
            ""servicePrincipalId"": ""xxxxxx-xxxx-xxxx-xxxx-xxxxxxxxx"",
            ""servicePrincipalKey"": {
                ""type"": ""AzureKeyVaultSecret"",
                ""store"": {
                    ""referenceName"": ""linked_service_kv_general"",
                    ""type"": ""LinkedServiceReference""
                },
                ""secretName"": ""adls-webapi-principalKey""
            },
            ""tenant"": ""xxxxxx-xxxx-xxxx-xxxx-xxxxxxxxx"",
            ""subscriptionId"": ""xxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx"",
            ""resourceGroupName"": ""rg-whatever""
        }
    }
}
</code></pre>

<p>Where the servicePrincipalKey does not have to be hard-coded and gets dynamically extracted from your Azure Key Vault ( using ""type"": ""AzureKeyVaultSecret"" and the secret name ""adls-webapi-principalKey"" )and in Azure DevOps you can specify the parameters values for service PRincipal ID , tenant, subscriptionID, etc for every target environment, which at the same time you can extract also from AKV (Azure Key Vault), I know that your use case is Powershell oriented but if that's the case you would still need some sort of config file to pull of these values, and of course it is not a good idea to version control config files so they have to live somewhere, I stronlgy recommend the AKV/Azure Devops approach. Hope this helps</p>
"
"53660996","how to enforce static folder paths after deflate within ADFv2","<p>I have a ZIP file which lives at a <a href=""https://www.sos.wa.gov/_assets/corps/jsonCorpsData.zip"" rel=""nofollow noreferrer"">generic URL</a> that contains four large JSON files.</p>

<p>In a perfect world, within a single pipeline, I would want each of these files to be extracted, parsed and loaded into their respective Azure tables.</p>

<p>I can see where I can use deflate instructions at many points within datasets and activities but each time I use one to deflate the ZIP, ADF wants to extract the output into a folder named with a GUID that I can't seem to find the source of.  Could someone please help me get toward my goal of unzipping the files and loading to tables?  This seems like a simple task and I feel like I'm missing something obvious.</p>
","<json><azure><azure-data-factory>","2018-12-06 23:03:36","53","0","1","53753130","<p>I was eventually able to figure this out by breaking the steps into further detail within the pipeline instead of having it all managed within one.  Felt cumbersome but it worked so...whatever!</p>
"
"53659568","In Azure Data Factory read excel files","<p>I am new to Azure Data Factory(ADF), I need to access/load excel files sitting in a blob into ADF but as ADF doesn't support excel format(supports tex/csv/json/.. only) is there a way to ingest excel files into ADF?
I really appreciate if anybody could help!</p>

<p>Thanks.</p>
","<azure><azure-data-factory>","2018-12-06 20:57:38","19963","6","6","53665343","<p>You are right, Azure Data Factory does not support to read <code>.xlsx</code> file, the workaround is to save your <code>.xlsx</code> file as a <code>.csv</code> file, I think it should work.</p>

<p>My <code>.xlsx</code> file:</p>

<p><a href=""https://i.stack.imgur.com/AENdF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AENdF.png"" alt=""enter image description here""></a></p>

<p>Save as <code>.csv</code> file, the info will not change:</p>

<p><a href=""https://i.stack.imgur.com/snPAp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/snPAp.png"" alt=""enter image description here""></a></p>

<p>Preview Data in ADF:</p>

<p><a href=""https://i.stack.imgur.com/r98Az.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/r98Az.png"" alt=""enter image description here""></a></p>

<p><strong>Besides</strong>, if you want to just copy the <code>.xlsx</code> file, no need to convert it to <code>.csv</code>, you just need to choose the <code>Binary Copy</code> option.</p>

<p><a href=""https://i.stack.imgur.com/2AdG0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2AdG0.png"" alt=""enter image description here""></a></p>
"
"53659568","In Azure Data Factory read excel files","<p>I am new to Azure Data Factory(ADF), I need to access/load excel files sitting in a blob into ADF but as ADF doesn't support excel format(supports tex/csv/json/.. only) is there a way to ingest excel files into ADF?
I really appreciate if anybody could help!</p>

<p>Thanks.</p>
","<azure><azure-data-factory>","2018-12-06 20:57:38","19963","6","6","53665623","<p>ADF does not support reading from xls file yet.</p>

<p>You can find solutions in this answer: <a href=""https://stackoverflow.com/questions/52514153/how-to-read-files-with-xlsx-and-xls-extension-in-azure-data-factory"">How to read files with .xlsx and .xls extension in Azure data factory? </a></p>
"
"53659568","In Azure Data Factory read excel files","<p>I am new to Azure Data Factory(ADF), I need to access/load excel files sitting in a blob into ADF but as ADF doesn't support excel format(supports tex/csv/json/.. only) is there a way to ingest excel files into ADF?
I really appreciate if anybody could help!</p>

<p>Thanks.</p>
","<azure><azure-data-factory>","2018-12-06 20:57:38","19963","6","6","53668621","<p>If you're familiar with SSIS, you could simply use Excel Source in your SSIS package, and then run it on SSIS Integration Runtime using Execute SSIS Package activity in ADF pipeline.</p>
"
"53659568","In Azure Data Factory read excel files","<p>I am new to Azure Data Factory(ADF), I need to access/load excel files sitting in a blob into ADF but as ADF doesn't support excel format(supports tex/csv/json/.. only) is there a way to ingest excel files into ADF?
I really appreciate if anybody could help!</p>

<p>Thanks.</p>
","<azure><azure-data-factory>","2018-12-06 20:57:38","19963","6","6","60527338","<p>An easier solution is to use a powerautomate flow to export an excel table as a CSV and trigger the datafactory process. The only issue is that an undocumented feature on power automate is  that you need to use a fixed Excel file name as passing the name as a variable fails.</p>
"
"53659568","In Azure Data Factory read excel files","<p>I am new to Azure Data Factory(ADF), I need to access/load excel files sitting in a blob into ADF but as ADF doesn't support excel format(supports tex/csv/json/.. only) is there a way to ingest excel files into ADF?
I really appreciate if anybody could help!</p>

<p>Thanks.</p>
","<azure><azure-data-factory>","2018-12-06 20:57:38","19963","6","6","62381677","<p>ADF V2 now supports reading data from an Excel file, <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-excel?WT.mc_id=linkedin"" rel=""noreferrer"">Here</a> is the link to the article.</p>

<p>Hope this helps!</p>
"
"53659568","In Azure Data Factory read excel files","<p>I am new to Azure Data Factory(ADF), I need to access/load excel files sitting in a blob into ADF but as ADF doesn't support excel format(supports tex/csv/json/.. only) is there a way to ingest excel files into ADF?
I really appreciate if anybody could help!</p>

<p>Thanks.</p>
","<azure><azure-data-factory>","2018-12-06 20:57:38","19963","6","6","62560603","<p>ADF now support Excel as a data source. You can read <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-excel"" rel=""nofollow noreferrer"">here</a></p>
"
"53659395","How to get the Azure Data Factory parameters into the ARM template parameters file (ARMTemplateParametersForFactory.json) after publishing","<p>I am trying to create my Azure DevOps release pipeline for Azure Data Factory.</p>

<p>I have followed the rather cryptic guide from Microsoft (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a> ) regarding adding additional parameters to the ARM template that gets generated when you do a publish (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template</a> )</p>

<p>Created a <code>arm-template-parameters-definition.json</code> file in the route of the master branch. When I do a publish, the <code>ARMTemplateParametersForFactory.json</code> in the <code>adf_publish</code> branch remains completely unchanged. I have tried many configurations.</p>

<p>I have defined some Pipeline Parameters in Data Factory and want them to be configurable in my deployment pipeline. Seems like an obvious requirement to me.</p>

<p>Have I missed something fundamental? Help please!</p>

<p>The JSON is as follows:</p>

<pre><code>{
    ""Microsoft.DataFactory/factories/pipelines"": {
        ""*"": {
            ""properties"": {
                ""parameters"": {
                        ""*"": ""=""                
                }
            }
        }
    },
    ""Microsoft.DataFactory/factories/integrationRuntimes"": {
        ""*"": ""=""
    },
    ""Microsoft.DataFactory/factories/triggers"": {},
    ""Microsoft.DataFactory/factories/linkedServices"": {},
    ""Microsoft.DataFactory/factories/datasets"": {}
}
</code></pre>
","<azure><azure-devops><azure-pipelines><azure-data-factory><azure-rm-template>","2018-12-06 20:44:35","10902","11","6","54357416","<p>Here are the necessary steps to clear up confusion:</p>

<ol>
<li>Add the arm-template-parameters-definition.json to your master branch.</li>
<li>Close and re-open your Dev ADF portal</li>
<li>Do a new Publish</li>
</ol>

<p>Your ARMTemplateParametersForFactory.json will then be updated.</p>
"
"53659395","How to get the Azure Data Factory parameters into the ARM template parameters file (ARMTemplateParametersForFactory.json) after publishing","<p>I am trying to create my Azure DevOps release pipeline for Azure Data Factory.</p>

<p>I have followed the rather cryptic guide from Microsoft (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a> ) regarding adding additional parameters to the ARM template that gets generated when you do a publish (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template</a> )</p>

<p>Created a <code>arm-template-parameters-definition.json</code> file in the route of the master branch. When I do a publish, the <code>ARMTemplateParametersForFactory.json</code> in the <code>adf_publish</code> branch remains completely unchanged. I have tried many configurations.</p>

<p>I have defined some Pipeline Parameters in Data Factory and want them to be configurable in my deployment pipeline. Seems like an obvious requirement to me.</p>

<p>Have I missed something fundamental? Help please!</p>

<p>The JSON is as follows:</p>

<pre><code>{
    ""Microsoft.DataFactory/factories/pipelines"": {
        ""*"": {
            ""properties"": {
                ""parameters"": {
                        ""*"": ""=""                
                }
            }
        }
    },
    ""Microsoft.DataFactory/factories/integrationRuntimes"": {
        ""*"": ""=""
    },
    ""Microsoft.DataFactory/factories/triggers"": {},
    ""Microsoft.DataFactory/factories/linkedServices"": {},
    ""Microsoft.DataFactory/factories/datasets"": {}
}
</code></pre>
","<azure><azure-devops><azure-pipelines><azure-data-factory><azure-rm-template>","2018-12-06 20:44:35","10902","11","6","54422611","<p>I've been struggling with this for a few days and did not found a lot of info, so here what I've found out. You have to put the <code>arm-template-parameters-definition.json</code> in the configured root folder of your collaboration branch:</p>

<p><a href=""https://i.stack.imgur.com/u83nE.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/u83nE.png"" alt=""data factory git settings""></a></p>

<p>So in my example, it has to look like this:</p>

<p><a href=""https://i.stack.imgur.com/dUEzQ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/dUEzQ.png"" alt=""arm-template-parameters-definition.json""></a></p>

<p>If you work in a separate branch, you can test your configuration by downloading the arm templates from the data factory. When you make a change in the parameters-definition you have to reload your browser screen (f5) to refresh the configuration.
<a href=""https://i.stack.imgur.com/qnGES.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/qnGES.png"" alt=""Data factory download arm template""></a></p>

<p>If you really want to parameterize all of the parameters in all of the pipelines, the following should work:</p>

<pre><code>""Microsoft.DataFactory/factories/pipelines"": {
    ""properties"": {
        ""parameters"":{
            ""*"":{
                ""defaultValue"":""=""
            }
        }
    }
}
</code></pre>

<p>I prefer specifying the parameters that I want to parameterize:</p>

<pre><code>""Microsoft.DataFactory/factories/pipelines"": {
    ""properties"": {
        ""parameters"":{
            ""LogicApp_RemoveFileFromADLSURL"":{
                ""defaultValue"":""=:-LogicApp_RemoveFileFromADLSURL:""
            },
            ""LogicApp_RemoveBlob"":{
                ""defaultValue"":""=:-LogicApp_RemoveBlob:""
            }
        }
    }
}
</code></pre>
"
"53659395","How to get the Azure Data Factory parameters into the ARM template parameters file (ARMTemplateParametersForFactory.json) after publishing","<p>I am trying to create my Azure DevOps release pipeline for Azure Data Factory.</p>

<p>I have followed the rather cryptic guide from Microsoft (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a> ) regarding adding additional parameters to the ARM template that gets generated when you do a publish (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template</a> )</p>

<p>Created a <code>arm-template-parameters-definition.json</code> file in the route of the master branch. When I do a publish, the <code>ARMTemplateParametersForFactory.json</code> in the <code>adf_publish</code> branch remains completely unchanged. I have tried many configurations.</p>

<p>I have defined some Pipeline Parameters in Data Factory and want them to be configurable in my deployment pipeline. Seems like an obvious requirement to me.</p>

<p>Have I missed something fundamental? Help please!</p>

<p>The JSON is as follows:</p>

<pre><code>{
    ""Microsoft.DataFactory/factories/pipelines"": {
        ""*"": {
            ""properties"": {
                ""parameters"": {
                        ""*"": ""=""                
                }
            }
        }
    },
    ""Microsoft.DataFactory/factories/integrationRuntimes"": {
        ""*"": ""=""
    },
    ""Microsoft.DataFactory/factories/triggers"": {},
    ""Microsoft.DataFactory/factories/linkedServices"": {},
    ""Microsoft.DataFactory/factories/datasets"": {}
}
</code></pre>
","<azure><azure-devops><azure-pipelines><azure-data-factory><azure-rm-template>","2018-12-06 20:44:35","10902","11","6","54802885","<p>I have experienced similar problems with the <code>ARMTemplateParametersForFactory.json</code> file not being updated whenever I publish and have changed the <code>arm-template-parameters-definition.json</code>.</p>

<p>I figured that I can force update the Publish branch by doing the following:</p>

<ol>
<li>Update the custom parameter definition file as you wish.</li>
<li>Delete <code>ARMTemplateParametersForFactory.json</code> from the Publish branch.</li>
<li>Refresh (F5) the Data Factory portal.</li>
<li>Publish.</li>
</ol>

<p>The easiest way to validate your custom parameter .json syntax seems to be by exporting the ARM template, just as Simon mentioned.</p>
"
"53659395","How to get the Azure Data Factory parameters into the ARM template parameters file (ARMTemplateParametersForFactory.json) after publishing","<p>I am trying to create my Azure DevOps release pipeline for Azure Data Factory.</p>

<p>I have followed the rather cryptic guide from Microsoft (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a> ) regarding adding additional parameters to the ARM template that gets generated when you do a publish (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template</a> )</p>

<p>Created a <code>arm-template-parameters-definition.json</code> file in the route of the master branch. When I do a publish, the <code>ARMTemplateParametersForFactory.json</code> in the <code>adf_publish</code> branch remains completely unchanged. I have tried many configurations.</p>

<p>I have defined some Pipeline Parameters in Data Factory and want them to be configurable in my deployment pipeline. Seems like an obvious requirement to me.</p>

<p>Have I missed something fundamental? Help please!</p>

<p>The JSON is as follows:</p>

<pre><code>{
    ""Microsoft.DataFactory/factories/pipelines"": {
        ""*"": {
            ""properties"": {
                ""parameters"": {
                        ""*"": ""=""                
                }
            }
        }
    },
    ""Microsoft.DataFactory/factories/integrationRuntimes"": {
        ""*"": ""=""
    },
    ""Microsoft.DataFactory/factories/triggers"": {},
    ""Microsoft.DataFactory/factories/linkedServices"": {},
    ""Microsoft.DataFactory/factories/datasets"": {}
}
</code></pre>
","<azure><azure-devops><azure-pipelines><azure-data-factory><azure-rm-template>","2018-12-06 20:44:35","10902","11","6","55264239","<p>You've got the right idea, but the arm-template-parameters-definition.json file needs to follow the hierarchy of the element you want to parameterize.  </p>

<p>Here is my pipeline activity I want to parameterize.  The ""url"" should change based on the environment it's deployed in</p>

<pre><code>{
    ""name"": ""[concat(parameters('factoryName'), '/ExecuteSPForNetPriceExpiringContractsReport')]"",
    ""type"": ""Microsoft.DataFactory/factories/pipelines"",
    ""apiVersion"": ""2018-06-01"",
    ""properties"": {
        ""description"": """",
        ""activities"": [
            {
                ""name"": ""NetPriceExpiringContractsReport"",
                ""description"": ""Passing values to the Logic App to generate the CSV file."",
                ""type"": ""WebActivity"",
                ""typeProperties"": {
                    ""url"": ""[parameters('ExecuteSPForNetPriceExpiringContractsReport_properties_1_typeProperties')]"",
                    ""method"": ""POST"",
                    ""headers"": {
                        ""Content-Type"": ""application/json""
                    },
                    ""body"": {
                        ""resultSet"": ""@activity('NetPriceExpiringContractsReportLookup').output""
                    }
                }
            }
        ]
    }
}
</code></pre>

<p>Here is the arm-template-parameters-definition.json file that turns that URL into a parameter.</p>

<pre><code>{
   ""Microsoft.DataFactory/factories/pipelines"": {
        ""properties"": {
            ""activities"": [{
                ""typeProperties"": {
                    ""url"": ""-::string""
                }
            }]
        }
    },
    ""Microsoft.DataFactory/factories/integrationRuntimes"": {},
    ""Microsoft.DataFactory/factories/triggers"": {},
    ""Microsoft.DataFactory/factories/linkedServices"": {
        ""*"": ""=""
    },
    ""Microsoft.DataFactory/factories/datasets"": {
        ""*"": ""=""
    }
}
</code></pre>

<p>So basically in the pipelines of the ARM template, it looks for properties -> activities -> typeProperties -> url in the JSON and parameterizes it.</p>
"
"53659395","How to get the Azure Data Factory parameters into the ARM template parameters file (ARMTemplateParametersForFactory.json) after publishing","<p>I am trying to create my Azure DevOps release pipeline for Azure Data Factory.</p>

<p>I have followed the rather cryptic guide from Microsoft (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a> ) regarding adding additional parameters to the ARM template that gets generated when you do a publish (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template</a> )</p>

<p>Created a <code>arm-template-parameters-definition.json</code> file in the route of the master branch. When I do a publish, the <code>ARMTemplateParametersForFactory.json</code> in the <code>adf_publish</code> branch remains completely unchanged. I have tried many configurations.</p>

<p>I have defined some Pipeline Parameters in Data Factory and want them to be configurable in my deployment pipeline. Seems like an obvious requirement to me.</p>

<p>Have I missed something fundamental? Help please!</p>

<p>The JSON is as follows:</p>

<pre><code>{
    ""Microsoft.DataFactory/factories/pipelines"": {
        ""*"": {
            ""properties"": {
                ""parameters"": {
                        ""*"": ""=""                
                }
            }
        }
    },
    ""Microsoft.DataFactory/factories/integrationRuntimes"": {
        ""*"": ""=""
    },
    ""Microsoft.DataFactory/factories/triggers"": {},
    ""Microsoft.DataFactory/factories/linkedServices"": {},
    ""Microsoft.DataFactory/factories/datasets"": {}
}
</code></pre>
","<azure><azure-devops><azure-pipelines><azure-data-factory><azure-rm-template>","2018-12-06 20:44:35","10902","11","6","58886301","<p>Just to clarify on top of Simon's great answer.  If you have non standard git hierarchy (i.e. you move the root to a sub-folder like I have done below with ""Source""), it can be confusing when the doc refers to the ""repo root"".  Hopefully this diagram helps.</p>

<p><a href=""https://i.stack.imgur.com/zhOaI.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/zhOaI.png"" alt=""enter image description here""></a></p>
"
"53659395","How to get the Azure Data Factory parameters into the ARM template parameters file (ARMTemplateParametersForFactory.json) after publishing","<p>I am trying to create my Azure DevOps release pipeline for Azure Data Factory.</p>

<p>I have followed the rather cryptic guide from Microsoft (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a> ) regarding adding additional parameters to the ARM template that gets generated when you do a publish (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#use-custom-parameters-with-the-resource-manager-template</a> )</p>

<p>Created a <code>arm-template-parameters-definition.json</code> file in the route of the master branch. When I do a publish, the <code>ARMTemplateParametersForFactory.json</code> in the <code>adf_publish</code> branch remains completely unchanged. I have tried many configurations.</p>

<p>I have defined some Pipeline Parameters in Data Factory and want them to be configurable in my deployment pipeline. Seems like an obvious requirement to me.</p>

<p>Have I missed something fundamental? Help please!</p>

<p>The JSON is as follows:</p>

<pre><code>{
    ""Microsoft.DataFactory/factories/pipelines"": {
        ""*"": {
            ""properties"": {
                ""parameters"": {
                        ""*"": ""=""                
                }
            }
        }
    },
    ""Microsoft.DataFactory/factories/integrationRuntimes"": {
        ""*"": ""=""
    },
    ""Microsoft.DataFactory/factories/triggers"": {},
    ""Microsoft.DataFactory/factories/linkedServices"": {},
    ""Microsoft.DataFactory/factories/datasets"": {}
}
</code></pre>
","<azure><azure-devops><azure-pipelines><azure-data-factory><azure-rm-template>","2018-12-06 20:44:35","10902","11","6","76123614","<p>I had this issue because of CORS extension in Chrome, which did not allow PATCH method used by ADF. Changing settings for the extension made publish process work fine.</p>
"
"53654131","parallel loading of csv from blob storage into Azure SQL Database","<p>I have a large csv file stored on Blob storage in azure.
I want to load it into an zure sql database as quickly as possible.</p>

<p>Ive tried running SSIS in the DataFactory integration Runtime but it is quite slow as it is one thread/process.</p>

<p>What is the best way to parallelize the data load from an a csv in azure to sql database?</p>

<p>Note, i am ok with moving the csv to alternative storage such as datalake if necessary.</p>
","<azure><azure-sql-database><azure-storage><azure-data-factory>","2018-12-06 14:58:21","1416","0","1","53685888","<p>The quickest way in Azure SQL is to use BULK operation (BULK INSERT or OPENROWSET BULK). You need to create an EXTERNAL DATA SOURCE in first place pointing to the Azure Blob Storage that contains the CSV you want to import, and then you can use BULK operation:</p>

<pre><code>SELECT * FROM OPENROWSET(BULK ...)
</code></pre>

<p>A full explanation and sample is here:</p>

<p><a href=""https://medium.com/@mauridb/automatic-import-of-csv-data-using-azure-functions-and-azure-sql-63e1070963cf"" rel=""nofollow noreferrer"">https://medium.com/@mauridb/automatic-import-of-csv-data-using-azure-functions-and-azure-sql-63e1070963cf</a></p>

<p>the example describe how to import files dropped in a Blob Storage. Multiple file will be imported in parallel.</p>

<p>For a complete description of how to bulk import data from Azure Blob Storage to Azure SQL, there are a lot of samples in the official documentation</p>

<p><a href=""https://learn.microsoft.com/en-us/sql/t-sql/statements/bulk-insert-transact-sql?view=sql-server-2017#f-importing-data-from-a-file-in-azure-blob-storage"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sql/t-sql/statements/bulk-insert-transact-sql?view=sql-server-2017#f-importing-data-from-a-file-in-azure-blob-storage</a></p>

<p>Another option is to use Azure Data Factory that will be as fast as using the BULK option just mentioned, but it requires the creation of an Azure Data Factory pipeline that adds some complexity to the solution...but, on the other hand, can be done without writing a single line of code.</p>
"
"53650490","Use output data from Copy activity in subsequent activity","<p>I have a ForEach activity which uses a Copy activity with an HTTP source and a blob storage sink to download a json file for each item. The HTTP source is set to Binary Copy whereas the blob storage sink is not, since I want to both copy the complete json file to blob storage and also extract some data from each json file and store this data in a database table in the following activity.</p>

<p>In the blob storage sink, I've added a column definition which extracts some data from the json file. The json files are stored in blob storage successfully, but how can I access the extracted data in the subsequent stored procedure activity?</p>
","<azure-data-factory>","2018-12-06 11:32:06","2265","0","1","53662216","<p>You can try using lookup activity to extract the data in the Blob and use the output in later activity. Refer to  <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity</a> to see whether lookup activity can satisfy your requirements.</p>
"
"53636436","Make a generic/parametrize trigger in azure data factory","<p>I want to load data from on premise to azure blobs. I have data on three on premise servers. Problem is that data copying should run at different time for each source.
Please suggest a way to do that.</p>
","<azure><triggers><azure-data-factory><data-pipeline>","2018-12-05 16:15:12","71","0","1","53638854","<p>The way to handle this would be three Triggers, each set one time and target.</p>
"
"53625603","How to rename a blob file using powershell","<p>seemingly simple task. I just want to rename a blob file, I know I have to copy it to rename or something, then delete the original but this is proving tricky. I have created the storage context (New-AzureStorageContext), and got the blob (Get-AzureStorageBlob), and found Start-AzureStorageBlobCopy, but how to I actually rename it? </p>

<p>I'd like to do this within the same container if possible as well. Ideally I'd run it in an Azure Runbook and call it using a webhook I Azure Data Factory v2. I did try to rename the file using 'Add Dynamic Content' in the copy job sink in DFv2, but I don't think you can. By the way, I just want to append the date to the existing file name. Thank you.</p>
","<powershell><blob><azure-data-factory>","2018-12-05 05:18:55","3461","2","1","53625935","<p>You can use my  <code>Rename-AzureStorageBlob</code>convenience function:</p>

<pre><code>function Rename-AzureStorageBlob
{
    [CmdletBinding()]
    Param
    (
        [Parameter(Mandatory=$true, ValueFromPipeline=$true, Position=0)]
        [Microsoft.WindowsAzure.Commands.Common.Storage.ResourceModel.AzureStorageBlob]$Blob,

        [Parameter(Mandatory=$true, Position=1)]
        [string]$NewName
    )

  Process {
    $blobCopyAction = Start-AzureStorageBlobCopy `
        -ICloudBlob $Blob.ICloudBlob `
        -DestBlob $NewName `
        -Context $Blob.Context `
        -DestContainer $Blob.ICloudBlob.Container.Name

    $status = $blobCopyAction | Get-AzureStorageBlobCopyState

    while ($status.Status -ne 'Success')
    {
        $status = $blobCopyAction | Get-AzureStorageBlobCopyState
        Start-Sleep -Milliseconds 50
    }

    $Blob | Remove-AzureStorageBlob -Force
  }
}
</code></pre>

<p>It accepts the blob as pipeline input so you can pipe the result of the Get-AzureStorageBlob to it and just provide a new name:</p>

<pre><code>$connectionString= 'DefaultEndpointsProtocol=https;AccountName....'
$storageContext = New-AzureStorageContext -ConnectionString $connectionString

Get-AzureStorageBlob -Container 'MyContainer' -Context $storageContext -Blob 'myBlob.txt'|
    Rename-AzureStorageBlob -NewName 'MyNewBlob.txt'
</code></pre>

<p>To append the date to the existing file name you can use something like:</p>

<pre><code>Get-AzureStorageBlob -Container 'MyContainer' -Context $storageContext -Blob 'myBlob.txt' | ForEach-Object { 
$_ | Rename-AzureStorageBlob -NewName ""$($_.Name)$(Get-Date -f ""FileDateTime"")"" }
</code></pre>

<p>Further reading: <a href=""https://about-azure.com/2018/02/13/rename-azure-storage-blob-using-powershell/"" rel=""nofollow noreferrer"">Rename Azure Storage Blob using PowerShell</a></p>
"
"53623673","Azure Datafactory Upload to Salesforce and reference field on another object","<p>This is the scenario - we are using Azure DataFactoryV2 to run a series of pipelines that take account data from a local datasource, transform it and upload into Salesforce.</p>

<p>Currently we are wanting to Import an Account and relate it to another account. There is the standard Parent/Child relationship for Accounts (eg a Reseller account and a child account) and this is using the internal Salesforce ID.</p>

<p>We are also using an External ID for the purpose of Upserting and this ID is unique to each record.</p>

<p>According to SF Documentation: <a href=""https://developer.salesforce.com/docs/atlas.en-us.api_rest.meta/api_rest/dome_upsert.htm"" rel=""nofollow noreferrer"">Here</a> - when you manually call the API and pass it a JSON file, you are able to add a relationship within the JSON:</p>

<pre><code>    {
   ""Name"" : ""NewAccount"",
   ""account__r"" :
   {
       ""Ext_UID__c"" : 123
   }

}
</code></pre>

<p>However, this doesn't appear to be doable in Azure DataFactoryV2 to specify a lookup relationship in the code, or if it is, I'm not sure how to do it.</p>

<p>For reference - here is the Pipeline JSON code:</p>

<pre><code>{
""name"": ""Import_to_Salesforce"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""Load_to_Salesforce"",
            ""type"": ""Copy"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false,
                ""secureInput"": false
            },
            ""userProperties"": [
                {
                    ""name"": ""Source"",
                    ""value"": ""[dbo].[Account]""
                },
                {
                    ""name"": ""Destination"",
                    ""value"": ""Account""
                }
            ],
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""SqlSource""
                },
                ""sink"": {
                    ""type"": ""SalesforceSink"",
                    ""writeBatchSize"": 5000,
                    ""writeBehavior"": ""upsert"",
                    ""externalIdFieldName"": ""Ext_UID__c"",
                    ""ignoreNullValues"": false
                },
                ""enableStaging"": false,
                ""enableSkipIncompatibleRow"": true,
                ""dataIntegrationUnits"": 0,
                ""translator"": {
                    ""type"": ""TabularTranslator"",
                    ""columnMappings"": {
                        ""Name"": ""Name"",
                        ""ParentId"": ""ParentId"",
                        ""BillingStreet"": ""BillingStreet"",
                        ""BillingCity"": ""BillingCity"",
                        ""BillingPostalCode"": ""BillingPostalCode"",
                        ""BillingCountry"": ""BillingCountry"",
                        ""ShippingStreet"": ""ShippingStreet"",
                        ""ShippingCity"": ""ShippingCity"",
                        ""ShippingPostalCode"": ""ShippingPostalCode"",
                        ""ShippingCountry"": ""ShippingCountry"",
                        ""Phone"": ""Phone"",
                        ""AccountNumber"": ""AccountNumber"",
                        ""Brand__c"": ""Brand__c"",
                        ""Account_Status__c"": ""Account_Status__c"",
                        ""Account_Type__c"": ""Account_Type__c"",
                        ""Preferred_Payment_Method__c"": ""Preferred_Payment_Method__c"",
                        ""Last_Account_Login__c"": ""Last_Account_Login__c"",
                        ""Ext_UID__c"": ""Ext_UID__c"",
                        ""Auto_Renew_Status__c"": ""Auto_Renew_Status__c"",
                        ""Account_Balance__c"": ""Account_Balance__c"",
                        ""Outstanding_Amount_30_days__c"": ""Outstanding_Amount_30_days__c"",
                        ""Outstanding_Amount_60_days__c"": ""Outstanding_Amount_60_days__c"",
                        ""Outstanding_Amount_90_days__c"": ""Outstanding_Amount_90_days__c"",
                        ""Account_Priority__c"": ""Account_Priority__c"",
                        ""Reseller__c"": ""Reseller__c"",
                        ""Last_Payment__c"": ""Last_Payment__c""
                    }
                }
            },
            ""inputs"": [
                {
                    ""referenceName"": ""Staging_Source"",
                    ""type"": ""DatasetReference""
                }
            ],
            ""outputs"": [
                {
                    ""referenceName"": ""Destination_Load_to_Salesforce"",
                    ""type"": ""DatasetReference""
                }
            ]
        }
    ]
},
""type"": ""Microsoft.DataFactory/factories/pipelines""}
</code></pre>

<p>Any input would be greatly appreciate.</p>
","<azure><salesforce><azure-data-factory><salesforce-lightning>","2018-12-05 01:00:49","1872","3","2","53624733","<p>Lookup Activity is available in ADF v2:</p>

<p><a href=""https://i.stack.imgur.com/7OeCj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7OeCj.png"" alt=""Lookup Activity in ADF v2""></a></p>

<p>Here you can select the dataset and the row at which you want to lookup.</p>

<p>Hope this helps!</p>
"
"53623673","Azure Datafactory Upload to Salesforce and reference field on another object","<p>This is the scenario - we are using Azure DataFactoryV2 to run a series of pipelines that take account data from a local datasource, transform it and upload into Salesforce.</p>

<p>Currently we are wanting to Import an Account and relate it to another account. There is the standard Parent/Child relationship for Accounts (eg a Reseller account and a child account) and this is using the internal Salesforce ID.</p>

<p>We are also using an External ID for the purpose of Upserting and this ID is unique to each record.</p>

<p>According to SF Documentation: <a href=""https://developer.salesforce.com/docs/atlas.en-us.api_rest.meta/api_rest/dome_upsert.htm"" rel=""nofollow noreferrer"">Here</a> - when you manually call the API and pass it a JSON file, you are able to add a relationship within the JSON:</p>

<pre><code>    {
   ""Name"" : ""NewAccount"",
   ""account__r"" :
   {
       ""Ext_UID__c"" : 123
   }

}
</code></pre>

<p>However, this doesn't appear to be doable in Azure DataFactoryV2 to specify a lookup relationship in the code, or if it is, I'm not sure how to do it.</p>

<p>For reference - here is the Pipeline JSON code:</p>

<pre><code>{
""name"": ""Import_to_Salesforce"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""Load_to_Salesforce"",
            ""type"": ""Copy"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false,
                ""secureInput"": false
            },
            ""userProperties"": [
                {
                    ""name"": ""Source"",
                    ""value"": ""[dbo].[Account]""
                },
                {
                    ""name"": ""Destination"",
                    ""value"": ""Account""
                }
            ],
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""SqlSource""
                },
                ""sink"": {
                    ""type"": ""SalesforceSink"",
                    ""writeBatchSize"": 5000,
                    ""writeBehavior"": ""upsert"",
                    ""externalIdFieldName"": ""Ext_UID__c"",
                    ""ignoreNullValues"": false
                },
                ""enableStaging"": false,
                ""enableSkipIncompatibleRow"": true,
                ""dataIntegrationUnits"": 0,
                ""translator"": {
                    ""type"": ""TabularTranslator"",
                    ""columnMappings"": {
                        ""Name"": ""Name"",
                        ""ParentId"": ""ParentId"",
                        ""BillingStreet"": ""BillingStreet"",
                        ""BillingCity"": ""BillingCity"",
                        ""BillingPostalCode"": ""BillingPostalCode"",
                        ""BillingCountry"": ""BillingCountry"",
                        ""ShippingStreet"": ""ShippingStreet"",
                        ""ShippingCity"": ""ShippingCity"",
                        ""ShippingPostalCode"": ""ShippingPostalCode"",
                        ""ShippingCountry"": ""ShippingCountry"",
                        ""Phone"": ""Phone"",
                        ""AccountNumber"": ""AccountNumber"",
                        ""Brand__c"": ""Brand__c"",
                        ""Account_Status__c"": ""Account_Status__c"",
                        ""Account_Type__c"": ""Account_Type__c"",
                        ""Preferred_Payment_Method__c"": ""Preferred_Payment_Method__c"",
                        ""Last_Account_Login__c"": ""Last_Account_Login__c"",
                        ""Ext_UID__c"": ""Ext_UID__c"",
                        ""Auto_Renew_Status__c"": ""Auto_Renew_Status__c"",
                        ""Account_Balance__c"": ""Account_Balance__c"",
                        ""Outstanding_Amount_30_days__c"": ""Outstanding_Amount_30_days__c"",
                        ""Outstanding_Amount_60_days__c"": ""Outstanding_Amount_60_days__c"",
                        ""Outstanding_Amount_90_days__c"": ""Outstanding_Amount_90_days__c"",
                        ""Account_Priority__c"": ""Account_Priority__c"",
                        ""Reseller__c"": ""Reseller__c"",
                        ""Last_Payment__c"": ""Last_Payment__c""
                    }
                }
            },
            ""inputs"": [
                {
                    ""referenceName"": ""Staging_Source"",
                    ""type"": ""DatasetReference""
                }
            ],
            ""outputs"": [
                {
                    ""referenceName"": ""Destination_Load_to_Salesforce"",
                    ""type"": ""DatasetReference""
                }
            ]
        }
    ]
},
""type"": ""Microsoft.DataFactory/factories/pipelines""}
</code></pre>

<p>Any input would be greatly appreciate.</p>
","<azure><salesforce><azure-data-factory><salesforce-lightning>","2018-12-05 01:00:49","1872","3","2","53626425","<p>Which SF connector you're using? If there's no meaningful name look at your API user's login history in Salesforce. It's probably listed as ""Simba Technologies"" or something like that?</p>

<p>For standard relationships you should be able to just type <strike><code>Account.Ext_UID__c</code></strike> <code>Parent.Ext_UID__c</code>like in this Contact load:</p>

<p><a href=""https://i.stack.imgur.com/d4GYC.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/d4GYC.png"" alt=""enter image description here""></a></p>

<p>For custom lookups/master-detail fields if the field API name is <code>Account__c</code> you can map it for upsert as <code>Account__r.Ext_UID__c</code>.</p>

<p>Just make sure that parent accounts exist before child accounts reference them... Maybe you loaded them in previous job, maybe it's best to load it in two stages:</p>

<ol>
<li>Flat insert/upsert of all accounts without parent mapped</li>
<li>Reparenting update/upsert that maps only this record's ext id and the parent's ext. id</li>
</ol>
"
"53619382","extract GUID and other attributes from pipeline within ADF?","<p>I'm wanting to create my own log file to store various attributes each time a pipeline is executed.  How do I add a step to my pipeline that records the pipeline GUID, timestamp for starting the process, timestamp for completion, completion status, successful record counts, error record counts, etc.?</p>

<p>I'm thinking I'd want need an output for different statuses but I'm not sure what to actually specify for the code to capture the different elements.</p>
","<azure><azure-sql-database><azure-data-factory>","2018-12-04 18:33:19","242","0","2","53621499","<p>You are going to want to use the PowerShell cmdlet <a href=""https://learn.microsoft.com/en-us/powershell/module/azurerm.datafactoryv2/Get-AzureRmDataFactoryV2ActivityRun?view=azurermps-6.13.0"" rel=""nofollow noreferrer"">Get-AzureRmDataFactoryV2ActivityRun</a> and direct the output to the desired logging mechanism. </p>

<p>Example Output: </p>

<blockquote>
<pre><code>ResourceGroupName : ADF
DataFactoryName   : WikiADF
ActivityName      : MyWebActivity
PipelineRunId     : f288712d-fb08-4cb8-96ef-82d3b9b30621
PipelineName      : DPWikisample
Input             : {method, url, headers, body...}
Output            : {operationstatus}
LinkedServiceName :
ActivityRunStart  : 9/14/2017 12:20:57 AM
ActivityRunEnd    : 9/14/2017 12:21:00 AM
DurationInMs      : 2768
Status            : Succeeded
Error             : {errorCode, message, failureType, target}
</code></pre>
</blockquote>

<p>For ADFv1, the applicable cmdlet is: <a href=""https://learn.microsoft.com/en-us/powershell/module/azurerm.datafactories/get-azurermdatafactoryrun?view=azurermps-6.13.0"" rel=""nofollow noreferrer"">Get-AzureRmDataFactoryRun</a></p>
"
"53619382","extract GUID and other attributes from pipeline within ADF?","<p>I'm wanting to create my own log file to store various attributes each time a pipeline is executed.  How do I add a step to my pipeline that records the pipeline GUID, timestamp for starting the process, timestamp for completion, completion status, successful record counts, error record counts, etc.?</p>

<p>I'm thinking I'd want need an output for different statuses but I'm not sure what to actually specify for the code to capture the different elements.</p>
","<azure><azure-sql-database><azure-data-factory>","2018-12-04 18:33:19","242","0","2","53753217","<p>I figured out how to do what I needed for now by setting variables within the pipeline since I didn't get any additional feedback from the previous user</p>
"
"53611682","Azure Data Factory CopyData Activity issue, error code 2200","<p>I am about to schedule the database insertion using Azure Data factory.</p>
<p>I am having 1 copydata and 2 lookups 1. Max Id inserted in destination table(LastValue in json code) 2. Max id from source table (Max).</p>
<p>To get the lookup value in Copy Data, i've written a query like i mentioned below but still i am getting error and i am unable to fetch Lookup value in select query.</p>
<p>select  *  from <code>customer</code> where  created_at&gt;=curdate()-30 and  id &gt; @{activity('CDNOps_LastUpdateValue').output.firstRow.LastValue} and id &lt;= @{activity('Source_MaxValue').output.firstRow.Max limit 20000}</p>
<p>Please help me out on this.</p>
<p>The error message i am getting while scheduling the data is,</p>
<p>&quot;errorCode&quot;: &quot;2200&quot;,
&quot;message&quot;: &quot;Failure happened on 'Source' side. ErrorCode=UserErrorOdbcOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR [42000] [Microsoft][MariaDB] You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near '&quot;LastValue&quot;:540418183}\n and id&lt;={&quot;Max&quot;:546239715}' at line 2.........</p>
","<azure><azure-pipelines><azure-data-factory>","2018-12-04 11:13:04","17283","0","1","53624000","<p>the below syntax should be the right one:</p>

<pre><code>select * from customer where created_at&gt;=curdate()-30 and id &gt; @{activity('CDNOps_LastUpdateValue').output.firstRow.LastValue} and id &lt;= @{activity('Source_MaxValue').output.firstRow.Max} limit 20000
</code></pre>

<p>Please note that when you wrap lookup activity output into @{}, this means you're trying to dynamically evaluate the value at the run time, so put ""limit 20000"", which doesn't belong to the lookup output, into <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#expressions"" rel=""nofollow noreferrer"">@{}</a> cause the error. Also ensure that <code>@{activity('CDNOps_LastUpdateValue').output.firstRow.LastValue}</code> and <code>@{activity('Source_MaxValue').output.firstRow.Max}</code> would be evaluated into a value that is comparable with column id in the table (same type at least). A small tips for debugging, you could create a user property in copy activity on UI to actively check whether the evaluated query result is what you want, see the picture below:
<a href=""https://i.stack.imgur.com/Ah4Dq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ah4Dq.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/lpD3r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lpD3r.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/Y0Qop.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Y0Qop.png"" alt=""enter image description here""></a></p>
"
"53602991","Azure Data Factory Connection to Google Big Query Timeout Issues","<p>I´m trying to <strong>grab Firebase analytics data from Google BigQuery with Azure Data Factory</strong>.</p>

<p>The Connection to BigQuery works but I have quite often timeout issues when running a (simple) query. <strong>3 out of 5 times I run into a timeout</strong>. If no timeout occurs I recive the data as expected.</p>

<p>Can someone of you confirm this issue? Or has an idea what´s the reason for the.</p>

<p>Thanks &amp; best,
Michael</p>
","<azure><azure-data-factory>","2018-12-03 22:35:16","854","1","1","53609009","<p>Timeout issues could happen in the Azure Data Factory sometimes. It is affected by source dataset, sink dataset, network, query performance and other factors, etc. After all, your connectors are not azure services.</p>

<p>You could try to set <code>timeout</code> param follow this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities#activity-policy-json-definition"" rel=""nofollow noreferrer"">json chart</a>. Or you could set <code>retry</code> times to deal with timeout issues.</p>

<p>If your sample data is so simple that can't be timeout,maybe you could commit feedback <a href=""https://feedback.azure.com/forums/270578-data-factory"" rel=""nofollow noreferrer"">here</a> to ask adf team about your concern.</p>
"
"53600427","formatting dynamic folder path within JSON","<p>looking for basic code format help.</p>

<p>I want to replace this code:</p>

<pre><code>{
""name"": ""BlobConnect"",
""properties"": {
    ""linkedServiceName"": {
        ""referenceName"": ""BlobConnect"",
        ""type"": ""LinkedServiceReference""
    },
    ""type"": ""AzureBlob"",
    ""typeProperties"": {
        ""fileName"": """",
        ""folderPath"": ""timtesting""
    }
},
""type"": ""Microsoft.DataFactory/factories/datasets""}
</code></pre>

<p>With something that dynamically assigns folder paths with whatever the current date is.  Another user suggested using:</p>

<pre><code>""folderPath"": {
      ""value"": ""@concat('test1/',
                formatDateTime(utcnow(),'yyyy'),'-', 
                formatDateTime(utcnow(),'MM'),'-',
                formatDateTime(utcnow(),'dd'))"",
       ""type"": ""Expression""}
</code></pre>

<p>but every time I replace the folder path section with this I continue to get formatting errors.  I'm not sure what exactly is wrong, several formatting sites confirm it's not correct but I am not sure how to fix.  Thanks!</p>
","<json><azure><azure-data-factory>","2018-12-03 19:17:57","47","0","1","53602699","<pre><code>{
""name"": ""BlobConnect"",
""properties"": {
    ""linkedServiceName"": {
        ""referenceName"": ""BlobConnect"",
        ""type"": ""LinkedServiceReference""
    },
    ""type"": ""AzureBlob"",
    ""typeProperties"": {
        ""fileName"": """",
        ""folderPath"": {
            ""value"": ""@concat('timtesting/', formatdatetime(utcnow(),'yyyy'),'-',formatdatetime(utcnow(),'MM'),'-',formatdatetime(utcnow(),'dd'))"",
            ""type"": ""Expression""
        }
    }
},
""type"": ""Microsoft.DataFactory/factories/datasets""
</code></pre>

<p>}</p>
"
"53598103","Unable to configure Azure Data Factory to get data from http endpoint","<p>I am trying to use Azure's data factory http connector, but I cannot go past step 2 (connection source), as, when I press the 'next' button, I get the error 'File schema is not defined'.</p>

<p>As can be seen on the attched picture, I get a 400 error from the server, but, when I execute the same get with postman, I do not get any errors from the server and I can download the file (which is an xlsx file, perhaps that is the problem).<a href=""https://i.stack.imgur.com/sva7Z.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sva7Z.jpg"" alt=""enter image description here""></a> </p>

<p>Any help is much appreciated! Thank you very much!!</p>
","<azure><azure-data-factory>","2018-12-03 16:42:05","799","1","2","53605418","<p>here you select the format with 'Text format', not sure your request result is csv or not.</p>
<blockquote>
<p>If you want to retrieve the data from an HTTP endpoint as-is without parsing it, skip the format setting.</p>
<p>If you want to parse the HTTP response content during copy, the following format types are supported: TextFormat, JsonFormat, AvroFormat, OrcFormat, and ParquetFormat. For more information, see Text format, JSON format, Avro format, Orc format, and Parquet format.</p>
</blockquote>
<p>This is the doc: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-http-connector"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-http-connector</a></p>
"
"53598103","Unable to configure Azure Data Factory to get data from http endpoint","<p>I am trying to use Azure's data factory http connector, but I cannot go past step 2 (connection source), as, when I press the 'next' button, I get the error 'File schema is not defined'.</p>

<p>As can be seen on the attched picture, I get a 400 error from the server, but, when I execute the same get with postman, I do not get any errors from the server and I can download the file (which is an xlsx file, perhaps that is the problem).<a href=""https://i.stack.imgur.com/sva7Z.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sva7Z.jpg"" alt=""enter image description here""></a> </p>

<p>Any help is much appreciated! Thank you very much!!</p>
","<azure><azure-data-factory>","2018-12-03 16:42:05","799","1","2","53612810","<p>If I've understood well your question I assume you are referring the UI Copy Activity flow (you are using the http connector for that purpose). If you can't detect or manually set a schema for your document, would binary copy work for you? 
On the first page of step 2 you will find the Binary Copy checkbox that may solve your problem.</p>
"
"53582848","azure data factory update","<p>I created two web services (Traning and predictive to use API's in the data factory.</p>

<ol>
<li>memory_train</li>
<li>memory_train [Predictive_Exp.]</li>
</ol>

<p>In data factory, I created ML Batch execution followed by ML Update Resource1. I used following API's and i/ps for my Data factory blocks.</p>

<p>ML Batch execution:</p>

<ol>
<li>I created a linked service that access API and key of my trained model</li>
<li>I created i/p and o/ps in a blob storage account.</li>
</ol>

<p>After this is a run , <code>.ilearner</code> file is stored in my blob</p>

<p>ML Update resource 1:</p>

<ol>
<li>I created patch endpoint in my memory_train [Predictive_Exp.] end point </li>
<li>I created second linked service that uses patch end point API and key in linked service, for update Resource end point, I used patch endpoint API again.</li>
<li>I tried to switch several API's for mr second linked service from default and patch, none of them could update my predictive model</li>
</ol>

<p>I get the following error for this Data factory block, can you suggest me if I am doing any mistakes</p>

<blockquote>
<pre><code>Activity ML Update Resource1 failed: UpdateResource has failed with error: {   ""error"": {
    ""code"": ""EditableResourcesNotAvailable"",
    ""message"": ""The specified resources do not exist or are not editable. Valid resource names: memory_train [trained model].""   } }.
</code></pre>
</blockquote>

<p>Diagnostic details: </p>

<blockquote>
  <p>job ID 011f3f75-8065-4835-bef5-143e7ae22111. Endpoint <a href=""https://management.azureml.net/workspaces/324b91422294411f9fa65d624cdd507c/webservices/88c2f89a72f14a539d529a319598f5aa/endpoints/patch"" rel=""nofollow noreferrer"">https://management.azureml.net/workspaces/324b91422294411f9fa65d624cdd507c/webservices/88c2f89a72f14a539d529a319598f5aa/endpoints/patch</a>.</p>
</blockquote>
","<azure><azure-data-factory><azure-machine-learning-service><ml-studio>","2018-12-02 17:34:13","106","1","1","55125514","<p>I don't know if you still have the error but if it is the case I think you should set your ML Update Resource as the following:</p>

<p>Go to your ML Update Resource / Settings / Trained Model : memory_train [trained model]</p>
"
"53548380","specify folder name during copy data task","<p>I have a Copy Data task that extracts a set of compressed files from an ""http file"" source task.</p>

<p>This Copy Data task uses an Azure Blob Storage linked connection that writes to a specific folder:
<a href=""https://i.stack.imgur.com/hL9Lh.png"" rel=""nofollow noreferrer"">blob link picture</a></p>

<p>The issue I'm running into is that this connection seems to create another folder beneath the path that the connection writes to with a GUID that appears to generate off the RunID for the <a href=""https://i.stack.imgur.com/R15nM.png"" rel=""nofollow noreferrer"">task</a>.</p>

<p>How do I specify what folder I want it to go to instead of writing to a random GUID?  I have another set of copy data tasks that's trying to look for the unzipped files in a folder but it can't find them each time because the GUID is always going to be different.</p>

<p>Thanks all.</p>
","<json><azure><azure-data-factory>","2018-11-29 22:11:52","119","1","2","53550434","<p>Based on your description, I think you could configure folder path dymatically in your blob storage dataset.</p>

<p>For example:</p>

<pre><code>""folderPath"": {
          ""value"": ""@concat('test1/',
                    formatDateTime(utcnow(),'yyyy'),'-', 
                    formatDateTime(utcnow(),'MM'),'-',
                    formatDateTime(utcnow(),'dd'))"",
           ""type"": ""Expression""
}
</code></pre>

<p>Then your files would be copied into <code>'yyyy-mm-dd'</code> folder which could be found by your another activity.</p>

<p><a href=""https://i.stack.imgur.com/YoUG7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YoUG7.png"" alt=""enter image description here""></a></p>
"
"53548380","specify folder name during copy data task","<p>I have a Copy Data task that extracts a set of compressed files from an ""http file"" source task.</p>

<p>This Copy Data task uses an Azure Blob Storage linked connection that writes to a specific folder:
<a href=""https://i.stack.imgur.com/hL9Lh.png"" rel=""nofollow noreferrer"">blob link picture</a></p>

<p>The issue I'm running into is that this connection seems to create another folder beneath the path that the connection writes to with a GUID that appears to generate off the RunID for the <a href=""https://i.stack.imgur.com/R15nM.png"" rel=""nofollow noreferrer"">task</a>.</p>

<p>How do I specify what folder I want it to go to instead of writing to a random GUID?  I have another set of copy data tasks that's trying to look for the unzipped files in a folder but it can't find them each time because the GUID is always going to be different.</p>

<p>Thanks all.</p>
","<json><azure><azure-data-factory>","2018-11-29 22:11:52","119","1","2","53550437","<p>Please don't set copy behavior. Then it will use the zip file name as the folder name. <a href=""https://i.stack.imgur.com/nWGMQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nWGMQ.png"" alt=""enter image description here""></a></p>
"
"53524681","using azure data factory to unzip multiple files from http site","<p>I have set up an ""http file"" data source in my ADF to connect to a specific URL (<a href=""https://www.sos.wa.gov/_assets/corps/txtCorpsData.zip"" rel=""nofollow noreferrer"">https://www.sos.wa.gov/_assets/corps/txtCorpsData.zip</a>) which points to a ZIP file that contains 4 separate .txt files.</p>

<p>The service successfully connects and unzips the file but it's only reading the first file in the ZIP archive.  How do I make the source separate out into 4 different individual sources?  I'm guessing there's some parameter I need to use but not sure what that could be.</p>

<p>Here's a screenshot of the connection detail:
<a href=""https://i.stack.imgur.com/yAg72.png"" rel=""nofollow noreferrer"">connection detail</a></p>
","<azure><zip><unzip><azure-data-factory>","2018-11-28 17:07:21","2003","0","2","53547118","<p>I think I figured it out, kinda:
Using a ""Copy Data"" task that points to an ""Http file"" as the source.
     That ""Http file"" source then looks at the url from my question as a Linked Service, the source also deflates the ZIP.
Within the ""Copy Data"" task the sink is a blob connection.</p>

<p>When I run this task it deflates the ZIP file into a new folder underneath the path at the blob connection.  This presents a new issue that I'm working on now, which is that the new folder creation appears to be whatever the GUID is for the pipeline running, somehow I need to figure out a way to specify the folder name so it's consistent...I'll likely post another question asking that later.</p>
"
"53524681","using azure data factory to unzip multiple files from http site","<p>I have set up an ""http file"" data source in my ADF to connect to a specific URL (<a href=""https://www.sos.wa.gov/_assets/corps/txtCorpsData.zip"" rel=""nofollow noreferrer"">https://www.sos.wa.gov/_assets/corps/txtCorpsData.zip</a>) which points to a ZIP file that contains 4 separate .txt files.</p>

<p>The service successfully connects and unzips the file but it's only reading the first file in the ZIP archive.  How do I make the source separate out into 4 different individual sources?  I'm guessing there's some parameter I need to use but not sure what that could be.</p>

<p>Here's a screenshot of the connection detail:
<a href=""https://i.stack.imgur.com/yAg72.png"" rel=""nofollow noreferrer"">connection detail</a></p>
","<azure><zip><unzip><azure-data-factory>","2018-11-28 17:07:21","2003","0","2","65592191","<p>in the sink settings set the copy behavior to None, so that you can retain the file name
<a href=""https://i.stack.imgur.com/1RCy2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1RCy2.png"" alt=""enter image description here"" /></a></p>
"
"53518485","Copy output data from Custom Activity Powershell script","<p>I have created a Custom Activity on ADF v2 that runs a Powershell script running the command “powershell .\script.ps1”.</p>

<p>Actually, the output of the script is saved on “StaName/adfjobs/activityRunId/stdout.txt” but I need to store the file in another container on the same StorageAccount, for exemple “StaName/outputs/stdout.txt”.</p>

<p>¿What’s the best way to perform this?
Create a Copy activity to copy the file? or there’s some method to send data directly to “StaName/outputs/” through the powershell script?</p>
","<azure><powershell><azure-data-factory>","2018-11-28 11:32:27","737","0","1","53530843","<p>You can create a copy activity to copy output file to the other container.</p>

<p>You can find my answer here <a href=""https://social.msdn.microsoft.com/Forums/en-US/5ceda984-874e-417b-8a28-7a512ede61d0/custom-activity-output-values-for-use-in-sequential-task?forum=AzureDataFactory"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/en-US/5ceda984-874e-417b-8a28-7a512ede61d0/custom-activity-output-values-for-use-in-sequential-task?forum=AzureDataFactory</a> to read from the script blob.</p>
"
"53517318","Python script in data factory","<p>I want to run a Python script in a Data Factory activity to transform a blob and store the result in a new blob. Is there a recommended way for doing this? Custom activity or Databricks Python activity, or something else? I can't find any good examples on Python custom activities.</p>
","<azure-data-factory>","2018-11-28 10:29:56","1450","0","1","53537682","<p>using the Custom activity or Databricks Python activity depends on where the python scripts is stored. The Azure Databricks Python Activity could runs a Python file in your Azure Databricks cluster, the Custom activity runs the python file in an Azure storage linked service.</p>

<p>The below two links give the elaborate introduction to these two activities, please go over and have a look.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-databricks-python"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-databricks-python</a>
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity</a></p>
"
"53499158","U-SQL Join failing","<p>Hi I am trying to join using U-SQL and the three input files are text file now I am getting error. I am new into these.
When I am trying to join then it is throwing below error. I have already tried with this: USING <code>Extractors.Csv(encoding:System.Text.Encoding.GetEncoding(""Windows-1252""));</code> but same error.
Could you please help to fix the error :</p>

<pre><code>{
    ""errorCode"": ""2703"",
    ""message"": ""Error Id: E_CSC_USER_SYNTAXERROR, Error Message: syntax error. Expected one of: identifier quoted-identifier variable . Error Id: E_CSC_USER_RESERVEDKEYWORDASIDENTIFIER, Error Message: Reserved keyword FROM is used as an identifier.. Error Id: E_CSC_USER_SYNTAXERROR, Error Message: syntax error. Expected one of: '(' ANTISEMIJOIN BROADCASTLEFT BROADCASTRIGHT CROSS EXCEPT FULL FULLCROSS GROUP HASH HAVING INDEXLOOKUP INNER INTERSECT JOIN LEFT LOOP MERGE ON OPTION ORDER OUTER OUTER UNION PAIR PIVOT PRESORT RIGHT SAMPLE SEMIJOIN SERIAL UNION UNPIVOT WHERE WITH ';' ')' ',' . "",
    ""failureType"": ""UserError"",
    ""target"": ""U-SQL1""
}
</code></pre>

<p>U-SQL Script</p>

<pre><code>@customerData = EXTRACT 
    Customerid string,
    Name string,
    City string,
    State string,
    Country string,
    Account_Created string
FROM ""/a_test/customer_data/[dbo].[customerData].txt""
USING Extractors.Csv(skipFirstNRows:1);


@productData = EXTRACT 
    Product string,
    Price string,
FROM ""/a_test/product_data/[dbo].[productData].txt""
USING Extractors.Csv(skipFirstNRows:1);


@transactionData = EXTRACT 
    Transaction_date string,
    Product string,
    Payment_Type string,
    Customerid string,
    Name string,
    City string,
    Account_Created string,
    Last_Login string,
    Latitude string,
    Longitude string
FROM ""/a_test/transaction_data/[dbo].[transactionData].txt""
USING Extractors.Csv(skipFirstNRows:1);

@result = 
SELECT 
    t.Customerid,
    t.Transaction_date,
    t.Product,
    t.Payment_Type,
    t.Name,
    t.City,
    c.State,
    c.Country,
    p.Price,
    t.Latitude,
    t.Longitude
  FROM @transactionData AS t
  INNER JOIN @productData AS p on t.Product = p.Product
  INNER JOIN @customerData AS c on t.Customerid = c.Customerid
  order by t.Customerid;

OUTPUT @result TO ""/a_test/final_output""
USING Outputters.Csv();
</code></pre>
","<azure-data-factory><u-sql>","2018-11-27 11:56:58","278","0","1","53500079","<pre><code>@productData = EXTRACT 
Product string,
Price string,
FROM ""/a_test/product_data/[dbo].[productData].txt""
USING Extractors.Csv(skipFirstNRows:1);
</code></pre>

<p>You have a comma (,) after Price string column, you should delete that one.</p>
"
"53477783","How to standardize the output of USQL to have data for all the columns when converted from JSON","<p><strong>How to standardize the output of USQL to have data for all the columns when converted from JSON</strong></p>

<p>We have a requirement to standardize the output of USQL. The USQL reads the JSON (source file) data and convert it to csv format. The problem is that the number of columns we have for each row in the csv is not the same because of missing data in the JSON. Sometimes the result set of USQL have a row in csv with ""N"" columns, another row is with ""N+1"" columns (cells). We would like to standardize the output, having the same number columns in csv for all the rows. How do we achieve this? We don't have any control over source file data, we would need to do the standardization while processing. Has anyone faced similar challenges and found a solution? Thanks for your help!</p>

<p><strong>Input details :</strong></p>

<pre><code>{""map"": {""key1"": 100,    ""key2"": 101,    ""key3"": 102},  ""id"": 2,  ""time"": 1540300241230}
{""map"": {""key1"": 200,    ""key2"": 201,    ""key3"": 202    ""key4"": 203},  ""id"": 2,  ""time"": 1540320246930}
{""map"": {""key1"": 300,    ""key3"": 301,    ""key4"": 303},  ""id"": 2,  ""time"": 1540350246930}
</code></pre>

<p><strong>Once the above JSON is converted to CSV based on some calculation</strong> </p>

<p><em>Output as is which is not correct</em></p>

<h2>key1, key2, key3, key4</h2>

<pre><code>100, 101, 102
200, 201, 202, 203
300, 301, 303
</code></pre>

<p>Value ""301"" is not associated with the key2</p>

<p><strong>Output expected -  # is the default for missing column values</strong></p>

<h2>key1, key2, key3, key4</h2>

<pre><code>100, 101, 102,  #
200, 201, 202, 203
300, #, 301, 303
</code></pre>

<p>Later all the headings ( key1, key2..) will be replaced with actual header names ( Pressure, Velocity...etc)</p>
","<json><azure><u-sql><azure-data-factory>","2018-11-26 09:09:41","99","0","1","53686058","<p>USE DATABASE [ADLSDB];</p>

<p>DECLARE EXTERNAL @INPUT_FILE string = ""/adlspath/keyValue.txt"";
DECLARE EXTERNAL @PIVOT_FILE string = ""/adlspath/pivot.txt"";</p>

<p>/* The meta data about the request starts - contents of the file request.json */</p>

<p>@requestData = EXTRACT id int, timestamp string, key string, value int FROM @INPUT_FILE USING Extractors.Csv();
@data  = SELECT id AS id, timestamp AS timestamp, key AS key, value AS value FROM @requestData;</p>

<p>DECLARE EXTERNAL @ids string = ""key1,key2,key3,key4""; //""external declaration""</p>

<p>@result = SELECT * FROM (SELECT id, timestamp, key, value FROM @data ) 
AS D PIVOT(SUM(value) FOR key IN(@ids AS heading)) AS P;</p>

<p>OUTPUT @result  TO @PIVOT_FILE USING Outputters.Csv(quoting:false, outputHeader:false);</p>

<p>I was able to get close to the solution by using the above code, however I am stuck at passing multiple values to the IN clause. The list of @ids, I will get at compile time of the USQL, but passing it as a comma separated  scalar variable does not produce the result. If I pass only one value ( assume key1) then the IN condition matches and output the rows for Key1. Anyone knows how to pass multiple values to IN clause in USQL PIVOT function.</p>

<p>------Updated------------</p>

<p>We were able to solve the problem by using dynamic USQL. One USQL will write the USQL statements to the output in required format. Then another data factory activity will read the dynamically generated USQL.</p>
"
"53474791","Cannot create connection in Azure Data Factory due to access issue","<p>I was given Owner access of an Azure data factory resource. I have to copy a csv from Blob Storage to Azure Table. While creating a copy data pipeline, I cannot create a connection to that csv file. A similar pipeline was successfully working in my free subscription.
I also have owner access of the Blob Storage. Even when I press test connection, it returns ""connection successful"" </p>

<p>The error that I get is:
The client  with object id  does not have authorization to perform action 'Microsoft.Resources/deployments/write' over scope .</p>

<p>I have added  in place of different ids in the error message.</p>
","<connection><roles><azure-data-factory><owner>","2018-11-26 04:29:11","1693","0","2","53484291","<p>The fact that the error says Write, makes me think that the issue is maybe with the Azure Table resource's permissions. These kind of errors are easy to troubleshoot now, because in the graphical interface you can click the Check Connection button.</p>

<p>Try going to the dataset you've created for this csv file, and click on Preview Data. If it shows data, then the problem is not in the connection to the csv file, and most likely it is with Azure Table.</p>

<p>Hope this helped!</p>
"
"53474791","Cannot create connection in Azure Data Factory due to access issue","<p>I was given Owner access of an Azure data factory resource. I have to copy a csv from Blob Storage to Azure Table. While creating a copy data pipeline, I cannot create a connection to that csv file. A similar pipeline was successfully working in my free subscription.
I also have owner access of the Blob Storage. Even when I press test connection, it returns ""connection successful"" </p>

<p>The error that I get is:
The client  with object id  does not have authorization to perform action 'Microsoft.Resources/deployments/write' over scope .</p>

<p>I have added  in place of different ids in the error message.</p>
","<connection><roles><azure-data-factory><owner>","2018-11-26 04:29:11","1693","0","2","53511978","<p>It was a problem with the roles. To create child objects in Data Factory web tool, one needs the ""Data Factory Contributor"" role for the whole resource group, not just for the resource itself.</p>
"
"53471377","dataFactory V2 - Wildcards","<p>I am trying to move &amp; decompress data from Azure Data Lake Storage Gen1. 
I have a couple of files with "".tsv.gz"" extension, and I want to decompress and move them to a different folder, which is in the same data lake.
I've tried to use the wildcard ""*.tsv.gz"" inside the connection configuration, so I can make this process at once.</p>

<p>Am I making some mistake?</p>

<p><a href=""https://i.stack.imgur.com/ACU6a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ACU6a.png"" alt=""enter image description here""></a></p>

<p>Thanks</p>
","<azure-data-lake><azure-data-factory>","2018-11-25 20:00:36","1403","0","1","53488033","<p>Just tested it, you should just use:</p>

<pre><code>*.tsv.gz
</code></pre>

<p>Without ' or ""</p>

<p>Hope this helped!</p>

<p>PS: also, remember to check the ""Copy file recursively"" when you select the dataset in the pipeline.</p>
"
"53433123","Azure Data Factory v2 - wrong year copying from parquet to SQL DB","<p>I'm having a weird issue with Azure Data Factory v2. There's a Spark Job which is running and producing parquet files as output, an ADFv2 copy activity then takes the output parquet and copies the data into an Azure SQL Database. All is working fine except for dates! When the data lands in SQL the year is 1969 years out. So todays date (2018-11-22) would land as 3987-11-22.</p>

<p>I've tried changing the source and destination types between Date, DateTime, DateTimeOffset and String but with no success. At the moment I'm correcting the dates in the database but this is not really ideal.</p>

<p>I've opened the source parquet files using Parquet Viewer, Spark and Python (desktop) and they all correctly show the year as 2018</p>
","<azure><azure-sql-database><parquet><azure-data-factory>","2018-11-22 14:28:38","1867","4","2","53455323","<p>According to parquet date type definition, 
<a href=""https://drill.apache.org/docs/parquet-format/#sql-types-to-parquet-logical-types"" rel=""nofollow noreferrer"">https://drill.apache.org/docs/parquet-format/#sql-types-to-parquet-logical-types</a>
The date is stored as ""the number of days from the Unix epoch, <strong>1 January 1970</strong>""</p>

<p>And ADF is using .net type doing the transformation. According to .net type definition, Time values are measured in 100-nanosecond units called ticks. A particular date is the number of ticks since 12:00 midnight, <strong>January 1, 000</strong>1 A.D. (C.E.) 
<a href=""https://learn.microsoft.com/en-us/dotnet/api/system.datetime?view=netframework-4.7.2"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/dotnet/api/system.datetime?view=netframework-4.7.2</a></p>

<p>Seems extra 1969 is added for this reason. But not sure whether is this a bug. What is your parquet data type? is it Date? and what is the sql data type?
Could you provide the copy activity run id? Or maybe some parquet sample data?</p>
"
"53433123","Azure Data Factory v2 - wrong year copying from parquet to SQL DB","<p>I'm having a weird issue with Azure Data Factory v2. There's a Spark Job which is running and producing parquet files as output, an ADFv2 copy activity then takes the output parquet and copies the data into an Azure SQL Database. All is working fine except for dates! When the data lands in SQL the year is 1969 years out. So todays date (2018-11-22) would land as 3987-11-22.</p>

<p>I've tried changing the source and destination types between Date, DateTime, DateTimeOffset and String but with no success. At the moment I'm correcting the dates in the database but this is not really ideal.</p>

<p>I've opened the source parquet files using Parquet Viewer, Spark and Python (desktop) and they all correctly show the year as 2018</p>
","<azure><azure-sql-database><parquet><azure-data-factory>","2018-11-22 14:28:38","1867","4","2","53477294","<p>Based on <a href=""https://github.com/apache/parquet-format/blob/master/Encodings.md"" rel=""nofollow noreferrer"">Parquet encoding definitions</a>,no <code>Date, DateTime, DateTimeOffset and String</code> format exist,so you do not need to try with these formats.</p>

<p>Based on this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#data-type-mapping-for-parquet-files"" rel=""nofollow noreferrer"">Data type mapping for Parquet files</a> in Azure Data Factory:</p>

<p><a href=""https://i.stack.imgur.com/BenLI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BenLI.png"" alt=""enter image description here""></a></p>

<p>The <code>DateTimeOffset</code> format corresponds to <code>Int96</code>,I suggest you trying this transmission on the source of parquet file.</p>
"
"53425938","Connection problem Azure Managed Instance and Data Factory with Windows authentication","<p>I have a Azure SQL Managed Instance and a Azure Data Factory V2.<br/><br/> 
Now I want to get a connection with Windows Authentication between a SQL database and ADF V2.<br/>I added a Active Directory User with the ""CREATE USER ... FROM EXTERNAL PROVIDER"" statement to the Managed Instance.<br/>
I tried it multiple times, but I don't get any connection.<br/>Only this error message: ""code:9056, message:Cannot connect to SQL Server"".<br/>
I also tried a AD user that is AD admin at this server, same result.
<br/><br/>
When I try the same user with SSMS and authentication mode ""Active Directory - Password"", it works perfectly.</p>
","<azure><windows-authentication><azure-data-factory><azure-sql-managed-instance>","2018-11-22 07:34:49","763","0","1","53455337","<p>Please install a selfhosted IR and use that to access your managed instance.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime</a></p>
"
"53425870","How to achieve dynamic columnmapping in azure data factory when Dynamics CRM is used as sink","<p>I have a requirement where i need to pass the column mapping dynamically from a stored procedure to the copy activity. This copy activity will perform update operation in Dynamics CRM. The source is SQL server (2014) and sink is Dynamics CRM.</p>

<p>I am fetching the column mapping from a stored procedure using look up activity and passing this parameter to copy activity.'</p>

<p>When i directly provide the below mentioned json value as default value to the parameter, the copy activity is updating the mapped fields correctly.</p>

<p>{""type"":""TabularTranslator"",""columnMappings"":{""leadid"":""leadid"",""StateCode"":""statecode""}}</p>

<p>But when the json value is fetched from the SP , it is not working . I am getting the error ColumnName is read only. </p>

<p>Please suggest if any conversion is required on the output of the loopup activity before passing the parameter to copy activity. Below is the output of the lookup activity.</p>

<p>{\""type\"":\""TabularTranslator\"",\""columnMappings\"":{\""leadid\"":\""leadid\"",\""StateCode\"":\""statecode\""}}</p>

<p>Appreciate a quick turnaround.</p>
","<azure><azure-data-factory><dynamics-365>","2018-11-22 07:30:35","1786","1","1","53426337","<p>Using parameter directly and Using lookup output are different. can you share how did you write the parameter from the output of lookup actvitiy.
you can refer to this doc <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity</a></p>
"
"53424666","Unable to load DLL jvm.dll The specified module could not be found Azure Data Factory V2 error","<p>We are trying to copy data from on-premise SQL Server to Azure Blob storage. But we are facing below error :</p>

<blockquote>
  <p>Activity Copy_741 failed: Failure happened on 'Sink' side. ErrorCode=UserErrorJreNotFound,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Java Runtime Environment could not be found.,Source=Microsoft.DataTransfer.Common,''Type=System.DllNotFoundException,Message=Unable to load DLL 'jvm.dll': The specified module could not be found. (Exception from HRESULT: 0x8007007E),Source=Microsoft.DataTransfer.Richfile.HiveOrcBridge,'</p>
</blockquote>

<p>On some other post, we found we need to install Microsoft Visual C++ 2010 Redistributable Package on Integration Runtime node but the latest version is already installed.</p>

<p>We are using Azure data factory v2. Also, we are able to transfer data from Azure SQL Server to Blob without any problem</p>
","<azure><azure-blob-storage><azure-data-factory>","2018-11-22 05:54:38","6460","1","2","53425753","<p>It might be related to <a href=""https://www.java.com/en/download/help/download_options.xml"" rel=""nofollow noreferrer"">Java</a> not being installed on the relevant machine.</p>

<p>Hope it helps!</p>
"
"53424666","Unable to load DLL jvm.dll The specified module could not be found Azure Data Factory V2 error","<p>We are trying to copy data from on-premise SQL Server to Azure Blob storage. But we are facing below error :</p>

<blockquote>
  <p>Activity Copy_741 failed: Failure happened on 'Sink' side. ErrorCode=UserErrorJreNotFound,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Java Runtime Environment could not be found.,Source=Microsoft.DataTransfer.Common,''Type=System.DllNotFoundException,Message=Unable to load DLL 'jvm.dll': The specified module could not be found. (Exception from HRESULT: 0x8007007E),Source=Microsoft.DataTransfer.Richfile.HiveOrcBridge,'</p>
</blockquote>

<p>On some other post, we found we need to install Microsoft Visual C++ 2010 Redistributable Package on Integration Runtime node but the latest version is already installed.</p>

<p>We are using Azure data factory v2. Also, we are able to transfer data from Azure SQL Server to Blob without any problem</p>
","<azure><azure-blob-storage><azure-data-factory>","2018-11-22 05:54:38","6460","1","2","53620175","<p>Converting to ORC files requires the Java Runtime Environment to be installed on your self-hosted integration runtime. If you have already double-checked that you have the JRE and the C++ 2010 Redistributable installed, here are some other things to check: </p>

<ul>
<li>Confirm that your copy activity is using the correct integration runtime in ADF.</li>
<li>Double-check that the IR and JRE match bit-wise (e.g., both 64-bit)</li>
<li>Check that JAVA_HOME is set correctly in the environment variables</li>
<li>Check the registry key – HKEY_LOCAL_MACHINE\Software\JavaSoft\Java Runtime Environment should have a Current Version entry that shows the current JRE version</li>
</ul>
"
"53416883","Unable to export 'Microsoft.DataFactory/factories' schema using powershell","<p>Am trying to export ARM template for a resourcegroup where in i have Azure data factory, but when using ""Export-AzureRmResourceGroup"" it throws a warning as below and none of ADF schema is downloaded in that template. is there a way to download ADF template using powershell??</p>

<pre><code>Export-AzureRmResourceGroup -ResourceGroupName ****************
WARNING: ExportTemplateCompletedWithErrors : Export template operation completed with errors. Some resources were not exported. Please see details for more information.
WARNING: ResourceTypeSchemaNotFound : The schema of resource type 'Microsoft.Databricks/workspaces' is not available. Resources of this type will not be exported to the template.
WARNING: ResourceTypeSchemaNotFound : The schema of resource type '**Microsoft.DataFactory/factories**' is not available. Resources of this type will not be exported to the template.
WARNING: ResourceTypeSchemaNotFound : The schema of resource type 'Microsoft.Portal/dashboards' is not available. Resources of this type will not be exported to the template.
</code></pre>
","<powershell><azure-data-factory><azure-rm-template>","2018-11-21 16:47:38","1115","0","4","53424129","<p>If all you need is the template, and if you are trying to download via PowerShell, then I assume you have access to that RG.So navigate to <code>Azure Portal-&gt;Resource Group</code> ( that has the ADF or any resource that you need. From the left nav bar , you'll find this <code>Automation Script</code><a href=""https://i.stack.imgur.com/aolgB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aolgB.png"" alt=""enter image description here""></a> . </p>

<p>Click on it and the template will appear. From here, you can <code>download</code> the template directly</p>
"
"53416883","Unable to export 'Microsoft.DataFactory/factories' schema using powershell","<p>Am trying to export ARM template for a resourcegroup where in i have Azure data factory, but when using ""Export-AzureRmResourceGroup"" it throws a warning as below and none of ADF schema is downloaded in that template. is there a way to download ADF template using powershell??</p>

<pre><code>Export-AzureRmResourceGroup -ResourceGroupName ****************
WARNING: ExportTemplateCompletedWithErrors : Export template operation completed with errors. Some resources were not exported. Please see details for more information.
WARNING: ResourceTypeSchemaNotFound : The schema of resource type 'Microsoft.Databricks/workspaces' is not available. Resources of this type will not be exported to the template.
WARNING: ResourceTypeSchemaNotFound : The schema of resource type '**Microsoft.DataFactory/factories**' is not available. Resources of this type will not be exported to the template.
WARNING: ResourceTypeSchemaNotFound : The schema of resource type 'Microsoft.Portal/dashboards' is not available. Resources of this type will not be exported to the template.
</code></pre>
","<powershell><azure-data-factory><azure-rm-template>","2018-11-21 16:47:38","1115","0","4","53505580","<p>This is because those ""ResourceTypeSchemas"" are not available to use in ARM templates. It doesn't matter if you go into the portal and try to get the resource template. It will show the same error there as well.</p>
"
"53416883","Unable to export 'Microsoft.DataFactory/factories' schema using powershell","<p>Am trying to export ARM template for a resourcegroup where in i have Azure data factory, but when using ""Export-AzureRmResourceGroup"" it throws a warning as below and none of ADF schema is downloaded in that template. is there a way to download ADF template using powershell??</p>

<pre><code>Export-AzureRmResourceGroup -ResourceGroupName ****************
WARNING: ExportTemplateCompletedWithErrors : Export template operation completed with errors. Some resources were not exported. Please see details for more information.
WARNING: ResourceTypeSchemaNotFound : The schema of resource type 'Microsoft.Databricks/workspaces' is not available. Resources of this type will not be exported to the template.
WARNING: ResourceTypeSchemaNotFound : The schema of resource type '**Microsoft.DataFactory/factories**' is not available. Resources of this type will not be exported to the template.
WARNING: ResourceTypeSchemaNotFound : The schema of resource type 'Microsoft.Portal/dashboards' is not available. Resources of this type will not be exported to the template.
</code></pre>
","<powershell><azure-data-factory><azure-rm-template>","2018-11-21 16:47:38","1115","0","4","58400230","<p>As mentioned <a href=""https://social.msdn.microsoft.com/Forums/azure/en-US/06b0a24a-c088-4ab1-ace0-5957bb4c2440/data-factory-resource-provider-wont-create-template-from-resource-group?forum=AzureDataFactory"" rel=""nofollow noreferrer"">there</a>, DataFactory template export is not supported yet.  </p>

<p>BUT, there is a solution which we've used in our CI/CD.  </p>

<p>First, we've created a dummy generic template for the datafactory (see the steps below), and then used either ARM template exported from the datafactory template or adf_publish (see the notes below) branch to update and finalize the datafactory.</p>

<p>To create a dummy datafactory template:</p>

<ol>
<li>On the portal try to create a dummy datafactory, and click on ""Automation options"" (<strong>instead</strong> of ""Create"")</li>
</ol>

<p><a href=""https://i.stack.imgur.com/XBVLY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XBVLY.png"" alt=""enter image description here""></a></p>

<ol start=""2"">
<li>This will create an empty datafactory ARM template, which then you can use for your pipelines. Now just click on the ""Download"" button and store the dummy template somewhere.</li>
</ol>

<p><a href=""https://i.stack.imgur.com/kuqxY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kuqxY.png"" alt=""enter image description here""></a></p>

<ol start=""3"">
<li>Afterwards you can use that template to create a new dummy datafactory if it doesn't exist, and then update it with the real ARM templates provided by either the ""Export ARM Template"" button inside the datafactory, or the adf_publ</li>
</ol>

<p><strong>Notes</strong>:<br>
1. We came to this solution as the ARM templates which were provided* by the azure portal, were <strong>not</strong> including the Datafactory resource itself, because of which deployment to a new Resource Group was <strong>FAILING</strong>.<br>
So, we had to create a dummy DataFactory first, and then update it with actual DataFactory template.<br>
2. By saying ""provided* by the azure portal"" we mean the ARM templates which are provided when you open the DF and click on ""ARM Template"" > ""Export ARM template"", or the one published in adf_publish branch
3. For creating CI/CD we used the other steps mentioned <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">there</a></p>
"
"53416883","Unable to export 'Microsoft.DataFactory/factories' schema using powershell","<p>Am trying to export ARM template for a resourcegroup where in i have Azure data factory, but when using ""Export-AzureRmResourceGroup"" it throws a warning as below and none of ADF schema is downloaded in that template. is there a way to download ADF template using powershell??</p>

<pre><code>Export-AzureRmResourceGroup -ResourceGroupName ****************
WARNING: ExportTemplateCompletedWithErrors : Export template operation completed with errors. Some resources were not exported. Please see details for more information.
WARNING: ResourceTypeSchemaNotFound : The schema of resource type 'Microsoft.Databricks/workspaces' is not available. Resources of this type will not be exported to the template.
WARNING: ResourceTypeSchemaNotFound : The schema of resource type '**Microsoft.DataFactory/factories**' is not available. Resources of this type will not be exported to the template.
WARNING: ResourceTypeSchemaNotFound : The schema of resource type 'Microsoft.Portal/dashboards' is not available. Resources of this type will not be exported to the template.
</code></pre>
","<powershell><azure-data-factory><azure-rm-template>","2018-11-21 16:47:38","1115","0","4","74875659","<p>You get exactly the same thing when you are trying to do via Azure directly:
<a href=""https://i.stack.imgur.com/SCFoA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SCFoA.png"" alt=""Data factory ARM template generation"" /></a></p>
<p>However there's a template defined by Microsoft for almost any object, which you can access <a href=""https://learn.microsoft.com/en-us/azure/templates/microsoft.datafactory/factories?pivots=deployment-language-arm-template"" rel=""nofollow noreferrer"">in here</a>, so if you're starting from scratch, this can be a good starting point to create your template.</p>
<p>If you already have something defined in your existing data factory, then you just need to export ARM template directly from your data factory and merge extracted template with the one from Microsoft documentation.</p>
<p><a href=""https://i.stack.imgur.com/CZe3b.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CZe3b.png"" alt=""how to export already existing data factory arm template"" /></a></p>
<p>For this to work you need to include two nodes, because the yare not currently included in this export functionality: for generating the data factory itself</p>
<pre><code>    {
      &quot;type&quot;: &quot;Microsoft.DataFactory/factories&quot;,
      &quot;apiVersion&quot;: &quot;2018-06-01&quot;,
      &quot;name&quot;: &quot;[parameters('dataFactoryName')]&quot;,
      &quot;location&quot;: &quot;[parameters('location')]&quot;,
      &quot;identity&quot;: {
        &quot;type&quot;: &quot;SystemAssigned&quot;
      }
    }
</code></pre>
<p>and you will also probably need a template for storage account</p>
<pre><code>    {
      &quot;type&quot;: &quot;Microsoft.Storage/storageAccounts&quot;,
      &quot;apiVersion&quot;: &quot;2021-08-01&quot;,
      &quot;name&quot;: &quot;[parameters('storageAccountName')]&quot;,
      &quot;location&quot;: &quot;[parameters('location')]&quot;,
      &quot;sku&quot;: {
        &quot;name&quot;: &quot;Standard_LRS&quot;
      },
      &quot;kind&quot;: &quot;StorageV2&quot;
    }
</code></pre>
"
"53415973","Can we set ADF parameters value or linked Service connection string in Custom TASK","<p>I want to dynamically change Connection string in Custom task and then want this to reflect in ADF pipeline? Is there a way I can set the pipeline Parameter value in Custom Code task and make my Connection String Parametrised in ADF pipeline? </p>

<p>Thanks</p>
","<azure-data-factory>","2018-11-21 16:00:09","387","0","1","53417098","<p>This feature is now supported by data factory, read more here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/parameterize-linked-services</a></p>

<p>Always think of the context where it will be running, for example if you reference a pipeline parameter from the linked service you will receive a warning. But if at runtime there is a pipeline that matches what you configured at the linked service, you will have no problems.</p>

<p>Hope this helped!</p>
"
"53412439","Azure Data Factory v2: Activity execute pipeline output","<p>Is there a way to reference the output of an executed pipeline in the activity &quot;Execute pipeline&quot;?</p>
<p>I.e.: master pipeline executes 2 pipelines in sequence. The first pipeline generates an own created run_id that needs to be forwarded as a parameter to the second pipeline.</p>
<p>I've read the documentation and checked that the master pipeline log the output of the first pipeline, but it looks like that this is not directly possible?</p>
<p>We've used until now only 2 pipelines without a master pipeline, but we want to re-use the logic more. Currently we have 1 pipeline that calls the next pipeline and forwards the run_id.</p>
<p>Update 2023-03-31
As of early 2023 we can have output from a pipeline, via using the newly introduced system variable 'Pipeline Return Value'.</p>
<p>Official documentation is here:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-return-value"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-return-value</a></p>
","<azure><azure-data-factory>","2018-11-21 12:50:59","12132","13","2","53415642","<p>The execute pipeline activity is just another activity with outputs that can be captured by other activities. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-pipeline-activity#type-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-pipeline-activity#type-properties</a></p>

<p>If you want to use the runId of the pipeline executed previosly, it would look like this:</p>

<pre><code>@activity('ExecutePipelineActivityName').output.pipeline.runId
</code></pre>

<p>Hope this helped!</p>
"
"53412439","Azure Data Factory v2: Activity execute pipeline output","<p>Is there a way to reference the output of an executed pipeline in the activity &quot;Execute pipeline&quot;?</p>
<p>I.e.: master pipeline executes 2 pipelines in sequence. The first pipeline generates an own created run_id that needs to be forwarded as a parameter to the second pipeline.</p>
<p>I've read the documentation and checked that the master pipeline log the output of the first pipeline, but it looks like that this is not directly possible?</p>
<p>We've used until now only 2 pipelines without a master pipeline, but we want to re-use the logic more. Currently we have 1 pipeline that calls the next pipeline and forwards the run_id.</p>
<p>Update 2023-03-31
As of early 2023 we can have output from a pipeline, via using the newly introduced system variable 'Pipeline Return Value'.</p>
<p>Official documentation is here:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-return-value"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-return-value</a></p>
","<azure><azure-data-factory>","2018-11-21 12:50:59","12132","13","2","62038291","<p>ExecutePipline currently cannot pass anything from its insides to its output.  You can only get the runID or name.</p>

<p>For some weird reason, the output of ExecutePipeline is returned not as a JSON object but as a string.  So if you try to select a property of output like this <code>@activity('ExecutePipelineActivityName').output.something</code> then you get this error:</p>

<p><em>Property selection is not supported on values of type 'String'</em></p>

<p>I found that I had to use the following to get the run ID:
<code>@json(activity('ExecutePipelineActivityName').output).pipelineRunId</code></p>
"
"53411576","pipeline fails for stored Procedure called in Copy Activity - Azure data factory V2","<p>We've a SQL server stored procedure which returns the incremental records. If there are no changes to the table, then nothing is returned.  Stored procedure does what is expected.</p>

<p>We're invoking the above said stored procedure via Copy activity in Azure data factory. It works fine for all the cases except when nothing (empty) is returned. </p>

<p>We are looking for an option, where Nothing(Empty) is returned from stored procedure, pipeline should skip and proceed further  and also mark the whole pipeline successful rather failed.</p>

<p>Thanks</p>
","<azure><azure-data-factory>","2018-11-21 11:59:13","2358","0","3","53418404","<p>Your stored procedure needs to end by a SELECT, so it returns something - including an empty set if there is no rows to return.</p>

<p>However, to skip the pipeline if there is no row, DraganB's last answer is pretty relevant, I had to do that a couple of time on my current project.</p>
"
"53411576","pipeline fails for stored Procedure called in Copy Activity - Azure data factory V2","<p>We've a SQL server stored procedure which returns the incremental records. If there are no changes to the table, then nothing is returned.  Stored procedure does what is expected.</p>

<p>We're invoking the above said stored procedure via Copy activity in Azure data factory. It works fine for all the cases except when nothing (empty) is returned. </p>

<p>We are looking for an option, where Nothing(Empty) is returned from stored procedure, pipeline should skip and proceed further  and also mark the whole pipeline successful rather failed.</p>

<p>Thanks</p>
","<azure><azure-data-factory>","2018-11-21 11:59:13","2358","0","3","53423210","<p>As @DraganB said in the comment, activities could run in the flow,so you could do <code>stored procedure activity --&gt; if activity --&gt; copy activity</code>.If the output of sp activity is empty,then don't run the copy activity and end the pipeline.</p>

<p>Another idea, maybe you could learn about azure function <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-external-table"" rel=""nofollow noreferrer"">external table trigger</a>. You could add a status column in your table such as <code>needToBeCopied</code>, every insert or update operation will change the column. Then filter the data which need to be copied while running the copy activity.</p>
"
"53411576","pipeline fails for stored Procedure called in Copy Activity - Azure data factory V2","<p>We've a SQL server stored procedure which returns the incremental records. If there are no changes to the table, then nothing is returned.  Stored procedure does what is expected.</p>

<p>We're invoking the above said stored procedure via Copy activity in Azure data factory. It works fine for all the cases except when nothing (empty) is returned. </p>

<p>We are looking for an option, where Nothing(Empty) is returned from stored procedure, pipeline should skip and proceed further  and also mark the whole pipeline successful rather failed.</p>

<p>Thanks</p>
","<azure><azure-data-factory>","2018-11-21 11:59:13","2358","0","3","53502996","<p>It got resolved. The real issue was Copy activity wasn't returning the correct error message. There was an issue with access control. </p>

<pre><code>Grant VIEW CHANGE TRACKING permission on a table to a user:
[sql]
GRANT VIEW CHANGE TRACKING ON OBJECT::dbo.databaselog to username
[/sql]
</code></pre>

<p>Incremental loading approach is through enabling Change Tracking on SQL Server Database and on required tables .
Azure data factory should have logged error as 'Insufficient permissions on so and so table'. Instead it failed the whole pipeline with error message as 'Stored procedure might be invalid or stored procedure doesn't return any output'.</p>

<p>Anyway, we assigned the right permissions and issue got resolved. Now, it creates an Empty file just with header record in it when there's no output returned from Stored Procedure likewise in - <a href=""https://social.msdn.microsoft.com/Forums/en-US/a57fb763-dc35-4144-96df-356acc3a692c/data-factory-avoiding-creation-of-empty-files?forum=AzureDataFactory"" rel=""nofollow noreferrer"">Data Factory Avoiding creation of empty files</a></p>
"
"53395750","How to pause data factory tumbling window trigger if error occurs","<p>I have a tumbling window trigger for a pipeline.  If a window fails I do not want any additional windows ran until the failed window is addressed.  What is the best way to handle this?</p>
","<azure-data-factory>","2018-11-20 14:58:33","527","0","1","53405733","<p>There is no native way to achieve this in tumbling window trigger, but ADF provides various control and transform activities for you to come up with a combination method:</p>

<p>Involve a flag stored in a table/file to indicate the pipeline running status in a window, after each window execution, update this flag, then check this flag before execute another window run. You may need a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">Lookup activity</a> to fetch the value of this flag, an <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-if-condition-activity"" rel=""nofollow noreferrer"">IF activity</a> to check the flag then a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">Custom</a>/<a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure"" rel=""nofollow noreferrer"">Stored Procedure</a> activity to update the value. HTH.</p>
"
"53389506","ETL using Azure Data Factory v2","<p>I just want to ask what is the proper approach and sample components to be used when you are performing an ETL and will be using Azure Data Factory v2. Will still there be a need to use SSIS or can Azure Data Factory handle all the processes in the pipelines to be used for an ETL solution ? </p>

<p>Thanks for answering everyone.</p>
","<azure><ssis><etl><azure-data-factory>","2018-11-20 09:04:03","377","1","2","53400298","<p>The answer to your question will be specific to your circumstances. Azure Data Factory is an orchestrator. As such, it doesn't ""do"" ETL*, rather it manages other services to do the work. The workers, therefore, be SSIS, or Stored Procedures, or U-SQL scripts, or Azure Batch, or any number of available activity types. So you do not <em>need</em> SSIS, but you can certainly still leverage it if it makes sense in your scenario.</p>

<p>*The exception is the ADF Copy activity that will allow you to specify a source and a sink and move data from one to the other.</p>
"
"53389506","ETL using Azure Data Factory v2","<p>I just want to ask what is the proper approach and sample components to be used when you are performing an ETL and will be using Azure Data Factory v2. Will still there be a need to use SSIS or can Azure Data Factory handle all the processes in the pipelines to be used for an ETL solution ? </p>

<p>Thanks for answering everyone.</p>
","<azure><ssis><etl><azure-data-factory>","2018-11-20 09:04:03","377","1","2","53404669","<p>If you're already familiar with SSIS/have existing SSIS ETL packages, you can simply lift &amp; shift them into SSIS in ADF (Azure-SSIS Integration Runtime).  If you don't want to use SSIS, ADF ETL/data flow feature will be introduced soon and is currently under preview.</p>
"
"53387960","How to Zip files on Storage with ADF","<p>Would there be a way to Zip/copy files on a blob-storage into another storage/folder but with a zipped result ?
You can deflate with ADF, but can you enflate ?
We receive +100k files a day, the next day I would like to zip them all into a archive folder.
Couldn't find a way ...
So any help/.hints appreciated.</p>
","<azure-storage><azure-data-factory>","2018-11-20 07:13:40","2184","0","1","53388209","<p>ADF supports zipdeflate format, which can archive multiple files into a single zip file. Refer to this doc: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#compression-support"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#compression-support</a>.</p>
"
"53385256","Azure Data Factory V2 - Parsing JSON HTTP GET Dataset","<p>I am trying to consume the API response below using the HTTP GET connector in Azure Data Factory V2. I am unable to get the Dataset properly parsed and I keep getting an error that the activity failed during deserialization as the data is not in proper JSON object nor array format. </p>

<p>I have tried to leverage JSONPath to isolate specific key-value pairs but I keep getting the same error. I have successfully done this with many other APIs but can't seem to figure this one out. Ultimately, I just want to create a table with a column date, and another one for Revenue, another one for Cost of Revenue, and so on.</p>

<p>Any ideas on the best way to parse this API response using Azure Data Factory V2? Thanks in advance.</p>

<pre><code> {
     ""AAPL"": {
         ""Revenue"": {
             ""2013-09"": ""170910"",
             ""2014-09"": ""182795"",
             ""2015-09"": ""233715"",
             ""2016-09"": ""215639"",
             ""2017-09"": ""229234"",
             ""TTM"": ""239176""
         },
         ""Cost of revenue"": {
            ""2013-09"": ""106606"",
            ""2014-09"": ""112258"",
            ""2015-09"": ""140089"",
            ""2016-09"": ""131376"",
            ""2017-09"": ""141048"",
            ""TTM"": ""147254""
         }
     } 
}
</code></pre>
","<json><azure><azure-data-factory>","2018-11-20 02:13:04","1823","1","1","53387028","<p>I think your issue is more of a data structure nature. The <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#json-format"" rel=""nofollow noreferrer"">JSON format</a> used when importing can be used with the <strong>jsonPathDefinition</strong> parameter to map paths as columns, the thing is, the formatter cannot perform a correlation between an item under <strong>Revenue</strong> and an item under <strong>Cost of revenue</strong> (meaning take item index 0 from both locations and create a single row for them).</p>

<p>I think you'll need to due some custom pre-processing on the <strong>JSON</strong> file before importing.</p>

<p>Hope it helps!</p>
"
"53358951","Copyind data from oracle to azure blob storage through azure data factory","<p>I have to copy data from oracle table to a csv file in azure blob. I used the copy activity component and add the proper configuration. But the problem is oracle data have some commas because of which the csv file generated does not contain proper data.Please help with this problem i am new to azure data factory  below is the link of my blob dataset properties.</p>

<p><a href=""https://i.stack.imgur.com/z7E4X.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/z7E4X.png</a></p>
","<azure><dataset><azure-blob-storage><azure-data-factory>","2018-11-18 08:02:14","655","1","2","53361329","<p>Actually I need some information from you. What is the delimiter used in your CSV file. Is it comma or some other like pipe. </p>

<p>If you have used comma as a delimiter then you need to use the replace function to replace the comma which is there in your data to prevent it from being delimited.</p>

<p>Use the below code to replace:</p>

<p><strong><em>Replace(name, "","" , "" "")</em></strong></p>

<p>/<em>Here name is the column name</em>/</p>

<p>If the delimiter is pipe then the data might contain pipe. You need to use the REPLACE function in the similar way.</p>

<p>Regards,</p>

<p>Pratik</p>
"
"53358951","Copyind data from oracle to azure blob storage through azure data factory","<p>I have to copy data from oracle table to a csv file in azure blob. I used the copy activity component and add the proper configuration. But the problem is oracle data have some commas because of which the csv file generated does not contain proper data.Please help with this problem i am new to azure data factory  below is the link of my blob dataset properties.</p>

<p><a href=""https://i.stack.imgur.com/z7E4X.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/z7E4X.png</a></p>
","<azure><dataset><azure-blob-storage><azure-data-factory>","2018-11-18 08:02:14","655","1","2","53777297","<p>Below image will solve your problem.
There is a property in advance tab of blob dataset called quote character.
please refer the below image</p>

<p><a href=""https://i.stack.imgur.com/ItgWO.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/ItgWO.png</a></p>
"
"53345297","Custom activities depends on activities from diferents pipelines in Azure Data Factory v2","<p>I have a custom activity in a project of Azure Data Factory v1 with 2 input datasets, these datasets are output of activities on different pipeline. The custom activity waits for these 2 activities to start. I have other custom activity on other pipeline with similar situation, with a common input dataset.</p>

<p>I want to do the same functionality on Azure Data Factory 2. But the custom activities don't have input datasets. </p>

<p>I read about the activity ExcecutePipeline and I think put 2 of them on the pipeline of the custom activity which invoke the pipeline when dependency activity is in. But if I do the same on the two pipelines where the custom activities are, the pipeline with the common activity of input dataset in custom activities run twice.</p>
","<azure-data-factory>","2018-11-16 20:59:37","313","-1","2","53350556","<p>Please take a look at the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities#activity-dependency"" rel=""nofollow noreferrer"">activity dependencies</a> in ADF v2. </p>

<p>And could you also elaborate a little bit more about your exact activity flow?</p>
"
"53345297","Custom activities depends on activities from diferents pipelines in Azure Data Factory v2","<p>I have a custom activity in a project of Azure Data Factory v1 with 2 input datasets, these datasets are output of activities on different pipeline. The custom activity waits for these 2 activities to start. I have other custom activity on other pipeline with similar situation, with a common input dataset.</p>

<p>I want to do the same functionality on Azure Data Factory 2. But the custom activities don't have input datasets. </p>

<p>I read about the activity ExcecutePipeline and I think put 2 of them on the pipeline of the custom activity which invoke the pipeline when dependency activity is in. But if I do the same on the two pipelines where the custom activities are, the pipeline with the common activity of input dataset in custom activities run twice.</p>
","<azure-data-factory>","2018-11-16 20:59:37","313","-1","2","53394624","<p>I found an answer here <a href=""https://social.msdn.microsoft.com/Forums/en-US/5e9d066a-6cb8-4fd0-b3d9-ac9ff08117a3/azure-data-factory-v2-activity-depends-on-activities-from-more-than-pipeline?forum=AzureDataFactory#5e9d066a-6cb8-4fd0-b3d9-ac9ff08117a3"" rel=""nofollow noreferrer"">Azure data factory v2 activity depends on activities from more than pipeline</a>. If I create a new pipeline and wrap the others pipeline here with the Execute Pipeline Activity and ""waitOnComplete"" activity, I can run the required logic.</p>
"
"53340791","Azure - Data Factory - New Pipeline Created","<p>is there any way to receive a mail (some kind of alert) when someone creates a new Pipeline in a Specific Data Factory?</p>

<p>Something like ""user XYZ created a new Pipeline""</p>

<p>Thanks for your inputs,
Marcelo</p>
","<azure><pipeline><azure-data-factory><audit>","2018-11-16 15:26:08","33","1","1","53352629","<p>To specify an alert definition, you create a JSON file describing the operations that you want to be alerted on.
Following example creates an alert for Run Completion.
Below JSON will help you to create similar for update alert. </p>

<pre>
   {
   ""contentVersion"": ""1.0.0.0"",
  ""$schema"": ""http://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json#"",
  ""parameters"": {},
  ""resources"": [
    {
      ""name"": ""ADFAlertsSlice"",
      ""type"": ""microsoft.insights/alertrules"",
      ""apiVersion"": ""2014-04-01"",
      ""location"": ""East US"",
      ""properties"": {
        ""name"": ""ADFAlertsSlice"",
        ""description"": ""One or more of the data slices for the Azure Data Factory has failed processing."",
        ""isEnabled"": true,
        ""condition"": {
          ""odata.type"": ""Microsoft.Azure.Management.Insights.Models.ManagementEventRuleCondition"",
          ""dataSource"": {
            ""odata.type"": ""Microsoft.Azure.Management.Insights.Models.RuleManagementEventDataSource"",
            ""operationName"": ""RunFinished"",
            ""status"": ""Failed"",
            ""subStatus"": ""FailedExecution""
          }
        },
        ""action"": {
          ""odata.type"": ""Microsoft.Azure.Management.Insights.Models.RuleEmailAction"",
          ""customEmails"": [
            ""@contoso.com""
          ]
        }
      }
    }
</pre>
"
"53340602","Azure Data Factory (SSIS) Execution Using Integration Runtime Throws ""Unexpected Termination""","<p>I have been using Azure Data Factory V2 for a while to execute SSIS packages from the SSISDB catalouges. 
Today (16-11-2018) I have encountered ""Unexpected Termination"" Failure message without any Warning and Error message. </p>

<p>Things than I have done:</p>

<ol>
<li>Executing the SSIS package manually from the SSISDB catalogue in SQL Server Management Services (SSMS).  What i have noticed is that it took an exceptionally long time to assign the task to a machine. When the package is assigned to a machine, within 1 or two minutes it throws back the Failure message.</li>
<li>There are 3 SSIS packages that is excecuted ""sequentially"" with the Azure Data Factory Pipeline. Often the 1st package is executed successfully, however the 2nd and 3rd package never succeded. </li>
<li>Another error message that I got is ""Failed pull task from SSISDB, please check if SSISDB has exceeded its limit"". </li>
</ol>

<p>I hope anyone can help me with this issue. I have been searching the web and could not find anything on this subject. </p>
","<azure><ssis><runtime><integration><azure-data-factory>","2018-11-16 15:16:00","394","0","1","53401621","<p>What tier of Azure sql server have you provisioned for the SSISDB to run on? If its too small, it may be taking too much time starting and throw a timeout. </p>

<p>Personally, I've had no problems provisioning an S3 Azure Sql Server.</p>

<p>Hope this helped!
Martin</p>
"
"53339725","Powershell to download ARM template for Azure data factory Pipeline","<p>i have a requirement to create an ADF pipeline using ARM template in powershell and it has to take inputs/validate few things from existing ADF piepline, for that reason i have to download the ARM tempalte for existing ADF pipeline through powershell. Can we do that for single ADF pipeline or multiple ones?
Note: existing pipeline is not created through ARM deployment, so i cant use ""Save-AzureRmDeploymentTemplate"" as i dont have deployment name created when pipeline is created through portal..</p>

<p>Any help is really appreciated.</p>
","<azure><powershell><azure-data-factory><azure-rm-template>","2018-11-16 14:24:21","1127","1","1","53350434","<p>Maybe you want to take a look at <a href=""https://learn.microsoft.com/en-us/powershell/module/azurerm.resources/export-azurermresourcegroup?view=azurermps-6.12.0"" rel=""nofollow noreferrer"">Export-AzureRmResourceGroup</a>. 
But I guess you can only export the entire resource group which may contain other things. You need put your data factory in a special resource group if you only want to export ADF.</p>

<p>Another way is to export arm template in ADF UI.</p>
"
"53321370","Azure Data Factory throws 'Length Required"" error on copy from SQL to ADLS","<p>I am trying to copy data from on-prem SQL server to Azure Data Lake Storage (ADLS) via Azure Data Factory (ADF). Everything seems to work, except when I run (debug or trigger) the pipeline, I get the error:</p>

<blockquote>
  <p>{
      ""errorCode"": ""2200"",
      ""message"": ""Failure happened on 'Sink' side. ErrorCode=UserErrorAdlsFileWriteFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Writing to 'AzureDataLakeStore' failed. Message: The remote server returned an error: (411) Length Required.. Response details: \r\nLength Required\r\n\r\n<h2>Length Required</h2>\r\n<hr><p>HTTP Error 411. The request must be chunked or have a content length.</p>\r\n\r\n,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Net.WebException,Message=The remote server returned an error: (411) Length Required.,Source=System,'"",
      ""failureType"": ""UserError"",
      ""target"": ""CopyData1""
  }</p>
</blockquote>

<p>What is really odd, is that the following pipelines DO work:</p>

<ul>
<li>SQL tbl1 -> SQL tbl2</li>
<li>ADLS source.txt -> ADLS sink.txt</li>
</ul>

<p>I.e. read/write access works as expected. The latter pipeline is also able to create/overwrite the sink.txt file.</p>

<p>But when I run the pipeline</p>

<ul>
<li>SQL tbl1 -> sink.txt</li>
</ul>

<p>I get the <strong>Length Required</strong> error. And if sink.txt exists, the pipeline even deletes it!</p>

<p>I'm using ADFv2, ADLS Gen1, ADF &amp; ADLS resides in the same subscription/resource group, using selfhosted/Azure Integration Runtime (for SQL / ADLS respectively).
I have tested with source statement as simple as ""SELECT 1 Col"". Also tested without dataset schema, and with schemas+mappings.</p>

<p>Is this a bug, or am I missing something?
Which “Length” is required?</p>

<hr>

<h2>EDIT 1: Minimal JSON scripts</h2>

<p>pipeline1.json</p>

<pre><code>{
    ""name"": ""pipeline1"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""CopyData1"",
                ""type"": ""Copy"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""SqlSource"",
                        ""sqlReaderQuery"": ""SELECT TOP 1 'x' AS col1 FROM sys.tables""
                    },
                    ""sink"": {
                        ""type"": ""AzureDataLakeStoreSink""
                    },
                    ""enableStaging"": false,
                    ""dataIntegrationUnits"": 0
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""table1"",
                        ""type"": ""DatasetReference""
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""sink1"",
                        ""type"": ""DatasetReference""
                    }
                ]
            }
        ]
    }
}
</code></pre>

<p>table1.json</p>

<pre><code>{
    ""name"": ""table1"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""SqlServer1"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""SqlServerTable"",
        ""typeProperties"": {
            ""tableName"": ""sys.tables""
        }
    }
}
</code></pre>

<p>sink1.json</p>

<pre><code>{
    ""name"": ""sink1"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureDataLakeStore1"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""AzureDataLakeStoreFile"",
        ""structure"": [
            {
                ""name"": ""col1"",
                ""type"": ""String""
            }
        ],
        ""typeProperties"": {
            ""format"": {
                ""type"": ""TextFormat"",
                ""columnDelimiter"": "","",
                ""rowDelimiter"": """",
                ""nullValue"": ""\\N"",
                ""treatEmptyAsNull"": true,
                ""skipLineCount"": 0,
                ""firstRowAsHeader"": true
            },
            ""fileName"": ""sink1.txt"",
            ""folderPath"": ""myDir""
        }
    }
}
</code></pre>

<hr>

<h2>EDIT 2: Summary of conducted tests</h2>

<ul>
<li>SQL   -> ADLS     Error</li>
<li>Oracle    -> ADLS     Error</li>
<li>SQL   -> Blob     OK</li>
<li>Oracle    -> Blob     OK</li>
<li>SQL   -> SQL      OK</li>
<li>ADLS  -> ADLS     OK</li>
<li>AzureSQLDB -> ADLS        OK</li>
</ul>
","<sql-server><azure><azure-data-lake><azure-data-factory>","2018-11-15 14:13:32","860","3","1","53371721","<p>Does your self-hosted IR has some proxy setting or goes through special network setting? Such error should be caused by the intermediate proxy service when ADF's ADLS connector tried to talk to the ADLS service.</p>
"
"53320690","Azure Data Factory - download Json Defitions","<p>So I am facing the following problem: I have a bunch of Azure Data Factory V1
Pipelines in one specific data factory, these pipelines, each have, around 400 data sets.
I need to move all of them to a new resource group / environment and put their json definition in a git repo.</p>

<p>So my questions is, how can I download all the pipelines definitions for a data factory and all the data sets definitions in their json format from Azure?</p>

<p>I don't want to click each one and copy-paste from the Azure UI, as it will take ages.</p>
","<azure-devops><azure-data-factory>","2018-11-15 13:35:45","1267","0","2","53327230","<p>For ADF V1, you can try using Visual Studio. </p>

<ol>
<li><p>Connect via Cloud Explorer to your data factory. </p></li>
<li><p>Select the data factory and choose Export to New Data Factory Project</p></li>
</ol>

<p>This is documented on <a href=""http://www.sqlservercentral.com/articles/Azure+Data+Factory/139478/"" rel=""nofollow noreferrer"">SQL Server Central</a>.</p>

<p>Another thing to try is to have Azure script out an ARM template of the Data Factory. </p>
"
"53320690","Azure Data Factory - download Json Defitions","<p>So I am facing the following problem: I have a bunch of Azure Data Factory V1
Pipelines in one specific data factory, these pipelines, each have, around 400 data sets.
I need to move all of them to a new resource group / environment and put their json definition in a git repo.</p>

<p>So my questions is, how can I download all the pipelines definitions for a data factory and all the data sets definitions in their json format from Azure?</p>

<p>I don't want to click each one and copy-paste from the Azure UI, as it will take ages.</p>
","<azure-devops><azure-data-factory>","2018-11-15 13:35:45","1267","0","2","53331972","<p>Call Rest API is good way for both V1 and V2. See <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/v2"" rel=""nofollow noreferrer"">this doc</a>. </p>
"
"53315487","How to query a document property from CosmosDb dataset in Azure Data Factory","<p>With Azure Data Factory I have a <code>CosmosDb</code> Dataset and picked a <code>Collection</code> from the dropdown list.</p>

<p>My Pipeline contains an Activity <code>Copy Data</code> and picked the Source Dataset mentioned above. Option 'Use Query' is set to <code>Query</code> so I have a textbox available.</p>

<p>When I enter a simple query like <code>select c.Name from c</code> I don't get any data and I don't understand why. How can I get the Name property from the dataset so it can be mapped?</p>

<p>This is the dataset:
<a href=""https://i.stack.imgur.com/gz7mw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gz7mw.png"" alt=""available data""></a></p>
","<azure><azure-cosmosdb><azure-data-factory>","2018-11-15 08:48:11","1090","2","2","53315925","<p>What is your precise requirement ?</p>

<ol>
<li><p>Table and Query is single selection, if you select Query, the table(collection) you defined in dataset will not work.</p></li>
<li><p>if you select a collection in dataset, and imported schema, you can switch to mapping in the copy activity, if you can't see any mapping, you should make sure you have selected a sink dataset and the sink has schema.</p></li>
<li><p>The preview window you opened in dataset will not work for the query in activity source. it's different scenario. so no matter what you input in the query, it will not update</p></li>
</ol>
"
"53315487","How to query a document property from CosmosDb dataset in Azure Data Factory","<p>With Azure Data Factory I have a <code>CosmosDb</code> Dataset and picked a <code>Collection</code> from the dropdown list.</p>

<p>My Pipeline contains an Activity <code>Copy Data</code> and picked the Source Dataset mentioned above. Option 'Use Query' is set to <code>Query</code> so I have a textbox available.</p>

<p>When I enter a simple query like <code>select c.Name from c</code> I don't get any data and I don't understand why. How can I get the Name property from the dataset so it can be mapped?</p>

<p>This is the dataset:
<a href=""https://i.stack.imgur.com/gz7mw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gz7mw.png"" alt=""available data""></a></p>
","<azure><azure-cosmosdb><azure-data-factory>","2018-11-15 08:48:11","1090","2","2","53387251","<p>Based on Supported capabilities in document: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db#supported-capabilities"" rel=""nofollow noreferrer"">Copy data to or from Azure Cosmos DB by using Azure Data Factory</a>, Azure Cosmos DB connector only supports Copying data from and to the Azure Cosmos DB <strong><em>SQL API</em></strong>. That's the reason that you saw the strange results.</p>

<p>So, you could choose <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-mongodb"" rel=""nofollow noreferrer"">mongo connector</a> to do your jobs which is mentioned in this case:<a href=""https://social.msdn.microsoft.com/Forums/security/en-US/52cddbf7-c132-490c-9088-65a38f9b7200/copy-activity-to-cosmosdb-with-mongo-api?forum=AzureDataFactory"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/security/en-US/52cddbf7-c132-490c-9088-65a38f9b7200/copy-activity-to-cosmosdb-with-mongo-api?forum=AzureDataFactory</a>.</p>
"
"53304273","Azure Data Factory Copy Activity with Source as Stored Procedure","<p>Couldn't figure out how can we use a stored procedure as source dataset in Azure Data Factory copy activity? Is there a way to have a stored procedure as the source data in a copy data task?</p>
","<azure-data-factory>","2018-11-14 16:02:08","1193","0","2","53313909","<p>Yes, ADF supports to read data from a stored procedure in Copy activity. See the below picture, we use an Azure SQL dataset as example, click the stored Procedure checkbox, select the stored procedure script in your database then fill the parameter if needed. This <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#azure-sql-database-as-the-source"" rel=""nofollow noreferrer"">doc</a> provides more information. Thanks.</p>

<p><a href=""https://i.stack.imgur.com/56KdD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/56KdD.png"" alt=""[1]: https://i.stack.imgur.com/iE5dp.png""></a></p>
"
"53304273","Azure Data Factory Copy Activity with Source as Stored Procedure","<p>Couldn't figure out how can we use a stored procedure as source dataset in Azure Data Factory copy activity? Is there a way to have a stored procedure as the source data in a copy data task?</p>
","<azure-data-factory>","2018-11-14 16:02:08","1193","0","2","65424801","<p>Be careful as when using the stored procedure source with 'auto create' table set a schema infer step is performed which executes the code in the stored procedure in a VERY peculiar way that can cause pipeline errors - especially if you have any dynamic SQL and/or conditions in the stored procedure code! There is a work-around I have discovered which allows the stored procedure to be written in a fully functional way without breaking the pipeline. I will write a separate article on it perhaps.</p>
"
"53301427","Can Azure Data Factory v2 work without Integration Runtime if hosted in the same Resource Group?","<p>I have an OData feed on port 7048 hosted in a specific Azure resource group. It is not publicly accessible, but other servers within the same resource group can talk to each other.</p>

<p>I have added an Azure Data Factory v2 within the same resource group, and I was expecting it to be able to read the OData feed. However when I click on test connection I get:</p>

<pre><code>Connection failed
[{""code"":10201,""message"":""Failed to create OData connection to **********""}]
</code></pre>

<p>There isn't much more information about this error anywhere. </p>

<p>The Integration Runtime is the default AutoResolveIntegrationRuntime. </p>

<p>Do I need another Integration Runtime, even though they are running on the same Resource Group? If not what does this error code 10201 mean? </p>
","<azure><azure-data-factory>","2018-11-14 13:31:40","241","0","1","53312242","<p>the error code is internal error code used for troubleshooting, but the error message is for customer read. You could make the OData source to Azure IP accessible so that AutoResourceIntegrationRuntime could access. Another option is create a selfhosted IR and allow the selfhosted IR's IP to access the OData source. </p>
"
"53300221","Copying files in fileshare with Azure Data Factory configuration problem","<p>I am trying to learn using the Azure Data Factory to copy data (a collection of csv files in a folder structure) from an Azure File Share to a Cosmos DB instance.</p>

<p>In Azure Data factory I'm creating a ""copy data"" activity and try to set my file share as source using the following host:</p>

<p><code>mystorageaccount.file.core.windows.net\\mystoragefilesharename</code></p>

<p>When trying to test the connection, I get the following error:</p>

<p><code>[{""code"":9059,""message"":""File path 'E:\\approot\\mscissstorage.file.core.windows.net\\mystoragefilesharename' is not supported. Check the configuration to make sure the path is valid.""}]</code> </p>

<p>Should I move the data to another storage type like a blob or I am not entering the correct host url?</p>
","<azure><azure-storage><azure-data-factory>","2018-11-14 12:26:00","3668","2","3","53302743","<p>You'll need to specify the host in json file like this ""\\myserver\share"" if you create pipeline with JSON directly or you use set the host url like this ""\myserver\share"" if you're using UI to setup pipeline.</p>

<p>Here is more info:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system#sample-linked-service-and-dataset-definitions"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system#sample-linked-service-and-dataset-definitions</a></p>
"
"53300221","Copying files in fileshare with Azure Data Factory configuration problem","<p>I am trying to learn using the Azure Data Factory to copy data (a collection of csv files in a folder structure) from an Azure File Share to a Cosmos DB instance.</p>

<p>In Azure Data factory I'm creating a ""copy data"" activity and try to set my file share as source using the following host:</p>

<p><code>mystorageaccount.file.core.windows.net\\mystoragefilesharename</code></p>

<p>When trying to test the connection, I get the following error:</p>

<p><code>[{""code"":9059,""message"":""File path 'E:\\approot\\mscissstorage.file.core.windows.net\\mystoragefilesharename' is not supported. Check the configuration to make sure the path is valid.""}]</code> </p>

<p>Should I move the data to another storage type like a blob or I am not entering the correct host url?</p>
","<azure><azure-storage><azure-data-factory>","2018-11-14 12:26:00","3668","2","3","53311353","<p>I believe when you create file linked service, you might choose public IR. If you choose public IR, local path (e.g c:\xxx, D:\xxx) is not allowed, because the machine that run your job is managed by us, which not contains any customer data. Please use self-hosted IR to copy your local files.</p>
"
"53300221","Copying files in fileshare with Azure Data Factory configuration problem","<p>I am trying to learn using the Azure Data Factory to copy data (a collection of csv files in a folder structure) from an Azure File Share to a Cosmos DB instance.</p>

<p>In Azure Data factory I'm creating a ""copy data"" activity and try to set my file share as source using the following host:</p>

<p><code>mystorageaccount.file.core.windows.net\\mystoragefilesharename</code></p>

<p>When trying to test the connection, I get the following error:</p>

<p><code>[{""code"":9059,""message"":""File path 'E:\\approot\\mscissstorage.file.core.windows.net\\mystoragefilesharename' is not supported. Check the configuration to make sure the path is valid.""}]</code> </p>

<p>Should I move the data to another storage type like a blob or I am not entering the correct host url?</p>
","<azure><azure-storage><azure-data-factory>","2018-11-14 12:26:00","3668","2","3","53346341","<p>Based on the link posted by <a href=""https://stackoverflow.com/users/10646221/nicolas-zhang"">Nicolas Zhang</a>: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system#sample-linked-service-and-dataset-definitions"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system#sample-linked-service-and-dataset-definitions</a> and the examples provided therein, I was able to solve it an successfully create the copy action. I had two errors (I'm configuring via the data factory UI and not directly the JSON):</p>

<ol>
<li>In the host path, the correct one should be: <code>\\mystorageaccount.file.core.windows.net\mystoragefilesharename\myfolderpath</code></li>
<li>The username and password must be the one corresponding to the storage account and not to the actual user's account which I was erroneously using.</li>
</ol>
"
"53293264","Azure Databricks to Event Hub","<p>I am very new to Databricks. So, pardon me please. Here is my requiremnt</p>

<ol>
<li>I have data stored in Azure DataLake</li>
<li>As per the requirement, we can only access data via Azure Databricks notebook</li>
<li>We have to pull the data from certain tables, join with other tables, aggregate</li>
<li>Send the data to an Event Hub</li>
</ol>

<p>How can I perform this activity. I assume there is not one shot process. I was planning to create a notebook and run it via Azure Data Factory. Pump the data in Blob and then using .Net send it to Event Hub. But, from Azure Data Factory we can only run the Azure Databricks notebook not store anywhere</p>
","<azure><azure-data-factory><azure-data-lake><databricks>","2018-11-14 04:36:59","5310","4","1","53301790","<p>Azure Databricks do support Azure Event Hubs as source and sink. Understand <a href=""https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html"" rel=""nofollow noreferrer"">Structured Streaming</a> - it is a stream processing engine in Apache Spark (available in Azure Databricks as well)</p>
<p>Create a notebook to do all your transformation (join, aggregation...) - assuming you are doing batch write to azure event hub.</p>
<p>Using Scala</p>
<pre><code>val connectionString = &quot;Valid EventHubs connection string.&quot;
val ehWriteConf = EventHubsConf(connectionString)
df.select(&quot;body&quot;)
.write
.format(&quot;eventhubs&quot;)
.options(ehWriteConf.toMap)    
.save()
</code></pre>
<p>Replace <code>.write</code> to <code>.writeStream</code> if your queries are streaming.</p>
<p>Using PySpark</p>
<pre><code>ds = df \
  .select(&quot;body&quot;) \
  .writeStream \
  .format(&quot;eventhubs&quot;) \
  .options(**ehWriteConf) \
  .option(&quot;checkpointLocation&quot;, &quot;///output.txt&quot;) \
  .start()
</code></pre>
<p>More things to consider when working with Azure Event Hubs is regarding partitions - it is optional, you can just send the body alone (which will do round robin model)</p>
<p>More information <a href=""https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/structured-streaming-eventhubs-integration.md#writing-data-to-eventhubs"" rel=""nofollow noreferrer"">here</a><br />
And the PySpark version <a href=""https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md"" rel=""nofollow noreferrer"">here</a></p>
"
"53283769","Creating a pipeline with a scheduled trigger with ADFV2","<p>I try to migrate a pipeline that already exists in ADFV1 to ADFV2 and have some issues with the concept of triggers. My pipeline has two activiries, the first one is an Azure Data Lake Analytics activity and the second a copy activity. 
The first activity runs a usql script where data is read from partioned folder /{yyyy}/{MM}/{dd}/, process it and write in folder /{yyyy}-{MM}-{dd}/.
Here are some JSON files from my factory (pipeline, trigger and datasets).</p>

<p>Pipeline:</p>

<pre><code>{
""name"": ""StreamCompressionBlob2SQL"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""compress"",
            ""type"": ""DataLakeAnalyticsU-SQL"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false,
                ""secureInput"": false
            },
            ""typeProperties"": {
                ""scriptPath"": ""d00044653/azure-configurations/usql-scripts/stream/compression.usql"",
                ""scriptLinkedService"": {
                    ""referenceName"": ""AzureBlobStorage"",
                    ""type"": ""LinkedServiceReference""
                },
                ""parameters"": {
                    ""Year"": {
                        ""value"": ""@formatDateTime(pipeline().parameters.windowStartTime,'yyyy')"",
                        ""type"": ""Expression""
                    },
                    ""Month"": {
                        ""value"": ""@formatDateTime(pipeline().parameters.windowStartTime,'MM')"",
                        ""type"": ""Expression""
                    },
                    ""Day"": {
                        ""value"": ""@formatDateTime(pipeline().parameters.windowStartTime,'dd')"",
                        ""type"": ""Expression""
                    }
                }
            },
            ""linkedServiceName"": {
                ""referenceName"": ""AzureDataLakeAnalytics1"",
                ""type"": ""LinkedServiceReference""
            }
        },
        {
            ""name"": ""Blob2SQL"",
            ""type"": ""Copy"",
            ""dependsOn"": [
                {
                    ""activity"": ""compress"",
                    ""dependencyConditions"": [
                        ""Succeeded""
                    ]
                }
            ],
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false,
                ""secureInput"": false
            },
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""BlobSource"",
                    ""recursive"": true
                },
                ""sink"": {
                    ""type"": ""SqlSink"",
                    ""writeBatchSize"": 10000
                },
                ""enableStaging"": false,
                ""dataIntegrationUnits"": 0,
                ""translator"": {
                    ""type"": ""TabularTranslator"",
                    ""columnMappings"": {
                        ""tag"": ""TAG"",
                        ""device_id"": ""DEVICE_ID"",
                        ""system_id"": ""SYSTEM_ID"",
                        ""utc"": ""UTC"",
                        ""ts"": ""TS"",
                        ""median"": ""MEDIAN"",
                        ""min"": ""MIN"",
                        ""max"": ""MAX"",
                        ""avg"": ""AVG"",
                        ""stdev"": ""STDEV"",
                        ""first_value"": ""FIRST_VALUE"",
                        ""last_value"": ""LAST_VALUE"",
                        ""message_count"": ""MESSAGE_COUNT""
                    }
                }
            },
            ""inputs"": [
                {
                    ""referenceName"": ""AzureBlobDataset_COMPRESSED_ASA_v1"",
                    ""type"": ""DatasetReference""
                }
            ],
            ""outputs"": [
                {
                    ""referenceName"": ""AzureSQLDataset_T_ASSET_MONITORING_WARM_ASA_v1"",
                    ""type"": ""DatasetReference""
                }
            ]
        }
    ],
    ""parameters"": {
        ""windowStartTime"": {
            ""type"": ""String""
        }
    }
}
</code></pre>

<p>}</p>

<p>Trigger:</p>

<pre><code>{
""name"": ""trigger1"",
""properties"": {
    ""runtimeState"": ""Started"",
    ""pipelines"": [
        {
            ""pipelineReference"": {
                ""referenceName"": ""StreamCompressionBlob2SQL"",
                ""type"": ""PipelineReference""
            },
            ""parameters"": {
                ""windowStartTime"": ""@trigger().scheduledTime""
            }
        }
    ],
    ""type"": ""ScheduleTrigger"",
    ""typeProperties"": {
        ""recurrence"": {
            ""frequency"": ""Day"",
            ""interval"": 1,
            ""startTime"": ""2018-08-17T10:46:00.000Z"",
            ""endTime"": ""2018-11-04T10:46:00.000Z"",
            ""timeZone"": ""UTC""
        }
    }
}
</code></pre>

<p>}</p>

<p>Input Dataset for Copy Activity:</p>

<pre><code>{
""name"": ""AzureBlobDataset_COMPRESSED_ASA_v1"",
""properties"": {
    ""linkedServiceName"": {
        ""referenceName"": ""AzureBlobStorage"",
        ""type"": ""LinkedServiceReference""
    },
    ""parameters"": {
        ""Year"": {
            ""type"": ""String"",
            ""defaultValue"": ""@formatDateTime(pipeline().parameters.windowStartTime,'yyyy')""
        },
        ""Month"": {
            ""type"": ""String"",
            ""defaultValue"": ""@formatDateTime(pipeline().parameters.windowStartTime,'yyyy')""
        },
        ""Day"": {
            ""type"": ""String"",
            ""defaultValue"": ""@formatDateTime(pipeline().parameters.windowStartTime,'yyyy')""
        }
    },
    ""type"": ""AzureBlob"",
    ""structure"": [
        {
            ""name"": ""tag"",
            ""type"": ""String""
        },
        {
            ""name"": ""device_id"",
            ""type"": ""String""
        },
        {
            ""name"": ""system_id"",
            ""type"": ""String""
        },
        {
            ""name"": ""utc"",
            ""type"": ""DateTime""
        },
        {
            ""name"": ""ts"",
            ""type"": ""DateTime""
        },
        {
            ""name"": ""median"",
            ""type"": ""Double""
        },
        {
            ""name"": ""min"",
            ""type"": ""Double""
        },
        {
            ""name"": ""max"",
            ""type"": ""Double""
        },
        {
            ""name"": ""avg"",
            ""type"": ""Double""
        },
        {
            ""name"": ""stdev"",
            ""type"": ""Double""
        },
        {
            ""name"": ""first_value"",
            ""type"": ""Double""
        },
        {
            ""name"": ""last_value"",
            ""type"": ""Double""
        },
        {
            ""name"": ""message_count"",
            ""type"": ""Int16""
        }
    ],
    ""typeProperties"": {
        ""format"": {
            ""type"": ""TextFormat"",
            ""columnDelimiter"": "";"",
            ""nullValue"": ""\\N"",
            ""treatEmptyAsNull"": true,
            ""skipLineCount"": 0,
            ""firstRowAsHeader"": true
        },
        ""fileName"": """",
        ""folderPath"": {
            ""value"": ""@concat('d00044653/processed/stream/compressed',dataset().Year,'-',dataset().Month,'-',dataset().Day)"",
            ""type"": ""Expression""
        }
    }
},
""type"": ""Microsoft.DataFactory/factories/datasets""
</code></pre>

<p>}</p>

<p>Output Dataset for Copy Activity:</p>

<pre><code>{
""name"": ""AzureSQLDataset_T_ASSET_MONITORING_WARM_ASA_v1"",
""properties"": {
    ""linkedServiceName"": {
        ""referenceName"": ""AzureSqlDatabase1"",
        ""type"": ""LinkedServiceReference""
    },
    ""type"": ""AzureSqlTable"",
    ""structure"": [
        {
            ""name"": ""TAG"",
            ""type"": ""String""
        },
        {
            ""name"": ""DEVICE_ID"",
            ""type"": ""String""
        },
        {
            ""name"": ""SYSTEM_ID"",
            ""type"": ""String""
        },
        {
            ""name"": ""UTC"",
            ""type"": ""DateTime""
        },
        {
            ""name"": ""TS"",
            ""type"": ""DateTime""
        },
        {
            ""name"": ""MEDIAN"",
            ""type"": ""Decimal""
        },
        {
            ""name"": ""MIN"",
            ""type"": ""Decimal""
        },
        {
            ""name"": ""MAX"",
            ""type"": ""Decimal""
        },
        {
            ""name"": ""AVG"",
            ""type"": ""Decimal""
        },
        {
            ""name"": ""STDEV"",
            ""type"": ""Decimal""
        },
        {
            ""name"": ""FIRST_VALUE"",
            ""type"": ""Decimal""
        },
        {
            ""name"": ""LAST_VALUE"",
            ""type"": ""Decimal""
        },
        {
            ""name"": ""MESSAGE_COUNT"",
            ""type"": ""Int32""
        }
    ],
    ""typeProperties"": {
        ""tableName"": ""[dbo].[T_ASSET_MONITORING_WARM]""
    }
},
""type"": ""Microsoft.DataFactory/factories/datasets""
</code></pre>

<p>}</p>

<p>My problem is that after publishing nothing happens. 
Any suggestions??</p>
","<azure><triggers><azure-pipelines><azure-data-factory>","2018-11-13 14:57:25","717","0","1","53301890","<p>Schedule trigger do not support backfill scenario (based on your trigger definition - you are starting from August 17th 2018). In schedule trigger, pipeline runs can be executed only on time periods from <strong>the current time</strong> and the future. </p>

<p>In your case, for backfill scenarios use Tumbling window trigger.</p>
"
"53282012","Variables and Parameters in Azure Data Factory?","<p>Im just learning ADF, can you explain to me the difference between Variables and Parameters? I cant find a good explanation about it.</p>
","<azure><azure-data-factory>","2018-11-13 13:23:56","19555","18","3","53292143","<p>Parameters are something that can be set to model (dataset, pipeline, etc) at runtime. Without parameters, you model is fixed, for example, without parameters you can only write query like ""select * from order where timestamp > '11/01/2018'"" in dataset, with parameter 'TS' you can write query like ""select * from order where timestamp > '@dataset.TS'"" so that your activity run will query and copy data in different time window.</p>

<p>Variables are something that contain real value at runtime and can be assigned to those parameters in app model(dataset, pipeline, etc) to get real things done. </p>

<p>Here are some examples:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#examples"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#examples</a></p>
"
"53282012","Variables and Parameters in Azure Data Factory?","<p>Im just learning ADF, can you explain to me the difference between Variables and Parameters? I cant find a good explanation about it.</p>
","<azure><azure-data-factory>","2018-11-13 13:23:56","19555","18","3","60838239","<p><strong>Parameters</strong> are set for the whole pipeline, and are constant during a pipeline run. You can <em>read</em> them during a run to control what the pipeline does. They are like <strong>constants</strong> in a programming language, defined at the top of the source code.</p>

<p><strong>Variables</strong> can be set at the start of a pipeline and <em>read and modified</em> during a run. They are like <strong>normal variables</strong> in a programming language.</p>
"
"53282012","Variables and Parameters in Azure Data Factory?","<p>Im just learning ADF, can you explain to me the difference between Variables and Parameters? I cant find a good explanation about it.</p>
","<azure><azure-data-factory>","2018-11-13 13:23:56","19555","18","3","71763923","<p>A variable can be used as a parameter (input). A constant can also be used as a parameter. Being a parameter is being used.</p>
"
"53280426","Right way to access private data from Azure Data Factory","<p>I am trying to understand what is the right architecture to use to access data from servers hosted on a private network (still running on Azure but not publicly accessible) and the Azure Data Factory service.</p>

<p>On some documentation Microsoft mentions the Integration Runtime as the solution:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime</a></p>

<p>While on other documentation it refers to a Data Gateway:
<a href=""https://learn.microsoft.com/en-us/azure/analysis-services/analysis-services-gateway"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/analysis-services/analysis-services-gateway</a></p>

<p>Both articles seem fairly recent. The two applications have different recommended requirements (one mentions 8 CPU cores! Which is an overkill for my requirements to ship a few hundred megabytes per night)</p>

<p>Given that the data sources are running on Azure, just not publicly accessible, is there a way to connect Azure Data Factory directly?</p>
","<azure><azure-data-factory>","2018-11-13 11:49:47","137","0","1","53283684","<p>The Self Hosted Integration Runtime in ADF should meet your requirement, this <a href=""https://blog.nicholasrogoff.com/2018/07/03/how-to-get-azure-data-factory-connecting-to-your-data-on-a-vnet-or-internal-network/"" rel=""nofollow noreferrer"">link</a> gives a complete example to access data under Azure VNet or private network.</p>
"
"53265932","Azure Data Factory V2 Copy activity Mapping deployment issue","<p>Consider following test Mapping for Data Factory Copy activity:</p>

<pre><code>""translator"": {
  ""columnMappings"": ""@json('{\""from\"":\""to\""}')"",
  ""type"": ""TabularTranslator""
}
</code></pre>

<p>After deploying pipeline with the help of <code>Set-AzureRmDataFactoryV2Pipeline</code> PowerShell cmdlet we get normally deployed pipeline with the exact <code>columnMappings</code> value as specified in source code. But if you try to be more dynamic:</p>

<pre><code>""translator"": {
  ""columnMappings"": ""@json(pipeline().parameters.Mapping)"",
  ""type"": ""TabularTranslator""
}
</code></pre>

<p>then after deployment you'll find that <code>translator</code> element is completely missing in pipeline. A workaround - set translator in Azure Portal Data Factory pipeline editing UI (either in Designer or JSON modes - both options work). But if after these manipulations you save pipeline JSON to the file and attempt to deploy it via <code>Set-AzureRmDataFactoryV2Pipeline</code> PowerShell cmdlet - bang, <code>translator</code> becomes missing. Expected result - deployment shall preserve <code>translator</code> element, because Portal JSON Editor preserves it. </p>

<p>We are doing automated deployment of pipelines (as you already figured out - with the help of <code>Set-AzureRmDataFactoryV2Pipeline</code>) and this bug breaks our automated deployment because it requires manual postdeployment pipeline editing on Azure Portal UI.</p>

<p>What may be the reason of such a buggy behavior? Can you suggest an idea how to work around this bug in automated way, or how to fix the code so it can be properly deployed with <code>Set-AzureRmDataFactoryV2Pipeline</code>?</p>
","<azure><deployment><azure-data-factory>","2018-11-12 16:06:22","245","0","1","53350493","<p>You could try whether ""Update-Module -Name AzureRm.DataFactoryV2"" helps. It might be caused by that your powershell module is out of date.</p>
"
"53248599","Converting the value in specific date format into another specified format in u-sql script using u-sql operators and functions","<p>lets say i have two variables a and b declared in the u-sql script as mentioned below. I have a scenario where i need to check the datetime value of column in a specific table is ranging in between two datetime values which are declared as variables below. </p>

<p>sample value of STARTDATETIME Column: 2018-11-06T14:06:15.0000000</p>

<p>I am able to convert time1 as it is straight forward and but i am not able to convert time2 which is in format ending with Z to StartDateTime Format .Can Someone help me on how i can do this Conversion Correctly in U-SQL Script</p>

<p>U-SQL Script:</p>

<pre><code>DECLARE @time1 string = ""11/11/2018 12:34:12 AM"";

DECLARE @time2 string = ""2018-11-11T06:46:52.3143918Z"";

//sql query snippet in u-sql script

@testfile = SELECT [PRODUCT],[STARTDATETIME] from TABLEA WHERE [STARTDATETIME] BETWEEN Convert.ToDateTime(@time1) AND ?????
</code></pre>
","<azure><azure-data-factory><u-sql>","2018-11-11 12:10:16","169","0","1","53270270","<p>I'd use the <a href=""https://learn.microsoft.com/en-us/dotnet/api/system.datetime.parseexact?view=netframework-4.7.2"" rel=""nofollow noreferrer"">DateTime.ParseExact</a></p>

<pre><code>@testfile = SELECT [PRODUCT],[STARTDATETIME] from TABLEA WHERE [STARTDATETIME] BETWEEN Convert.ToDateTime(@time1) AND DateTime.ParseExact(@time2, ""yyyy-MM-ddTHH:mm:ss.fffffffZ"", NULL);
</code></pre>
"
"53214441","Authenticate to Azure Sql Database without Active Directory? I Don't have SQL admin credentials either","<p>I have learned a very painful lesson a that you cannot authenticate to Azure SQL database using Active Directory in the Cloud using  Custom .Net activity.</p>

<p>At this point I'm looking for alternatives.</p>

<p>The code I'm using is:</p>

<pre><code>    string constring=
          ""Server = server,1433; "" +
          ""Initial Catalog =dbname; "" +
          ""Persist Security Info = False; "" +
          ""MultipleActiveResultSets = False; "" +
          ""Encrypt = True; TrustServerCertificate = False;"" +
          "" Authentication = Active Directory Integrated"";


 SqlConnection con = new SqlConnection(constring);
 con.Open()
</code></pre>

<p>Which fails in when deployed. I get an ""unable to load asadsql.dll"" error.</p>

<p>Any guidance would be greatly appreciated. </p>

<p>I'm using framework 4.7.2, is there anyway around this?</p>
","<azure><azure-sql-database><azure-active-directory><azure-data-factory>","2018-11-08 18:59:19","119","0","1","53216543","<p>If you are able to run your SQL/AAD app successfully from your laptop and can connect to SQLDB, you should be able to run it from the Web app. I am slightly confused about ADF in this scenario </p>
"
"53209711","Unable to load data from tab delimited to a table using datafactory version 1","<p>I have a data in excel which is being converted to tab delimited, and placed in a folder when the pipeline runs, it takes that file and runs. I am having a trouble loading 3 columns data which has all the special character and the error I am having is </p>

<blockquote>
  <p>ErrorCode=FailedDbOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The
  given value of type String from the data source cannot be converted to
  type nvarchar of the specified target column.. String or binary data
  would be
  truncated.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.InvalidOperationException,Message=The
  given value of type String from the data source cannot be converted to
  type nvarchar of the specified target.</p>
</blockquote>

<p>FYI,  the character length is all okay and within the limit so that is not the issue in here.  </p>
","<azure><azure-data-factory>","2018-11-08 14:24:14","87","2","1","53214424","<p>Well, after a lot of in depth searching the excel by loading chunks by chunks I found out what and which cell was causing the issue. There were lot of line breaks in the cell which was causing the issue. Removed all the line breaks and loaded the file. It was successful!</p>
"
"53207239","Azure Data Factory V2 - Mutliple Instance of Same Pipeline Triggered parallely","<p>We've created a re-usable azure data factory V2 pipeline. We're thinking to invoke this pipeline from different master pipelineS. These master pipelines may run in parallel. So, my concern is will this re-usable run as multiple instance process OR experience deadlock ?</p>

<p>Do I need to make any settings to run the re-usable pipeline with multiple instances(In case, by default multiple instantiation is not supported)?</p>

<p>thanks </p>
","<azure><azure-data-factory>","2018-11-08 11:55:00","2379","1","1","53221081","<blockquote>
  <p>Do I need to make any settings to run the re-usable pipeline with
  multiple instances(In case, by default multiple instantiation is not
  supported)?</p>
</blockquote>

<p>As I know, no such specific settings you need to configure. However, based on this <a href=""https://github.com/MicrosoftDocs/azure-docs/blob/master/includes/azure-data-factory-limits.md"" rel=""nofollow noreferrer"">azure-data-factory-limits</a>, azure data factory V2 have many limitations.</p>

<p>Such as Concurrent pipeline runs per pipeline is <code>100</code> and Write API calls is <code>2500/hr</code>. You need to optimize your behaviors against the limitations.In addition, you could <a href=""https://azure.microsoft.com/en-us/blog/azure-limits-quotas-increase-requests/"" rel=""nofollow noreferrer"">contact support</a> about your custom requirements.</p>
"
"53205644","Azure Datafactory V2 - IfCondition activity cannot include another IfCondition activity","<p>I'm using  Azure Data factory V2 and I need to implement two levels of checks with in a parent check validation. example: IF Condition one is true Then I need to check for Condition 2. And, If Condition2 is True then Check for Condition 3.</p>

<p>It's kind of hierarchical checks. When I implement using multiple IF Condition activities within parent IF condition activity, then it doesn't work. I do see following error, when i implement it. </p>

<pre><code>""ERROR: IfCondition activity cannot include another IfCondition activity""
</code></pre>
","<azure><azure-data-factory>","2018-11-08 10:17:25","6163","3","2","53209319","<p>ADF- V2 doesn't support multiple IfCondition activities with in a single pipeline.</p>

<p>So as a workaround, we used multiple filter activities upfront and split the pipeline flow before IfCondition Activity is invoked.</p>

<p>Thanks</p>
"
"53205644","Azure Datafactory V2 - IfCondition activity cannot include another IfCondition activity","<p>I'm using  Azure Data factory V2 and I need to implement two levels of checks with in a parent check validation. example: IF Condition one is true Then I need to check for Condition 2. And, If Condition2 is True then Check for Condition 3.</p>

<p>It's kind of hierarchical checks. When I implement using multiple IF Condition activities within parent IF condition activity, then it doesn't work. I do see following error, when i implement it. </p>

<pre><code>""ERROR: IfCondition activity cannot include another IfCondition activity""
</code></pre>
","<azure><azure-data-factory>","2018-11-08 10:17:25","6163","3","2","54771945","<p>You can use some of the dynamic functions to implement a more complex logic with and() and or().</p>

<p>You can put several continuous if's that set up variables, and use those variables to determine the outcome at the end. </p>

<p>And if that doesn't work for you, or you don't feel like it, you can always Execute a Pipeline inside of the If that uses another If Activity. But I wouldn't recommend this if you want to keep your monitor as easy to read as possible since it launches another pipeline (or more than one)</p>
"
"53200216","How to load python libraries in Azure Data Factory Custom Activity?","<p>I understand you can use a batch custom activity with custom VM image with preloaded python libraries to run your python programs through ADF. </p>

<p>Can we also use a plain linux VM image and load the python libraries as part of the execution of the custom activity? I understand this might increase the execution time since libraries have to be installed each time, but I want to find out how it is done. Explanation/example is appreciated.</p>

<p>Thanks!</p>
","<python><azure><azure-data-factory><azure-batch><custom-activity>","2018-11-08 01:13:11","1169","1","1","53219540","<p>You can use Azure Batch  <a href=""https://learn.microsoft.com/azure/batch/batch-api-basics#start-task"" rel=""nofollow noreferrer"">start task</a> to install pre defined libraries efficiently. All jobs submitted via Custom Activity against the same pool will have access to them. </p>

<p>Alternatively, you can install dependent libraries by executing a shell scripts against Linux pool nodes which would setup the libraries and then trigger the python code.</p>
"
"53194861","Which Azure products are needed for a staging database?","<p>I have several external data APIs that I access using some Python scripts. My scripts run from an on-premises server, transform the data, and store it in a SQL Server database on the same server. I suppose it's a rudimentary ETL system run with Python and T-SQL.</p>

<p>The system is about to grow quite a bit with new APIs and will require more complex data pipelines (for example, some of the API data will be spun off to more than one table). I think this would be a good time to move the system onto Azure (we are heavily integrated with Microsoft so it will have to be Azure!).</p>

<p>I have spent a few days researching the Azure products that would let me run Python scripts to access data from web APIs and store the processed data in a cloud database. I'm looking for advice on what sort of Azure products other people have used for similar jobs. At the moment it seems I will need:</p>

<ol>
<li>Azure SQL Database to hold the processed data that can be accessed by various colleagues.</li>
<li>Azure Data Factory to manage, log, and schedule the pipeline jobs and to run my custom Python scripts (is this even possible?).</li>
<li>Azure Batch to run the aforementioned Python scripts but I'm not sure about this.</li>
</ol>

<p>I want to put together a proposal basically and start thinking about costs but it would be good to hear from someone who has done something similar - am I on the right track or completely off? Should I just stay on-premises? Thank you in advance.</p>
","<azure><azure-sql-database><etl><azure-data-factory><azure-batch>","2018-11-07 17:39:17","251","0","2","53201062","<ol>
<li>Azure SQL Database, Azure SQL Data Warehouse are good for relational data. And if you want to use NoSQL, you could go with Azure Cosmos DB. If you want to use Files to store data, you could use Azure Data Lake. </li>
<li>For python scripts, you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">custom activity</a> or <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-databricks-python"" rel=""nofollow noreferrer"">Data bricks</a> for Azure Data Factory.</li>
</ol>
"
"53194861","Which Azure products are needed for a staging database?","<p>I have several external data APIs that I access using some Python scripts. My scripts run from an on-premises server, transform the data, and store it in a SQL Server database on the same server. I suppose it's a rudimentary ETL system run with Python and T-SQL.</p>

<p>The system is about to grow quite a bit with new APIs and will require more complex data pipelines (for example, some of the API data will be spun off to more than one table). I think this would be a good time to move the system onto Azure (we are heavily integrated with Microsoft so it will have to be Azure!).</p>

<p>I have spent a few days researching the Azure products that would let me run Python scripts to access data from web APIs and store the processed data in a cloud database. I'm looking for advice on what sort of Azure products other people have used for similar jobs. At the moment it seems I will need:</p>

<ol>
<li>Azure SQL Database to hold the processed data that can be accessed by various colleagues.</li>
<li>Azure Data Factory to manage, log, and schedule the pipeline jobs and to run my custom Python scripts (is this even possible?).</li>
<li>Azure Batch to run the aforementioned Python scripts but I'm not sure about this.</li>
</ol>

<p>I want to put together a proposal basically and start thinking about costs but it would be good to hear from someone who has done something similar - am I on the right track or completely off? Should I just stay on-premises? Thank you in advance.</p>
","<azure><azure-sql-database><etl><azure-data-factory><azure-batch>","2018-11-07 17:39:17","251","0","2","53352731","<p>Azure SQL Warehouse should be used if the amount of data you want to load is in petabytes. Also, Azure Data warehouse is not meant for complex transformations. I would recommend it for plain data load with PolyBase.</p>
"
"53194315","Migrating datasets and pipelines from one DF to the other DF","<p>I have a a data factory where there are 7 pipelines and respective data sets were built. Now those data sets and pipelines needs to be moved from that data factory to other data factory. May I know which is the best way to migrate all the pipelines and data sets from one DF to the other? Step by step process would be helpful.</p>
","<azure><azure-sql-database><azure-data-factory>","2018-11-07 17:02:56","382","0","1","53195061","<p>Please see this previous Stack Overflow thread pertaining to the same question: <a href=""https://stackoverflow.com/questions/52320386/how-to-export-pipeline-in-datafactory-v2-or-migrate-to-another/"">How to export pipeline in datafactory v2 or migrate to another</a></p>

<p>Another option is to use Visual Studio, where you can export the configuration from one and import to another ADF instance. Some reconfiguration steps are necessary. </p>
"
"53191428","Run .EXE in using Azure ADF V2","<p>I'm fairly new to Azure and I have setup an .exe file to run in an Azure ADF V2.</p>

<p>I my Custom Activity setup with the account and batch setup.</p>

<p>When I try to run the exe in debug it just keeps running indefinitely with no fail or anything.</p>

<p>The exe gets data from multiple web apis and loops through it using C# code and outputs to a database. I'm not looking for alternative to this. The exe works fine on my machine but not sure if it's working in Azure.</p>

<p>I've been setting everything up through the portal as well.</p>

<p>Here are the settings I have:</p>

<p>Command: program.exe</p>

<p>FolderPath: mycontainer</p>

<p>I copied the exe directly in to the container (mycontainer)</p>

<p>Any guidance appreciated!</p>
","<c#><azure><azure-sql-database><azure-data-factory>","2018-11-07 14:26:58","1069","0","2","53400424","<p>Your Azure batch storage account should have a folder for each run (named based on the RunId Giod). Have you looked at the job output to see if there are any output files (there should always be an output directory with stdout.txt and stderr.txt files)? You can add Console.WriteLine commands in your EXE and they will write to the stdout.txt file - that may help you diagnose.<a href=""https://i.stack.imgur.com/UWI0v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UWI0v.png"" alt=""enter image description here""></a></p>
"
"53191428","Run .EXE in using Azure ADF V2","<p>I'm fairly new to Azure and I have setup an .exe file to run in an Azure ADF V2.</p>

<p>I my Custom Activity setup with the account and batch setup.</p>

<p>When I try to run the exe in debug it just keeps running indefinitely with no fail or anything.</p>

<p>The exe gets data from multiple web apis and loops through it using C# code and outputs to a database. I'm not looking for alternative to this. The exe works fine on my machine but not sure if it's working in Azure.</p>

<p>I've been setting everything up through the portal as well.</p>

<p>Here are the settings I have:</p>

<p>Command: program.exe</p>

<p>FolderPath: mycontainer</p>

<p>I copied the exe directly in to the container (mycontainer)</p>

<p>Any guidance appreciated!</p>
","<c#><azure><azure-sql-database><azure-data-factory>","2018-11-07 14:26:58","1069","0","2","53549842","<p>For some reason Debug wasn't working, I had to actually Trigger the pipeline to get it work!</p>
"
"53178767","Get Last value of timestamp in Azure Data Factory with Cosmos as source","<p>My pipeline has 2 activities for this test.  A lookup activity and a stored procedure which simply takes the output of lookup and then uses it as a parameter to execute.</p>

<p>I want to the get the max value of _ts field (Comos DB Unix Timestamp) in a query that I'm writing.</p>

<p><a href=""https://i.stack.imgur.com/p4MtC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p4MtC.png"" alt=""enter image description here""></a></p>

<p>This is my query on the source:</p>

<pre><code>select max(c._ts) AS UnixTimestamp  from c 
where udf.convertTime(c._ts) &gt;= '@{formatDateTime(addhours(pipeline().TriggerTime, -1), 'yyyy-MM-ddTHH:mm:ssZ' )}' 
AND udf.convertTime(c._ts) &lt; '@{formatDateTime(pipeline().TriggerTime, 'yyyy-MM-ddTHH:mm:ssZ' )}'
</code></pre>

<p>When I debug this is the translated input:</p>

<pre><code>{
    ""source"": {
        ""type"": ""DocumentDbCollectionSource"",
        ""query"": ""select max(c._ts) AS UnixTimestamp  from c \nwhere udf.convertTime(c._ts) &gt;= '2018-11-06T18:19:56Z' \nAND udf.convertTime(c._ts) &lt; '2018-11-06T19:19:56Z'"",
        ""nestingSeparator"": "".""
    },
    ""dataset"": {
        ""referenceName"": ""SourceComosDB"",
        ""type"": ""DatasetReference"",
        ""parameters"": {}
    },
    ""firstRowOnly"": false
}
</code></pre>

<p>The output of the lookup activity is coming out to be:</p>

<pre><code>{
    ""count"": 18,
    ""value"": [
        {},
        {},
        {},
        {},
        {},
        {},
        {},
        {},
        {},
        {},
        {},
        {},
        {},
        {},
        {},
        {},
        {},
        {}
    ],
    ""effectiveIntegrationRuntime"": ""DefaultIntegrationRuntime (East US 2)""
}
</code></pre>

<p>and hence the stored procedure is not executed.</p>

<p>How can I get the max value of _ts within the query?</p>
","<azure><azure-cosmosdb><azure-data-factory>","2018-11-06 19:32:21","1236","0","1","53183139","<p>I tested your code and did not reproduce your issue.</p>

<p>my sample code:</p>

<p><a href=""https://i.stack.imgur.com/6T9Gf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6T9Gf.png"" alt=""enter image description here""></a></p>

<p>lookup activity output with your SQL:</p>

<p><a href=""https://i.stack.imgur.com/0JBt2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0JBt2.png"" alt=""enter image description here""></a></p>

<p>lookup activity output with your SQL:</p>

<p><a href=""https://i.stack.imgur.com/eQaF9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eQaF9.png"" alt=""enter image description here""></a></p>

<p>One thing, it can't output <code>18</code> values if you used <code>MAX</code> query, supposed to be just one value. </p>

<p>The another thing, you should execute the SQK in cosmos db query editor to see if any results match the filter.</p>
"
"53177818","Setting Default value in Data Factory Copy Activity in mappings","<p>Just wondering how can we set a Default value in Data Factory Copy Activity in mappings.</p>

<p>Trying to do following</p>

<p><a href=""https://stackoverflow.com/questions/53175753/azure-data-factory-source-dataset-value-from-parameter"">Azure Data Factory Source Dataset value from Parameter</a></p>

<p>But if that's not possible, can I set a default value through parameter while defining mappings?</p>
","<azure-data-factory>","2018-11-06 18:25:35","2419","0","1","53247267","<p>To achieve your requirement, you could define a pipeline parameter with ""Object""  type and give it a default value, eg. name it as ""columnMapping"" and set the default value as: </p>

<pre><code>                {
                    ""type"": ""Object"",
                    ""defaultValue"": {
                        ""type"": ""TabularTranslator"",
                        ""columnMappings"": {
                            ""Prop_0"": ""Prop_0"",
                            ""Prop_1"": ""Prop_1"",
                            ""Prop_2"": ""Prop_2""
                        }
                    }
                }
</code></pre>

<p>then assign the parameter to the copy activity column mapping:</p>

<pre><code>          ""translator"": {
                            ""value"": ""@pipeline().parameters.columnMappings"",
                            ""type"": ""Expression""
                        }
</code></pre>
"
"53175753","Azure Data Factory Source Dataset value from Parameter","<p>I have a Dataset in Azure Datafactory backed by a CSV file. I added an additional column in Dataset and want to pass it's value from Dataset parameter but value never gets copied to the column</p>

<pre><code>  ""type"": ""AzureBlob"",
    ""structure"": 
    [
        {
            ""name"": ""MyField"",
            ""type"": ""String""
        }
    ]
</code></pre>

<p>I have a defined parameter as well</p>

<pre><code>   ""parameters"": {
        ""MyParameter"": {
            ""type"": ""String"",
            ""defaultValue"": ""ABC""
        }
    }  
</code></pre>

<p>How can copy the parameter value to Column? I tried following but doesn't work</p>

<pre><code>   ""type"": ""AzureBlob"",
    ""structure"": 
    [
        {
            ""name"": ""MyField"",
            ""type"": ""String"",
            ""value"": ""@dataset().MyParameter""
        }
    ]
</code></pre>

<p>But this does not work. I am getting NULL in destination although parameter value is set</p>
","<azure-data-factory>","2018-11-06 16:13:59","4596","3","1","53295358","<p>Based on document: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">Expressions and functions in Azure Data Factory</a> , <code>@dataset().XXX</code> is not supported in Azure Data Factory so far. So, you can't use parameters value as custom column into sink or source with native copy activity directly.</p>

<p>However, you could adopt below workarounds:</p>

<p>1.You could create a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">custom activity</a> and write code to do whatever you need.</p>

<p>2.You could stage the <code>csv</code> file in a azure data lake, then execute a <code>U-SQL</code> script to read the data from the file and append the new column with the pipeline <code>rundId</code>.  Then output it to a new area in the data lake so that the data could be picked up by the rest of your pipeline. To do this, you just need to simply pass a Parameter to U-SQL from ADF. Please refer to the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-data-lake-analytics"" rel=""nofollow noreferrer"">U-SQL Activity</a>.</p>

<p>In this thread: <a href=""https://stackoverflow.com/questions/51760981/use-adf-pipeline-parameters-as-source-to-sink-columns-while-mapping"">use adf pipeline parameters as source to sink columns while mapping</a>, the customer used the second way.</p>
"
"53163307","Is it possible to use Visual Studio for Azure Data Factory?","<p>I am new to Azure. I would like to learn the architecture deployed in my company which i shown below on diagram. Can anyone point me to some video example or something that could reflect that from diagram below. I also have access to Azure portal that i have some money credit so if it is possible i could create some test environment based on that diagram.</p>

<p>P.S Is it possible to use Visual Studio for any kind of work based on that diagram or everything have to be created and develop from Azure portal?</p>

<pre><code>Datasource Oracle DB --&gt; on prem gateway --&gt; ADF--&gt; Azure DB --&gt; AAS --&gt; PowerBI
SQL EDP --------------------------------------^
</code></pre>
","<azure><azure-sql-database><azure-data-factory>","2018-11-05 22:40:02","338","0","2","53176511","<p>Azure Data Factory has a 1:M capability with various data sources. One instance of Azure Data Factory will support multiple data movement capabilities: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities#data-movement-activities"" rel=""nofollow noreferrer"">Data movement activities</a></p>

<p>Information about On-Premise Gateway:</p>

<blockquote>
  <p>The on-premises data gateway acts as a bridge, providing secure data transfer between on-premises data sources and your Azure Analysis Services servers in the cloud. In addition to working with multiple Azure Analysis Services servers in the same region, the latest version of the gateway also works with Azure Logic Apps, Power BI, Power Apps, and Microsoft Flow. You can associate multiple services in the same subscription and same region with a single gateway. </p>
</blockquote>

<p><a href=""https://learn.microsoft.com/en-us/azure/analysis-services/analysis-services-gateway"" rel=""nofollow noreferrer"">Connecting to on-premises data sources with Azure On-premises Data Gateway</a></p>
"
"53163307","Is it possible to use Visual Studio for Azure Data Factory?","<p>I am new to Azure. I would like to learn the architecture deployed in my company which i shown below on diagram. Can anyone point me to some video example or something that could reflect that from diagram below. I also have access to Azure portal that i have some money credit so if it is possible i could create some test environment based on that diagram.</p>

<p>P.S Is it possible to use Visual Studio for any kind of work based on that diagram or everything have to be created and develop from Azure portal?</p>

<pre><code>Datasource Oracle DB --&gt; on prem gateway --&gt; ADF--&gt; Azure DB --&gt; AAS --&gt; PowerBI
SQL EDP --------------------------------------^
</code></pre>
","<azure><azure-sql-database><azure-data-factory>","2018-11-05 22:40:02","338","0","2","53199409","<p>You've got a fairly straightforward BI architecture there with the following logical components:</p>

<ul>
<li>raw / source data</li>
<li>integration</li>
<li>data mart / dimensional model</li>
<li>semantic</li>
<li>visualisation</li>
</ul>

<p>The physical components look a bit like this:</p>

<p><a href=""https://i.stack.imgur.com/1G5El.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1G5El.png"" alt=""physical components""></a></p>

<p>The physical components can be described like this:</p>

<ul>
<li>Oracle database - former market leader database product.  I would guess your employers have rejected OBIEE for some reason</li>
<li>Self-hosted Integration Runtime (SHIR)<s>On-premises data gateway</s> - the SHIR <s>gateway</s> enables the movement of data from on-prem data sources to the cloud.  This <strong>must</strong> be used when moving data from on-prem to Azure SQL DB using Data Factory.  Use the SHIR with Data Factory and the Gateway with Power BI and Azure Analysis Services.</li>
<li>Data Factory - Azure ELT tool for moving data from place to place.  ETL feature <a href=""https://www.youtube.com/watch?v=_XdZLaz00WM"" rel=""nofollow noreferrer"">Data Flow</a> currently in preview.</li>
<li>Azure SQL DB - PaaS SQL database, scalable via service tiers.  If your data in Oracle is not already in a data mart / dimensional format, then it can be made so here</li>
<li>Azure Analysis Services (AAS) - PaaS OLAP in-memory engine, scalable for fast slice-and-dice, drill down and semantic modelling.  Tabular only.</li>
<li>Power BI - increasingly powerful visualisation tool.  Run dashboard in DirectQuery / LiveConnection mode to avoid entirely duplicating the tabular model from AAS in Power BI.</li>
</ul>

<p>In answer to some of your questions: you can have one Azure Data Factory with many <strong>pipelines</strong>.  The Visual Studio Azure Data Factory project type is now defunct.</p>

<p>As to ""why"" for certain technologies:</p>

<ul>
<li>why Oracle - Who knows.</li>
<li>why SHIR - SHIR is compulsory when moving data from on-prem to cloud with ADF</li>
<li>why Azure SQL DB - lightweight and powerful PaaS DB requiring no infra and low TCO; scalable.  Might be location for restructuring of data from raw / relational structure to dimensional in readiness for semantic layer if your data is not already in that format in Oracle</li>
<li>why AAS - fast, in-memory slice-and-dice; scalable, can pause, can be interrogated by Excel, Power BI Desktop, SSMS, VS, other clients etc.  Optionally has row-level security (RLS)</li>
<li>Power BI - online service Power BI.com offers easy sharing within organisation, even externally.</li>
<li>why all the components together - you could (in theory) go straight from Oracle to Power BI with a Power BI gateway (I think) BUT you would then have to do all the modelling in Power BI and your model is then only really accessible from Power BI.  In this model, users with SQL skills can query the data mart, users with DAX (or Excel, or Power BI Desktop) skills can query the AAS tabular model, AAS is very scalable component, etc</li>
</ul>

<p>These opinions are strictly my own personal ones and the value of them may go down, as well as up.</p>

<p>HTH</p>
"
"53163178","Azure Data Factory V2 - Copy Task fails HTTP file to Azure Blob Store","<p>In my code I have an HTTP file activity in the ""Source"" of a copy task. This hits an Azure Function HTTP endpoint and returns a String when complete. I want to store that String result into a ""Sink"" of Azure Blob.</p>

<p>My Linked Service looks like so.</p>

<p><a href=""https://i.stack.imgur.com/fKGKT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fKGKT.png"" alt=""Linked Service""></a></p>

<p>My Dataset looks like so.</p>

<p><a href=""https://i.stack.imgur.com/Gq1pn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Gq1pn.png"" alt=""enter image description here""></a></p>

<p>I get the following error when debugging </p>

<p>""{
    ""errorCode"": ""2200"",
    ""message"": ""Failure happened on 'Sink' side. ErrorCode=UserErrorInvalidHttpRequestHeaderFormat,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Failed to set addtional http header,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.ArgumentException,Message=Specified value has invalid HTTP Header characters.\r\nParameter name: name,Source=System,'"",
    ""failureType"": ""UserError"",
    ""target"": ""Copy Data1""
}""</p>
","<azure><azure-data-factory>","2018-11-05 22:29:13","2910","2","2","53164564","<p>according to the error message, the problem lies on the requestHeader setting. Please note that the format of the requestHeader in HTTP dataset should be like <strong>""key1:value1\nkey2:value2\nkey3:value3""</strong>, so in your case, pass <strong>""Content-Type"": ""application/json""</strong> to requestHeader should be the right format. Thanks.</p>
"
"53163178","Azure Data Factory V2 - Copy Task fails HTTP file to Azure Blob Store","<p>In my code I have an HTTP file activity in the ""Source"" of a copy task. This hits an Azure Function HTTP endpoint and returns a String when complete. I want to store that String result into a ""Sink"" of Azure Blob.</p>

<p>My Linked Service looks like so.</p>

<p><a href=""https://i.stack.imgur.com/fKGKT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fKGKT.png"" alt=""Linked Service""></a></p>

<p>My Dataset looks like so.</p>

<p><a href=""https://i.stack.imgur.com/Gq1pn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Gq1pn.png"" alt=""enter image description here""></a></p>

<p>I get the following error when debugging </p>

<p>""{
    ""errorCode"": ""2200"",
    ""message"": ""Failure happened on 'Sink' side. ErrorCode=UserErrorInvalidHttpRequestHeaderFormat,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Failed to set addtional http header,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.ArgumentException,Message=Specified value has invalid HTTP Header characters.\r\nParameter name: name,Source=System,'"",
    ""failureType"": ""UserError"",
    ""target"": ""Copy Data1""
}""</p>
","<azure><azure-data-factory>","2018-11-05 22:29:13","2910","2","2","67300518","<p>It's yet another unfortunate ADF/Synapse Pipelines quirk. Unlike for Web activities, or REST Datasets, here we have a single input for headers, as opposed to header/value input rows in those other tools.</p>
<p>After fiddling around with various combinations, the ONLY one that worked when using <strong>DYNAMIC input</strong> was the following:</p>
<pre><code>@{string('Authorization: Bearer justanexample')}
</code></pre>
"
"53147832","How to trigger an alert notification of a long-running process in Azure Data Factory V2 using either Azure Monitor or ADF itself?","<p>I've been trying to find the best way to trigger an alert when an ADF task (i.e. CopyActivity or Stored Procedure Task) has been running for more than N hours, I wanted to use the Azure Monitor as it is one of the recommended notification services in Azure, however I have not been able to find a ""Running"" criteria, hence I had to play with the available criteria (Succeeded and Failed) and check this every N hours, however this is still not perfect as I don't know when the process started and we may run the process manually multiple times a day, is there any way you would recommend doing this? like a event-based notification that listens to some time variable and as soon as it is greater than the threshold triggers an email notification?</p>
","<azure><azure-data-factory>","2018-11-05 03:11:48","2285","4","2","53149595","<blockquote>
  <p>is there any way you would recommend doing this? like a event-based
  notification that listens to some time variable and as soon as it is
  greater than the threshold triggers an email notification?</p>
</blockquote>

<p>Based on your requirements, I suggest you using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-programmatically#net"" rel=""nofollow noreferrer"">Azure Data Factory SDKs</a> to monitor your pipelines and activities.</p>

<p>You could create a <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-timer"" rel=""nofollow noreferrer"">time trigger Azure Function</a> which is triggered every N hours. In that trigger function :</p>

<ol>
<li><p>You could list all <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.activityrunsoperationsextensions.querybypipelinerun?view=azure-dotnet"" rel=""nofollow noreferrer"">running activities</a> in data factory account. </p></li>
<li><p>Then loop them to monitor the <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models.activityrun.durationinms?view=azure-dotnet#Microsoft_Azure_Management_DataFactory_Models_ActivityRun_DurationInMs"" rel=""nofollow noreferrer"">DurationInMs</a> Property in <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models.activityrun?view=azure-dotnet"" rel=""nofollow noreferrer"">ActivityRun Class</a> to check if any activity has been running for more than N hours and it's still In-Progress <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models.activityrun.status?view=azure-dotnet#Microsoft_Azure_Management_DataFactory_Models_ActivityRun_Status"" rel=""nofollow noreferrer"">status</a>.</p></li>
<li><p>Finally, send the email or kill the activity or do whatever you want.</p></li>
</ol>
"
"53147832","How to trigger an alert notification of a long-running process in Azure Data Factory V2 using either Azure Monitor or ADF itself?","<p>I've been trying to find the best way to trigger an alert when an ADF task (i.e. CopyActivity or Stored Procedure Task) has been running for more than N hours, I wanted to use the Azure Monitor as it is one of the recommended notification services in Azure, however I have not been able to find a ""Running"" criteria, hence I had to play with the available criteria (Succeeded and Failed) and check this every N hours, however this is still not perfect as I don't know when the process started and we may run the process manually multiple times a day, is there any way you would recommend doing this? like a event-based notification that listens to some time variable and as soon as it is greater than the threshold triggers an email notification?</p>
","<azure><azure-data-factory>","2018-11-05 03:11:48","2285","4","2","62388351","<p>I would suggest simple solution:
Kusto query for listing all pipeline runs where status is ""Queued"" and joining it on CorrelationId with those that we are not interested in - typically ""Succeeded"", ""Failed"". Join flavor <strong>leftanti</strong> does the job by ""Returning all the records from the left side that don't have matches from the right."" (as specified in MS documentation).
Next step would be to set your desired timeout value - it is 30m in the example code below.
Finally, you can configure Alert rule based on this query and get your email notification, or whatever you need.</p>

<p><em>ADFPipelineRun
 | where Status == ""Queued""
 | join kind=leftanti ( ADFPipelineRun
 | where Status in (""Failed"", ""Succeeded"") )
 on CorrelationId
| where Start &lt; ago(30m)</em></p>

<p>I tested this only briefly, maybe there is something missing. I have an idea about adding other statuses to be removed from result - like ""Cancelled"".</p>
"
"53119503","Loop over each file in folder directory and check date Azure Data Factory V2 -wrong code","<p>I want to loop over each file in a stfp folder and check whether it is new or not and then copy the new files on a Data Lake
Right now I have the below code but I don't think it is correct. There is no usage of <code>@item()</code> in the second <code>GetLastModifyfromFile</code> activity to refer to the items last date  in the loop but rather to a completely different data set called SrcLocalFile.</p>

<pre><code>{
""name"": ""IncrementalloadfromSingleFolder"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""GetFileList"",
            ""type"": ""GetMetadata"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false
            },
            ""typeProperties"": {
                ""dataset"": {
                    ""referenceName"": ""SrcLocalDir"",
                    ""type"": ""DatasetReference""
                },
                ""fieldList"": [
                    ""childItems""
                ]
            }
        },
        {
            ""name"": ""ForEachFile"",
            ""type"": ""ForEach"",
            ""dependsOn"": [
                {
                    ""activity"": ""GetFileList"",
                    ""dependencyConditions"": [
                        ""Succeeded""
                    ]
                }
            ],
            ""typeProperties"": {
                ""items"": {
                    ""value"": ""@activity('GetFileList').output.childItems"",
                    ""type"": ""Expression""
                },
                ""activities"": [
                    {
                        ""name"": ""GetLastModifyfromFile"",
                        ""type"": ""GetMetadata"",
                        ""policy"": {
                            ""timeout"": ""7.00:00:00"",
                            ""retry"": 0,
                            ""retryIntervalInSeconds"": 30,
                            ""secureOutput"": false
                        },
                        ""typeProperties"": {
                            ""dataset"": {
                                ""referenceName"": ""SrcLocalFile"",
                                ""type"": ""DatasetReference""
                            },
                            ""fieldList"": [
                                ""lastModified""
                            ]
                        }
                    },
                    {
                        ""name"": ""IfNewFile"",
                        ""type"": ""IfCondition"",
                        ""dependsOn"": [
                            {
                                ""activity"": ""GetLastModifyfromFile"",
                                ""dependencyConditions"": [
                                    ""Succeeded""
                                ]
                            }
                        ],
                        ""typeProperties"": {
                            ""expression"": {
                                ""value"": ""@and(less(activity('GetLastModifyfromFile').output.lastModified, pipeline().parameters.current_time), greaterOrEquals(activity('GetLastModifyfromFile').output.lastModified, pipeline().parameters.last_time))"",
                                ""type"": ""Expression""
                            },
                            ""ifTrueActivities"": [
                                {
                                    ""name"": ""CopyNewFiles"",
                                    ""type"": ""Copy"",
                                    ""policy"": {
                                        ""timeout"": ""7.00:00:00"",
                                        ""retry"": 0,
                                        ""retryIntervalInSeconds"": 30,
                                        ""secureOutput"": false
                                    },
                                    ""typeProperties"": {
                                        ""source"": {
                                            ""type"": ""FileSystemSource"",
                                            ""recursive"": false
                                        },
                                        ""sink"": {
                                            ""type"": ""BlobSink""
                                        },
                                        ""enableStaging"": false,
                                        ""dataIntegrationUnits"": 0
                                    },
                                    ""inputs"": [
                                        {
                                            ""referenceName"": ""SrcLocalFile"",
                                            ""type"": ""DatasetReference""
                                        }
                                    ],
                                    ""outputs"": [
                                        {
                                            ""referenceName"": ""TgtBooksBlob"",
                                            ""type"": ""DatasetReference""
                                        }
                                    ]
                                }
                            ]
                        }
                    }
                ]
            }
        }
    ],
    ""parameters"": {
        ""current_time"": {
            ""type"": ""String"",
            ""defaultValue"": ""2018-04-01T00:00:00Z""
        },
        ""last_time"": {
            ""type"": ""String"",
            ""defaultValue"": ""2018-03-01T00:00:00Z""
        }
    },
    ""folder"": {
        ""name"": ""IncrementalLoadSingleFolder""
    }
},
""type"": ""Microsoft.DataFactory/factories/pipelines""
</code></pre>

<p>}</p>
","<azure><azure-data-factory>","2018-11-02 13:29:46","2823","1","1","53197496","<p>Just a thought - I don't see your dataset definition but...</p>

<p>Should you pass in path and file name to the dataset as parameters?</p>

<p>i.e. add 2 parameters to the dataset definition for path and file (say pathparam and fileparam).  Use those parameters in the dataset's fileName and folderName settings as @dataset().pathparam and @dataset().fileparam.  </p>

<p>In the code above, pass in parameters a new ""parameters"" section of the dataset input with pathparam and fileparam equal to the folder and child item you retrieved from earlier activity.</p>

<p>note - there was a bug that the dataset name could not have spaces in it.</p>
"
"53104943","Azure Data Factory v2 not displayed in Server Explorer or Cloud Explorer in Visual Studio 2015","<p>I have a data factory (version 2) and a bunch of other resources in my Azure subscription, and have logged into that subscription using Visual Studio 2015. When I open the Server Explorer or Cloud Explorer, I can see most of my resources (ie SQL Servers &amp; Databases, Data Lakes, Logic Apps, and Streaming Jobs), but my data factory is not shown in either. Is data factory not supported by this view, or is there a way I can fix this?</p>

<p>I believe I have everything up to date, and am currently using Windows 10 version 1803. I've looked through SO and MSDN and can't find anything on not seeing data factories specifically.</p>

<p><strong>Installed Products:</strong></p>

<ul>
<li>Visual Studio 2015 Update 3 (version 14.0.25431.01) </li>
<li>Azure App Service Tools v3.0.0 (version 14.0.30823.0) </li>
<li>Common Azure Tools (version 1.8)</li>
<li>DataFactoryProject (version 1.0) </li>
<li>Microsoft Azure Data Factory Node Node (version 1.0) </li>
<li>Microsoft Azure Tools (version 2.9)</li>
</ul>
","<visual-studio><azure><azure-data-factory>","2018-11-01 16:01:09","448","0","1","53167854","<p>As same as you, azure Data Factory V2 doesn't show up in <code>Cloud Explorer</code> in both of version 2015 and version 2017.</p>

<p>Based on this <a href=""https://marketplace.visualstudio.com/items?itemName=ms-azuretools.CloudExplorerForVS#overview"" rel=""nofollow noreferrer"">document</a>, the tool only supports for subscritptions by resources that have Role Based Access Control (RBAC) applied as well as those in Cloud Solution Providers (CSP) and DreamSpark programs.</p>

<p>You could write a review <a href=""https://marketplace.visualstudio.com/items?itemName=ms-azuretools.CloudExplorerForVS#review-details"" rel=""nofollow noreferrer"">here</a> to find the facts.</p>
"
"53103618","Data Migration from Azure SQL Databases to Dynamics 365 for Marketing using Azure Data Factory","<p>I want to migrate data from Azure SQL Database(around 100 databases) to Dynamics 365 for marketing using Azure Data Factory. As I am new to Azure, I have several questions regarding this.</p>

<ol>
<li><p>Is this possible to copy from such large number of databases to Dynamics 365 for Marketing?</p></li>
<li><p>I have several tables and long list of columns from databases, but I want to move only some data to Dynamics 365. Is it possible to create related records in CRM and assign them to the primary entity using Azure Data Factory?</p></li>
<li><p>Can anyone explain me what needs to be done in order to achieve this?</p></li>
</ol>

<p>Thank You.</p>
","<azure><dynamics-365><azure-data-factory>","2018-11-01 14:45:41","520","1","1","53105335","<p>that is a very big question, let's break it down a bit: </p>

<ul>
<li>Q: Does Azure Data Factory support Azure SQL and Dynamics 365 </li>
<li><p>A: Yes. both are supported as a source and a sink (target) data store. The list of supported connectors <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">is here</a></p></li>
<li><p>Q: Can you import data from multiple databases and sources</p></li>
<li><p>A: Sure, but you need to be a bit clever.
Some tools and techniques that will help: </p>

<ol>
<li>Use the Data Factory <a href=""http://blogs.adatis.co.uk/matthow/post/Using-Lookup-Execute-Pipeline-and-For-Each-Activity-in-Azure-Data-Factory-V2"" rel=""nofollow noreferrer"">loop</a> construct</li>
<li>Use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-resource-manager-template"" rel=""nofollow noreferrer"">Azure ARM templates</a> to help set up your database connections (you can write some simple code to automatically generate all the relevant linked services) </li>
<li>Use queries to shape the data you want to import. Look for sqlReaderQuery in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database"" rel=""nofollow noreferrer"">these docs</a> </li>
</ol></li>
<li><p>Q: Is it possible to create related records in CRM and assign them to the primary entity using Azure Data Factory?</p></li>
<li>A: I don't think this is possible. You are limited to what the connector will do for you. Look for the ""Sink"" section in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-dynamics-crm-office-365"" rel=""nofollow noreferrer"">this documentation</a> to see what the connector can do. An alternative is to write your own <a href=""https://github.com/Azure/Azure-DataFactory/tree/master/Samples/ADFv2CustomActivitySample"" rel=""nofollow noreferrer"">custom activity</a> that does sink, and then you have full control over the behaviour. </li>
</ul>

<p>Hope that gets you started! </p>
"
"53102684","Problem setting up azure datafactory to azure devops git","<p>I am trying to hook up my Azure Data Factory to Azure Devops Git.   However when i select ""Azure Devops Git"" as repository type i get:</p>

<p>""Could not find a valid Visual Studio Online account""</p>

<p>I have made sure that my Devops is 
""This organization is backed by the  Azure Active Directory.""</p>

<p>Have anybody seen this problem?</p>
","<git><azure><azure-devops><devops><azure-data-factory>","2018-11-01 13:50:57","2090","1","3","57577969","<p>I Know it is a very old thread. I faced the same problem today. Tried various things, but what worked for me is initializing one of the repositories in Azure DevOps. </p>

<p><a href=""https://i.stack.imgur.com/nM2ZW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nM2ZW.png"" alt=""enter image description here""></a></p>
"
"53102684","Problem setting up azure datafactory to azure devops git","<p>I am trying to hook up my Azure Data Factory to Azure Devops Git.   However when i select ""Azure Devops Git"" as repository type i get:</p>

<p>""Could not find a valid Visual Studio Online account""</p>

<p>I have made sure that my Devops is 
""This organization is backed by the  Azure Active Directory.""</p>

<p>Have anybody seen this problem?</p>
","<git><azure><azure-devops><devops><azure-data-factory>","2018-11-01 13:50:57","2090","1","3","59652429","<p>In case you are the admin of your Azure portal and Azure Devops account then do the following.</p>

<ol>
<li>Go to organization Settings of Azure DevOps</li>
<li>Now Go to Azure Active Directory and then click on connect Directory and follow the process that pops up on the window.</li>
</ol>

<p>After above steps move to Repository Settings in Azure Data Factory and after selecting Azure DevOps Git as repository type, check the box for Select different directory and choose default directory in the next option and there you go...</p>
"
"53102684","Problem setting up azure datafactory to azure devops git","<p>I am trying to hook up my Azure Data Factory to Azure Devops Git.   However when i select ""Azure Devops Git"" as repository type i get:</p>

<p>""Could not find a valid Visual Studio Online account""</p>

<p>I have made sure that my Devops is 
""This organization is backed by the  Azure Active Directory.""</p>

<p>Have anybody seen this problem?</p>
","<git><azure><azure-devops><devops><azure-data-factory>","2018-11-01 13:50:57","2090","1","3","61071482","<p>Also - if you just created your organization and connected the AAD - be sure and log out of all the browsers and log back in. I left the portal open in a browser and until I logged out I couldn't see a repo. </p>
"
"53084539","Azure data factory (ADFv2) - how to process multiple input files from different folder in a USQL job","<p>We have a requirement to process multiple files from different folders using Azure data factory and USQL. </p>

<p>Here is the folder structure we have</p>

<p>Year --> Month --> Day</p>

<p>We have a folder for every date, say 1,2,3...31. The requirement is to read files from specific folders and pass it to USQL to do analytics processing. We need to process data for multiple dates. Is there any way in data factory we can read data from multiple folders. </p>

<p>Example: I need to read data for the dates, 1,7 and 10 for a specific month. I do not want to read all the files for the month</p>

<p>Please let us know if you have come across a solution for the above scenario.</p>
","<azure><u-sql><azure-data-factory>","2018-10-31 13:28:56","500","1","2","53102322","<p>yes there is. You should use Virtual columns. Example:
Your file has only column1 and column2.</p>

<p>path1=/yourFolder/2018/11/1/file.csv</p>

<p>path2=/yourFolder/2018/10/25/file.csv</p>

<pre><code>DECLARE date1 = new DateTime(2018,11,1);
DECLARE date2 = new DateTime(2018,10,25);
@inputData = EXTRACT column1 string,
column2 string, 
FileDate DateTime //this is virtual column
FROM ""/yourFolder/{FileDate:yyyy}/{FileDate:MM}/{FileDate:dd}/file.csv""
USING Extractors.Text(delimiter:';',skipFirstNRows:1);
@res = SELECT * FROM @inputData WHERE FileDate == date1 AND FileDate ==date2;
</code></pre>

<p>This way you can manage your folder structure using virtual columns. 
You can ofcourse use BETWEEN if you want to take files from some date range etc. 
HTH </p>

<p>P.S. You can send parameters from ADF to USQL stored procedure i just gave an example how to work with specific files. </p>
"
"53084539","Azure data factory (ADFv2) - how to process multiple input files from different folder in a USQL job","<p>We have a requirement to process multiple files from different folders using Azure data factory and USQL. </p>

<p>Here is the folder structure we have</p>

<p>Year --> Month --> Day</p>

<p>We have a folder for every date, say 1,2,3...31. The requirement is to read files from specific folders and pass it to USQL to do analytics processing. We need to process data for multiple dates. Is there any way in data factory we can read data from multiple folders. </p>

<p>Example: I need to read data for the dates, 1,7 and 10 for a specific month. I do not want to read all the files for the month</p>

<p>Please let us know if you have come across a solution for the above scenario.</p>
","<azure><u-sql><azure-data-factory>","2018-10-31 13:28:56","500","1","2","53193830","<p>Below code worked. As suggested above.</p>

<p>DECLARE EXTERNAL @startDate string = ""2018-08-01"";  // value will be passed from ADF
DECLARE EXTERNAL @endDate string = ""2018-08-31"";    // value will be passed from ADF</p>

<p>DECLARE @report_start_date DateTime = DateTime.ParseExact(@startDate,""yyyy-MM-dd"",CultureInfo.InvariantCulture);
DECLARE @report_end_date DateTime = DateTime.ParseExact(@endDate,""yyyy-MM-dd"",CultureInfo.InvariantCulture);</p>

<p>DECLARE @INPUT_FILE=@valueFromADF+""/{date:yyyy}/{date:M}/{date:d}/{filename}.txt"";</p>

<p>@dataAsString = EXTRACT value string,date DateTime FROM @INPUT_FILE USING Extractors.Text(delimiter:'\n');
@dataAsStrings = SELECT * FROM @dataAsString WHERE date BETWEEN  @report_start_date AND   @report_end_date;</p>
"
"53074516","Incrementally copy S3 to azure blob","<p>I am exploring ways to incrementally copy S3 blobs to azure blobs. I tried azure data factory and I was not able to find this option.</p>

<p>S3 has millions of objects and without an incremental option it takes hours to complete.</p>

<p>I am open to explore other tools/options.</p>
","<azure><amazon-s3><azure-data-factory><migrate>","2018-10-31 00:04:24","573","0","1","53075193","<p>if you need to do daily/hourly incrementally copy, the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger"" rel=""nofollow noreferrer"">scheduled</a>/<a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger"" rel=""nofollow noreferrer"">tumbling window</a> trigger should be the option. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-read-write-partitioned-data#use-a-pipeline-parameter"" rel=""nofollow noreferrer"">Here</a> is an example to reference. ADFv2 also supports compression in copy, you could specify the type and level of compression for the s3 object, click <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#compression-support"" rel=""nofollow noreferrer"">here</a> for more information.</p>
"
"53072270","Release pipeline conflict with integration runtime","<p>This question relates to how to propagate a data factory through CI (in VSTS) if there is a self hosted Integration Runtime defined in the Data Factory.</p>

<p>I have a 3 environments set up - Dev / UAT / Prod each with their own data factory.</p>

<p>The Dev hosts the master collaboration branch.  I am using VSTS to retrieve the artifacts from the adf_publish branch and deploying the template to UAT (prod will be done later).  I followed much of what is in this guide <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">here</a>.</p>

<p>When deploying to blank UAT with a self-hosted integration runtime (IR), the IR that is deployed in UAT is a copy of the shared IR from dev (not a linked type) and this causes an error since the credentials used by the IR will not be correct.  I expect this since we are really just deploying an exact copy of the Resource Group template with just the factory name overridden however the IR will not work without it being re-credentialled with the self hosted IR VMs.</p>

<p>If I pre-register a linked IR with the UAT environment (linked to the dev IRs), then the deployment fails with a conflict because an IR in the resource group template is the same name as the one I just created in UAT.  If it is a different name - no conflict but the linked services will be pointing to the template IR and not the one I created for UAT </p>

<p>The docs have a note that says the IR runtime should be the same across all the platforms but I do not think this can be true - one of them (presumably the source/dev) must be a shared type and the others linked and authorized.  </p>

<p>One option I could see (untested) is to have each environments IR reference be a separate connection to an actual IR but then there then needs to be some way of overriding the linked services to point to the current environments IR reference (by template parameter override?).  In this scenario, we need to block the templates IR from being deployed as it won't be needed and won't work.</p>

<p>Has anyone had success in getting CI working in this situation?  My sense is the doc was written with the globally shared IR.  Either that or I need to better understand the aim of Auto Integration setting in the linked services definition.</p>

<p>Many thanks.
Mark.</p>
","<azure-data-factory>","2018-10-30 20:18:17","1678","2","1","53183839","<p><strong>Update</strong> 
I think there are a couple of bugs in the service so not expecting an answer.  I'll post updates here if I see resolution from the bug report I have posted <a href=""https://social.msdn.microsoft.com/Forums/en-US/002e6f51-4656-4dda-ade1-a35747ffeb63/adf-continuous-integration-datalake-fails-if-self-hosted-integration-selected?forum=AzureDataFactory"" rel=""nofollow noreferrer"">here</a> for the dev group.  </p>

<p>In a nutshell, this only affects you if </p>

<ol>
<li>you have a self hosted integration runtime (IR), and  </li>
<li>you are trying to deploy a template to a new data factory from an existing data factory (as you would in Dev->UAT->Prod)  </li>
<li>you have a datalake (ADL) linked service defined and using the self hosted IR.</li>
</ol>

<p>If you have a self hosted IR in the template, the newly deployed copy will not be registered with any server (either linked or unique to the new ADF) as the template only records an IR, it does not instantiate one.  </p>

<p>While this can be fixed in post deployment config or scripting, what it can't fix is the dependency in ADL.  This is because the ADL linked service wants to encrypt the service principal with the IR....but the IR does not exist at the time of template deployment (i.e. is not configured on a server and not active).</p>

<p>It is no better if you select Managed service identity as the auth on the ADL linked service instead of service principal, then the template fails to deploy because there are no credentials to encrypt and it looks like the resource is expecting to encrypt something.</p>

<p>The work around right now is to use Azure hosted IR for datalake connections.  Unfortunately for us this causes a security problem because shared IRs cannot be whitelisted in our ADL Gen 1.</p>

<p>I'll keep you posted.</p>
"
"53069984","How do I move a pipeline from an Azure Data Factory V2 to another (same Resource Group)?","<p>What is the easiest way of moving a pipeline across from an Azure Data Factory V2 to another?</p>

<p>Both ADF V2 are in the same resource group.</p>
","<sql><azure><pipeline><azure-data-factory>","2018-10-30 17:41:13","7412","3","3","53075025","<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">Continuous integration &amp; delivery</a> in Azure Data Factory moves pipelines from one environment (development, test, production) to another, which should meet your requirement.</p>
"
"53069984","How do I move a pipeline from an Azure Data Factory V2 to another (same Resource Group)?","<p>What is the easiest way of moving a pipeline across from an Azure Data Factory V2 to another?</p>

<p>Both ADF V2 are in the same resource group.</p>
","<sql><azure><pipeline><azure-data-factory>","2018-10-30 17:41:13","7412","3","3","53090649","<p>If this is a one off move, then export the RM template and import it to the other data factory remembering to change the parameters as appropriate (like the name).</p>

<p>If you have a self hosted Integration Runtime, you'll need to fix the IR reference once it is imported because it will replicate the IR but that IR should be linked to the original or register its own IR.  </p>

<p>If you combine Wang's suggestion and have a self hosted IR, then I'd monitor my post <a href=""https://stackoverflow.com/questions/53072270/release-pipeline-conflict-with-integration-runtime"">here</a> for some issues I am having with that.</p>

<p>M.</p>
"
"53069984","How do I move a pipeline from an Azure Data Factory V2 to another (same Resource Group)?","<p>What is the easiest way of moving a pipeline across from an Azure Data Factory V2 to another?</p>

<p>Both ADF V2 are in the same resource group.</p>
","<sql><azure><pipeline><azure-data-factory>","2018-10-30 17:41:13","7412","3","3","61661176","<p>I have found another way of moving pipelines from one ADF to another irrespective of Resource Group.</p>

<ol>
<li>Hover the mouse over the pipeline name and go to ""action menu"".</li>
<li>You will see following options in this menu</li>
</ol>

<p><a href=""https://i.stack.imgur.com/DQQTL.png"" rel=""nofollow noreferrer"">enter image description here</a></p>

<pre><code>Open
Clone
Move to
Download Support files
Delete
</code></pre>

<ol start=""3"">
<li><p>Click on ""Download support files"" option available in the action menu. It will allow you to download a zip version of the ADF artifacts linked to this pipeline locally on your system/laptop.</p></li>
<li><p>Please note that in case you have a driver pipeline which is implicitly calling other pipelines(one or more) and if you want to export the entire set of these pipelines then u just need to export the support files for the main pipeline which calls other pipelines.</p></li>
<li><p>When you open this zip folder locally on ur system or laptop, you will see files in this order</p></li>
</ol>

<pre><code>$ ls -lart
total 29
drwxr-xr-x 1 Is 1049089   0 May  6 14:16 ../
drwxr-xr-x 1 Is 1049089   0 May  6 14:16 ./
drwxr-xr-x 1 Is 1049089   0 May  6  2020 trigger/
drwxr-xr-x 1 Is 1049089   0 May  6  2020 pipeline/
drwxr-xr-x 1 Is 1049089   0 May  6  2020 linkedService/
drwxr-xr-x 1 Is 1049089   0 May  6  2020 integrationRuntime/
-rw-r--r-- 1 Is 1049089 260 May  6  2020 info.txt
-rw-r--r-- 1 Is 1049089 739 May  6  2020 diagnostic.json
drwxr-xr-x 1 Is 1049089   0 May  6  2020 dataset/
</code></pre>

<ol start=""6"">
<li><p>Next you need to create a local copy of your Git repo or Azure Devops Git repo on your system locally. Now, create a branch from your collaboration branch, say import-pipeline.</p></li>
<li><p>When you list the contents of your new branch import-pipeline, you will see the artifacts as below </p></li>
</ol>

<pre><code>$ ls -lart
total 37
drwxr-xr-x 1 Is 1049089  0 May  6 14:34 ../
-rw-r--r-- 1 Is 1049089 58 May  6 14:34 README.md
drwxr-xr-x 1 Is 1049089  0 May  6 14:34 ./
drwxr-xr-x 1 Is 1049089  0 May  6 14:36 notebooks/
drwxr-xr-x 1 Is 1049089  0 May  6 14:36 dataset/
drwxr-xr-x 1 Is 1049089  0 May  6 14:36 integrationRuntime/
drwxr-xr-x 1 Is 1049089  0 May  6 14:36 linkedService/
drwxr-xr-x 1 Is 1049089  0 May  6 14:36 pipeline/
drwxr-xr-x 1 Is 1049089  0 May  6 14:36 trigger/
drwxr-xr-x 1 Is 1049089  0 May  6 14:36 .git/
</code></pre>

<ol start=""8"">
<li><p>Now, you need to manually copy the artifacts from following folders of zip as mentioned in step 5 to corresponding folders of the import-pipeline branch mentioned in step 7
'''
dataset
pipeline
trigger
linkedSerevice
'''
Do not copy the integrationRuntime because the integrationRuntime say self-hosted will vary from project to project.</p></li>
<li><p>After importing the artifacts as mentioned in step 8, please change the values access credentials for linked services, key vault urls, secret names if any. The objective here is to make all the linked services should be able to connect successfully and all datasets should be able to browse successfully.</p></li>
<li><p>You need to now push the changes of your local branch import-pipeline back to the remote repo. At this stage, you should be able to see the new pipeline and its artifacts in your ADF when you select the branch import-pipeline in Git mode.</p></li>
<li><p>Test the newly imported pipeline in debug mode in your ADF. If satisfied, then merge the import-pipeline branch to the collaboration branch and publish your changes to the datafactory.</p></li>
</ol>
"
"53059331","Azure - Copy files from local to blob in real time","<p>I am working on Azure platform and perform data integration (ETL) activities using Azure Data Factory v2. I got a requirement to parse the message files in .txt format real time as and when they are dropped into external SFTP path. </p>

<p>The pipeline looks as below:-</p>

<p>Source --> SFTP server --> Azure Blob --> ADF Pipeline -- > Database</p>

<p>The pipeline should pickup the file from SFTP server and copy it to Azure blob as and when the files are dropped in external SFTP server. From there, the rest of the processing happens. </p>

<p>Can someone let me know how to make ADF job run in real time to look for new files deposited in external SFTP path? Any suggestions of using different tool/technology in Azure is also fine.</p>
","<azure><azure-eventhub><azure-data-factory>","2018-10-30 07:30:13","570","0","1","53059496","<p>You should have a look at <a href=""https://azure.microsoft.com/en-us/services/logic-apps/"" rel=""nofollow noreferrer"">Azure Logic App</a>, it has an <a href=""https://learn.microsoft.com/en-us/connectors/sftpconnector/"" rel=""nofollow noreferrer"">SFTP connector</a>:</p>

<ul>
<li><a href=""https://learn.microsoft.com/en-us/azure/connectors/connectors-create-api-sftp#sftp-trigger-when-a-file-is-added-or-modified"" rel=""nofollow noreferrer"">SFTP trigger: When a file is added or modified</a></li>
</ul>

<p>Then you copy your file to blob storage (using the <a href=""https://learn.microsoft.com/en-us/connectors/azureblobconnector/"" rel=""nofollow noreferrer"">blob connector</a>):</p>

<ul>
<li><a href=""https://learn.microsoft.com/en-us/azure/connectors/connectors-create-api-azureblobstorage#add-blob-storage-action"" rel=""nofollow noreferrer"">Create and manage blobs in Azure blob storage with Azure Logic Apps</a></li>
</ul>

<p>Then you trigger your ADP pipeline using the <a href=""https://learn.microsoft.com/en-us/connectors/azuredatafactory/#create-a-pipeline-run"" rel=""nofollow noreferrer"">ADP connector</a> (in preview)</p>

<p>So the logic app will be triggered when a new file is uploaded to the FTP then will copy the file to blob storage and trigger the ADP pipeline run without any code.</p>
"
"53052181","Can an System assigned managed service identity be added to an AAD group?","<p>I have an Azure Data Factory V2 service running with an MSI identity.  This service needs to access a Data Lake Gen 1 with thousands of folders and millions of files.</p>

<p>For efficiency, we have a group assigned to the root of the data lake which has RX permissions and these are inherited and defaulted throughout the tree.</p>

<p>I'd like to add the above ADF MSI to this group and I cannot figure out how to via the portal AAD blade.</p>

<p>I can assign this MSI to the datalake directly but it then has to update millions of files which is slow and error prone (the blade needs to be kept open while the permissions are applied and this often fails over the hours it takes due to a network glitch).</p>

<p>Mark.</p>
","<azure-data-lake><azure-data-factory><azure-managed-identity>","2018-10-29 19:02:58","4732","8","2","53055258","<p>Yes. You can add a system assigned managed identity to an Azure AD group. See this link, for how it can be achieved via PowerShell: <a href=""https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/tutorial-windows-vm-access-sql#create-a-group-in-azure-ad-and-make-the-vms-system-assigned-managed-identity-a-member-of-the-group"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/tutorial-windows-vm-access-sql#create-a-group-in-azure-ad-and-make-the-vms-system-assigned-managed-identity-a-member-of-the-group</a> </p>
"
"53052181","Can an System assigned managed service identity be added to an AAD group?","<p>I have an Azure Data Factory V2 service running with an MSI identity.  This service needs to access a Data Lake Gen 1 with thousands of folders and millions of files.</p>

<p>For efficiency, we have a group assigned to the root of the data lake which has RX permissions and these are inherited and defaulted throughout the tree.</p>

<p>I'd like to add the above ADF MSI to this group and I cannot figure out how to via the portal AAD blade.</p>

<p>I can assign this MSI to the datalake directly but it then has to update millions of files which is slow and error prone (the blade needs to be kept open while the permissions are applied and this often fails over the hours it takes due to a network glitch).</p>

<p>Mark.</p>
","<azure-data-lake><azure-data-factory><azure-managed-identity>","2018-10-29 19:02:58","4732","8","2","68512672","<p>This is also possible using the Azure CLI now:</p>
<pre><code>az ad group member add --group &lt;Group Object ID or Name&gt; --member-id &lt;Object ID of your managed identity&gt;
</code></pre>
"
"53047576","Azure Logic App - Update Blob API Connection through powershell","<p>I've searched online and browsed the available powershell cmdlets to try and find a solution for this problem but have been unsuccessful. Essentially, I have a few Data Factory pipelines that copy/archive incoming files and will use a web http post component that will invoke a Logic App that connects to a Blob container and will delete the incoming file. The issue I'm facing is that we have several automation runbooks that will rest Blob access keys every X days. When the Blob keys get reset the Logic App will fail whenever this happens because the connection is manually created in the designer itself and I can't specify a connection string that could pull from the Keyvault, as an example. Inside of the {Logic App > API Connections > Edit API Connection} we can manually update the connection string/key but obviously for an automated process we should be able to do this programmatically.</p>

<p>Is there a powershell cmdlet or other method I'm not seeing that would allow me to update/edit the API Connections that get created when using and Blob component inside a Logic App?</p>

<p>Any insights is appreciated!</p>
","<azure><azure-data-factory><azure-powershell><azure-logic-apps>","2018-10-29 14:23:47","1009","2","1","53057606","<p>Once you've rotated your key in the storage account, you can use an ARM template to update your connection API. In this ARM template, the connection api is created referencing the storage account internally so you don't have to provide the key:</p>

<p><strong>azuredeploy.json</strong> file:</p>

<pre><code>{
  ""$schema"": ""http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#"",
  ""contentVersion"": ""1.0.0.0"",
  ""parameters"": {
    ""azureBlobConnectionAPIName"": {
      ""type"": ""string"",
      ""metadata"": {
        ""description"": ""The name of the connection api to access the azure blob storage.""
      }
    },
    ""storageAccountName"": {
      ""type"": ""string"",
      ""metadata"": {
        ""description"": ""The Storage Account Name.""
      }
    }
  },
  ""variables"": {},
  ""resources"": [
    {
      ""type"": ""Microsoft.Web/connections"",
      ""name"": ""[parameters('azureBlobConnectionAPIName')]"",
      ""apiVersion"": ""2016-06-01"",
      ""location"": ""[resourceGroup().location]"",
      ""scale"": null,
      ""properties"": {
        ""displayName"": ""[parameters('azureBlobConnectionAPIName')]"",
        ""parameterValues"": {
          ""accountName"": ""[parameters('storageAccountName')]"",
          ""accessKey"": ""[listKeys(resourceId('Microsoft.Storage/storageAccounts', parameters('storageAccountName')),'2015-05-01-preview').key1]""
        },
        ""api"": {
          ""id"": ""[concat('subscriptions/', subscription().subscriptionId, '/providers/Microsoft.Web/locations/', resourceGroup().location, '/managedApis/azureblob')]""
        }
      },
      ""dependsOn"": []
    }
  ]
}
</code></pre>

<p><strong>azuredeploy.parameters.json</strong> file:</p>

<pre><code>{
  ""$schema"": ""https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#"",
  ""contentVersion"": ""1.0.0.0"",
  ""parameters"": {
    ""azureBlobConnectionAPIName"": {
      ""value"": ""myblobConnectionApiName""
    },
    ""storageAccountName"": {
      ""value"": ""myStorageAccountName""
    }
  }
}
</code></pre>

<p>You can them execute the arm template like that:</p>

<pre><code>Connect-AzureRmAccount

Select-AzureRmSubscription -SubscriptionName &lt;yourSubscriptionName&gt;

New-AzureRmResourceGroupDeployment -Name ""ExampleDeployment"" -ResourceGroupName ""MyResourceGroupName"" `
-TemplateFile ""D:\Azure\Templates\azuredeploy.json"" `
-TemplateParameterFile ""D:\Azure\Templates\azuredeploy.parameters.json""
</code></pre>

<p>to get started with ARM template and powerhsell, you cam have a look at this article:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/azure-resource-manager/resource-group-template-deploy"" rel=""nofollow noreferrer"">Deploy resources with Resource Manager templates and Azure PowerShell</a></p>
"
"53034259","Coping files from Azure blob storage to azure data lake store","<p>I am Coping files from Azure blob storage to azure data lake store, I need to pick files from year(folder)\month(folder)\day(txt files are on day bases).I am able to do one file with hadrcoded path but i am not able to pick file per day and process to copy in azure data lake store. Can anyone please help me.</p>

<p>I am using ADF V2 and using UI designer to create my connections,datasets and pipeline my steps are which i is working fine</p>

<ul>
<li>copy file from blob storage to data lake store</li>
<li>picking that file from data lake store and processing through usql for transform data.</li>
<li>that transform data i am saving in Azure SQL DB</li>
</ul>

<p>Please give me answer i am not able to get any help b/c all help is in JSON i am looking how i will define and pass parameters in UI designer.</p>

<p>Thanks </p>
","<azure-data-factory><azure-data-lake>","2018-10-28 17:29:22","388","1","1","53041919","<p>For the partitioned file path part, you could take a look at <a href=""https://stackoverflow.com/questions/51936847/azure-datafactory-incremental-blob-copy"">this post</a>. 
You could use copy data tool to handle it. </p>
"
"53010757","SSIS-IR ""There is no active worker agent.""","<p>I have a SSIS-IR running in data factory that is misbehaving!</p>

<p>I have a SSIS project that I have added a package that processes a file and if this job fails due to misconfigured file path or other file related issue I get an error in the SSIDB as per the image. ""Unexpected Termination""
<a href=""https://i.stack.imgur.com/pHD1N.png"" rel=""nofollow noreferrer"">img</a></p>

<p>Any subsequent job or project deployment will throw the error ""There is no active worker agent.""</p>

<p>The agent is started in Data Factory and is showing as available so I'm at a bit of a loss. Restarting solves the issue but it takes circa 30 minutes to bounce the integration runtime so is a real PITA when trying to solve my file problem.</p>
","<ssis><azure-data-factory>","2018-10-26 14:20:05","573","0","1","53210626","<p>There is a known issue that may cause the ""There is no active worker agent"" error in SSIS Integration Runtime now and we are actively working on the fix. Could you try to set you Logging Level to None when executing the package and see whether it solves the problem? Thanks.</p>

<p>Best regards,
Rodge</p>
"
"53001538","ADF Self hosted integration - how to get this to work when you have a code repository","<p>I set up the integration services self host using the template using the template <a href=""https://github.com/Azure/azure-quickstart-templates/tree/master/101-vms-with-selfhost-integration-runtime"" rel=""nofollow noreferrer"">here</a>.  The ADF I have is V2 and is has a code repository.  </p>

<p>When I use this template, the integration is added to the adf_publish branch (normally hidden) and not the master stored in VSTS/Git.  This means that none of the connections in the linked services can reference the self hosted integration because it is not visible to that branch.  </p>

<p>I have tried setting up a shared link but this does not work for 2 reasons.  The master branch does not have permission to the integration service and the adf_publish integration service cannot grant access to ""itself"" meaning you cannot select the current data factory as a grant.</p>

<p>Am i doing something wrong or does anyone know a work around?</p>

<p>Thanks, Mark.</p>
","<azure-data-factory>","2018-10-26 04:40:27","816","1","2","53043046","<p>If you set up a self-hosted IR in branch A, the IR will show in branch A (Git mode) and Data Factory mode. You can use the IR in these two mode.
If you want to use it in branch B, you can merge branch A to branch B.</p>
"
"53001538","ADF Self hosted integration - how to get this to work when you have a code repository","<p>I set up the integration services self host using the template using the template <a href=""https://github.com/Azure/azure-quickstart-templates/tree/master/101-vms-with-selfhost-integration-runtime"" rel=""nofollow noreferrer"">here</a>.  The ADF I have is V2 and is has a code repository.  </p>

<p>When I use this template, the integration is added to the adf_publish branch (normally hidden) and not the master stored in VSTS/Git.  This means that none of the connections in the linked services can reference the self hosted integration because it is not visible to that branch.  </p>

<p>I have tried setting up a shared link but this does not work for 2 reasons.  The master branch does not have permission to the integration service and the adf_publish integration service cannot grant access to ""itself"" meaning you cannot select the current data factory as a grant.</p>

<p>Am i doing something wrong or does anyone know a work around?</p>

<p>Thanks, Mark.</p>
","<azure-data-factory>","2018-10-26 04:40:27","816","1","2","53055276","<p>I dug into the RM template that I referenced and it will add the integration runtime (IR) service to the Data Factory mode ""branch"" and not the collaboration branch (master) which is the root cause.</p>

<p>Bo's answer is correct though the issue was the template - not manual setup of the IR.</p>

<p>The work around was to manually create the IR record in the master branch then create the IR VMs manually and install and authorize the integration gateway on those VMs.  This then propagates to the Data Factory mode and (hopefully) through the CI pipeline.</p>

<p>I have posted a bug to the template repo on GitHub - hopefully they can update it to work with Git hosted collaboration branch (master in our case).</p>

<p>Thanks, Mark.</p>
"
"53000355","Azure Data Factory copy data is slow","<p>Source database: PostgreSQL hosted on Azure VM D16s_v3
Destination database: SQL Server developer edition hosted on Azure VM D4s_v3
Source database is around 1TB in size
Destination database is empty with existing schema identical to source database</p>

<p>Throughput is only 1mb/s. Nothing helps. (I've selected max DIU) SQL Server doesn't have any keys or indexes at this point.</p>

<p>Batch size is 10000 </p>

<p>See screenshot:
<a href=""https://i.stack.imgur.com/yKpJv.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/yKpJv.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2018-10-26 01:51:04","5567","5","3","53002180","<p>Increase the batch size to 1000000.</p>
"
"53000355","Azure Data Factory copy data is slow","<p>Source database: PostgreSQL hosted on Azure VM D16s_v3
Destination database: SQL Server developer edition hosted on Azure VM D4s_v3
Source database is around 1TB in size
Destination database is empty with existing schema identical to source database</p>

<p>Throughput is only 1mb/s. Nothing helps. (I've selected max DIU) SQL Server doesn't have any keys or indexes at this point.</p>

<p>Batch size is 10000 </p>

<p>See screenshot:
<a href=""https://i.stack.imgur.com/yKpJv.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/yKpJv.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2018-10-26 01:51:04","5567","5","3","54651891","<p>If you are using TableName option then you should have that Table inside Dataset dropdown box. If you are extracting using SQL query then please check inside Dataset connection, click on edit and remove table name.</p>

<p>I had hit the same issue. If you select the query option and provide tablename in dataset, then you are confusing Azure Datafactory and making it ambiguous to decide on which option.</p>
"
"53000355","Azure Data Factory copy data is slow","<p>Source database: PostgreSQL hosted on Azure VM D16s_v3
Destination database: SQL Server developer edition hosted on Azure VM D4s_v3
Source database is around 1TB in size
Destination database is empty with existing schema identical to source database</p>

<p>Throughput is only 1mb/s. Nothing helps. (I've selected max DIU) SQL Server doesn't have any keys or indexes at this point.</p>

<p>Batch size is 10000 </p>

<p>See screenshot:
<a href=""https://i.stack.imgur.com/yKpJv.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/yKpJv.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2018-10-26 01:51:04","5567","5","3","55669876","<p>I got nailed by something similar when using ADF to copy data from an on-premises Oracle source to an Azure SQL Database sink. The same exact job performed via SSIS was something like 5 times faster. We began to suspect that something was amiss with data types, because the problem disappeared if we cast all of our high-precision Oracle NUMBER columns to less precision, or to something like integer.</p>

<p>It got so bad that we opened a case with Microsoft about it, and our worst fears were confirmed.</p>

<p>The Azure Data Factory runtime decimal type has a maximum precision of 28. If a decimal/numeric value from the source has a higher precision, ADF will first cast it to a string. The performance of the string casting code is abysmal.</p>

<p>Check to see if your source has any high-precision numeric data, or if you have not explicitly defined schema, see if you're perhaps accidentally using string.</p>
"
"52996257","Pass parameter to U-SQL Partition function","<p>We want to pass a parameter to U-SQL from Azure Data Factory v2 in order to control our level of parallelism in ADLA.</p>

<p>I, therefore, have the parameter <code>@USQL_Parallelism</code> that I pass from ADF.</p>

<p>How do I best integrate that in my script?</p>

<pre><code>@CompleteExtractClean = 
    SELECT
        [CE].[FileName],
        [CE].[iDocId],
        XXX
    FROM @CompleteExtract AS [CE]
    OPTION(PARTITION(iDocId)=(PARTITIONCOUNT=@USQL_Parallelism));
</code></pre>

<p>I now get the following error:</p>

<blockquote>
  <p>Invalid syntax found in the script.</p>
</blockquote>

<p>What's the syntax to correctly handle the parameter?</p>
","<u-sql><azure-data-factory>","2018-10-25 18:54:22","136","0","1","53000184","<p>were you using ADFv2 UI tool? If so, please follow the below picture to pass value to ""USQL_Parallelism"".</p>

<p><a href=""https://i.stack.imgur.com/h0i3J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/h0i3J.png"" alt=""enter image description here""></a>
if you were using Powershell or Rest API to run U-SQL activity, please add the following code in ""typeProperties"" part in pipeline json.</p>

<pre><code>""parameters"": {
  ""USQL_Parallelism"": ""1""  // the value you need
}
</code></pre>
"
"52989698","Copy Data From Azure Blob Storage to AWS S3","<p>I am new to Azure Data Factory and have an interesting requirement. </p>

<p>I need to move files from Azure Blob storage to Amazon S3, ideally using Azure Data Factory. </p>

<p>However S3 isnt supported as a sink;</p>

<p><a href=""https://i.stack.imgur.com/NwWsY.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/NwWsY.png"" alt=""enter image description here""></a></p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview</a></p>

<p>I also understand from a variety of comments i've read on here that you cannot directly copy from Blob Storage to S3 - you would need to download the file locally and then upload it to S3.</p>

<p>Does anyone know of any examples, in Data factory, SSIS or Azure Runbook that can do such a thing, I suppose an option would be to write an azure logic-app or function that is called from Data Factory. </p>
","<amazon-s3><ssis><azure-storage><azure-blob-storage><azure-data-factory>","2018-10-25 12:49:20","20980","10","4","53081579","<p>Managed to get something working on this - it might be useful for someone else. </p>

<p>I decided to write an azure function that uses a HTTP request as a trigger.</p>

<p>These two posts helped me a lot;</p>

<p><a href=""https://stackoverflow.com/questions/36411536/how-can-i-use-nuget-packages-in-my-azure-functions/53053897#53053897"">How can I use NuGet packages in my Azure Functions?</a></p>

<p><a href=""https://stackoverflow.com/questions/46861723/copy-from-azure-blob-to-aws-s3-using-c-sharp"">Copy from Azure Blob to AWS S3 using C#</a></p>

<p>Please note my answer to the Nuget packages if you are using Azure functions 2.x.</p>

<p>Here is the code - you can modify the basis of this to your needs. 
I return a JSON Serialized object because Azure Data Factory requires this as a response from a http request sent from a pipeline;</p>

<pre><code>#r ""Microsoft.WindowsAzure.Storage""
#r ""Newtonsoft.Json""
#r ""System.Net.Http""

using System.Net;
using Microsoft.AspNetCore.Mvc;
using Microsoft.Extensions.Primitives;
using Newtonsoft.Json;
using Microsoft.WindowsAzure.Storage.Blob;
using System.Net.Http;
using Amazon.S3; 
using Amazon.S3.Model;
using Amazon.S3.Transfer;
using Amazon.S3.Util;


public static async  Task&lt;IActionResult&gt; Run(HttpRequest req, ILogger log)
{
    log.LogInformation(""Example Function has recieved a HTTP Request"");

    // get Params from query string
    string blobUri = req.Query[""blobUri""];
    string bucketName = req.Query[""bucketName""];

    // Validate query string
    if (String.IsNullOrEmpty(blobUri) || String.IsNullOrEmpty(bucketName)) {

        Result outcome = new Result(""Invalid Parameters Passed to Function"",false,""blobUri or bucketName is null or empty"");
        return new BadRequestObjectResult(outcome.ConvertResultToJson());
    }

    // cast the blob to its type
    Uri blobAbsoluteUri = new Uri(blobUri);
    CloudBlockBlob blob = new CloudBlockBlob(blobAbsoluteUri);

    // Do the Copy
    bool resultBool = await CopyBlob(blob, bucketName, log);

    if (resultBool) { 
        Result outcome = new Result(""Copy Completed"",true,""Blob: "" + blobUri + "" Copied to Bucket: "" + bucketName);
        return (ActionResult)new OkObjectResult(outcome.ConvertResultToJson());       
    }
    else {
        Result outcome = new Result(""ERROR"",false,""Copy was not successful Please review Application Logs"");
        return new BadRequestObjectResult(outcome.ConvertResultToJson()); 
    }  
}

static async Task&lt;bool&gt; CopyBlob(CloudBlockBlob blob, string existingBucket, ILogger log) {

        var accessKey = ""myAwsKey"";
        var secretKey = ""myAwsSecret"";
        var keyName = blob.Name;

        // Make the client 
        AmazonS3Client myClient = new AmazonS3Client(accessKey, secretKey, Amazon.RegionEndpoint.EUWest1);

        // Check the Target Bucket Exists; 
        bool bucketExists = await AmazonS3Util.DoesS3BucketExistAsync (myClient,existingBucket);

        if (!bucketExists) {
            log.LogInformation(""Bucket: "" + existingBucket + "" does not exist or is inaccessible to the application"");
            return false;
        }

        // Set up the Transfer Utility
        TransferUtility fileTransferUtility = new TransferUtility(myClient);

        // Stream the file
        try {

            log.LogInformation(""Starting Copy"");

            using (var stream = await blob.OpenReadAsync()) {

                // Note: You need permissions to not be private on the source blob
                log.LogInformation(""Streaming"");

                await fileTransferUtility.UploadAsync(stream,existingBucket,keyName);

                log.LogInformation(""Streaming Done"");   
            }

            log.LogInformation(""Copy completed"");
        }
        catch (AmazonS3Exception e) {
                log.LogInformation(""Error encountered on server. Message:'{0}' when writing an object"", e.Message);
            }
        catch (Exception e) {
                log.LogInformation(""Unknown encountered on server. Message:'{0}' when writing an object"", e.Message);
                return false;
        }

        return true; 
    }

public class Result {

    public string result;
    public bool outcome;
    public string UTCtime;
    public string details; 

    public Result(string msg, bool outcomeBool, string fullMsg){
        result=msg;
        UTCtime=DateTime.Now.ToString(""yyyy-MM-dd h:mm:ss tt"");
        outcome=outcomeBool;
        details=fullMsg;
    }

    public string ConvertResultToJson() {
        return JsonConvert.SerializeObject(this);
    } 
}
</code></pre>
"
"52989698","Copy Data From Azure Blob Storage to AWS S3","<p>I am new to Azure Data Factory and have an interesting requirement. </p>

<p>I need to move files from Azure Blob storage to Amazon S3, ideally using Azure Data Factory. </p>

<p>However S3 isnt supported as a sink;</p>

<p><a href=""https://i.stack.imgur.com/NwWsY.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/NwWsY.png"" alt=""enter image description here""></a></p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview</a></p>

<p>I also understand from a variety of comments i've read on here that you cannot directly copy from Blob Storage to S3 - you would need to download the file locally and then upload it to S3.</p>

<p>Does anyone know of any examples, in Data factory, SSIS or Azure Runbook that can do such a thing, I suppose an option would be to write an azure logic-app or function that is called from Data Factory. </p>
","<amazon-s3><ssis><azure-storage><azure-blob-storage><azure-data-factory>","2018-10-25 12:49:20","20980","10","4","59302430","<p><strong>Download Files From Azure Storage using AzCopy into a temporal local repository</strong></p>

<p>You can download the files from Azure Cloud storage to your local system, just follow the below command, use the recursive flag to copy all the files</p>

<pre><code>azcopy /Source:[source_container_url] /Dest:[local_file_path] /Sourcekey:[source_storage_account_access_key] /s
</code></pre>

<p><strong>Upload Local Files to Amazon S3 using aws s3 cp command</strong></p>

<pre><code> aws s3 cp local_file_path s3://my-bucket/ --recursive
</code></pre>
"
"52989698","Copy Data From Azure Blob Storage to AWS S3","<p>I am new to Azure Data Factory and have an interesting requirement. </p>

<p>I need to move files from Azure Blob storage to Amazon S3, ideally using Azure Data Factory. </p>

<p>However S3 isnt supported as a sink;</p>

<p><a href=""https://i.stack.imgur.com/NwWsY.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/NwWsY.png"" alt=""enter image description here""></a></p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview</a></p>

<p>I also understand from a variety of comments i've read on here that you cannot directly copy from Blob Storage to S3 - you would need to download the file locally and then upload it to S3.</p>

<p>Does anyone know of any examples, in Data factory, SSIS or Azure Runbook that can do such a thing, I suppose an option would be to write an azure logic-app or function that is called from Data Factory. </p>
","<amazon-s3><ssis><azure-storage><azure-blob-storage><azure-data-factory>","2018-10-25 12:49:20","20980","10","4","74146210","<p>You can use <a href=""https://github.com/skyplane-project/skyplane"" rel=""nofollow noreferrer"">Skyplane</a> to copy data across clouds (110X speedup over CLI tools, with automatic compression to save on egress). To transfer from Azure blob storage to S3 you can call one of the commands:</p>
<pre><code>skyplane cp -r az://azure-bucket-name/ s3://aws-bucket-name/ 
skyplane sync -r az://azure-bucket-name/ s3://aws-bucket-name/
</code></pre>
"
"52989698","Copy Data From Azure Blob Storage to AWS S3","<p>I am new to Azure Data Factory and have an interesting requirement. </p>

<p>I need to move files from Azure Blob storage to Amazon S3, ideally using Azure Data Factory. </p>

<p>However S3 isnt supported as a sink;</p>

<p><a href=""https://i.stack.imgur.com/NwWsY.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/NwWsY.png"" alt=""enter image description here""></a></p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview</a></p>

<p>I also understand from a variety of comments i've read on here that you cannot directly copy from Blob Storage to S3 - you would need to download the file locally and then upload it to S3.</p>

<p>Does anyone know of any examples, in Data factory, SSIS or Azure Runbook that can do such a thing, I suppose an option would be to write an azure logic-app or function that is called from Data Factory. </p>
","<amazon-s3><ssis><azure-storage><azure-blob-storage><azure-data-factory>","2018-10-25 12:49:20","20980","10","4","75226406","<p>ADF now includes SFTP as a sink. From the same link provided in the question (supported as a sink is the far-right column):</p>
<p><a href=""https://i.stack.imgur.com/jGiM5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jGiM5.png"" alt=""SFTP Sink"" /></a></p>
<p>Using <a href=""https://aws.amazon.com/aws-transfer-family/"" rel=""nofollow noreferrer"">the AWS Transfer family</a> you can set up an SFTP server and add a user with an SSH public key, then use that configuration to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sftp?tabs=data-factory"" rel=""nofollow noreferrer"">set up an SFTP connection from ADF</a> that will connect directly to an S3 bucket.</p>
"
"52987372","How to execute U-SQL Stored Procedure In ADLA from Azure Data Factory V2","<p>How can I execute a U-SQL stored procedure in ADLA  from Azure Data Factory?
As of now, I could see that U-SQL activity looks for U-SQL script and it's location. </p>

<p>Likewise 'Stored Procedure' activity wherein any RDBMS stored procedures can be executed, I just need to invoke U-SQL stored procedure, without any wrapper scripts.</p>

<p>Am I missing anything here ?</p>
","<azure><u-sql><azure-data-factory>","2018-10-25 10:44:05","339","1","1","52996277","<p>U-SQL Stored Procedures can only be executed from a U-SQL script, so you would still need to wrap the call in a U-SQL script and reference the wrapper script in your ADF activity.</p>
"
"52975611","How to enable diagnostics in an Azure datafactory after creation","<p>I am trying to enable diagnostics for an azure datafactory using an ARM Template after is has been created via a c# automation application. I am attempting to use the steps here for the Non-Compute Resource Template:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/monitoring-and-diagnostics/monitoring-enable-diagnostic-logs-using-template"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/monitoring-and-diagnostics/monitoring-enable-diagnostic-logs-using-template</a></p>

<p>Step 2 of the above instructions state: </p>

<blockquote>
  <p>In the resources array of the resource for which you want to enable
  Diagnostic Logs, add a resource of type [resource
  namespace]/providers/diagnosticSettings.</p>
</blockquote>

<p>Here are where my questions lie:</p>

<p>I was hoping that I could enable the diagnostics for a datafactory(or any resource really) outside of the resources array for for that resource since the data factory is not being created as part of the ARM template. Is that possible?</p>

<p>If so what is the [resource namespace] in the above quote? I have tried using ""Microsoft.DataFactory/providers/diagnosticSettings"", but that fails as an invalid resource.</p>

<p>Here is the JSON I have thus far(remember this is outside of the resource array for the data factory, because it is already created in an earlier step).</p>

<pre><code> {
    ""type"": ""Microsoft.DataFactory/providers/diagnosticSettings"",
    ""name"": ""[concat('Microsoft.Insights/', parameters('factoryName'))]"",
    ""apiVersion"": ""2017-05-01-preview"",
    ""properties"": {
      ""name"": ""[parameters('factoryName')]"",
      ""workspaceId"": ""[parameters('workspaceId')]"",
      ""logs"": [
        {
          ""category"": ""/* log category name */"",
          ""enabled"": true,
          ""retentionPolicy"": {
            ""days"": 0,
            ""enabled"": false
          }
        }
      ],
      ""metrics"": [
        {
          ""category"": ""AllMetrics"",
          ""enabled"": true,
          ""retentionPolicy"": {
            ""enabled"": false,
            ""days"": 0
          }
        }
      ]
    }
  }
</code></pre>
","<azure><azure-data-factory><azure-rm-template>","2018-10-24 18:18:13","1154","4","1","53101125","<h2>try this (This worked for me)</h2>

<p>Let's think for example:</p>

<p>Deployment name: AzureADF-DiagSettings-Deployment <br/>
Deployment resource group: ADFactoryRG <br/>
Azure Data Factory instance name: ADFactory <br/>
Diagnostic settings name (In ADFactory): DiagService <br/>
Log analytics instance name: OMSWorkspace <br/>
Log analytics resource group: OMSWorkspaceRG <br/></p>

<pre><code>{
  ""$schema"": ""https://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json#"",
  ""contentVersion"": ""1.0.0.0"",
  ""parameters"": {},
  ""variables"": {},
  ""resources"": [
    {
      ""apiVersion"": ""2017-05-10"",
      ""name"": ""AzureADF-DiagSettings-Deployment"",
      ""type"": ""Microsoft.Resources/deployments"",
      ""resourceGroup"": ""ADFactoryRG"",
      ""properties"": {
        ""mode"": ""Incremental"",
        ""template"": {
          ""$schema"": ""https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#"",
          ""contentVersion"": ""1.0.0.0"",
          ""parameters"": {},
          ""variables"": {},
          ""resources"": [
            {
              ""type"": ""microsoft.datafactory/factories/providers/diagnosticsettings"",
              ""name"": ""ADFactory/Microsoft.Insights/DiagService"",
              ""apiVersion"": ""2017-05-01-preview"",
              ""properties"": {
                ""name"": ""DiagService"",
                ""storageAccountId"": null,
                ""eventHubAuthorizationRuleId"": null,
                ""eventHubName"": null,
                ""workspaceId"": ""OMSWorkspaceRG/Microsoft.OperationalInsights/workspaces/OMSWorkspace"",
                ""logs"": [
                  {
                    ""category"": ""PipelineRuns"",
                    ""enabled"": true,
                    ""retentionPolicy"": {
                        ""enabled"": false,
                        ""days"": 0
                    }
                  },
                  {
                    ""category"": ""TriggerRuns"",
                    ""enabled"": true,
                    ""retentionPolicy"": {
                        ""enabled"": false,
                        ""days"": 0
                    }
                  },
                  {
                    ""category"": ""ActivityRuns"",
                    ""enabled"": true,
                    ""retentionPolicy"": {
                        ""enabled"": false,
                        ""days"": 0
                    }
                  }
                ],
                ""metrics"": [
                  {
                    ""category"": ""AllMetrics"",
                    ""timeGrain"": ""PT1M"",
                    ""enabled"": true,
                    ""retentionPolicy"": {
                      ""enabled"": false,
                      ""days"": 0
                    }
                  }
                ]
              }
            }
          ],
          ""outputs"": {}
        },
        ""parameters"": {}
      }
    }
  ],
  ""outputs"": {}
}
</code></pre>
"
"52974860","Duplicate column name in table storage source in Azure data factory","<p><strong>Setup:</strong><br>
1. I have a Azure table storage table with the following schema</p>

<pre><code>PartitionKey
RowKey
TimeStamp [Generated By Azure table storage automatically]
DataField1
TimeStamp [Field added by code]
</code></pre>

<p>I have added this table as a source in my data factory.</p>

<p><strong>Issue</strong><br>
When i try to publish this data set, it gives me an error saying
<code>The name 'TIMESTAMP' is duplicated in 2 Schema</code></p>

<p>I tried removing the code generated TimeStamp in the <code>structure</code> of data. But the error still persists.</p>

<p>Please do suggest any way to import this dataset.</p>
","<azure-table-storage><azure-data-factory>","2018-10-24 17:29:40","2184","0","1","52986539","<p>Did you use the ADF visual authoring tool? 
Just checking the duplicated column and clicking delete button should work.</p>

<p>In you case you said you already remove the column but the error still exists, I suspect that you import the schema again before publishing somehow, thus the schema structure change is reverted. If that's not the case, please provide more details.</p>
"
"52974476","Azure Data Warehouse to CosmosDB sync with data factory pipeline","<p>I would like to set up a pipeline to sync data from Data Warehouse to no sql CosmosDB. Copy tool works fine for one to one table relations but for one to many obviously, I will have duplicates of objects in my NoSQL DB. What is the best way to solve this issue and have an array of one to many items instead of duplicating rows?
Thanks in advance</p>
","<azure><azure-cosmosdb><data-warehouse><azure-data-factory><warehouse>","2018-10-24 17:05:33","190","0","1","52984933","<p>In your case, I don’t think copy activity can achieve that. Copy activity just copy data from one table to another by appending new documents or do upsert based on cosmos dB ID.  Maybe you could write your own code to do the merging and then use ADF custom activity to invoke your code. 
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity</a></p>
"
"52963718","Azure Data Factory v2 Not Null Columns in sink","<p>I'm trying out Azure Data Factory v2 and I want to pipe data from an SQL source to an Oracle sink.</p>

<p>My problem is, that I have several Not Null columns in my Oracle tables which specify for example the date and time of which a dataset is loaded into Oracle. These columns however aren't existant in the SQL tables so when I want to start the pipeline I get the error that these columns can't be null in the Oracle sink.</p>

<p>My question is now, is it possible to artificially add these columns during the pipeline run so that these columns get filled by the Data Factory?<br>
Can I use a stored procedure or a custom activity for that?<br>
 Or do I have to create a Powershell script which ""hardcodes"" the values I want to add to the source?</p>
","<sql><oracle><powershell><azure-data-factory>","2018-10-24 07:58:26","1897","1","2","52964384","<p>Does this column has default value ? can you add default to this column then try? I not familiar with oracle pipe data however a similar approach in the below example adding a default value to a not null column.</p>

<pre><code>drop table ex_employee
/
create table ex_employee (id number(1) null ,name varchar2(100)  default 'A' not null )
/
insert into ex_employee(id)
select 1 from dual
/
commit
/

selecT * from ex_employee where id=1
</code></pre>
"
"52963718","Azure Data Factory v2 Not Null Columns in sink","<p>I'm trying out Azure Data Factory v2 and I want to pipe data from an SQL source to an Oracle sink.</p>

<p>My problem is, that I have several Not Null columns in my Oracle tables which specify for example the date and time of which a dataset is loaded into Oracle. These columns however aren't existant in the SQL tables so when I want to start the pipeline I get the error that these columns can't be null in the Oracle sink.</p>

<p>My question is now, is it possible to artificially add these columns during the pipeline run so that these columns get filled by the Data Factory?<br>
Can I use a stored procedure or a custom activity for that?<br>
 Or do I have to create a Powershell script which ""hardcodes"" the values I want to add to the source?</p>
","<sql><oracle><powershell><azure-data-factory>","2018-10-24 07:58:26","1897","1","2","52972309","<p>You can accomplish this in ADFv2 using a query against your source dataset in the Copy activity to insert values.</p>

<p>Using the table ex_employee, with the following configuration in each database:</p>

<p>Source table (SQL):</p>

<pre><code>ID int not null,
Name nvarchar(25) not null
</code></pre>

<p>Sink table (Oracle):</p>

<pre><code>ID number(p,0) not null,
Name nvarchar2(25) not null,
CreatedDate timestamp not null
</code></pre>

<p>In the Source configuration on your Copy activity in ADF, you would select the Query option under Use Query, and input a query, such as:</p>

<pre><code>SELECT ID, Name, CURRENT_TIMESTAMP AS CreatedDate FROM ex_employee
</code></pre>

<p>This will take the existing values from your SQL table, and insert a default value into the result set, which can then be inserted into your Oracle sink.</p>
"
"52954618","Multiple Source To a copy activity in Azure Data Factory","<p>On-premise stored procedure accepts two parameters and  returns multiple records, which needs to be written to azure data lake store(adls).
Those 2 parameters are existing in two different files, which I trying to  read from 2 individual Look UP activities in ADF.</p>

<p>Unfortunately Copy activity where stored procedure is being executed, accepts only one dependent input link. So how can I pass these two parameters to copy activity? </p>

<p>Are there any work arounds to look up two different files and pass the returned values to stored procedure?</p>
","<azure><azure-data-factory>","2018-10-23 17:21:55","2549","0","1","52960291","<p>suppose the 2 parameters store in two different blob files and the stored procedure is invoked in copy source side, please follow the below step:</p>

<ol>
<li><p>Create two lookup activity referring to blob dataset, this step is for look up 2 parameters. Let's suppose these 2 parameters reside in the first row of the 2 files respectively, so check the ""First row only"" in lookup activity.</p></li>
<li><p>Create a copy activity with the condition of these two lookup activity running successfully, invoke the stored procedure in copy source.</p></li>
<li><p>Import the two parameters of the stored procedure, here we name them as ""para1"" and ""para2"", the pass ""@activity('Lookup1').output.firstRow"" and ""@activity('Lookup2').output.firstRow"" to 2 parameters.</p></li>
</ol>

<p>The following pic shows this example:
<a href=""https://i.stack.imgur.com/xpdoU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xpdoU.png"" alt=""enter image description here""></a></p>
"
"52946836","Spark Job Chaining using Azure Data Factory V1","<p>Suppose we have three spark jobs say S1,S2 and S3 .</p>

<p>I want to configure these two jobs in a way so that once S1 is complete successfully, then S2 should be invoked .
On the other hand ,if S1 is failed then S3 should be invoked .
How this can be done using Azure Data Factory V1 ?</p>
","<apache-spark><azure-data-factory>","2018-10-23 10:30:11","47","0","1","53040546","<p>If you could use v2, this is <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities#activity-dependency"" rel=""nofollow noreferrer"">the v2 doc</a> for activity dependency.</p>
"
"52943945","Cannot read Excel file using U-SQL","<p>I am trying to read an Excel file and have to write in a csv file in Azure Datalake.
When I am trying to do it then it is showing error.</p>

<p><strong>U-SQL Script:</strong></p>

<pre><code>DECLARE @ExcelFile = @""/Output/demog_data_Merged_08022017.xlsx"";

@result01 = EXTRACT Id string,
UNIQUE_ID long,
SOL_ID int,
EMAIL_ID string,
mobilenumber string,
CUST_OPN_DATE DateTime,
gender char,
age int,
CUR_CITY string,
CUR_COUNTRY string,
CUR_PIN string,
NRE_CNTRY string,
MARITAL_STATUS char,
FREZ_CODE char,
UNFREEZ_DATE DateTime,
LAST_FREZ_DATE DateTime,
DORMANCY_STATUS char,
AVAILABLE_AMOUNT double,
ACCOUNT_OPEN_DATE DateTime,
nullcol string,
Salaried_account_flag char,
ACCOUNT_TYPE string
FROM @ExcelFile
USING new oh22is.Analytics.Formats.ExcelExtractor(""result01"");


@result02 = SELECT * FROM @result01;

OUTPUT @result02 TO ""/output/demog_for_report.csv""
USING Outputters.Csv();
</code></pre>

<p><strong>Error:</strong></p>

<pre><code>{
    ""errorCode"": ""2703"",
    ""message"": ""Error Id: E_CSC_USER_INVALIDCSHARP, Error Message: C# error CS0246: The type or namespace name 'oh22is' could not be found (are you missing a using directive or an assembly reference?). "",
    ""failureType"": ""UserError"",
    ""target"": ""U-SQL1""
}
</code></pre>
","<azure><azure-data-factory><u-sql>","2018-10-23 07:58:36","506","0","1","56479802","<p>Excel files cannot be read without an assembly reference. You need the files <code>DocumentFormat.OpenXml.dll</code> and <code>oh22is.Analytics.Formats.dll</code> in your data lake directory, alongside with your excel file (not necessarily at the same folder).</p>

<p>Assembly references hold the file reading logic and act as a gateway between the u-sql internal representation of data and the file format that, yielding data as it can be processed.</p>

<p>Sadly these files are not distributed individually as far as I know and Microsoft seems to demands you to compile it manually <a href=""https://github.com/Azure/AzureDataLake/tree/master/Samples/ExcelExtractor/oh22is.Analytics.Formats"" rel=""nofollow noreferrer"">from  source</a> with <a href=""https://github.com/Azure/AzureDataLake/"" rel=""nofollow noreferrer"">this repository</a> with Visual Studio. The good thing about using VS is that you can reference the assemblies directly to fasten your development (but I find it pointless, since I only use it for excel extracting and only need to generate the files once). The compilation process should also give you the dynamic linked library from the <code>documentformat.openxml</code> <a href=""https://www.nuget.org/packages/DocumentFormat.OpenXml/"" rel=""nofollow noreferrer"">package</a>, so that you don't have to download it or extract it from the <code>.nupkg</code> file, which, if you do, prefer to use the version at <code>/lib/net40/DocumentFormat.OpenXml.dll</code>, which is what worked for my xlsx file (2007-2019 format).</p>

<p>After putting the assembly files (two <code>.dll</code> files) in your data lake, make note of their path and use them like the following u-sql script:</p>

<pre><code>// Register the dependency to the analytics assembly (xml file reader)
DROP ASSEMBLY IF EXISTS openxml;
CREATE ASSEMBLY openxml FROM @""/MyProject/Assemblies/DocumentFormat.OpenXml.dll"";
REFERENCE ASSEMBLY openxml;

// Register the analytics assembly that read our excel file
DROP ASSEMBLY IF EXISTS analytics;
CREATE ASSEMBLY analytics FROM @""/MyProject/Assemblies/oh22is.Analytics.Formats.dll"";
REFERENCE ASSEMBLY analytics;

// Define a local variable for the excel file
DECLARE @ExcelFile = @""/MyProject/MyFolder/test-file.xlsx"";

@sheet = EXTRACT
    A string,
    B string,
    C string
FROM @ExcelFile
    USING new oh22is.Analytics.Formats.ExcelExtractor(""Sheet1"");

//And you can save, transform, select it like you would use any other data:

OUTPUT (SELECT * FROM @sheet) TO ""/MyProject/output.csv"" USING Outputters.Csv();
</code></pre>
"
"52937550","How can you get valid JSON for Azure Data Factory V2 Datasets/Pipelines using PowerShell?","<p>I have some pipelines in Azure Data Factory V2 that I need to update. Specifically I need to update the schema of the datasets. I initially created them using the user interface. Since the number of columns is very large, the user interface has become very slow for working on them, so I'd like to switch to using PowerShell. </p>

<p>I put together a script that I am able to use to update them from a JSON file using the <code>Set-AzureRmDataFactoryV2Dataset</code> and <code>Set-AzureRmDataFactoryV2Pipeline</code> cmdlets. I'd like to also be able to generate JSON files with the current configurations using PowerShell. I've tried using the matching <code>Get-AzureRmDataFactoryV2Dataset</code> and <code>Get-AzureRmDataFactoryV2Pipeline</code> cmdlets which have been useful for getting general information about them, but I haven't been able to go from the PSDataSet object returned to a valid JSON file. Below are the commands that I've been trying, but it ends up giving me a bunch of empty JSON arrays for the schema.</p>

<pre><code>$dtSrc = Get-AzureRmDataFactoryV2Dataset -ResourceGroupName $resourceGroupName -DataFactoryName $dataFactoryName -Name $dtSrcName
ConvertTo-Json $dtSrc.Properties -Depth 100 | Out-File ""$dtSrcName.json"" -Width 1000000
</code></pre>

<p><strong>Edit:</strong> Based on the answer by Wang Zhang, I've edited my <code>ConvertTo-Json</code> statement as shown below. The output looks much better now (no empty arrays), but it still doesn't match the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-datasets-linked-services#dataset-json"" rel=""nofollow noreferrer"">Azure Dataset JSON schema.</a></p>

<pre><code>$dtSrc = Get-AzureRmDataFactoryV2Dataset -ResourceGroupName $resourceGroupName -DataFactoryName $dataFactoryName -Name $dtSrcName
ConvertTo-Json $dtSrc -Depth 1 | Out-File ""$dtSrcName.json"" -Width 1000000
</code></pre>
","<json><powershell><azure-data-factory>","2018-10-22 20:47:24","1261","0","2","52940828","<p>would you please tell the reason that you set ""<em>-Depth -100</em>"" in the second Command? Seems remove it and modify the second command as the following works well:</p>

<p><em>ConvertTo-Json $dtSrc.Properties | Out-File ""$dtSrcName.json"" -Width 1000000</em></p>

<p>Please have a try.</p>
"
"52937550","How can you get valid JSON for Azure Data Factory V2 Datasets/Pipelines using PowerShell?","<p>I have some pipelines in Azure Data Factory V2 that I need to update. Specifically I need to update the schema of the datasets. I initially created them using the user interface. Since the number of columns is very large, the user interface has become very slow for working on them, so I'd like to switch to using PowerShell. </p>

<p>I put together a script that I am able to use to update them from a JSON file using the <code>Set-AzureRmDataFactoryV2Dataset</code> and <code>Set-AzureRmDataFactoryV2Pipeline</code> cmdlets. I'd like to also be able to generate JSON files with the current configurations using PowerShell. I've tried using the matching <code>Get-AzureRmDataFactoryV2Dataset</code> and <code>Get-AzureRmDataFactoryV2Pipeline</code> cmdlets which have been useful for getting general information about them, but I haven't been able to go from the PSDataSet object returned to a valid JSON file. Below are the commands that I've been trying, but it ends up giving me a bunch of empty JSON arrays for the schema.</p>

<pre><code>$dtSrc = Get-AzureRmDataFactoryV2Dataset -ResourceGroupName $resourceGroupName -DataFactoryName $dataFactoryName -Name $dtSrcName
ConvertTo-Json $dtSrc.Properties -Depth 100 | Out-File ""$dtSrcName.json"" -Width 1000000
</code></pre>

<p><strong>Edit:</strong> Based on the answer by Wang Zhang, I've edited my <code>ConvertTo-Json</code> statement as shown below. The output looks much better now (no empty arrays), but it still doesn't match the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-datasets-linked-services#dataset-json"" rel=""nofollow noreferrer"">Azure Dataset JSON schema.</a></p>

<pre><code>$dtSrc = Get-AzureRmDataFactoryV2Dataset -ResourceGroupName $resourceGroupName -DataFactoryName $dataFactoryName -Name $dtSrcName
ConvertTo-Json $dtSrc -Depth 1 | Out-File ""$dtSrcName.json"" -Width 1000000
</code></pre>
","<json><powershell><azure-data-factory>","2018-10-22 20:47:24","1261","0","2","52951917","<p>You could just download the ARM templates for your ADF (they use the json format) edit them and upload them again, either by using the UI, az-cli or even powershell. </p>

<p>Or even better, link a git repository to the ADF and then just clone the repo locally, make your changes and publish them to ADF by doing a push to the specified production branch (normally master).</p>
"
"52936138","Azure Data Factory to call on-prem Alteryx API","<p>I need to start a job in Alteryx (on-prem) from an ADFv2 pipeline and can't find a way to do so.  The built in Web activity doesn't seem to support OAUTH 1.0 signature process that Alteryx requires.  When trying to configure, I always get the error ""Error calling the endpoint"".  I've tried using the Web Activity + HTTP Linked Service and can't figure out the correct way to do this.  I need to do a POST (start job) and a GET (get job status) to an on-prem Alteryx API and I'm not even sure which activity is the best activity to do this.</p>
","<azure><azure-data-factory><alteryx>","2018-10-22 19:03:19","686","0","2","52951635","<p>U're right: web activity doesn't support more complex scenarios than just a simple request, and linked service doesn't support oAuth 1.0 auth.</p>

<p>From my point of view you're gonna have to extract the signature process that oAuth 1.0 requires. You could do one of the two:</p>

<ul>
<li><p>Use a custom activity where u're going to code the signature process and also call your Alteryx job.</p></li>
<li><p>Use something like an Azure Function to do the signature process and call your job. And then use the web activity to call your function (it's like calling a normal api with some custom headers for auth).</p></li>
</ul>

<p>Hope it helps.</p>
"
"52936138","Azure Data Factory to call on-prem Alteryx API","<p>I need to start a job in Alteryx (on-prem) from an ADFv2 pipeline and can't find a way to do so.  The built in Web activity doesn't seem to support OAUTH 1.0 signature process that Alteryx requires.  When trying to configure, I always get the error ""Error calling the endpoint"".  I've tried using the Web Activity + HTTP Linked Service and can't figure out the correct way to do this.  I need to do a POST (start job) and a GET (get job status) to an on-prem Alteryx API and I'm not even sure which activity is the best activity to do this.</p>
","<azure><azure-data-factory><alteryx>","2018-10-22 19:03:19","686","0","2","55026689","<p>I opened a ticket with Microsoft, and this was the response:</p>

<blockquote>
  <p>Currently the web activity can only run in ADF web role, not IR. This means the web activity does direct calls from the Azure servers to the end points, and is not capable of doing so through a self-hosted IR.</p>
</blockquote>
"
"52928907","How to copy Redshift S3 Unloaded Data on daily basis to Azure?","<p>I want to build a mechanism that will unload the data from redshift to S3 on daily basis in a folder and that data will get copied to Auzre on daily basis, I want to build it in such a way so that unload will take place on daily basis, and daily the old S3 file will get deleted so that Azure will take only the latest files in the Azure SQL Database. Can this be done. I am not able to find much on this.</p>
","<amazon-web-services><azure><amazon-s3><azure-data-factory>","2018-10-22 12:04:23","191","0","1","52931834","<p>ADF doesn't support s3 as sink. So you can't copy files to s3 with built-in copy activity.
But when source is Redshift, ADF provides an <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-amazon-redshift#use-unload-to-copy-data-from-amazon-redshift"" rel=""nofollow noreferrer"">unload setting</a> which will help you to unload data to s3 first. </p>

<p>Then you could just focus on copying data from Redshift to azure sql. In you case, you could <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger"" rel=""nofollow noreferrer"">use schedule trigger</a> or <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger"" rel=""nofollow noreferrer"">tumbling window trigger</a> to build a daily based pipeline.  </p>
"
"52918230","Random Username and password failures azure data factory","<p>Im trying to move data from an on-prem oracle database to an azure sql server instance with a self hosted IR since after the recent update(i think that was the cause) i keep getting random Invalid User name and Password on the copy activities.</p>

<p>Is there anyway to rollback the update or any ideas on how can i fix this ?</p>

<p><a href=""https://i.stack.imgur.com/duU1z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/duU1z.png"" alt=""Run 1""></a>
<a href=""https://i.stack.imgur.com/0v7Aj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0v7Aj.png"" alt=""Run2""></a></p>
","<azure-data-factory>","2018-10-21 17:52:47","182","1","1","53163210","<p>We encountered this starting on October 19 2018 with usernames that have underscores in them. We switched to using a username without underscores and that allowed us to make consistent connections from DataFactory.</p>
"
"52897373","Choosing between U-SQL and Spark / Databricks","<p>I have experience in making ETL solutions based on Azure Data Factory and Azure Data Lake Analytics (U-SQL).</p>

<p>But seems like Microsoft has started forcing Azure Databricks.</p>

<p>Is U-SQL is dying? I have not seen any news about new features since July.</p>

<p>The upcoming project is pretty simple. We have about 0.5 Tb of small JSON files stored on Azure Data Lake Storage. They needs to be transformed into a flat tables and joined in some way.</p>

<p>So my question is what to choose for a new project ADF+U-SQL or ADF+DataBricks?</p>
","<apache-spark><apache-spark-sql><azure-data-factory><u-sql><databricks>","2018-10-19 17:34:18","1476","2","1","52906690","<p>Spark's programming model for data engineering/transformation is fundamentally more flexible and extensible than U-SQL. </p>

<p>For small, simple projects you wouldn't notice the difference and I'd recommend you go with whatever you are familiar with. For complex projects and/or ones where you expect significant flux in requirements, I would strongly recommend Spark using one of the supported languages: Scala, Java, Python or R and not SparkSQL. The reason for the recommendation is that Spark's domain specific language (DSL) for data transformations makes the equivalent of SQL code generation, which is the trick all BI/analytics/warehousing tools use under the covers to manage complexity, very easy. It allows logic/configuration/customization to be organized and managed in manners that are impossible or impractical when dealing with SQL which, we should not forget, is a 40+ year old language.</p>

<p>For an extreme example of the level of abstraction that's possible with Spark, you might enjoy <a href=""https://databricks.com/session/the-smart-data-warehouse-goal-based-data-production"" rel=""nofollow noreferrer"">https://databricks.com/session/the-smart-data-warehouse-goal-based-data-production</a></p>

<p>I would also recommend Spark if you are dealing with dirty/untrusted data (the JSON in your case) where you'd like to have a highly controlled/custom ingestion process. In that case, you might benefit from some of the ideas in the <a href=""https://swoop-inc.github.io/spark-records/"" rel=""nofollow noreferrer"" title=""spark-records"">spark-records</a> library for bulletproof data processing. <a href=""https://databricks.com/session/bulletproof-jobs-patterns-for-large-scale-spark-processing"" rel=""nofollow noreferrer"">https://databricks.com/session/bulletproof-jobs-patterns-for-large-scale-spark-processing</a></p>

<p>When it comes to using Spark, especially for new users, Databricks provides the best managed environment. We've been a customer for years managing petabytes of very complex data. People on our team who come from SQL backgrounds and are not software developers use SparkSQL in Databricks notebooks but they benefit from the tooling/abstractions the data engineering and data science teams create for them.</p>

<p>Good luck with your project!</p>
"
"52889723","Unable to create SSIS Integration Runtime on Azure Data Factory","<p>I'm having an error when i try to create my SSIS Integration Runtime on Azure Data Factory :</p>

<p>Microsoft.Batch resource provider is not registered under the same subscription of VNet</p>

<p><a href=""https://i.stack.imgur.com/3NoAt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3NoAt.png"" alt=""enter image description here""></a></p>

<p>And here is the detail of the error :
<a href=""https://i.stack.imgur.com/QjDYq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QjDYq.png"" alt=""enter image description here""></a></p>

<p>Thank you</p>
","<azure><ssis><azure-data-factory>","2018-10-19 09:38:42","508","1","2","53273176","<p>A resource provider is a service that supplies the resources you can deploy and manage through Resource Manager. Each resource provider offers a set of resources and operations for working with an Azure service. For example, if you want to store keys and secrets, you work with the Microsoft.KeyVault resource provider.</p>

<p>It appears the SSIS Integration runtime requires a batch account. But you are not able to provision a batch account because the resource provider is not registered. To register a resource provider, you must have permission to perform the /register/action operation for the resource provider. This operation is included in the Contributor and Owner roles.</p>

<p>You can use PowerShell to register it: </p>

<pre><code>Register-AzureRmResourceProvider -ProviderNamespace Microsoft.Batch
</code></pre>

<p>You can also register it through the portal. Go to Subscriptions -> Resource Providers. Search for Microsoft.Batch and click the register link on that result. </p>

<p>You can find more info in the MS Docs <a href=""https://learn.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-supported-services#portal"" rel=""nofollow noreferrer"">here</a>.</p>
"
"52889723","Unable to create SSIS Integration Runtime on Azure Data Factory","<p>I'm having an error when i try to create my SSIS Integration Runtime on Azure Data Factory :</p>

<p>Microsoft.Batch resource provider is not registered under the same subscription of VNet</p>

<p><a href=""https://i.stack.imgur.com/3NoAt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3NoAt.png"" alt=""enter image description here""></a></p>

<p>And here is the detail of the error :
<a href=""https://i.stack.imgur.com/QjDYq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QjDYq.png"" alt=""enter image description here""></a></p>

<p>Thank you</p>
","<azure><ssis><azure-data-factory>","2018-10-19 09:38:42","508","1","2","55582704","<p>You get this error when you join the SSIS Integration Runtime to Azure VNet.</p>

<p>Go to your Subscription -> Resource Provider -> Microsoft.Batch and register it. </p>

<p>Microsoft.Batch is required because when you join the Integration Runtime to the VNet, Azure, behind the scenes uses Azure Batch service to provision necessary resources like Load Balancer, NSG, Public IP to continue the communication even after IR is within the VNet</p>
"
"52889081","Use Azure Functions as custom activity in ADFv2","<p>Is it possible to somehow package and execute already written azure function as a custom activity in azure data factory? </p>

<p>My workflow is next: </p>

<p>I want to use azure function (which is doing some data processing) in ADF pipeline as a custom activity. This custom activity is just one of the activities in pipeline but its key to be executed.</p>
","<azure><azure-data-factory>","2018-10-19 09:01:20","403","1","1","52889931","<blockquote>
  <p>Is it possible to somehow package and execute already written azure
  function as a custom activity in azure data factory?</p>
</blockquote>

<p>As I know, there is no way to do that so far. In my opinion, you do not need to package the Azure Function. I suggest you using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">Web Activity</a> to invoke the endpoint of your Azure Function which could merge into previous pipeline nicely.</p>
"
"52877620","U-SQL: how to deal with schema changes?","<p>My original script is something like this:</p>

<pre><code>@input = EXTRACT A string, B string, C string, 
         year string, month string, day string, filename string
    FROM @folder + ""/{year}/{month}/{day}/{filename}.csv""
    USING Extractors.Csv(skipFirstNRows : 1);

@input = SELECT A, B, C FROM @input;

OUTPUT @input
    TO @parent + ""/testtest.csv""
    USING Outputters.Csv(outputHeader : true);
</code></pre>

<p>This works fine, but sometimes the schema (columns) of the source file may change. The columns may become A, B, C, D or A, B, E.</p>

<p>I know Visual Studio can generate EXTRACT scripts. Is there a way to make U-SQL (or Visual Studio) deal with this and generate the extraction script dynamically and automatically?</p>
","<sql><visual-studio><azure><azure-data-factory><u-sql>","2018-10-18 15:33:27","145","3","1","52934421","<p>The Csv extractor not allow schema changes.
If you change the schema you will need to change your u-sql code!</p>

<p>The solution is create a custom extractor to do your job, or you can check flexible extractor that allows flexible columns schema.</p>

<p><a href=""https://blogs.msdn.microsoft.com/mrys/2016/08/15/how-to-deal-with-files-containing-rows-with-different-column-counts-in-u-sql-introducing-a-flexible-schema-extractor/"" rel=""nofollow noreferrer"">https://blogs.msdn.microsoft.com/mrys/2016/08/15/how-to-deal-with-files-containing-rows-with-different-column-counts-in-u-sql-introducing-a-flexible-schema-extractor/</a></p>
"
"52874553","ADF - Web Activity - 'String' does not match the field: 'body'","<p>I created a Web Activity in ADF and I am trying to send data with the POST method.
According to the documentation, the body takes a String (or expression with resultType string). I want to use the @concat String Function as shown in the image below.</p>

<p>I get a warning that says:
Expression of type: 'String' does not match the field: 'body'</p>

<p>I am confused, String does not match the body? The documentation says, the body takes a String?
Can anyone help?</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity</a></p>

<p><a href=""https://i.stack.imgur.com/fWvXa.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fWvXa.png"" alt=""&#39;String&#39; does not match the field: &#39;body&#39;""></a></p>
","<azure-data-factory>","2018-10-18 12:55:34","7554","7","1","52884699","<p>It should be an object type. Please use <code>@json('{""key"": ""value""}')</code></p>
"
"52870459","Can Azure Data Factory V2 copy files that are updated every 20 seconds from on-premises to Azure Data Lake as soon as they change","<p>I am trying to use ADF V2 to copy real time data from on-premises to Azure Data Lake Store.  I already installed a self-hosted Integration Runtime on the on-premises server.  Then I used the Copy Data wizard to create the pipeline and the datasets.  </p>

<p>I tried using a tumbling window trigger to fire as soon the file changes, but saw that the minimum time it will fire is 15 minutes.</p>

<p>I saw that there is a Get Metadata activity that can get the Last Modified Time on a file system file.  And I see that the Dataset allows parameters to be added.  However, in V2 I see no Help button or examples on that.  So I am assuming I would have to change the JSON file.</p>

<p>However, before I do that: the question is</p>

<blockquote>
  <blockquote>
    <p>can the pipeline be triggered - every 2 or 3 seconds (continuously) - to check if the files have changed and then copy them to Azure Data Lake store?</p>
  </blockquote>
</blockquote>

<p>If that is possible, any example(s) would be really helpful.</p>

<p>Thanks</p>
","<azure-data-factory>","2018-10-18 08:57:37","306","1","1","52894779","<p>What is your source on the on-premise server? You can use a combination of logic app and ADF to, so logic app can be triggered as soon as file arrives at your on-premise source side, and then the logic app can invoke data factory pipeline to copy that file.</p>
"
"52865232","Azure data factory pipeline - copy blob and store filename in a DocumentDB or in Azure SQL","<p>I set up 2 blob storage folders called ""input"" and ""output"". My pipeline gets triggered when a new file arrives in ""input"" and copies that file to the ""output"" folder. Furthermore I do have a Get Metadata activity where I receive the copied filename(s).</p>

<p>Now I would like to store the filename(s) of the copied data into a DocumentDB.
I tried to use the ForEach activity with it, but here I am stuck.</p>

<p>Basically I tried to use parts from this answer: <a href=""https://stackoverflow.com/questions/51853329/add-file-name-as-column-in-data-factory-pipeline-destination/51858408#51858408"">Add file name as column in data factory pipeline destination</a></p>

<p>But I don't know what to assign as Source in the CopyData activity since my source are the filenames from the ForEach activity - or am I wrong?</p>
","<azure-data-factory>","2018-10-18 00:12:27","1345","1","2","52866040","<p>You may try use a custom activity to insert filenames into Document Db.</p>

<p>You can pass filenames as parameters to the custom activity, and write your own code to insert data into Document Db.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity</a></p>
"
"52865232","Azure data factory pipeline - copy blob and store filename in a DocumentDB or in Azure SQL","<p>I set up 2 blob storage folders called ""input"" and ""output"". My pipeline gets triggered when a new file arrives in ""input"" and copies that file to the ""output"" folder. Furthermore I do have a Get Metadata activity where I receive the copied filename(s).</p>

<p>Now I would like to store the filename(s) of the copied data into a DocumentDB.
I tried to use the ForEach activity with it, but here I am stuck.</p>

<p>Basically I tried to use parts from this answer: <a href=""https://stackoverflow.com/questions/51853329/add-file-name-as-column-in-data-factory-pipeline-destination/51858408#51858408"">Add file name as column in data factory pipeline destination</a></p>

<p>But I don't know what to assign as Source in the CopyData activity since my source are the filenames from the ForEach activity - or am I wrong?</p>
","<azure-data-factory>","2018-10-18 00:12:27","1345","1","2","52866113","<p>Based on your requirements, I suggest you using <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-triggered-function"" rel=""nofollow noreferrer"">Blob Trigger Azure Functions</a> to combine with your current Azure data factory business.</p>

<p>Step 1: still use event trigger in adf to transfer between input and output.</p>

<p>Step 2: assign Blob Trigger Azure Functions to output folder.</p>

<p>Step 3: the function will be triggered as soon as a new file created into it.Then get the file name and use Document DB SDK to store it into document db.</p>

<p>.net document db SDK: <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/sql-api-sdk-dotnet"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/cosmos-db/sql-api-sdk-dotnet</a></p>

<p>Blob trigger bindings, please refer to here: <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob</a></p>
"
"52859958","Is there any workaround for using Append blob in Azure Data Factory copy?","<p>I don't want my copy data activity in ADF pipeline to overwrite a blob file but instead, append the new data to it. But ADF only supports Blockblob so is there a way to append data to an existing file? </p>
","<azure><azure-blob-storage><azure-data-factory>","2018-10-17 16:53:01","3244","3","1","52957565","<p>You can use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">Web Activity</a> instead of Copy Activity. Using <a href=""https://learn.microsoft.com/en-us/rest/api/storageservices/blob-service-rest-api"" rel=""nofollow noreferrer"">Blob Service Rest API</a> you could send a PUT request with data you want to append to a blob.
You can optionally consume datasets and linked services by this activity.</p>

<p><strong>JSON:</strong></p>

<pre><code>{  
   ""name"":""AppendDataActivity"",
   ""type"":""WebActivity"",
   ""typeProperties"":{  
      ""method"":""Put"",
      ""url"":""https://&lt;myAccount&gt;.blob.core.windows.net/&lt;myContainer&gt;/&lt;myBlob&gt;&lt;SASToken&gt;&amp;comp=appendblock"",
      ""headers"":{  
         ""Content-Type"":""application/json"" 
      },
      ""datasets"":[  
         {  
            ""referenceName"":""&lt;consumedDatasetName&gt;"",
            ""type"":""DatasetReference"",
            ""parameters"":{  
               ...
            }
         }
      ],
      ""linkedServices"":[  
         {  
            ""referenceName"":""&lt;consumedLinkedServiceName&gt;"",
            ""type"":""LinkedServiceReference""
         }
      ]
   }
}
</code></pre>

<p><em>Note that Body can only support JSON object or JSON value, JSON array is being fixed but non-JSON values aren't supported. For this workaround you could use an Azure Function as endpoint, where you could append files or whatever you want.</em></p>
"
"52843154","Web activity throws overlimit error when calling rest api","<p>My ADF pipeline has a lookup activity which uses a sql query to get data from a table and passes it to a web activity which posts the JSON to an API (azure app service). When the query gets 1000 rows it works fine but when I try over 5000 rows the web activity returns the error.</p>

<pre><code>""errorCode"": ""2001"",
""message"": ""The length of execution ouput is over limit (around 1M currently). "",
""failureType"": ""UserError"",
</code></pre>

<p>When I post the 5000 rows to the API using postman it works fine. Any idea what this error means and how to resolve it?</p>

<p>Thanks in advance.</p>
","<azure><azure-data-factory>","2018-10-16 19:56:35","2952","2","2","52844136","<p>Looks like the web activity times out after 1 minute if the endpoint its calling doesn't respond.</p>

<p><a href=""https://i.stack.imgur.com/CWeFJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CWeFJ.png"" alt=""enter image description here""></a></p>
"
"52843154","Web activity throws overlimit error when calling rest api","<p>My ADF pipeline has a lookup activity which uses a sql query to get data from a table and passes it to a web activity which posts the JSON to an API (azure app service). When the query gets 1000 rows it works fine but when I try over 5000 rows the web activity returns the error.</p>

<pre><code>""errorCode"": ""2001"",
""message"": ""The length of execution ouput is over limit (around 1M currently). "",
""failureType"": ""UserError"",
</code></pre>

<p>When I post the 5000 rows to the API using postman it works fine. Any idea what this error means and how to resolve it?</p>

<p>Thanks in advance.</p>
","<azure><azure-data-factory>","2018-10-16 19:56:35","2952","2","2","52865841","<p>As you found, web activity has times out limitation for 1 minute. Also, based on the above error <code>The length of execution output is over limit (around 1M currently).</code>, web activity also has output size limitation for 1 MB.</p>

<p>You could find the limitation rules <a href=""https://learn.microsoft.com/en-us/azure/azure-subscription-service-limits#data-factory-limits"" rel=""nofollow noreferrer"">here</a> and some of the them could be adjusted if you ask for <a href=""https://azure.microsoft.com/en-us/blog/azure-limits-quotas-increase-requests/"" rel=""nofollow noreferrer"">Contact Support</a>.</p>

<p>If nothing they can do,I provide you with a workaround that you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">ForEach Activity</a>. Maybe you need to use <code>paging query</code> for your rest api and return a limited number of the data each time. Then query your data in loop until the number of return data is lower than threshold value.</p>
"
"52840057","Pass a value from inside the Azure ADF pipeline to a PowerShell where the pipeline invoked","<p>I want to do some steps in my PowerShell based on a value from an Azure ADF(Azure Data Factory) pipeline. How can I pass a value from an ADF pipeline to the PowerShell, where I invoked this ADF Pipeline?  So that, I can do the appropriate steps in the PowerShell based on a value I received from ADF pipeline.</p>

<p>NOTE: I am not looking for the run-status of the pipeline (success, failure etc), but I am looking for some variable-value that we get inside a pipeline - say, a flag-value we obtained from a table using a Lookup activity etc.</p>

<p>Any thoughts?</p>
","<azure><powershell><azure-pipelines><azure-data-factory>","2018-10-16 16:29:14","754","0","1","52868493","<p>KPK, the requirements you're talking about definitely can be fulfilled though I do not know where does your Powershell scripts run.</p>

<p>You could write your Powershell scripts in <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook"" rel=""nofollow noreferrer"">HTTP Trigger Azure Function</a>,please refer to this <a href=""https://www.brianbunke.com/blog/2018/02/27/powershell-in-azure-functions/"" rel=""nofollow noreferrer"">doc</a>. Then you could get the output of the pipeline in Powershell: </p>

<p><a href=""https://learn.microsoft.com/en-us/powershell/module/azurerm.datafactoryv2/invoke-azurermdatafactoryv2pipeline?view=azurermps-4.4.1#outputs"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/powershell/module/azurerm.datafactoryv2/invoke-azurermdatafactoryv2pipeline?view=azurermps-4.4.1#outputs</a>.</p>

<p>Then pass the value you want to HTTP Trigger Azure Function as parameters.</p>
"
"52825497","How do I raise an error from if activity in ADFV2?","<p>I have a child pipeline that receives instructions from a control file using a dataset.  These instructions define what directory to copy files from.  </p>

<p>First this child pipeline checks that a file exists via Get Metadata activity on the source folder.  It then executes a child pipeline to process the data if one or more childitems are returned from GetMetaData.</p>

<p>In the control dataset, there is also a required Y/N field meaning I can ignore the error if the folder or file does not exist.  </p>

<p>If the folder does not exist, the GetMEtadata will fail.  If it exists but no files, I get 0 child items.  So 2 different things happen for file or folder missing (an error or 0 items).</p>

<p>In either case, I need to route the output of GetMetaData to an IF that checks if the file was required.  If not, consume the error and return.  If it was required, raise an error.</p>

<p>I can't find a way to raise an error though.  Just as important, is there an alternative approach that would work or fit better with ADF V2 design?</p>

<p>Many thanks,
Mark.</p>
","<azure-data-factory>","2018-10-15 22:20:37","7823","9","6","52829367","<p>I have the similar scenario and I manage that with SqlStoreProcedure.</p>
<p><code>&quot;storedProcedureName&quot;: &quot;sp_executesql&quot;, &quot;storedProcedureParameters&quot;: { &quot;stmt&quot;: { &quot;value&quot;: &quot;Declare @err_msg NVARCHAR(150)SET @err_msg=N'Error occurred in this pipeline somehow somewhere something. Best regards. EXISTSCheers'RAISERROR(@err_msg,15,1)&quot;, &quot;type&quot;: &quot;string&quot; } }</code></p>
<p>StoredProcedureName: sp_executesql</p>
<p>StoredProcedureParameter: stmt</p>
<p>Value: Declare @err_msg NVARCHAR (150) SET @err_msg=N'Error occurred in this pipeline somehow somewhere something. Best regards. EXISTSCheers'RAISERROR (@err_msg,15,1) (of course you can change error text:)</p>
<p>Type: string</p>
"
"52825497","How do I raise an error from if activity in ADFV2?","<p>I have a child pipeline that receives instructions from a control file using a dataset.  These instructions define what directory to copy files from.  </p>

<p>First this child pipeline checks that a file exists via Get Metadata activity on the source folder.  It then executes a child pipeline to process the data if one or more childitems are returned from GetMetaData.</p>

<p>In the control dataset, there is also a required Y/N field meaning I can ignore the error if the folder or file does not exist.  </p>

<p>If the folder does not exist, the GetMEtadata will fail.  If it exists but no files, I get 0 child items.  So 2 different things happen for file or folder missing (an error or 0 items).</p>

<p>In either case, I need to route the output of GetMetaData to an IF that checks if the file was required.  If not, consume the error and return.  If it was required, raise an error.</p>

<p>I can't find a way to raise an error though.  Just as important, is there an alternative approach that would work or fit better with ADF V2 design?</p>

<p>Many thanks,
Mark.</p>
","<azure-data-factory>","2018-10-15 22:20:37","7823","9","6","58142557","<p>I use a Set Variable Task to achieve this. </p>

<p>In the variable expression, I divide by zero. This throws an error which propigates up my pipeline(s) as needed. Most generic way I could think to do it.</p>

<pre><code>""name"": ""Throw Error"",
""description"": ""You can't throw an error in DF so I try to divide by zero. :)"",
""type"": ""SetVariable"",
""dependsOn"": [],
""userProperties"": [],
""typeProperties"": {
   ""variableName"": ""ThrowError"",
   ""value"": {
      ""value"": ""@div(1,0)"",
      ""type"": ""Expression""
   }
}
</code></pre>
"
"52825497","How do I raise an error from if activity in ADFV2?","<p>I have a child pipeline that receives instructions from a control file using a dataset.  These instructions define what directory to copy files from.  </p>

<p>First this child pipeline checks that a file exists via Get Metadata activity on the source folder.  It then executes a child pipeline to process the data if one or more childitems are returned from GetMetaData.</p>

<p>In the control dataset, there is also a required Y/N field meaning I can ignore the error if the folder or file does not exist.  </p>

<p>If the folder does not exist, the GetMEtadata will fail.  If it exists but no files, I get 0 child items.  So 2 different things happen for file or folder missing (an error or 0 items).</p>

<p>In either case, I need to route the output of GetMetaData to an IF that checks if the file was required.  If not, consume the error and return.  If it was required, raise an error.</p>

<p>I can't find a way to raise an error though.  Just as important, is there an alternative approach that would work or fit better with ADF V2 design?</p>

<p>Many thanks,
Mark.</p>
","<azure-data-factory>","2018-10-15 22:20:37","7823","9","6","60539251","<p>The solution I am using is to create a mock API in API Management that returns 500 HTTP code together with an error message. More details on how to do this can be found here: <a href=""https://learn.microsoft.com/en-us/azure/api-management/mock-api-responses"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/api-management/mock-api-responses</a>.</p>

<p>From inside the ADF pipeline, I call this mock API using Web Activity passing in the error message.</p>

<p>Then I can see the failed pipeline with expected error.</p>
"
"52825497","How do I raise an error from if activity in ADFV2?","<p>I have a child pipeline that receives instructions from a control file using a dataset.  These instructions define what directory to copy files from.  </p>

<p>First this child pipeline checks that a file exists via Get Metadata activity on the source folder.  It then executes a child pipeline to process the data if one or more childitems are returned from GetMetaData.</p>

<p>In the control dataset, there is also a required Y/N field meaning I can ignore the error if the folder or file does not exist.  </p>

<p>If the folder does not exist, the GetMEtadata will fail.  If it exists but no files, I get 0 child items.  So 2 different things happen for file or folder missing (an error or 0 items).</p>

<p>In either case, I need to route the output of GetMetaData to an IF that checks if the file was required.  If not, consume the error and return.  If it was required, raise an error.</p>

<p>I can't find a way to raise an error though.  Just as important, is there an alternative approach that would work or fit better with ADF V2 design?</p>

<p>Many thanks,
Mark.</p>
","<azure-data-factory>","2018-10-15 22:20:37","7823","9","6","66623655","<p>I use a Web Activity and GET a URL that doesn't exist: <code>www.notreal-nvlsuifhg9348h4932.com</code>.</p>
<p>(I do like Chris Hume's Set 1/0 option though. Might steal that ;) )</p>
"
"52825497","How do I raise an error from if activity in ADFV2?","<p>I have a child pipeline that receives instructions from a control file using a dataset.  These instructions define what directory to copy files from.  </p>

<p>First this child pipeline checks that a file exists via Get Metadata activity on the source folder.  It then executes a child pipeline to process the data if one or more childitems are returned from GetMetaData.</p>

<p>In the control dataset, there is also a required Y/N field meaning I can ignore the error if the folder or file does not exist.  </p>

<p>If the folder does not exist, the GetMEtadata will fail.  If it exists but no files, I get 0 child items.  So 2 different things happen for file or folder missing (an error or 0 items).</p>

<p>In either case, I need to route the output of GetMetaData to an IF that checks if the file was required.  If not, consume the error and return.  If it was required, raise an error.</p>

<p>I can't find a way to raise an error though.  Just as important, is there an alternative approach that would work or fit better with ADF V2 design?</p>

<p>Many thanks,
Mark.</p>
","<azure-data-factory>","2018-10-15 22:20:37","7823","9","6","67827628","<p>I have found the easiest way to throw an error is to do a SQL lookup on the following query:</p>
<pre><code>THROW 99999, 'Error message thrown', 1
</code></pre>
<p>For us, we make extensive use of Azure Function Apps, so we created a special function called &quot;ThrowError&quot; that throws an error with a message provided as a parameter. That is the easiest only if you already use Azure Function Apps, not worth it to just create the one function.</p>
"
"52825497","How do I raise an error from if activity in ADFV2?","<p>I have a child pipeline that receives instructions from a control file using a dataset.  These instructions define what directory to copy files from.  </p>

<p>First this child pipeline checks that a file exists via Get Metadata activity on the source folder.  It then executes a child pipeline to process the data if one or more childitems are returned from GetMetaData.</p>

<p>In the control dataset, there is also a required Y/N field meaning I can ignore the error if the folder or file does not exist.  </p>

<p>If the folder does not exist, the GetMEtadata will fail.  If it exists but no files, I get 0 child items.  So 2 different things happen for file or folder missing (an error or 0 items).</p>

<p>In either case, I need to route the output of GetMetaData to an IF that checks if the file was required.  If not, consume the error and return.  If it was required, raise an error.</p>

<p>I can't find a way to raise an error though.  Just as important, is there an alternative approach that would work or fit better with ADF V2 design?</p>

<p>Many thanks,
Mark.</p>
","<azure-data-factory>","2018-10-15 22:20:37","7823","9","6","70240573","<p>In November 2021 ADF gained the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-fail-activity"" rel=""nofollow noreferrer"">Fail</a> activity, in the General group.</p>
<p>It has two specific properties, Fail message and Error code, both of which are dynamic. They behave exactly as would be expected.</p>
<p>When executed it always has the status Failed. The message and code are evaluated and appear in the pipeline output.</p>
"
"52822956","Deploy nested resources separately","<p>Due to separation of duty I need to split an existing ARM template into two single templates - one for the resource and one for the logging</p>

<p>the original templates looks like this:</p>

<pre><code>""resources"": [
{ // https://learn.microsoft.com/en-us/azure/templates/microsoft.datafactory/factories
  ""type"": ""Microsoft.DataFactory/factories"",
  ""name"": ""[variables('dataFactoryName')]"",
  ""apiVersion"": ""[variables('apiVersion')]"",
  ""location"": ""[resourceGroup().location]"",
  ""tags"": {},
  ""identity"": {
    ""type"": ""SystemAssigned""
  },
  ""properties"": {},
  ""resources"": [
    {
      ""type"": ""providers/diagnosticSettings"",
      ""name"": ""[concat('Microsoft.Insights/', variables('logSettingName'))]"",
      ""dependsOn"": [
        ""[resourceId('Microsoft.DataFactory/factories', variables('dataFactoryName'))]""
      ],
      ""apiVersion"": ""2017-05-01-preview"",
      ""location"": ""[resourceGroup().location]"",
      ""tags"": {},
      ""properties"": {
        ""name"": ""[variables('logSettingName')]"",

        ""workspaceId"": ""[concat('/subscriptions/', subscription().subscriptionId, '/resourceGroups/', parameters('logAnalyticsObject').resourceGroup, '//providers/Microsoft.OperationalInsights/workspaces/', parameters('logAnalyticsObject').name)]"",

        ""logs"": ""[parameters('logAnalyticsObject').adfv2.logs]"",
        ""metrics"": ""[parameters('logAnalyticsObject').adfv2.metrics]""
      }
    }
  ]
}
</code></pre>

<p>The first part is quite easy, I just remove the sub-resource but how to get the second part (resource with ""type"": ""providers/diagnosticSettings"") correctly so it can be deployed from a different template?
Is this possible at all or are these strongly tied together?</p>

<p>I already tried different things like</p>

<pre><code>""type"": ""Microsoft.DataFactory/factories/providers/diagnosticSettings"",
""name"": ""[concat('Microsoft.Insights/', variables('name'))]"",
</code></pre>

<p>but ended up with error messages like this:</p>

<pre><code>Unable to process template language expressions for resource '/subscriptions/fb1e20c4-0878-4949-ac10-f92a9ac35db4/resourceGroups/swe-sdp-dv0
-rgp-adp/providers/Microsoft.Resources/deployments/DataFactory_LogAnalytics_Resource' at line '67' and column '5'. 'Unable to evaluate template language 
function 'resourceId': function requires exactly one multi-segmented argument which must be resource type including resource provider namespace. Current 
function arguments 'fb1e20c4-0878-4949-ac10-f92a9ac35db4,swe-sdp-dv0-rgp-anl,Microsoft.Insights,swe-sdp-dv0-oms-001'. Please see 
https://aka.ms/arm-template-expressions/#resourceid for usage details.
</code></pre>

<p>I think to make it work I would need the right combination of ""type"", ""name"" and probably also ""dependsOn""</p>
","<azure><azure-resource-manager><azure-data-factory>","2018-10-15 18:44:58","121","0","1","52823112","<p>ok, according to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor#set-up-diagnostic-logs"" rel=""nofollow noreferrer"">this</a>, you would need to do this:</p>

<pre><code>""type"": ""Microsoft.DataFactory/factories/providers/diagnosticSettings"",
""name"": ""[concat(variables('dataFactoryName'), '/Microsoft.Insights/', variables('name'))]"",
</code></pre>

<p>you dont need dependsOn, because resources are under different templates.</p>
"
"52818037","How to execute on-premises python script from ADF","<p>I have a python script resides in an Azure VM. This script uses a few local files from this VM. I need to create an ADF  pipeline which will execute this python script residing in this on-premises VM. As the script is placed on-premises, I can't use any cluster activity of ADF. So Basically the pipeline should connect to the VM and trigger the script execution. I could think of an option of using Custom Activity of ADF and trigger Powershell command from there to this on-premises python script. But not sure how to connect to on-premises scripts. </p>
","<azure><azure-data-factory>","2018-10-15 13:36:42","1624","0","1","52850577","<p>After my researching, you do could run Python script in ADF custom activity. However, based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity#custom-activity"" rel=""nofollow noreferrer"">official document</a>,you need to rely on Azure Batch Service to put your scripts and dependencies to the folder path in the Azure Batch Service. So, I think it's properly for you to execute on-premises Python scripts situation.</p>

<p>I provide you with a workaround.</p>

<p>step1: expose an endpoint to executing your on-premises Python scripts, of course, the local files could be touched.</p>

<p>step2: then use <a href=""https://learn.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-about-vpngateways"" rel=""nofollow noreferrer"">VPN gateway</a> to get access to network channels between on-premises and Azure side.</p>

<p>step3: use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">Web activity</a> in ADF to invoke the exposed endpoint and get executing results.</p>
"
"52792960","Can I connect ADF V1 Self Hosted IR in ADF V2?","<p>I have some pipelines running in ADF V1, I want to run one more pipeline which involves AWS Redshift as a source, and it needs a Self hosted IR to do that, currently we dont have any option to specify the IR in AWS Redshift connection, so am wondering how this can be done if I Can connect the ADF V1 IR to ADF V2?</p>
","<azure><azure-data-factory>","2018-10-13 12:31:55","77","0","1","52808669","<p>Currently we cannot reuse the ADF V1 self-hosted IR in ADF V2. You may need to create a new self-hosted IR in ADF V2, then select this IR in ""Connect via integration runtime"" property.</p>
"
"52784228","Azure Data Factory from VM SQL Server to File on FTP","<p>This year We moved from hosted servers to Azure VM's, we run two production servers (SQL and IIS). A vital component of our business is bulk transfer of data file.  We take customers data from our SQL Server and then write it out to a file (XLS, CSV, XML, PDF, Word, etc.) and then either email these files to customers or in most cases, push them into their FTP server.  We also have a few import procedures where we retrieve data files.  All of this is currently done with SSIS packages.</p>

<p>We're examining a move to Azure Data Factory as a replacement for SSIS so that we can possibly move to either Azure SQL (if we can work out Broker Services limitations) or an Azure SQL Managed Instance.</p>

<p>I've done some preliminary work with ADF but I saw a couple of posts about lack of FTP support.  Is it possible to create/deliver files to FTP and retrieve/consume files from FTP using ADF?  Also, almost all of these jobs are automated and we use SQL Agent to run the packages.  What is the Azure equivalent for scheduling these jobs to run?</p>
","<sql-server><azure><ssis><azure-data-factory><azure-managed-database>","2018-10-12 17:15:56","361","1","1","52824869","<p>There is automation in ADF but the scheduler is per pipeline. <a href=""https://learn.microsoft.com/en-us/azure/automation/"" rel=""nofollow noreferrer"">Azure Automation</a> is more powerful and can automate more than one pipeline (Azure Data Factory v2), if needed.</p>

<p><a href=""https://khangtran.wordpress.com/2017/06/12/automation-with-azure-data-factory-adf/"" rel=""nofollow noreferrer"">Automation with Azure Data Factory (ADF)</a></p>

<p>You can receive files from FTP into an Azure Data Factory pipeline: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-ftp"" rel=""nofollow noreferrer"">Copy data from FTP server by using Azure Data Factory</a> The idea is that you receive a file via FTP to submit to a particular pipeline activity, and that activity pushes data to an Azure data source. It might be possible to reverse the flow, and send data out.</p>

<p>The Azure SQL Database Managed Instance is the most on-premise like database (PaaS) service but SQL Server deployed on an Azure VM still has more functionality. </p>
"
"52781266","Rerun activity in Azure Data Factory V2","<p>I have a project in azure data factory v1, in this project I can rerun a single activity in a pipeline using the monitor functionality, but now I want to migrate this project to v2, but in this version I think that I cant't rerun an activity using the monitor panel. 
Is there any way of rerun a single activity of a pipeline in azure data factory V2?</p>
","<azure-data-factory>","2018-10-12 14:05:08","490","2","1","52993872","<p>If you use custom activities (Batch Service | Custom), you can rerun all the pipeline or each activity in monitor functionality.</p>

<p>With other kinds of activities for now it's not possible to rerun.</p>
"
"52776497","Set Variable activity Azure Data Factory v2","<p>I am trying this new functionality, and when I try to use set variable activity inside foreach loop I cannot select a variable that I declared in a pipeline. 
Also inside IF activity. 
Is it supposed to behave like this? That you cant set variable inside some inner activities, only at the root level of the pipeline?</p>
","<azure><azure-data-factory>","2018-10-12 09:30:17","1471","1","1","52810033","<p>This is a known bug, where set variables and append variables activities are not correctly detecting changes when they're nested in another activity. Actively working on a fix for, hopefully will resolve this problem soon :)</p>
"
"52775029","Commit ""local"" data factory changes to Azure DevOps GIT","<p>I've added Azure DevOps Git-integration some weeks ago. 
Somehow, I've changed from Git to ""Data Factory"" and added some pipelines and datasets. </p>

<p><a href=""https://i.stack.imgur.com/52wUr.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/52wUr.jpg"" alt=""&quot;local&quot; changes""></a></p>

<p>When I now change back to git, these new pipelines and datasets are missing.
<a href=""https://i.stack.imgur.com/iDTuO.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iDTuO.jpg"" alt=""enter image description here""></a></p>

<p>How do I now get this changes to the git repo?</p>

<p>Removing and re-adding repo doesn't work.</p>

<p>Thank in advance.</p>
","<git><azure><azure-data-factory>","2018-10-12 08:07:20","606","2","1","52775610","<p>Seems changes in Data factory mode will not be saved to Git.</p>

<p>But you can try recreating a git repo, and load existing resource to it.
<a href=""https://i.stack.imgur.com/NYgd8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NYgd8.png"" alt=""Repository Settings""></a></p>
"
"52767120","How to successfuly connect the IR Gateway to Redshift and pull the data?","<p>Can anyone tell me how can we successfully connect the IR Gateway to Redshift and pull the data into Azure, currently I am not able to connect to redshift, I have white listed the Integration Runtime IP in the AWS Security rolls, is there any way by which we can find out what is the IP of the IR?</p>
","<azure><azure-data-factory>","2018-10-11 18:56:30","112","0","1","52777174","<p>did you use Self-hosted Integration Runtime? If yes, the IP address of the IR should be IP address of the machine it's installed on. Follow this <a href=""https://docs.aws.amazon.com/redshift/latest/gsg/rs-gsg-authorize-cluster-access.html"" rel=""nofollow noreferrer"">link</a> to see how to grant access for IR to Amazon Redshift.</p>
"
"52763383","Iterate JSON Objects from ODATA feed and download content from each into Blob using Azure Data Factory or Logic Apps","<p>I've been struggling with this for a few days. I have an odata feed with 100+ objects. I want to get a list of all of the objects and build a query for each one that pulls the data from each and creates a file in blob storage with the content of each file. Surely someone has done this before and can help.</p>

<p>I'm open to using Azure Data Factory or Azure Logic. This is a sample payload from the odata feed metadata.</p>

<pre><code>{
  ""odata.metadata"": ""https://somewebsite/Apps/WebServices/OData.svc/$metadata"",
  ""value"": [
    {
      ""name"": ""Table1"",
      ""url"": ""Table1""
    },
    {
      ""name"": ""Table2"",
      ""url"": ""Table2""
    },
    {
      ""name"": ""Table3"",
      ""url"": ""Table3""
    }
  ]
}
</code></pre>
","<azure-logic-apps><azure-data-factory><power-automate>","2018-10-11 15:05:30","460","0","1","52770883","<p>You may achieve your goal using Azure Data Factory.</p>

<p>You can get a list of all the objects in odata using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">lookup activity</a>, then link a  <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">foreach activity</a> after the lookup activity, with a copy activity in the foreach activity.</p>
"
"52756010","How to write a file path in Json for this REST_API on Azure Data Factory V2","<p>I am using Azure Data Factory V2 and I wonder whether anyone could help me writing the file path for this REST_API Activity that has a Body parameter that can only be written in JSON. </p>

<p>This REST_API is part of a pipeline that must be triggered by an event. As you can see below the Pipeline parameter ""BlobName"" requires a file path to be set dynamically. This is a blob file that has to be stored in this container called ""source-csv"". I am trying to use <code>source-csv\@{triggerBody().filename}</code> on the ""Trigger Run Parameters"" but it won't work. Can anyone help please?</p>

<p><a href=""https://i.stack.imgur.com/JTGCa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JTGCa.png"" alt=""Pipeline parameters on the bottom as mention above and trigger run parameters on the right hand side""></a></p>
","<json><rest><azure><parameters><azure-data-factory>","2018-10-11 08:52:29","114","0","1","52759454","<p>The below is the solution:</p>

<p>{""blobname"": ""source-csv/@{triggerBody().filename}""}</p>

<p>Thanks</p>
"
"52729007","ADFv2 Queue time","<p>I have a pipeline with a few copy activities. Some of those activities are in charge of copying large amounts of data from a storage account to the same storage account but in a compressed manner (I'm talking about a few TB of data).</p>

<p>After running the pipeline for a few hours I noticed that some activities show ""Queue"" time on the monitoring blade and I was wondering what can be the reason for that ""Queue"" time. And more importantly if I'm being billed for that time also because from what I understand my ADF is not doing anything.</p>

<p><a href=""https://i.stack.imgur.com/l83FR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l83FR.png"" alt=""Queue time""></a></p>

<p>Can someone shed some light? :)</p>
","<azure><azure-data-factory>","2018-10-09 20:41:32","4857","4","2","52731767","<p>Based on the the chart in this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#monitor-visually"" rel=""nofollow noreferrer"">ADF Monitor</a>, you could find the same metrics in the example.</p>

<p><a href=""https://i.stack.imgur.com/IUiXb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IUiXb.png"" alt=""enter image description here""></a></p>

<p>In fact,it's metrics in the <code>executionDetails</code> parameter.<code>Queue Time+ Transfer Time= Duration Time.</code></p>

<blockquote>
  <p>More details on the stages copy activity goes through, and the
  corresponding steps, duration, used configurations, etc. It's not
  recommended to parse this section as it may change.</p>
</blockquote>

<p>Please refer to the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance#parallel-copy"" rel=""nofollow noreferrer"">Parallel Copy</a>, copy activity will create parallel tasks to transfer data internally. Activities are all in active state in both queue time and  transfer time, never stop in queue time so that it's billed during the whole duration time. I think it's inevitable loss in data transfer process and has been digested by adf internally. You could try to adjust <code>parallelCopies</code> param to see if anything changes.</p>

<p>If you do concern the cost, you could submit feedback <a href=""https://feedback.azure.com/forums/270578-data-factory"" rel=""nofollow noreferrer"">here</a> to ask for statements from Azure Team.</p>
"
"52729007","ADFv2 Queue time","<p>I have a pipeline with a few copy activities. Some of those activities are in charge of copying large amounts of data from a storage account to the same storage account but in a compressed manner (I'm talking about a few TB of data).</p>

<p>After running the pipeline for a few hours I noticed that some activities show ""Queue"" time on the monitoring blade and I was wondering what can be the reason for that ""Queue"" time. And more importantly if I'm being billed for that time also because from what I understand my ADF is not doing anything.</p>

<p><a href=""https://i.stack.imgur.com/l83FR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l83FR.png"" alt=""Queue time""></a></p>

<p>Can someone shed some light? :)</p>
","<azure><azure-data-factory>","2018-10-09 20:41:32","4857","4","2","52880590","<p>(Posting this as an answer because of the comment chars limit)</p>

<p>After a long discussion with Azure Support and reaching out to someone at the ADF product team I got some answers:</p>

<p>1 - The queue time is not being billed.</p>

<p>2 - Initially, the orchestration ADF system puts the job in a queue and it gets ""queue time"" until the infrastructure picks it up and start the processing part.</p>

<p>3 - In my case the queue time was increasing after the job started because of a bug in the underlying backend executor (it uses Azure Batch). Apparently the executors were crashing and my job was suffering from ""re-pickup"" time, thus increasing the queue time. This explained why after some time I started to see that the execution time and the transferred data were decreasing. The ETA for this bugfix is at the end of the month. Additionally the job that I was executing timed out (after 7 days) and after checking the billing I confirmed that I wasn't charged a dime for it.</p>
"
"52724412","Write Azure Data Factory Output Parameter to dataset","<p>Is it possible to write an output parameter to a dataset? </p>

<p>I have a meta data activity that stores the file name of an azure blob dataset and I would like write that value into another azure blob dataset as an additional column via a copy activity. </p>

<p>Thanks</p>
","<azure-data-factory>","2018-10-09 15:19:44","1227","1","1","52735566","<p>If you are looking to get the output of the previous operation as an input to the next operation, you could probably go ahead in the following manner,</p>

<p>I am hoping that the attribute you are getting is the child Items, the values for this can be obtained in the next step using the following expression.</p>

<p><code>@activity('Name_of_activity').output.childItems.</code> </p>

<p>This would return an <code>Array</code> of your subfolders.</p>

<p>The following link should help you with the expression in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">ADF</a></p>
"
"52721399","How to run PowerShell from Azure Data Factory","<p>I have <code>PowerShell</code> script which splits a complex CSV file to a smaller CSV file for every 1000 records. Here is the code:</p>

<pre><code>$i=0;Get-Content C:\Users\dell\Desktop\Powershell\Input\bigsizeFile.csv -ReadCount 1000 | %{$i++; $_ | Out-File C:\Users\dell\Desktop\Powershell\Output\file$i.csv
}
</code></pre>

<p>Now I want to use this script in Azure PowerShell and I want to run this from Azure Data Factory. Can Someone please help with this.</p>
","<powershell><azure-data-factory><azure-powershell><azure-data-lake>","2018-10-09 12:44:22","10448","5","2","52724261","<p>you could execute your powershell command by using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">Custom activity</a> in ADFv2. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity#executing-commands"" rel=""nofollow noreferrer"">Here</a> is an example.</p>
"
"52721399","How to run PowerShell from Azure Data Factory","<p>I have <code>PowerShell</code> script which splits a complex CSV file to a smaller CSV file for every 1000 records. Here is the code:</p>

<pre><code>$i=0;Get-Content C:\Users\dell\Desktop\Powershell\Input\bigsizeFile.csv -ReadCount 1000 | %{$i++; $_ | Out-File C:\Users\dell\Desktop\Powershell\Output\file$i.csv
}
</code></pre>

<p>Now I want to use this script in Azure PowerShell and I want to run this from Azure Data Factory. Can Someone please help with this.</p>
","<powershell><azure-data-factory><azure-powershell><azure-data-lake>","2018-10-09 12:44:22","10448","5","2","58662407","<p>I was able to run a PowerShell script by loading the script as an application in the Batch account associated to the Batch Custom Activity.  From there it was figuring out the correct syntactical sugar to use the environment variables to get the path to my script and run it.  I did this like so:</p>

<pre><code>powershell powershell -command (\""(Get-ChildItem Env:AZ_BATCH_APP_PACKAGE_powershellscripts#1.0).Value\"" + '\\Powershell\\processFromAzure.ps1
</code></pre>

<p>If you just want to call from the task dir, this should work:</p>

<pre><code>powershell powershell -command (""$env:AZ_BATCH_TASK_DIR"" + '\wd\processFromAzure.ps1')
</code></pre>

<p>Hope this helps someone else out there!</p>
"
"52709643","Set-AzureRmDataFactoryV2IntegrationRuntime : Method not found","<p>When I am trying ""Set-AzureRmDataFactoryV2IntegrationRuntime"" command constantly getting below error.</p>

<blockquote>
  <p>Set-AzureRmDataFactoryV2IntegrationRuntime : Method not found:
  'Newtonsoft.Json.Serialization.IAttributeProvider
  Newtonsoft.Json.Serialization.JsonProperty.get_AttributeProvider()'.
  At C:\Users\ravi\source\repos\test.ps1:12 char:5
  +     Set-AzureRmDataFactoryV2IntegrationRuntime `
  +     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
      + CategoryInfo          : CloseError: (:) [Set-AzureRmData...egrationRuntime], MissingMethodException
      + FullyQualifiedErrorId : Microsoft.Azure.Commands.DataFactoryV2.SetAzureDataFactoryIntegrationRuntimeCommand</p>
</blockquote>

<p>When I ran ""Get-Module"" command I see below list of modules</p>

<blockquote>
  <p>ModuleType Version    Name<br>
  ExportedCommands</p>
  
  <p>---------- -------    ----                                ----------------</p>
  
  <p>Script     <strong>0.5.10</strong>     <strong>AzureRM.DataFactoryV2</strong><br>
  {Get-AzureRmDataFactoryV2, Get-AzureRmDataFactoryV2ActivityRun,
  Get-AzureRmDat...</p>
  
  <p>Script     <strong>5.6.0</strong>      <strong>AzureRM.profile</strong><br>
  {Add-AzureRmEnvironment, Clear-AzureRmContext, Clear-AzureRmDefault,
  Connect-A...</p>
  
  <p>Manifest   3.1.0.0    Microsoft.PowerShell.Management<br>
  {Add-Computer, Add-Content, Checkpoint-Computer, Clear-Content...}</p>
  
  <p>Manifest   3.0.0.0    Microsoft.PowerShell.Security<br>
  {ConvertFrom-SecureString, ConvertTo-SecureString, Get-Acl,
  Get-AuthenticodeSi...</p>
  
  <p>Manifest   3.1.0.0    Microsoft.PowerShell.Utility        {Add-Member,
  Add-Type, Clear-Variable, Compare-Object...}</p>
  
  <p>Manifest   3.0.0.0    Microsoft.WSMan.Management<br>
  {Connect-WSMan, Disable-WSManCredSSP, Disconnect-WSMan,
  Enable-WSManCredSSP...}</p>
  
  <p>Script     0.2.0      PowerShellEditorServices.Commands<br>
  {ConvertFrom-ScriptExtent, ConvertTo-ScriptExtent, Find-Ast,
  Get-Token...}</p>
  
  <p>Script     0.2.0      PowerShellEditorServices.VSCode<br>
  {Close-VSCodeHtmlContentView, New-VSCodeHtmlContentView,
  Set-VSCodeHtmlContent...</p>
</blockquote>

<p>I have installed Azure PowerShell MSI link given on below page.</p>

<p>Page URL: <a href=""https://learn.microsoft.com/en-us/powershell/azure/other-install?view=azurermps-6.9.0"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/powershell/azure/other-install?view=azurermps-6.9.0</a></p>

<p>MSI URL: <a href=""https://github.com/Azure/azure-powershell/releases/download/v6.9.0-September2018/azure-powershell.6.9.0.msi"" rel=""nofollow noreferrer"">https://github.com/Azure/azure-powershell/releases/download/v6.9.0-September2018/azure-powershell.6.9.0.msi</a></p>

<p>Update:
When I was trying to create IS manually I see that its created but with ""Unavailable"" status </p>

<p><a href=""https://i.stack.imgur.com/L2vIw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L2vIw.png"" alt=""enter image description here""></a></p>

<p>Updated on 10/10 for Joy Wang
<a href=""https://i.stack.imgur.com/IEsjz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IEsjz.png"" alt=""enter image description here""></a></p>
","<azure><powershell><azure-powershell><azure-data-factory>","2018-10-08 20:26:51","676","0","1","52712817","<p>I test the <code>Set-AzureRmDataFactoryV2IntegrationRuntime</code> on my side, it works fine. I recommend you to remove the modules and install again,  refer to this <a href=""https://learn.microsoft.com/en-us/powershell/azure/install-azurerm-ps?view=azurermps-6.9.0"" rel=""nofollow noreferrer"">link</a>.</p>

<p><strong>My specific command</strong>:</p>

<pre><code>Set-AzureRmDataFactoryV2IntegrationRuntime -ResourceGroupName 'joywebapp' -DataFactoryName 'joyfactoryv2' -Name 'integrationRuntime35' -Type ""SelfHosted"" -Description 'New description111'
</code></pre>

<p><a href=""https://i.stack.imgur.com/WtX6J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WtX6J.png"" alt=""enter image description here""></a></p>

<p><strong>My module</strong> :</p>

<p><a href=""https://i.stack.imgur.com/dBODn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dBODn.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p>When I was trying to create IS manually I see that its created but with ""Unavailable"" status</p>
</blockquote>

<p>To fix this issue of <code>Self-Hosted</code> type runtime, you could install the <strong>Microsoft Integration Runtime Configuration Manager</strong>, in the runtime -> Edit -> Click here to launch the express setup for this computer, refer to the screenshot.</p>

<p><a href=""https://i.stack.imgur.com/KuO9Z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KuO9Z.png"" alt=""enter image description here""></a></p>

<p>After installing it, you will see them like the screenshot.</p>

<p><a href=""https://i.stack.imgur.com/5sJrI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5sJrI.png"" alt=""enter image description here""></a></p>

<p><strong>Check in the portal</strong>:</p>

<p><a href=""https://i.stack.imgur.com/MiH4p.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MiH4p.png"" alt=""enter image description here""></a></p>
"
"52709523","How to schedule an ADF Pipeline to run mutiple hours daily?","<p>How can we schedule and ADF V1 Pipeline to run 4 times daily at  6:30 am, 9:30 am, 12:30 pm, and 3:30 pm???</p>
","<azure><azure-data-factory>","2018-10-08 20:16:28","2094","0","3","52709579","<p>Unfortunately you cannot. You need to have an even distribution. You can run every four hours, but not limited to half a day. The nearest you can come with the standard tooling is 06:30, 09:30, 12:30, 15:30, 18:30, 21:30, 00:30, 03:30.</p>
"
"52709523","How to schedule an ADF Pipeline to run mutiple hours daily?","<p>How can we schedule and ADF V1 Pipeline to run 4 times daily at  6:30 am, 9:30 am, 12:30 pm, and 3:30 pm???</p>
","<azure><azure-data-factory>","2018-10-08 20:16:28","2094","0","3","52714745","<p>ADF V2 <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger"" rel=""nofollow noreferrer"">Schedule Trigger</a> is what you want.<a href=""https://i.stack.imgur.com/KTWga.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KTWga.png"" alt=""enter image description here""></a></p>
"
"52709523","How to schedule an ADF Pipeline to run mutiple hours daily?","<p>How can we schedule and ADF V1 Pipeline to run 4 times daily at  6:30 am, 9:30 am, 12:30 pm, and 3:30 pm???</p>
","<azure><azure-data-factory>","2018-10-08 20:16:28","2094","0","3","52714916","<p>Since you are using ADF V1, based on statement in this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-scheduling-and-execution#dataset-availability"" rel=""nofollow noreferrer"">doc</a>, it's hard to implement your requirements.No such features could be rely on.</p>

<p>However, I provide you with a workaround that you could use <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-timer"" rel=""nofollow noreferrer"">Azure Time Trigger Function</a> because it supports <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-timer#cron-expressions"" rel=""nofollow noreferrer"">CRON expressions</a>. Please set the expression as <code>""0 30 6-15/3 * * *""</code> so that it could be triggered as your description. Then in the function code,you could fill the adf pipeline sdk execution code.</p>

<p>Of course, if you want to navigate to ADF V2, it has been already described in @Fang Liu's answer.</p>
"
"52706659","Use Jupyter notebook without DataBricks in Azure Data Factory?","<p>I gather from the documentation that we can use Jupyter notebooks only with Databricks Spark cluster. </p>

<p>Is there a way around this? Can I call Jupyter notebook as an activity from ADF without Databricks environment? I would like to have a simple way to call some python code from ADF.</p>

<p>Thanks!</p>
","<python><azure><jupyter-notebook><azure-data-factory>","2018-10-08 16:38:21","1394","2","1","52712294","<p>You can try <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">Custom Activity</a> in ADF. Custom activity supports cmd command, so you can use command line to invoke python script.</p>

<p>And there's another example of using python in custom activity:
<a href=""https://github.com/rawatsudhir1/ADFPythonCustomActivity"" rel=""nofollow noreferrer"">https://github.com/rawatsudhir1/ADFPythonCustomActivity</a></p>

<p>Hope it helps.</p>
"
"52683533","Expression invalid","<p>Azure Data Factory error: </p>

<pre><code>The expression 'item().$v.collection.$v' cannot be evaluated because property '$v' doesn't exist, available properties are '$t, $v._id.$t, $v._id.$v, $v.id.$t, $v.id.$v, $v.database.$t, $v.database.$v, $v.collection.$t, $v.collection.$v, id, _self, _etag, _rid, _attachments, _ts'
</code></pre>

<p>How can I get around that ?
I am using this expression in forEach which is connected to lookup activity which is reading values from CosmosDB.  I am interested only in single column, but SQL: </p>

<pre><code>select collection from backups 
</code></pre>

<p>didn't work, hence I switched from ""Query"" to ""Table"", hence output of lookup activity contains json object with fields containing $</p>
","<azure-data-factory>","2018-10-06 21:34:54","220","0","1","52685722","<p>this error results from for each activity treats ""."" as the property accessor, please use the expression ""@item()['$v.collection.$v']"" to get around the error. Thanks.</p>
"
"52664930","How to replace row delimiter character with another character?","<p>Here the line in which i am fetching the ""comment"" attribute from dynamics CRM using FetchXml</p>

<p>attribute name=""comment""</p>

<p>If in a comment attribute the ""/r/n"" is coming in between them and if i want to replace that with some another special character, how can I replace ""/r/n"" with another, in FetchXml query? </p>
","<xml><azure><dynamics-crm><azure-data-factory><fetchxml>","2018-10-05 11:44:33","330","0","1","52665690","<p>You can't replace <code>r/n/</code> as part of the FetchXml query (This would actually mean doing an update on the comment on the database). You first retrieve the records, and then do the replacing.</p>

<p>For example, say you've got the comment field on the Account entity, then (on server side):</p>

<pre><code>string fetch = @""  
   &lt;fetch version='1.0' mapping='logical' distinct='false'&gt;  
     &lt;entity name='account'&gt;   
        &lt;attribute name='comment'/&gt;   
     &lt;/entity&gt;   
   &lt;/fetch&gt; "";   

EntityCollection result = _serviceProxy.RetrieveMultiple(new FetchExpression(fetch));
foreach (var account in result.Entities)
{
   // replace \r\n with a comma
   string comment = account.GetAttributeValue&lt;string&gt;(""comment""); 
   if (!string.IsNullOrEmpty(comment))
   {
       var lines = comment.Split(new string[] { ""\r\n"" }, StringSplitOptions.None);
       Console.WriteLine(string.Join("","", lines)); 
   } 
}
</code></pre>
"
"52664415","Why we need ML batch execution and update resource option in azure data factory","<p>Why we need to ML Batch Execution and ML Update resource option in Data factory ? How this can be used to retrain machine learning when updating a blob file ?</p>

<p><a href=""https://i.stack.imgur.com/8KWH0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8KWH0.png"" alt=""enter image description here""></a></p>
","<c#><azure><azure-blob-storage><azure-data-factory><azure-machine-learning-service>","2018-10-05 11:16:38","142","1","1","52853464","<p>Ml Batch Execution- to call retraining experiment and get a .ilearner file as output.
ML Update Resource- Use the above .ilearner as input and call patch endpoint of predictive web service to Update resource.</p>
"
"52661559","Get list of all files in a azure data lake directory to a look up activity in ADFV2","<p>I have a number of files in azure data lake storage, i am creating a pipeline in ADFV2 to get the list of all the files in a folder in ADLS. How to do this?</p>
","<azure><azure-storage><azure-pipelines><azure-data-lake><azure-data-factory>","2018-10-05 08:35:40","11182","0","2","52661626","<p>You should use Get metadata activity. 
Check <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">this</a></p>
"
"52661559","Get list of all files in a azure data lake directory to a look up activity in ADFV2","<p>I have a number of files in azure data lake storage, i am creating a pipeline in ADFV2 to get the list of all the files in a folder in ADLS. How to do this?</p>
","<azure><azure-storage><azure-pipelines><azure-data-lake><azure-data-factory>","2018-10-05 08:35:40","11182","0","2","52694985","<p>You could follow below steps to list files in ADLS.</p>

<p>1: Use ADLS SDK to get the list file names in a specific directory and output the results. Such as Java SDK <a href=""https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-get-started-java-sdk#list-directory-contents"" rel=""nofollow noreferrer"">here</a>. Of course, you could use .net or Python.</p>

<pre><code>// list directory contents
List&lt;DirectoryEntry&gt; list = client.enumerateDirectory(""/a/b"", 2000);
System.out.println(""Directory listing for directory /a/b:"");
for (DirectoryEntry entry : list) {
    printDirectoryInfo(entry);
}
System.out.println(""Directory contents listed."");
</code></pre>

<p>2. Compile the file so that it could be executed.Store it into azure blob storage.</p>

<p>3.Use custom activity in azure data factory to configure the blob storage path and execute the program. More details,please follow this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity#custom-activity"" rel=""nofollow noreferrer"">document</a>.</p>

<p>You could use custom activity in Azure data factory.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-get-started-java-sdk#list-directory-contents"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-get-started-java-sdk#list-directory-contents</a></p>
"
"52660529","how to create linked service to adla","<p>I'm trying to create linked service to azure data lake analytics and it is showing the error like this</p>

<pre><code>[{""code"":9512,""message"":""Failed to connect to ADLA account 'miniprojectanalytics'
with error 'The client '8899dd6a-45b1-4955-a3b9-7b3a95dd6dfa' with object id
'8899dd6a-45b1-4955-a3b9-7b3a95dd6dfa' does not have authorization to perform action
'Microsoft.Authorization/permissions/read' over scope '
subscriptions/8e9519a3-7b76-40fe-9b64-752ed2d0d4dc/resourceGroups/harigrp/providers/Microsoft.DataLakeAnalytics/accounts/miniprojectanalytics/providers/Microsoft.Authorization'.
Trace: 0941cde4-bff5-42b4-8186-1bd08f04944b Time: 2018-10-05T00:27:23.0661884-07:00'.""}]
</code></pre>

<p>could anyone please assist me how to resolve this </p>
","<azure><azure-data-factory>","2018-10-05 07:34:13","429","0","1","52662327","<p>I believe you need to give your service account access on the ADLA via the Add User Wizard.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-data-lake-analytics"" rel=""nofollow noreferrer"">From here</a></p>

<p><strong>Service principal authentication</strong></p>

<p>The Azure Data Lake Analytics linked service requires a service principal authentication to connect to the Azure Data Lake Analytics service. To use service principal authentication, register an application entity in Azure Active Directory (Azure AD) and grant it the access to both the Data Lake Analytics and the Data Lake Store it uses. For detailed steps, see Service-to-service authentication. Make note of the following values, which you use to define the linked service:</p>

<ul>
<li>Application ID </li>
<li>Application key </li>
<li>Tenant ID</li>
</ul>

<p>Grant service principal permission to your Azure Data Lake Anatlyics using the Add User Wizard.</p>
"
"52651726","How to load files from azure powershell to Azure Data Lake","<p>How to load .ps1 files from Azure Powershell to Azure data lake?</p>

<p>I have tried in my local machine (Powershell), there it is working fine.</p>

<p>while uploading it into the Azure Powershell, we are not able to fetch the exact location. And how do we create a file in Azure data lake using Azure Powershell?</p>
","<azure><azure-data-factory><azure-data-lake><azure-cloud-shell>","2018-10-04 16:41:02","669","1","1","52718090","<p>If we want to create a file we could use Azure powershell command.</p>

<pre><code>New-AzureRmDataLakeStoreItem -AccountName ""AccountName"" -Path ""/NewFile.txt""
</code></pre>

<p>If we want to upload a local file to Azure data lake</p>

<pre><code>Import-AzureRmDataLakeStoreItem -AccountName ""ContosoADL"" -Path ""C:\SrcFile.csv"" -Destination ""/MyFiles/File.csv"" -Concurrency 4
</code></pre>

<p>But before excute the command please make sure that you have permission to do that. How to assign the role to application or service principle, please refer to this <a href=""http://%20https://learn.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal#assign-application-to-role"" rel=""nofollow noreferrer"">tutorial</a>.</p>

<p>For more Powershell command about data lake please refer to this <a href=""https://learn.microsoft.com/en-us/powershell/module/azurerm.datalakestore/?view=azurermps-6.9.0#data_lake_store"" rel=""nofollow noreferrer"">document</a>.</p>
"
"52647832","Convert epoch time in Data Factory to datetime","<p>I have the following query.  This query copies the data from <code>Cosmos DB</code> to <code>Azure Data Lake</code>.</p>

<pre><code>select c.Tag
from c
where 
c.data.timestamp &gt;= '@{formatDateTime(addminutes(pipeline().TriggerTime, -15), 'yyyy-MM-ddTHH:mm:ssZ' )}' 
</code></pre>

<p>However, I've got to use the <code>_ts</code> which is the epoch time when the document was created on the <code>cosmos DB</code> collection instead of <code>c.data.timestamp</code>.  How do I convert epoch time to date time and compare with it with <code>'@{formatDateTime(addminutes(pipeline().TriggerTime, -15), 'yyyy-MM-ddTHH:mm:ssZ' )}'</code> </p>

<p>I have also tried using 
    <code>dateadd( SECOND, c._ts, '1970-1-1' )</code> which clearly isn't supported.</p>
","<sql><azure><azure-cosmosdb><azure-data-factory><epoch>","2018-10-04 13:21:17","2320","3","1","52657587","<p>As @Chris said, you could use <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/programming#udf"" rel=""nofollow noreferrer"">UDF</a> in cosmos db query.</p>

<p>udf:</p>

<pre><code>function convertTime(unix_timestamp){
      var date = new Date(unix_timestamp * 1000);
      return date;
}
</code></pre>

<p>sql:</p>

<p><a href=""https://i.stack.imgur.com/RNjWO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RNjWO.png"" alt=""enter image description here""></a></p>

<p>You could merge it into your transfer sql:</p>

<pre><code>select c.Tag
from c
where 
udf.convertTime(c._ts) &gt;= '@{formatDateTime(addminutes(pipeline().TriggerTime, -15), 'yyyy-MM-ddTHH:mm:ssZ' )}'
</code></pre>
"
"52643888","filter data from azure data storage in data factory v2","<p>I am new to Azure Data Factory v2. We have a table in a Azure data storage and I am able to load all data in a Azure SQL database by using the copy data option. </p>

<p>But what I would like to achieve is filter the data in the data storage by the field status that is an integer field. I tried some examples from the Microsoft website. But every time I get the bad syntax error when I run the pipeline. 
So what I tried is, in the source tab I choose my data store as source data set, with the source table documentStatus. And I clicked on use query and put this line in:
""azureTableSourceQuery"": ""$$Text.Format('Status = 2')"" 
But when I run this I get this error: The remote server returned an error: (400) Bad Request.</p>

<p>Can anybody help me with writing a correct query so I can filter my source on this status field?</p>

<p>Thanks</p>
","<azure><azure-storage><azure-data-factory>","2018-10-04 09:57:57","1031","1","1","52657778","<p>Please set <code>""azureTableSourceQuery"": ""Status eq 2""</code>:</p>

<p><a href=""https://i.stack.imgur.com/YyHzQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YyHzQ.png"" alt=""enter image description here""></a></p>

<p>Please refer to this <a href=""https://learn.microsoft.com/en-us/rest/api/storageservices/querying-tables-and-entities#supported-comparison-operators"" rel=""nofollow noreferrer"">doc</a> related to Azure Table Storage Filter Sql.</p>
"
"52643359","Azure Data Factory V2: Event trigger with file name as variable","<p>Is there any way to have a copy activity on ADF V2 trigged by an event in which the source file name is a variable? I know you should define default value for the ""sourceFile"" in the pipeline parameters (as we see below). However I would like to have no default value and have the pipeline trigged by any file with any title. In order words the file name would be checked at run time. </p>

<p><a href=""https://i.stack.imgur.com/kxn5v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kxn5v.png"" alt=""enter image description here""></a></p>
","<sql><azure><parameters><azure-data-factory>","2018-10-04 09:26:33","1979","0","1","52658021","<p>Based on your requirements, I suggest you could get to know <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">Lookup Activity</a> in Azure Data Factory.</p>

<blockquote>
  <p>Lookup activity reads and returns the content of a configuration file
  or table. It also returns the result of executing a query or stored
  procedure. The output from Lookup activity can be used in a subsequent
  copy or transformation activity if it's a singleton value. The output
  can be used in a ForEach activity if it's an array of attributes.</p>
</blockquote>

<p>Please see the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity#copy-activity-example"" rel=""nofollow noreferrer"">Copy Activity example</a> in above doc.You could store the file name in a JSON file into Azure Blob Storage and JSON file is modified dynamically.The lookup activity will retrieve the specific filename and you could use it in expression: <code>@{activity('MyLookupActivity').output.firstRow.FileName</code>.</p>
"
"52637153","Run .Net and Python applications on Azure Data Factory?","<p>I am trying to create a pipeline in Azure which involves running .Net applications and python applications as part
of the workflow. These applications read/write to ADLS and Azure databases. I am wondering what are the best tools for the job.</p>

<p>So far I have landed on using ADF, with Azure Batch/Custom activity for .Net; and same or Databricks 
notebook/Python activity for Python <a href=""https://learn.microsoft.com/en-us/azure/data-factory/compute-linked-services"" rel=""nofollow noreferrer"">as per this</a>.
(preferably avoid having to spin up a Databricks cluster.) </p>

<p>Other options I considered: use Azure Functions for .Net (and use it in ADF) but not sure if it is only suitable for small snippets of code 
or entier applications. Use Logic Apps to run .net/python applications in a
Docker container activity step which seems like an overkill.</p>

<p>Thanks in advance.</p>
","<python><.net><azure><azure-data-factory><azure-logic-apps>","2018-10-03 23:59:49","625","1","1","52735219","<p>Per my knowledge, maybe Azure Function is not suitable for entire application. Based on the statement in <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-overview"" rel=""nofollow noreferrer"">Azure Function Overview</a>, Functions is a great solution for processing data, integrating systems, working with the internet-of-things (IoT), and building simple APIs and microservices. It is more suitable for snippet of code to process data in schedule.</p>

<p>For both .Net application and Python application, you could use custom activity. Please see the <a href=""https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/data-factory/v1/data-factory-use-custom-activities.md"" rel=""nofollow noreferrer"">Github guidance</a> and this <a href=""https://github.com/rawatsudhir1/ADFPythonCustomActivity"" rel=""nofollow noreferrer"">Python tutorial</a>.</p>
"
"52634694","Prevent empty file generation using Azure Data Factory Copy Activity","<p>I'm using Azure Data Factory to copy data from Azure Cosmos DB to Azure Data Lake.  My pipeline consists of a copy activity which copies data to the Data lake sink.  </p>

<p>This is my query on the source dataset:</p>

<pre><code>select * from c 
where c.data.timestamp &gt;= '@{formatDateTime(addminutes(pipeline().TriggerTime, -15), 'yyyy-MM-ddTHH:mm:ssZ' )}' 
AND c.data.timestamp &lt; '@{formatDateTime(pipeline().TriggerTime, 'yyyy-MM-ddTHH:mm:ssZ' )}'
</code></pre>

<p>I'm getting the data for the last 15 minutes before the trigger time.</p>

<p>Now, if there is no data retrieved by the query then the copy activity generates an empty file and stores it in the data lake.  I want to prevent that.  Is there any way I can achieve this?</p>
","<azure><azure-data-lake><azure-data-factory>","2018-10-03 19:51:20","4692","2","3","52722885","<p>You could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""noreferrer"">lookup activity</a> and then use an if activity to decide whether you need to run the copy activity.</p>

<p>In the lookup activity, you could set firstRowOnly as true since you only want to check whether there are data.</p>

<p><a href=""https://i.stack.imgur.com/Z7qUT.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Z7qUT.png"" alt=""enter image description here""></a></p>
"
"52634694","Prevent empty file generation using Azure Data Factory Copy Activity","<p>I'm using Azure Data Factory to copy data from Azure Cosmos DB to Azure Data Lake.  My pipeline consists of a copy activity which copies data to the Data lake sink.  </p>

<p>This is my query on the source dataset:</p>

<pre><code>select * from c 
where c.data.timestamp &gt;= '@{formatDateTime(addminutes(pipeline().TriggerTime, -15), 'yyyy-MM-ddTHH:mm:ssZ' )}' 
AND c.data.timestamp &lt; '@{formatDateTime(pipeline().TriggerTime, 'yyyy-MM-ddTHH:mm:ssZ' )}'
</code></pre>

<p>I'm getting the data for the last 15 minutes before the trigger time.</p>

<p>Now, if there is no data retrieved by the query then the copy activity generates an empty file and stores it in the data lake.  I want to prevent that.  Is there any way I can achieve this?</p>
","<azure><azure-data-lake><azure-data-factory>","2018-10-03 19:51:20","4692","2","3","52723922","<p>I tried the following and it is working: I'm checking if the lookup entry returns more than 0 rows.</p>

<p><a href=""https://i.stack.imgur.com/94lEW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/94lEW.png"" alt=""enter image description here""></a></p>
"
"52634694","Prevent empty file generation using Azure Data Factory Copy Activity","<p>I'm using Azure Data Factory to copy data from Azure Cosmos DB to Azure Data Lake.  My pipeline consists of a copy activity which copies data to the Data lake sink.  </p>

<p>This is my query on the source dataset:</p>

<pre><code>select * from c 
where c.data.timestamp &gt;= '@{formatDateTime(addminutes(pipeline().TriggerTime, -15), 'yyyy-MM-ddTHH:mm:ssZ' )}' 
AND c.data.timestamp &lt; '@{formatDateTime(pipeline().TriggerTime, 'yyyy-MM-ddTHH:mm:ssZ' )}'
</code></pre>

<p>I'm getting the data for the last 15 minutes before the trigger time.</p>

<p>Now, if there is no data retrieved by the query then the copy activity generates an empty file and stores it in the data lake.  I want to prevent that.  Is there any way I can achieve this?</p>
","<azure><azure-data-lake><azure-data-factory>","2018-10-03 19:51:20","4692","2","3","62561817","<p>This is an older thread but someone might have a more elegant way to handle the issue above that ADF produces a file even there are 0 records.  Here are my concerns with the Lookup approach or having a post-process clean up the empty file.</p>
<ol>
<li>It's inefficient to query database twice just to check if there are rows the first time.</li>
<li>Using the [IF Condition] componenet is not possible if you are already inside an [if component] or [case] component of ADF.  (This is an ADF constraint/shortcoming also).</li>
<li>Cleaning up the empty file is also inefficient, and not an option if you are triggering off the event of the file being created since it causes a false-positive as it is written before you can clean it up.</li>
</ol>
"
"52631410","Azure data factory V2 copy with stored procedure","<p>How can I pass the output from the source section of a copy activity as a parameter for a stored procedure.</p>

<p>The stored procedure will be used in the sink section of the same copy activity.</p>

<p>Thank You.</p>
","<azure><azure-data-factory>","2018-10-03 16:09:38","825","0","1","52685783","<p>what's the meaning of ""<em>the output from the source section of a copy activity</em>""? In ADF, only <a href=""https://learn.microsoft.com/en-us/azure/data-factory/compare-versions#feature-comparison"" rel=""nofollow noreferrer"">activities/pipelines</a> have the ""output"" property. If you want to copy data using the stored procedure in copy activity, this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#invoking-stored-procedure-for-sql-sink"" rel=""nofollow noreferrer"">link</a> may give you some reference.</p>
"
"52625367","How to insert Data into the Computed Columns using ADF V1","<p>How can we insert data into the computed columns using ADF V1. Currently we cannot insert the data into the Azure SQL Database.</p>
","<azure><azure-data-factory>","2018-10-03 10:47:02","217","0","2","52933422","<p>Suppose the source system is a JDBC connection and the target system is an Azure Sql database table that has a computed column (say extracting year from a date string ""20181022""). When ADF tries to load the table it complains about the column mismatch in with the target system. 
Is there a way to get around this?</p>
"
"52625367","How to insert Data into the Computed Columns using ADF V1","<p>How can we insert data into the computed columns using ADF V1. Currently we cannot insert the data into the Azure SQL Database.</p>
","<azure><azure-data-factory>","2018-10-03 10:47:02","217","0","2","64936913","<p>I have a similar situation and also looking for a better solution.
I am copying data from a prod db and updating the QA db with it.</p>
<p>Here's how I have been handling it so far: modify the hashbytes column in the target QA db table so that it is a varchar. But this solution is less than ideal because this will have to be done manually every single time a code change is pushed to the affected table.</p>
<p>Your situation is different.
If it were me, I would do a straight copy of the data into the Azure SQL Database. No computed tables. No modifications. Then I would stage the data into another table that had the computed columns I was after.
It is an Extract, Load, Transform approach, compared to the traditional Extract, Transform, Load method.</p>
<p>This question is really old, but hey, I found this question just now; maybe other will, too.</p>
"
"52612382","Azure Data Factory select property ""status"": ""Succeeded"" from previous activity","<p>with Data Factory V2 I'm trying to implement a stream of data copy from one Azure SQL database to another.</p>

<p>I would like to perform a conditional activity <code>If Condition</code> depends on the success of the previous activities execute by the pipeline, but in the expression to be included in the activity of <code>If Condition</code> I can not select the output property <code>""status"": ""Succeeded""</code>.</p>

<p>Before the activity of <code>If Condition</code> I have two data copy activities.</p>

<p>I added an <code>If Condition</code> activity to my flow because the tasks to perform after copying the data depend on the success of <strong>all</strong> the copy activities.</p>

<p>i.e. </p>

<p>if all copy activities are performed correctly then the true condition will be executed. If only one copy activity is successful and the other fails then the false condition is executed</p>

<p>The output of each copy activities is as follows:</p>

<pre><code>Output
{
    ""dataRead"": 213156,
    ""dataWritten"": 213156,
    ""rowsRead"": 3554,
    ""rowsCopied"": 3554,
    ""copyDuration"": 4,
    ""throughput"": 52.04,
    ""errors"": [],
    ""effectiveIntegrationRuntime"": ""DefaultIntegrationRuntime (West Europe)"",
    ""usedDataIntegrationUnits"": 4,
    ""usedParallelCopies"": 1,
    ""executionDetails"": [
        {
            ""source"": {
                ""type"": ""AzureSqlDatabase""
            },
            ""sink"": {
                ""type"": ""AzureSqlDatabase""
            },
            ""status"": ""Succeeded"",
            ""start"": ""2018-10-02T13:42:37.7396813Z"",
            ""duration"": 4,
            ""usedDataIntegrationUnits"": 4,
            ""usedParallelCopies"": 1,
            ""detailedDurations"": {
                ""queuingDuration"": 3,
                ""preCopyScriptDuration"": 0,
                ""timeToFirstByte"": 0,
                ""transferDuration"": 1
            }
        }
    ]
}
</code></pre>

<p>And I structured my expression for <code>If Condition</code> activity like that:</p>

<pre><code>@and(equals(activity('Copy_Activity1').output.executionDetails[3],'Succeeded'), equals(activity('Copy_Activity2').output.executionDetails[3],'Succeeded'))
</code></pre>

<p>But he gives me the following error:</p>

<pre><code>""error"": {
    ""code"": ""InvalidTemplate"",
    ""message"": ""Unable to process template language expressions in action 'If Condition1' inputs at line '1' and column '1294': 'The template language expression 'and(equals(activity('Copy_Item_Budget_Name').output.executionDetails[3],'Succeeded'), equals(activity('Copy_Item_Budget_Entry').output.executionDetails[3],'Succeeded'))' cannot be evaluated because array index '3' is outside bounds (0, 0) of array. Please see https://aka.ms/logicexpressions for usage details.'.""
}
</code></pre>

<p>But even with the guide I can not solve the problem.</p>

<p>Does anyone know how to solve the problem? Thank you</p>
","<azure><if-statement><azure-data-factory>","2018-10-02 16:14:41","8932","2","3","52614932","<p>For clarity, that's not how flow control works in ADF.</p>

<p>You don't need to query the result of the previous shape, instead, you change the Activity Connector to branch based on the outcome.</p>

<p>After connecting two Activities, right click on the line/arrow.  Then you can choose to run the next Activity on any of Success, Failure, Completion or Skip.</p>

<p>You can link any number of Activities before and after it. </p>
"
"52612382","Azure Data Factory select property ""status"": ""Succeeded"" from previous activity","<p>with Data Factory V2 I'm trying to implement a stream of data copy from one Azure SQL database to another.</p>

<p>I would like to perform a conditional activity <code>If Condition</code> depends on the success of the previous activities execute by the pipeline, but in the expression to be included in the activity of <code>If Condition</code> I can not select the output property <code>""status"": ""Succeeded""</code>.</p>

<p>Before the activity of <code>If Condition</code> I have two data copy activities.</p>

<p>I added an <code>If Condition</code> activity to my flow because the tasks to perform after copying the data depend on the success of <strong>all</strong> the copy activities.</p>

<p>i.e. </p>

<p>if all copy activities are performed correctly then the true condition will be executed. If only one copy activity is successful and the other fails then the false condition is executed</p>

<p>The output of each copy activities is as follows:</p>

<pre><code>Output
{
    ""dataRead"": 213156,
    ""dataWritten"": 213156,
    ""rowsRead"": 3554,
    ""rowsCopied"": 3554,
    ""copyDuration"": 4,
    ""throughput"": 52.04,
    ""errors"": [],
    ""effectiveIntegrationRuntime"": ""DefaultIntegrationRuntime (West Europe)"",
    ""usedDataIntegrationUnits"": 4,
    ""usedParallelCopies"": 1,
    ""executionDetails"": [
        {
            ""source"": {
                ""type"": ""AzureSqlDatabase""
            },
            ""sink"": {
                ""type"": ""AzureSqlDatabase""
            },
            ""status"": ""Succeeded"",
            ""start"": ""2018-10-02T13:42:37.7396813Z"",
            ""duration"": 4,
            ""usedDataIntegrationUnits"": 4,
            ""usedParallelCopies"": 1,
            ""detailedDurations"": {
                ""queuingDuration"": 3,
                ""preCopyScriptDuration"": 0,
                ""timeToFirstByte"": 0,
                ""transferDuration"": 1
            }
        }
    ]
}
</code></pre>

<p>And I structured my expression for <code>If Condition</code> activity like that:</p>

<pre><code>@and(equals(activity('Copy_Activity1').output.executionDetails[3],'Succeeded'), equals(activity('Copy_Activity2').output.executionDetails[3],'Succeeded'))
</code></pre>

<p>But he gives me the following error:</p>

<pre><code>""error"": {
    ""code"": ""InvalidTemplate"",
    ""message"": ""Unable to process template language expressions in action 'If Condition1' inputs at line '1' and column '1294': 'The template language expression 'and(equals(activity('Copy_Item_Budget_Name').output.executionDetails[3],'Succeeded'), equals(activity('Copy_Item_Budget_Entry').output.executionDetails[3],'Succeeded'))' cannot be evaluated because array index '3' is outside bounds (0, 0) of array. Please see https://aka.ms/logicexpressions for usage details.'.""
}
</code></pre>

<p>But even with the guide I can not solve the problem.</p>

<p>Does anyone know how to solve the problem? Thank you</p>
","<azure><if-statement><azure-data-factory>","2018-10-02 16:14:41","8932","2","3","52658256","<p>From the output data, <code>executionDetails</code> is an array with only one item which contains an object. So the expression should be: <code>activity('Copy_Activity1').output.executionDetails[0].status</code>.</p>
"
"52612382","Azure Data Factory select property ""status"": ""Succeeded"" from previous activity","<p>with Data Factory V2 I'm trying to implement a stream of data copy from one Azure SQL database to another.</p>

<p>I would like to perform a conditional activity <code>If Condition</code> depends on the success of the previous activities execute by the pipeline, but in the expression to be included in the activity of <code>If Condition</code> I can not select the output property <code>""status"": ""Succeeded""</code>.</p>

<p>Before the activity of <code>If Condition</code> I have two data copy activities.</p>

<p>I added an <code>If Condition</code> activity to my flow because the tasks to perform after copying the data depend on the success of <strong>all</strong> the copy activities.</p>

<p>i.e. </p>

<p>if all copy activities are performed correctly then the true condition will be executed. If only one copy activity is successful and the other fails then the false condition is executed</p>

<p>The output of each copy activities is as follows:</p>

<pre><code>Output
{
    ""dataRead"": 213156,
    ""dataWritten"": 213156,
    ""rowsRead"": 3554,
    ""rowsCopied"": 3554,
    ""copyDuration"": 4,
    ""throughput"": 52.04,
    ""errors"": [],
    ""effectiveIntegrationRuntime"": ""DefaultIntegrationRuntime (West Europe)"",
    ""usedDataIntegrationUnits"": 4,
    ""usedParallelCopies"": 1,
    ""executionDetails"": [
        {
            ""source"": {
                ""type"": ""AzureSqlDatabase""
            },
            ""sink"": {
                ""type"": ""AzureSqlDatabase""
            },
            ""status"": ""Succeeded"",
            ""start"": ""2018-10-02T13:42:37.7396813Z"",
            ""duration"": 4,
            ""usedDataIntegrationUnits"": 4,
            ""usedParallelCopies"": 1,
            ""detailedDurations"": {
                ""queuingDuration"": 3,
                ""preCopyScriptDuration"": 0,
                ""timeToFirstByte"": 0,
                ""transferDuration"": 1
            }
        }
    ]
}
</code></pre>

<p>And I structured my expression for <code>If Condition</code> activity like that:</p>

<pre><code>@and(equals(activity('Copy_Activity1').output.executionDetails[3],'Succeeded'), equals(activity('Copy_Activity2').output.executionDetails[3],'Succeeded'))
</code></pre>

<p>But he gives me the following error:</p>

<pre><code>""error"": {
    ""code"": ""InvalidTemplate"",
    ""message"": ""Unable to process template language expressions in action 'If Condition1' inputs at line '1' and column '1294': 'The template language expression 'and(equals(activity('Copy_Item_Budget_Name').output.executionDetails[3],'Succeeded'), equals(activity('Copy_Item_Budget_Entry').output.executionDetails[3],'Succeeded'))' cannot be evaluated because array index '3' is outside bounds (0, 0) of array. Please see https://aka.ms/logicexpressions for usage details.'.""
}
</code></pre>

<p>But even with the guide I can not solve the problem.</p>

<p>Does anyone know how to solve the problem? Thank you</p>
","<azure><if-statement><azure-data-factory>","2018-10-02 16:14:41","8932","2","3","52695825","<p>If your requirement is to <strong>run some activities after ALL the copy activities completed successfully</strong>, Johns-305's answer is actually correct.</p>

<p>Here's the example with more detailed information. Copy activities are <code>activity 1</code> and <code>activity 2</code>, other activities to run after them are <code>activity 3</code> and <code>activity 4</code>, no dependency between <code>activity 3</code> and <code>activity 4</code>. The activities should be linked as below picture. Please be aware that, the <code>activity 3</code> and <code>activity 4</code> will not be run twice, they will be run only after <strong>both</strong> of <code>activity 1</code> and <code>activity 2</code> are succeeded.
<a href=""https://i.stack.imgur.com/bN3eS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bN3eS.png"" alt=""Activity graph of the example""></a></p>
"
"52612165","Batch pipeline is not working in azure data factory","<p>I create a pipeline having 5 Activity. If anyone of the activity will fail it should run the batch process. So in the fail scenario, I have pointed to the batch process.
Problem is when I point all Activity to batch process it is not triggering but when I removed all activity and point only one activity, It starts working.
I don't know what is the issue.
For more details, Please find the screenshots.</p>

<p><a href=""https://i.stack.imgur.com/491It.png"" rel=""nofollow noreferrer"">In this image if any one of the activity will fail, then email should trigger. But its not working. Please check another image also.</a></p>

<p><a href=""https://i.stack.imgur.com/5YdOo.png"" rel=""nofollow noreferrer"">while in this image its working as expected.</a></p>

<p>Please can someone look into this.</p>
","<c#><azure><azure-data-factory>","2018-10-02 16:01:23","81","-1","1","52658225","<p>Actually in the first image, the email will trigger when all the activities fail. The joined line indicates AND logic.</p>

<p>If you want to make it send email once any of the activities fails, you can try to add an <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-pipeline-activity"" rel=""nofollow noreferrer"">ExecutePipeline activity</a>, and set <code>waitOnCompletion</code> as <code>true</code>, and reference your pipeline in the executePipeline activity.</p>
"
"52609528","Azure Data Factory: How does export of U-SQL variables to next box in a pipeline work?","<p>I have a Pipeline in my Azure Data Factory with a,</p>

<p>U-SQL --> ForEach --> Web</p>

<p>..flow setup.</p>

<p>My U-SQL will eventually do a ""select"" of a single column and I want to call an API for each of the selected rows in the single column.</p>

<p>Can I do this like that? </p>

<p>If yes, How do I get the variable holding the selected query output in the U-SQL script out to the Azure Data Factory?</p>

<p>(so the ForEach can pick it up as a list and send each entry to the Web box, which calls the API and gets the data I need)</p>

<hr>

<p>Right now, my alternative is to take a U-SQL box that selects the column and exports it to /Temp on ADLS and then have one big Azure Batch C# box that read the file and manually for loops the lines and runs the API call for each line. I am just hoping there would be a prettier way more modular ADF style way of doing the same thing.</p>
","<u-sql><azure-data-factory>","2018-10-02 13:36:29","89","0","1","52615880","<p>U-SQL scripts do not return data, so you are on the right path. Azure Batch adds another layer of complexity that you probably don't need in this case. An ADF <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">Lookup Activity</a> can read Blob Storage and ADLS Gen 1, so your pipeline could do the following:</p>

<ol>
<li>U-SQL Activity outputs column to blob.</li>
<li>Lookup Activity reads blob. </li>
<li>Foreach Activity loops through Lookup results. </li>
<li>-> internal Web Activity calls API.</li>
</ol>
"
"52568317","Is there a way to add a column in the copy activity?","<p>I have to call a webservice with multiple times with a diferent parameter value.</p>

<p>therefore i created a foreach task containing a copy activity.</p>

<p>Now i want to save the output of this call along with the parameter value.</p>

<p>Cann i somehow add a new additional field in the copy activity mapping to my parameter value? (@item().value)</p>
","<azure-data-factory>","2018-09-29 12:20:58","296","0","1","52568810","<p>what is your copy source type? You might use a query to include the item().value</p>

<p>You could also reference this post.
<a href=""https://stackoverflow.com/questions/51853329/add-file-name-as-column-in-data-factory-pipeline-destination/51858408#51858408"">Add file name as column in data factory pipeline destination</a></p>
"
"52556066","Where to start with Azure Data Factory","<p>I'm new to Azure Data Factory and I'm working on a proof of concept for my organisation, I'm finding it hard to get good information on fairly basic things and I'm hoping someone can point me to some good resources for my use case. </p>

<p>I know this question is quite general, but any help would be useful. I'm going around in circles at the moment and I feel like I'm wasting a lot of time. Something that would take me a few minutes in ssis has taken hours of research so far and I still haven't progressed much.</p>

<p>Here's the use case:</p>

<ul>
<li>A gzip archive arrives in blob storage every hour, it has several .tsv files in it, but I want to extract one, which has web click stream data.</li>
<li>I want to extract this one .tsv file from the archive, append the datetime to the name and then save it to Azure data lake storage.</li>
<li>I want this to happen each time a new gzip archive arrives.</li>
</ul>

<p>So far I have:</p>

<ul>
<li>Azure Data Factory V2 setup</li>
<li>Linked Service setup to the blob container</li>
<li>Linked Service setup to data lake store Gen1</li>
<li>I think all the permissions and firewall issues sorted for ADF to access storage.</li>
</ul>

<p>Is Azure Data Factory the right tool for this job? and if so, where do I go from here? How do I build datasets and a pipeline to achieve the use case and how do I schedule this to run when a new zip arrives?</p>
","<azure><azure-data-lake><azure-data-factory>","2018-09-28 13:06:29","414","2","2","52558482","<p>Azure Data Factory builts for the complex hybrid extract-transform-load (ETL), extract-load-transform (ELT), and data integration projects, which is also the right tool for this job. Based on the current knowledge, you need to do the following setting in your data factory:</p>

<ol>
<li>Create a pipeline to run the whole workflow, in which a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">Copy activity</a> is involved, and the source dataset is <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage"" rel=""nofollow noreferrer"">blob</a> and sink dataset is <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-store"" rel=""nofollow noreferrer"">data lake store Gen1</a>. Note that the source blob dataset refers to your blob linkedservice and the sink data lake store Gen1 refers to data lake store Gen1 linkedservice.</li>
<li>For the blob source dataset setting, set the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#compression-support"" rel=""nofollow noreferrer"">compression type property as GZIP</a>, this allows ADF to read GZIP compressed data from the blob.</li>
<li>Use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger"" rel=""nofollow noreferrer"">event trigger</a> to fire the pipeline run each time a new gzip archive arrives.</li>
</ol>
"
"52556066","Where to start with Azure Data Factory","<p>I'm new to Azure Data Factory and I'm working on a proof of concept for my organisation, I'm finding it hard to get good information on fairly basic things and I'm hoping someone can point me to some good resources for my use case. </p>

<p>I know this question is quite general, but any help would be useful. I'm going around in circles at the moment and I feel like I'm wasting a lot of time. Something that would take me a few minutes in ssis has taken hours of research so far and I still haven't progressed much.</p>

<p>Here's the use case:</p>

<ul>
<li>A gzip archive arrives in blob storage every hour, it has several .tsv files in it, but I want to extract one, which has web click stream data.</li>
<li>I want to extract this one .tsv file from the archive, append the datetime to the name and then save it to Azure data lake storage.</li>
<li>I want this to happen each time a new gzip archive arrives.</li>
</ul>

<p>So far I have:</p>

<ul>
<li>Azure Data Factory V2 setup</li>
<li>Linked Service setup to the blob container</li>
<li>Linked Service setup to data lake store Gen1</li>
<li>I think all the permissions and firewall issues sorted for ADF to access storage.</li>
</ul>

<p>Is Azure Data Factory the right tool for this job? and if so, where do I go from here? How do I build datasets and a pipeline to achieve the use case and how do I schedule this to run when a new zip arrives?</p>
","<azure><azure-data-lake><azure-data-factory>","2018-09-28 13:06:29","414","2","2","57746029","<p>In terms of getting help, guidance and documentation on Azure Data Factory gen 2, one of the best places is from within the designer itself.  There is a help icon in the top right-hand side, offering links to the Guided Tour and Documentation:</p>

<p><a href=""https://i.stack.imgur.com/1bjnE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1bjnE.png"" alt=""Azure Data Factory designer""></a></p>

<p>Guided tour is context sensitive, so it's worth clicking it different places to get help, eg in the Copy Activity, from within a dataset etc</p>

<p>The documentation has a mix of helpful features, from videos, tutorials, and the 5-min quickstarts and of course it's always kept up-to-date.</p>

<p>Finally <a href=""https://stackoverflow.com/questions/tagged/azure-data-factory-2"">Stack Overflow</a> and <a href=""https://social.msdn.microsoft.com/Forums/vstudio/en-US/home?forum=AzureDataFactory"" rel=""nofollow noreferrer"">MSDN</a> are great resources for getting help on ADF.  I'm pretty sure members of the product team come on and answer questions so you can't get better help than that.  This tends to work best when you've got a specific question or error message and having something to show.</p>
"
"52553365","Azure Data Factory activity copy: Evaluate column in sink table with @pipeline().TriggerTime","<p>With Data Factory V2 I'm trying to implement a stream of data copy from one Azure SQL database to another.</p>

<p>I have mapped all the columns of the source table with the sink table but in the sink table I have an empty column where I would like to enter the pipeline run time. </p>

<p>Does anyone know how to fill this column in the sink table without it being present in the source table?</p>

<p>Below there is the code of my copy pipeline</p>

<pre><code>{
""name"": ""FLD_Item_base"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""Copy_Team"",
            ""description"": ""copytable"",
            ""type"": ""Copy"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false,
                ""secureInput"": false
            },
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""SqlSource""
                },
                ""sink"": {
                    ""type"": ""SqlSink"",
                    ""writeBatchSize"": 10000,
                    ""preCopyScript"": ""TRUNCATE TABLE Team_new""
                },
                ""enableStaging"": false,
                ""dataIntegrationUnits"": 0,
                ""translator"": {
                    ""type"": ""TabularTranslator"",
                    ""columnMappings"": {
                        ""Code"": ""Code"",
                        ""Name"": ""Name""
                    }
                }
            },
            ""inputs"": [
                {
                    ""referenceName"": ""Team"",
                    ""type"": ""DatasetReference""
                }
            ],
            ""outputs"": [
                {
                    ""referenceName"": ""Team_new"",
                    ""type"": ""DatasetReference""
                  }
              ]
          }
      ]
  }
}
</code></pre>

<p>In my sink table I already have the column <code>data_load</code>where I would like to insert the pipeline execution date, but I did not currently map it.</p>
","<azure><azure-sql-database><azure-data-factory>","2018-09-28 10:27:40","2080","2","2","52556672","<p>you can consider using stored procedure at the sink side to apply the source data into the sink table by designating ""<strong><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#azure-sql-database-as-the-sink"" rel=""nofollow noreferrer"">sqlWriterStoredProcedureName</a></strong>"" of the SqlSink. Pass the pipeline run time to the stored procedure as the parameter and insert into sink table. </p>
"
"52553365","Azure Data Factory activity copy: Evaluate column in sink table with @pipeline().TriggerTime","<p>With Data Factory V2 I'm trying to implement a stream of data copy from one Azure SQL database to another.</p>

<p>I have mapped all the columns of the source table with the sink table but in the sink table I have an empty column where I would like to enter the pipeline run time. </p>

<p>Does anyone know how to fill this column in the sink table without it being present in the source table?</p>

<p>Below there is the code of my copy pipeline</p>

<pre><code>{
""name"": ""FLD_Item_base"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""Copy_Team"",
            ""description"": ""copytable"",
            ""type"": ""Copy"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false,
                ""secureInput"": false
            },
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""SqlSource""
                },
                ""sink"": {
                    ""type"": ""SqlSink"",
                    ""writeBatchSize"": 10000,
                    ""preCopyScript"": ""TRUNCATE TABLE Team_new""
                },
                ""enableStaging"": false,
                ""dataIntegrationUnits"": 0,
                ""translator"": {
                    ""type"": ""TabularTranslator"",
                    ""columnMappings"": {
                        ""Code"": ""Code"",
                        ""Name"": ""Name""
                    }
                }
            },
            ""inputs"": [
                {
                    ""referenceName"": ""Team"",
                    ""type"": ""DatasetReference""
                }
            ],
            ""outputs"": [
                {
                    ""referenceName"": ""Team_new"",
                    ""type"": ""DatasetReference""
                  }
              ]
          }
      ]
  }
}
</code></pre>

<p>In my sink table I already have the column <code>data_load</code>where I would like to insert the pipeline execution date, but I did not currently map it.</p>
","<azure><azure-sql-database><azure-data-factory>","2018-09-28 10:27:40","2080","2","2","52585664","<p>Based on your situation, please configure SQL Server stored procedure in your SQL Server sink as a workaround.</p>

<p>Please follow the steps from this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#invoking-stored-procedure-for-sql-sink"" rel=""nofollow noreferrer"">doc</a>:</p>

<p>Step 1: Configure your Sink dataset:</p>

<p><a href=""https://i.stack.imgur.com/Zrcb2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Zrcb2.png"" alt=""enter image description here""></a></p>

<p>Step 2: Configure Sink section in copy activity as follows:</p>

<p><a href=""https://i.stack.imgur.com/6SCvR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6SCvR.png"" alt=""enter image description here""></a></p>

<p>Step 3: In your database, define the table type with the same name as sqlWriterTableType. Notice that the schema of the table type should be same as the schema returned by your input data.</p>

<pre><code>    CREATE TYPE [dbo].[testType] AS TABLE(
    [ID] [varchar](256) NOT NULL,
    [EXECUTE_TIME] [datetime] NOT NULL
)
GO
</code></pre>

<p>Step 4: In your database, define the stored procedure with the same name as <code>SqlWriterStoredProcedureName</code>. It handles input data from your specified source, and merge into the output table. Notice that the parameter name of the stored procedure should be the same as the ""tableName"" defined in dataset.</p>

<pre><code>Create PROCEDURE convertCsv @ctest [dbo].[testType] READONLY
AS
BEGIN
  MERGE [dbo].[adf] AS target
  USING @ctest AS source
  ON (1=1)
  WHEN NOT MATCHED THEN
      INSERT (id,executeTime)
      VALUES (source.ID,GETDATE());
END
</code></pre>
"
"52551213","DataFactory Web API pass scheduled start time","<p>I am using Azure Data Factory V2 and trying to pass the scheduled start and end time (from a timer trigger) to a API call. Is this not possible or am I missing a trick? </p>

<p>Thanks </p>

<p>One route that I pursued.</p>

<p><a href=""https://i.stack.imgur.com/zWwEB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zWwEB.png"" alt=""enter image description here""></a></p>

<p>And another through setting the trigger scheduled start as an input parameter. 
<a href=""https://i.stack.imgur.com/US6PF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/US6PF.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2018-09-28 08:22:07","49","0","1","52552214","<p>Sorted it out, I was trying to set the trigger value as the default for the pipeline variables. When you set up pipeline variables, they become available to the trigger, and then in the second page of your trigger configuration, you set the pipeline parameters to the trigger values. </p>

<p>Since the fields are truncated in the image, the properties are: </p>

<ul>
<li>@trigger().outputs.windowStartTime</li>
<li>@trigger().outputs.windowEndTime  </li>
</ul>

<p><a href=""https://i.stack.imgur.com/mcLEa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mcLEa.png"" alt=""enter image description here""></a></p>
"
"52548681","API access from Azure Data Factory","<ol>
<li><p>I want to create a ADF pipeline which needs to access an API and using some filter parameter it will get data from there and write the output in JSON format in DataLake. How can I do that??</p></li>
<li><p>After the JSON available in Lake it needs to be converted to CSV file. How to do?</p></li>
</ol>
","<azure-data-factory>","2018-09-28 05:09:44","628","1","2","52549057","<p>the below work follow may meet your requirement:</p>

<ol>
<li>Involve a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">Copy activity</a> in ADFv2, where the source dataset is <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-http"" rel=""nofollow noreferrer"">HTTP data store</a> and the destination is the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-store"" rel=""nofollow noreferrer"">Azure Data lake store</a>, HTTP source data store allows you to fetch data by calling API and Copy activity will copy data into your destination data lake.</li>
<li>Chain an <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-data-lake-analytics"" rel=""nofollow noreferrer"">U-SQL activity</a> after Copy activity, once the Copy activity succeeds, it'll run the U-SQL script to convert json file to CSV file.</li>
</ol>
"
"52548681","API access from Azure Data Factory","<ol>
<li><p>I want to create a ADF pipeline which needs to access an API and using some filter parameter it will get data from there and write the output in JSON format in DataLake. How can I do that??</p></li>
<li><p>After the JSON available in Lake it needs to be converted to CSV file. How to do?</p></li>
</ol>
","<azure-data-factory>","2018-09-28 05:09:44","628","1","2","52549107","<p>You can create a pipeline with copy activity from HTTP connector to Datalake connector. Use HTTP as the copy source to access the API (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-http"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-http</a>), specify the format in dataset as JSON. Reference <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#json-format"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#json-format</a> on how to define the schema. Use Datalake connector as the copy sink, specify the format as Text format, and do some modification like row delimiter and column delimiter according to your need.</p>
"
"52534252","Azure Data Factory activity copy get multiple table name from foreach activity","<p>with Data Factory V2 I'm trying to implement a stream of data copy from one Azure SQL database to another.
I have to merge multiple tables into one target table.</p>

<p>To do this I created a lookup activity that creates the name of the three tables to be copied. The output <code>JSON</code> file is passed to a foreach activity that should copy the data of each table into the destination table, but the pipeline execution is not successful.</p>

<p>I report the code of my pipeline below:</p>

<pre><code>{
""name"": ""FLD_Item_base"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""myLookup"",
            ""type"": ""Lookup"",
            ""dependsOn"": [
                {
                    ""activity"": ""myStoredProcedure"",
                    ""dependencyConditions"": [
                        ""Succeeded""
                    ]
                }
            ],
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false,
                ""secureInput"": false
            },
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""SqlSource"",
                    ""sqlReaderQuery"": ""SELECT tsy_company_desc, tsy_tabella_desc, tsy_company_id, \ncase when tsy_company_desc = 'all' \nthen '['+ tsy_tabella_desc + ']' else '['+tsy_company_desc + '$' + tsy_tabella_desc + ']'  end as nome_tabella_completo \nfrom dbo.TSY_FLUSSI_LOAD_DWH \nwhere tsy_flusso_desc = 'item_base' \nand tsy_tabella_desc='Item' \n""
                },
                ""dataset"": {
                    ""referenceName"": ""TLD_Item"",
                    ""type"": ""DatasetReference""
                },
                ""firstRowOnly"": false
            }
        },
        {
            ""name"": ""myStoredProcedure"",
            ""type"": ""SqlServerStoredProcedure"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false,
                ""secureInput"": false
            },
            ""typeProperties"": {
                ""storedProcedureName"": ""[dbo].[myStoredProcedure]""
            },
            ""linkedServiceName"": {
                ""referenceName"": ""Sink"",
                ""type"": ""LinkedServiceReference""
            }
        },
        {
            ""name"": ""IterateSQLTables"",
            ""type"": ""ForEach"",
            ""dependsOn"": [
                {
                    ""activity"": ""myLookup"",
                    ""dependencyConditions"": [
                        ""Succeeded""
                    ]
                }
            ],
            ""typeProperties"": {
                ""items"": {
                    ""value"": ""@activity('myLookup').output"",
                    ""type"": ""Expression""
                },
                ""activities"": [
                    {
                        ""name"": ""FullCopyActivity"",
                        ""type"": ""Copy"",
                        ""policy"": {
                            ""timeout"": ""7.00:00:00"",
                            ""retry"": 0,
                            ""retryIntervalInSeconds"": 30,
                            ""secureOutput"": false,
                            ""secureInput"": false
                        },
                        ""typeProperties"": {
                            ""source"": {
                                ""type"": ""SqlSource"",
                                ""sqlReaderQuery"": {
                                    ""value"": ""SELECT * FROM @{item().value.name_tab}"",
                                    ""type"": ""Expression""
                                }
                            },
                            ""sink"": {
                                ""type"": ""SqlSink"",
                                ""writeBatchSize"": 10000
                            },
                            ""enableStaging"": false,
                            ""dataIntegrationUnits"": 0
                        },
                        ""inputs"": [
                            {
                                ""referenceName"": ""Source"",
                                ""type"": ""DatasetReference""
                            }
                        ],
                        ""outputs"": [
                            {
                                ""referenceName"": ""TLD_Item"",
                                ""type"": ""DatasetReference""
                            }
                        ]
                    }
                ]
            }
        }
</code></pre>

<p>Pipeline execution returns the following error:</p>

<pre><code>{
""errorCode"": ""400"",
""message"": ""Activity failed because an inner activity failed"",
""failureType"": ""UserError"",
""target"": ""IterateSQLTables""
}
</code></pre>

<p>Does anyone know how to solve the problem? Thank you</p>
","<azure><foreach><azure-data-factory>","2018-09-27 09:54:42","1311","1","1","52535930","<p>Please try <code>@activity('myLookup').output.value</code> instead of <code>@activity('myLookup').output</code> in the <code>items</code> of activity <code>IterateSQLTables</code>.</p>

<p>You can find document for lookup activity <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity#use-the-lookup-activity-result-in-a-subsequent-activity"" rel=""nofollow noreferrer"">here</a></p>
"
"52518303","ADFV2 Custom Activity","<p>Building on the example given here in ADFV2 documentation - <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity#executing-commands"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity#executing-commands</a></p>

<p>The code snippet from the same documentation mentioned above, can't be used as is - </p>

<p>dynamic activity = JsonConvert.DeserializeObject(File.ReadAllText(""activity.json""));
Console.WriteLine(activity.typeProperties.extendedProperties.connectionString.value</p>

<p>As this would give a RuntimeBinderException:
'Newtonsoft.Json.Linq.JValue' does not contain a definition for 'value'</p>
","<azure><json-deserialization><azure-data-factory><custom-activity>","2018-09-26 12:57:17","254","0","1","52559430","<p>I'm not sure about the connection string format, but I have several custom (Azure Batch) activities running in ADFv2. I use "".ToString()"" instead of "".value"" in the console app:</p>

<pre><code>    // Parse activity and reference objects info from input files
    var parameters = new Dictionary&lt;string, string&gt;();    
    dynamic activity = JsonConvert.DeserializeObject(File.ReadAllText(""activity.json""));
    parameters.Add(""url"", activity.typeProperties.extendedProperties.Url.ToString());
</code></pre>
"
"52514153","How to read files with .xlsx and .xls extension in Azure data factory?","<p>I am trying to read and excel file in Azure Blob Storage with .xlsx extension in my azure data factory dataset. it throws following error </p>

<pre><code>Error found when processing 'Csv/Tsv Format Text' source 'Filename.xlsx' with row number 3: found more columns than expected column count: 1.
</code></pre>

<p>What are the right Column and row delimiters for excel files to be read in azure Data factory</p>
","<excel><azure><azure-data-factory>","2018-09-26 09:16:45","16554","6","4","52516089","<p><strong>Update March 2022:</strong>  ADF now has better support for Excel via Mapping Data Flows:</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-excel"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/format-excel</a></p>
<p>Excel files have a proprietary format and are not simple delimited files.  As indicated <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/import-export/import-data-from-excel-to-sql?view=sql-server-2017#adf"" rel=""nofollow noreferrer"">here</a>, Azure Data Factory does <strong>not</strong> have a direct option to import Excel files, eg you <strong>cannot</strong> create a Linked Service to an Excel file and read it easily.  Your options are:</p>
<ol>
<li>Export or convert the data as flat files eg before transfer to cloud, as .csv, tab-delimited, pipe-delimited etc are easier to read than Excel files.  This is your simplest option although obviously requires a change in process.</li>
<li>Try shredding the XML - create a custom task to open the Excel file as XML and extract your data as suggested <a href=""https://stackoverflow.com/questions/44109957/loading-data-from-excel-file-into-azure-datawarehouse"">here</a>.</li>
<li>SSIS packages are now supported in Azure Data Factory (with the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-invoke-ssis-package-ssis-activity"" rel=""nofollow noreferrer"">Execute SSIS package</a> activity) and have better support for Excel files, eg a Connection Manager.   So it <em>may</em> be an option to create an SSIS package to deal with the Excel and host it in ADFv2.  <strong>Warning!</strong>  I have not tested this, I am only speculating it is possible.  Also there is the overhead of creating an Integration Runtime (IR) for running SSIS in ADFv2.</li>
<li><s>Try some other custom activity, eg there is a custom U-SQL Extractor for shredding XML on github <a href=""https://github.com/Azure/AzureDataLake/tree/master/Samples/ExcelExtractor"" rel=""nofollow noreferrer"">here</a>.</s></li>
<li>Try and read the Excel using Databricks, some examples <a href=""https://stackoverflow.com/questions/44196741/how-to-construct-dataframe-from-a-excel-xls-xlsx-file-in-scala-spark"">here</a> although spinning up a Spark cluster to read a few Excel files does seem somewhat overkill.  This might be a good option if Spark is already in your architecture.</li>
</ol>
<p>Let us know how you get on.</p>
"
"52514153","How to read files with .xlsx and .xls extension in Azure data factory?","<p>I am trying to read and excel file in Azure Blob Storage with .xlsx extension in my azure data factory dataset. it throws following error </p>

<pre><code>Error found when processing 'Csv/Tsv Format Text' source 'Filename.xlsx' with row number 3: found more columns than expected column count: 1.
</code></pre>

<p>What are the right Column and row delimiters for excel files to be read in azure Data factory</p>
","<excel><azure><azure-data-factory>","2018-09-26 09:16:45","16554","6","4","54848999","<p>Azure Data Factory does not have a direct option to upload the excel, however, it can be uploaded <strong>through linked service</strong> via some configurations. Follow below steps to do so:</p>

<ol>
<li>Create a Linked service to the source using the appropriate protocol(FTP, Fileshare, SFTP etc.)</li>
<li>Create a Linked Service to your cloud storage.</li>
<li>Take a <strong>Copy Data</strong> activity, define a filesystem Source Dataset and Sink using your previously defined linked services</li>
<li>In Source and Sink, tick the Binary file option.<a href=""https://i.stack.imgur.com/FJbod.png"" rel=""nofollow noreferrer"">Binary file option</a></li>
<li>Publish and execute your pipeline</li>
</ol>
"
"52514153","How to read files with .xlsx and .xls extension in Azure data factory?","<p>I am trying to read and excel file in Azure Blob Storage with .xlsx extension in my azure data factory dataset. it throws following error </p>

<pre><code>Error found when processing 'Csv/Tsv Format Text' source 'Filename.xlsx' with row number 3: found more columns than expected column count: 1.
</code></pre>

<p>What are the right Column and row delimiters for excel files to be read in azure Data factory</p>
","<excel><azure><azure-data-factory>","2018-09-26 09:16:45","16554","6","4","60435187","<p>I would like to elaborate a bit more on option 3. SSIS packages. I tested the solution and it sorta worked. First of all - SSIS IR is super expensive and You would like to use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-schedule-azure-ssis-integration-runtime"" rel=""nofollow noreferrer"">http post's to start-stop</a> it while it is not needed.</p>

<p>Secondly, SSIS does not support processing blob files out of the box. There are some ways around it. You can use some third-party soft (didn't try that), or <a href=""https://learn.microsoft.com/en-us/sql/integration-services/lift-shift/ssis-azure-files-file-shares?view=sql-server-ver15"" rel=""nofollow noreferrer"">explore options</a></p>

<p>In my case I used trick to deploy package that download file locally, process and then delete it. Since SSIS IR is a VM, it basically did the same on IR after deployment. It was a trick rather than solution and it was not 100% stable. Another issue - numeric fields got some insignificant error during processing - eg. uploading 25 resulted in 25.0000000001 at db site. No idea why. I did not have enough time to investigate issues deeper.</p>
"
"52514153","How to read files with .xlsx and .xls extension in Azure data factory?","<p>I am trying to read and excel file in Azure Blob Storage with .xlsx extension in my azure data factory dataset. it throws following error </p>

<pre><code>Error found when processing 'Csv/Tsv Format Text' source 'Filename.xlsx' with row number 3: found more columns than expected column count: 1.
</code></pre>

<p>What are the right Column and row delimiters for excel files to be read in azure Data factory</p>
","<excel><azure><azure-data-factory>","2018-09-26 09:16:45","16554","6","4","62429364","<p>Azure Data Factory V2 has recently released an update to support parsing Excel(.xls) files on existing connectors.</p>

<p>Currently, the connections supporting excel files are:</p>

<ul>
<li>Amazon S3</li>
<li>Azure Blob</li>
<li>Azure Data Lake Storage Gen1</li>
<li>Azure Data Lake Storage Gen2</li>
<li>Azure File Storage</li>
<li>File System</li>
<li>FTP</li>
<li>Google Cloud Storage</li>
<li>HDFS</li>
<li>HTTP</li>
<li>SFTP</li>
</ul>

<p>More details can be found here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-excel"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/format-excel</a></p>
"
"52513409","Dynamic content in GlobalParameters in Azure Data Factory V2 ML Batch execution web service","<p>I'm trying to use Dynamic Content within the GlobalParameters on a ML Batch Execution module in Azure Data Factory V2.</p>

<p>Seems everything I try comes up with the same errors.  Please see attached screen shots.</p>

<p><a href=""https://i.stack.imgur.com/RghZF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RghZF.png"" alt=""Trying to do dynamic content globalParameters""></a></p>

<p>Here is the only bit of documentation I can find on the subject ...</p>

<p><a href=""https://i.stack.imgur.com/dWdP2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dWdP2.png"" alt=""documentation""></a></p>
","<azure><azure-data-factory><ml-studio>","2018-09-26 08:36:09","147","0","2","52529043","<p>It seems that this is just a bug.  Whenever you click on Add Dynamic Content and click Finish it wipes out all your parameters setup manually and gives you an error about GlobalParameters cannot be a String type.</p>

<p>The secret is to setup your parameters manually.  Then use the Dynamic Content window just to allow you to choose your dynamic parts.  It will always say your syntax is wrong but don't worry.  Just COPY your code and click CANCEL on the Dynamic Content window.  Then paste the dynamic code into the manually created parameters.</p>

<p>This will work!  Hoever, if you accidentally click Add Dynamic Content and Click Finish, it will wipe all your parameters and you'll have to start again.</p>
"
"52513409","Dynamic content in GlobalParameters in Azure Data Factory V2 ML Batch execution web service","<p>I'm trying to use Dynamic Content within the GlobalParameters on a ML Batch Execution module in Azure Data Factory V2.</p>

<p>Seems everything I try comes up with the same errors.  Please see attached screen shots.</p>

<p><a href=""https://i.stack.imgur.com/RghZF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RghZF.png"" alt=""Trying to do dynamic content globalParameters""></a></p>

<p>Here is the only bit of documentation I can find on the subject ...</p>

<p><a href=""https://i.stack.imgur.com/dWdP2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dWdP2.png"" alt=""documentation""></a></p>
","<azure><azure-data-factory><ml-studio>","2018-09-26 08:36:09","147","0","2","59467863","<p>It works for me, just using <code>@variables('input1')</code> as parameter:</p>

<p><a href=""https://i.stack.imgur.com/Hq5Am.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Hq5Am.png"" alt=""enter image description here""></a></p>
"
"52511815","how to run exe file in azure data factroy with input parameters?","<p>i have a console application. i built this application and uploaded it to the Azure  blob storage. Then i run this application Azure data factory pipeline. All are fine but the problem is if i want to add new parameters(get input) to console application how can i do that? Is there any specific way to do it?</p>

<pre><code> {
    ""name"": ""samplebatch"",
    ""type"": ""Custom"",
    ""policy"": {
        ""timeout"": ""7.00:00:00"",
        ""retry"": 0,
        ""retryIntervalInSeconds"": 30,
        ""secureOutput"": false
    },    
""typeProperties"": {
        ""command"": ""SampleApp.exe"",
        ""folderPath"": ""customactv2/SampleApp"",
        ""resourceLinkedService"": {
          ""referenceName"": ""StorageLinkedService"",
          ""type"": ""LinkedServiceReference""
        }
   ""linkedServiceName"": {
        ""referenceName"": ""dataloadbatchservice"",
        ""type"": ""LinkedServiceReference""
    }
}
</code></pre>

<p>This is what i have done so far in the data factory pipeline code.</p>
","<c#><azure><azure-blob-storage><azure-data-factory>","2018-09-26 07:03:35","547","1","1","52528438","<p>Please refer to the <code>extendedProperties</code> property in <code>typeProperties</code>, you could use it.</p>

<blockquote>
  <p>User-defined properties that can be passed to the custom application
  in JSON format so your custom code can reference additional properties</p>
</blockquote>

<p>Doc: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity#custom-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity#custom-activity</a></p>

<p>Sample:<a href=""https://github.com/Azure/Azure-DataFactory/blob/master/Samples/ADFv2CustomActivitySample/MyCustomActivityPipeline.json"" rel=""nofollow noreferrer"">https://github.com/Azure/Azure-DataFactory/blob/master/Samples/ADFv2CustomActivitySample/MyCustomActivityPipeline.json</a></p>
"
"52511571","What is the way to incremental sftp from remote server to azure using azure data factory","<p>What is the way to incremental sftp from remote server to azure using azure data factory. I have a scenario where i need to copy files from remote server to azure.</p>
","<azure><azure-data-factory>","2018-09-26 06:48:53","225","0","1","52564721","<p>to do this kind of copy task in Azure Data Factory, create a pipeline with a Copy activity in it, set the source dataset as <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sftp"" rel=""nofollow noreferrer"">SFTP dataset</a> and the sink dataset as <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage"" rel=""nofollow noreferrer"">Azure blob</a>. You could also consider to use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-copy-data-tool"" rel=""nofollow noreferrer"">Copy data tool</a> in Azure data factory.</p>
"
"52510494","Azure Data Factory outbound data transfer pricing","<p>I want to use Azure Data Factory V2 to copy data from Blob Storage to another Blob Storage. Two Blob storage are the same region and Data Factory is different region.</p>

<p>So outbound data transfer will be charged or not?</p>
","<azure><azure-data-factory>","2018-09-26 05:25:42","455","1","2","52511672","<p>Bandwidth refers to data moving in and out of Azure data centers.</p>

<p><strong>Data going out of Azure data centres are charged as follows:</strong></p>

<p><a href=""https://i.stack.imgur.com/G80u2.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G80u2.jpg"" alt=""enter image description here""></a></p>

<p>Use of the copy activity to egress data out of an Azure datacenter will incur additional network bandwidth charges, which will show up as a separate outbound data transfer line item on your bill. <a href=""https://azure.microsoft.com/en-in/pricing/details/bandwidth/"" rel=""nofollow noreferrer"">More information</a>.</p>
"
"52510494","Azure Data Factory outbound data transfer pricing","<p>I want to use Azure Data Factory V2 to copy data from Blob Storage to another Blob Storage. Two Blob storage are the same region and Data Factory is different region.</p>

<p>So outbound data transfer will be charged or not?</p>
","<azure><azure-data-factory>","2018-09-26 05:25:42","455","1","2","54459542","<p>Azure Data Factory charges you in a variety of modes as below based on your integration runtime. 
How ever, when your Copy Activity moves data with a same data center. You will not be charged for that. (DIU's count is not considered for billing)
Pipeline activities: $0.005/hour**</p>
"
"52504167","Azure Data factory pipeline varied execution times","<p>We have about 190 hourly usage files that need to arrive in the data lake in a 24 hour period before we can kick off our pipeline which starts off with an analytics activity. We have had this pipeline run on a scheduler on an estimated time of when we expect all files to have arrived but doesn't always happen so we would need to re-run the slices for the missing files. </p>

<p>Is there a more efficient way to handle this and not have the pipeline on a schedule and have it triggered by the event that all files have arrived in the datalake.</p>

<p>TIA for input!</p>
","<azure><azure-data-lake><azure-data-factory>","2018-09-25 17:56:09","268","0","1","52558942","<p>You can add an Event Trigger when a new blob is created (or deleted). We do this in production with a logic app, but data factory V2 appear to support it now as well. The benefit is that you don't have to estimate the proper frequency, you can just execute when necessary.</p>

<p>NOTE: there is a limit to the number of concurrent pipelines you can have executing, so if you dropped all 190 files into blob storage at once, you may run into resource availability issues.</p>
"
"52491959","Azure Data Factory Dataset with @ in column name","<p>I have a Dataset coming from a Rest webservice having an @ in the column name:
Like:
    <code>{
        data[{
                 @id : 1,
                 @value : ""a""
            }, {
                 @id : 2,
                 @value : ""b""
            }
        ]
    }</code></p>

<p>i want to use it in a foreach and access the specific column:
in the foreach i get the output like <code>@activity('Lookup').output.value</code></p>

<p>in the foreach there is a stored procedure
as parameter input i tried to get the column: i tried <code>@item().@value</code> but got the error ""the string character '@' at position 'xx' is not expected"".</p>

<p>is there a way to escape the @ in the column name? or can i rename the column?</p>

<p>Thank you very much</p>

<p>edit: 
here is the JSON from the ADF pipeline:</p>

<pre><code>{
""name"": ""pipeline3"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""Lookup1"",
            ""type"": ""Lookup"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false,
                ""secureInput"": false
            },
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""HttpSource"",
                    ""httpRequestTimeout"": ""00:01:40""
                },
                ""dataset"": {
                    ""referenceName"": ""HttpFile1"",
                    ""type"": ""DatasetReference""
                },
                ""firstRowOnly"": false
            }
        },
        {
            ""name"": ""ForEach2"",
            ""type"": ""ForEach"",
            ""dependsOn"": [
                {
                    ""activity"": ""Lookup1"",
                    ""dependencyConditions"": [
                        ""Succeeded""
                    ]
                }
            ],
            ""typeProperties"": {
                ""items"": {
                    ""value"": ""@activity('Lookup1').output.value"",
                    ""type"": ""Expression""
                },
                ""activities"": [
                    {
                        ""name"": ""Stored Procedure12"",
                        ""type"": ""SqlServerStoredProcedure"",
                        ""policy"": {
                            ""timeout"": ""7.00:00:00"",
                            ""retry"": 0,
                            ""retryIntervalInSeconds"": 30,
                            ""secureOutput"": false,
                            ""secureInput"": false
                        },
                        ""typeProperties"": {
                            ""storedProcedureName"": ""[dbo].[testnv]"",
                            ""storedProcedureParameters"": {
                                ""d"": {
                                    ""value"": {
                                        ""value"": ""@item().@accno"",
                                        ""type"": ""Expression""
                                    },
                                    ""type"": ""String""
                                }
                            }
                        },
                        ""linkedServiceName"": {
                            ""referenceName"": ""AzureSqlDatabase1"",
                            ""type"": ""LinkedServiceReference""
                        }
                    }
                ]
            }
        }
    ]
},
""type"": ""Microsoft.DataFactory/factories/pipelines""
</code></pre>

<p>}</p>
","<azure-data-factory>","2018-09-25 06:33:44","1744","0","1","52499203","<p>Please try ""@item()['@accno']"" for @item().@accno
Also replied in MSDN.</p>
"
"52486559","'Failed to encrypt sub-resource payload' error when attempting CI/CD","<p>We are trying to setup CI/deployment with DevOps using the documentation provided here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a>. We are using a shared IR that has been set up in the target environment prior to deployment.</p>

<p>The release succeeds if the deployment mode setting is set to validation only, but fails when incremental or complete is selected. We get the following error when using override template parameters:</p>

<blockquote>
  <p>2018-09-21T17:07:43.2936188Z ##[error]BadRequest: {</p>
  
  <p>""error"": {</p>
  
  <p>""code"": ""BadRequest"",</p>
  
  <p>""message"": ""Failed to encrypt sub-resource payload</p>
</blockquote>
","<azure><azure-data-factory>","2018-09-24 20:01:52","5468","3","6","52490051","<p>Please make sure your shared IR is online when doing the deployment, otherwise you may meet this problem because self-hosted IR will be used to encrypt your payload. </p>

<p>If you confirm the above action is done and you still have this error, please share the request activity ID to us and we can do some further investigation.</p>
"
"52486559","'Failed to encrypt sub-resource payload' error when attempting CI/CD","<p>We are trying to setup CI/deployment with DevOps using the documentation provided here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a>. We are using a shared IR that has been set up in the target environment prior to deployment.</p>

<p>The release succeeds if the deployment mode setting is set to validation only, but fails when incremental or complete is selected. We get the following error when using override template parameters:</p>

<blockquote>
  <p>2018-09-21T17:07:43.2936188Z ##[error]BadRequest: {</p>
  
  <p>""error"": {</p>
  
  <p>""code"": ""BadRequest"",</p>
  
  <p>""message"": ""Failed to encrypt sub-resource payload</p>
</blockquote>
","<azure><azure-data-factory>","2018-09-24 20:01:52","5468","3","6","53872747","<p>Make sure that you've entered the right connection string into your parameters JSON for any linked services you are using. This fixed the error for me although I don't have a full CI/CD environment with IR established.</p>
"
"52486559","'Failed to encrypt sub-resource payload' error when attempting CI/CD","<p>We are trying to setup CI/deployment with DevOps using the documentation provided here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a>. We are using a shared IR that has been set up in the target environment prior to deployment.</p>

<p>The release succeeds if the deployment mode setting is set to validation only, but fails when incremental or complete is selected. We get the following error when using override template parameters:</p>

<blockquote>
  <p>2018-09-21T17:07:43.2936188Z ##[error]BadRequest: {</p>
  
  <p>""error"": {</p>
  
  <p>""code"": ""BadRequest"",</p>
  
  <p>""message"": ""Failed to encrypt sub-resource payload</p>
</blockquote>
","<azure><azure-data-factory>","2018-09-24 20:01:52","5468","3","6","70103849","<p>I can solve it using the Azure Key Vault.</p>
<p>I added the connection string as a Secret.
In the connection string I also included the authentication data (username and password).</p>
<p>The limitation of this approach is that the possibility of passing the parameters is lost.
For example: dynamic values such as the name of the database or the user.</p>
"
"52486559","'Failed to encrypt sub-resource payload' error when attempting CI/CD","<p>We are trying to setup CI/deployment with DevOps using the documentation provided here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a>. We are using a shared IR that has been set up in the target environment prior to deployment.</p>

<p>The release succeeds if the deployment mode setting is set to validation only, but fails when incremental or complete is selected. We get the following error when using override template parameters:</p>

<blockquote>
  <p>2018-09-21T17:07:43.2936188Z ##[error]BadRequest: {</p>
  
  <p>""error"": {</p>
  
  <p>""code"": ""BadRequest"",</p>
  
  <p>""message"": ""Failed to encrypt sub-resource payload</p>
</blockquote>
","<azure><azure-data-factory>","2018-09-24 20:01:52","5468","3","6","70262154","<p>I would request you to look into the connection string for the respective Linked Service for which you have attached IR. For my ASQL based Linked service I had to use something like this , simple server name would not suffice and you will get <code>&quot;message&quot;: &quot;Failed to encrypt sub-resource payload</code></p>
<pre><code>&quot;typeProperties&quot;: {
            &quot;connectionString&quot;: &quot;Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=axxx-xxx-xx-xxxx.database.windows.net;Initial Catalog=\&quot;@{split(linkedService().LS_ASQL_SERVERDB,';')[1]}\&quot;&quot;
        }
</code></pre>
"
"52486559","'Failed to encrypt sub-resource payload' error when attempting CI/CD","<p>We are trying to setup CI/deployment with DevOps using the documentation provided here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a>. We are using a shared IR that has been set up in the target environment prior to deployment.</p>

<p>The release succeeds if the deployment mode setting is set to validation only, but fails when incremental or complete is selected. We get the following error when using override template parameters:</p>

<blockquote>
  <p>2018-09-21T17:07:43.2936188Z ##[error]BadRequest: {</p>
  
  <p>""error"": {</p>
  
  <p>""code"": ""BadRequest"",</p>
  
  <p>""message"": ""Failed to encrypt sub-resource payload</p>
</blockquote>
","<azure><azure-data-factory>","2018-09-24 20:01:52","5468","3","6","73412023","<p>I override parameter because of the connection string was secure. Use dummy value of(username, password, connection string) if You don't have original ones and then deploy.</p>
"
"52486559","'Failed to encrypt sub-resource payload' error when attempting CI/CD","<p>We are trying to setup CI/deployment with DevOps using the documentation provided here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a>. We are using a shared IR that has been set up in the target environment prior to deployment.</p>

<p>The release succeeds if the deployment mode setting is set to validation only, but fails when incremental or complete is selected. We get the following error when using override template parameters:</p>

<blockquote>
  <p>2018-09-21T17:07:43.2936188Z ##[error]BadRequest: {</p>
  
  <p>""error"": {</p>
  
  <p>""code"": ""BadRequest"",</p>
  
  <p>""message"": ""Failed to encrypt sub-resource payload</p>
</blockquote>
","<azure><azure-data-factory>","2018-09-24 20:01:52","5468","3","6","75262737","<p>The IR already being running doesn't make sense when doing a full deployment of an ADF instance.  The IR key is generated within the instance of ADF you deploy, meaning you've created circular logic: you cannot deploy IR until the deployment of ADF is complete, but you can't complete the deployment of ADF until the IR is deployed.</p>
<p>So far our answer has been to let the arm template fail at this point, which is after the IR registration in the template so the IR key is then generated.  We use that to deploy the IR, then re-run the template and it succeeds... it's stupid and hacky and there has to be a more sane way to do this than intentional failure/retry.</p>
"
"52481046","SSIS Package Execution Failed (Data Factory) - Script Component not loading code","<p>Since last week, my SSIS packages running in Azure Data Factory (v2) are failing one by one. They haven't changed though. All the packages are failing with the same series of errors, as shown below.</p>

<p>The packages are running perfectly in Visual Studio, but when I deploy them to Data Factory, I got these messages.</p>

<p>Any thoughts on where to look?</p>

<p>I've tried to recompiled the script components, to redeploy the whole project. I've also tried to copy the C# code into a new component, unfortunately results in the same issue. </p>

<p><a href=""https://i.stack.imgur.com/R5RmU.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/R5RmU.png"" alt=""Errors""></a></p>
","<azure><ssis><factory><azure-data-factory>","2018-09-24 13:58:20","670","5","1","53162248","<p>So I was driven kind of crazy by this, had the exact same problem, and was troubleshooting with the assistance of <a href=""https://www.innovativearchitects.com/"" rel=""nofollow noreferrer"">Innovative Architects</a>, a group near Atlanta. And we discovered that <strong><em>LITERALLY TODAY</em></strong> Microsoft released an updated version that fixes this exact problem. So apparently it was a bug that they fixed.</p>

<p>See <a href=""https://learn.microsoft.com/en-us/sql/ssdt/changelog-for-sql-server-data-tools-ssdt?view=sql-server-2017#ssdt-for-visual-studio-2017-1582"" rel=""nofollow noreferrer"">here</a> for the fix.</p>
"
"52479935","How to use an Azure Function (Python) as a HTTP dataset in an Azure Data Factory","<p>I keep getting connection timeouts when I try to create a dataset (HTTP) in  an Azure Data Factory which is based on a Azure Function (HTTP trigger):</p>

<pre><code>  The cloud service request timed out. Please retry. Activity ID:9d70efcd-c842-4484-9313-4872208a2a9e
</code></pre>

<p>However, if I call function from anywhere else e.g. from <a href=""https://apitester.com/shared/checks/d6108814e83149b4baee7f2b60786c82"" rel=""nofollow noreferrer"">apitester.com</a>, I get the desired response:</p>

<ul>
<li>Request URL: <a href=""https://srf-scadapa-fa2-windows.azurewebsites.net/api/HttpTriggerPython32?code=L5WVNJh8loDv3mZFcD/AKatNRoYfYoHlDbEBk4AEGrbDA39ddAXsyg=="" rel=""nofollow noreferrer"">https://srf-scadapa-fa2-windows.azurewebsites.net/api/HttpTriggerPython32?code=L5WVNJh8loDv3mZFcD/AKatNRoYfYoHlDbEBk4AEGrbDA39ddAXsyg==</a></li>
<li>Request method: post</li>
<li>Request body: { ""group_name"": ""Azure POC"" }</li>
<li><p>Response body:</p>

<p>{""timestamp"":""2018-09-24 14:23:42"",""python_version"":""3.5.4 (v3.5.4:3f56838, Aug  8 2017, 02:17:05) [MSC v.1900 64 bit (AMD64)]""}</p></li>
</ul>

<p>The source code of the function HttpTriggerPython32 is as follows:</p>

<pre><code>import os, sys, json
from datetime import datetime

postreqdata = json.loads(open(os.environ['req']).read())

responseData = {
    'timestamp' : datetime.now(pytz.timezone(""Europe/Zurich"")).strftime(""%Y-%m-%d %H:%M:%S""),
    'python_version' : sys.version
}

response = open(os.environ['res'], 'w')
response.write(str(responseData))
</code></pre>

<p>I've successfully added other 3rd party HTTP services as datasets in the data factory. Also, I've managed to call the function by using a Web activity.
The error only occurs when I try to use the function as a dataset source.</p>

<p>To add the function as data factory dataset I created a new dataset with the following parameters:</p>

<pre><code>{
    ""name"": ""HttpFile1"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AF_srfscadapa_af1"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""HttpFile"",
        ""typeProperties"": {
            ""format"": {
                ""type"": ""JsonFormat"",
                ""filePattern"": ""setOfObjects""
            },
            ""relativeUrl"": ""HttpTriggerPython32?code=L5WVNJh8loDv3mZFcD/AKatNRoYfYoHlDbEBk4AEGrbDA39ddAXsyg=="",
            ""requestMethod"": ""Post"",
            ""requestBody"": ""{\n    \""group_name\"": \""Azure POC\""\n}""
        }
    }
}
</code></pre>

<p>The linked service ""AF_srfscadapa_af1"" is configured as follows:</p>

<ul>
<li>Connect via integration runtime: AutoResolveIntegrationRuntime</li>
<li>Base URL: <a href=""https://srf-scadapa-fa2-windows.azurewebsites.net/api/"" rel=""nofollow noreferrer"">https://srf-scadapa-fa2-windows.azurewebsites.net/api/</a></li>
<li>Server Certificate Validation: False</li>
<li>Authentification type: Anonymous</li>
</ul>
","<python><azure><azure-functions><azure-data-factory>","2018-09-24 12:53:52","725","0","2","52487213","<p>Manually defining all the columns of the source and increasing the timeout in the copy activity solved the problem.</p>

<p>I was using the Data Factory GUI where it's not possible to set a timeout for previewing a source or importing a schema. So in this particular case the ""Import schema"" function simply does not work.</p>
"
"52479935","How to use an Azure Function (Python) as a HTTP dataset in an Azure Data Factory","<p>I keep getting connection timeouts when I try to create a dataset (HTTP) in  an Azure Data Factory which is based on a Azure Function (HTTP trigger):</p>

<pre><code>  The cloud service request timed out. Please retry. Activity ID:9d70efcd-c842-4484-9313-4872208a2a9e
</code></pre>

<p>However, if I call function from anywhere else e.g. from <a href=""https://apitester.com/shared/checks/d6108814e83149b4baee7f2b60786c82"" rel=""nofollow noreferrer"">apitester.com</a>, I get the desired response:</p>

<ul>
<li>Request URL: <a href=""https://srf-scadapa-fa2-windows.azurewebsites.net/api/HttpTriggerPython32?code=L5WVNJh8loDv3mZFcD/AKatNRoYfYoHlDbEBk4AEGrbDA39ddAXsyg=="" rel=""nofollow noreferrer"">https://srf-scadapa-fa2-windows.azurewebsites.net/api/HttpTriggerPython32?code=L5WVNJh8loDv3mZFcD/AKatNRoYfYoHlDbEBk4AEGrbDA39ddAXsyg==</a></li>
<li>Request method: post</li>
<li>Request body: { ""group_name"": ""Azure POC"" }</li>
<li><p>Response body:</p>

<p>{""timestamp"":""2018-09-24 14:23:42"",""python_version"":""3.5.4 (v3.5.4:3f56838, Aug  8 2017, 02:17:05) [MSC v.1900 64 bit (AMD64)]""}</p></li>
</ul>

<p>The source code of the function HttpTriggerPython32 is as follows:</p>

<pre><code>import os, sys, json
from datetime import datetime

postreqdata = json.loads(open(os.environ['req']).read())

responseData = {
    'timestamp' : datetime.now(pytz.timezone(""Europe/Zurich"")).strftime(""%Y-%m-%d %H:%M:%S""),
    'python_version' : sys.version
}

response = open(os.environ['res'], 'w')
response.write(str(responseData))
</code></pre>

<p>I've successfully added other 3rd party HTTP services as datasets in the data factory. Also, I've managed to call the function by using a Web activity.
The error only occurs when I try to use the function as a dataset source.</p>

<p>To add the function as data factory dataset I created a new dataset with the following parameters:</p>

<pre><code>{
    ""name"": ""HttpFile1"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AF_srfscadapa_af1"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""HttpFile"",
        ""typeProperties"": {
            ""format"": {
                ""type"": ""JsonFormat"",
                ""filePattern"": ""setOfObjects""
            },
            ""relativeUrl"": ""HttpTriggerPython32?code=L5WVNJh8loDv3mZFcD/AKatNRoYfYoHlDbEBk4AEGrbDA39ddAXsyg=="",
            ""requestMethod"": ""Post"",
            ""requestBody"": ""{\n    \""group_name\"": \""Azure POC\""\n}""
        }
    }
}
</code></pre>

<p>The linked service ""AF_srfscadapa_af1"" is configured as follows:</p>

<ul>
<li>Connect via integration runtime: AutoResolveIntegrationRuntime</li>
<li>Base URL: <a href=""https://srf-scadapa-fa2-windows.azurewebsites.net/api/"" rel=""nofollow noreferrer"">https://srf-scadapa-fa2-windows.azurewebsites.net/api/</a></li>
<li>Server Certificate Validation: False</li>
<li>Authentification type: Anonymous</li>
</ul>
","<python><azure><azure-functions><azure-data-factory>","2018-09-24 12:53:52","725","0","2","52487252","<p>Azure Functions (along with pretty much all serverless platforms) has to deal with the cold-start latency problem, where the first request to a function application after a period of no use takes longer. This is because the platform needs to instantiate an instance of your application before servicing the request. This can add a non-trivial amount of time to a request, which could have increased your latency over what is allowed by the default timeout for Azure Data Lake.</p>

<p>According to the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-http#copy-activity-properties"" rel=""nofollow noreferrer"">documentation for Azure Data Factory</a>, Copy Activities with HTTP as a source have a timeout field. Configuring this to a higher value may fix this issue.</p>
"
"52466612","Reading from specific files on U-SQL","<p>I am new to U-SQL and have started querying files. Based on instructions that I have seen on documentation and here on stack overflow, I have a written a query for extracting metadata from a set of json files as below.</p>

<pre><code>REFERENCE ASSEMBLY [Newtonsoft.Json];
REFERENCE ASSEMBLY [Microsoft.Analytics.Samples.Formats];

USING Microsoft.Analytics.Samples.Formats.Json;

DECLARE @InputFile string = ""/test/{*}.json"";
DECLARE @OutputFile string = ""/metadata.csv"";

@json =
EXTRACT
        id string,
        date DateTime,
        type string,
        uri = FILE.URI()
      , modified_date = FILE.MODIFIED()
      , created_date = FILE.CREATED()
      , file_sz = FILE.LENGTH()
FROM
    @InputFile
USING new JsonExtractor();

@json2 =
    SELECT 
    uri
    modified_date,
    created_date,
    file_sz
FROM @json;

@json3 =
    SELECT DISTINCT uri,
    modified_date,
    created_date,
    file_sz

    FROM @json2;
OUTPUT @json3
TO @OutputFile
USING Outputters.Csv(outputHeader:true,quoting:true);
DROP ASSEMBLY [Newtonsoft.Json];
DROP ASSEMBLY [Microsoft.Analytics.Samples.Formats];
</code></pre>

<p>This generates the required metadata( I run this on Azure portal even though here the locations are relative)</p>

<p>My questions are:</p>

<p>1) How can we use values from a column(a column of filenames) from an internal table / file as a list of file paths for input? </p>

<p>2) How can we append new data to an existing file and update the metadata file with list of new files.</p>

<p>My metadata looks like this:</p>

<pre><code>uri           created_date       modified_date   file_sz
/…/abc.json      09-22-2018        09-23-2018       250
/…/del.json      09-24-2018        09-24-2018       126
</code></pre>

<p>EXPECTED OUTPUT (if possible)</p>

<pre><code>@filespresent =
SELECT uri
FROM @metadata

DECLARE @Inputfile string = @filespresent
</code></pre>

<p>The main purpose of doing this is I get a new batch of files everyday and I want to read only the files that were uploaded today. </p>

<p>The filename do not contain date and the only way I can extract the date information is from inside the file. I extract metadata in a separate query and then run the main job on the files selected from the metadata file.</p>

<p>If there are other workaround to this, they are also most welcome.</p>

<p>Any help is appreciated.</p>
","<azure-data-factory><azure-data-lake><u-sql>","2018-09-23 13:32:59","245","1","2","52476803","<p>That is a limitation for the moment. You can vote <a href=""https://feedback.azure.com/forums/327234-data-lake/suggestions/31732294-allow-rowset-variables-to-be-converted-to-scalar-v"" rel=""nofollow noreferrer"">here</a> for that feature. Also stated here the same <a href=""https://stackoverflow.com/questions/40094944/convert-rowset-variables-to-scalar-value"">issue</a>.</p>

<p>The only way to get around that is to run the u-sql script with a powershell that injects an external variable like this:</p>

<pre><code>DECLARE EXTERNAL @InputFile string = ""/Input/sample{n}.json"";
</code></pre>

<p>If you try to run this you get an error of ""Rowset variable is not a scalar variable""...</p>

<pre><code>REFERENCE ASSEMBLY [Newtonsoft.Json];
REFERENCE ASSEMBLY [Microsoft.Analytics.Samples.Formats];

USING Microsoft.Analytics.Samples.Formats.Json;

DECLARE @InputFile string = ""/Input/sample4.json"";
DECLARE @OutputFile1 string = ""/Output/metadata1.csv"";
DECLARE @OutputFile2 string = ""/Output/metadata2.csv"";
DECLARE @OutputFile3 string = ""/Output/metadata3.csv"";

@json1 =
EXTRACT
        // Json fields
        id string,   
        file string,
        // Virtual columns
        n string,
        uri = FILE.URI()
      , modified_date = FILE.MODIFIED()
      , created_date = FILE.CREATED()
      , file_sz = FILE.LENGTH()
FROM
    @InputFile // This file has a file set within
USING new JsonExtractor();

OUTPUT @json1
TO @OutputFile1
USING Outputters.Csv(outputHeader:true,quoting:true);

@json2 =
    SELECT 
    file
FROM @json1;

OUTPUT @json2
TO @OutputFile2
USING Outputters.Csv(outputHeader:true,quoting:true);

@json3 =
EXTRACT
        // Json fields
        id string,   
        file string,
        // Virtual columns
        n string,
        uri = FILE.URI()
      , modified_date = FILE.MODIFIED()
      , created_date = FILE.CREATED()
      , file_sz = FILE.LENGTH()
FROM
    @json2 // This is a rowset variable (with our fileset)
USING new JsonExtractor();

OUTPUT @json3
TO @OutputFile3
USING Outputters.Csv(outputHeader:true,quoting:true);
</code></pre>
"
"52466612","Reading from specific files on U-SQL","<p>I am new to U-SQL and have started querying files. Based on instructions that I have seen on documentation and here on stack overflow, I have a written a query for extracting metadata from a set of json files as below.</p>

<pre><code>REFERENCE ASSEMBLY [Newtonsoft.Json];
REFERENCE ASSEMBLY [Microsoft.Analytics.Samples.Formats];

USING Microsoft.Analytics.Samples.Formats.Json;

DECLARE @InputFile string = ""/test/{*}.json"";
DECLARE @OutputFile string = ""/metadata.csv"";

@json =
EXTRACT
        id string,
        date DateTime,
        type string,
        uri = FILE.URI()
      , modified_date = FILE.MODIFIED()
      , created_date = FILE.CREATED()
      , file_sz = FILE.LENGTH()
FROM
    @InputFile
USING new JsonExtractor();

@json2 =
    SELECT 
    uri
    modified_date,
    created_date,
    file_sz
FROM @json;

@json3 =
    SELECT DISTINCT uri,
    modified_date,
    created_date,
    file_sz

    FROM @json2;
OUTPUT @json3
TO @OutputFile
USING Outputters.Csv(outputHeader:true,quoting:true);
DROP ASSEMBLY [Newtonsoft.Json];
DROP ASSEMBLY [Microsoft.Analytics.Samples.Formats];
</code></pre>

<p>This generates the required metadata( I run this on Azure portal even though here the locations are relative)</p>

<p>My questions are:</p>

<p>1) How can we use values from a column(a column of filenames) from an internal table / file as a list of file paths for input? </p>

<p>2) How can we append new data to an existing file and update the metadata file with list of new files.</p>

<p>My metadata looks like this:</p>

<pre><code>uri           created_date       modified_date   file_sz
/…/abc.json      09-22-2018        09-23-2018       250
/…/del.json      09-24-2018        09-24-2018       126
</code></pre>

<p>EXPECTED OUTPUT (if possible)</p>

<pre><code>@filespresent =
SELECT uri
FROM @metadata

DECLARE @Inputfile string = @filespresent
</code></pre>

<p>The main purpose of doing this is I get a new batch of files everyday and I want to read only the files that were uploaded today. </p>

<p>The filename do not contain date and the only way I can extract the date information is from inside the file. I extract metadata in a separate query and then run the main job on the files selected from the metadata file.</p>

<p>If there are other workaround to this, they are also most welcome.</p>

<p>Any help is appreciated.</p>
","<azure-data-factory><azure-data-lake><u-sql>","2018-09-23 13:32:59","245","1","2","52488160","<p>The features you would like (e.g., reading the paths from a file, and appending to an existing file) is not available.</p>

<p>Appending to a file you could do by reading the file, union the new data and then writing the result into the same file.</p>

<p>However, by looking at your scenario of **The main purpose of doing this is I get a new batch of files everyday and I want to read only the files that were uploaded today. **</p>

<p>You can do your <code>EXTRACT</code> as above and then put a filter onto the <code>created_date</code> or <code>modified_date</code> column to only select the files that are created or modified for the specific date. E.g. (typed into stackoverflow),</p>

<pre><code>DECLARE EXTERNAL @last_processed_modified_date = DateTime.Now();

@json = EXTRACT
    id string,
    date DateTime,
    type string,
    uri = FILE.URI()
  , modified_date = FILE.MODIFIED()
  , created_date = FILE.CREATED()
  , file_sz = FILE.LENGTH()
FROM @InputFile
USING new JsonExtractor();

@json = SELECT * FROM @json WHERE modified_date &gt; @last_processed_modified_date;

…
</code></pre>
"
"52463960","How to read only a few lines of data in Lookup activity in Azure data factory","<p>I am trying to read second row of my csv using Azure data factory Lookup activity. I will make a post request with that row as body using web activity, But if a file is large then i am getting a exception since lookup activity cannot load more data, is there a way to read specific number of lines in LookUp Activity.</p>
","<azure><azure-data-factory>","2018-09-23 07:23:37","742","2","1","52512779","<blockquote>
  <p>Is there a way to read specific number of lines in LookUp Activity?</p>
</blockquote>

<p>Based on my <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity#type-properties"" rel=""nofollow noreferrer"">research</a>, there is no such property for you to read specific number of lines in Lookup Activity.</p>

<p>So,maybe lookup activity is not a suitable option. I suggest you that create an <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook"" rel=""nofollow noreferrer"">Azure Function Http Trigger</a> to process your CSV file as you want with the C# package(or other language) and output the specific data. Then execute the Azure function in Azure data factory web activity and get the output data. Finally, go along with your custom subsequent activity.</p>
"
"52460137","Azure data factory pipleline folder property","<p>Azure data factory v2 allows to organize the pipelines using a folder structure in the UI. In the pipeline json there is additional property called ""folder"". I don't see such properly in the PipelineResource class. How de we programmatically set this property?</p>
","<azure-data-factory>","2018-09-22 19:27:34","322","0","2","52605795","<p>You do could find the <code>Folder</code> property in PipelineResource Class here:<a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models.pipelineresource?view=azure-dotnet"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models.pipelineresource?view=azure-dotnet</a>.</p>

<p>Please see this <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models.pipelineresource.folder?view=azure-dotnet#Microsoft_Azure_Management_DataFactory_Models_PipelineResource_Folder"" rel=""nofollow noreferrer"">doc</a> and <a href=""https://github.com/MicrosoftDocs/azure-docs/blob/f3f0681b1ff8f280bfbbb45316fd6ed3ffd9da9d/articles/data-factory/quickstart-create-data-factory-dot-net.md"" rel=""nofollow noreferrer"">adf .net source code</a>.</p>
"
"52460137","Azure data factory pipleline folder property","<p>Azure data factory v2 allows to organize the pipelines using a folder structure in the UI. In the pipeline json there is additional property called ""folder"". I don't see such properly in the PipelineResource class. How de we programmatically set this property?</p>
","<azure-data-factory>","2018-09-22 19:27:34","322","0","2","52618252","<p>If you didn’t find it, you need update the SDK version.</p>
"
"52460022","Edit Visual Studio .sln file","<p>Exporting a Data Factory project to Visual Studio forces a few things on developers that adds about an hour or so to one's development time per project.</p>

<p>The project IS going to be forced to the Users Documents Visual Studio Projects folder, there WILL be an extra path element the same name as the project and the .sln file WILL be where Microsoft wants it.</p>

<p>This wasn't avoidable and editing the .sln file in Visual Studio of course opens the solution ... always.</p>

<p>Here is how I had to handle Visual Studio 2015 both forcing a new directory for the solution and then making its own folder with the same name for the project.</p>

<p>It will be noted there is no way in Visual Studio to simply open the .sln file and edit it with the handy dandy context sensitive highlighting.  It always opens the solution.</p>

<p>So, I copied the .sln file and renamed the extension to .xml.  Then opened it in Visual Studio and edited it there.</p>

<p>This way I was able to remove everything from the default c:\ partition over to where I like to store info:
1. By client
2. By type of project
3. By folder named for the project
4. .sln and project folder in the same directory level</p>

<p>There may be a better way but at the time I didn't want to Google, I wanted to get my project set up and going.</p>

<p>I'm setting up configuration files for an Azure Data Factory and had downloaded the project from Azure.  By default, it sets up the project on the c:\ partition -- doesn't allow any deviation from this.</p>

<p>So ... copy everything from your VS(Version) Projects:</p>

<p><a href=""https://i.stack.imgur.com/Y8pzx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Y8pzx.png"" alt=""enter image description here""></a></p>

<p>Copy it to the new directory:</p>

<p><a href=""https://i.stack.imgur.com/QIASY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QIASY.png"" alt=""enter image description here""></a></p>

<p>Note, I've already removed the extra folder with the same project name.  So the path was D:\Clients\ISSI\Azure Projects\MergedDataFactory\MergedDataFactory.  The extra \MergedDataFactory in the path is removed and the contents pushed up to next higher directory level.</p>

<p>Now, do this. Copy the .sln, rename the extension to .xml.  For me, I then make a copy of the xml and edit that one.</p>

<p>This will open in visual studio and allow editing.</p>

<p><a href=""https://i.stack.imgur.com/TyQOD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TyQOD.png"" alt=""enter image description here""></a></p>

<p>What is handy is elements of the .sln are identified by GUIDs.  So the only items that need to change are:</p>

<ol>
<li>The path -- put in your desired path</li>
<li>Remove the extra project name in the path</li>
<li>Put the content where it needs to be</li>
</ol>

<p>Change this:</p>

<pre><code>C:\Users\Michael Durthaler\Documents\Visual Studio 2015\Projects\MergedDataFactory\MergedDataFactory\MergedDataFactory.dfproj
</code></pre>

<p>To this:</p>

<pre><code>D:\Clients\ISSI\Azure Projects\MergedDataFactory\MergedDataFactory.dfproj

&gt; Microsoft Visual Studio Solution File, Format Version 12.00
# Visual Studio 14
VisualStudioVersion = 14.0.25420.1
MinimumVisualStudioVersion = 10.0.40219.1
Project(""{3466B219-07DB-46AE-8697-CFD2A1650EC8}"") = ""MergedDataFactory"", ""C:\Users\Michael Durthaler\Documents\Visual Studio 2015\Projects\MergedDataFactory\MergedDataFactory\MergedDataFactory.dfproj"", ""{6F37DAB2-09D5-45FB-BBBE-EC6B98E429AC}""
EndProject
Global
GlobalSection(SolutionConfigurationPlatforms) = preSolution
Debug|x86 = Debug|x86
Release|x86 = Release|x86
EndGlobalSection
GlobalSection(ProjectConfigurationPlatforms) = postSolution
{6F37DAB2-09D5-45FB-BBBE-EC6B98E429AC}.Debug|x86.ActiveCfg = Debug|x86
{6F37DAB2-09D5-45FB-BBBE-EC6B98E429AC}.Debug|x86.Build.0 = Debug|x86
{6F37DAB2-09D5-45FB-BBBE-EC6B98E429AC}.Release|x86.ActiveCfg = Release|x86
{6F37DAB2-09D5-45FB-BBBE-EC6B98E429AC}.Release|x86.Build.0 = Release|x86
EndGlobalSection
GlobalSection(SolutionProperties) = preSolution
HideSolutionNode = FALSE
EndGlobalSection
EndGlobal
</code></pre>

<p>Next, I rename the copied XML file to .sln.  This way I've got the original .sln, the copied to .xml version and the new .sln file:</p>

<p><a href=""https://i.stack.imgur.com/iim9J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iim9J.png"" alt=""enter image description here""></a></p>

<p>Close out of Visual Studio, double click the new .sln file and see that everything opens as it should:</p>

<p><a href=""https://i.stack.imgur.com/f4u1j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f4u1j.png"" alt=""enter image description here""></a></p>

<p>Then just delete old files.</p>

<p>It would be nice if Microsoft would not default to the c:\ partition -- this will eventually bloat the c:\ partition because it is a fixed size.  And if they'd see it more practical for a developer to organize as they see fit, that would be great also.</p>
","<visual-studio><azure-data-factory>","2018-09-22 19:14:01","2380","0","1","52460024","<p>This is my work around for ADF exports being forced to a certain directory, not allowing the developer the freedom of choice of organizing their projects themselves.</p>
"
"52448483","Issue with CSV as a source in datafactory","<p>I have a CSV</p>

<pre><code>""Heading"",""Heading"",""Heading"",LF
""Data1"",""Data2"",""Data3"",LF
""Data4"",""Data5"",""Data6"",LF
</code></pre>

<p>And for the above CSV row limiter is LF </p>

<p>Issue is last comma. When I try to preview data after setting first column as heading and skip rows as 0 in source of copy activity in data factory, it throws error stating last column is null.</p>

<p>If I remove last comma.ie</p>

<pre><code>""Heading"",""Heading"",""Heading""LF
""Data1"",""Data2"",""Data3""LF
""Data4"",""Data5"",""Data6""LF
</code></pre>

<p>It will work fine. </p>

<p>It's not possible to edit CSV as each CSV may contain 500k records.</p>

<p>How to solve this?</p>

<hr>

<p>Addition details:</p>

<p>CSV i am uploading<a href=""https://i.stack.imgur.com/glVUg.jpg"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p>My azure portal setting
<a href=""https://i.stack.imgur.com/Tq6ld.jpg"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p>Error message on preview data
<a href=""https://i.stack.imgur.com/FNIU3.jpg"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p>if i remove the first row as header i could see an empty column
<a href=""https://i.stack.imgur.com/SOFrU.jpg"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<azure><azure-data-factory>","2018-09-21 17:24:39","5299","1","1","52491622","<p>Please try to set <code>Row delimiter</code> as <code>Line Feed(\n)</code>.</p>

<p><a href=""https://i.stack.imgur.com/pH8BF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pH8BF.png"" alt=""enter image description here""></a></p>

<p>I tested your sample csv file and it works fine.</p>

<p><a href=""https://i.stack.imgur.com/SbNFa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SbNFa.png"" alt=""enter image description here""></a></p>

<p><strong><em>output:</em></strong></p>

<p><a href=""https://i.stack.imgur.com/7IwcY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7IwcY.png"" alt=""enter image description here""></a></p>

<hr>

<p>I tried to create the same file with you and reproduce your issue.It seems the check mechanism of adf. You need to remove the first row as header selection to escape this check. If you do not want to do that, you have to preprocess your CSV files. </p>

<p>I suggest you below two workarounds.</p>

<p>1.Use Azure Function Http Trigger. You could pass the CSV file name as parameter into Azure Function.Then use Azure Blob Storage SDK to process your csv file to cut the last comma.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook</a></p>

<p>2.Use Azure Stream Analytics. You could configure your blob storage as input and create another container as output. Then use SQL query to process your CSV data.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal</a></p>
"
"52437047","ADFV 2 Spark Activity with Scala throwing error with error code 2312","<p>Using Azure Data Factory Version 2, we have created a Spark Activity ( a simple Hello World example ), but it throws Error with Error Code 2312 </p>

<p>Our configuration is Hdinsight cluster with Azure Data Lake as primary storage. 
We also tried spinning up an HDInsight cluster with Azure Blob Storage as primary storage and there as well we are facing same issue. </p>

<p>We further tried replacing Scala code with Python scrip ( simple hello world example ), But facing same issue. </p>

<p>Has anyone encountered this issue, are we missing any basic setting</p>

<p>Thanks in advance</p>
","<azure><azure-hdinsight><azure-data-factory>","2018-09-21 05:19:27","276","2","2","56034434","<p>May be its too late and you have already solved your issue . However , you can try below 
Use azure databricks . Create a new instance of databricks and run your sample hello world in notebook . if its works in notebook then call the same notebook in adf . 
hope it helps </p>
"
"52437047","ADFV 2 Spark Activity with Scala throwing error with error code 2312","<p>Using Azure Data Factory Version 2, we have created a Spark Activity ( a simple Hello World example ), but it throws Error with Error Code 2312 </p>

<p>Our configuration is Hdinsight cluster with Azure Data Lake as primary storage. 
We also tried spinning up an HDInsight cluster with Azure Blob Storage as primary storage and there as well we are facing same issue. </p>

<p>We further tried replacing Scala code with Python scrip ( simple hello world example ), But facing same issue. </p>

<p>Has anyone encountered this issue, are we missing any basic setting</p>

<p>Thanks in advance</p>
","<azure><azure-hdinsight><azure-data-factory>","2018-09-21 05:19:27","276","2","2","62507143","<p>@Yogesh, have you tried debugging the issue through ADF by opting Debug as the screenshot? That might help you get the exact root cause. I would suggest trying using the spark-submit with the jar in the Linux box to find out the exact cause.
Also, you can find more info on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-factory-troubleshoot-guide#error-code-2312"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-factory-troubleshoot-guide#error-code-2312</a>
<a href=""https://i.stack.imgur.com/jkQnY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jkQnY.png"" alt=""enter image description here"" /></a></p>
"
"52434622","Dynamically rename/map columns according to Azure Table data model convention","<p>How would you dynamically rename/map columns according to Azure Table data model convention, property key name should follow C# identifiers. Since we cannot guarantee the columns coming to us conform to the standard, or when we get new columns in, that it is automatically fixed.</p>

<p>Example:</p>

<pre><code>column_1 (something_in_parens), column with spaces, ...
returned...
column_1 something_in_parens, column_with_spaces, ...
</code></pre>

<p>The obvious solution might be to run a databricks python step in front of the Copy Data functionality, but maybe Copy Data is able to inflect the right schema?</p>
","<azure><azure-table-storage><azure-data-factory>","2018-09-20 23:28:14","418","1","1","56996509","<pre class=""lang-py prettyprint-override""><code>columns = [""some Not so nice column Names"", ""Another ONE"", ""Last_one""]
​
new_columns = [x.lower().replace("" "", ""_"") for x in columns]

# returns ['some_not_so_nice_column_names', 'another_one', 'last_one']
</code></pre>
"
"52421118","How can I read a secure string within a custom activity in Data Factory v2?","<p>I currently have a DFv2 pipeline which uses a custom activity and I am trying to figure out how to read a secure string passed to the custom activity.</p>

<p>The reason I want to do this is so that my custom activity may add this secure string as part of a request to an external API.</p>

<p>Here is a simplified version of my custom activity :</p>

<p><code>
{
    ""name"": ""CustomActivity"",
    ""type"": ""Custom"",
    ""policy"": {
        ""timeout"": ""7.00:00:00"",
        ""retry"": 0,
        ""retryIntervalInSeconds"": 30,
        ""secureOutput"": false,
        ""secureInput"": false
    },
    ""typeProperties"": {
        ""command"": ""CustomCode.exe"",
        ""resourceLinkedService"": {
            ""referenceName"": ""AzureStorageLinkedService"",
            ""type"": ""LinkedServiceReference""
        },
        ""folderPath"": ""foldercontainingcustomcode"",
        ""extendedProperties"": {
            ""url"": ""sampleUrl"",
            ""apiKey"": {
                ""type"": ""SecureString"",
                ""value"": ""**********""
            }
        },
        ""referenceObjects"": {
            ""linkedServices"": [
                {
                    ""referenceName"": ""AzureStorageLinkedService"",
                    ""type"": ""LinkedServiceReference""
                }
            ],
            ""datasets"": [
            ]
        }
    },
    ""linkedServiceName"": {
        ""referenceName"": ""AzureBatchLinkedService"",
        ""type"": ""LinkedServiceReference""
    }
}
</code></p>

<p>Here is some sample custom code :</p>

<pre class=""lang-cs prettyprint-override""><code>class Program
{
    static void Main(string[] args)
    {
        dynamic activity = JsonConvert.DeserializeObject(File.ReadAllText(""activity.json""));
        dynamic apiKey = activity.typeProperties.extendedProperties.apiKey.value;
    }
}
</code></pre>

<p>My question is : How can I achieve this since reading apiKey directly will just read the string '*****' ? </p>
","<azure><azure-data-factory>","2018-09-20 08:51:08","504","0","1","52437538","<p>for security reason, data factory will never return the secure string in any case.</p>

<p>if you want to use the key in your custom exe, you can keep it as encrypted plain string in the JSON, and use your cert or symmetric key to decrypt it in your code. or you can reference a Azure Key Vault so that you can access it in your code too.</p>
"
"52419942","Run DataLakeAnalyticsU-SQL script in Data Factory Pipeline","<p>I Want To create a pipeline that contains a U-SQL script which combines multiple log files in Azure DataLake Store into one file, I tried to do that by creating a text file in my store and then add it in the pipeline <strong>scriptpath</strong> but I get an error and I searched for that and I found that the ADL isn't supported in the pipeline so I want to write the U-SQL script in the pipeline using <strong>script</strong> property. I tried to do that using this script but I get an error and I can't deploy it so anyone can help to do that?
Here's My Pipeline script:</p>

<pre><code>    {
        ""name"": ""RG-GatherData"",
        ""properties"": {
            ""description"": ""description"",
            ""activities"": [
                {
                    ""name"": ""DataLakeAnalyticsUSqlActivityTemplate"",
                    ""type"": ""DataLakeAnalyticsU-SQL"",
                    ""linkedServiceName"": ""AzureDataLakeAnalyticsLinkedService"",

                    ""typeProperties"": {
                        ""script"": ""
    @log = EXTRACT [""VersionID""] int,
               [""NodeName""] string,
               [""UpdateIng Area""] string,
               [""ActionDate""] string,
               [""UserName""] string,
               [""Code part Type""]  string,
               [""DocCode""] string,
               [""Header Entity Id""] string,
               [""Common Entity Id""] string,
               [""Attribute Name""] string,
               [""Latest Update Value""] string,
               [""Previous Update Value""] string  
          FROM @in
          USING Extractors.Csv(skipFirstNRows: 1);

OUTPUT @log
TO @out
USING Outputters.Csv();

                        ,
                        ""degreeOfParallelism"": 3,
                        ""priority"": 100,
                        ""parameters"": {
                            ""in"": ""/RowLogs/InPut/RoyalGardens/{*}.csv"",
                            ""out"": ""/RowLogs/OutPut/RoyalGardens/Alllog.csv""
                        }
                    },
                    ""policy"": {
                        ""concurrency"": 1,
                        ""executionPriorityOrder"": ""OldestFirst"",
                        ""retry"": 3,
                        ""timeout"": ""10:00:00""
                    },
                    ""scheduler"": {
                        ""frequency"": ""Day"",
                        ""interval"": 1
                    }
                }
            ],
            ""start"": ""2018-09-20T00:06:00Z"",
            ""end"": ""2099-12-30T22:00:00Z""
        }
    }
</code></pre>
","<azure><azure-data-factory><azure-data-lake><u-sql>","2018-09-20 07:36:40","67","0","1","52558968","<p>Store the U-SQL script in Blob Storage and reference it via a Blob Storage linked service.</p>
"
"52411383","How to create a column of row id in Spark dataframe for each distinct column value using Scala","<p>I have a data frame in scala spark as</p>

<p>category | score | </p>

<p>A   |       0.2</p>

<p>A   |       0.3</p>

<p>A   |       0.3</p>

<p>B    |      0.9</p>

<p>B    |      0.8</p>

<p>B     |     1</p>

<p>I would like to 
add a row id column as</p>

<p>category | score | row-id</p>

<p>A      |    0.2  |    0</p>

<p>A      |    0.3    |  1</p>

<p>A      |    0.3     | 2 </p>

<p>B       |   0.9     | 0</p>

<p>B     |     0.8   |   1 </p>

<p>B      |    1      |  2</p>

<p>Basically I want the row id to be monotonically increasing for each distinct value in column category. I already have a sorted dataframe so all the rows with same category are grouped together. However, I still don't know how to generate the row_id that restarts when a new category appears.  Please help!</p>
","<scala><apache-spark><azure-data-factory>","2018-09-19 17:34:00","968","1","1","52411451","<p>This is a good use case for <a href=""https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-functions-windows.html"" rel=""nofollow noreferrer""><code>Window</code></a> aggregation functions</p>

<pre><code>import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.row_number
import df.sparkSession.implicits._

val window = Window.partitionBy('category).orderBy('score)
df.withColumn(""row-id"", row_number.over(window))
</code></pre>

<p>Window functions work kind of like <code>groupBy</code> except that instead of each <em>group</em> returning a single value, each <em>row</em> in each group returns a single value. In this case the value is the row's position <em>within</em> the group of rows of the same category. Also, if this is the effect that you are trying to achieve, then you don't need to have pre-sorted the column <code>category</code> beforehand.</p>
"
"52388310","How to pass Azure Data Factory Linked Service connection string as plain text?","<p>Using Azure Data Factory, I am trying to create a linked service, connecting to a local sql server through a self-hosted Integration Runtime. I am using the Python Azure SDK.</p>

<p>Creation of the service succeeds, but I am not able to get the connection string right in my script. It's supposed to be the server name and database name in plain text, however, the <a href=""https://learn.microsoft.com/da-dk/python/api/azure-mgmt-datafactory/azure.mgmt.datafactory.models.SqlServerLinkedService?view=azure-python"" rel=""nofollow noreferrer"">sdk</a> only accepts the variable as a SecureString.</p>

<p>Here is some sample code:</p>

<pre><code>from azure.mgmt.datafactory.models import (SecureString, SqlServerLinkedService)
from pprint import pprint
from azure_setup_automation.utils import print_item, pdir
from utils.config import get_config_sql, get_config_azure
from azure_setup_automation.utils import (get_azure_credentials)
from azure.mgmt.datafactory import DataFactoryManagementClient

conf = get_config_azure()
local_conf = get_config_sql()
credentials = get_azure_credentials(conf)
adf_client = DataFactoryManagementClient(credentials, conf.subscription_id)

connection_string_source = (
    f'Data Source={local_conf.server};'
    f'Initial Catalogue={local_conf.database};'
    'Integrated Security=False;'
)

ls_name_local_sql = '02_test_linked_service_local_sql'

ls_local_storage = SqlServerLinkedService(
    connection_string=SecureString(connection_string_source),
    connect_via={""referenceName"": ""IRTest1"", ""type"": ""IntegrationRuntimeReference""},
    user_name=local_conf.user_id,
    password=SecureString(local_conf.password)
)

ls_obj_local_sql = adf_client.linked_services.create_or_update(
    conf.resource_group_name,
    conf.azure_data_factory_name,
    ls_name_local_sql,
    properties = ls_local_storage
)

#print_item(ls_obj_local_sql)
#pdir(ls_obj_local_sql)
#print('-------------------------')
#print (ls_obj_local_sql.type)
#print (ls_obj_local_sql.serialize())

pprint (ls_obj_local_sql.as_dict())
</code></pre>

<p>This prints the following.</p>

<pre><code>{'etag': 'XXXX-0000-0000-0000-5ba0fcb10000',
 'id': '/subscriptions/XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXX/resourceGroups/XXX/providers/Microsoft.DataFactory/factories/XXXX/linkedservices/02_test_linked_service_local_sql',
 'name': '02_test_linked_service_local_sql',
 'properties': {'connect_via': {'reference_name': 'IRTest1',
                                'type': 'IntegrationRuntimeReference'},
                'connection_string': {'type': 'SecureString',
                                      'value': '**********'},
                'encrypted_credential': 'XXXXXXXXXXXXXXXXXXXXXXX',
                'type': 'SqlServer',
                'user_name': 'XXXXX\\XXXXX'},
 'type': 'Microsoft.DataFactory/factories/linkedservices'}
</code></pre>

<p>Here is a picture off of the ADF portal. Illustrating how the parameters show up asterisks, and how the connection fails: <a href=""https://i.stack.imgur.com/GaNWl.png"" rel=""nofollow noreferrer"">Link to image</a></p>

<p>Is there any way to pass these parameters in plain text without changing them manually?</p>
","<azure-data-factory>","2018-09-18 13:59:06","1354","2","1","52406598","<p>It was all thanks to a typo. Apparently the Americans have a different way of spelling for Catalogue:</p>

<pre><code>f'Initial Catalog={local_conf.database};'
</code></pre>

<p>In the end, there is no need for the connection string to be in plain text, even if that's the way the portal interface presents it. I misunderstood what was going on, thinking that the error was about the sdk not being able to parse the SecureString.</p>
"
"52386193","Azure Data Factory processed row count","<p>We have ForEach iteration and within that we have Execute Pipeline activity. This activity call child pipeline which has copy activity. 
<a href=""https://i.stack.imgur.com/X8blC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X8blC.png"" alt=""enter image description here""></a> </p>

<p>We have ID associated with every iteration and unique SELECT query to copy data. But we also want to update processed row count after each successful Execute Pipeline activity. Stored Proc activity (or any other way) should get processed row count and update into database. This is very easy in SSIS but can't figure out way to do this in Azure Data Factory.</p>

<p>I have referred <a href=""https://stackoverflow.com/questions/51553141/azure-data-factory-check-rowcount-of-copied-records"">this</a> SO post but this doesn't fit into our requirement. We don't want to change anything in child pipeline. </p>
","<sql-server><azure><azure-data-factory>","2018-09-18 12:03:31","2572","0","1","52407663","<p>Currently, execute pipeline activity only output the pipeline run id and pipeline name of the child pipeline. </p>

<p>The easiest way would be to move the stored procedure activity into your child pipeline. If you don't want to change the child pipeline, maybe you could write <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">custom activity</a> to get the pipeline run details based on the pipeline run id. </p>
"
"52382706","ADF V2 - Parameterize a data copy pipeline based on a table column","<p>with Azure Data Factory V2, through the portal</p>
<blockquote>
<p><a href=""https://adf.azure.com"" rel=""nofollow noreferrer"">https://adf.azure.com</a></p>
</blockquote>
<p>I created a Pipeline for incremental copying of data from multiple tables, from one Azure SQL database to another Azure SQL database.</p>
<p>To create it, I have adapted the following example to my needs:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-multiple-tables-portal"" rel=""nofollow noreferrer"">Incrementally load data from multiple tables</a></p>
<p>Following is the json file related to the pipeline created:</p>
<pre><code>{
&quot;name&quot;: &quot;IncrementalCopyPipeline&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;IterateSQLTables&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@pipeline().parameters.tableList&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;LookupOldWaterMarkActivity&quot;,
                        &quot;type&quot;: &quot;Lookup&quot;,
                        &quot;policy&quot;: {
                            &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                            &quot;retry&quot;: 0,
                            &quot;retryIntervalInSeconds&quot;: 30,
                            &quot;secureOutput&quot;: false,
                            &quot;secureInput&quot;: false
                        },
                        &quot;typeProperties&quot;: {
                            &quot;source&quot;: {
                                &quot;type&quot;: &quot;SqlSource&quot;,
                                &quot;sqlReaderQuery&quot;: {
                                    &quot;value&quot;: &quot;select * \nfrom watermarktable \nwhere TableName  =  '@{item().TABLE_NAME}'&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            },
                            &quot;dataset&quot;: {
                                &quot;referenceName&quot;: &quot;WatermarkDataset&quot;,
                                &quot;type&quot;: &quot;DatasetReference&quot;
                            }
                        }
                    },
                    {
                        &quot;name&quot;: &quot;LookupNewWaterMarkActivity&quot;,
                        &quot;type&quot;: &quot;Lookup&quot;,
                        &quot;policy&quot;: {
                            &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                            &quot;retry&quot;: 0,
                            &quot;retryIntervalInSeconds&quot;: 30,
                            &quot;secureOutput&quot;: false,
                            &quot;secureInput&quot;: false
                        },
                        &quot;typeProperties&quot;: {
                            &quot;source&quot;: {
                                &quot;type&quot;: &quot;SqlSource&quot;,
                                &quot;sqlReaderQuery&quot;: {
                                    &quot;value&quot;: &quot;select MAX(@{item().WaterMark_Column}) as NewWatermarkvalue \nfrom @{item().TABLE_NAME}&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            },
                            &quot;dataset&quot;: {
                                &quot;referenceName&quot;: &quot;SourceDataset&quot;,
                                &quot;type&quot;: &quot;DatasetReference&quot;
                            }
                        }
                    },
                    {
                        &quot;name&quot;: &quot;IncrementalCopyActivity&quot;,
                        &quot;type&quot;: &quot;Copy&quot;,
                        &quot;dependsOn&quot;: [
                            {
                                &quot;activity&quot;: &quot;LookupNewWaterMarkActivity&quot;,
                                &quot;dependencyConditions&quot;: [
                                    &quot;Succeeded&quot;
                                ]
                            },
                            {
                                &quot;activity&quot;: &quot;LookupOldWaterMarkActivity&quot;,
                                &quot;dependencyConditions&quot;: [
                                    &quot;Succeeded&quot;
                                ]
                            }
                        ],
                        &quot;policy&quot;: {
                            &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                            &quot;retry&quot;: 0,
                            &quot;retryIntervalInSeconds&quot;: 30,
                            &quot;secureOutput&quot;: false,
                            &quot;secureInput&quot;: false
                        },
                        &quot;typeProperties&quot;: {
                            &quot;source&quot;: {
                                &quot;type&quot;: &quot;SqlSource&quot;,
                                &quot;sqlReaderQuery&quot;: {
                                    &quot;value&quot;: &quot;select * from @{item().TABLE_NAME} \nwhere @{item().WaterMark_Column} &gt; '@{activity('LookupOldWaterMarkActivity').output.firstRow.WatermarkValue}' and @{item().WaterMark_Column} &lt;= '@{activity('LookupNewWaterMarkActivity').output.firstRow.NewWatermarkvalue}'&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            },
                            &quot;sink&quot;: {
                                &quot;type&quot;: &quot;SqlSink&quot;,
                                &quot;writeBatchSize&quot;: 10000,
                                &quot;sqlWriterStoredProcedureName&quot;: {
                                    &quot;value&quot;: &quot;@{item().StoredProcedureNameForMergeOperation}&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                },
                                &quot;sqlWriterTableType&quot;: {
                                    &quot;value&quot;: &quot;@{item().TableType}&quot;,
                                    &quot;type&quot;: &quot;Expression&quot;
                                }
                            },
                            &quot;enableStaging&quot;: false,
                            &quot;dataIntegrationUnits&quot;: 0
                        },
                        &quot;inputs&quot;: [
                            {
                                &quot;referenceName&quot;: &quot;SourceDataset&quot;,
                                &quot;type&quot;: &quot;DatasetReference&quot;
                            }
                        ],
                        &quot;outputs&quot;: [
                            {
                                &quot;referenceName&quot;: &quot;SinkDataset&quot;,
                                &quot;type&quot;: &quot;DatasetReference&quot;,
                                &quot;parameters&quot;: {
                                    &quot;SinkTableName&quot;: &quot;@{item().TABLE_NAME}&quot;
                                }
                            }
                        ]
                    },
                    {
                        &quot;name&quot;: &quot;StoredProceduretoWriteWatermarkActivity&quot;,
                        &quot;type&quot;: &quot;SqlServerStoredProcedure&quot;,
                        &quot;dependsOn&quot;: [
                            {
                                &quot;activity&quot;: &quot;IncrementalCopyActivity&quot;,
                                &quot;dependencyConditions&quot;: [
                                    &quot;Succeeded&quot;
                                ]
                            }
                        ],
                        &quot;policy&quot;: {
                            &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                            &quot;retry&quot;: 0,
                            &quot;retryIntervalInSeconds&quot;: 30,
                            &quot;secureOutput&quot;: false,
                            &quot;secureInput&quot;: false
                        },
                        &quot;typeProperties&quot;: {
                            &quot;storedProcedureName&quot;: &quot;[dbo].[sp_write_watermark]&quot;,
                            &quot;storedProcedureParameters&quot;: {
                                &quot;LastModifiedtime&quot;: {
                                    &quot;value&quot;: {
                                        &quot;value&quot;: &quot;@{activity('LookupNewWaterMarkActivity').output.firstRow.NewWatermarkvalue}&quot;,
                                        &quot;type&quot;: &quot;Expression&quot;
                                    },
                                    &quot;type&quot;: &quot;DateTime&quot;
                                },
                                &quot;TableName&quot;: {
                                    &quot;value&quot;: {
                                        &quot;value&quot;: &quot;@{activity('LookupOldWaterMarkActivity').output.firstRow.TableName}&quot;,
                                        &quot;type&quot;: &quot;Expression&quot;
                                    },
                                    &quot;type&quot;: &quot;String&quot;
                                }
                            }
                        },
                        &quot;linkedServiceName&quot;: {
                            &quot;referenceName&quot;: &quot;SqlServerLinkedService_dest&quot;,
                            &quot;type&quot;: &quot;LinkedServiceReference&quot;
                        }
                    }
                ]
            }
        }
    ],
    &quot;parameters&quot;: {
        &quot;tableList&quot;: {
            &quot;type&quot;: &quot;Object&quot;,
            &quot;defaultValue&quot;: [
                {
                    &quot;TABLE_NAME&quot;: &quot;customer_table&quot;,
                    &quot;WaterMark_Column&quot;: &quot;LastModifytime&quot;,
                    &quot;TableType&quot;: &quot;DataTypeforCustomerTable&quot;,
                    &quot;StoredProcedureNameForMergeOperation&quot;: &quot;sp_upsert_customer_table&quot;
                },
                {
                    &quot;TABLE_NAME&quot;: &quot;project_table&quot;,
                    &quot;WaterMark_Column&quot;: &quot;Creationtime&quot;,
                    &quot;TableType&quot;: &quot;DataTypeforProjectTable&quot;,
                    &quot;StoredProcedureNameForMergeOperation&quot;: &quot;sp_upsert_project_table&quot;
                }
            ]
        }
    }
}
}
</code></pre>
<p>In my table I have a column that distinguishes between different companies and so I would like to add another parameter to this pipeline. I have a table like this:</p>
<pre><code>NAME    LASTMODIFY                 COMPANY
John    2015-01-01 00:00:00.000    1
Mike    2016-02-02 01:23:00.000    2
Andy    2017-03-04 05:16:00.000    3
Annie   2018-09-08 00:00:00.000    1
</code></pre>
<p>Someone would know how to insert a parameter into the pipeline in order to specify which company to copy and which one to not copy?</p>
<p>Does any suggestion? Thanks in advance to everyone!</p>
","<azure><azure-sql-database><azure-data-factory>","2018-09-18 08:56:50","1225","1","1","54031132","<p>Not exactly clear on what you're asking, so apologies if I am missing the mark, but:</p>

<p>Copy allows for a stored procedure that you can use to potentially solve your problem. Take a look at this example: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#invoking-stored-procedure-for-sql-sink"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#invoking-stored-procedure-for-sql-sink</a></p>

<p>It uses a Stored Procedure to MERGE performing an UPDATE or INSERT dependent on the JOIN matching. It also allows for parameters to be passed.</p>

<p>So if you are trying to COPY only certain cases based on a parameter, the MERGE join may help.</p>
"
"52335266","Azure Data Factory V2: How to pass a file name to stored procedure variable","<p>I have a big fact Azure SQL table with the following structure:</p>

<pre><code>Company     Revenue
-------------------
A              100
B              200
C              100
.               .
.               .
.               .
</code></pre>

<p>I am now building a stored procedure on Azure Data Factory V2 that will delete all records of a special company from the Azure SQL fact table above in a monthly basis. For this exercise this special company shall be identified by the variable @company. The structure of the stored procedure was created as: </p>

<pre><code>@company NVARCHAR(5)

DELETE FROM table 
WHERE [company] = @company
</code></pre>

<p>As I will have different Excel files from each company that will be inserting data into this table in a monthly basis (with Copy Activity), I want to use the stored procedure above to delete the old data from that company before I add the most updated one.</p>

<p>I would like then to pass to the variable ""@company"" the name of that Excel file (stored in a blob container) so that the stored procedure knows what is the relevant data to be deleted from the fact table. For example: If the Excel file is ""A"", the stored procedure shall be ""delete from table where company = A"". </p>

<p>Any ideas on how to pass the Excel file names to the variable ""@company"" and set this up on Azure Data Factory V2?</p>
","<sql><azure><parameters><azure-data-factory>","2018-09-14 15:41:43","2654","2","2","52380009","<blockquote>
<p>Any ideas on how to pass the Excel file names to the variable
&quot;@company&quot; and set this up on Azure Data Factory V2?</p>
</blockquote>
<p>Based on your description, I found <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#event-based-trigger"" rel=""nofollow noreferrer"">Event-based trigger</a> in azure data factory maybe will meet your needs. Event-based trigger runs pipelines in response to an event, such as the arrival of a file, or the deletion of a file, in Azure Blob Storage.</p>
<p>So, when the new excel file created in the blob storage (BTW, it only supports V2 storage account,more details please refer to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger#data-factory-ui"" rel=""nofollow noreferrer"">article</a>), you could get the <code>@triggerBody().folderPath</code> and <code>@triggerBody().fileName</code>. To use the values of these properties in a pipeline, you must map the properties to pipeline parameters. After mapping the properties to parameters, you can access the values captured by the trigger through the <code>@pipeline.parameters.parameterName</code> expression throughout the pipeline. (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger#map-trigger-properties-to-pipeline-parameters"" rel=""nofollow noreferrer"">doc</a>)</p>
<p>You could get the filename and pass it into your stored procedure. Then do the deletion and copy activities.</p>
"
"52335266","Azure Data Factory V2: How to pass a file name to stored procedure variable","<p>I have a big fact Azure SQL table with the following structure:</p>

<pre><code>Company     Revenue
-------------------
A              100
B              200
C              100
.               .
.               .
.               .
</code></pre>

<p>I am now building a stored procedure on Azure Data Factory V2 that will delete all records of a special company from the Azure SQL fact table above in a monthly basis. For this exercise this special company shall be identified by the variable @company. The structure of the stored procedure was created as: </p>

<pre><code>@company NVARCHAR(5)

DELETE FROM table 
WHERE [company] = @company
</code></pre>

<p>As I will have different Excel files from each company that will be inserting data into this table in a monthly basis (with Copy Activity), I want to use the stored procedure above to delete the old data from that company before I add the most updated one.</p>

<p>I would like then to pass to the variable ""@company"" the name of that Excel file (stored in a blob container) so that the stored procedure knows what is the relevant data to be deleted from the fact table. For example: If the Excel file is ""A"", the stored procedure shall be ""delete from table where company = A"". </p>

<p>Any ideas on how to pass the Excel file names to the variable ""@company"" and set this up on Azure Data Factory V2?</p>
","<sql><azure><parameters><azure-data-factory>","2018-09-14 15:41:43","2654","2","2","68771568","<p>I you use the stored procedure activity you can fill the parameters from pipeline parameters: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure</a></p>
<p><a href=""https://i.stack.imgur.com/6XYwV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6XYwV.png"" alt=""ADF stored procedures"" /></a></p>
<p>You can click on &quot;Import&quot; to automatically add the parameters after selecting a stored procedure or add them by hand. Then you can use a pipeline expression to dynamically fill them, e.g. as suggested by Jay Gong using the trigger parameters  <code>@triggerBody().folderPath</code> and <code>@triggerBody().fileName</code>.</p>
<p>Alternatively, instead of using a stored procedure, you could add a pre-copy script to your copy activity:</p>
<p><a href=""https://i.stack.imgur.com/dXIuG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dXIuG.png"" alt=""Pre-copy script"" /></a></p>
<p>This shows up only for appropriate sinks, such as a database table. You can fill this script dynamically as well. In your case, this could look like:</p>
<pre><code>@{concat('DELETE FROM table 
WHERE [company] = ''',triggerBody().fileName,'''')}
</code></pre>
<p>It might also make sense, to add a parameter to the pipeline containing the filename, and setting it to <code>@triggerBody().fileName</code> or any more complex expression, in case you are using it multiple times.</p>
"
"52334901","Azure Data Factory Copy Data dynamically get last blob","<p>I have an Azure Data Factory Pipeline that runs on a Blob Created Trigger, I want it to grab the last Blob added and copy that to the desired location.</p>

<p>How do I dynamically generate the file path for this outcome?</p>

<ul>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables"" rel=""nofollow noreferrer"">System Variables</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">Expressions and Functions</a>
<a href=""https://i.stack.imgur.com/NqHFh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NqHFh.png"" alt=""enter image description here""></a></li>
</ul>
","<azure><azure-data-factory>","2018-09-14 15:18:36","1955","1","1","52340763","<p>""<strong>@triggerBody().folderPath</strong>"" and ""<strong>@triggerBody().fileName</strong>"" captures the last created blob file path in event trigger. You need to map your pipeline parameter to these two trigger properties. Please follow this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger#map-trigger-properties-to-pipeline-parameters"" rel=""nofollow noreferrer"">link</a> to do the parameter passing and reference. Thanks.</p>
"
"52331139","What is the ADF v2 equivalent of WindowStart?","<p>We are migrating from ADF V1 to ADF V2. Our pipelines are scheduled to run daily and we have that part of V2 working. A pipeline will start at the appointed time. </p>

<p>My question is how do I pass the time that the scheduled trigger is scheduled to start into the where clause of a select statement in a copy activity? I suspect I've overlooked or misread some documentation, but I just can't figure it out. </p>

<p>No matter what I try, the variable does not seem to evaluate at runtime. So, the where clause looks something like this when it runs:
where last_update>=’@pipeline().TriggerTime’
And obviously @pipeline().TriggerTime is not a date.</p>

<p>I’ve pulled this list of parameters to try from <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables</a></p>

<p>This is what worked in V1:
""sqlReaderQuery"": ""$$Text.Format('select columns from table where last_update>=\'{0:yyyy-MM-dd}\'', WindowStart, WindowEnd)""</p>

<p>Things I've tried: </p>

<p>-- Passing @trigger().scheduledTime in as a parameter in the trigger to @{formatDateTime(pipeline().parameters.startDate,'yyyy-MM-dd')} in the where clause with a parameter in the pipeline defined as startDate of type string.</p>

<p>-- Setting @pipeline().TriggerTime as the default value for the pipeline().parameters.startDate.</p>

<p>-- Calling @pipeline().TriggerTime in the where clause.</p>

<p>Many thanks in advance.</p>

<hr>

<p>EDIT:</p>

<h2>Pipeline Source</h2>

<pre><code>{
    ""name"": ""PL_ADLS_RAW_IDMTables_DAILY"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""isStartDateNotNull"",
                ""description"": ""If the startDate parameter == 0 then run the full load. If the startDate parameter != 0 then run for &gt;= startDate parameter."",
                ""type"": ""IfCondition"",
                ""typeProperties"": {
                    ""expression"": {
                        ""value"": ""@equals(pipeline().parameters.startDate,'0')"",
                        ""type"": ""Expression""
                    },
                    ""ifFalseActivities"": [
                        {
                            ""name"": ""copy_ENTITY_FULL"",
                            ""description"": ""Copies ENTITY from IDM Database to ADLS Raw Zone"",
                            ""type"": ""Copy"",
                            ""policy"": {
                                ""timeout"": ""7.00:00:00"",
                                ""retry"": 0,
                                ""retryIntervalInSeconds"": 60,
                                ""secureOutput"": false,
                                ""secureInput"": false
                            },
                            ""typeProperties"": {
                                ""source"": {
                                    ""type"": ""SqlSource"",
                                    ""sqlReaderQuery"": {
                                        ""value"": ""SELECT [ENTITY_ID],[ENTITY_TYPE_ID],REPLACE(REPLACE([SHORT_NAME], CHAR(13),' '), CHAR(10),' '),REPLACE(REPLACE([ENTITY_NAME], CHAR(13),' '), CHAR(10),' '),REPLACE(REPLACE([ENTITY_DESCRIPTION], CHAR(13),' '), CHAR(10),' '),REPLACE(REPLACE([STATUS], CHAR(13),' '), CHAR(10),' '),[STATUS_DATE],[REVISION_ID],[CREATED_BY],[CREATION_DATE],[LAST_UPDATED_BY],[LAST_UPDATE_DATE],REPLACE(REPLACE([COMMENTS], CHAR(13),' '), CHAR(10),' ') FROM [dbo].[ENTITY] where LAST_UPDATE_DATE &gt;= '@{formatDateTime(pipeline().parameters.startDate,'yyyy-MM-dd')}'"",
                                        ""type"": ""Expression""
                                    }
                                },
                                ""sink"": {
                                    ""type"": ""AzureDataLakeStoreSink""
                                },
                                ""enableStaging"": false,
                                ""dataIntegrationUnits"": 0
                            },
                            ""inputs"": [
                                {
                                    ""referenceName"": ""DS_IN_SQL_IDM_ENTITY"",
                                    ""type"": ""DatasetReference""
                                }
                            ],
                            ""outputs"": [
                                {
                                    ""referenceName"": ""DS_OUT_ADLS_RAW_IDM_ENTITY"",
                                    ""type"": ""DatasetReference""
                                }
                            ]
                        }
                    ],
                    ""ifTrueActivities"": [
                        {
                            ""name"": ""copy_ENTITY"",
                            ""description"": ""Copies ENTITY from IDM Database to ADLS Raw Zone"",
                            ""type"": ""Copy"",
                            ""policy"": {
                                ""timeout"": ""7.00:00:00"",
                                ""retry"": 0,
                                ""retryIntervalInSeconds"": 60,
                                ""secureOutput"": false,
                                ""secureInput"": false
                            },
                            ""typeProperties"": {
                                ""source"": {
                                    ""type"": ""SqlSource"",
                                    ""sqlReaderQuery"": {
                                        ""value"": ""SELECT [ENTITY_ID],[ENTITY_TYPE_ID],REPLACE(REPLACE([SHORT_NAME], CHAR(13),' '), CHAR(10),' '),REPLACE(REPLACE([ENTITY_NAME], CHAR(13),' '), CHAR(10),' '),REPLACE(REPLACE([ENTITY_DESCRIPTION], CHAR(13),' '), CHAR(10),' '),REPLACE(REPLACE([STATUS], CHAR(13),' '), CHAR(10),' '),[STATUS_DATE],[REVISION_ID],[CREATED_BY],[CREATION_DATE],[LAST_UPDATED_BY],[LAST_UPDATE_DATE],REPLACE(REPLACE([COMMENTS], CHAR(13),' '), CHAR(10),' ') FROM [dbo].[ENTITY]"",
                                        ""type"": ""Expression""
                                    }
                                },
                                ""sink"": {
                                    ""type"": ""AzureDataLakeStoreSink""
                                },
                                ""enableStaging"": false,
                                ""dataIntegrationUnits"": 0
                            },
                            ""inputs"": [
                                {
                                    ""referenceName"": ""DS_IN_SQL_IDM_ENTITY"",
                                    ""type"": ""DatasetReference""
                                }
                            ],
                            ""outputs"": [
                                {
                                    ""referenceName"": ""DS_OUT_ADLS_RAW_IDM_ENTITY"",
                                    ""type"": ""DatasetReference""
                                }
                            ]
                        }
                    ]
                }
            }
        ],
        ""parameters"": {
            ""startDate"": {
                ""type"": ""String""
            }
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>

<h2>Trigger Source:</h2>

<pre><code>{
    ""name"": ""TR_SCHED_0800EST"",
    ""properties"": {
        ""description"": ""Daily 0800 EST"",
        ""runtimeState"": ""Stopped"",
        ""pipelines"": [
            {
                ""pipelineReference"": {
                    ""referenceName"": ""PL_ADLS_RAW_IDMTables_DAILY"",
                    ""type"": ""PipelineReference""
                },
                ""parameters"": {
                    ""startDate"": ""@trigger().scheduledTime""
                }
            }
        ],
        ""type"": ""ScheduleTrigger"",
        ""typeProperties"": {
            ""recurrence"": {
                ""frequency"": ""Day"",
                ""interval"": 1,
                ""startTime"": ""2018-08-30T13:00:00Z"",
                ""timeZone"": ""UTC"",
                ""schedule"": {
                    ""minutes"": [
                        0
                    ],
                    ""hours"": [
                        13
                    ]
                }
            }
        }
    }
}
</code></pre>

<hr>

<p>EDIT</p>

<p>This works</p>

<p><a href=""https://i.stack.imgur.com/NRSaw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NRSaw.png"" alt=""enter image description here""></a></p>

<p>This does not work</p>

<p><a href=""https://i.stack.imgur.com/BfTIz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BfTIz.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2018-09-14 11:34:31","1109","0","1","52332832","<p>Passing @trigger().scheduledTime in as a parameter in the trigger to @{formatDateTime(pipeline().parameters.startDate,'yyyy-MM-dd')} in the where clause with a parameter in the pipeline defined as startDate of type string. 
is the right way. </p>

<p>When you edit the trigger in the UI,
Make sure you pass value in the following way.
<a href=""https://i.stack.imgur.com/R18WJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/R18WJ.png"" alt=""enter image description here""></a></p>
"
"52328097","ADF - SSIS Execution Package Error 114 - Ssis Operation failed with unexpected operation status : Running","<p>When running an SSIS package in ADF that takes > +/- 15 minutes, I always get an error message inside ADF (see screenshot).The bizarre thing is that when I look at the execution report itself in the integration services catalog, the status is still on running and eventually will end in succeeded (see screenshot).</p>

<p>Anyone experienced the same? It is frustrating as I can't complete a whole pipeline because of this error.</p>

<p>FYI, the data is coming from a SQL Managed Instance. Region: West-Europe.</p>

<p>The screenshots can also be found on via these two URL's in case they don't show properly inside this question:
<a href=""https://ibb.co/gufVpp"" rel=""nofollow noreferrer"">https://ibb.co/gufVpp</a>
<a href=""https://ibb.co/dupqpp"" rel=""nofollow noreferrer"">https://ibb.co/dupqpp</a></p>

<p>Many thanks in advance!</p>

<p>[ADF error 114 message][Succeeded in Execution report]</p>
","<azure><ssis><azure-data-factory>","2018-09-14 08:40:20","429","0","2","52409798","<p>Yes we're experiencing the same thing. Very frustrating.
There is a timeout property in the json 'policy' section, but it looks like it's ignored.
I believe you can get around this by executing SSIS using a stored proc instead of Execute SSIS, but not tried it yet.</p>
"
"52328097","ADF - SSIS Execution Package Error 114 - Ssis Operation failed with unexpected operation status : Running","<p>When running an SSIS package in ADF that takes > +/- 15 minutes, I always get an error message inside ADF (see screenshot).The bizarre thing is that when I look at the execution report itself in the integration services catalog, the status is still on running and eventually will end in succeeded (see screenshot).</p>

<p>Anyone experienced the same? It is frustrating as I can't complete a whole pipeline because of this error.</p>

<p>FYI, the data is coming from a SQL Managed Instance. Region: West-Europe.</p>

<p>The screenshots can also be found on via these two URL's in case they don't show properly inside this question:
<a href=""https://ibb.co/gufVpp"" rel=""nofollow noreferrer"">https://ibb.co/gufVpp</a>
<a href=""https://ibb.co/dupqpp"" rel=""nofollow noreferrer"">https://ibb.co/dupqpp</a></p>

<p>Many thanks in advance!</p>

<p>[ADF error 114 message][Succeeded in Execution report]</p>
","<azure><ssis><azure-data-factory>","2018-09-14 08:40:20","429","0","2","52494482","<p>So I finally got Microsoft sort this out for me. </p>

<p>The problem is that when the first task flow runs in the package, the second one starts as well although it will not show that the 2nd task is running. Because the 2nd task is expecting to use the output from the 1st task, an error occurs. </p>

<p>This is a known issue at Microsoft. They have sorted this out for me, it appears there isn't much you can do on your side to sort this out. Log a ticket with Microsoft instead.</p>
"
"52320386","how to export pipeline in datafactory v2 or migrate to another","<p>I'm trying export one pipeline created in datafactory v2 or migrate to another, but not found the option,</p>

<p>Could you help me please </p>
","<azure><pipeline><azure-data-factory>","2018-09-13 19:15:05","15648","8","2","52324169","<p>As I know, you could learn about Continuous Integration in Azure Data Factory. You could find below statement in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">Continuous integration and deployment in Azure Data Factory</a>.</p>
<blockquote>
<p>For Azure Data Factory, continuous integration &amp; deployment means
moving Data Factory pipelines from one environment (development, test,
production) to another. To do continuous integration &amp; deployment, you
can use Data Factory UI integration with Azure Resource Manager
templates. The Data Factory UI can generate a Resource Manager
template when you select the ARM template options. When you select
Export ARM template, the portal generates the Resource Manager
template for the data factory and a configuration file that includes
all your connections strings and other parameters. Then you have to
create one configuration file for each environment (development, test,
production). The main Resource Manager template file remains the same
for all the environments.</p>
</blockquote>
<p>More detail steps and video,just refer to the above link.</p>
"
"52320386","how to export pipeline in datafactory v2 or migrate to another","<p>I'm trying export one pipeline created in datafactory v2 or migrate to another, but not found the option,</p>

<p>Could you help me please </p>
","<azure><pipeline><azure-data-factory>","2018-09-13 19:15:05","15648","8","2","58732924","<p>You can create a template for each <code>pipeline</code> that you have in development environment or other environment. Then you need to create a <code>configuration file</code> for each pipeline's template. If you have three environments you need to create three configuration files per environment (Dev,QC,Prod) for each pipeline.</p>

<p>To export a template from a pipeline that you have right now, you have to open that pipeline in <code>Azure Portal</code> and then press <code>Save as template</code>. </p>

<p><a href=""https://i.stack.imgur.com/9klai.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9klai.png"" alt=""enter image description here""></a></p>

<p>Then you need to configure you <code>Git</code>. Then press <code>Export template</code> </p>

<p><a href=""https://i.stack.imgur.com/TlSEV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TlSEV.png"" alt=""enter image description here""></a></p>

<p>Or you can open your Pipeline and click on this menu :</p>

<p><a href=""https://i.stack.imgur.com/1I3jb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1I3jb.png"" alt=""enter image description here""></a></p>

<p>If you use the second way, the template file will be generated automatically. </p>

<p><a href=""https://i.stack.imgur.com/NHRlJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NHRlJ.png"" alt=""enter image description here""></a></p>

<p>Create a Configuration file for your pipeline's template. A configuration file can be like that</p>

<pre><code>{  
    ""$schema"":""https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#"",
    ""contentVersion"":""1.0.0.0"",
    ""parameters"":{  
        ""dataFactoryName"":{  
            ""value"":""cslg-df-dev""
        },
        ""dataFactoryLocation"":{  
            ""value"":""East US""
        },
        ""storageAccountName"":{  
            ""value"":""cslgsadev""
        },
        ""storageAccountKey"":{  
            ""value"":""T5aVtCTKM4T0XWitf7loD9sOkbdcHd3hIVCEJRiwnUr7vzuWX0da02UNOr1z8znuTOef6ChqcnYwB3byXr2yCg==""
        },
        ""triggerStartTime"":{  
            ""value"":""2019-09-08T11:00:00""
        },
        ""triggerEndTime"":{  
            ""value"":""2019-09-08T14:00:00""
        }
    }
}
</code></pre>

<p>Open <code>Power shell</code> on your computer. Run the folowing command to import your pipeline to new <code>Resource Group</code> for other environment.</p>

<pre><code>Connect-AzAccount

New-AzResourceGroupDeployment -Name MyARMDeployment -ResourceGroupName cslg-rg-QC -TemplateFile C:\...\ADFTutorialARM.json -TemplateParameterFile C:\...\ADFTutorialARM-Parameters.json
</code></pre>

<p>you can see <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-resource-manager-template"" rel=""nofollow noreferrer"">Create an Azure data factory using Azure Resource Manager template</a> link if you need more information.</p>
"
"52320089","Azure Data Factory get data from foreach value","<p>I've a lookup activity with gets value from a container within a blob.  I've a foreach activity which is connected to the lookup activity </p>

<p><a href=""https://i.stack.imgur.com/opMm2.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/opMm2.png"" alt=""enter image description here""></a></p>

<p>and under settings it has the value:</p>

<pre><code>@activity('LookupMessageType').output.value
</code></pre>

<p>I've another copy activity which runs within this foreach activity.  It copies data from cosmos DB to Azure Data Lake.<br>
<a href=""https://i.stack.imgur.com/MMcmy.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/MMcmy.png"" alt=""enter image description here""></a></p>

<p>This is the query in the source dataset:</p>

<pre><code>select c.Tag, data.assetTag, data.timestamp, data.jsonVersion, data.messageType, data.speed from c join data in c.data
where (data.speed&gt;  item().speed_Value) AND
(data.timestamp &gt;= '@{formatDateTime(addhours(pipeline().TriggerTime, -1), 'yyyy-MM-ddTHH:mm:ssZ' )}' 
AND data.timestamp &lt; '@{formatDateTime(pipeline().TriggerTime, 'yyyy-MM-ddTHH:mm:ssZ' )}')
</code></pre>

<p>I get an error as I run this pipeline:</p>

<pre><code>{
    ""errorCode"": ""2200"",
    ""message"": ""Failure happened on 'Source' side. ErrorCode=UserErrorDocumentDBReadError,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=DocumentDb operation failed: Message: {\""errors\"":[{\""severity\"":\""Error\"",\""location\"":{\""start\"":231,\""end\"":235},\""code\"":\""SC2005\"",\""message\"":\""'item' is not a recognized built-in function name.\""}]}\r\nActivityId: *redacted*, documentdb-dotnet-sdk/1.21.1 Host/64-bit MicrosoftWindowsNT/6.2.9200.0.,Source=Microsoft.DataTransfer.ClientLibrary.DocumentDb,''Type=Microsoft.Azure.Documents.BadRequestException,Message=Message: {\""errors\"":[{\""severity\"":\""Error\"",\""location\"":{\""start\"":231,\""end\"":235},\""code\"":\""SC2005\"",\""message\"":\""'item' is not a recognized built-in function name.\""}]}\r\nActivityId: redacted, documentdb-dotnet-sdk/1.21.1 Host/64-bit MicrosoftWindowsNT/6.2.9200.0,Source=Microsoft.Azure.Documents.Client,''Type=System.Runtime.InteropServices.COMException,Message=Exception from HRESULT: 0x800A0B00,Source=,'"",
    ""failureType"": ""UserError"",
    ""target"": ""Copy Data1""
}
</code></pre>

<p>It is saying item is not a built in function name.  I want to use value from the foreach activity to query the source.  </p>

<p>Furthermore, here is my dynamic filepath content</p>

<pre><code>@concat('test_', item().speed, '.csv')
</code></pre>

<p>I get the desired dynamic file structure with this expression, if I am using static values in the query like:   <code>data.speed&gt; 500</code>  </p>
","<azure><azure-cosmosdb><azure-data-factory>","2018-09-13 18:51:09","9236","7","1","52320461","<p>So, I figured out what was wrong with the query:
instead of </p>

<p><code>where (data.speed&gt;  item().speed_Value)</code> </p>

<p>I had to use</p>

<pre><code>where (data.speed&gt; @{item().speed_value})
</code></pre>
"
"52314042","Configure Self hosted integration runtime for ADF v1","<p>I have installed self hosted IR on my PC and am trying to use it in my ADF (SQL Server to Azure SQL DB) pipeline. When i run the pipeline it fails with the below error.</p>

<p>InvalidParameter,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The value of the property 'msiAuthenticator' is invalid: 'The required property is not specified. Parameter name: </p>
","<azure><azure-sql-database><azure-data-factory>","2018-09-13 12:45:09","523","-1","1","52360395","<p>I think you can try the copy tool UI and set up it again.
<a href=""https://i.stack.imgur.com/wyYvY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wyYvY.png"" alt=""Copy Data Tool""></a>
did you use the encrypted credential for your linked service, what authentication type did you use ? to know your scenario, I need more information.</p>
"
"52313633","Azure Data Factory dynamic output path based on source dataset payload","<p>I have a stream analytics job which constantly dumps data in Cosmos DB.  The payload has a property ""Type"" which determines the payload itself.  i.e. which columns are included in the payload. It is an integer value of either 1 or 2.</p>

<p>I'm using Azure Data Factory V2 to copy data from Cosmos DB to Data Lake.  I've created a pipeline with an activity that does this job. I'm setting the output path folder name using : </p>

<p><code>@concat('datafactoryingress/rawdata/',dataset().productFilter,'/',formatDateTime(utcnow(),'yyyy'),'/')</code></p>

<p>What I want in the datafactory is to identify the payload itself, i.e. determine if the type is 1 or 2 and then determine if the data goes in folder 1 or folder 2.  I want to iterate the data from Cosmos DB and determine the message type and segregate based on message Type and set the folder paths dynamically.</p>

<p>Is there a way to do that?  Can I check the Cosmos DB document to find out the message type and then how do I set the folder path dynamically based on that?</p>
","<azure><azure-cosmosdb><azure-data-factory>","2018-09-13 12:19:56","949","2","1","52327308","<blockquote>
<p>Is there a way to do that? Can I check the Cosmos DB document to find
out the message type and then how do I set the folder path dynamically
based on that?</p>
</blockquote>
<p>Unfortunately, based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">doc</a>, dynamic content from source dataset is not supported by adf so far. You can't grab the fields in the source data as sink output dynamic parameters. Based on your situation, I suggest you setting up two separate pipelines to transfer data according to the <code>Type</code> field respectively.</p>
<p>If the <code>Type</code> field is varied and you do want to differentiate the output path, the ADF may not be the suitable choice for you. You could write logical code to fulfill your needs.</p>
"
"52293377","Building this pipeline on Azure Data Factory V2","<p>I am currently trying to set up this pipeline on Azure Data Factory V2 (as you can see in the picture attached). In summary this ERP system will export in a monthly basis this report (CSV file with actual and forecast data) and this will be saved in a blob container. As soon as this file CSV is saved, an event trigger should activate this stored procedure that will - in turn - erase all actual data from my fact table in Azure SQL as this gets replaced every month. </p>

<p>Once actual data is deleted, the pipeline would have subsequently a copy activity that would - in turn - copy the CSV report (actuals + forecast) to same fact table in Azure SQL. Once the copy activity is finished, the HTTP logic APP would delete that new CSV file from the blob container. This workflow would be a recurrent event to be carried out very month. </p>

<p>So far I have been able to run these 3 x activities independently. However, when I join them in the same pipeline, I have had some parameters errors when trying to ""publish all"". Therefore I am not sure whether I need to have the same parameters for each activity in the pipeline? </p>

<p>The JSON code for my pipeline is the following:</p>

<pre><code>{
    ""name"": ""TM1_pipeline"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""Copy Data1"",
                ""type"": ""Copy"",
                ""dependsOn"": [
                    {
                        ""activity"": ""Stored Procedure1"",
                        ""dependencyConditions"": [
                            ""Succeeded""
                        ]
                    }
                ],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false
                },
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""BlobSource"",
                        ""recursive"": false
                    },
                    ""sink"": {
                        ""type"": ""SqlSink"",
                        ""writeBatchSize"": 10000
                    },
                    ""enableStaging"": false,
                    ""dataIntegrationUnits"": 0
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""SourceDataset_e7y"",
                        ""type"": ""DatasetReference"",
                        ""parameters"": {
                            ""copyFolder"": {
                                ""value"": ""@pipeline().parameters.sourceFolder"",
                                ""type"": ""Expression""
                            },
                            ""copyFile"": {
                                ""value"": ""@pipeline().parameters.sourceFile"",
                                ""type"": ""Expression""
                            }
                        }
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""DestinationDataset_e7y"",
                        ""type"": ""DatasetReference""
                    }
                ]
            },
            {
                ""name"": ""Stored Procedure1"",
                ""type"": ""SqlServerStoredProcedure"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""typeProperties"": {
                    ""storedProcedureName"": ""[dbo].[test_sp]""
                },
                ""linkedServiceName"": {
                    ""referenceName"": ""AzureSqlDatabase"",
                    ""type"": ""LinkedServiceReference""
                }
            },
            {
                ""name"": ""Web1"",
                ""type"": ""WebActivity"",
                ""dependsOn"": [
                    {
                        ""activity"": ""Copy Data1"",
                        ""dependencyConditions"": [
                            ""Succeeded""
                        ]
                    }
                ],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""typeProperties"": {
                    ""url"": ""..."",
                    ""method"": ""POST"",
                    ""body"": {
                        ""value"": ""@pipeline().parameters.BlobName"",
                        ""type"": ""Expression""
                    }
                }
            }
        ],
        ""parameters"": {
            ""sourceFolder"": {
                ""type"": ""String"",
                ""defaultValue"": ""@pipeline().parameters.sourceFolder""
            },
            ""sourceFile"": {
                ""type"": ""String"",
                ""defaultValue"": ""@pipeline().parameters.sourceFile""
            },
            ""BlobName"": {
                ""type"": ""String"",
                ""defaultValue"": {
                    ""blobname"": ""source-csv/test.csv""
                }
            }
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>

<p><a href=""https://i.stack.imgur.com/IRDK2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IRDK2.png"" alt=""enter image description here""></a></p>
","<azure><parameters><azure-sql-database><pipeline><azure-data-factory>","2018-09-12 10:50:02","2201","0","1","52312183","<p>Please follow <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger"" rel=""nofollow noreferrer"">this doc</a> to configure you blob event trigger and pass the right value to your parameters. </p>
"
"52271088","Execute python scripts in Azure DataFactory","<p>I have my data stored in blobs and I have written a python script to do some computations and create another csv. How can I execute this in Azure Data Factory ?</p>
","<azure><azure-storage><azure-data-factory>","2018-09-11 07:46:34","16110","5","2","52271285","<p>You could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">Azure Data Factory V2 custom activity</a> for your requirements. You can directly execute a command to invoke Python script using Custom Activity.</p>
<p>Please refer to this <a href=""https://github.com/rawatsudhir1/ADFPythonCustomActivity"" rel=""nofollow noreferrer"">sample</a> on the GitHub.</p>
"
"52271088","Execute python scripts in Azure DataFactory","<p>I have my data stored in blobs and I have written a python script to do some computations and create another csv. How can I execute this in Azure Data Factory ?</p>
","<azure><azure-storage><azure-data-factory>","2018-09-11 07:46:34","16110","5","2","52271730","<p>Another option is using a DatabricksSparkPython Activity. This makes sense if you want to scale out, but could require some code modifications for PySpark support. Prerequisite of cause is an Azure Databricks workspace. You have to upload your script to DBFS and can trigger it via Azure Data Factory. The following example triggers the script pi.py:</p>

<pre><code>{
    ""activity"": {
        ""name"": ""MyActivity"",
        ""description"": ""MyActivity description"",
        ""type"": ""DatabricksSparkPython"",
        ""linkedServiceName"": {
            ""referenceName"": ""MyDatabricksLinkedservice"",
             ""type"": ""LinkedServiceReference""
        },
        ""typeProperties"": {
            ""pythonFile"": ""dbfs:/docs/pi.py"",
            ""parameters"": [
                ""10""
            ],
            ""libraries"": [
                {
                    ""pypi"": {
                        ""package"": ""tensorflow""
                    }
                }
            ]
        }
    }
}
</code></pre>

<p>See the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-databricks-python"" rel=""noreferrer"">Documentation</a> for more details.</p>
"
"52259456","Serializing Array inside cosmos DB when using ADF","<p>I've the following format within my COSMOS DB document.</p>

<pre><code>""Tag"": ""SPEEDSTER"",
    ""data"": [
        {
            ""timestamp"": ""2018-09-05T13:55:09.297Z"",
            ""jsonVersion"": 1,
            ""speed"": 404
        }
    ]
</code></pre>

<p>While importing the schema within ADF copy pipeline, the array is not supported.  Is there any way I can achieve this?</p>
","<azure><azure-cosmosdb><azure-data-factory>","2018-09-10 13:57:47","458","1","2","52267826","<p>what is your sink data store?</p>

<p>If you want to export the document as is, for example, to a json format file, you could use the export json as is feature. To achieve that, you could remove structure in your data set and translator in your copy activity.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db#importexport-json-documents"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db#importexport-json-documents</a></p>

<p>If you want to extract data out from the array, you could write your own query to do some transformation.</p>
"
"52259456","Serializing Array inside cosmos DB when using ADF","<p>I've the following format within my COSMOS DB document.</p>

<pre><code>""Tag"": ""SPEEDSTER"",
    ""data"": [
        {
            ""timestamp"": ""2018-09-05T13:55:09.297Z"",
            ""jsonVersion"": 1,
            ""speed"": 404
        }
    ]
</code></pre>

<p>While importing the schema within ADF copy pipeline, the array is not supported.  Is there any way I can achieve this?</p>
","<azure><azure-cosmosdb><azure-data-factory>","2018-09-10 13:57:47","458","1","2","52268220","<p>You need to use SQL query in the copy activity to design your schema of your cosmos db source data.</p>
<p><a href=""https://i.stack.imgur.com/zNsx6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zNsx6.png"" alt=""enter image description here"" /></a></p>
<p>I tested above SQL based on your sample data, and it was copied to a txt file in the blob storage successfully.</p>
<pre><code>select c.id,c.Tag,data.timestamp,data.jsonVersion,data.speed from c
</code></pre>
<p><em><strong>Output:</strong></em></p>
<p><a href=""https://i.stack.imgur.com/kd7ZS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kd7ZS.png"" alt=""enter image description here"" /></a></p>
"
"52256761","In AzureDataFactory,Unable to configure a collection of .wav files in container as 'Dataset'","<p>In AzureDataFactory,Unable to configure a collection of .wav files in container as 'Dataset' . Dataset is formed by the data inside those files
My task is to iterate names of files present in a blob container and use the file name in web activity </p>

<p>I thought to use Lookup to store the file name collections in variable as array . But I need to connect to a dataset. While creating a dataset,
 it doesn't accept collections  of files in container as dataset. It automatically takes the data in each files(which are inside the container) as dataset.</p>

<p><a href=""https://i.stack.imgur.com/XVYwI.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<azure><azure-data-factory>","2018-09-10 11:23:26","51","1","1","52268016","<p>You could chain a for each  activity after your lookup activity. Put the web activity inside into the for each activity 
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity</a></p>
"
"52251770","Azure Data Factory HTTP endpoint to CosmosDB error converting datetime","<p>I have a HTTP endpoint that returns data including a date time in the format: </p>

<pre><code>""RowInsertDateTime"": ""2018-09-10T15:31:21.223"",
</code></pre>

<p>I have had the data factory pipeline fail to import this row as a DATETIME due to this error: </p>

<blockquote>
<pre><code>""message"": ""ErrorCode=UserErrorInvalidDataValue,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=,Source=,''Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Column
</code></pre>
  
  <p>'RowInsertDateTime' contains an invalid value
  '2018-09-10T15:39:50.33'. Cannot convert '2018-09-10T15:39:50.33' to
  type 'DateTime' with format
  'YYYY-MM-DDTHH:MM:SS'.,Source=Microsoft.DataTransfer.Common,''Type=System.FormatException,Message=String
  was not recognized as a valid
  DateTime.,Source=mscorlib,''Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Column
  'RowInsertDateTime' contains an invalid value
  '2018-09-10T15:39:50.33'. Cannot convert '2018-09-10T15:39:50.33' to
  type 'DateTime' with format
  'YYYY-MM-DDTHH:MM:SS'.,Source=Microsoft.DataTransfer.Common,''Type=System.FormatException,Message=String
  was not recognized as a valid
  DateTime.,Source=mscorlib,''Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Column
  'RowInsertDateTime' contains an invalid value
  '2018-09-10T15:39:50.33'. Cannot convert '2018-09-10T15:39:50.33' to
  type 'DateTime' with format
  'YYYY-MM-DDTHH:MM:SS'.,Source=Microsoft.DataTransfer.Common,''Type=System.FormatException,Message=String
  was not recognized as a valid
  DateTime.,Source=mscorlib,''Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Column
  'RowInsertDateTime' contains an invalid value
  '2018-09-10T15:39:50.33'. Cannot convert '2018-09-10T15:39:50.33' to
  type 'DateTime' with format
  'YYYY-MM-DDTHH:MM:SS'.,Source=Microsoft.DataTransfer.Common,''Type=System.FormatException,Message=String
  was not recognized as a valid DateTime.,Source=mscorlib,'"",</p>
</blockquote>

<p>I have tried setting a format of YYYY-MM-DDTHH:MM:SS, but I get the same failure. Any ideas? I thought the date time format was valid. </p>
","<datetime-format><azure-data-factory>","2018-09-10 05:47:50","234","0","1","52253205","<p>I see you have milliseconds in a date, so can you try using this format ""yyyy-MM-ddTHH:mm:ss.fff""</p>
"
"52249928","Generating and storing JSON files from the run-time parameters passed to Azure Data Factory v2 pipeline?","<p>Can we create a file (preferably json) and store it in its supported storage sinks (like Blob, Azure Data Lake Service etc) using the parameters that are passed to Azure Data Factory v2 pipeline at run-time. I suppose it can be done via Azure Batch but it seems to be an overkill for such a trivial task. Is there a better way to do that?</p>
","<azure><azure-storage><azure-data-factory>","2018-09-10 01:21:06","744","2","1","52250338","<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data"" rel=""nofollow noreferrer"">Here</a> are all the transform activities ADFv2 currently equips with, I'm afraid there isn't a direct way to create a file in ADFv2. You could leverage <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">Custom activity</a> to achieve this by running your customized code logic on an Azure Batch pool of virtual machines. Hope it'll help a little.</p>
"
"52249904","Invoking Ruby Scripts or Executables on on-premise server using Azure Data Factory v2?","<p>How can I invoke Ruby scripts or executable(s) on on-premise server using Azure Data Factory v2 (ADF)? In my view, it falls under the category of Custom Activity and ADF as of now supports custom activity only via Azure Batch and Azure HDInsight. But what about running custom activity on on-premise server/network?
It's understandable that for on-premise server or networks behind firewall, one needs to setup self-hosted integration run-time (IR). But as per documentation of ADF, it appears that self-hosted IR can be used for copy activity. Can I run a custom activity on self-hosted integration run-time? </p>

<p>one workaround can be that I use Azure Batch that SSH into my on-premise server and executes the scripts but due to certain security restrictions, I cant proceed with this possible solution.</p>
","<ruby><azure><azure-data-factory><azure-batch>","2018-09-10 01:17:03","555","2","1","52256250","<p>Azure batch linked service can also reference a selfhosted IR. So you should be able to run a custom activity on selfhosted integration runtime. 
You could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-portal"" rel=""nofollow noreferrer"">ADF UI</a> to have a try.</p>
"
"52247184","Stream Data from SQL Server into Azure Data Warehouse","<p>The Microsoft process looks like a batch import method of copying data from SQL Server into Azure Data Warehouse.</p>

<p><a href=""https://i.stack.imgur.com/op0mW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/op0mW.png"" alt=""enter image description here""></a></p>

<p>Is there a simpler method, conducting every second of streaming data from MS SQL Server into Datawarehouse. This seems overly complicated with two ETL steps, (Azure Data Factory, and then Polybase) . Can we continually stream data from SQL Server into Data Warehouse? (We know AWS allows streaming of data from SQL server into  Redshift DW). <a href=""https://stackoverflow.com/questions/51054085/stream-data-from-sql-server-into-redshift-with-kinesis-firehose"">Stream Data from SQL Server into Redshift</a></p>

<p><a href=""https://azure.microsoft.com/en-us/services/sql-data-warehouse/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/services/sql-data-warehouse/</a></p>
","<sql-server><azure><azure-sql-database><data-warehouse><azure-data-factory>","2018-09-09 17:53:17","986","2","2","52248839","<p>Your <a href=""https://stackoverflow.com/questions/51054085/stream-data-from-sql-server-into-redshift-with-kinesis-firehose"">link</a> does not explain how to <a href=""https://aws.amazon.com/streaming-data/"" rel=""nofollow noreferrer"">stream data</a> from SQL Server to Redshift; rather it suggests a data migration service for continually ETL'ing data from source to destination. If this is your aim, your can write an <a href=""https://learn.microsoft.com/en-us/sql/integration-services/ssis-how-to-create-an-etl-package?view=sql-server-2017"" rel=""nofollow noreferrer"">SSIS package</a> (i.e. migration service) and schedule it on a continuing basis using <a href=""https://learn.microsoft.com/en-us/sql/integration-services/lift-shift/ssis-azure-schedule-packages?view=sql-server-2017#activity"" rel=""nofollow noreferrer"">Azure Data Factory</a>.</p>
"
"52247184","Stream Data from SQL Server into Azure Data Warehouse","<p>The Microsoft process looks like a batch import method of copying data from SQL Server into Azure Data Warehouse.</p>

<p><a href=""https://i.stack.imgur.com/op0mW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/op0mW.png"" alt=""enter image description here""></a></p>

<p>Is there a simpler method, conducting every second of streaming data from MS SQL Server into Datawarehouse. This seems overly complicated with two ETL steps, (Azure Data Factory, and then Polybase) . Can we continually stream data from SQL Server into Data Warehouse? (We know AWS allows streaming of data from SQL server into  Redshift DW). <a href=""https://stackoverflow.com/questions/51054085/stream-data-from-sql-server-into-redshift-with-kinesis-firehose"">Stream Data from SQL Server into Redshift</a></p>

<p><a href=""https://azure.microsoft.com/en-us/services/sql-data-warehouse/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/services/sql-data-warehouse/</a></p>
","<sql-server><azure><azure-sql-database><data-warehouse><azure-data-factory>","2018-09-09 17:53:17","986","2","2","57642682","<p>Create a DFT in SSIS package with OLEDB Source and  Destination. Schedule a job in SSMS. This much simpler.</p>
"
"52243619","Azure Data Factory not visible in PowerShell environment","<p>I have been created several Azure Data Factories on the Portal then I would like to monitor them from PowerShell. 
After login to my subscription via PS I have tried this command:</p>

<pre><code>Get-AzureRmDataFactory -ResourceGroupName 'Myresource'
</code></pre>

<p>and get back only the PS prompt nothing else (neither Data Factory names nor an error message). </p>

<p>On the Portal still I see the Data Factories.
If I create a Data Factory via PS with 
<code>New-AzureRmDataFactory</code> command then I see that Data Factory on the Portal and on the PS, too.</p>

<p>Am I missing something?</p>
","<powershell><azure-data-factory>","2018-09-09 10:35:01","102","2","1","52250051","<p>Are you using ADF v1 or v2?</p>

<p>If v2, you can try <code>Get-AzureRmDataFactoryV2 -ResourceGroupName ""Myresource""
</code>.</p>
"
"52225957","ADFv2: How to remove connection to GIT repository once set up?","<p>Currently, ADFv2 allows you to set up Code Repository and automatically synchronise JSON files of components of pipelines, datasets, etc with repo (e.g.GIT).
But, once you set it up - how to actually remove it or change configuration (not branch)?
<a href=""https://i.stack.imgur.com/SqzBG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SqzBG.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2018-09-07 15:52:09","2711","4","1","52231230","<p>In overview tab, there is a repository settings button, click it, you will see the remove repository button. <a href=""https://i.stack.imgur.com/Tr2cF.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Tr2cF.png"" alt=""enter image description here""></a></p>
"
"52225172","Azure Data Factory Pipeline run based on parameters","<p>I'm using ADF to copy data from Cosmos DB to Azure Data Lake.  I've scheduled it to run every 24 hours.  Since ADF will copy all the data from the source into the sink, I use windowStart and windowEnd parameters to filter the data.  The filtering is done on timestamp which is inside Cosmos Document DB.  </p>

<p>To run a pipeline, I've to manually specify the windowStart and windowEnd UTC time, which is not feasible.  Is there a way to automate this?  What I want is to set the windowStart time to the (scheduled time - 1 day)  and windowEnd time to be the schedule time.  This way I can get all the data from the previous day.</p>

<p>The generated query is:</p>

<pre><code>select * from c 
where c.data.timestamp &gt;= '@{formatDateTime(pipeline().parameters.windowStart, 'yyyy-MM-ddTHH:mm:ssZ' )}' 
AND c.data.timestamp &lt; '@{formatDateTime(pipeline().parameters.windowEnd, 'yyyy-MM-ddTHH:mm:ssZ' )}'
</code></pre>

<p>How can I set the windowStart and windowEnd dynamically to be equal and -1 day to the scheduled time?</p>
","<azure><azure-data-factory>","2018-09-07 14:59:41","2976","0","1","52231263","<p>If you are using <strong><a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger"" rel=""nofollow noreferrer"">schedule trigger</a></strong>, passing the following values into your pipeline. Use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#date-functions"" rel=""nofollow noreferrer"">adddays</a> function on your scheduled time. 
You could use ADF UI to help you edit/new the trigger.
<a href=""https://i.stack.imgur.com/MWo9U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MWo9U.png"" alt=""enter image description here""></a></p>

<pre><code>""name"": ""trigger1"",
""properties"": {
    ""runtimeState"": ""Stopped"",
    ""pipelines"": [
        {
            ""pipelineReference"": {
                ""referenceName"": ""CopyPipeline_9ca"",
                ""type"": ""PipelineReference""
            },
            ""parameters"": {
                ""windowStart"": ""@adddays(trigger().scheduledTime, -1)"",
                ""windowEnd"": ""@trigger().scheduledTime""
            }
        }
    ],
</code></pre>
"
"52189221","Azure Data Factory v2 - SSIS lift and shift","<p>I'm in the process of evaluating the possibilites of lifting and shifting my ssis packages to ADFv2 but without testing I'm finding it hard to see if all SSIS functionalities are supported. 
For example my package unzips files, modifies contents of files (script task) saving new version in different directory, loads modified files to DB and update data etc</p>

<p>What I'm not sure about is unzipping the files (I dont want to transfer unzipped files from on prem) and also modifying files with script task. I believe these would have to be moved outside of SSIS and created as an activity of ADF? And leave only the load of files, updating data etc as my SSIS package? Probably with the files stored in Blob storage?</p>

<p>Or can all this still be done directly in SSIS?</p>

<p>Thanks</p>
","<azure><azure-data-factory>","2018-09-05 15:54:49","782","1","1","52195765","<p>What you currently do using SSIS on premises, you could also do using SSIS in ADF.  For example, you could install additional (un)zip programs using custom setup and utilize the %TEMP% folder/current working directory (""."") of your SSIS IR to modify files, see</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-configure-azure-ssis-ir-custom-setup"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-configure-azure-ssis-ir-custom-setup</a></p>

<p><a href=""https://learn.microsoft.com/en-us/sql/integration-services/lift-shift/ssis-azure-files-file-shares?view=sql-server-2017"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sql/integration-services/lift-shift/ssis-azure-files-file-shares?view=sql-server-2017</a></p>
"
"52170435","Column defined in source Dataset could not be found in the actual source","<p>I have an ADF Copy Data flow and I'm getting the following error at runtime:</p>

<p><a href=""https://i.stack.imgur.com/zz27l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zz27l.png"" alt=""enter image description here""></a></p>

<p>My source is defined as follows:</p>

<p><a href=""https://i.stack.imgur.com/Sa1uB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Sa1uB.png"" alt=""enter image description here""></a></p>

<p>In my data set, the column is defined as shown below:</p>

<p><a href=""https://i.stack.imgur.com/VNOul.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VNOul.png"" alt=""enter image description here""></a></p>

<p>As you can see from the second image, the column IsLiftStation is defined in the source. Any idea why ADF cannot find the column?</p>
","<azure-data-factory>","2018-09-04 16:19:36","3498","1","3","52199694","<p>could you check that is there a column named 'ae_type_id' in your schema? If that's the case, could you remove that column and try again? The columns in the schema must be aligned with columns in the query.</p>
"
"52170435","Column defined in source Dataset could not be found in the actual source","<p>I have an ADF Copy Data flow and I'm getting the following error at runtime:</p>

<p><a href=""https://i.stack.imgur.com/zz27l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zz27l.png"" alt=""enter image description here""></a></p>

<p>My source is defined as follows:</p>

<p><a href=""https://i.stack.imgur.com/Sa1uB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Sa1uB.png"" alt=""enter image description here""></a></p>

<p>In my data set, the column is defined as shown below:</p>

<p><a href=""https://i.stack.imgur.com/VNOul.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VNOul.png"" alt=""enter image description here""></a></p>

<p>As you can see from the second image, the column IsLiftStation is defined in the source. Any idea why ADF cannot find the column?</p>
","<azure-data-factory>","2018-09-04 16:19:36","3498","1","3","54302714","<p>I've had the same error.  You can solve this by either selecting all columns (*) in the source and then mapping those you want to the sink schema, or by 'clearing' the mapping in which case the ADF Copy component will auto map to columns in the sink schema (best if columns have the same names in source and sink).  Either of these approaches works.</p>

<p>Unfortunately,  clicking the import schema button in the mapping tab doesn't work.  It does produce the correct column mappings based on the columns in the source query but I still get the original error 'the column could not be located in the actual source' after doing this mapping.</p>
"
"52170435","Column defined in source Dataset could not be found in the actual source","<p>I have an ADF Copy Data flow and I'm getting the following error at runtime:</p>

<p><a href=""https://i.stack.imgur.com/zz27l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zz27l.png"" alt=""enter image description here""></a></p>

<p>My source is defined as follows:</p>

<p><a href=""https://i.stack.imgur.com/Sa1uB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Sa1uB.png"" alt=""enter image description here""></a></p>

<p>In my data set, the column is defined as shown below:</p>

<p><a href=""https://i.stack.imgur.com/VNOul.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VNOul.png"" alt=""enter image description here""></a></p>

<p>As you can see from the second image, the column IsLiftStation is defined in the source. Any idea why ADF cannot find the column?</p>
","<azure-data-factory>","2018-09-04 16:19:36","3498","1","3","71568009","<p>The issue is caused by an incomplete schema in one of the data sources.  My solution is:</p>
<ol>
<li>Step through the data flow selecting the first schema, Import projection</li>
<li>Go to the flow and Data Preview</li>
<li>Repeat for each step.</li>
</ol>
<p>In my case, there were trailing commas in one of the CSV files.  This caused automated column names to be created in the import allowing me to fix the data file.</p>
"
"52165397","Azure data factory web activity with MSI authentication","<p>We are using Azure data factory to copy data from on-premise to Azure. We have implemented multiple activities to complete the data copy. Until now, we are using basic authentication for web activity to call web API methods.</p>

<p>As per the latest monitoring UI, it also supports MSI authentication. We have tried to use but no any luck. Also, tried to search related things but does not get any information related to data factory web activity and MSI authentication.</p>

<p>How can we achieve this authentication for Web Activity?</p>

<p>Regards,</p>

<p>Shrikant</p>
","<azure-web-app-service><azure-data-factory>","2018-09-04 11:35:06","2843","0","1","52198375","<blockquote>
  <p>How can we achieve this authentication for Web Activity?</p>
</blockquote>

<p>If you use the Azure function/WebApp MSI, you need to config the resource.</p>

<pre><code>https://management.azure.com/
</code></pre>

<p>For other resources, you could refer to this <a href=""https://learn.microsoft.com/en-us/azure/active-directory/managed-service-identity/services-support-msi#azure-services-that-support-azure-ad-authentication"" rel=""nofollow noreferrer"">document</a>.</p>
"
"52160348","How to execute R script along with Azure Data Factory?","<p>Being novice to ADF, just wandoring whether we can set up the R script execution with the help of Azure Data Factory?</p>

<p>Any useful link to the information would be so much appriciated. :)</p>

<p>Thanks,
Abhijeet</p>
","<r><azure><azure-data-factory>","2018-09-04 06:49:24","2047","1","1","52161318","<p>You need to create an <code>ADF custom activity</code> to execute the <code>R</code> scripts in <code>Azure Batch Service</code> or on an <code>Azure HDInsight cluster</code> with <code>R</code> installed.</p>

<p>Links <a href=""https://dataninjago.com/2018/07/20/execute-r-scripts-from-azure-data-factory-v2-through-azure-batch-service/"" rel=""nofollow noreferrer"">here</a> &amp; <a href=""https://github.com/Azure/Azure-DataFactory/tree/master/Samples/RunRScriptUsingADFSample"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Comprehensive guide <a href=""https://azure.microsoft.com/en-in/resources/samples/data-factory-r-server-apache-spark-pipeline/"" rel=""nofollow noreferrer"">here</a> and <a href=""https://opdhsblobprod01.blob.core.windows.net/contents/4a6d75bb3af747de838e6ccc97c5d978/7fbd9eb738ab353b11a30ebc231daab2?sv=2015-04-05&amp;sr=b&amp;sig=QrEu23n3Zo%2FhTD5hQz7AZ9w2sy0E3jztujmR5001ins%3D&amp;st=2018-09-04T07%3A31%3A42Z&amp;se=2018-09-05T07%3A41%3A42Z&amp;sp=r"" rel=""nofollow noreferrer"">here</a> for information on <code>Azure HDInsight linked services</code>.</p>
"
"52159006","ResourceNotFound, The Resource Microsoft.DataFactory/factories/... under resource group '...' was not found","<p>I was running following azure rest api call through the console given in web. Though we set up every thing correctly and try to retrieve pipelines in the data factory attached to a resource group gives following error. Please help me out to if anyone know how to get rid of this error.</p>

<p>The Resource 'Microsoft.DataFactory/factories/avengersadf' under resource group 'ADFResourceGroup' was not found.</p>

<pre><code>GET https://management.azure.com/subscriptions/guid/resourceGroups/ADFResourceGroup/providers/Microsoft.DataFactory/factories/avengersadf/pipelines/Dotnet%20Helloworld?api-version=2018-06-01
Authorization: Bearer xxx

Response Code: 404

cache-control: no-cache
content-type: application/json; charset=utf-8
expires: -1
pragma: no-cache
x-ms-correlation-request-id: aad8af0d-4fca-47df-ad48-2e627b402e87
x-ms-failure-cause: gateway
x-ms-request-id: aad8af0d-4fca-47df-ad48-2e627b402e87
x-ms-routing-request-id: SOUTHEASTASIA:20180903T184319Z:aad8af0d-4fca-47df-ad48-2e627b402e87
Body
{
  ""error"": {
    ""code"": ""ResourceNotFound"",
    ""message"": ""/avengersadf' under resource group 'ADFResourceGroup' was not found.""
  }
}
</code></pre>

<p><a href=""https://i.stack.imgur.com/jxML3.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<rest><azure><http><pipeline><azure-data-factory>","2018-09-04 04:52:16","947","0","1","52159449","<p>This means that there is no such a resource under this resource group. check spelling and\or subscription</p>

<p>ps. also, you should never post bearer tokens online, somebody could use that token to impersonate you, you really want to change password now</p>
"
"52116811","Retrieve Azure Data Factory Service Identity Application ID","<p>I have created one Data Factory and Key Vault using C# Code, I would like to set Access Policy of Key Vault. 
For that I want data factory <em>""Service Identity Application ID""</em> (Highlighted in red in attached image) using C# code. 
How could I do that?</p>

<p><img src=""https://i.stack.imgur.com/NBqAE.png"" alt=""""></p>
","<c#><azure-active-directory><azure-keyvault><azure-data-factory>","2018-08-31 13:43:53","4917","1","3","52127183","<p>If you want to get the access token with the application id, hope <a href=""https://learn.microsoft.com/en-us/azure/app-service/app-service-managed-service-identity#asal"" rel=""nofollow noreferrer"">this doc</a> could help.</p>
"
"52116811","Retrieve Azure Data Factory Service Identity Application ID","<p>I have created one Data Factory and Key Vault using C# Code, I would like to set Access Policy of Key Vault. 
For that I want data factory <em>""Service Identity Application ID""</em> (Highlighted in red in attached image) using C# code. 
How could I do that?</p>

<p><img src=""https://i.stack.imgur.com/NBqAE.png"" alt=""""></p>
","<c#><azure-active-directory><azure-keyvault><azure-data-factory>","2018-08-31 13:43:53","4917","1","3","52141376","<p>Yes you can do this from C# code. </p>

<p>Here is a quick sample code that I wrote to get the Service Identity Application ID from C# code.</p>

<p>Pre-requisite is to install the following packages from your package manager console (Tools -> NuGet Package Manager -> Package Manager Console):</p>

<pre><code>Install-Package Microsoft.Azure.Management.DataFactory -Prerelease
Install-Package Microsoft.Azure.Management.ResourceManager -Prerelease
Install-Package Microsoft.IdentityModel.Clients.ActiveDirectory
</code></pre>

<p>After the packages are installed, use code below</p>

<pre><code>using System;
using Microsoft.Rest;
using Microsoft.Azure.Management.ResourceManager;
using Microsoft.Azure.Management.DataFactory;
using Microsoft.IdentityModel.Clients.ActiveDirectory;

namespace GetDataFactory
{
    class Program
    {
        static void Main(string[] args)
        {

            // Set variables
            string tenantID = ""&lt;your tenant ID&gt;"";
            string applicationId = ""&lt;your application ID&gt;"";
            string authenticationKey = ""&lt;your authentication key for the application&gt;"";
            string subscriptionId = ""&lt;your subscription ID where the data factory resides&gt;"";
            string resourceGroup = ""&lt;your resource group where the data factory resides&gt;"";
            string dataFactoryName = ""&lt;specify the name of data factory to create. It must be globally unique.&gt;"";

            // Authenticate and create a data factory management client
            var context = new AuthenticationContext(""https://login.windows.net/"" + tenantID);
            ClientCredential cc = new ClientCredential(applicationId, authenticationKey);
            AuthenticationResult result = context.AcquireTokenAsync(""https://management.azure.com/"", cc).Result;
            ServiceClientCredentials cred = new TokenCredentials(result.AccessToken);
            var client = new DataFactoryManagementClient(cred) { SubscriptionId = subscriptionId };

            var myFactory = client.Factories.Get(resourceGroup, dataFactoryName);

            //Getting principal Id as you mentioned in question, but you can get more information from the Identity object as per your need.
            Guid? principalId = myFactory.Identity.PrincipalId;

        }
    }
}
</code></pre>

<p>Once you have all the identity information, you can update the access policy for the keyvault to give required permissions (like listing keys, get/list secrets etc.) to the application (whose Id you've highlighted in the image)</p>

<ol>
<li><p>Use KeyVaultManagementClient class -</p>

<p><a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.keyvault.keyvaultmanagementclient?view=azure-dotnet"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.keyvault.keyvaultmanagementclient?view=azure-dotnet</a></p>

<p><a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.keyvault.vaultsoperationsextensions.updateaccesspolicy?view=azure-dotnet"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.keyvault.vaultsoperationsextensions.updateaccesspolicy?view=azure-dotnet</a></p></li>
<li><p>Use Fluent API -</p>

<p>Look at this sample on Github - <a href=""https://github.com/Azure-Samples/key-vault-dotnet-manage-key-vaults"" rel=""nofollow noreferrer"">https://github.com/Azure-Samples/key-vault-dotnet-manage-key-vaults</a></p>

<pre><code>        Utilities.Log(""Authorizing the application associated with the current service principal..."");

        vault1 = vault1.Update()
                .DefineAccessPolicy()
                    .ForServicePrincipal(SdkContext.AzureCredentialsFactory.FromFile(Environment.GetEnvironmentVariable(""AZURE_AUTH_LOCATION"")).ClientId)
                    .AllowKeyAllPermissions()
                    .AllowSecretPermissions(SecretPermissions.Get)
                    .AllowSecretPermissions(SecretPermissions.List)
                    .Attach()
                .Apply();

        Utilities.Log(""Updated key vault"");
        Utilities.PrintVault(vault1);

        //============================================================
        // Update a key vault

        Utilities.Log(""Update a key vault to enable deployments and add permissions to the application..."");

        vault1 = vault1.Update()
                .WithDeploymentEnabled()
                .WithTemplateDeploymentEnabled()
                .UpdateAccessPolicy(vault1.AccessPolicies[0].ObjectId)
                    .AllowSecretAllPermissions()
                    .Parent()
                .Apply();

        Utilities.Log(""Updated key vault"");
        // Print the network security group
        Utilities.PrintVault(vault1);
</code></pre></li>
<li><p>Use Rest API</p>

<p><a href=""https://learn.microsoft.com/en-us/rest/api/keyvault/vaults/updateaccesspolicy"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/keyvault/vaults/updateaccesspolicy</a></p></li>
</ol>
"
"52116811","Retrieve Azure Data Factory Service Identity Application ID","<p>I have created one Data Factory and Key Vault using C# Code, I would like to set Access Policy of Key Vault. 
For that I want data factory <em>""Service Identity Application ID""</em> (Highlighted in red in attached image) using C# code. 
How could I do that?</p>

<p><img src=""https://i.stack.imgur.com/NBqAE.png"" alt=""""></p>
","<c#><azure-active-directory><azure-keyvault><azure-data-factory>","2018-08-31 13:43:53","4917","1","3","53289338","<p>If you want to retrieve the app id of an existing ADF, you need to do 2 trips.  </p>

<p>The first is to retrieve the service identity for resource manager.  @rohit's first code block does this in c#.  This retrieves the object ID of the principal, not the app ID which is an attribute of that object.</p>

<p>The second is to retrieve the application ID from active directory via RM.  You can then use this to assign access policy.  For example, in powershell you would do this:</p>

<p>First Step:</p>

<pre><code>$principal = (Get-AzureRmDataFactoryV2 -ResourceGroupName ""yourRG"" -Name yourADF).identity.PrincipalId
</code></pre>

<p>then second step... </p>

<pre><code>$appId = (Get-AzureRmADServicePrincipal -ObjectId $principal).ApplicationId
</code></pre>

<p>The c# equivilent should be easy to figure out from this.</p>
"
"52106028","Data migration from Oracle and transformation from XML to JSon","<p>Can Azure Data Factory transform a string from Oracle (an xml) to a JSon Object  to save in a collection in Cosmos as a Object?
I've tried, but I only get a string (json object) as a simple attribute in Cosmos DB.</p>
","<oracle><azure><azure-cosmosdb><azure-data-factory>","2018-08-30 22:11:39","144","1","1","52107628","<p>Per my experience, azure data factory can transfer data, but will not help you do some serialization steps. So, you data stored in cosmos db maybe like <code>""info"": ""{\""Id\"":\""1\"",\""Name\"":\""AA\"",\""Address\"":\""HQ - Main Line\""}""</code>. </p>

<p>To handle with this solution,  I suggest you using <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-cosmos-db-triggered-function"" rel=""nofollow noreferrer"">Azure Function Cosmos DB Trigger</a>. Please refer to my code:</p>

<pre><code>using System;
using System.Collections.Generic;
using Microsoft.Azure.Documents;
using Microsoft.Azure.Documents.Client;
using Microsoft.Azure.WebJobs;
using Microsoft.Azure.WebJobs.Host;
using Newtonsoft.Json.Linq;

namespace ProcessJson
{
    public class Class1
    {
        [FunctionName(""DocumentUpdates"")]
        public static void Run(
        [CosmosDBTrigger(databaseName:""db"",collectionName: ""item"", ConnectionStringSetting = ""CosmosDBConnection"",LeaseCollectionName = ""leases"",
            CreateLeaseCollectionIfNotExists = true)]
        IReadOnlyList&lt;Document&gt; documents,
        TraceWriter log)
        {
            log.Verbose(""Start........."");
            String endpointUrl = ""https://***.documents.azure.com:443/"";
            String authorizationKey = ""***"";
            String databaseId = ""db"";
            String collectionId = ""import"";

            DocumentClient client = new DocumentClient(new Uri(endpointUrl), authorizationKey);

            for (int i = 0; i &lt; documents.Count; i++)
            {
                Document doc = documents[i];
                String info = doc.GetPropertyValue&lt;String&gt;(""info"");
                JObject o = JObject.Parse(info);

                doc.SetPropertyValue(""info"", o);

                client.ReplaceDocumentAsync(UriFactory.CreateDocumentUri(databaseId, collectionId, doc.Id), doc);

                log.Verbose(""Update document Id "" + doc.Id);
            }
        }
    }
}
</code></pre>

<p>In addition,you could refer to my previous case:<a href=""https://stackoverflow.com/questions/50272509/azure-cosmos-db-sql-how-to-unescape-inner-json-property/50364301#50364301"">Azure Cosmos DB SQL - how to unescape inner json property</a></p>

<p>Hope it helps you.</p>
"
"52103543","Pass parameters from one notebook to another","<p>I have two notebooks in python. </p>

<p>The first one checks if a file exits in the data lake. I want to return a Boolean from here and the filePath if it exits. </p>

<p>The next notebook will then uses these Params as in input. How is this possible?</p>

<p>Also could I use a condition IF in the pipeline to check the returned Boolean?</p>

<p>Kind of new to Azure ADF</p>
","<python><azure><jupyter-notebook><azure-data-factory>","2018-08-30 18:46:44","1359","0","1","52133455","<p>One method to pass messages between separate python scripts or jupyter notebooks is to use the <a href=""https://pyzmq.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">pyzmq</a> library. Run pairserver in one notebook and pairclient in another. You will see messages being passed from one to the other. This does add an extra dependency to your code, but pyzmq is a mature package.</p>

<p>pairserver.ipynb</p>

<pre><code>#!/usr/bin/python3
import zmq
import random
import time

port = '5556'
context = zmq.Context()
socket = context.socket(zmq.PAIR)
socket.bind('tcp://*:%s' % port)

while True:
    socket.send(b'Server message to client')
    msg = socket.recv()
    print(msg)
    time.sleep(1)
</code></pre>

<p>pairclient.ipynb</p>

<pre><code>#!/usr/bin/python3
import zmq
import random
import sys
import time

port = '5556'
context = zmq.Context()
socket = context.socket(zmq.PAIR)
socket.connect(""tcp://localhost:%s"" % port)

while True:
    msg = socket.recv()
    print(msg)
    socket.send_string(""client message to server"")
    time.sleep(1)
</code></pre>
"
"52097236","Dynamic Azure Data Factory v2 pipelines","<p>So we've got a factory with ~400 datasets and ~200 pipelines and it's getting unwieldy. Focusing on copying from sql source to blob sink. Since we are copying to blob the schema has no impact. I'd like to have one dataset for each source, one dataset for each blob account and one pipeline for each combination of source/blob account, dynamically feeding it the config from a lookup.</p>

<p>We've successfully developed a pipeline that uses dummy datasets for source and sink. It works if you feed it a query, container name and folder name.</p>

<pre><code>{
    ""name"": ""pipeline1"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""DynamicCopy"",
                ""type"": ""Copy"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""SqlSource"",
                        ""sqlReaderQuery"": ""select 1 a""
                    },
                    ""sink"": {
                        ""type"": ""BlobSink""
                    },
                    ""enableStaging"": false,
                    ""dataIntegrationUnits"": 0
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""AzureSql"",
                        ""type"": ""DatasetReference""
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""AzureBlob"",
                        ""type"": ""DatasetReference"",
                        ""parameters"": {
                            ""container"": ""raw-test"",
                            ""folder"": ""test""
                        }
                    }
                ]
            }
        ]
    }
}
</code></pre>

<p>When we put a lookup before it and wrap it in a foreach, it stops working. With the not so helpful </p>

<blockquote>
<pre><code>""errorCode"": ""400"",
""message"": ""Activity failed because an inner activity failed"",
""failureType"": ""UserError"",
""target"": ""ForEach""
</code></pre>
</blockquote>

<p>The lookup stored procedure <code>[dbo].[adfdynamic]</code> is not actually referred to in the foreach yet:</p>

<pre><code>create proc adfdynamic as
select 'raw-test' container, 'test_a' folder, 'select 1 a, 2 b' 
UNION ALL    
select 'raw-test' container, 'test_b' folder, 'select 3 c, 2 d' 
</code></pre>

<p>So what I desired behaviour is:</p>

<ul>
<li>one blob in raw-test@..myblob.../test_a/out.dsv with content <code>{'a,b','1,2'}</code></li>
<li>one blob in raw-test@..myblob.../test_b/out.dsv with content <code>{'c,d','3,2'}</code></li>
</ul>

<p>sql dataset:</p>

<pre><code>{
    ""name"": ""AzureSql"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""Dest"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""AzureSqlTable"",
        ""structure"": [
            {
                ""name"": ""CustomerKey"",
                ""type"": ""Int32""
            },
            {
                ""name"": ""Name"",
                ""type"": ""String""
            }
        ],
        ""typeProperties"": {
            ""tableName"": ""[dbo].[DimCustomer]""
        }
    }
}
</code></pre>

<p>blob dataset:</p>

<pre><code>{
    ""name"": ""AzureBlob"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureStorage1"",
            ""type"": ""LinkedServiceReference""
        },
        ""parameters"": {
            ""container"": {
                ""type"": ""String""
            },
            ""folder"": {
                ""type"": ""String""
            }
        },
        ""type"": ""AzureBlob"",
        ""typeProperties"": {
            ""format"": {
                ""type"": ""TextFormat"",
                ""columnDelimiter"": "","",
                ""treatEmptyAsNull"": false,
                ""skipLineCount"": 0,
                ""firstRowAsHeader"": false
            },
            ""fileName"": {
                ""value"": ""@{dataset().folder}/out.dsv"",
                ""type"": ""Expression""
            },
            ""folderPath"": {
                ""value"": ""@dataset().container"",
                ""type"": ""Expression""
            }
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p>and the non-working dynamic pipeline:</p>

<pre><code>{
    ""name"": ""Copy"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""ForEach"",
                ""type"": ""ForEach"",
                ""dependsOn"": [
                    {
                        ""activity"": ""Lookup"",
                        ""dependencyConditions"": [
                            ""Succeeded""
                        ]
                    }
                ],
                ""typeProperties"": {
                    ""items"": {
                        ""value"": ""@activity('Lookup').output.value"",
                        ""type"": ""Expression""
                    },
                    ""activities"": [
                        {
                            ""name"": ""Copy"",
                            ""type"": ""Copy"",
                            ""policy"": {
                                ""timeout"": ""7.00:00:00"",
                                ""retry"": 0,
                                ""retryIntervalInSeconds"": 30,
                                ""secureOutput"": false,
                                ""secureInput"": false
                            },
                            ""typeProperties"": {
                                ""source"": {
                                    ""type"": ""SqlSource"",
                                    ""sqlReaderQuery"": {
                                        ""value"": ""select 1 a, 2 b from dest"",
                                        ""type"": ""Expression""
                                    }
                                },
                                ""sink"": {
                                    ""type"": ""BlobSink""
                                },
                                ""enableStaging"": false,
                                ""dataIntegrationUnits"": 0
                            },
                            ""inputs"": [
                                {
                                    ""referenceName"": ""AzureSql"",
                                    ""type"": ""DatasetReference""
                                }
                            ],
                            ""outputs"": [
                                {
                                    ""referenceName"": ""AzureBlob"",
                                    ""type"": ""DatasetReference"",
                                    ""parameters"": {
                                        ""container"": {
                                            ""value"": ""raw-test"",
                                            ""type"": ""Expression""
                                        },
                                        ""folder"": {
                                            ""value"": ""folder"",
                                            ""type"": ""Expression""
                                        }
                                    }
                                }
                            ]
                        }
                    ]
                }
            },
            {
                ""name"": ""Lookup"",
                ""type"": ""Lookup"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                }
            }
        ]
    }
}
</code></pre>

<p>Apologies about the formatting. too much code in one message?</p>
","<azure><azure-data-factory>","2018-08-30 12:35:18","891","2","2","52107889","<ol>
<li>In you lookup activity, please check whether your firstRowOnly property. Is it false or True? By default, it is true. </li>
<li>In the UI, you could set a breakpoint to debug your lookup activity. Then you could see whether the output is what you want. </li>
</ol>

<p><a href=""https://i.stack.imgur.com/5xpFT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5xpFT.png"" alt=""enter image description here""></a></p>
"
"52097236","Dynamic Azure Data Factory v2 pipelines","<p>So we've got a factory with ~400 datasets and ~200 pipelines and it's getting unwieldy. Focusing on copying from sql source to blob sink. Since we are copying to blob the schema has no impact. I'd like to have one dataset for each source, one dataset for each blob account and one pipeline for each combination of source/blob account, dynamically feeding it the config from a lookup.</p>

<p>We've successfully developed a pipeline that uses dummy datasets for source and sink. It works if you feed it a query, container name and folder name.</p>

<pre><code>{
    ""name"": ""pipeline1"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""DynamicCopy"",
                ""type"": ""Copy"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""SqlSource"",
                        ""sqlReaderQuery"": ""select 1 a""
                    },
                    ""sink"": {
                        ""type"": ""BlobSink""
                    },
                    ""enableStaging"": false,
                    ""dataIntegrationUnits"": 0
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""AzureSql"",
                        ""type"": ""DatasetReference""
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""AzureBlob"",
                        ""type"": ""DatasetReference"",
                        ""parameters"": {
                            ""container"": ""raw-test"",
                            ""folder"": ""test""
                        }
                    }
                ]
            }
        ]
    }
}
</code></pre>

<p>When we put a lookup before it and wrap it in a foreach, it stops working. With the not so helpful </p>

<blockquote>
<pre><code>""errorCode"": ""400"",
""message"": ""Activity failed because an inner activity failed"",
""failureType"": ""UserError"",
""target"": ""ForEach""
</code></pre>
</blockquote>

<p>The lookup stored procedure <code>[dbo].[adfdynamic]</code> is not actually referred to in the foreach yet:</p>

<pre><code>create proc adfdynamic as
select 'raw-test' container, 'test_a' folder, 'select 1 a, 2 b' 
UNION ALL    
select 'raw-test' container, 'test_b' folder, 'select 3 c, 2 d' 
</code></pre>

<p>So what I desired behaviour is:</p>

<ul>
<li>one blob in raw-test@..myblob.../test_a/out.dsv with content <code>{'a,b','1,2'}</code></li>
<li>one blob in raw-test@..myblob.../test_b/out.dsv with content <code>{'c,d','3,2'}</code></li>
</ul>

<p>sql dataset:</p>

<pre><code>{
    ""name"": ""AzureSql"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""Dest"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""AzureSqlTable"",
        ""structure"": [
            {
                ""name"": ""CustomerKey"",
                ""type"": ""Int32""
            },
            {
                ""name"": ""Name"",
                ""type"": ""String""
            }
        ],
        ""typeProperties"": {
            ""tableName"": ""[dbo].[DimCustomer]""
        }
    }
}
</code></pre>

<p>blob dataset:</p>

<pre><code>{
    ""name"": ""AzureBlob"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureStorage1"",
            ""type"": ""LinkedServiceReference""
        },
        ""parameters"": {
            ""container"": {
                ""type"": ""String""
            },
            ""folder"": {
                ""type"": ""String""
            }
        },
        ""type"": ""AzureBlob"",
        ""typeProperties"": {
            ""format"": {
                ""type"": ""TextFormat"",
                ""columnDelimiter"": "","",
                ""treatEmptyAsNull"": false,
                ""skipLineCount"": 0,
                ""firstRowAsHeader"": false
            },
            ""fileName"": {
                ""value"": ""@{dataset().folder}/out.dsv"",
                ""type"": ""Expression""
            },
            ""folderPath"": {
                ""value"": ""@dataset().container"",
                ""type"": ""Expression""
            }
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p>and the non-working dynamic pipeline:</p>

<pre><code>{
    ""name"": ""Copy"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""ForEach"",
                ""type"": ""ForEach"",
                ""dependsOn"": [
                    {
                        ""activity"": ""Lookup"",
                        ""dependencyConditions"": [
                            ""Succeeded""
                        ]
                    }
                ],
                ""typeProperties"": {
                    ""items"": {
                        ""value"": ""@activity('Lookup').output.value"",
                        ""type"": ""Expression""
                    },
                    ""activities"": [
                        {
                            ""name"": ""Copy"",
                            ""type"": ""Copy"",
                            ""policy"": {
                                ""timeout"": ""7.00:00:00"",
                                ""retry"": 0,
                                ""retryIntervalInSeconds"": 30,
                                ""secureOutput"": false,
                                ""secureInput"": false
                            },
                            ""typeProperties"": {
                                ""source"": {
                                    ""type"": ""SqlSource"",
                                    ""sqlReaderQuery"": {
                                        ""value"": ""select 1 a, 2 b from dest"",
                                        ""type"": ""Expression""
                                    }
                                },
                                ""sink"": {
                                    ""type"": ""BlobSink""
                                },
                                ""enableStaging"": false,
                                ""dataIntegrationUnits"": 0
                            },
                            ""inputs"": [
                                {
                                    ""referenceName"": ""AzureSql"",
                                    ""type"": ""DatasetReference""
                                }
                            ],
                            ""outputs"": [
                                {
                                    ""referenceName"": ""AzureBlob"",
                                    ""type"": ""DatasetReference"",
                                    ""parameters"": {
                                        ""container"": {
                                            ""value"": ""raw-test"",
                                            ""type"": ""Expression""
                                        },
                                        ""folder"": {
                                            ""value"": ""folder"",
                                            ""type"": ""Expression""
                                        }
                                    }
                                }
                            ]
                        }
                    ]
                }
            },
            {
                ""name"": ""Lookup"",
                ""type"": ""Lookup"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                }
            }
        ]
    }
}
</code></pre>

<p>Apologies about the formatting. too much code in one message?</p>
","<azure><azure-data-factory>","2018-08-30 12:35:18","891","2","2","52527096","<p>Not exactly an answer to your question, but something I did to make life simpler was to create a Dataset called GenericBlob.  This had 2 parameters container and path.
This may help simplify what you're doing.  I too used to have 20 blob datasets, now I have one ...   (this is assuming the blobs are in the same storage account).</p>
"
"52081708","convert Csv file into Xml on Azure data lake store using Azure Powershell runbook","<p>I  want to convert CSV file into XML on Azure data lake store using Azure <strong>Powershell</strong>.
I was using this code on <strong>Runbook of azure automation</strong>
and it worked fine but <strong>No XML is being generated</strong></p>
<pre><code>$cred = Get-AutomationPSCredential -Name 'xyz'
$subscriptionName = 'Pay-As-You-Go'
$null = Add-AzureRmAccount -Credential $cred
Select-AzureRmSubscription -SubscriptionName $subscriptionName | Out-Null 
$adla = &quot;TestAzure&quot;
$obj = Get-AzureRmDataLakeStoreItem -AccountName $adla -Path &quot;/custom/logile/masters/Test/Test.csv&quot; 
 $xmlobj = ConvertTo-Xml -InputObject $obj -As 'string' 
$xmlobj | Out-File -FilePath &quot;/custom/logile/masters/Test/xmlOut.xml&quot;
</code></pre>
<p>This code is working fine till &quot;$obj= Get&quot; here, but I am not able to write into Azure data lake.
Is there any way that I can use Object to write the file in Azure data lake.</p>
","<powershell><azure-data-factory><azure-data-lake><azure-runbook>","2018-08-29 15:59:13","411","5","1","57875201","<p>Thanks for reaching out! </p>

<p>You were trying to export the xml file using Out-File cmdlet to an nonexistent path <code>/custom/logile/masters/Test</code> in current instance of the Azure Automation execution. And you were not uploading the xml file to a Data Lake Store.</p>

<p>To accomplish your requirement you would have to use the below commands by replacing last line in your runbook (i.e., last line is <code>$xmlobj | Out-File -FilePath ""/custom/logile/masters/Test/xmlOut.xml""</code>)</p>

<pre><code>$xmlobj | Out-File -FilePath ""$env:temp/xmlOut.xml""
Import-AzureRmDataLakeStoreItem -AccountName $adla -Path ""$env:temp/xmlOut.xml"" -Destination ""/custom/logile/masters/Test/xmlOut.xml""
</code></pre>

<p>I have tested it successfully. If interested you may check below screenshots for illustration.</p>

<p><a href=""https://i.stack.imgur.com/Zbrbv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Zbrbv.png"" alt=""Shell power runbook""></a></p>

<p><a href=""https://i.stack.imgur.com/TL4v4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TL4v4.png"" alt=""Test""></a></p>

<p><a href=""https://i.stack.imgur.com/NA0R6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NA0R6.png"" alt=""Data Explorer""></a></p>

<p>Note that the first few lines of my runbook in the above screenshot are a bit different from yours because I have given Automation Account RunAs account the permission to Data Lake store destination folder. So you may ignore this part.</p>

<p>Hope this helps!! Cheers!!</p>
"
"52078162","Permission Denied when running Azure Batch Command on Linux VM","<p>I am trying to run a task using a custom activity on Azure Batch. My custom activity looks like the following (edited for brevity):</p>

<pre><code>  ""typeProperties"": {
       ""command"": ""/bin/bash -c \""run.sh\""\n"",
           ...
           ""userIdentity"": {
               ""autoUser"": {
                   ""scope"": ""pool"",
                   ""elevationLevel"": ""admin""
                }
            }
        }
</code></pre>

<p>My run.sh script simply contains:</p>

<pre><code>docker run hello-world
</code></pre>

<p>When I run this I get the following error in stderr.txt:</p>

<pre><code>docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.37/containers/create: dial unix /var/run/docker.sock: connect: permission denied.
See 'docker run --help'.
</code></pre>

<p>I thought running as admin might get me access to the socket required to run docker however this does not seem to be the case. How can I run docker commands as an Azure Batch command without permission errors? </p>

<p>P.S. I would like a solution that can be done from the portal ideally by editing config JSON</p>

<p>Thanks for the help!</p>
","<azure><azure-data-factory><azure-batch>","2018-08-29 13:03:48","1762","0","1","52090100","<p>In the Azure portal, when you finish all the preparation before creating the task, such as pools, jobs, etc. Then you can create the tasks to do what you want.</p>

<p>For your issue, when you create the task, the screenshot like below:</p>

<p><a href=""https://i.stack.imgur.com/her03.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/her03.png"" alt=""enter image description here""></a></p>

<p>If you choose the <strong>User identity</strong> with <strong>Pool autouser, Admin</strong>, then you will get success. And the output will like below:</p>

<p><a href=""https://i.stack.imgur.com/vw0bz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vw0bz.png"" alt=""enter image description here""></a></p>

<p>But if you choose others, you will get the error as you posted and the output like below:</p>

<p><a href=""https://i.stack.imgur.com/qUgwS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qUgwS.png"" alt=""enter image description here""></a></p>

<p>I just test the Batch task in the Azure portal. Hope this will help you.</p>
"
"52070651","filter activity in azure data factory","<p>I am trying to make a pipeline which filters the input from the source but can not find a way to store the output from the filter in some table or any file.</p>

<p>this is the error if I try to save the output from the filter activity:
<img src=""https://i.stack.imgur.com/vSxgT.png"" alt=""""></p>

<p>this is the expression as an input for copy activity
<img src=""https://i.stack.imgur.com/UCNeB.png"" alt=""""></p>

<p>This is the same problem I face even when I try to store the output from metadata activity in a table or any file</p>
","<azure><azure-data-factory>","2018-08-29 06:17:25","4422","0","1","52071013","<p>Please check the output of copy filter activity in the output tab, I think <code>@activity('MyFilterActivity').output.value</code> would be an array.</p>

<p>The query does not support an array as input. You can use <code>@activity('MyFilterActivity').output.value[0]</code> or <code>foreach</code> activity.</p>
"
"52060606","Copy Data - How to skip Identity columns","<p>I'm designing a Copy Data task where the Sink SQL Server table contains an Identity column. The Copy Data task always wants me to map that column when, in my opinion, it should just not include the column in the list of columns to map. Does anyone know how I can get the ADF Copy Data task to ignore Sink Identity columns?</p>
","<azure-data-factory>","2018-08-28 14:44:13","1804","3","2","52060941","<p>If you are using copy data tool, and in your sql server, the ID is set as auto-increment, then it should not show out at the mapping step. Please tell us if it is not the case.</p>

<p>If you are using the create pipeline/dataset, you could just go to the sink dataset schema tab, remove the id column. And then go to the copy activity mapping tab, click import schemes again. ID column should has disappeared now.</p>
"
"52060606","Copy Data - How to skip Identity columns","<p>I'm designing a Copy Data task where the Sink SQL Server table contains an Identity column. The Copy Data task always wants me to map that column when, in my opinion, it should just not include the column in the list of columns to map. Does anyone know how I can get the ADF Copy Data task to ignore Sink Identity columns?</p>
","<azure-data-factory>","2018-08-28 14:44:13","1804","3","2","64936950","<p>You could include a SET_IDENTITY_INSERT_ON statement for the given table before executing the copy step. After completed, set it to OFF.</p>
"
"52059249","Azure Blob Storage as Source and FTP as destination","<p>Is there anyway I can transfer txt files from my Azure Blob Storage to a FTP directly, going serverless?
If possible using SSIS or Azure Data Factory.</p>

<p>Thanks!</p>
","<azure><ssis><ftp><azure-data-factory><azure-blob-storage>","2018-08-28 13:34:27","845","0","3","52060831","<p>You could use SSIS. Azure data factory copy activity doesn’t support ftp as sink.</p>
"
"52059249","Azure Blob Storage as Source and FTP as destination","<p>Is there anyway I can transfer txt files from my Azure Blob Storage to a FTP directly, going serverless?
If possible using SSIS or Azure Data Factory.</p>

<p>Thanks!</p>
","<azure><ssis><ftp><azure-data-factory><azure-blob-storage>","2018-08-28 13:34:27","845","0","3","52062676","<p>SSIS has a lot of connectors that can talk directly to AZURE storage. As for FTP, you may have to use a third party software (WinSCP) that can accomplish uploading of the file to FTP (if the built in FTP Task doesnt accomplish it already). If you are looking to go directly from Azure to FTP, you may have to rely on custom C# code. I am not even sure if that is possible. </p>
"
"52059249","Azure Blob Storage as Source and FTP as destination","<p>Is there anyway I can transfer txt files from my Azure Blob Storage to a FTP directly, going serverless?
If possible using SSIS or Azure Data Factory.</p>

<p>Thanks!</p>
","<azure><ssis><ftp><azure-data-factory><azure-blob-storage>","2018-08-28 13:34:27","845","0","3","52105550","<p>you can use <a href=""https://azure.microsoft.com/en-us/services/logic-apps/"" rel=""nofollow noreferrer"">Azure Logic App</a>:</p>

<ul>
<li><a href=""https://learn.microsoft.com/en-us/azure/connectors/connectors-create-api-azureblobstorage"" rel=""nofollow noreferrer"">Connectors to blob storage</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/connectors/connectors-create-api-ftp"" rel=""nofollow noreferrer"">Connectors to FTP</a></li>
</ul>

<p>A simple logic app to push a blob to a FTP would be:</p>

<p><a href=""https://i.stack.imgur.com/NVz5N.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NVz5N.jpg"" alt=""Azure LogicApp - Blob to FTP""></a></p>
"
"52059160","ADF not picking up correct source columns in Copy Data task","<p>In my Copy Data source, I am using a query as follows:</p>

<p><a href=""https://i.stack.imgur.com/Ac7I2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ac7I2.png"" alt=""enter image description here""></a></p>

<p>You can see that I'm adding two additional columns to the result set that are not part of the base table. When I go to the Mapping tab, those two columns do not appear in the source. I assume they should?</p>
","<azure-data-factory>","2018-08-28 13:29:37","95","0","1","52061046","<p>This is because your query is an expression rather than a literal string. In this case, if you go to mapping tab and click import schemes, it actually won’t call API to retrieve your schema because the expressions need to be evaluated at runtime. So the UI will just use the schema you specified at source dataset schema tab. In your case, you could add your columns there.</p>
"
"52049576","How to create dependency between activities of Pipeline for Azure Data Factory in Python","<p>In an Azure DataFactory pipeline, I try to make two CopyActivities run sequentially, i.e. first one copies data from blob to SQL table, THEN the second one copies the SQL table to another database. </p>

<p>I tried below code, but the result pipeline has no dependency built upon activities (checked in Azure UI from workflow diagram and JSON). When I run the pipeline, I got error message like below:
<em>""ErrorResponseException: The template validation failed: 'The 'runAfter' property of template action 'my second activity nameScope' at line '1' and column '22521' contains non-existent action. balababla....""</em></p>

<p>Once I manually add the dependency in Azure UI, I can run the pipeline successfully.</p>

<p>I appreciate if someone could point me to example code (Python/C#/Powershell) or document.
My Python code:</p>

<pre><code>    def createDataFactoryRectStage(self,  
                               aPipelineName, aActivityStageName, aActivityAcquireName,
                               aRectFileName, aRectDSName,
                               aStageTableName, aStageDSName,
                               aAcquireTableName, aAcquireDSName):
    adf_client = self.__getAdfClient()

    ds_blob = AzureBlobDataset(linked_service_name = LinkedServiceReference(AZURE_DATAFACTORY_LS_BLOB_RECT), 
                               folder_path=PRJ_AZURE_BLOB_PATH_RECT, 
                               file_name = aRectFileName,
                               format = {""type"": ""TextFormat"",
                                         ""columnDelimiter"": "","",
                                         ""rowDelimiter"": """",
                                         ""nullValue"": ""\\N"",
                                         ""treatEmptyAsNull"": ""true"",
                                         ""firstRowAsHeader"": ""true"",
                                         ""quoteChar"": ""\"""",})    
    adf_client.datasets.create_or_update(AZURE_RESOURCE_GROUP, AZURE_DATAFACTORY, aRectDSName, ds_blob)

    ds_stage= AzureSqlTableDataset(linked_service_name = LinkedServiceReference(AZURE_DATAFACTORY_LS_SQLDB_STAGE), 
                                   table_name='[dbo].[' + aStageTableName + ']')      
    adf_client.datasets.create_or_update(AZURE_RESOURCE_GROUP, AZURE_DATAFACTORY, aStageDSName, ds_stage)

    ca_blob_to_stage = CopyActivity(aActivityStageName, 
                                    inputs=[DatasetReference(aRectDSName)], 
                                    outputs=[DatasetReference(aStageDSName)], 
                                    source= BlobSource(), 
                                    sink= SqlSink(write_batch_size = AZURE_SQL_WRITE_BATCH_SIZE))

    ds_acquire= AzureSqlTableDataset(linked_service_name = LinkedServiceReference(AZURE_DATAFACTORY_LS_SQLDB_ACQUIRE), 
                                     table_name='[dbo].[' + aAcquireTableName + ']')      
    adf_client.datasets.create_or_update(AZURE_RESOURCE_GROUP, AZURE_DATAFACTORY, aAcquireDSName, ds_acquire)
    dep = ActivityDependency(ca_blob_to_stage, dependency_conditions =[DependencyCondition('Succeeded')])

    ca_stage_to_acquire = CopyActivity(aActivityAcquireName, 
                                       inputs=[DatasetReference(aStageDSName)], 
                                       outputs=[DatasetReference(aAcquireDSName)], 
                                       source= SqlSource(), 
                                       sink= SqlSink(write_batch_size = AZURE_SQL_WRITE_BATCH_SIZE),
                                       depends_on=[dep])

    p_obj = PipelineResource(activities=[ca_blob_to_stage, ca_stage_to_acquire], parameters={})

    return adf_client.pipelines.create_or_update(AZURE_RESOURCE_GROUP, AZURE_DATAFACTORY, aPipelineName, p_obj)
</code></pre>
","<python><dependencies><azure-data-factory>","2018-08-28 03:15:48","1073","2","2","52053112","<p>Here is an example in <code>C#</code> which basically does <code>Chaining activities</code> and chain activities in a sequence within a pipeline. Remember in ADFV1 we had to configure the output of an activity as an input of another activity to chain them and make them depend on each other.</p>

<p>Pipeline code snippet (notice the dependsOn property which makes sure that the second activity runs after the first activity has completed it's run successfully) - </p>

<pre><code>static PipelineResource PipelineDefinition(DataFactoryManagementClient client) {
 Console.WriteLine(""Creating pipeline "" + pipelineName + ""..."");
 PipelineResource resource = new PipelineResource {
   Activities = new List &lt; Activity &gt; {
    new CopyActivity {
     Name = copyFromBlobToSQLActivity,
      Inputs = new List &lt; DatasetReference &gt; {
       new DatasetReference {
        ReferenceName = blobSourceDatasetName
       }
      },
      Outputs = new List&lt;DatasetReference&gt;
      {
       new DatasetReference {
        ReferenceName = sqlDatasetName
       }
      },
      Source = new BlobSource {},
      Sink = new SqlSink {}
    },
    new CopyActivity {
     Name = copyToSQLServerActivity,
      Inputs = new List &lt; DatasetReference &gt; {
       new DatasetReference {
        ReferenceName = sqlDatasetName
       }
      },
      Outputs = new List&lt;DatasetReference&gt;
      {
       new DatasetReference {
        ReferenceName = sqlDestinationDatasetName
       }
      },
      Source = new SqlSource {},
      Sink = new SqlSink {},
      DependsOn = new List &lt; ActivityDependency &gt; {
       new ActivityDependency {
        Activity = copyFromBlobToSQLActivity,
         DependencyConditions = new List &lt; String &gt; {
          ""Succeeded""
         }
       }
      }
    }
   }
 };
 Console.WriteLine(SafeJsonConvert.SerializeObject(resource, client.SerializationSettings));
 return resource;
}
</code></pre>

<p>Do check the ADFV2 tutorial <a href=""https://opdhsblobprod01.blob.core.windows.net/contents/4a6d75bb3af747de838e6ccc97c5d978/1b9c613d0b759aa54550eb176ff987cd?sv=2015-04-05&amp;sr=b&amp;sig=VH6By15Uu2Qj70h2gL%2B%2F8%2B5rrnxIzVw6Ytv27blMjks%3D&amp;st=2018-08-28T07%3A38%3A02Z&amp;se=2018-08-29T07%3A48%3A02Z&amp;sp=r"" rel=""nofollow noreferrer"">here</a> for a comprehensive explanation and more scenarios.</p>
"
"52049576","How to create dependency between activities of Pipeline for Azure Data Factory in Python","<p>In an Azure DataFactory pipeline, I try to make two CopyActivities run sequentially, i.e. first one copies data from blob to SQL table, THEN the second one copies the SQL table to another database. </p>

<p>I tried below code, but the result pipeline has no dependency built upon activities (checked in Azure UI from workflow diagram and JSON). When I run the pipeline, I got error message like below:
<em>""ErrorResponseException: The template validation failed: 'The 'runAfter' property of template action 'my second activity nameScope' at line '1' and column '22521' contains non-existent action. balababla....""</em></p>

<p>Once I manually add the dependency in Azure UI, I can run the pipeline successfully.</p>

<p>I appreciate if someone could point me to example code (Python/C#/Powershell) or document.
My Python code:</p>

<pre><code>    def createDataFactoryRectStage(self,  
                               aPipelineName, aActivityStageName, aActivityAcquireName,
                               aRectFileName, aRectDSName,
                               aStageTableName, aStageDSName,
                               aAcquireTableName, aAcquireDSName):
    adf_client = self.__getAdfClient()

    ds_blob = AzureBlobDataset(linked_service_name = LinkedServiceReference(AZURE_DATAFACTORY_LS_BLOB_RECT), 
                               folder_path=PRJ_AZURE_BLOB_PATH_RECT, 
                               file_name = aRectFileName,
                               format = {""type"": ""TextFormat"",
                                         ""columnDelimiter"": "","",
                                         ""rowDelimiter"": """",
                                         ""nullValue"": ""\\N"",
                                         ""treatEmptyAsNull"": ""true"",
                                         ""firstRowAsHeader"": ""true"",
                                         ""quoteChar"": ""\"""",})    
    adf_client.datasets.create_or_update(AZURE_RESOURCE_GROUP, AZURE_DATAFACTORY, aRectDSName, ds_blob)

    ds_stage= AzureSqlTableDataset(linked_service_name = LinkedServiceReference(AZURE_DATAFACTORY_LS_SQLDB_STAGE), 
                                   table_name='[dbo].[' + aStageTableName + ']')      
    adf_client.datasets.create_or_update(AZURE_RESOURCE_GROUP, AZURE_DATAFACTORY, aStageDSName, ds_stage)

    ca_blob_to_stage = CopyActivity(aActivityStageName, 
                                    inputs=[DatasetReference(aRectDSName)], 
                                    outputs=[DatasetReference(aStageDSName)], 
                                    source= BlobSource(), 
                                    sink= SqlSink(write_batch_size = AZURE_SQL_WRITE_BATCH_SIZE))

    ds_acquire= AzureSqlTableDataset(linked_service_name = LinkedServiceReference(AZURE_DATAFACTORY_LS_SQLDB_ACQUIRE), 
                                     table_name='[dbo].[' + aAcquireTableName + ']')      
    adf_client.datasets.create_or_update(AZURE_RESOURCE_GROUP, AZURE_DATAFACTORY, aAcquireDSName, ds_acquire)
    dep = ActivityDependency(ca_blob_to_stage, dependency_conditions =[DependencyCondition('Succeeded')])

    ca_stage_to_acquire = CopyActivity(aActivityAcquireName, 
                                       inputs=[DatasetReference(aStageDSName)], 
                                       outputs=[DatasetReference(aAcquireDSName)], 
                                       source= SqlSource(), 
                                       sink= SqlSink(write_batch_size = AZURE_SQL_WRITE_BATCH_SIZE),
                                       depends_on=[dep])

    p_obj = PipelineResource(activities=[ca_blob_to_stage, ca_stage_to_acquire], parameters={})

    return adf_client.pipelines.create_or_update(AZURE_RESOURCE_GROUP, AZURE_DATAFACTORY, aPipelineName, p_obj)
</code></pre>
","<python><dependencies><azure-data-factory>","2018-08-28 03:15:48","1073","2","2","62844521","<p>Just in case anyone comes to the same problem as this old question like I did, there is a subtle bug in the python code</p>
<p>changing dep to use the activity name, not a reference to the activity object made it work for me.</p>
<pre><code>dep = ActivityDependency(aActivityStageName, dependency_conditions =[DependencyCondition('Succeeded')])
</code></pre>
"
"52014916","Enumerate blob names in Azure Data Factory v2","<p>I need to enumerate all the blob names that sit in an Azure Blobs container and dump the list to a file in another blob storage.</p>

<p>The part that I cannot master is the enumeration.
Thanks.</p>
","<azure-data-factory>","2018-08-25 07:13:17","5101","5","1","52014968","<p>Get metadata activity is what you want.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity</a></p>

<ol>
<li><p>Please use childItems to get all the files. And then use a foreach to iterate the childItems
<a href=""https://i.stack.imgur.com/QKGmf.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/QKGmf.png"" alt=""enter image description here""></a></p></li>
<li><p>Inside the for each activity, you may want to check if each item is a file. You could use if activity and the following expression. 
<a href=""https://i.stack.imgur.com/I6kZa.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/I6kZa.png"" alt=""enter image description here""></a></p></li>
<li><p>Then in the ""If true"" activity, assume you want to copy data, you could use @item().name to get each of your file name. 
<a href=""https://i.stack.imgur.com/GiwHl.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/GiwHl.png"" alt=""enter image description here""></a></p></li>
</ol>

<p>You could find more documentations with this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-portal"" rel=""noreferrer"">link</a>. </p>
"
"52005554","Datafactory V2 Replace ""\N"" null value with blank","<p>Presently within Azure Datafactory V2, the default null value for flat file output is set as ""\N"", which causes the downstream processes to fail as datafactroy is creating flat files with a float, int or decimal value set to ""\N"".</p>

<p>Within the advanced features of the datafactory datasets, we can set the null value to a certain character: </p>

<p><a href=""https://i.stack.imgur.com/p3lrN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p3lrN.png"" alt=""enter image description here""></a> </p>

<p>What value can I set this to so as to permit simply an empty space instead of a "" "", blank space or other character that would cause issues?</p>
","<azure><null><azure-data-factory>","2018-08-24 13:28:45","5619","3","1","52006341","<p>Just remove \N and leave that input box empty will achieve this.</p>

<p>You could also click the <strong>code</strong> button （right top conner of the following screen shot) in the UI to see the json payload of the dataset. </p>

<p><a href=""https://i.stack.imgur.com/oV0iY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oV0iY.png"" alt=""enter image description here""></a></p>
"
"51998284","How do i send messages to Azure Service Bus after completion of activity in Azure data Factory","<p>I am trying to send a message to azure service bus through REST API after an activity is completed in Azure data factory pipeline. But how can i generate access token to send messages inside azure data factory.</p>

<p>Azure data factory has only on web activity through which i can call rest endpoints, Is there a solution possible with it.</p>
","<azure><access-token><azureservicebus><azure-data-factory>","2018-08-24 05:57:06","2797","2","3","51999239","<p>I recommand that you could use the Azure Httptrigger function to that.</p>

<p>You could implement send message with Azure httptigger function. And Call Http trigger function in the Azure datafactory.</p>

<p>For more information about Azure Httptrigger function, please refer to this <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook"" rel=""nofollow noreferrer"">tutorial</a>.</p>

<p>And then we could use the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">Web activity</a> to call an Azure function endpoint from a Data Factory pipeline.</p>
"
"51998284","How do i send messages to Azure Service Bus after completion of activity in Azure data Factory","<p>I am trying to send a message to azure service bus through REST API after an activity is completed in Azure data factory pipeline. But how can i generate access token to send messages inside azure data factory.</p>

<p>Azure data factory has only on web activity through which i can call rest endpoints, Is there a solution possible with it.</p>
","<azure><access-token><azureservicebus><azure-data-factory>","2018-08-24 05:57:06","2797","2","3","68836053","<p>There is a way to use the ADF web activity with to interact with the Service Bus REST api directly using the ADF's managed identitity.</p>
<p>It requires granting relevant permission on the Service Bus namespace for the ADF's managed identity (e.g. by assigning the <a href=""https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#azure-service-bus-data-sender"" rel=""nofollow noreferrer"">Azure Service Bus Data Sender</a> role). Rather than generating an access token, you can use the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity#managed-identity"" rel=""nofollow noreferrer"">managed identity authentication</a> option in the web activity and set the resource value as <code>https://servicebus.azure.net</code>.</p>
<p>For a worked example please take a look at my <a href=""https://medium.com/asos-techblog/sending-messages-from-azure-data-factory-to-service-bus-86d4be6dd357"" rel=""nofollow noreferrer"">blog post on sending messages to service bus from ADF</a>.</p>
"
"51998284","How do i send messages to Azure Service Bus after completion of activity in Azure data Factory","<p>I am trying to send a message to azure service bus through REST API after an activity is completed in Azure data factory pipeline. But how can i generate access token to send messages inside azure data factory.</p>

<p>Azure data factory has only on web activity through which i can call rest endpoints, Is there a solution possible with it.</p>
","<azure><access-token><azureservicebus><azure-data-factory>","2018-08-24 05:57:06","2797","2","3","72900507","<p>There is two ways to send messages from ADF to a Service Bus.</p>
<p>First, you need to grant the Azure Service Bus data sender role.</p>
<p><a href=""https://i.stack.imgur.com/Y8PwE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Y8PwE.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/wY5RC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wY5RC.png"" alt=""enter image description here"" /></a></p>
<p>You can then send messages using a copy activity, which allows you to send an entire data set, or using a web activity, which allows you to send a body or a parameter.</p>
<p>Settings in Linked Service for <strong>Copy Activity</strong>:</p>
<p><a href=""https://i.stack.imgur.com/x9wL9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x9wL9.png"" alt=""enter image description here"" /></a></p>
<p>Settings in <strong>Web Activity</strong>:</p>
<p><a href=""https://i.stack.imgur.com/gIw1a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gIw1a.png"" alt=""enter image description here"" /></a></p>
"
"51991972","LastUpdatedDate in ODS","<p>I am migrating data from SAP HANA view to ODS (Azure Data Factory). From there, the other third-party company is moving data to Salesforce database. Now, when I migrate it we are doing a truncate and load in sink.</p>
<p>There is no column in source which shows the date or last updated date when the news rows are added in SAP HANA.</p>
<p>Do we need to have the date in the source, or any other way we can write it in ODS?</p>
<p>It must show with a last updated date or something to denote when a row has been inserted or changed after initial load. So that they have a track when loading onto Salesforce database.</p>
","<azure><azure-sql-database><azure-data-factory><hana>","2018-08-23 18:15:43","44","0","1","51992978","<p>Truncate and Load a staging table, then run a stored procedure to <a href=""https://learn.microsoft.com/en-us/sql/t-sql/statements/merge-transact-sql?view=sql-server-2017"" rel=""nofollow noreferrer"">MERGE</a> into your target table, marking inserted and updated rows with the current sysdatetime().  Or MERGE from the staging table into a <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/tables/temporal-tables?view=sql-server-2017"" rel=""nofollow noreferrer"">Temporal Table</a>, or a table with <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/track-changes/about-change-tracking-sql-server?view=sql-server-2017"" rel=""nofollow noreferrer"">Change Tracking</a> enabled to track the changes automatically.</p>
"
"51981883","Azure Data Factory - Copying FTP recursively to database","<p>I am trying to copy csv files that are recursively put inside a FTP folder to a SQL database in Azure. I also have subscription to Azure Data Factory. Can some one help me with the easiest way to accomplish this ?</p>

<p>What I have tried</p>

<ol>
<li><p>Used the copy utility inside the Azure portal. The problem with it is that, it is asking me to copy to a table defined in the database. Can it just read from the csv files and detect the schema and put it into tables directly ?</p></li>
<li><p>Tried to use .NET sdk. But I don't see an example for ftp to sql database in azure directly. The examples only tell how to store into a blob storage.</p></li>
</ol>
","<azure><csv><ftp><recursive-query><azure-data-factory>","2018-08-23 08:56:07","1387","1","2","51982812","<p>If the table in database already exists, you can use ADF to copy from FTP to Azure SQL. Reference ADF docs here <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-copy-data-portal"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-copy-data-portal</a>. Though it's a tutorial from Azure Blob to Azure SQL, you can set FTP as source and other steps are similiar.</p>

<p>But ADF can only copy to existing tables, if the table doesn't exist before, you will have to write a sql script to create the table before the copy. You can put the script at pipeline-copyactivity-sink-""Pre-copy script"". It will be ran before copy starts.</p>
"
"51981883","Azure Data Factory - Copying FTP recursively to database","<p>I am trying to copy csv files that are recursively put inside a FTP folder to a SQL database in Azure. I also have subscription to Azure Data Factory. Can some one help me with the easiest way to accomplish this ?</p>

<p>What I have tried</p>

<ol>
<li><p>Used the copy utility inside the Azure portal. The problem with it is that, it is asking me to copy to a table defined in the database. Can it just read from the csv files and detect the schema and put it into tables directly ?</p></li>
<li><p>Tried to use .NET sdk. But I don't see an example for ftp to sql database in azure directly. The examples only tell how to store into a blob storage.</p></li>
</ol>
","<azure><csv><ftp><recursive-query><azure-data-factory>","2018-08-23 08:56:07","1387","1","2","51982838","<blockquote>
  <p>Can it just read from the csv files and detect the schema and put it
  into tables directly ?</p>
</blockquote>

<p>Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#dataset-properties"" rel=""nofollow noreferrer"">sql database dataset</a> in azure data factory, <code>tableName</code> property is required. So, if you need to transfer csv data into sql database, you need to create related tables so that they could establish a mapping relationship.</p>

<blockquote>
  <p>Tried to use .NET sdk. But I don't see an example for ftp to sql
  database in azure directly. The examples only tell how to store into a
  blob storage.</p>
</blockquote>

<p>Based on your requirements, I suggest you follow two steps.</p>

<p>Step 1 : Create a copy activity from <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-ftp#copy-activity-properties"" rel=""nofollow noreferrer"">ftp</a> to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage"" rel=""nofollow noreferrer"">azure blob storage</a>.</p>

<p>Step 2: Create another copy activity from <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage"" rel=""nofollow noreferrer"">azure blob storage</a> to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#supported-capabilities"" rel=""nofollow noreferrer"">sql database</a>.</p>

<p>You can combine two activity into one flow.</p>

<p>Hope it helps you,any concern please let me know.</p>

<hr>

<p><strong><em>Update answer:</em></strong></p>

<p>You still need to copy the csv files from ftp into azure blob storage.If you have 2 target tables,just create two containers in blob storage so that you could divide the files into the specific container. </p>

<p>Then just configure the container name in azure blob storage dataset.</p>

<p><a href=""https://i.stack.imgur.com/bsRst.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bsRst.png"" alt=""enter image description here""></a></p>

<p>It will loop all the files in the container.</p>
"
"51966559","Reading blob content from logic apps","<p>I have created a blob file using Azure Data Factory.
When reading the blob content using Logic Apps, I am getting <code>77u/</code> in the prefix of the content because of this unable to convert blob content to JSON. Tried converting blob content to XML then to JSON but it's not working.</p>

<p>Can anyone let me know how to resolve the issue?</p>

<p>Below is the blob Content:</p>

<pre><code>""body"": {
        ""$content-type"": ""application/octet-stream"",
        ""$content"": ""77u/W3siUHJvcF8wIjoiQUFBIiwiUHJvcF8xIjoiQjAxNTEwMDEiLCJQcm9wXzIiOiJEIiwiUHJvcF8zIjoiMjAxODA3MDQxNzA1MDIiLCJQcm9wXzQiOiJNSSIsIlByb3BfNSI6Ik4yRVhNSURQIiwiUHJvcF82IjoiQk0iLCJQcm9wXzciOiJVS0RDIiwiUHJvcF84IjoiMTI2NjM3IiwiUHJvcF85IjoiT1BFUiIsIlByb3BfMTAiOm51bGx9DQoseyJQcm9wXzAiOiJNSUQiLCJQcm9wXzEiOiJOMkVYTUlEUCIsIlByb3BfMiI6IjIwMTgwNzA0IiwiUHJvcF8zIjpudWxsLCJQcm9wXzQiOm51bGwsIlByb3BfNSI6bnVsbCwiUHJvcF82IjpudWxsLCJQcm9wXzciOm51bGwsIlByb3BfOCI6bnVsbCwiUHJvcF85IjpudWxsLCJQcm9wXzEwIjpudWxsfQ0KLHsiUHJvcF8wIjoiTUlQIiwiUHJvcF8xIjoiMDEiLCJQcm9wXzIiOiIwLjAiLCJQcm9wXzMiOiIwLjAiLCJQcm9wXzQiOiIwLjAiLCJQcm9wXzUiOiIwLjAiLCJQcm9wXzYiOm51bGwsIlByb3BfNyI6bnVsbCwiUHJvcF84IjpudWxsLCJQcm9wXzkiOm51bGwsIlByb3BfMTAiOm51bGx9DQoseyJQcm9wXzAiOiJNSVAiLCJQcm9wXzEiOiIwMiIsIlByb3BfMiI6IjAuMCIsIlByb3BfMyI6IjAuMCIsIlByb3BfNCI6IjAuMCIsIlByb3BfNSI6IjAuMCIsIlByb3BfNiI6bnVsbCwiUHJvcF83IjpudWxsLCJQcm9wXzgiOm51bGwsIlByb3BfOSI6bnVsbCwiUHJvcF8xMCI6bnVsbH0NCix7IlByb3BfMCI6Ik1JUCIsIlByb3BfMSI6IjAzIiwiUHJvcF8yIjoiMC4wIiwiUHJvcF8zIjoiMC4wIiwiUHJvcF80IjoiMC4wIiwiUHJvcF81IjoiMC4wIiwiUHJvcF82IjpudWxsLCJQcm9wXzciOm51bGwsIlByb3BfOCI6bnVsbCwiUHJvcF85IjpudWxsLCJQcm9wXzEwIjpudWxsfQ0KLHsiUHJvcF8wIjoiTUlQIiwiUHJvcF8xIjoiMDQiLCJQcm9wXzIiOiIwLjAiLCJQcm9wXzMiOiIwLjAiLCJQcm9wXzQiOiIwLjAiLCJQcm9wXzUiOiIwLjAiLCJQcm9wXzYiOm51bGwsIlByb3BfNyI6bnVsbCwiUHJvcF84IjpudWxsLCJQcm9wXzkiOm51bGwsIlByb3BfMTAiOm51bGx9DQoseyJQcm9wXzAiOiJaWloiLCJQcm9wXzEiOiI3IiwiUHJvcF8yIjoiMTU5NjUyNDE0NyIsIlByb3BfMyI6bnVsbCwiUHJvcF80IjpudWxsLCJQcm9wXzUiOm51bGwsIlByb3BfNiI6bnVsbCwiUHJvcF83IjpudWxsLCJQcm9wXzgiOm51bGwsIlByb3BfOSI6bnVsbCwiUHJvcF8xMCI6bnVsbH0NCl0=""
    }
</code></pre>
","<azure-blob-storage><azure-data-factory>","2018-08-22 12:09:41","1650","3","2","51984845","<p>The prefix <code>77u/</code> in Base64 content is caused by <a href=""https://en.wikipedia.org/wiki/Byte_order_mark"" rel=""nofollow noreferrer"">UTF-8 BOM</a>. By decoding the Base64 content, you can see <code>ï»¿</code> which matches UTF-8 BOM characters at the beginning.</p>

<p>ADF will always generate BOM if you specified <code>encodingName</code> in your dataset format. So you can remove the <code>encodingName</code> in <code>format</code> to generate file without BOM.</p>
"
"51966559","Reading blob content from logic apps","<p>I have created a blob file using Azure Data Factory.
When reading the blob content using Logic Apps, I am getting <code>77u/</code> in the prefix of the content because of this unable to convert blob content to JSON. Tried converting blob content to XML then to JSON but it's not working.</p>

<p>Can anyone let me know how to resolve the issue?</p>

<p>Below is the blob Content:</p>

<pre><code>""body"": {
        ""$content-type"": ""application/octet-stream"",
        ""$content"": ""77u/W3siUHJvcF8wIjoiQUFBIiwiUHJvcF8xIjoiQjAxNTEwMDEiLCJQcm9wXzIiOiJEIiwiUHJvcF8zIjoiMjAxODA3MDQxNzA1MDIiLCJQcm9wXzQiOiJNSSIsIlByb3BfNSI6Ik4yRVhNSURQIiwiUHJvcF82IjoiQk0iLCJQcm9wXzciOiJVS0RDIiwiUHJvcF84IjoiMTI2NjM3IiwiUHJvcF85IjoiT1BFUiIsIlByb3BfMTAiOm51bGx9DQoseyJQcm9wXzAiOiJNSUQiLCJQcm9wXzEiOiJOMkVYTUlEUCIsIlByb3BfMiI6IjIwMTgwNzA0IiwiUHJvcF8zIjpudWxsLCJQcm9wXzQiOm51bGwsIlByb3BfNSI6bnVsbCwiUHJvcF82IjpudWxsLCJQcm9wXzciOm51bGwsIlByb3BfOCI6bnVsbCwiUHJvcF85IjpudWxsLCJQcm9wXzEwIjpudWxsfQ0KLHsiUHJvcF8wIjoiTUlQIiwiUHJvcF8xIjoiMDEiLCJQcm9wXzIiOiIwLjAiLCJQcm9wXzMiOiIwLjAiLCJQcm9wXzQiOiIwLjAiLCJQcm9wXzUiOiIwLjAiLCJQcm9wXzYiOm51bGwsIlByb3BfNyI6bnVsbCwiUHJvcF84IjpudWxsLCJQcm9wXzkiOm51bGwsIlByb3BfMTAiOm51bGx9DQoseyJQcm9wXzAiOiJNSVAiLCJQcm9wXzEiOiIwMiIsIlByb3BfMiI6IjAuMCIsIlByb3BfMyI6IjAuMCIsIlByb3BfNCI6IjAuMCIsIlByb3BfNSI6IjAuMCIsIlByb3BfNiI6bnVsbCwiUHJvcF83IjpudWxsLCJQcm9wXzgiOm51bGwsIlByb3BfOSI6bnVsbCwiUHJvcF8xMCI6bnVsbH0NCix7IlByb3BfMCI6Ik1JUCIsIlByb3BfMSI6IjAzIiwiUHJvcF8yIjoiMC4wIiwiUHJvcF8zIjoiMC4wIiwiUHJvcF80IjoiMC4wIiwiUHJvcF81IjoiMC4wIiwiUHJvcF82IjpudWxsLCJQcm9wXzciOm51bGwsIlByb3BfOCI6bnVsbCwiUHJvcF85IjpudWxsLCJQcm9wXzEwIjpudWxsfQ0KLHsiUHJvcF8wIjoiTUlQIiwiUHJvcF8xIjoiMDQiLCJQcm9wXzIiOiIwLjAiLCJQcm9wXzMiOiIwLjAiLCJQcm9wXzQiOiIwLjAiLCJQcm9wXzUiOiIwLjAiLCJQcm9wXzYiOm51bGwsIlByb3BfNyI6bnVsbCwiUHJvcF84IjpudWxsLCJQcm9wXzkiOm51bGwsIlByb3BfMTAiOm51bGx9DQoseyJQcm9wXzAiOiJaWloiLCJQcm9wXzEiOiI3IiwiUHJvcF8yIjoiMTU5NjUyNDE0NyIsIlByb3BfMyI6bnVsbCwiUHJvcF80IjpudWxsLCJQcm9wXzUiOm51bGwsIlByb3BfNiI6bnVsbCwiUHJvcF83IjpudWxsLCJQcm9wXzgiOm51bGwsIlByb3BfOSI6bnVsbCwiUHJvcF8xMCI6bnVsbH0NCl0=""
    }
</code></pre>
","<azure-blob-storage><azure-data-factory>","2018-08-22 12:09:41","1650","3","2","68206563","<p>I think the answer is a bit late for you, but I hope it can help others who are facing the same problem.
I solved this problem by replacing the BOM character '77u/' by an empty string ('') at e.g. the Compose activity:</p>
<p><em>json(base64ToString(replace(body('Get_blob_content')?['$content'],'77u/','')))</em></p>
<p>It may not be a perfect solution, but it works well for me. :)</p>
<p>Regards,
Lukas</p>
"
"51965514","reading a specific bucket file from bucketed table created by parent pipeline","<p>I have an ADF (v1) pipeline P1 with a single activity of type HDInsightHive. It's a monthly pipeline. The associated query produces a partitioned and bucketed external table (3 buckets).</p>

<p>So when we look at the external data produced by it, this is what it looks like:-</p>

<pre><code>  ../basefolder/2018/07/000000_0
  ../basefolder/2018/07/000000_1
  ../basefolder/2018/07/000000_2
</code></pre>

<p>So these are the 3 files associated with the 3 buckets within the monthly partition (in the example it is July 2018). So because of bucketing , for every month there will be these 3 files.</p>

<p>Now I want to create 3 child pipelines again of HDInsightHive type (but it could be different type as well, that's not imp) which will depend on P1 i.e. wait for P1 to finish processing for a given month slice. Say the 3 pipelines are C1,C2,C3. But, though all 3 should wait for P1 to finish processing for a given month, when the processing of C1,C2,C3 starts for that month, they should consume 000000_0, 000000_1 and 000000_2 respectively. If I just create 3 pipelines like that making P1 as parent, ideally they will start consuming all the 3 files.</p>

<p>I need a way so that I can dictate C1, C2 &amp; C3 that they should only read file with specific names.</p>

<p>The issue is that the output dataset of P1 will be the common input dataset for C1,C2,C3. So I can't hardcode file name as part of this common input dataset.</p>

<p>Here is the diagram of what I am trying to achieve:-</p>

<p><a href=""https://i.stack.imgur.com/8Yw5h.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8Yw5h.jpg"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2018-08-22 11:07:52","42","0","2","51967492","<p>One solution that comes to my mind is that for the parent pipeline P1, even though it is single activity, I will declare 3 output datasets D1,D2,D3 with different output file names. And then when I declare 3 child pipelines C1,C2,C3 , I will point these to D1,D2,D3. This is just a vague idea, I will have to do a POC around it.</p>

<p><strong>Update:</strong> it works using the method I described.</p>
"
"51965514","reading a specific bucket file from bucketed table created by parent pipeline","<p>I have an ADF (v1) pipeline P1 with a single activity of type HDInsightHive. It's a monthly pipeline. The associated query produces a partitioned and bucketed external table (3 buckets).</p>

<p>So when we look at the external data produced by it, this is what it looks like:-</p>

<pre><code>  ../basefolder/2018/07/000000_0
  ../basefolder/2018/07/000000_1
  ../basefolder/2018/07/000000_2
</code></pre>

<p>So these are the 3 files associated with the 3 buckets within the monthly partition (in the example it is July 2018). So because of bucketing , for every month there will be these 3 files.</p>

<p>Now I want to create 3 child pipelines again of HDInsightHive type (but it could be different type as well, that's not imp) which will depend on P1 i.e. wait for P1 to finish processing for a given month slice. Say the 3 pipelines are C1,C2,C3. But, though all 3 should wait for P1 to finish processing for a given month, when the processing of C1,C2,C3 starts for that month, they should consume 000000_0, 000000_1 and 000000_2 respectively. If I just create 3 pipelines like that making P1 as parent, ideally they will start consuming all the 3 files.</p>

<p>I need a way so that I can dictate C1, C2 &amp; C3 that they should only read file with specific names.</p>

<p>The issue is that the output dataset of P1 will be the common input dataset for C1,C2,C3. So I can't hardcode file name as part of this common input dataset.</p>

<p>Here is the diagram of what I am trying to achieve:-</p>

<p><a href=""https://i.stack.imgur.com/8Yw5h.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8Yw5h.jpg"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2018-08-22 11:07:52","42","0","2","51981169","<p>If you could switch to ADF V2, I think this would be easy to achieve with <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-pipeline-activity"" rel=""nofollow noreferrer"">execute pipeline activity</a> and ADF v2 <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">parameters</a>.</p>
"
"51963579","How to pass parameter to pipeline in azure data factory?","<p>I have a file in azure data lake in which datetime is stored, so I want to pass that datetime to the pipeline as a parameter in azure data factory and use that parameter in xml query used in pipeline. How can I do that and use that parameter in a query in pipeline?</p>

<p>Here the the condition of fetch xml query in which i want to use that parameter</p>

<p>""condition attribute=""createdon"" operator=""on-or-after"" value=""2018-08-20T22:10:35.1065671-07:00""""</p>

<p>In above condition I have used static datetime, which I want dynamically from parameter.</p>
","<azure><dynamics-crm><azure-data-factory><azure-data-lake><fetchxml>","2018-08-22 09:19:55","2473","1","1","51964688","<p>You can easily use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-portal"" rel=""nofollow noreferrer"">ADF V2 UI</a> to author the pipeline.
You could use a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity#type-properties"" rel=""nofollow noreferrer"">lookup activity</a> to read the datetime from azure data lake and then reference the output of the lookup activity.</p>

<p>@activity('Lookup1').output.firstRow.Prop_0</p>

<p>Lookup1 is your look up activity name. Prop_0 is your data header name. You could use debug button to check the output of your lookup activity.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>'""condition attribute=""createdon"" operator=""on-or-after"" value=""@{activity(\'Lookup1\').output.firstRow.Prop_0}""'</code></pre>
</div>
</div>
</p>

<p><a href=""https://i.stack.imgur.com/WBpsM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WBpsM.png"" alt=""enter image description here""></a></p>
"
"51936847","Azure DataFactory Incremental BLOB copy","<p>I've made a pipeline to copy data from one blob storage to another. I want to have incremental copy if it's possible, but haven't found a way to specify it. The reason is I want to run this on a schedule and only copy any new data since last run.</p>
","<azure><azure-data-factory>","2018-08-20 18:49:14","836","0","2","51937849","<p>I'm going to presume that by 'incremental' you mean new blobs added to a container.  There is no easy way to copy changes to a specific blob.</p>

<p>So, this is not possible automatically when running on a schedule since 'new' is not something the scheduler can know.</p>

<p>Instead, you can use a Blob created Event Trigger, then cache the result (Blob name) somewhere else.  Then, when your schedule runs, it can read those names and copy only those blobs.</p>

<p>You have many options to cache.  A SQL Table, another blob.</p>

<p>Note: The complication here is trying to do this on a schedule.  If you can adjust the parameters to merely copy every new file, it's very, very easy because you can just copy the blob that created the trigger.</p>

<p>Another option is to copy the blob on create using the trigger to a temporary/staging container, then use a schedule to move those files to the ultimate destination.</p>
"
"51936847","Azure DataFactory Incremental BLOB copy","<p>I've made a pipeline to copy data from one blob storage to another. I want to have incremental copy if it's possible, but haven't found a way to specify it. The reason is I want to run this on a schedule and only copy any new data since last run.</p>
","<azure><azure-data-factory>","2018-08-20 18:49:14","836","0","2","51947903","<ol>
<li>If your blob name is well named with timestamp, you could follow this doc to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-read-write-partitioned-data"" rel=""nofollow noreferrer"">copy partitioned data.</a> You could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-data-tool"" rel=""nofollow noreferrer"">copy data tool</a> to setup the pipeline. You could select tumbling window and then in file path filed input {year}/{month}/{day}/fileName and choose the right pattern. It will help you construct the parameters.
<a href=""https://i.stack.imgur.com/Q8rqQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q8rqQ.png"" alt=""enter image description here""></a></li>
<li>If you blob name is not well named with timestamp, you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">get metadata activity</a> to check the last modified time. Please reference <a href=""https://stackoverflow.com/questions/50298122/azure-data-factory-incremental-data-load-from-sftp-to-blob"">this post</a>.</li>
</ol>

<p>Event trigger is just one way to control when the pipeline should run. You could also use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger"" rel=""nofollow noreferrer"">tumbling window trigger</a> or <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger"" rel=""nofollow noreferrer"">schedule trigger</a> in your scenarios.</p>
"
"51936642","Error linking Azure Blob Storage account to Azure Data Factory","<p>I have created an Azure Data Factory and I am trying to create a new linked service to using an Azure Blob Storage account. The connection test passes successfully, but each time I try to link the service I am greeted with the following error:</p>

<pre><code>Cannot read property 'Symbol(Symbol.iterator)' of undefined
</code></pre>

<p>I am using the Azure Data Factory V2 and the service I am trying to link is a StorageV2 storage account.</p>

<p>Not quite sure where to go from here, any suggestions would be greatly appreciated. Thanks!</p>
","<azure><azure-data-factory>","2018-08-20 18:31:56","791","1","2","51937499","<p>I was having a similar problem. I was logged in with 2 different accounts. I logged out of both, signed in with one and it worked. Hope this helps you.</p>
"
"51936642","Error linking Azure Blob Storage account to Azure Data Factory","<p>I have created an Azure Data Factory and I am trying to create a new linked service to using an Azure Blob Storage account. The connection test passes successfully, but each time I try to link the service I am greeted with the following error:</p>

<pre><code>Cannot read property 'Symbol(Symbol.iterator)' of undefined
</code></pre>

<p>I am using the Azure Data Factory V2 and the service I am trying to link is a StorageV2 storage account.</p>

<p>Not quite sure where to go from here, any suggestions would be greatly appreciated. Thanks!</p>
","<azure><azure-data-factory>","2018-08-20 18:31:56","791","1","2","51939616","<p>As of this afternoon I have been having the same issue (it worked last Friday). It occurred trying to create linked services for Azure Storage or SQL Database. I tried various browsers, different Azure subscriptions, different client machines, deploying Data Factory and storage accounts in different regions and probably a few other things that I have forgotten now. </p>

<p>Ultimately, I just gave up on the portal and created the linked services from PowerShell which worked just fine.</p>

<pre><code>$resourceGroupName = ""&lt;Resource group name&gt;""
$dataFactoryName = ""&lt;Data factory name&gt;""
$storageAccountName = ""&lt;Azure storage account name&gt;""
$storageAccountKey = ""&lt;Azure storage account key&gt;""

## JSON definition of the linked service. 
$storageLinkedServiceDefinition = @""
{
    ""name"": ""AzureStorageLinkedService"",
    ""properties"": {
        ""type"": ""AzureStorage"",
        ""typeProperties"": {
            ""connectionString"": {
                ""value"": ""DefaultEndpointsProtocol=https;AccountName=$storageAccountName;AccountKey=$storageAccountKey"",
                ""type"": ""SecureString""
            }
        }
    }
}
""@

## IMPORTANT: stores the JSON definition in a file that will be used by the Set-AzureRmDataFactoryV2LinkedService command. 
$storageLinkedServiceDefinition | Out-File c:\AzureStorageLinkedService.json

## Creates a linked service in the data factory
Set-AzureRmDataFactoryV2LinkedService -DataFactoryName $dataFactoryName -ResourceGroupName $resourceGroupName -Name ""AzureStorageLinkedService"" -File c:\AzureStorageLinkedService.json
</code></pre>
"
"51932851","Getting Error while publishing ADFv2","<p>I have done the changes in the Azure data factory pipeline. Selected the ""VSTS Git"" option and trying to publish the changes it was working fine but now started getting below error message:</p>

<p><code>Error while publishing: Cannot read property 'constructor' of undefined</code></p>

<p>Retried by removing the changes but still getting same issue.</p>
","<azure-data-factory>","2018-08-20 14:22:50","3546","3","1","57585570","<p>There could be a broken resource in your collaberation branch which is causing the issue.
You can re-sync Git and Live mode by disconnecting and reconnecting Git. 
<em>Please note that you would lose the differences between the two.</em></p>

<p>Hope this will help. </p>
"
"51928291","Azure Databricks with Github","<p>I am working with Databricks notebook and I synced it with GitHub. We are 2 members working on 2 different branches in Github repo. When we ran Azure Data Factory activity on that notebook, It ran the latest version of that notebook. </p>

<p>So whats the purpose of having GitHub as version control since we can't have control over Notebook version while executing from outside.</p>

<p>What If many developers commit their changes but at the EOD we need master branch changes to be executed which are most stable one. </p>
","<azure><github><databricks><azure-data-factory>","2018-08-20 10:00:09","856","4","2","60708517","<p>Databricks notebook does not reload from the git. 
You need to make a copy of the notebook in personal folder, develop and commit to git feature branch. 
After pull request into the main branch, you need to (re)deploy your notebooks from git. </p>

<p>The notebook which is running your code should not be altered, only the personal copy.  </p>
"
"51928291","Azure Databricks with Github","<p>I am working with Databricks notebook and I synced it with GitHub. We are 2 members working on 2 different branches in Github repo. When we ran Azure Data Factory activity on that notebook, It ran the latest version of that notebook. </p>

<p>So whats the purpose of having GitHub as version control since we can't have control over Notebook version while executing from outside.</p>

<p>What If many developers commit their changes but at the EOD we need master branch changes to be executed which are most stable one. </p>
","<azure><github><databricks><azure-data-factory>","2018-08-20 10:00:09","856","4","2","63606059","<p>We're actually not using the whole git sync on databricks, but are using the export_dir / import_dir functionality from databricks-cli. This way we have more control over what gets imported, and when.. And you can have commits over multiple notebooks (since one feature usually crosses more than one notebook).</p>
<p>Hopefully this helps.</p>
"
"51919859","How to trigger a pipeline in Azure Data Factory v2 or a Azure Databricks Notebook by a new file in Azure Data Lake Store gen1","<p>I am using a <strong>Azure Data Lake Store gen1</strong> for storing JSON files. Based on these files i have Notebooks in <strong>Azure Databricks</strong> for processing them. Now i want to trigger such a <strong>Azure Databricks</strong> Notebook when a new file is creating in <strong>Azure Data Lake Store gen1</strong>. I couldnt find any Trigger which could do this. do you know any way?</p>
","<azure><azure-data-lake><databricks><azure-data-factory>","2018-08-19 16:50:39","638","3","1","51923880","<p>Currently, this is <strong>not yet</strong> implemented/Supported by <em>Microsoft</em>. But it is on their Roadmap(I believe).</p>

<p>You can do this in 2 ways,</p>

<ol>
<li><p>Azure Functions(through Event Grid) </p></li>
<li><p>Logic Apps</p></li>
</ol>

<hr>

<h2>Option #1</h2>

<p>Currently, Microsoft is building on #1.</p>

<p>You can track the issue <a href=""https://github.com/Azure/Azure-Functions/issues/353"" rel=""nofollow noreferrer"">here</a>.</p>

<p>As per this </p>

<blockquote>
  <p>This feature is not a high priority for us right now, but I will note
  that the announcement for Azure Event Grid listed Data Lake as one of
  the integrations they are building. Once you can subscribe to Data
  Lake updates through Event Grid, running an Azure Function would be
  trivial (see here for some info).</p>
</blockquote>

<p>You can <a href=""https://feedback.azure.com/forums/327234-data-lake/suggestions/32983252-support-event-grid"" rel=""nofollow noreferrer"">vote your voice</a> to support the event grid (provider) in DataLake.</p>

<hr>

<h2>Option #2</h2>

<p>This is also not yet implemented, but you can <a href=""https://feedback.azure.com/forums/287593-logic-apps/suggestions/19528168-trigger-function-or-logic-app-when-file-is-added"" rel=""nofollow noreferrer"">Upvote your voice</a> here to support this feature</p>
"
"51906430","What is the default frequency if it is not explicitly specified in a pipeline activity","<p>If we don't explicitly define 'frequency' in a pipeline activity, what is the default? I have noticed that 'hour' seems to be the default but I just want to make sure. My activity type is HDInsightHive , assuming that this default does not change across different activity types. I am using ADF v1.</p>
","<azure-data-factory>","2018-08-18 07:42:32","63","0","2","51909555","<p>If you are looking for ADFV2 then we have triggers in place to schedule the pipeline. When we come to the point of scheduling a trigger which determines when to execute the pipeline. </p>

<p>Coming to frequency - the supported values include ""minute"", ""hour"", ""day"", ""week"", and ""month"" and this is a required property which has to be defined while creation.</p>

<p>When you create any trigger through Data Factory UI - you would be presented with this screen -</p>

<p><a href=""https://i.stack.imgur.com/zlA8x.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zlA8x.jpg"" alt=""enter image description here""></a></p>

<p>As you could see the recurrence(frequency) has been defaulted to - ""Every Minute"".</p>
"
"51906430","What is the default frequency if it is not explicitly specified in a pipeline activity","<p>If we don't explicitly define 'frequency' in a pipeline activity, what is the default? I have noticed that 'hour' seems to be the default but I just want to make sure. My activity type is HDInsightHive , assuming that this default does not change across different activity types. I am using ADF v1.</p>
","<azure-data-factory>","2018-08-18 07:42:32","63","0","2","51916063","<p>Assume you are using v1. Then according to <a href=""http://Currently,%20output%20dataset%20drives%20the%20schedule.%20In%20other%20words,%20the%20schedule%20specified%20for%20the%20output%20dataset%20is%20used%20to%20run%20an%20activity%20at%20runtime.%20Therefore,%20you%20must%20create%20an%20output%20dataset%20even%20if%20the%20activity%20does%20not%20produce%20any%20output.%20If%20the%20activity%20doesn&#39;t%20take%20any%20input,%20you%20can%20skip%20creating%20the%20input%20dataset."" rel=""nofollow noreferrer"">this doc</a>:</p>

<p><strong>Currently, output dataset drives the schedule.</strong> In other words, the schedule specified for the output dataset is used to run an activity at runtime. Therefore, you must create an output dataset even if the activity does not produce any output. If the activity doesn't take any input, you can skip creating the input dataset.</p>

<p>And scheduler is a required property for dataset.</p>

<p>The scheduler property for an activity is optional. If you do specify this property, it must match the cadence you specify in the definition of output dataset for the activity. Currently, output dataset is what drives the schedule. Therefore, you must create an output dataset even if the activity does not produce any output.</p>
"
"51902760","Make Web Activity work with response that's not a proper Json","<p>I'm using Web Activity to make a POST request to a slack webhook.
The problem is that the slack webhook responds with a 200 and an ""ok"" string as the body of the response and the web activity doesn't seems to like that because I'm getting the error:</p>

<pre><code>{
    ""errorCode"": ""2108"",
    ""message"": ""Response Content is not a valid JObject"",
    ""failureType"": ""UserError"",
    ""target"": ""activityName""
}</code></pre>

<p>So my question is if there is a workaround for this or is there is an alternative to use web activity. For the time being I know I can use a custom activity and even I'm thinking of setting up an azure function that acts as a proxy and thus returns a proper Json.</p>

<p>Note: I'm using this slack integration to alert when something goes wrong. Of course I'm also open to suggestions on how to alert my team when something is not working in ADF (sending an email wouldn't work because our inboxes are completely cluttered, so slack seemed like a good alternative for a change).</p>
","<azure><azure-data-factory>","2018-08-17 20:56:24","1340","0","2","51916151","<p>According to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity#type-properties"" rel=""nofollow noreferrer"">the official doc</a>, REST endpoints that the web activity invokes <strong>must return a response of type JSON</strong>. The activity will timeout at 1 minute with an error if it does not receive a response from the endpoint.</p>

<p>So currently I think your workaround should be the right way.</p>

<p>And besides of email, I think ADF also provides other approaches to monitor your data factory. Please find <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor"" rel=""nofollow noreferrer"">this doc</a> for more details.</p>
"
"51902760","Make Web Activity work with response that's not a proper Json","<p>I'm using Web Activity to make a POST request to a slack webhook.
The problem is that the slack webhook responds with a 200 and an ""ok"" string as the body of the response and the web activity doesn't seems to like that because I'm getting the error:</p>

<pre><code>{
    ""errorCode"": ""2108"",
    ""message"": ""Response Content is not a valid JObject"",
    ""failureType"": ""UserError"",
    ""target"": ""activityName""
}</code></pre>

<p>So my question is if there is a workaround for this or is there is an alternative to use web activity. For the time being I know I can use a custom activity and even I'm thinking of setting up an azure function that acts as a proxy and thus returns a proper Json.</p>

<p>Note: I'm using this slack integration to alert when something goes wrong. Of course I'm also open to suggestions on how to alert my team when something is not working in ADF (sending an email wouldn't work because our inboxes are completely cluttered, so slack seemed like a good alternative for a change).</p>
","<azure><azure-data-factory>","2018-08-17 20:56:24","1340","0","2","51991507","<p>You can use a lookup activity with an HTTP dataset. You can define the dataset to be a REST call, and I don't think the lookup activity is picky about the response format.</p>

<p>Another option might be to use an Azure Logic App to update Slack. You can either use it as a 'proxy' to be called by ADF, or see if it can be set up to perform the monitoring you want as well.</p>
"
"51898851","azure data factory - How To Trim Whitespace for CopyData","<p>I'm using the CopyData component to Extract-Load data from pipe-delimited files in to Azure SQL DW. </p>

<p>Generally this is working fine, but it seems the default behavior is to not trim whitespace on <strong>string</strong> columns in the delimited file. So the sink to the Azure SQL DW table can't handle the column varchar width. </p>

<hr>

<h2>Example</h2>

<pre>
""Value Name Example                                      "" 
</pre>

<p>This won't work with a <strong>VARCHAR(20)</strong> table column target. Truncation error occurs.</p>

<hr>

<p>What is the normal practice for this? Is it considered ""Transform"", and so we have to resort to SSIS or other compute?</p>

<p>Thank you.</p>
","<azure><etl><azure-data-factory>","2018-08-17 15:43:01","1581","1","1","51902605","<p>The lack of a JSON trim() expression notwithstanding, expressions don't seem to work in the dataset mapping table...(hopefully) yet.</p>

<p>If you have a SQL Server instance available, you can pass the data set as a table value to a Stored Procedure that trims the fields and performs any other necessary modifications.</p>
"
"51883988","Data Factory - append fields to JSON sink","<p>I am using the copy activity to copy/transform a JSON source dataset into JSON sink dataset. Need to append a few audit fields to the output - such as transform date using @utcnow expression function.</p>

<p>How can this be accomplished?</p>
","<azure-data-factory>","2018-08-16 19:16:52","933","0","1","51897075","<p>It looks like the databricks activity handles this functionality pretty well.</p>

<pre><code>df_new = df.select(""table.field1"",""table.field2"",""table.field3"").withColumn(""TransferDate"", current_timestamp())
</code></pre>
"
"51877036","Run Azure Data factory single activity for backdates","<p>PROBLEM: I have updated the stored procedure and this stored procedure is run after every 1 hour through Azure-pipeline one activity. This pipeline contains multiple activity but I want to run this specific Activity in pipeline.</p>
","<sql-server><azure><visual-studio-2015><azure-data-factory>","2018-08-16 12:25:08","1972","1","2","51892553","<p>Per my knowledge, you can't run single specific activity in one pipeline. Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-rest-api#create-pipeline-run"" rel=""nofollow noreferrer"">Azure Data Factory REST API</a>, only execute pipeline could be found. No such execute activity is supported so far.</p>

<p>It seems the only way is that you clone your activity into another pipeline if your activity doesn't have dependency relationship with other activities. </p>

<p>You could also refer to the similar <a href=""https://social.msdn.microsoft.com/Forums/azure/en-US/6d5c2a24-20c3-450b-b866-593cf4f6bdbe/how-to-run-single-activity-in-azure-data-factory-pipeline?forum=AzureDataFactory"" rel=""nofollow noreferrer"">case</a>.</p>

<p>Hope it helps you.</p>
"
"51877036","Run Azure Data factory single activity for backdates","<p>PROBLEM: I have updated the stored procedure and this stored procedure is run after every 1 hour through Azure-pipeline one activity. This pipeline contains multiple activity but I want to run this specific Activity in pipeline.</p>
","<sql-server><azure><visual-studio-2015><azure-data-factory>","2018-08-16 12:25:08","1972","1","2","69624955","<p>If you wish to do it from the Azure portal you can connect the activity you intend to run with the others and then in the connector select the SKIPPED property.</p>
"
"51871825","Azure Data factory, How to incrementally copy blob data to sql","<p>I have a azure blob container where some json files with data gets put every 6 hours and I want to use Azure Data Factory to copy it to an Azure SQL DB. The file pattern for the files are like this: ""customer_year_month_day_hour_min_sec.json.data.json""</p>

<p>The blob container also has other json data files as well so I have filter for the files in the dataset. </p>

<p>First question is how can I set the file path on the blob dataset to only look for the json files that I want? I tried with the wildcard *.data.json but that doesn't work. The only filename wildcard I have gotten to work is *.json</p>

<p>Second question is how can I copy data only from the new files (with the specific file pattern) that lands in the blob storage to Azure SQL? I have no control of the process that puts the data in the blob container and cannot move the files to another location which makes it harder.</p>

<p>Please help.</p>
","<azure-data-factory>","2018-08-16 07:31:01","1489","5","2","51872370","<p>You could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger"" rel=""nofollow noreferrer"">ADF event trigger</a> to achieve this.</p>
<p>Define your event trigger as 'blob created' and specify the <em>blobPathBeginsWith</em> and <em>blobPathEndsWith</em> property based on your filename pattern.</p>
<p>For the first question, when an event trigger fires for a specific blob, the event captures the folder path and file name of the blob into the properties <strong>@triggerBody().folderPath</strong> and <strong>@triggerBody().fileName</strong>. You need to map the properties to pipeline parameters and pass @pipeline.parameters.parameterName expression to your fileName in copy activity.</p>
<p>This also answers the second question, each time the trigger is fired, you'll get the fileName of the newest created files in <strong>@triggerBody().folderPath</strong> and <strong>@triggerBody().fileName</strong>.
Thanks.</p>
"
"51871825","Azure Data factory, How to incrementally copy blob data to sql","<p>I have a azure blob container where some json files with data gets put every 6 hours and I want to use Azure Data Factory to copy it to an Azure SQL DB. The file pattern for the files are like this: ""customer_year_month_day_hour_min_sec.json.data.json""</p>

<p>The blob container also has other json data files as well so I have filter for the files in the dataset. </p>

<p>First question is how can I set the file path on the blob dataset to only look for the json files that I want? I tried with the wildcard *.data.json but that doesn't work. The only filename wildcard I have gotten to work is *.json</p>

<p>Second question is how can I copy data only from the new files (with the specific file pattern) that lands in the blob storage to Azure SQL? I have no control of the process that puts the data in the blob container and cannot move the files to another location which makes it harder.</p>

<p>Please help.</p>
","<azure-data-factory>","2018-08-16 07:31:01","1489","5","2","51885100","<p>I understand your situation.  Seems they've used a new platform to recreate a decades old problem. :)</p>

<p>The patter I would setup first looks something like:</p>

<ol>
<li>Create a Storage Account Trigger that will fire on every new file in the source container.</li>
<li>In the triggered Pipeline, examine the blog name to see if it fits your parameters.  If no, just end, taking no action.  If so, binary copy the blob to a account/container your app owns, leaving the original in place.</li>
<li>Create another Trigger on your container that runs the import Pipeline.</li>
<li>Run your import process.</li>
</ol>

<p>Couple caveats your management has to understand.  You can be very, very reliable, but cannot guarantee compliance because there is no transaction/contract between you and the source container.  Also, there may be a sequence gap since a small file can usually process while a larger file is processing.</p>

<p>If for any reason you do miss a file, all you need to do is copy it to your container where your process will pick it up.  You can load all previous blobs in the same way.</p>
"
"51865911","Syntax Error in Azure Data Factory SOQL Query WHERE LastModifiedDate >= datetime","<p>I'm working in <strong>Azure Data Factory V2</strong>, attempting to query from a Salesforce object where the LastModifiedDate in the object is greater than or equal to a recent date. I've been receiving syntax errors on extremely simple SOQL queries, queries that work just fine in Salesforce Workbench. Below, I've listed examples of queries I've run in the ADF Copy Data activity, along with the errors I received when debugging the pipeline.</p>

<p><strong>Query 1</strong></p>

<pre><code> Select Id from ""Object"" WHERE LastModifiedDate &gt;=
 @{formatDateTime(activity('Yesterday').output.firstRow.Yesterday,'yyyy-MM-ddTHH:mm:ssZ')}
</code></pre>

<p><strong>Result 1</strong></p>

<pre><code>{
    ""errorCode"": ""2200"",
    ""message"": ""Failure happened on 'Source' side. ErrorCode=UserErrorOdbcOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR [HY000] [Microsoft][Salesforce] (120) SOQL_FIRST mode prepare failure:\nSOQL error: [Microsoft][Salesforce] (30) Syntax error or access violation when parsing SOQL.\nSQL error: [Microsoft][SQLEngine] (31480) syntax error near 'Select Id from \""Object\"" WHERE LastModifiedDate &gt;= 2018-08-13T16&lt;&lt;&lt; ??? &gt;&gt;&gt;:05:06Z'.,Source=Microsoft.DataTransfer.ClientLibrary.Odbc.OdbcConnector,''Type=System.Data.Odbc.OdbcException,Message=ERROR [HY000] [Microsoft][Salesforce] (120) SOQL_FIRST mode prepare failure:\nSOQL error: [Microsoft][Salesforce] (30) Syntax error or access violation when parsing SOQL.\nSQL error: [Microsoft][SQLEngine] (31480) syntax error near 'Select Id from \""Object\"" WHERE LastModifiedDate &gt;= 2018-08-13T16&lt;&lt;&lt; ??? &gt;&gt;&gt;:05:06Z'.,Source=Microsoft Salesforce ODBC Driver,'"",
    ""failureType"": ""UserError"",
    ""target"": ""Event Fix Pipeline""
}
</code></pre>

<p><strong>Query 2</strong></p>

<pre><code>SELECT Id FROM ""Object"" WHERE LastModifiedDate &gt;= 2018-08-14T00:00:00Z
</code></pre>

<p><strong>Result 2</strong></p>

<pre><code>{
    ""errorCode"": ""2200"",
    ""message"": ""Failure happened on 'Source' side. ErrorCode=UserErrorOdbcOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR [HY000] [Microsoft][Salesforce] (120) SOQL_FIRST mode prepare failure:\nSOQL error: [Microsoft][Salesforce] (30) Syntax error or access violation when parsing SOQL.\nSQL error: [Microsoft][SQLEngine] (31480) syntax error near 'SELECT Id FROM \""Object\"" WHERE LastModifiedDate &gt;= 2018-08-14T00&lt;&lt;&lt; ??? &gt;&gt;&gt;:00:00Z'.,Source=Microsoft.DataTransfer.ClientLibrary.Odbc.OdbcConnector,''Type=System.Data.Odbc.OdbcException,Message=ERROR [HY000] [Microsoft][Salesforce] (120) SOQL_FIRST mode prepare failure:\nSOQL error: [Microsoft][Salesforce] (30) Syntax error or access violation when parsing SOQL.\nSQL error: [Microsoft][SQLEngine] (31480) syntax error near 'SELECT Id FROM \""Object\"" WHERE LastModifiedDate &gt;= 2018-08-14T00&lt;&lt;&lt; ??? &gt;&gt;&gt;:00:00Z'.,Source=Microsoft Salesforce ODBC Driver,'"",
    ""failureType"": ""UserError"",
    ""target"": ""Event Fix Pipeline""
}
</code></pre>

<p>What could be causing this syntax error? </p>
","<salesforce><soql><azure-data-factory>","2018-08-15 20:25:31","2706","2","2","51869651","<p>Removing the quotation marks around Object seems to solve your problem.</p>

<p>SELECT Id FROM Object WHERE LastModifiedDate >= 2018-08-14T00:00:00Z.</p>

<p>I tested the query with quotation marks in Salesforce Developer Console and got ""Unknown error parsing query"", so I believe it's an invalid SOQL query with the quotation marks.</p>
"
"51865911","Syntax Error in Azure Data Factory SOQL Query WHERE LastModifiedDate >= datetime","<p>I'm working in <strong>Azure Data Factory V2</strong>, attempting to query from a Salesforce object where the LastModifiedDate in the object is greater than or equal to a recent date. I've been receiving syntax errors on extremely simple SOQL queries, queries that work just fine in Salesforce Workbench. Below, I've listed examples of queries I've run in the ADF Copy Data activity, along with the errors I received when debugging the pipeline.</p>

<p><strong>Query 1</strong></p>

<pre><code> Select Id from ""Object"" WHERE LastModifiedDate &gt;=
 @{formatDateTime(activity('Yesterday').output.firstRow.Yesterday,'yyyy-MM-ddTHH:mm:ssZ')}
</code></pre>

<p><strong>Result 1</strong></p>

<pre><code>{
    ""errorCode"": ""2200"",
    ""message"": ""Failure happened on 'Source' side. ErrorCode=UserErrorOdbcOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR [HY000] [Microsoft][Salesforce] (120) SOQL_FIRST mode prepare failure:\nSOQL error: [Microsoft][Salesforce] (30) Syntax error or access violation when parsing SOQL.\nSQL error: [Microsoft][SQLEngine] (31480) syntax error near 'Select Id from \""Object\"" WHERE LastModifiedDate &gt;= 2018-08-13T16&lt;&lt;&lt; ??? &gt;&gt;&gt;:05:06Z'.,Source=Microsoft.DataTransfer.ClientLibrary.Odbc.OdbcConnector,''Type=System.Data.Odbc.OdbcException,Message=ERROR [HY000] [Microsoft][Salesforce] (120) SOQL_FIRST mode prepare failure:\nSOQL error: [Microsoft][Salesforce] (30) Syntax error or access violation when parsing SOQL.\nSQL error: [Microsoft][SQLEngine] (31480) syntax error near 'Select Id from \""Object\"" WHERE LastModifiedDate &gt;= 2018-08-13T16&lt;&lt;&lt; ??? &gt;&gt;&gt;:05:06Z'.,Source=Microsoft Salesforce ODBC Driver,'"",
    ""failureType"": ""UserError"",
    ""target"": ""Event Fix Pipeline""
}
</code></pre>

<p><strong>Query 2</strong></p>

<pre><code>SELECT Id FROM ""Object"" WHERE LastModifiedDate &gt;= 2018-08-14T00:00:00Z
</code></pre>

<p><strong>Result 2</strong></p>

<pre><code>{
    ""errorCode"": ""2200"",
    ""message"": ""Failure happened on 'Source' side. ErrorCode=UserErrorOdbcOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR [HY000] [Microsoft][Salesforce] (120) SOQL_FIRST mode prepare failure:\nSOQL error: [Microsoft][Salesforce] (30) Syntax error or access violation when parsing SOQL.\nSQL error: [Microsoft][SQLEngine] (31480) syntax error near 'SELECT Id FROM \""Object\"" WHERE LastModifiedDate &gt;= 2018-08-14T00&lt;&lt;&lt; ??? &gt;&gt;&gt;:00:00Z'.,Source=Microsoft.DataTransfer.ClientLibrary.Odbc.OdbcConnector,''Type=System.Data.Odbc.OdbcException,Message=ERROR [HY000] [Microsoft][Salesforce] (120) SOQL_FIRST mode prepare failure:\nSOQL error: [Microsoft][Salesforce] (30) Syntax error or access violation when parsing SOQL.\nSQL error: [Microsoft][SQLEngine] (31480) syntax error near 'SELECT Id FROM \""Object\"" WHERE LastModifiedDate &gt;= 2018-08-14T00&lt;&lt;&lt; ??? &gt;&gt;&gt;:00:00Z'.,Source=Microsoft Salesforce ODBC Driver,'"",
    ""failureType"": ""UserError"",
    ""target"": ""Event Fix Pipeline""
}
</code></pre>

<p>What could be causing this syntax error? </p>
","<salesforce><soql><azure-data-factory>","2018-08-15 20:25:31","2706","2","2","68816382","<p>Lol, In my case it was a semicolon at the end of the SOQL statement.</p>
"
"51860202","Azure Data Factory v2 fails at sending data to Azure Search","<p>I'm trying to transfer some data to Azure Search, but for some reason it fails with <code>Invalid linked service reference. Name: AzureSearch1</code></p>

<p>I have set up an Azure Search Linked Service like this:</p>

<pre><code>{
""name"": ""AzureSearch1"",
""properties"": {
    ""type"": ""AzureSearch"",
    ""typeProperties"": {
        ""url"": ""https://xxxxxx.search.windows.net"",
        ""key"": {
            ""type"": ""AzureKeyVaultSecret"",
            ""store"": {
                ""referenceName"": ""AzureKeyVault"",
                ""type"": ""LinkedServiceReference""
            },
            ""secretName"": ""Search-AdminKey""
        }
    },
    ""connectVia"": {
        ""referenceName"": ""integrationRuntime1"",
        ""type"": ""IntegrationRuntimeReference""
    }
}
}
</code></pre>

<p>""Test connection"" works fine.</p>

<p>Now, I'm trying to create an Azure Search Indexer like this:</p>

<pre><code>{
    ""name"": ""AzureSearchIndex_PriceSheet"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureSearch1"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""AzureSearchIndex"",
        ""typeProperties"": {
            ""indexName"": ""pricesheet""
        }
    }
}
</code></pre>

<p>but it fails when I click ""Preview Data"" or ""Import Schema"" with this error:
<code>Invalid linked service reference. Name: AzureSearch1. Activity ID:2fa29fe9-ca5d-4308-af62-xxxxxxxxx</code></p>

<p>The integration pipeline is set to ""West Europe"" and Azure Search in provisioned in that region too.</p>

<p>Any thoughts?
Thanks!</p>
","<azure><azure-cognitive-search><azure-data-factory>","2018-08-15 14:00:47","522","1","1","51872960","<p>I tried to reproduce your issue but failed.Please refer to my working configuration:</p>

<p>My Azure Search Linked Service:</p>

<pre><code>{
    ""name"": ""AzureSearch1"",
    ""properties"": {
        ""type"": ""AzureSearch"",
        ""typeProperties"": {
            ""url"": ""https://***.search.windows.net"",
            ""key"": {
                ""type"": ""AzureKeyVaultSecret"",
                ""store"": {
                    ""referenceName"": ""AzureKeyVault1"",
                    ""type"": ""LinkedServiceReference""
                },
                ""secretName"": ""testas""
            }
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/linkedservices""
}
</code></pre>

<p>My Azure Search Indexer:</p>

<pre><code>{
    ""name"": ""AzureSearchIndex1"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureSearch1"",
            ""type"": ""LinkedServiceReference""
        },
        ""folder"": {
            ""name"": ""azureSearch""
        },
        ""type"": ""AzureSearchIndex"",
        ""typeProperties"": {
            ""indexName"": ""documentdb-index""
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p><strong><em>Perview Data:</em></strong></p>

<p><a href=""https://i.stack.imgur.com/PuJCA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PuJCA.png"" alt=""enter image description here""></a></p>

<p>I presumed it may because the secret which is stored in AKV has expired,then the link lost connection.I suggest you re-creating secret in AKV(just set default configuration) and try again.</p>

<p>Hope it helps you.Any concern,let me know.</p>
"
"51853329","Add file name as column in data factory pipeline destination","<p>I am new to DF. i am loading  bunch of csv files into a table and i would like to capture the name of the csv file as a new column in the destination table.  </p>

<p>Can someone please help how i can achieve this ? thanks in advance </p>
","<pipeline><azure-data-factory>","2018-08-15 05:17:41","6045","1","3","51858408","<p>If your destination is azure table storage, you could put your filename into partition key column. Otherwise, I think there is no native way to do this with ADF. You may need <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">custom activity</a> or <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#invoking-stored-procedure-for-sql-sink"" rel=""nofollow noreferrer"">stored procedure</a>. 
<a href=""https://i.stack.imgur.com/XBee4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XBee4.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/ze4qQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ze4qQ.png"" alt=""enter image description here""></a></p>
"
"51853329","Add file name as column in data factory pipeline destination","<p>I am new to DF. i am loading  bunch of csv files into a table and i would like to capture the name of the csv file as a new column in the destination table.  </p>

<p>Can someone please help how i can achieve this ? thanks in advance </p>
","<pipeline><azure-data-factory>","2018-08-15 05:17:41","6045","1","3","51904650","<ol>
<li><p>A post said the could use data bricks to handle this.
<a href=""https://stackoverflow.com/questions/51883988/data-factory-append-fields-to-json-sink"">Data Factory - append fields to JSON sink</a></p></li>
<li><p>Another post said they are using USQL to hanlde this.
<a href=""https://stackoverflow.com/questions/51760981/use-adf-pipeline-parameters-as-source-to-sink-columns-while-mapping"">use adf pipeline parameters as source to sink columns while mapping</a></p></li>
<li><p>For stored procedure, please reference this post. <a href=""https://stackoverflow.com/questions/51352725/azure-data-factory-mapping-2-columns-in-one-column"">Azure Data Factory mapping 2 columns in one column</a></p></li>
</ol>
"
"51853329","Add file name as column in data factory pipeline destination","<p>I am new to DF. i am loading  bunch of csv files into a table and i would like to capture the name of the csv file as a new column in the destination table.  </p>

<p>Can someone please help how i can achieve this ? thanks in advance </p>
","<pipeline><azure-data-factory>","2018-08-15 05:17:41","6045","1","3","66317165","<p>If you use a Mapping data flow, there is an option under source settings to hold the File name being used. And later in it can be mapped to column in Sink.</p>
"
"51847265","Azure Data Factory ARM template","<p>I am creating an ARM template to deploy the Data Factory. Everything is working fine except the self hosted Integration Runtime. I have one On-premise SQL Server and another On-premise Oracle DB. Both are connected using self hosted Integration Runtime. I am getting the below error while deploying the ARM template.</p>

<pre><code>2018-08-13T14:11:34.9569812Z ##[error]At least one resource deployment operation failed. Please list deployment operations for details. Please see https://aka.ms/arm-debug for usage details.
2018-08-13T14:11:34.9582896Z ##[error]Details:
2018-08-13T14:11:34.9587327Z ##[error]BadRequest: {
  ""code"": ""BadRequest"",
  ""message"": ""Failed to encrypt sub-resource payload {\r\n  \""Id\"": \""/subscriptions/bf2c5c07-2536-497d-9b87-7f0a0fa5a2b3/resourceGroups/ResourceGroup_Name/providers/Microsoft.DataFactory/factories/DataFactoryName/linkedservices/Oracle_Linked_ServiceName\"",\r\n  \""Name\"": \""Oracle_Linked_ServiceName\"",\r\n  \""Properties\"": {\r\n    \""type\"": \""Oracle\"",\r\n    \""typeProperties\"": {\r\n      \""type\"": \""********************\"",\r\n      \""connectionString\"": \""********************\""\r\n    },\r\n    \""connectVia\"": {\r\n      \""referenceName\"": \""IntegrationRuntime-Name\"",\r\n      \""type\"": \""IntegrationRuntimeReference\""\r\n    }\r\n  }\r\n} and error is: Failed to encrypted linked service credentials on self-hosted IR 'IntegrationRuntime-Name', reason is: NotFound, error message is: No online instance.."",
  ""target"": ""/subscriptions/bf2c5c07-2536-497d-9b87-7f0a0fa5a2b3/resourceGroups/ResourceGroup_Name/providers/Microsoft.DataFactory/factories/DataFactoryName/linkedservices/Oracle_Linked_ServiceName"",
  ""details"": null,
  ""error"": null
} undefined
2018-08-13T14:11:34.9599952Z ##[error]BadRequest: {
  ""code"": ""BadRequest"",
  ""message"": ""Failed to encrypt sub-resource payload {\r\n  \""Id\"": \""/subscriptions/bf2c5c07-2536-497d-9b87-7f0a0fa5a2b3/resourceGroups/ResourceGroup_Name/providers/Microsoft.DataFactory/factories/DataFactoryName/linkedservices/Source_OnPremSQL\"",\r\n  \""Name\"": \""Source_OnPremSQL\"",\r\n  \""Properties\"": {\r\n    \""type\"": \""SqlServer\"",\r\n    \""typeProperties\"": {\r\n      \""connectionString\"": \""********************\"",\r\n      \""username\"": \""********************\"",\r\n      \""password\"": \""********************\""\r\n    },\r\n    \""connectVia\"": {\r\n      \""referenceName\"": \""IntegrationRuntime-Name\"",\r\n      \""type\"": \""IntegrationRuntimeReference\""\r\n    }\r\n  }\r\n} and error is: Failed to encrypted linked service credentials on self-hosted IR 'IntegrationRuntime-Name', reason is: NotFound, error message is: No online instance.."",
  ""target"": ""/subscriptions/bf2c5c07-2536-497d-9b87-7f0a0fa5a2b3/resourceGroups/ResourceGroup_Name/providers/Microsoft.DataFactory/factories/DataFactoryName/linkedservices/Source_OnPremSQL"",
  ""details"": null,
  ""error"": null
} undefined
2018-08-13T14:11:34.9602482Z ##[error]Task failed while creating or updating the template deployment.
2018-08-13T14:11:34.9611870Z ##[section]Finishing: DataFactoryDeployment
</code></pre>
","<azure><azure-resource-manager><azure-rm-template><azure-data-factory>","2018-08-14 17:56:59","1612","2","2","51895615","<p>Make sure your selfhosted IR is online. It is needed to encrypt your credentials.</p>
"
"51847265","Azure Data Factory ARM template","<p>I am creating an ARM template to deploy the Data Factory. Everything is working fine except the self hosted Integration Runtime. I have one On-premise SQL Server and another On-premise Oracle DB. Both are connected using self hosted Integration Runtime. I am getting the below error while deploying the ARM template.</p>

<pre><code>2018-08-13T14:11:34.9569812Z ##[error]At least one resource deployment operation failed. Please list deployment operations for details. Please see https://aka.ms/arm-debug for usage details.
2018-08-13T14:11:34.9582896Z ##[error]Details:
2018-08-13T14:11:34.9587327Z ##[error]BadRequest: {
  ""code"": ""BadRequest"",
  ""message"": ""Failed to encrypt sub-resource payload {\r\n  \""Id\"": \""/subscriptions/bf2c5c07-2536-497d-9b87-7f0a0fa5a2b3/resourceGroups/ResourceGroup_Name/providers/Microsoft.DataFactory/factories/DataFactoryName/linkedservices/Oracle_Linked_ServiceName\"",\r\n  \""Name\"": \""Oracle_Linked_ServiceName\"",\r\n  \""Properties\"": {\r\n    \""type\"": \""Oracle\"",\r\n    \""typeProperties\"": {\r\n      \""type\"": \""********************\"",\r\n      \""connectionString\"": \""********************\""\r\n    },\r\n    \""connectVia\"": {\r\n      \""referenceName\"": \""IntegrationRuntime-Name\"",\r\n      \""type\"": \""IntegrationRuntimeReference\""\r\n    }\r\n  }\r\n} and error is: Failed to encrypted linked service credentials on self-hosted IR 'IntegrationRuntime-Name', reason is: NotFound, error message is: No online instance.."",
  ""target"": ""/subscriptions/bf2c5c07-2536-497d-9b87-7f0a0fa5a2b3/resourceGroups/ResourceGroup_Name/providers/Microsoft.DataFactory/factories/DataFactoryName/linkedservices/Oracle_Linked_ServiceName"",
  ""details"": null,
  ""error"": null
} undefined
2018-08-13T14:11:34.9599952Z ##[error]BadRequest: {
  ""code"": ""BadRequest"",
  ""message"": ""Failed to encrypt sub-resource payload {\r\n  \""Id\"": \""/subscriptions/bf2c5c07-2536-497d-9b87-7f0a0fa5a2b3/resourceGroups/ResourceGroup_Name/providers/Microsoft.DataFactory/factories/DataFactoryName/linkedservices/Source_OnPremSQL\"",\r\n  \""Name\"": \""Source_OnPremSQL\"",\r\n  \""Properties\"": {\r\n    \""type\"": \""SqlServer\"",\r\n    \""typeProperties\"": {\r\n      \""connectionString\"": \""********************\"",\r\n      \""username\"": \""********************\"",\r\n      \""password\"": \""********************\""\r\n    },\r\n    \""connectVia\"": {\r\n      \""referenceName\"": \""IntegrationRuntime-Name\"",\r\n      \""type\"": \""IntegrationRuntimeReference\""\r\n    }\r\n  }\r\n} and error is: Failed to encrypted linked service credentials on self-hosted IR 'IntegrationRuntime-Name', reason is: NotFound, error message is: No online instance.."",
  ""target"": ""/subscriptions/bf2c5c07-2536-497d-9b87-7f0a0fa5a2b3/resourceGroups/ResourceGroup_Name/providers/Microsoft.DataFactory/factories/DataFactoryName/linkedservices/Source_OnPremSQL"",
  ""details"": null,
  ""error"": null
} undefined
2018-08-13T14:11:34.9602482Z ##[error]Task failed while creating or updating the template deployment.
2018-08-13T14:11:34.9611870Z ##[section]Finishing: DataFactoryDeployment
</code></pre>
","<azure><azure-resource-manager><azure-rm-template><azure-data-factory>","2018-08-14 17:56:59","1612","2","2","54934420","<p>If you used a connection string and password to make your connection, then the ARM template will have them, so you need to fill the connection strings again based on what is expected from each connector.</p>

<p>I had the same issue and fixed by filling oracle connection string with <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-oracle#linked-service-properties"" rel=""nofollow noreferrer"">this</a> and Azure SQL connector with <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#sql-authentication"" rel=""nofollow noreferrer"">this</a>(use the JSON value key as a template)</p>

<p>To get rid of this issue permanently I suggest you use the <a href=""https://azure.microsoft.com/pt-br/blog/secure-credential-management-for-etl-workloads-using-azure-data-factory-and-azure-key-vault/"" rel=""nofollow noreferrer"">Azure Key Vault</a> to store your credentials, so when a new import is needed, the ARM template will carry the secret keys, but not it's values.</p>

<p>PS: you need to authorize Data Factory to it to be able to access the stored credentials.</p>
"
"51844954","Hash of File on Azure Data Lake Store","<p>I am new to using azure data lake store and azure analytics. </p>

<h2>Question</h2>

<blockquote>
  <p>Is there a way to get the hash of the file (or files) stored in Azure Datalake store ? So that I can analyze whether data has changed</p>
</blockquote>

<p>I have a bunch of input files stored with a similar structure</p>

<ul>
<li><p>/Input/</p>

<ul>
<li>Client-01/</li>
<li>Product-A/

<ul>
<li>Input01.csv</li>
</ul></li>
</ul></li>
<li><p>/Input/</p>

<ul>
<li>Client-02/</li>
<li>Product-A/

<ul>
<li>Input01.csv</li>
<li>Input02.csv</li>
</ul></li>
</ul></li>
</ul>

<h2>What have I tried</h2>

<h2>Part 01</h2>

<p>I was able to get the <code>Get-FileHash</code> locally but could NOT find anything for ADLS or anything remotely similar to this</p>

<pre><code>Get-FileHash 
   ""Input/Client-01/*.csv"" -Algorithm MD5 | ConvertTo-Json &gt;&gt; statistics.json
</code></pre>

<p>to generate hashes like</p>

<pre><code>[
    {
        ""Algorithm"":  ""MD5"",
        ""Hash"":  ""BA961B4B72DC602C2D2CA2B13EFC09DB"",
        ""Path"":  ""Input/Client-01/Input01.csv""
    },
    {
        ""Algorithm"":  ""MD5"",
        ""Hash"":  ""B0528707D4E689EEEFE1AA1811063014"",
        ""Path"":  ""Input/Client-02/Input01.csv""
    },
    {
        ""Algorithm"":  ""MD5"",
        ""Hash"":  ""60D71494355E7EE941782F1BE2969F3C"",
        ""Path"":  ""Input/Client-02/Input02.csv""
    }
]
</code></pre>

<h2>Part 02</h2>

<p>I was able to get some more details using </p>

<pre><code>Get-AzureRmDataLakeStoreChildItem -Account 
   $datalakeStoreName -Path 
   $path | ConvertTo-Json
</code></pre>

<p>which results in </p>

<pre><code>{
    ""LastWriteTime"":  ""\/Date(1534185132238)\/"",
    ""LastAccessTime"":  ""\/Date(1534185132180)\/"",
    ""Expiration"":  null,
    ""Name"":  ""Input01.csv"",
    ""Path"":  ""/Input/Client-01/"",
    ""AccessTime"":  1534185132180,
    ""BlockSize"":  268435456,
    ""ChildrenNum"":  null,
    ""ExpirationTime"":  null,
    ""Group"":  ""e045d366-777b-4e7a-a01d-79dbf0e28a61"",
    ""Length"":  127,
    ""ModificationTime"":  1534185132238,
    ""Owner"":  ""3bb6c9c4-da61-4cc2-b6ef-f4739adafff5"",
    ""PathSuffix"":  ""Input01.csv"",
    ""Permission"":  ""770"",
    ""Type"":  0,
    ""AclBit"":  true
}
</code></pre>

<h3>Drawbacks :</h3>

<ul>
<li>there is no hash :(</li>
<li>running this on a schedule would involve something like a batch service on data factory (its technically not a draw back, but it was for me as I am invested in batch services yet..)</li>
</ul>

<h2>Part 3 : using ADLS nuget</h2>

<p>the ADLS nuget supports few endpoints. I was specially looking at <code>DirectoryEntry</code> however the model did not expose <code>BlockSize</code> available in other endpoints :(</p>

<blockquote>
  <p><a href=""https://github.com/Azure-Samples/data-lake-store-adls-dot-net-get-started/blob/master/AdlsSDKGettingStarted/Program.cs"" rel=""nofollow noreferrer"">https://github.com/Azure-Samples/data-lake-store-adls-dot-net-get-started/blob/master/AdlsSDKGettingStarted/Program.cs</a></p>
</blockquote>

<pre><code>private static void PrintDirectoryEntry(DirectoryEntry entry)
        {
            Console.WriteLine($""Name: {entry.Name}"");
            Console.WriteLine($""FullName: {entry.FullName}"");
            Console.WriteLine($""Length: {entry.Length}"");
            Console.WriteLine($""Type: {entry.Type}"");
            Console.WriteLine($""User: {entry.User}"");
            Console.WriteLine($""Group: {entry.Group}"");
            Console.WriteLine($""Permission: {entry.Permission}"");
            Console.WriteLine($""Modified Time: {entry.LastModifiedTime}"");
            Console.WriteLine($""Last Accessed Time: {entry.LastAccessTime}"");
            Console.WriteLine();
        }
</code></pre>

<h2>Part 4 : using webHDFS API (Somewhat worked)</h2>

<blockquote>
  <p><a href=""https://learn.microsoft.com/en-us/rest/api/datalakestore/webhdfs-filesystem-apis"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/datalakestore/webhdfs-filesystem-apis</a></p>
</blockquote>

<p>I was able to use the <code>op=LISTSTATUS</code><a href=""http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/WebHDFS.html#List_a_Directory"" rel=""nofollow noreferrer"">documentation link</a> to get <code>FileStatuses</code> which has both <code>blocksize</code> and <code>length</code>. so this is somewhat helpful</p>

<pre><code>HTTP/1.1 200 OK
Content-Type: application/json
Content-Length: 427

{
  ""FileStatuses"":
  {
    ""FileStatus"":
    [
      {
        ""accessTime""      : 1320171722771,
        ""blockSize""       : 33554432,
        ""group""           : ""supergroup"",
        ""length""          : 24930,
        ""modificationTime"": 1320171722771,
        ""owner""           : ""webuser"",
        ""pathSuffix""      : ""a.patch"",
        ""permission""      : ""644"",
        ""replication""     : 1,
        ""type""            : ""FILE""
      },
      {
        ""accessTime""      : 0,
        ""blockSize""       : 0,
        ""group""           : ""supergroup"",
        ""length""          : 0,
        ""modificationTime"": 1320895981256,
        ""owner""           : ""username"",
        ""pathSuffix""      : ""bar"",
        ""permission""      : ""711"",
        ""replication""     : 0,
        ""type""            : ""DIRECTORY""
      },
      ...
    ]
  }
}
</code></pre>
","<powershell><azure-data-lake><azure-data-factory>","2018-08-14 15:29:35","936","2","1","51853076","<p>Are you looking to identify if the file has changed or actually identify the rows within the file that has changed? if you want to identify row changes then use an ADLA job to run a U-SQL Script or function That creates a row hash. </p>

<p>If you want to identify if the file has changed, I suspect you would need to run a job to loop through all of the files and for each file generate a hash. Then you could store this value in either another file or a table where you maintain a list of the files and the historical hash values. 
It's not going to be a single step process Azure Data Factory or a PowerShell Runbook would be the best way to orchestrate this process.</p>
"
"51841351","How to archive my on premise files once they are processed into azure data lake?","<p>I have a pipeline activity which processes my on premise files inside file share into azure data lake. And then I have to automate the process of whenever the data is processed inside the data lake, I want to archive my source files.</p>

<p>STEP1: I have builtin a logic app which can automatically copy any new data from IN folder to OUT directory and delete the old files in IN directory.</p>

<p>STEP2: I have builtin my pipeline which processes data inside my Data lake.</p>

<p>Now how can I trigger my logic app inside my pipeline through some activity which will automatically delete my on premise files by calling my logic app ?</p>

<p>Please suggest</p>
","<azure><filesystems><azure-data-factory>","2018-08-14 12:23:03","74","-3","1","51895918","<ol>
<li><p>ADF Web Activity can be used to call a custom REST endpoint from a Data Factory pipeline. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">enter link description here</a>
And as far as I know,logic app can listen http request. So maybe you could chain a web activity after your data processing activity?</p></li>
<li><p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">ADF custom activity</a> runs your customized code logic on an Azure Batch pool of virtual machines. So I think you could also put your trigger logic into custom activity?</p></li>
<li><p>Another way I could think is, maybe you could invoke your logic app based on some schedule? like daily or weekly?</p></li>
</ol>
"
"51830812","When doing an Import of an Azure Data Factory V2 ARM template, unexpected characters are added at the top","<p>I'm following the Continuous <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">integration deployment guide</a> for azure data factory V2, I'm stuck at the Import ARM step, for some reason, after Loading the arm_template.zip I get unexpected characters that cause my json file to fail<a href=""https://i.stack.imgur.com/yK7xj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yK7xj.png"" alt=""enter image description here""></a></p>

<p>Any idea idea on how to avoid these or clean these?</p>

<p>For now I just removed everything before the opening bracket and made sure the JSON was well formed and looks like it worked! however I'm still curious to know why the Import is getting these unexpected characters</p>
","<azure><azure-data-factory>","2018-08-13 21:27:45","1119","0","1","51914954","<p>The unexpected characters are actually zip file header.</p>

<p>You need unzip the arm_template.zip yourself and upload arm_template.json file only.</p>
"
"51829062","How to move on premises file to a different directory using azure data factory","<p>I am trying to move on premises file to a different directory after they are done getting copied over to azure blob storage, using azure data factory. Does anybody have an idea on how to achieve it?  </p>
","<azure><azure-storage><azure-data-factory>","2018-08-13 19:10:43","636","3","2","51836495","<p>I suggest you using <code>Azure Trigger Function</code> to implement your requirements.</p>

<p>Step 1:
You could create an <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-triggered-function"" rel=""nofollow noreferrer"">Azure Function Blob Trigger</a> to monitor your blob account.</p>

<p>Step 2:
Whenever a blob file enters a blob storage , the code in the trigger will be executed. You could get the blob full name and push it as message into e.g. <code>Azure Queue Storage</code>.</p>

<p>Step 3:
Then you create an on premise service (or application or script) which runs continuously to monitor the queue storage.If a message enters, then get the file name and move it to another folder.</p>

<p>Hope it helps you.</p>
"
"51829062","How to move on premises file to a different directory using azure data factory","<p>I am trying to move on premises file to a different directory after they are done getting copied over to azure blob storage, using azure data factory. Does anybody have an idea on how to achieve it?  </p>
","<azure><azure-storage><azure-data-factory>","2018-08-13 19:10:43","636","3","2","51839024","<p>If you want to achieve using Azure Data Factory - </p>

<p>Have an event-based triggers that you can create in your Data Factory(V2) pipeline, this would basically trigger pipelines based on events.
From the documentation:</p>

<blockquote>
  <p>As soon as the file arrives in your storage location and the
  corresponding blob is created, this event triggers and runs your Data
  Factory pipeline. You can create a trigger that responds to a blob
  creation event, a blob deletion event, or both events, in your Data
  Factory pipelines.</p>
</blockquote>

<p>Link - <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger"" rel=""nofollow noreferrer"">here</a>. &amp; <a href=""https://azure.microsoft.com/en-us/blog/event-trigger-based-scheduling-in-azure-data-factory/"" rel=""nofollow noreferrer"">here</a></p>

<p>Once this is done your next step would be copy activity for Azure Data Factory to copy data from and to file system. Note - you need to set up a Self-hosted Integration Runtime for copy activity to work on file system (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">link</a>)</p>

<p>Link - <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system"" rel=""nofollow noreferrer"">here</a>.</p>
"
"51788496","Daylight Saving Time Changes in Trigger (Data Factory v2)","<p>How can I set a Trigger in a pipeline (Data Factory v2) to scheduler a specific start hour to a Country with Daylight Saving Time Changes?</p>
","<azure-data-factory>","2018-08-10 14:17:57","2176","1","3","51791502","<p>According to the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers"" rel=""nofollow noreferrer"">documentation</a>, even though there's a time zone property for the trigger, only UTC is currently supported. When other time zones are supported, specifying that time zone would make the trigger fire at that local time.</p>
"
"51788496","Daylight Saving Time Changes in Trigger (Data Factory v2)","<p>How can I set a Trigger in a pipeline (Data Factory v2) to scheduler a specific start hour to a Country with Daylight Saving Time Changes?</p>
","<azure-data-factory>","2018-08-10 14:17:57","2176","1","3","55944802","<p>I have not tried this, and it's a bit hacky, but as a workaround, could you not change to a hourly scheduled trigger and then pass in the UTC triggered time to the pipeline. You could then have another trigger param which is the timezone and actual time that when you want to exectute. </p>

<p>The first step on the pipeline would hit an Azure function and convert the time to your local time zone (which would consider daylight saving). An IF statement would then exit or continue if the resulting time matched the requested timezone hour.</p>

<p>In the interests of time/cost/tech debt I probably would not do though - I would just manually work out the different and update it when daylight saving happened.</p>
"
"51788496","Daylight Saving Time Changes in Trigger (Data Factory v2)","<p>How can I set a Trigger in a pipeline (Data Factory v2) to scheduler a specific start hour to a Country with Daylight Saving Time Changes?</p>
","<azure-data-factory>","2018-08-10 14:17:57","2176","1","3","63927267","<p>I’ve found a simple work around is to start your pipeline with the following if condition.</p>
<pre><code>if(Equals(addHours(convertTimeZone(utcNow(), 'UTC', 'Central Standard Time'), 5, 'HH'),utcNow('HH')),true,false)
</code></pre>
<p>Lets break the if down into the following lines…</p>
<pre><code>1. if(
2. Equals(
3. addHours(convertTimeZone(utcNow(), 'UTC', 'Central Standard Time'), 5, 'HH')
4. ,utcNow('HH')
5. )
6. ,true,false)
</code></pre>
<p>In line 3 we get current time in UTC (19:30) and convert to CST (15:30), add 5 hours, and then take just the hours value
19:30 &gt;&gt; 15:30 + 5:00 = 19</p>
<p>In line 4 we are getting the timestamp in UTC and extracting just the hours (HH) and in this case it is 19.</p>
<p>Finally this is wrapped in an “Equals” statement… if the two hours values are equal then true, else false.</p>
<p>If true, we do nothing and continue on to the next step of the pipeline.  If false then we are no longer in day light savings time and we wait one hour to continue.  I have found this works even when the difference between UTC and CST spans midnight.</p>
<p>Below is the json….</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;name&quot;: &quot;If DST&quot;,
  &quot;type&quot;: &quot;IfCondition&quot;,
  &quot;dependsOn&quot;: [],
  &quot;userProperties&quot;: [],
  &quot;typeProperties&quot;: {
    &quot;expression&quot;: {
      &quot;value&quot;: &quot;@if(Equals(addHours(convertTimeZone(utcNow(), 'UTC', 'Central Standard Time'), 5, 'HH'),utcNow('HH')),true,false)&quot;,
      &quot;type&quot;: &quot;Expression&quot;
    },
    &quot;ifFalseActivities&quot;: [
      {
        &quot;name&quot;: &quot;Wait 1 hour&quot;,
        &quot;type&quot;: &quot;Wait&quot;,
        &quot;dependsOn&quot;: [],
        &quot;userProperties&quot;: [],
        &quot;typeProperties&quot;: {
          &quot;waitTimeInSeconds&quot;: 3600
        }
      }
    ]
  }
}
</code></pre>
"
"51787360","Azure Data Factory v2 and Integration Runtime in different geo datacenters","<p>On the general availability list we may see that ""Integration Runtime"" feature is generally available in some datacenters where even Azure Data Factory v2 is not yet available (i.e. I really cannot provision it there).</p>

<p>Logically, Azure Data Factory is assumed as a parenting relationship for ""Integration Runtime"".  Even though I can provision each one of them in different geographical regions.  Does it imply anything? Is there any explicit network communication ongoing between these resources?  </p>

<p>I am asking b/c we consider enabling ""Integration Runtime"" in the regions where ADF v2 itself is not yet generally available.</p>
","<azure><networking><ssis><region><azure-data-factory>","2018-08-10 13:12:09","511","2","1","51814306","<p>Please reference <a href=""https://learn.microsoft.com/en-us/azure/data-factory/introduction#supported-regions"" rel=""nofollow noreferrer"">this doc</a></p>

<p>""However, a data factory can access data stores and compute services in other Azure regions to move data between data stores or process data using compute services.</p>

<p>Azure Data Factory itself does not store any data. It lets you create data-driven workflows to orchestrate the movement of data between supported data stores and the processing of data using compute services in other regions or in an on-premises environment. It also allows you to monitor and manage workflows by using both programmatic and UI mechanisms.</p>

<p>Although Data Factory is available only in certain regions, the service that powers the data movement in Data Factory is available globally in several regions. If a data store is behind a firewall, then a Self-hosted Integration Runtime that's installed in your on-premises environment moves the data instead.</p>

<p>For an example, let's assume that your compute environments such as Azure HDInsight cluster and Azure Machine Learning are running out of the West Europe region. You can create and use an Azure Data Factory instance in East US or East US 2 and use it to schedule jobs on your compute environments in West Europe. It takes a few milliseconds for Data Factory to trigger the job on your compute environment, but the time for running the job on your computing environment does not change.""</p>
"
"51776046","Azure Data Factory mapping part of a value","<p>I'm copying data from an CSV file in Azure Blob storage to Azure Table storage with Azure Data Factory v2.
One of the columns in the sources contains UK post codes, e.g. RM1 1AA. I'd like to use the first part as a Partition Key in the table.</p>

<p>I'm trying to use dynamic content, but I can't find a function to target the value. My column is called ""pcds"". What do I write instead of the ""???"" to make it capture the first part from the pcds column?</p>

<p><a href=""https://i.stack.imgur.com/3EHA5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3EHA5.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2018-08-09 21:31:14","380","1","1","51784159","<p>Based on my <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">research</a>, you can not set dynamic content for the source column.You could set partition key as source column exactly or just set it from parameters.</p>

<p><a href=""https://i.stack.imgur.com/36vVJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/36vVJ.png"" alt=""enter image description here""></a></p>

<p>However, I provide you with a workaround. You could use <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-table"" rel=""nofollow noreferrer"">Azure Funtion Table Storage Trigger</a> to process the partition key as your want when it is inserted into Table Storage.Just get the partition key value and use <code>str.Substring</code> method in Azure Function code.</p>

<p>Hope it helps you.</p>
"
"51763462","How to unzip a file in Microsoft Azure which is of size 900mb?","<p>I have a zip file in my blob of size 900mb, so how can I unzip the same in the Azure platform itself?</p>

<p>Have tried using blob to blob unzipping using logical app. But there the maximum file size is 50mb only.</p>

<p>Any inputs is appreciated.</p>
","<visual-studio><azure><unzip><azure-data-factory><azure-logic-apps>","2018-08-09 09:30:13","4472","0","2","51768355","<p>You have the option to go with Azure Data Factory. Azure Data Factory supports to decompress data during copy. Specify the compression property in an input dataset and the copy activity reads the compressed data from the source and decompress it.</p>

<p>Also,there is an option to specify the property in an output dataset which would make the copy activity compress then write data to the sink.</p>

<p>For your use-case - you need to read a compressed (example GZIP) data from an Azure blob, decompress it and write result data to an Azure blob, so define the input Azure Blob dataset with compression type set to GZIP.</p>

<p>Link - <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#compression-support"" rel=""nofollow noreferrer"">ADF - compression support</a></p>
"
"51763462","How to unzip a file in Microsoft Azure which is of size 900mb?","<p>I have a zip file in my blob of size 900mb, so how can I unzip the same in the Azure platform itself?</p>

<p>Have tried using blob to blob unzipping using logical app. But there the maximum file size is 50mb only.</p>

<p>Any inputs is appreciated.</p>
","<visual-studio><azure><unzip><azure-data-factory><azure-logic-apps>","2018-08-09 09:30:13","4472","0","2","51797089","<p>As Abhishek mentioned, you could use ADF. And you could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-copy-data-tool"" rel=""nofollow noreferrer"">copy data tool</a> to help you create the pipeline.
For example, if you just want to unzip a file, you could use the following settings.<a href=""https://i.stack.imgur.com/L7Fqu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L7Fqu.png"" alt=""enter image description here""></a></p>
"
"51760981","use adf pipeline parameters as source to sink columns while mapping","<p>I have an ADF pipeline with copy activity, I'm copying data from blob storage CSV file to SQL database, this is working as expected. I need to map Name of the CSV file (this coming from pipeline parameters) and save it in the destination table. I'm wondering if there is a way to map parameters to destination columns.</p>
","<azure-data-factory>","2018-08-09 07:19:45","3245","2","2","51815046","<p>Column name can't directly use parameters. But you can use parameter for the whole structure property in dataset and columnMappings property in copy activity. 
This might be a little tedious as you will need to write the whole structure array and columnMappings on your own and pass them as parameters into pipeline.</p>
"
"51760981","use adf pipeline parameters as source to sink columns while mapping","<p>I have an ADF pipeline with copy activity, I'm copying data from blob storage CSV file to SQL database, this is working as expected. I need to map Name of the CSV file (this coming from pipeline parameters) and save it in the destination table. I'm wondering if there is a way to map parameters to destination columns.</p>
","<azure-data-factory>","2018-08-09 07:19:45","3245","2","2","64136418","<p>In DF v2 in Copy Data activity, it is possible to add a new column to the source with value $$FILEPATH, and then each record will have a name of the input file.</p>
<p>Azure DF v2, CopyData activity -&gt; Source
<a href=""https://i.stack.imgur.com/KEnxd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KEnxd.png"" alt=""CopyData activity -&gt; Source"" /></a></p>
"
"51745187","Access Azure Blob storage account from azure data factory","<p>I have a folder with list of files in my storage account and having been trying to delete one of the files using pipeline. In-order to get that done I have used ""Web"" in pipeline, copied the blob storage url and access keys.</p>

<p>Tired using the access keys directly under Headers|Authorization. Also tried the concept of Shared Keys at <a href=""https://learn.microsoft.com/en-us/azure/storage/common/storage-rest-api-auth#creating-the-authorization-header"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/storage/common/storage-rest-api-auth#creating-the-authorization-header</a></p>

<p>Even tried getting this work with curl, but it returned an Authentication Error every time I tried to run</p>

<pre><code># List the blobs in an Azure storage container.

echo ""usage: ${0##*/} &lt;storage-account-name&gt; &lt;container-name&gt; &lt;access-key&gt;""

storage_account=""$1""
container_name=""$2""
access_key=""$3""

blob_store_url=""blob.core.windows.net""
authorization=""SharedKey""

request_method=""DELETE""
request_date=$(TZ=GMT LC_ALL=en_US.utf8 date ""+%a, %d %h %Y %H:%M:%S %Z"")
#request_date=""Mon, 18 Apr 2016 05:16:09 GMT""
storage_service_version=""2018-03-28""

# HTTP Request headers
x_ms_date_h=""x-ms-date:$request_date""
x_ms_version_h=""x-ms-version:$storage_service_version""

# Build the signature string
canonicalized_headers=""${x_ms_date_h}\n${x_ms_version_h}""
canonicalized_resource=""/${storage_account}/${container_name}""

string_to_sign=""${request_method}\n\n\n\n\n\n\n\n\n\n\n\n${canonicalized_headers}\n${canonicalized_resource}\ncomp:list\nrestype:container""


# Decode the Base64 encoded access key, convert to Hex.
decoded_hex_key=""$(echo -n $access_key | base64 -d -w0 | xxd -p -c256)""


# Create the HMAC signature for the Authorization header
signature=$(printf ""$string_to_sign"" | openssl dgst -sha256 -mac HMAC -macopt ""hexkey:$decoded_hex_key"" -binary |  base64 -w0)

authorization_header=""Authorization: $authorization $storage_account:$signature""

curl \
  -H ""$x_ms_date_h"" \
  -H ""$x_ms_version_h"" \
  -H ""$authorization_header"" \
  -H ""Content-Length: 0""\
  -X DELETE  ""https://${storage_account}.${blob_store_url}/${container_name}/myfile.csv_123""
</code></pre>

<p>The curl command returns an error:</p>

<p><code>&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;&lt;Error&gt;&lt;Code&gt;AuthenticationFailed&lt;/Code&gt;&lt;Message&gt;Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.
RequestId:XX
Time:2018-08-09T10:09:41.3394688Z&lt;/Message&gt;&lt;AuthenticationErrorDetail&gt;The MAC signature found in the HTTP request 'xxx' is not the same as any computed signature. Server used following string to sign: 'DELETE
</code></p>
","<azure><azure-data-factory>","2018-08-08 11:09:04","790","0","2","51763053","<p>You cannot authorize directly from the Data Factory to the storage account API. I suggest that you use an Logic App. The Logic App has built in support for Blob store:
<a href=""https://learn.microsoft.com/en-us/azure/connectors/connectors-create-api-azureblobstorage"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/connectors/connectors-create-api-azureblobstorage</a></p>

<p>You can call the Logic App from the Data Factory Web Activity. Using the body of the Data Factory request you can pass variables to the Logic app like the blob path.</p>
"
"51745187","Access Azure Blob storage account from azure data factory","<p>I have a folder with list of files in my storage account and having been trying to delete one of the files using pipeline. In-order to get that done I have used ""Web"" in pipeline, copied the blob storage url and access keys.</p>

<p>Tired using the access keys directly under Headers|Authorization. Also tried the concept of Shared Keys at <a href=""https://learn.microsoft.com/en-us/azure/storage/common/storage-rest-api-auth#creating-the-authorization-header"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/storage/common/storage-rest-api-auth#creating-the-authorization-header</a></p>

<p>Even tried getting this work with curl, but it returned an Authentication Error every time I tried to run</p>

<pre><code># List the blobs in an Azure storage container.

echo ""usage: ${0##*/} &lt;storage-account-name&gt; &lt;container-name&gt; &lt;access-key&gt;""

storage_account=""$1""
container_name=""$2""
access_key=""$3""

blob_store_url=""blob.core.windows.net""
authorization=""SharedKey""

request_method=""DELETE""
request_date=$(TZ=GMT LC_ALL=en_US.utf8 date ""+%a, %d %h %Y %H:%M:%S %Z"")
#request_date=""Mon, 18 Apr 2016 05:16:09 GMT""
storage_service_version=""2018-03-28""

# HTTP Request headers
x_ms_date_h=""x-ms-date:$request_date""
x_ms_version_h=""x-ms-version:$storage_service_version""

# Build the signature string
canonicalized_headers=""${x_ms_date_h}\n${x_ms_version_h}""
canonicalized_resource=""/${storage_account}/${container_name}""

string_to_sign=""${request_method}\n\n\n\n\n\n\n\n\n\n\n\n${canonicalized_headers}\n${canonicalized_resource}\ncomp:list\nrestype:container""


# Decode the Base64 encoded access key, convert to Hex.
decoded_hex_key=""$(echo -n $access_key | base64 -d -w0 | xxd -p -c256)""


# Create the HMAC signature for the Authorization header
signature=$(printf ""$string_to_sign"" | openssl dgst -sha256 -mac HMAC -macopt ""hexkey:$decoded_hex_key"" -binary |  base64 -w0)

authorization_header=""Authorization: $authorization $storage_account:$signature""

curl \
  -H ""$x_ms_date_h"" \
  -H ""$x_ms_version_h"" \
  -H ""$authorization_header"" \
  -H ""Content-Length: 0""\
  -X DELETE  ""https://${storage_account}.${blob_store_url}/${container_name}/myfile.csv_123""
</code></pre>

<p>The curl command returns an error:</p>

<p><code>&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;&lt;Error&gt;&lt;Code&gt;AuthenticationFailed&lt;/Code&gt;&lt;Message&gt;Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.
RequestId:XX
Time:2018-08-09T10:09:41.3394688Z&lt;/Message&gt;&lt;AuthenticationErrorDetail&gt;The MAC signature found in the HTTP request 'xxx' is not the same as any computed signature. Server used following string to sign: 'DELETE
</code></p>
","<azure><azure-data-factory>","2018-08-08 11:09:04","790","0","2","55136706","<pre><code>using System;
using System.Collections.Generic;
using System.Linq;
using Microsoft.Rest;
using Microsoft.Azure.Management.ResourceManager;
using Microsoft.Azure.Management.DataFactory;
using Microsoft.Azure.Management.DataFactory.Models;
using Microsoft.IdentityModel.Clients.ActiveDirectory;
using Microsoft.WindowsAzure.Storage;

namespace ClearLanding
{
    class Program
    {
        static void Main(string[] args)
        {
            CloudStorageAccount backupStorageAccount = CloudStorageAccount.Parse(""DefaultEndpointsProtocol=https;AccountName=yyy;AccountKey=xxx;EndpointSuffix=core.windows.net"");
            var backupBlobClient = backupStorageAccount.CreateCloudBlobClient();
            var backupContainer = backupBlobClient.GetContainerReference(""landing"");
            var tgtBlobClient = backupStorageAccount.CreateCloudBlobClient();
            var tgtContainer = tgtBlobClient.GetContainerReference(""backup"");
            string[] folderNames = args[0].Split(new char[] { ',', ' ' }, StringSplitOptions.RemoveEmptyEntries);
            foreach (string folderName in folderNames)
            {
                var list = backupContainer.ListBlobs(prefix: folderName + ""/"", useFlatBlobListing: false);
                foreach (Microsoft.WindowsAzure.Storage.Blob.IListBlobItem item in list)
                {
                    if (item.GetType() == typeof(Microsoft.WindowsAzure.Storage.Blob.CloudBlockBlob))
                    {
                        Microsoft.WindowsAzure.Storage.Blob.CloudBlockBlob blob = (Microsoft.WindowsAzure.Storage.Blob.CloudBlockBlob)item;
                        if (!blob.Name.ToUpper().Contains(""DO_NOT_DEL""))
                        {
                            var tgtBlob = tgtContainer.GetBlockBlobReference(blob.Name + ""_"" + DateTime.Now.ToString(""yyyyMMddHHmmss""));
                            tgtBlob.StartCopy(blob);
                            blob.Delete();

                        }
                    }
                }
            }
        }
    }
}
</code></pre>

<p>I tried resolving this by compiling the above code and referencing it using custom activity in C# pipeline. The code snippet above transfers files from landing folder to a backup folder and deletes the file from landing</p>
"
"51725097","How to connect a MySQL database hosted on cloud to Azure Data Factory?","<p>If I have a database that is hosted on cloud, hence publicly accessible, is it possibible to connect to it via Azure Integration-Runtime, or do I have to set up a self-hosted IR?</p>

<p>In the official documentation it's not clear, one <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats"" rel=""nofollow noreferrer"">article</a> indicating Azure IR not supported for MySQL, while digging through the detailed <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-mysql#linked-service-properties"" rel=""nofollow noreferrer"">connector's article</a>, it says the opposite.</p>
","<azure><azure-data-factory><azure-database-mysql>","2018-08-07 10:56:35","1128","2","2","51726587","<p>If your server  is publicly accessible, then you don’t need install a selfhosted IR. You could use ADF v2 UI to have a quick try. It provides the test connection function.</p>

<p>Seems the first article is out of date. Previously, it needs a MySQL driver to connect MySQL. But currently, ADF has a default driver now. </p>
"
"51725097","How to connect a MySQL database hosted on cloud to Azure Data Factory?","<p>If I have a database that is hosted on cloud, hence publicly accessible, is it possibible to connect to it via Azure Integration-Runtime, or do I have to set up a self-hosted IR?</p>

<p>In the official documentation it's not clear, one <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats"" rel=""nofollow noreferrer"">article</a> indicating Azure IR not supported for MySQL, while digging through the detailed <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-mysql#linked-service-properties"" rel=""nofollow noreferrer"">connector's article</a>, it says the opposite.</p>
","<azure><azure-data-factory><azure-database-mysql>","2018-08-07 10:56:35","1128","2","2","52854166","<p>Yes ,you need to install <strong>Self-hosted IR</strong> in order to access your Cloud hosted MySQL database.
As of today what I've tried, ""Azure Integration-Runtime"" does have that capability to connect cloud hosted MYSQL database.</p>
"
"51723710","c# Copy SQL table data to another DB with Where clause filter using Data Factory","<p>I am in a process to copy data from one SQL database (Source) and move to another SQL Database (destination) through data factory using c# code.
I am able to copy all the data from a source table to destination table, but i want to move filtered data only, like SELECT * FROM Source.tbl WHERE Category = 5. There would be around 10-15 table that i would move data. Can you provide me sample code which may help me?</p>

<p>My code for moving single table all data..</p>

<p>// Authenticate and create a data factory management client
            var context = new AuthenticationContext(""<a href=""https://login.windows.net/"" rel=""nofollow noreferrer"">https://login.windows.net/</a>"" + tenantID);
            ClientCredential cc = new ClientCredential(AppID, AuthKey);
            AuthenticationResult result = context.AcquireTokenAsync(""<a href=""https://management.azure.com/"" rel=""nofollow noreferrer"">https://management.azure.com/</a>"", cc).Result;
            ServiceClientCredentials cred = new TokenCredentials(result.AccessToken);
            var client = new DataFactoryManagementClient(cred) { SubscriptionId = SubscriptionID };</p>

<pre><code>        // Create data factory
        Factory dataFactory = new Factory { Location = Region, Identity = new FactoryIdentity() };

        // This line throws error, we cannot proceed further. unless we get access of creating DF or update or access.
        client.Factories.CreateOrUpdate(ResourceGroup, DataFactoryName, dataFactory);

        var DF = client.Factories.Get(ResourceGroup, DataFactoryName);

        while (DF.ProvisioningState == ""PendingCreation"")
        {
            System.Threading.Thread.Sleep(1000);
        }

        LinkedServiceResource storageLinkedService = new LinkedServiceResource(
            new AzureSqlDatabaseLinkedService
            {
                ConnectionString = new SecureString(SourceSQLConnString)
            }
        );
        client.LinkedServices.CreateOrUpdate(ResourceGroup, DataFactoryName, SourceSQLLinkedServiceName, storageLinkedService);

        LinkedServiceResource sqlDbLinkedService = new LinkedServiceResource(
            new AzureSqlDatabaseLinkedService
            {
                ConnectionString = new SecureString(DestSQLConnString)
            }
        );
        client.LinkedServices.CreateOrUpdate(ResourceGroup, DataFactoryName, DestSQLLinkedServiceName, sqlDbLinkedService);

        DatasetResource SourceSQLDataSet = new DatasetResource(
            new AzureSqlTableDataset
            {
                LinkedServiceName = new LinkedServiceReference
                {
                    ReferenceName = SourceSQLLinkedServiceName
                },
                TableName = Table, 
            }
        );
        client.Datasets.CreateOrUpdate(ResourceGroup, DataFactoryName, SourceSQLDataSetName, SourceSQLDataSet);

        // Create a Azure SQL Database dataset
        DatasetResource DestSQLDataSet = new DatasetResource(
            new AzureSqlTableDataset
            {
                LinkedServiceName = new LinkedServiceReference
                {
                    ReferenceName = DestSQLLinkedServiceName
                },
                TableName = Table
            }
        );
        client.Datasets.CreateOrUpdate(ResourceGroup, DataFactoryName, DestSQLDataSetName, DestSQLDataSet);

        PipelineResource pipeline = new PipelineResource
        {
            Activities = new List&lt;Activity&gt;
            {
                new CopyActivity
                {
                    Name = ""CopyFromSQLToSQL"",
                    Inputs = new List&lt;DatasetReference&gt;
                    {
                        new DatasetReference()
                        {
                            ReferenceName = SourceSQLDataSetName
                        }
                    },
                    Outputs = new List&lt;DatasetReference&gt;
                    {
                        new DatasetReference
                        {
                            ReferenceName = DestSQLDataSetName
                        }
                    },
                    Source = new SqlSource(),
                    Sink = new SqlSink { }
                }
            }
        };
        client.Pipelines.CreateOrUpdate(ResourceGroup, DataFactoryName, PipelineName, pipeline);

        // Create a pipeline run
        CreateRunResponse runResponse = client.Pipelines.CreateRunWithHttpMessagesAsync(ResourceGroup, DataFactoryName, PipelineName).Result.Body;

        // Monitor the pipeline run
        PipelineRun pipelineRun;
        while (true)
        {
            pipelineRun = client.PipelineRuns.Get(ResourceGroup, DataFactoryName, runResponse.RunId);
            if (pipelineRun.Status == ""InProgress"")
                System.Threading.Thread.Sleep(15000);
            else
                break;
        }
</code></pre>
","<azure-data-factory>","2018-08-07 09:46:22","157","0","2","51726420","<p>You could put your query into the SqlReaderQuery property of your sql Source. </p>
"
"51723710","c# Copy SQL table data to another DB with Where clause filter using Data Factory","<p>I am in a process to copy data from one SQL database (Source) and move to another SQL Database (destination) through data factory using c# code.
I am able to copy all the data from a source table to destination table, but i want to move filtered data only, like SELECT * FROM Source.tbl WHERE Category = 5. There would be around 10-15 table that i would move data. Can you provide me sample code which may help me?</p>

<p>My code for moving single table all data..</p>

<p>// Authenticate and create a data factory management client
            var context = new AuthenticationContext(""<a href=""https://login.windows.net/"" rel=""nofollow noreferrer"">https://login.windows.net/</a>"" + tenantID);
            ClientCredential cc = new ClientCredential(AppID, AuthKey);
            AuthenticationResult result = context.AcquireTokenAsync(""<a href=""https://management.azure.com/"" rel=""nofollow noreferrer"">https://management.azure.com/</a>"", cc).Result;
            ServiceClientCredentials cred = new TokenCredentials(result.AccessToken);
            var client = new DataFactoryManagementClient(cred) { SubscriptionId = SubscriptionID };</p>

<pre><code>        // Create data factory
        Factory dataFactory = new Factory { Location = Region, Identity = new FactoryIdentity() };

        // This line throws error, we cannot proceed further. unless we get access of creating DF or update or access.
        client.Factories.CreateOrUpdate(ResourceGroup, DataFactoryName, dataFactory);

        var DF = client.Factories.Get(ResourceGroup, DataFactoryName);

        while (DF.ProvisioningState == ""PendingCreation"")
        {
            System.Threading.Thread.Sleep(1000);
        }

        LinkedServiceResource storageLinkedService = new LinkedServiceResource(
            new AzureSqlDatabaseLinkedService
            {
                ConnectionString = new SecureString(SourceSQLConnString)
            }
        );
        client.LinkedServices.CreateOrUpdate(ResourceGroup, DataFactoryName, SourceSQLLinkedServiceName, storageLinkedService);

        LinkedServiceResource sqlDbLinkedService = new LinkedServiceResource(
            new AzureSqlDatabaseLinkedService
            {
                ConnectionString = new SecureString(DestSQLConnString)
            }
        );
        client.LinkedServices.CreateOrUpdate(ResourceGroup, DataFactoryName, DestSQLLinkedServiceName, sqlDbLinkedService);

        DatasetResource SourceSQLDataSet = new DatasetResource(
            new AzureSqlTableDataset
            {
                LinkedServiceName = new LinkedServiceReference
                {
                    ReferenceName = SourceSQLLinkedServiceName
                },
                TableName = Table, 
            }
        );
        client.Datasets.CreateOrUpdate(ResourceGroup, DataFactoryName, SourceSQLDataSetName, SourceSQLDataSet);

        // Create a Azure SQL Database dataset
        DatasetResource DestSQLDataSet = new DatasetResource(
            new AzureSqlTableDataset
            {
                LinkedServiceName = new LinkedServiceReference
                {
                    ReferenceName = DestSQLLinkedServiceName
                },
                TableName = Table
            }
        );
        client.Datasets.CreateOrUpdate(ResourceGroup, DataFactoryName, DestSQLDataSetName, DestSQLDataSet);

        PipelineResource pipeline = new PipelineResource
        {
            Activities = new List&lt;Activity&gt;
            {
                new CopyActivity
                {
                    Name = ""CopyFromSQLToSQL"",
                    Inputs = new List&lt;DatasetReference&gt;
                    {
                        new DatasetReference()
                        {
                            ReferenceName = SourceSQLDataSetName
                        }
                    },
                    Outputs = new List&lt;DatasetReference&gt;
                    {
                        new DatasetReference
                        {
                            ReferenceName = DestSQLDataSetName
                        }
                    },
                    Source = new SqlSource(),
                    Sink = new SqlSink { }
                }
            }
        };
        client.Pipelines.CreateOrUpdate(ResourceGroup, DataFactoryName, PipelineName, pipeline);

        // Create a pipeline run
        CreateRunResponse runResponse = client.Pipelines.CreateRunWithHttpMessagesAsync(ResourceGroup, DataFactoryName, PipelineName).Result.Body;

        // Monitor the pipeline run
        PipelineRun pipelineRun;
        while (true)
        {
            pipelineRun = client.PipelineRuns.Get(ResourceGroup, DataFactoryName, runResponse.RunId);
            if (pipelineRun.Status == ""InProgress"")
                System.Threading.Thread.Sleep(15000);
            else
                break;
        }
</code></pre>
","<azure-data-factory>","2018-08-07 09:46:22","157","0","2","51799155","<p>I talked to the Data Factory support, they said we have not implemented yet, 
Create Data Factory, 
Create linked services
In loop create datasets and Create copy activity</p>
"
"51711240","How to upsert/insert records in all tables in an Azure SQL Database with Azure Data Factory v2","<p>I have an Azure SQL database with many tables that I want to update frequently with any change made, be it an update or an insert, using Azure Data Factory v2.
There is a section in the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-multiple-tables-portal"" rel=""nofollow noreferrer"">documentation</a> that explains how to do this.
However, the example is about two tables, and for each table a TYPE needs to be defined, and for each table a Stored Procedure is built.
I don't know how to generalize this for a large number of tables.
Any suggestion would be welcome.</p>
","<azure><insert-update><multiple-tables><azure-data-factory>","2018-08-06 15:40:10","3492","5","1","69915025","<p>You can follow my answer <a href=""https://stackoverflow.com/a/69896947/7392069"">https://stackoverflow.com/a/69896947/7392069</a> but I don't know how to generalise creation of table types and stored procedures, but at least the metadata table of the metadata driven copy task provides a lot of comfort to achieve what you need.</p>
"
"51710623","Azure Data Factory Pipeline Logs","<p>Where does data logs of Azure Pipeline v2 gets stored, I would like to retrieve data of failed pipelines for specific date.( Dont want to use azure portal to view these data). Is there any table/view holds such datalogs  from database. </p>
","<azure><azure-sql-database><azure-data-factory>","2018-08-06 15:04:16","4545","4","4","51714501","<p>To my knowledge, to obtain diagnostic logs you can use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor"" rel=""nofollow noreferrer"">Azure Monitor</a>, <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-factory-monitor-oms"" rel=""nofollow noreferrer"">Operations Management Suite (OMS)</a>, or monitor those pipelines <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-visually"" rel=""nofollow noreferrer"">visually</a>.</p>
"
"51710623","Azure Data Factory Pipeline Logs","<p>Where does data logs of Azure Pipeline v2 gets stored, I would like to retrieve data of failed pipelines for specific date.( Dont want to use azure portal to view these data). Is there any table/view holds such datalogs  from database. </p>
","<azure><azure-sql-database><azure-data-factory>","2018-08-06 15:04:16","4545","4","4","51716125","<p>By Azure Pipeline v2, you mean Azure Data Factory v2. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor"" rel=""nofollow noreferrer"">Alert and Monitor data factories using Azure Monitor</a></p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor#diagnostic-logs"" rel=""nofollow noreferrer"">Diagnostic logs:</a></p>

<ul>
<li>Save them to a Storage Account for auditing or manual inspection. You can specify the retention time (in days) using the diagnostic settings.</li>
<li>Stream them to Event Hubs for ingestion by a third-party service or custom analytics solution such as PowerBI.</li>
<li>Analyze them with Log Analytics</li>
</ul>
"
"51710623","Azure Data Factory Pipeline Logs","<p>Where does data logs of Azure Pipeline v2 gets stored, I would like to retrieve data of failed pipelines for specific date.( Dont want to use azure portal to view these data). Is there any table/view holds such datalogs  from database. </p>
","<azure><azure-sql-database><azure-data-factory>","2018-08-06 15:04:16","4545","4","4","57388198","<p>The logs are stored on Azure Data Factory web server for 45 days. If you want to get the pipeline run and activity run metadata, you can use Azure Data Factory SDK to extract the information you need and save it somewhere you want.</p>
"
"51710623","Azure Data Factory Pipeline Logs","<p>Where does data logs of Azure Pipeline v2 gets stored, I would like to retrieve data of failed pipelines for specific date.( Dont want to use azure portal to view these data). Is there any table/view holds such datalogs  from database. </p>
","<azure><azure-sql-database><azure-data-factory>","2018-08-06 15:04:16","4545","4","4","64190216","<p>Recommended approach on this for log term analysis as well as limiting access to a production data factory would be to configure logs to be sent to log analytics. Be sure to enable dedicated logging tables as this will help on the backend in terms of organizing your logs.</p>
<p>From there you can also set up alerts and access groups running off of log analytics queries for better monitoring.</p>
"
"51703432","How to connect Azure Managed Instance in Azure Linked service","<p>I am trying to make the ""Managed Instance"" as destination in Azure Data Factory(copy data). But, I couldn't see any option to select ""Managed Instance"" in Linked service. </p>

<p><a href=""https://i.stack.imgur.com/hWrb6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hWrb6.jpg"" alt=""enter image description here""></a></p>

<p>Here is the list of Azure supported data stores, </p>

<p><a href=""https://i.stack.imgur.com/ad599.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ad599.jpg"" alt=""enter image description here""></a></p>

<p>Source : <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-datasets-linked-services#dataset-type"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-datasets-linked-services#dataset-type</a></p>

<p>I tried selecting ""Azure SQL Database"" and entered the managed instance credentials, obviously it didn't work. Is there a workaround for this? </p>

<p>Is there a way to make managed instance as my data store ? </p>
","<azure><azure-sql-database><azure-data-factory><azure-managed-database>","2018-08-06 08:26:12","1125","0","3","51703496","<p>I hope you mean SQL Server hosted in VM as Managed Instance. Yes. It is supported for copy Activity. </p>

<p>In the link you provided : <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server</a> , I am giving the snapshot of SQL Server. </p>

<p><a href=""https://i.stack.imgur.com/70zt1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/70zt1.png"" alt=""Sql Server supported in the Database Section""></a>
Please refer to the link: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server</a> </p>
"
"51703432","How to connect Azure Managed Instance in Azure Linked service","<p>I am trying to make the ""Managed Instance"" as destination in Azure Data Factory(copy data). But, I couldn't see any option to select ""Managed Instance"" in Linked service. </p>

<p><a href=""https://i.stack.imgur.com/hWrb6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hWrb6.jpg"" alt=""enter image description here""></a></p>

<p>Here is the list of Azure supported data stores, </p>

<p><a href=""https://i.stack.imgur.com/ad599.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ad599.jpg"" alt=""enter image description here""></a></p>

<p>Source : <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-datasets-linked-services#dataset-type"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-datasets-linked-services#dataset-type</a></p>

<p>I tried selecting ""Azure SQL Database"" and entered the managed instance credentials, obviously it didn't work. Is there a workaround for this? </p>

<p>Is there a way to make managed instance as my data store ? </p>
","<azure><azure-sql-database><azure-data-factory><azure-managed-database>","2018-08-06 08:26:12","1125","0","3","51710582","<p>For connecting to Azure SQL Managed Instance, you can choose “SQL Server” as the Linked Service type and associate a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">self-hosted Integration Runtime</a> installed on a VM inside the VNet. Microsoft is planning to add Azure Managed Instance as an option in the near future.</p>
"
"51703432","How to connect Azure Managed Instance in Azure Linked service","<p>I am trying to make the ""Managed Instance"" as destination in Azure Data Factory(copy data). But, I couldn't see any option to select ""Managed Instance"" in Linked service. </p>

<p><a href=""https://i.stack.imgur.com/hWrb6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hWrb6.jpg"" alt=""enter image description here""></a></p>

<p>Here is the list of Azure supported data stores, </p>

<p><a href=""https://i.stack.imgur.com/ad599.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ad599.jpg"" alt=""enter image description here""></a></p>

<p>Source : <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-datasets-linked-services#dataset-type"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-datasets-linked-services#dataset-type</a></p>

<p>I tried selecting ""Azure SQL Database"" and entered the managed instance credentials, obviously it didn't work. Is there a workaround for this? </p>

<p>Is there a way to make managed instance as my data store ? </p>
","<azure><azure-sql-database><azure-data-factory><azure-managed-database>","2018-08-06 08:26:12","1125","0","3","51716167","<p>Here is a good blog post that discusses how they work together:</p>

<p><a href=""https://medium.com/@mauridb/azure-sql-managed-instances-and-azure-data-factory-a-walk-through-bfb93e79ac0c"" rel=""nofollow noreferrer"">Azure SQL Managed Instances and Azure Data Factory: a walk-through</a></p>
"
"51702031","Field name can not contain colon in mapping using Azure Data Factory to convert AWS S3 .csv to CosmosDb","<p>I am trying to use Azure Data Factory to convert CSV data from an Amazon S3 bucket into JSON Document (collection) in CosmosDB. I get the error in the title - Field name can not contain colon in mapping - despite me having no colon that I can find in any of my column names, source or destination. Please help? </p>

<p><a href=""https://i.stack.imgur.com/RGCoE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RGCoE.png"" alt=""enter image description here""></a></p>
","<azure><mapping><azure-data-factory>","2018-08-06 06:52:05","182","0","2","51704342","<p>This error occurs when any name in column mapping contains colon. </p>

<p>Could you click the 'Code' button on the top right corner of the pipeline edit canvas, and check proproties -> activities -> typeProperties -> translator -> columnMappings, if there is any colon in key or value under it?</p>
"
"51702031","Field name can not contain colon in mapping using Azure Data Factory to convert AWS S3 .csv to CosmosDb","<p>I am trying to use Azure Data Factory to convert CSV data from an Amazon S3 bucket into JSON Document (collection) in CosmosDB. I get the error in the title - Field name can not contain colon in mapping - despite me having no colon that I can find in any of my column names, source or destination. Please help? </p>

<p><a href=""https://i.stack.imgur.com/RGCoE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RGCoE.png"" alt=""enter image description here""></a></p>
","<azure><mapping><azure-data-factory>","2018-08-06 06:52:05","182","0","2","51720560","<p>I find a colon in 'PostCode:' of the sink column. BTW, the validation for column mapping names will be removed in the next few weeks. For now you need to rename it to bypass the validation.</p>
"
"51671622","How can I convert a UTF-8 encoded file with BOM to a UTF-8 encoded file without BOM in Azure Data Factory V1/V2?","<p>I currently have a V2 Data Factory which copies UTF-8 encoded files with BOM to a storage location and I would like to remove the BOM as part of the standard copy activity.</p>

<p>Is there a way to do this ?</p>
","<azure><utf-8><azure-data-factory>","2018-08-03 11:28:05","1779","4","2","51722241","<p>Per my experience, there is no such middleware processing mechanism for you to remove <code>bom header</code> in <code>Azure Data Factory Copy activity</code>.</p>

<p>However,  I provide you with a workaround that you could use <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-triggered-function"" rel=""nofollow noreferrer"">Blob Trigger Azure Function</a> to execute business logical code when your file moved into blob storage.</p>

<p><strong><em>remove bom sample code:</em></strong></p>

<pre><code>public static string RemoveBom(String desc, Encoding encode)
{
    string bomString = encode.GetString(encode.GetPreamble());
    if (!string.IsNullOrEmpty(bomString) &amp;&amp; desc.StartsWith(bomString))
    {
        desc = desc.Remove(0, bomString.Length);
    }
    return desc;
}
</code></pre>

<p>Hope it helps you.</p>
"
"51671622","How can I convert a UTF-8 encoded file with BOM to a UTF-8 encoded file without BOM in Azure Data Factory V1/V2?","<p>I currently have a V2 Data Factory which copies UTF-8 encoded files with BOM to a storage location and I would like to remove the BOM as part of the standard copy activity.</p>

<p>Is there a way to do this ?</p>
","<azure><utf-8><azure-data-factory>","2018-08-03 11:28:05","1779","4","2","51771025","<p>It turns out this is actually very simple to do. In the output dataset, if we do not specify an encoding Data Factory will then use UTF-8 by default without adding the BOM.
The following datasets illustrate this principal:</p>

<p>Here is my input dataset:</p>

<p><code>
{
    ""name"": ""InputBlobs"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureStorageLinkedService"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""AzureBlob"",
        ""typeProperties"": {
            ""format"": {
                ""type"": ""TextFormat"",
                ""columnDelimiter"": ""|"",
                ""rowDelimiter"": """",
                ""nullValue"": ""\\N"",
                ""encodingName"": ""UTF-16"",
                ""treatEmptyAsNull"": true,
                ""skipLineCount"": 0,
                ""firstRowAsHeader"": false
            },
            ""fileName"": """",
            ""folderPath"": ""folder_path""
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></p>

<p>Here is my output dataset :</p>

<p><code>
{
    ""name"": ""OutputBlobs"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureStorageLinkedService"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""AzureBlob"",
        ""typeProperties"": {
            ""format"": {
                ""type"": ""TextFormat"",
                ""columnDelimiter"": ""|"",
                ""rowDelimiter"": """",
                ""nullValue"": ""\\N"",
                ""treatEmptyAsNull"": true,
                ""skipLineCount"": 0,
                ""firstRowAsHeader"": false
            },
            ""fileName"": """",
            ""folderPath"": ""another_folder_path""
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></p>
"
"51671479","how to move files located in on-premises windows based file share using scala?","<p>Background : I am using Azure ADFV2 to move data from fileshare to ADLS, after the file is moved successfully I want to archive the file within fileshare location.</p>

<p>How do I connect to on-premises windows based file share and move the files from one folder to another within the fileshare using scala. I am not sure how to establish the connectivity to a file share. </p>
","<scala><azure-data-factory><fileshare>","2018-08-03 11:20:14","700","0","1","51671836","<p>You can use file system linked service to establish the connectivity to a file share.</p>

<ol>
<li><p>create a self-hosted integration runtime on ADF and install it on your machine.</p></li>
<li><p>create a file system linked service, and for ""Connect via integration runtime"" field, choose the self-hosted ir you created in 1.</p></li>
<li><p>configure you linked service and dataset as the doc instructs: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system</a></p></li>
</ol>
"
"51669177","Is it possible to add client authentication certificate (.pfx) to Web Activities in Data Factory using powershell","<p>I want to add client authentication certificate (.pfx) to Web Activities in Data Factory. Certificate needs to be fetched from Azure key vault.</p>

<p>Right now this is done manually.</p>

<p>I want to automate the same , is it possible to do using powershell.</p>
","<powershell><azure><azure-data-factory><azure-keyvault>","2018-08-03 09:11:45","682","2","1","52163454","<blockquote>
  <p>I want to automate the same , is it possible to do using powershell.</p>
</blockquote>

<p>Yes, we could use <a href=""https://learn.microsoft.com/en-us/powershell/module/azurerm.datafactoryv2/set-azurermdatafactoryv2pipeline?view=azurermps-6.8.1"" rel=""nofollow noreferrer"">Set-AzureRmDataFactoryV2Pipeline</a> to add the client authentication certificate.</p>

<p>The following is the detail steps, you could refer to</p>

<p>1.Create a web-activity definition json , for more information please refer to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">control-flow-web-activity</a></p>

<p>2.Get the pfx file from keyvault, the pfx file get from keyvault is that without password. So we need to add the password to the pfx </p>

<pre><code>$kvSecret = Get-AzureKeyVaultSecret -VaultName $vaultName -Name $secretName
$kvSecretBytes = [System.Convert]::FromBase64String($kvSecret.SecretValueText)
$certCollection = New-Object System.Security.Cryptography.X509Certificates.X509Certificate2Collection
$certCollection.Import($kvSecretBytes,$null,[System.Security.Cryptography.X509Certificates.X509KeyStorageFlags]::Exportable)
$password = '******'
$protectedCertificateBytes = $certCollection.Export([System.Security.Cryptography.X509Certificates.X509ContentType]::Pkcs12, $password)
$pfxPath = [Environment]::GetFolderPath(""Desktop"") + ""\MyCert.pfx""
[System.IO.File]::WriteAllBytes($pfxPath, $protectedCertificateBytes)
</code></pre>

<p>3.Update the json file with pfx value and password</p>

<p>4.Run the set-AzureRmDataFactory command</p>

<pre><code>Set-AzureRmDataFactoryV2Pipeline -ResourceGroupName ""resourceGroup"" -Name ""name"" -DataFactoryName ""factoryName"" -File ""C:\webactivity.json""
</code></pre>
"
"51656521","Copy Activity is failing with the following error","<blockquote>
  <p>{
      ""errorCode"": ""2200"",
      ""message"": ""ErrorCode=FailedDbOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A
  database operation failed with the following error: 
  'PdwManagedToNativeInteropException ErrorNumber: 46724, MajorCode:
  467, MinorCode: 24, Severity: 20, State: 2, Exception of type
  'Microsoft.SqlServer.
  DataWarehouse.Tds.PdwManagedToNativeInteropException' was
  thrown.',Source=,''Type=System.Data.SqlClient.SqlException,Message=PdwManagedToNativeInteropException
  ErrorNumber: 46724,  MajorCode: 467, MinorCode: 24, Severity: 20,
  State: 2, Exception of type
  'Microsoft.SqlServer.DataWarehouse.Tds.PdwManagedToNativeInteropException'
  was thrown., Source=.Net SqlClient Data
  Provider,SqlErrorNumber=100000,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=100000,State=1,Message=PdwManagedToNativeInteropException
  ErrorNumber: 46724, MajorCode: 467, MinorCode: 24, Severity: 20,
  State: 2, Exception of type
  'Microsoft.SqlServer.DataWarehouse.Tds.PdwManagedToNativeInteropException'
  was thrown.,},],'"",
      ""failureType"": ""UserError"",
      ""target"": ""hana_ODS_DEV"" }</p>
</blockquote>

<p>Can anyone please let me know the main cause for this error? I am trying to migrate the data from SAP HANA onto Azure SQL Dataware house</p>
","<azure><azure-sql-database><hana><azure-data-factory>","2018-08-02 14:49:56","13661","5","3","51660465","<p>Well, I found an answer for this issues. Sometimes it might be the issue with yor source data types and column data types. If you are migrating data from any source to Azure SQL without any BLOB in between and facing this issues, and want to know the exact issue where it is lying, click on allow poly base in your sink, enable the staging in settings and give a path for blob staging storage, debug it.... It shows you the exact cause of where it is causing you the error. </p>
"
"51656521","Copy Activity is failing with the following error","<blockquote>
  <p>{
      ""errorCode"": ""2200"",
      ""message"": ""ErrorCode=FailedDbOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A
  database operation failed with the following error: 
  'PdwManagedToNativeInteropException ErrorNumber: 46724, MajorCode:
  467, MinorCode: 24, Severity: 20, State: 2, Exception of type
  'Microsoft.SqlServer.
  DataWarehouse.Tds.PdwManagedToNativeInteropException' was
  thrown.',Source=,''Type=System.Data.SqlClient.SqlException,Message=PdwManagedToNativeInteropException
  ErrorNumber: 46724,  MajorCode: 467, MinorCode: 24, Severity: 20,
  State: 2, Exception of type
  'Microsoft.SqlServer.DataWarehouse.Tds.PdwManagedToNativeInteropException'
  was thrown., Source=.Net SqlClient Data
  Provider,SqlErrorNumber=100000,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=100000,State=1,Message=PdwManagedToNativeInteropException
  ErrorNumber: 46724, MajorCode: 467, MinorCode: 24, Severity: 20,
  State: 2, Exception of type
  'Microsoft.SqlServer.DataWarehouse.Tds.PdwManagedToNativeInteropException'
  was thrown.,},],'"",
      ""failureType"": ""UserError"",
      ""target"": ""hana_ODS_DEV"" }</p>
</blockquote>

<p>Can anyone please let me know the main cause for this error? I am trying to migrate the data from SAP HANA onto Azure SQL Dataware house</p>
","<azure><azure-sql-database><hana><azure-data-factory>","2018-08-02 14:49:56","13661","5","3","54461411","<p>This error occurs when one or more columns of destination have defined size lesser than the data which you are trying to copy.
Just increase the size of the columns and your ADF will run smoothly.
Sharing the snippet query to increase the size of the column in ADW,</p>

<p>alter table <em>table_name</em>
alter column <em>columne_name</em> datatype NULL</p>

<p>Note -</p>

<ol>
<li>You cannot decrease the size of columns in ADW. You can just increase the size.</li>
<li>The error message does not give details about the column or row because of which copy activity fails. You need to figure that out manually.</li>
</ol>

<p>Let me know if this works.</p>
"
"51656521","Copy Activity is failing with the following error","<blockquote>
  <p>{
      ""errorCode"": ""2200"",
      ""message"": ""ErrorCode=FailedDbOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=A
  database operation failed with the following error: 
  'PdwManagedToNativeInteropException ErrorNumber: 46724, MajorCode:
  467, MinorCode: 24, Severity: 20, State: 2, Exception of type
  'Microsoft.SqlServer.
  DataWarehouse.Tds.PdwManagedToNativeInteropException' was
  thrown.',Source=,''Type=System.Data.SqlClient.SqlException,Message=PdwManagedToNativeInteropException
  ErrorNumber: 46724,  MajorCode: 467, MinorCode: 24, Severity: 20,
  State: 2, Exception of type
  'Microsoft.SqlServer.DataWarehouse.Tds.PdwManagedToNativeInteropException'
  was thrown., Source=.Net SqlClient Data
  Provider,SqlErrorNumber=100000,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=100000,State=1,Message=PdwManagedToNativeInteropException
  ErrorNumber: 46724, MajorCode: 467, MinorCode: 24, Severity: 20,
  State: 2, Exception of type
  'Microsoft.SqlServer.DataWarehouse.Tds.PdwManagedToNativeInteropException'
  was thrown.,},],'"",
      ""failureType"": ""UserError"",
      ""target"": ""hana_ODS_DEV"" }</p>
</blockquote>

<p>Can anyone please let me know the main cause for this error? I am trying to migrate the data from SAP HANA onto Azure SQL Dataware house</p>
","<azure><azure-sql-database><hana><azure-data-factory>","2018-08-02 14:49:56","13661","5","3","61251629","<p>I got the same error and both the above points mentioned are useful to find a solution for this. </p>

<p>This is what we did</p>

<p><strong>We got this error while copying data from blob storage to azure data warehouse table.</strong> 
We had already set up the logging and fault tolerance to ""skip and log incompatible rows""and even after that we were getting the error and the activity terminated as a failure which is indeed because the size of the source column values was greater than the target column size. 
We then set up a staging area inside the blob storage, somehow the load skipped the incompatible rows and processed the data. </p>

<p>Not sure if it was because of logging (logically should have been) or the staging property or combination of both.</p>

<p>Incompatible rows and the file/s responsible for it were stored inside the Log/polybase/.../rejectedrows folder.</p>
"
"51656408","Azure Data Factory - Execute SSIS Package with Package Parameters","<p>I'm trying to execute a SSIS package with parameters in Azure Data Factory v2 using the ""Execute SSIS Package"" activity. </p>

<p>However I get the following error:</p>

<pre><code>Activity Execute SSIS Package failed: Failed get parameter info of parameter JobID.
</code></pre>

<p>Pipeline set up like so:</p>

<pre><code>{
""name"": ""Master Load"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""Execute SSIS Package"",
            ""type"": ""ExecuteSSISPackage"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false
            },
            ""typeProperties"": {
                ""packageLocation"": {
                    ""packagePath"": ""blah/blah/blah.dtsx""
                },
                ""loggingLevel"": ""Basic"",
                ""environmentPath"": ""blah/blah"",
                ""connectVia"": {
                    ""referenceName"": ""blah-SSISIR"",
                    ""type"": ""IntegrationRuntimeReference""
                },
                ""packageParameters"": {
                    ""JobID"": {
                        ""value"": ""@pipeline().parameters.JobID""
                    }
                }
            }
        }
    ],
    ""parameters"": {
        ""JobID"": {
            ""type"": ""Int"",
            ""defaultValue"": -1
        }
    },
    ""folder"": {
        ""name"": ""Master Loads""
    }
},
""type"": ""Microsoft.DataFactory/factories/pipelines""}
</code></pre>

<p>JobID is a Project parameter in the SSIS project I have. SSIS 2016 and we deploy the package to ADFv2/Azure/whatever you want to call it.</p>

<p>The package runs successfully without the packageParameters section and just uses the default values in the package but we want to set these at runtime.</p>

<p>Anyone come across this or have any examples to share?</p>

<p>Thanks</p>
","<parameters><ssis><azure-data-factory>","2018-08-02 14:43:46","791","3","1","54240904","<p>You state that JobID is a project parameter, then you should use <code>projectParameters</code> rather than <code>packageParameters</code> in your json</p>
"
"51650449","Secure ADF v2 Pipeline Parameter String for WebActivity","<p>I have ADF v2 Pipeline with a WebActivity which has a REST Post Call to get Jwt Access token from AD token api (<a href=""https://login.microsoftonline.com/myorg.onmicrosoft.com/oauth2/token"" rel=""noreferrer"">https://login.microsoftonline.com/myorg.onmicrosoft.com/oauth2/token</a>)</p>

<p>I have to pass username and password in the body. Right now, i'm using pipeline parameters to pass these with the request and is working fine. </p>

<pre><code>username=@{pipeline().parameters.username}
&amp;password=@{pipeline().parameters.password}
</code></pre>

<p>But, the parameters tab has plain text which i have to secure. </p>

<p><a href=""https://i.stack.imgur.com/O2j10.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/O2j10.png"" alt=""enter image description here""></a></p>

<p>now, what options do i have to secure the parameter values i'm using in this pipeline instead of plain text. </p>

<p>i have explored this article <a href=""https://learn.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault#reference-secret-stored-in-key-vault"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault#reference-secret-stored-in-key-vault</a>
But, this is to store secrets for data stores. In my web activity i do not have any dataset. it is just a web activity with rest call. </p>

<p>Any help or pointers would be appreciated. Thanks</p>
","<azure-keyvault><azure-data-factory><securestring>","2018-08-02 09:46:42","1342","9","2","55089557","<p>I have implemented little differently,here is my implementation.</p>

<ol>
<li>Store your credential in storage account of your choice.</li>
<li>use lookup activity in data factory.</li>
<li>use lookup activity output for your rest api call.</li>
</ol>

<p>I hope this will help. in your case you can use something like this </p>

<p>create a file generateToken.json
<code>
{
  ""resource"":""xxxxxxxxxxxxxxxx"",
  ""client_id"":""xxxxxxxxxxxxxxx""
  ""grant_type"":""xxxxxxxxxxxxxxxx""
  ""username"":""xxxxxxxxxxxxxxxxxxx""
  ""password"":""xxxxxxxxxxxxxxxxxxxx""
}</code></p>

<p>if you are concern about security of password, decode your password before you add to the generateToken.json and decode at data factory before you make rest api call to generate token using data factory decodeBase64 function. </p>

<p>Viral</p>
"
"51650449","Secure ADF v2 Pipeline Parameter String for WebActivity","<p>I have ADF v2 Pipeline with a WebActivity which has a REST Post Call to get Jwt Access token from AD token api (<a href=""https://login.microsoftonline.com/myorg.onmicrosoft.com/oauth2/token"" rel=""noreferrer"">https://login.microsoftonline.com/myorg.onmicrosoft.com/oauth2/token</a>)</p>

<p>I have to pass username and password in the body. Right now, i'm using pipeline parameters to pass these with the request and is working fine. </p>

<pre><code>username=@{pipeline().parameters.username}
&amp;password=@{pipeline().parameters.password}
</code></pre>

<p>But, the parameters tab has plain text which i have to secure. </p>

<p><a href=""https://i.stack.imgur.com/O2j10.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/O2j10.png"" alt=""enter image description here""></a></p>

<p>now, what options do i have to secure the parameter values i'm using in this pipeline instead of plain text. </p>

<p>i have explored this article <a href=""https://learn.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault#reference-secret-stored-in-key-vault"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault#reference-secret-stored-in-key-vault</a>
But, this is to store secrets for data stores. In my web activity i do not have any dataset. it is just a web activity with rest call. </p>

<p>Any help or pointers would be appreciated. Thanks</p>
","<azure-keyvault><azure-data-factory><securestring>","2018-08-02 09:46:42","1342","9","2","69670275","<h2>Solution 1</h2>
<p>In short, use the unimportant passwords in the current ADF, and configure important passwords in production ADF.</p>
<ol>
<li>store your password and others in ADF, and save them into your git repository. This ADF is for the test.</li>
<li>publish</li>
<li>go to Azure DevOps, and release an ADF from the published template with the password.</li>
</ol>
<h2>Solution 2</h2>
<p>Use Web Activity to get key-vault and send output to other activities.</p>
"
"51625549","Error Handling on Azure Data Factory V2","<p>Team.</p>

<p>I have a situation to handle errors in ADF2 using Web Activity. Within a pipeline, any activity fails, it should trigger web activity. I attached the web activity after each activity's <code>failed</code> status. Therefore the web activity's <code>dependsOn</code> property looks like:</p>

<pre><code>""dependsOn"": {
  ""activity1"": [
    ""Failed""
  ],
  ""activity2"": [
    ""Failed""
  ]
}
</code></pre>

<p>However, the web activity only gets fired when all <code>activity1</code> and <code>activity2</code> are failed. What I wanted was to get that fired either <code>activity1</code> or <code>activity2</code> is failed. Instead, I used the <code>IfCondition</code> activity but it doesn't seem to be the right (correct or proper) way. What would be the best practice?</p>

<p>I tried to get all activities from filtering <code>pipeline().activities</code>, but it's not supported, either.</p>
","<azure-data-factory>","2018-08-01 04:52:47","4206","5","1","53000633","<p>You may try <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-pipeline-activity"" rel=""noreferrer"">ExecutePipeline</a> with <code>waitOnCompletion</code>checked. Any inner pipeline failure within <code>ExecutePipeline</code> will make the outside <code>ExecutePipeline</code> fail.</p>
"
"51608625","Azure Data Factory get data for ""For Each""component from query","<p>The situation is as follows: I have a table in my database that recieves about 3 million rows each day. We want to archive this table on a regular base, so that only the 8 most recents weeks are in the table. The rest of the data can be archived tot AZure Data lake.
I allready found out how to do this by one day at a time. But now I want to run this pipeline each week for the first seven days in the table. I assume I should do this with the ""For Each"" component. It should itterate along the seven distinct dates that are present in the dataset I want to backup. This dataset is copied from the source table to an archive table on forehand.
It's not difficult to get the distinct dates with a SQL query, but how to get the result of this query into an array that is used for the ""For Each"" component?</p>
","<foreach><parameters><azure-data-factory>","2018-07-31 08:18:12","7628","1","3","51610616","<p>You can use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">lookup activity</a> to fetch the column content, and the output will be like</p>

<pre><code>{
""count"": ""2"",
""value"": [
    {
        ""Id"": ""1"",
        ""TableName"" : ""Table1""
    },
    {
        ""Id"": ""2"",
        ""TableName"" : ""Table2""
    }
]
}
</code></pre>

<p>Then you can pass the value array to the Foreach activity items field by using the pattern of @activity('MyLookupActivity').output.value</p>

<p>ref doc: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity#use-the-lookup-activity-result-in-a-subsequent-activity"" rel=""nofollow noreferrer"">Use the Lookup activity result in a subsequent activity</a></p>
"
"51608625","Azure Data Factory get data for ""For Each""component from query","<p>The situation is as follows: I have a table in my database that recieves about 3 million rows each day. We want to archive this table on a regular base, so that only the 8 most recents weeks are in the table. The rest of the data can be archived tot AZure Data lake.
I allready found out how to do this by one day at a time. But now I want to run this pipeline each week for the first seven days in the table. I assume I should do this with the ""For Each"" component. It should itterate along the seven distinct dates that are present in the dataset I want to backup. This dataset is copied from the source table to an archive table on forehand.
It's not difficult to get the distinct dates with a SQL query, but how to get the result of this query into an array that is used for the ""For Each"" component?</p>
","<foreach><parameters><azure-data-factory>","2018-07-31 08:18:12","7628","1","3","51653903","<p>I post this as an answer, because the error does not fit into a comment :D</p>

<p>have seen antoher option to accomplish this. That is by executing a pipeline from another pipeline. And in that way I can define the dates that I should iterate over as a parameter in the second pipeline (learn.microsoft.com/en-us/azure/data-factory/…). But unformtunately this leads to the same rsult as when just using the foreach parameter. Because in the filename of my data lake file I have to use: @{item().columname}. I can see in the monitoring view that the right values are passed in the iteration steps, but I keep getting an error:</p>

<p>{
    ""errorCode"": ""2200"",
    ""message"": ""Failure happened on 'Sink' side. ErrorCode=UserErrorFailedFileOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=The request to 'Unknown' failed and the status code is 'BadRequest', request id is ''. {\""error\"":{\""code\"":\""BadRequest\"",\""message\"":\""A potentially dangerous Request.Path value was detected from the client (:). Trace: cf3b4c3f-1681-4073-b225-17e1c07ec76d Time: 2018-08-02T05:16:13.2141897-07:00\""}} ,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Net.WebException,Message=The remote server returned an error: (400) Bad Request.,Source=System,'"",
    ""failureType"": ""UserError"",
    ""target"": ""CopyDancerDatatoADL""
}</p>
"
"51608625","Azure Data Factory get data for ""For Each""component from query","<p>The situation is as follows: I have a table in my database that recieves about 3 million rows each day. We want to archive this table on a regular base, so that only the 8 most recents weeks are in the table. The rest of the data can be archived tot AZure Data lake.
I allready found out how to do this by one day at a time. But now I want to run this pipeline each week for the first seven days in the table. I assume I should do this with the ""For Each"" component. It should itterate along the seven distinct dates that are present in the dataset I want to backup. This dataset is copied from the source table to an archive table on forehand.
It's not difficult to get the distinct dates with a SQL query, but how to get the result of this query into an array that is used for the ""For Each"" component?</p>
","<foreach><parameters><azure-data-factory>","2018-07-31 08:18:12","7628","1","3","51707996","<p>The issue is solved thanks to a co-worker.
What we have to do is assign a parameter to the dataset of the sink. Does not matter how you name this and you do not have to assign a value to it. But let's assume this parameter is called ""date""
After that you can use this parameter in the filename of the sink (also in dataset) with by using ""@dataset().Date"".
After that you go back to the copyactivity and in the sink you assign a dataset property to @item().DateSelect. (DateSelect is the field name from the array that is passed to the For Each activity)</p>

<p>See also the answer from Bo Xioa as part of the answer</p>

<p>This way it works perfectly. It's just a shame that this is not well documented</p>
"
"51602676","Importing Schema Issues","<p>I have my source table in SAP HANA with almost 80 columns, the destination tables on azure DW/Mapping document has 60 columns. (They are mapped to source columns with different names). Now When I am trying to create a source dataset I gave the linked service which was created by my TL and Once I Select that HANA linked to service, It is not showing me any table option under linked service to select and import schemas. Why is that? It is throwing gateway timeout error. 
PS: The Linked service was created by my manager and I don't know any credentials for that</p>
","<azure><azure-active-directory><hana><azure-data-factory>","2018-07-30 21:42:14","1348","-1","2","51604462","<p>SAP Hana only supports query, not table name.</p>

<ol>
<li>Create a pipeline first and drag a copy activity into the pipeline. Reference your dataset in your copy activity. </li>
<li>Then you will see a browser table button. You could construct your query there.
<a href=""https://i.stack.imgur.com/m2D8E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m2D8E.png"" alt=""enter image description here""></a></li>
<li>When you want to edit your json, please use the button instead of the button in the activity node.
<a href=""https://i.stack.imgur.com/1XZGt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1XZGt.png"" alt=""enter image description here""></a></li>
</ol>
"
"51602676","Importing Schema Issues","<p>I have my source table in SAP HANA with almost 80 columns, the destination tables on azure DW/Mapping document has 60 columns. (They are mapped to source columns with different names). Now When I am trying to create a source dataset I gave the linked service which was created by my TL and Once I Select that HANA linked to service, It is not showing me any table option under linked service to select and import schemas. Why is that? It is throwing gateway timeout error. 
PS: The Linked service was created by my manager and I don't know any credentials for that</p>
","<azure><azure-active-directory><hana><azure-data-factory>","2018-07-30 21:42:14","1348","-1","2","51656716","<p>I solved it, the issues were source has column concatenation and destination table didn't have a correct logic in concatenation and few columns which were not present in source, I did the explicit mapping. And made it one-one relation from source to destination in Azure. It works now. Thank you all!</p>
"
"51591123","Oracle Store Procedure in Azure Data Factory V2","<p>I have created a store procedure in Oracle. Can anyone let me know if there is a procedure to execute this oracle Store Procedure from Azure Data Factory V2?</p>
","<azure-data-factory>","2018-07-30 09:42:18","1856","1","1","51594054","<ol>
<li>You could try <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-oracle#oracle-as-a-sink-type"" rel=""nofollow noreferrer"">preCopyScript</a> in copy activity.</li>
<li>You could try <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">custom activity</a>.</li>
</ol>
"
"51589006","Activities not showing in Monitor & Manage","<p>For one of my pipelines in ADF (v1), the activities already executed are not showing before a certain date in Monitor &amp; Manage in the Azure portal. </p>

<p>However, if I go to that same Data Factory in the Azure portal, click the pipeline and find a slice, I can see all slices already executed. From here, I can run the slice again, - and then when executing that activity slice, it will show up in Monitor &amp; Manage. </p>

<p><a href=""https://i.stack.imgur.com/WpIvg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WpIvg.png"" alt=""enter image description here""></a></p>

<p>My problem with this is that I need to re-run hundreds of activities' specific slices. However, these slices are not showing in Monitor &amp; Manage. And only in Monitor &amp; Manage is it possible to re-run multiple slices, by marking those multiple slices. 
Any idea how I get those activities' slices to show up so I can re-run many slices?</p>

<p>Thanks!</p>
","<azure><azure-data-factory>","2018-07-30 07:31:25","506","0","2","51591711","<p><a href=""https://i.stack.imgur.com/gC2B1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gC2B1.png"" alt=""enter image description here"" /></a>
Have you tried changing the start time and end time?</p>
"
"51589006","Activities not showing in Monitor & Manage","<p>For one of my pipelines in ADF (v1), the activities already executed are not showing before a certain date in Monitor &amp; Manage in the Azure portal. </p>

<p>However, if I go to that same Data Factory in the Azure portal, click the pipeline and find a slice, I can see all slices already executed. From here, I can run the slice again, - and then when executing that activity slice, it will show up in Monitor &amp; Manage. </p>

<p><a href=""https://i.stack.imgur.com/WpIvg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WpIvg.png"" alt=""enter image description here""></a></p>

<p>My problem with this is that I need to re-run hundreds of activities' specific slices. However, these slices are not showing in Monitor &amp; Manage. And only in Monitor &amp; Manage is it possible to re-run multiple slices, by marking those multiple slices. 
Any idea how I get those activities' slices to show up so I can re-run many slices?</p>

<p>Thanks!</p>
","<azure><azure-data-factory>","2018-07-30 07:31:25","506","0","2","51693451","<p>By design, it is <strong>not possible</strong> to see or run slices that are more than <strong>45 days old</strong>, from <strong>Monitor &amp; Manage</strong>. </p>

<p>This is the documentation: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor#persist-data-factory-data"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor#persist-data-factory-data</a></p>

<p>Note that you can still use the other part of the Azure portal to see and re-run older slices, but you can only run one slice at a time. </p>

<p>Not sure why these two parts use different API's to show slices, but that's how it works. </p>

<p>:-)</p>
"
"51557316","how to connect existing gateway in Data Factory V2 linked service","<p>We have multiple pipelines in Data Factory V1 for each brand for our organization and We have common gateway named “SQLServerGateway” (Self-Hosted) for on-premises SQL Server for all these pipelines which are running well on a scheduled basis.  </p>

<p>Now, we are trying to create a single test pipeline in Data Factory V2 which is doing the same job in Data Factory V1. Hence we are creating Linked Services in Data Factory V2 and we are trying to link existing gateway “SQLServerGateway” in linked services in V2. But we are not able to fetch that gateway (SQLServerGateway) in dropdown while creating new linked service for on-premises SQL Server. </p>

<p>Due to gateway not populating in the dropdown, we coded the below part in an advanced note.  But we still receive some error while testing the connection. 
Hence we would like to know how to connect the existing gateway in Data Factory V2 linked service. </p>

<pre><code>{
    ""name"": ""SQLConn_RgTest"",
    ""properties"": {
        ""type"": ""SqlServer"",
        ""typeProperties"": {
            ""connectionString"": {           
                ""type"": ""SecureString"",             
                ""value"": ""Data Source=XXXX;Initial Catalog=XXXX;Integrated Security=False;user id=XXXX;password=XXXX;""
            }
        },
        ""connectVia"": {
            ""referenceName"": ""SQLServerGateway"",
            ""type"": ""SelfHostedIntegrationRuntime""
        }
    }
}
</code></pre>
","<azure><azure-data-factory>","2018-07-27 11:48:22","324","2","1","51644811","<p>Currently you cannot reuse the V1 gateways in Data Factory V2.</p>

<p>For your scenario, you need to create a new self-hosted IR in Data Factory V2 and install it on another machine to run the test pipeline. You can follow <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-hybrid-copy-portal#create-a-pipeline"" rel=""nofollow noreferrer"">this tutorial</a> to setup the pipeline. And we will support sharing the self-hosted IR across V2 data factories soon.</p>
"
"51553141","Azure Data Factory check rowcount of copied records","<p>I am designing a ADF pipeline that copies rows from a SQL table to a folder in Azure Data Lake. After that the rows in SQL should be deleted. But for this delete action takes place I want to know if the number rows that are copied are the same as the number of rows that were I selected in the beginning of the pipeline.
Is there a way to get the rowcount fo the copy action and use this in another action (like a lookup)</p>

<p>Edit follow up question:
Bo Xiao's answer is OK. BUt then I have a follow up question. After the copy-activity I put an If Condition with the following expression:</p>

<pre><code>@activity('LookUpActivity').output.firstRow.RecordsRead ==      @{activity('copyActivity').output.rowsCopied
</code></pre>

<p>But then I get the error: @activity('LookUpActivity').output.firstRow.RecordsRead == @{activity('copyActivity').output.rowsCopied</p>

<p>Isn't it possible to compare output parameters of two activities to see if this is True?</p>

<p>extra edit: I just found an error in this piece of code. I forgot a ""{"" at the begin of the code. But then the code is still wrong. To compare two outputs from earlier activities the code must be:</p>

<pre><code>@equals(activity('LookUpActivity').output.firstRow.RecordsRead,activity('copyActivity').output.rowsCopied)
</code></pre>
","<azure-data-factory>","2018-07-27 07:41:47","18887","7","1","51553521","<p>You can find copied rows in activity output as pictured below.</p>
<p>And you can use the output value like this:</p>
<pre><code>@activity('copyActivity').output.rowsCopied
</code></pre>
<p><a href=""https://i.stack.imgur.com/wrPoo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wrPoo.png"" alt=""enter image description here"" /></a></p>
"
"51542670","I need to get only updated data from on-promise database","<p>I need to get only updated data from on-promise database. Because on-promise database updates weekly. If I set up a scheduler with Azure Data Factory, would that work?</p>
","<azure><azure-data-factory>","2018-07-26 15:36:21","37","0","2","51543341","<p>Have you looked at creating an integration run time with a linked service, have a look at the link, there is also change tracking you can setup  <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-multiple-tables-portal"" rel=""nofollow noreferrer"">Incrementally load data from a source data store to a destination data store</a></p>
"
"51542670","I need to get only updated data from on-promise database","<p>I need to get only updated data from on-promise database. Because on-promise database updates weekly. If I set up a scheduler with Azure Data Factory, would that work?</p>
","<azure><azure-data-factory>","2018-07-26 15:36:21","37","0","2","51746406","<p>Azure Data Factory supports scheduler feature, you could create a Schedule Trigger and would help.
Here is the link to follow: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger</a> </p>
"
"51531383","Error while starting ADFv2 Azure-SSIS Integration Runtime","<p>We are trying to start the Azure-SSIS Integration runtime in ADFv2 using the <strong>Start-AzureRmDataFactoryV2IntegrationRuntime</strong> PowerShell command. The command runs for 20 minutes and then returns the following error: <em>An error occurred while requesting the status of Integration Runtime, but you can manually execute the following command: Get-AzureRmDataFactoryV2IntegrationRuntime -DataFactoryName MyDataFactory -ResourceGroupName MyResourceGroup -Name MyIR –Status</em>. The IR is getting started, but not sure why the command shows this error. Could you let me know how to resolve this?</p>
","<ssis><azure-data-factory>","2018-07-26 05:09:05","398","0","2","51531518","<p>There seems to be an IR provisioning error, so could you show the complete error message in PSH?  Alternatively, the error message can also be found on the IR monitoring page of ADF web app.</p>
"
"51531383","Error while starting ADFv2 Azure-SSIS Integration Runtime","<p>We are trying to start the Azure-SSIS Integration runtime in ADFv2 using the <strong>Start-AzureRmDataFactoryV2IntegrationRuntime</strong> PowerShell command. The command runs for 20 minutes and then returns the following error: <em>An error occurred while requesting the status of Integration Runtime, but you can manually execute the following command: Get-AzureRmDataFactoryV2IntegrationRuntime -DataFactoryName MyDataFactory -ResourceGroupName MyResourceGroup -Name MyIR –Status</em>. The IR is getting started, but not sure why the command shows this error. Could you let me know how to resolve this?</p>
","<ssis><azure-data-factory>","2018-07-26 05:09:05","398","0","2","51979912","<p>I found Azure powershell version 5.1.1 has this issue, and when I upgrade Azure powershell to version 6.6, there is no such issue. you can upgrade to newest Azure powershell.</p>
"
"51522697","U-SQL Azure Data Lake Analytics search files by date","<p>I have U-SQL script that uses file pattern to find files in Azure Data Lake and extracts some data from them:</p>

<pre><code>DECLARE @input_file string = @""\data\{*}\{*}\{*}.avro"";
@data = EXTRACT 
        Column1 string,
        Column2 double
FROM @input_file
USING new MyExtractors.AvroExtractor();
</code></pre>

<p>File pattern is:</p>

<pre><code>data/{Namespace}-{EventHub}-{PartitionId}/{Year}-{Month}-{Day}/{Hour}-{Minute}-{Second}
</code></pre>

<p><strong>Problem</strong>: Custom extractor is executing very slow. I have many files in the Data Lake and it takes 15hrs to process and costs $600USD per run. Too slow and too expensive.</p>

<p>I only need to extract fresh data from files that are not more than 90 days old. How can I filter out old files using file pattern, file date modified or any other technique?</p>
","<azure><azure-data-factory><azure-data-lake><u-sql><azure-stream-analytics>","2018-07-25 15:29:05","505","0","2","51558970","<p>You can leverage GetMetadata activity in Azure data factory to retrieve lastModifiedTime of the files.</p>

<p>ref doc:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">Get metadata activity in Azure Data Factory</a></p>

<p>And there's a relevant post about incremental copy:</p>

<p><a href=""https://stackoverflow.com/questions/50298122/azure-data-factory-incremental-data-load-from-sftp-to-blob"">Azure data factory | incremental data load from SFTP to Blob</a></p>
"
"51522697","U-SQL Azure Data Lake Analytics search files by date","<p>I have U-SQL script that uses file pattern to find files in Azure Data Lake and extracts some data from them:</p>

<pre><code>DECLARE @input_file string = @""\data\{*}\{*}\{*}.avro"";
@data = EXTRACT 
        Column1 string,
        Column2 double
FROM @input_file
USING new MyExtractors.AvroExtractor();
</code></pre>

<p>File pattern is:</p>

<pre><code>data/{Namespace}-{EventHub}-{PartitionId}/{Year}-{Month}-{Day}/{Hour}-{Minute}-{Second}
</code></pre>

<p><strong>Problem</strong>: Custom extractor is executing very slow. I have many files in the Data Lake and it takes 15hrs to process and costs $600USD per run. Too slow and too expensive.</p>

<p>I only need to extract fresh data from files that are not more than 90 days old. How can I filter out old files using file pattern, file date modified or any other technique?</p>
","<azure><azure-data-factory><azure-data-lake><u-sql><azure-stream-analytics>","2018-07-25 15:29:05","505","0","2","51622270","<p>You could use the <code>.AddDays</code> method of <code>DateTime.Now</code>, although whether or not this actually filters out all your files is (I think) dependent on your custom extractor, eg</p>

<pre><code>//DECLARE @input_file string = @""\data\{*}\{*}\{*}.csv"";
DECLARE @input_file string = @""\data\{Namespace}-{EventHub}-{PartitionId}\{xdate:yyyy}-{xdate:MM}-{xdate:dd}\{Hour}-{Minute}-{Second}.csv"";


// data/{Namespace}-{EventHub}-{PartitionId}/{Year}-{Month}-{Day}/{Hour}-{Minute}-{Second}
@input =
    EXTRACT Column1 string,
            Column2 double,
            xdate DateTime,
            Namespace string,
            EventHub string,
            PartitionId string,
            Hour int,
            Minute int,
            Second int
    FROM @input_file
    USING Extractors.Csv();
//USING new MyExtractors.AvroExtractor();


@output =
    SELECT Column1,
           Column2
    FROM @input
    WHERE xdate &gt; DateTime.Now.AddDays(-90);


OUTPUT @output
TO ""/output/output.csv""
USING Outputters.Csv();
</code></pre>

<p>In my simple tests with .Csv this worked to reduce the input stream from 4 to 3 streams, but as mentioned I'm not sure if this will work with your custom extractor.</p>

<p><a href=""https://i.stack.imgur.com/3GBAF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3GBAF.png"" alt=""Local results""></a></p>
"
"51521638","Input file path with dates ADFV2","<p>Currently we are moving out data analytics pipeline to ADFV2. In V1 we defined the input filepath in the parameter file as - <code>/Inputpath/{filedate:yyyy}.{filedate:MM}.{filedate:dd}/RealtimeUsage.{filedate:yyyy}.{filedate:MM}.{filedate:dd}.{filedate:HH}.{*}csv</code></p>

<p>How is this accomplished in ADFV2. The activity reads in the parameter from the parameter file as such - </p>

<pre><code>""ActivityName"": {
""path"": ""$.properties.activities[?(@.name=='ActivityName')].typeProperties.ParameterName"",
          ""value"": ""/Input/{filedate:yyyy}.{filedate:MM}.{filedate:dd}/RealtimeUsage.{filedate:yyyy}.{filedate:MM}.{filedate:dd}.{filedate:HH}.{*}csv""
</code></pre>
","<u-sql><azure-data-factory>","2018-07-25 14:38:15","144","0","1","51529985","<p>If you want to read a file in the following pattern:
Inputpath/2018.07.26/RealtimeUsage.2018.07.26.01.{*}csv</p>

<p>You could write your file path expression in the following:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>""fileName"": {
type: ""Expression"",
value: ""RealtimeUsage.@{formatDateTime(pipeline().parameters.windowStart,'yyyy')}.@{formatDateTime(pipeline().parameters.windowStart,'MM')}.@{formatDateTime(pipeline().parameters.windowStart,'dd')}.@{formatDateTime(pipeline().parameters.windowStart,'HH')}.{*}csv"",
}</code></pre>
</div>
</div>
</p>

<p>You could also use the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-copy-data-tool"" rel=""nofollow noreferrer"">copy data tool</a> to help you generate this expression.
<a href=""https://i.stack.imgur.com/jWX9O.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jWX9O.png"" alt=""enter image description here""></a></p>
"
"51519726","issue in loading data with Azure data factory","<p>I am trying to load a lot of csv files from blob storage to Azure SQL Data Warehouse through Azure data factory. As I am dealing with massive number of rows, the desired approach is to use PolyBase to bulk loading the data. When I point the source to one single file, SQL DW PolyBase is displayed as true but when I point to all  csv files, the SQL DW PolyBase is displayed as false. Does anyone have experienced this issue?</p>
","<azure-data-factory><azure-synapse><polybase>","2018-07-25 13:09:58","255","0","1","51986110","<p>You could always change the allow polybase to true in UI.<a href=""https://i.stack.imgur.com/QbZgS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QbZgS.png"" alt=""enter image description here""></a>
Or specify this property in json:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>                    ""sink"": {
                        ""type"": ""SqlDWSink"",
                        ""allowPolyBase"": true
                         }</code></pre>
</div>
</div>
</p>
"
"51508393","ADF v2 Cancel InProgress Pipeline","<p>I can get the List of Pipelines using the following but cannot find the option to cancel the InProgress pipeline. How can i cancel pipeline that are in progress ?</p>

<pre><code>var pf = new PipelineRunFilterParameters();
            pf.LastUpdatedAfter = DateTime.Now.AddDays(-20);
            pf.LastUpdatedBefore = DateTime.Now.AddDays(2);

            var pr = client.PipelineRuns.QueryByFactory(resourceGroup, dataFactoryName,pf);
            foreach(var r in pr.Value)
            {
                Console.WriteLine(r.PipelineName + "" ... "" + r.Status);

                if (r.Status == ""InProgress"")
                   //How do i Cancel this pipeline ?   

            }
</code></pre>
","<azure-data-factory>","2018-07-24 22:27:16","1001","1","1","51514583","<p>there is a method Suspend, that will suspend a running pipeline instance. Check <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactories.pipelineoperationsextensions.suspend?view=azure-dotnet"" rel=""nofollow noreferrer"">this</a></p>

<p>Did not try it by myself through.</p>
"
"51479292","Passing source file name to destination in an ADFv1 pipeline","<h2>Scenario</h2>

<p>I'm developing an ETL with Azure Data Factory v1 (unfortunately <strong>I can't use Azure Data Factory v2</strong>).</p>

<p>I want to read all .csv files from a given blob storage container, then write the content of each file to a table in SQL Azure.</p>

<h2>The problem</h2>

<p>The destination table contains all the columns from the csv file. It must also contain a new column with the name of the file where the data come from. </p>

<p>This is where I'm stuck: I can't find a way to pass the file name from the source dataset (.csv file from blob storage source) to the destination dataset (Sql Azure sink).</p>

<h2>What I have already tried</h2>

<p>I already implemented a pipeline that reads a file from blob storage and saves it to a table in SQL Azure.</p>

<p>Here is an excerpt from the json that copies a single file to SQL Azure:</p>

<pre class=""lang-js prettyprint-override""><code>{
    ""name"": ""pipelineFileImport"",
    ""properties"": {
        ""activities"": [
            {
                ""type"": ""Copy"",
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""BlobSource"",
                        ""recursive"": false
                    },
                    ""sink"": {
                        ""type"": ""SqlSink"",
                        ""writeBatchSize"": 0,
                        ""writeBatchTimeout"": ""00:00:00""
                    },
                    ""translator"": {
                        ""type"": ""TabularTranslator"",
                        ""columnMappings"": ""TypeOfRecord:TypeOfRecord,TPMType:TPMType,...""
                    }
                },
                ""inputs"": [
                    {
                        ""name"": ""InputDataset-cn0""
                    }
                ],
                ""outputs"": [
                    {
                        ""name"": ""OutputDataset-cn0""
                    }
                ],
                ""policy"": {
                    ""timeout"": ""1.00:00:00"",
                    ""concurrency"": 1,
                    ""executionPriorityOrder"": ""NewestFirst"",
                    ""style"": ""StartOfInterval"",
                    ""retry"": 3,
                    ""longRetry"": 0,
                    ""longRetryInterval"": ""00:00:00""
                },
                ""scheduler"": {
                    ""frequency"": ""Day"",
                    ""interval"": 1
                },
                ""name"": ""Activity-0-pipelineFileImport_csv-&gt;[staging]_[Files]""
            }
        ],
        ""start"": ""2018-07-20T09:50:55.486Z"",
        ""end"": ""2018-07-20T09:50:55.486Z"",
        ""isPaused"": false,
        ""hubName"": ""test_hub"",
        ""pipelineMode"": ""OneTime"",
        ""expirationTime"": ""3.00:00:00"",
        ""datasets"": [
            {
                ""name"": ""InputDataset-cn0"",
                ""properties"": {
                    ""structure"": [
                        {
                            ""name"": ""TypeOfRecord"",
                            ""type"": ""String""
                        },
                        {
                            ""name"": ""TPMType"",
                            ""type"": ""String""
                        },
                        ...
                    ],
                    ""published"": false,
                    ""type"": ""AzureBlob"",
                    ""linkedServiceName"": ""Source-TestBlobStorage"",
                    ""typeProperties"": {
                        ""fileName"": ""testFile001.csv"",
                        ""folderPath"": ""fileinput"",
                        ""format"": {
                            ""type"": ""TextFormat"",
                            ""columnDelimiter"": "";"",
                            ""firstRowAsHeader"": true
                        }
                    },
                    ""availability"": {
                        ""frequency"": ""Day"",
                        ""interval"": 1
                    },
                    ""external"": true,
                    ""policy"": {}
                }
            },
            {
                ""name"": ""OutputDataset-cn0"",
                ""properties"": {
                    ""structure"": [
                        {
                            ""name"": ""TypeOfRecord"",
                            ""type"": ""String""
                        },
                        {
                            ""name"": ""TPMType"",
                            ""type"": ""String""
                        },...
                    ],
                    ""published"": false,
                    ""type"": ""AzureSqlTable"",
                    ""linkedServiceName"": ""Destination-SQLAzure-cn0"",
                    ""typeProperties"": {
                        ""tableName"": ""[staging].[Files]""
                    },
                    ""availability"": {
                        ""frequency"": ""Day"",
                        ""interval"": 1
                    },
                    ""external"": false,
                    ""policy"": {}
                }
            }
        ]
    }
}
</code></pre>

<h2>What I need</h2>

<p>I need a way to pass the name of the source file to the destination dataset in order to write it in the SQL Azure database.</p>
","<azure><azure-storage><etl><azure-blob-storage><azure-data-factory>","2018-07-23 12:52:01","1354","0","1","51481279","<p>No native way to handle this. But I think you could use a stored procedure to achieve this.</p>

<p>Please reference for stored procedure property.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-azure-sql-connector#copy-activity-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-azure-sql-connector#copy-activity-properties</a></p>
"
"51448253","Routing an XML web service onto azure using SSIS","<p>we have a webservice call to export the contract header data from Source to external system.The web service will extract all the contract workspaces information  based on the date range in the CSV file format. I tried to use an http connector in azure to move that web service data onto azure which was unsuccessful. Now trying to do it with SSIS to call that service and route it onto azure. I need a step by step procedure on how can we achieve this. There is a BOT process in Azure but, we are trying to implement it using SSIS. Can anyone please help me on this?</p>
","<c#><sql><azure><ssis><azure-data-factory>","2018-07-20 18:12:19","134","0","1","51521278","<p>that's straightforward, create a logic app based on HTTP trigger, then you have data factory connector in connectors. </p>

<p><a href=""http://blogs.adatis.co.uk/matthow/post/Using-ADF-V2-Activities-in-Logic-Apps"" rel=""nofollow noreferrer"">This</a> is a basic scenario how you can do that. After submitting logic app it will generate URL for HTTP request. </p>

<p>There is 3 Data Factory actions in Logic app:</p>

<ul>
<li>Cancel a pipeline run</li>
<li>Create a pipeline run (this will create a new pipeline)</li>
<li>Get a pipeline run (this will run existing pipeline)</li>
</ul>

<p>I guess you need to choose <em>Get a pipeline run</em></p>

<p>Hope this is helpful.</p>
"
"51433976","Has anyone managed to successfully pass parameters to SSIS via a Data Factory ExecuteSSISPackage task?","<p>I am trying to pass parameters to SSIS via a Data Factory ExecuteSSISPackage task.  </p>

<p>I have added project parameters, package parameters etc to the JSON code but they are always null in SSIS.  An excerpt of the JSON code is shown below.</p>

<pre><code>""packageParameters "": {
     ""X"": { ""value"": ""@pipeline().parameters.X""},
     ""Y"": { ""value : ""xyzzy""  }}
</code></pre>

<p>If you have successfully passed parameters can you give me some hints or direct me to documentation.  It has been difficult so far to debug.</p>
","<parameters><ssis-2012><azure-data-factory>","2018-07-20 01:49:48","296","1","1","51435693","<p>Could you remove the blank space within ""packageParameters "" and try again? I have tried it, and it works fine to me.</p>
"
"51430990","Azure ADF v2- How to add/append timestamp to file name getting deployed to Azure Data Lake","<p>I am deploying a file to azure datalake but want to append datetime stamp to filename . I  created a dataset and created a parameter with value as utcnow().
But file is not getting deployed with timestamp appended.</p>

<p>{</p>

<pre><code>""name"": ""tst"",

""properties"": {

    ""linkedServiceName"": {

        ""referenceName"": ""LS_ADLS_AzureDataLakeStoreLinkedService"",

        ""type"": ""LinkedServiceReference""

    },

    ""parameters"": {

        ""customVariable"": {

            ""type"": ""String"",

            ""defaultValue"": ""utcnow()""

        }

    },

    ""type"": ""AzureDataLakeStoreFile"",

    ""typeProperties"": {

        ""format"": {

            ""type"": ""TextFormat"",

            ""columnDelimiter"": ""|"",

            ""rowDelimiter"": """",

            ""nullValue"": """",

            ""treatEmptyAsNull"": true,

            ""skipLineCount"": 0,

            ""firstRowAsHeader"": true

        },

        ""fileName"": ""abc-{customVariable}"",

        ""folderPath"": ""clusters/diageo-eun-analytics-nonprod-hdi-hd-nampolaris-dev01/hive/warehouse/Spirit_Deal/tst"",

        ""partitionedBy"": [

            {

                ""name"": ""customVariable"",

                ""value"": {

                    ""type"": ""DateTime"",

                    ""date"": ""SliceStart"",

                    ""format"": ""yyyyMMddHH""

                }

            }

        ]

    }

},

""type"": ""Microsoft.DataFactory/factories/datasets""
</code></pre>

<p>}</p>

<p>But filename is now coming with timestamp appended i.e. abc-{customVariable}.Please tell how to add in ADV v2</p>
","<azure><azure-data-lake><azure-data-factory>","2018-07-19 20:10:27","3056","1","1","51440702","<p>You definitely can parameterize file names and pathes. Phps. you try @CONCAT. The following example works, showing how to build folder pathes and filenames dynamically:</p>

<pre><code>{
    ""name"": ""TgtADLSSQL"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""Destination-DataLakeStore-gqf"",
            ""type"": ""LinkedServiceReference""
        },
        ""folder"": {
            ""name"": ""Target""
        },
        ""type"": ""AzureDataLakeStoreFile"",
        ""typeProperties"": {
            ""format"": {
                ""type"": ""ParquetFormat""
            },
            ""fileName"": {
                ""value"": ""@CONCAT('Ingest_Date=', formatDateTime(trigger().startTime, 'yyyy-MM-dd/'),'Incremental-', pipeline().RunId, '.snappy.parquet')"",
                ""type"": ""Expression""
            },
            ""folderPath"": {
                ""value"": ""/data/raw/corporate/sql/@{item().TABLE_NAME}"",
                ""type"": ""Expression""
             }
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p>See also <a href=""http://%20https://stackoverflow.com/questions/48373223/azure-data-factory-v2-dataset-dynamic-folder"" rel=""nofollow noreferrer"">Dynamic folders in ADF V2</a>.</p>
"
"51430708","Is it possible to send messages to an Azure Service Bus Queue or Event Hub from a U-SQL script?","<p>Is it possible for an Azure U-SQL script to put messages on an Azure Service Bus Queue or an Azure Event Hub? Please cite some documentation, if you can find it (since I can't find it).</p>
","<azure><azure-data-factory><azureservicebus><azure-data-lake><azure-eventhub>","2018-07-19 19:48:26","372","1","2","51433239","<p>I got my answer <a href=""https://social.msdn.microsoft.com/Forums/en-US/6b12001f-8684-48aa-a4a8-9f798e55d294/is-it-possible-to-send-an-external-http-request-or-messages-to-an-azure-service-bus-queue-from-a?forum=AzureDataLake"" rel=""nofollow noreferrer"">here</a>.</p>

<blockquote>
  <p>U-SQL scripts cannot access any external services, including Azure services such as web apps (with only a few exceptions like ADLS and WASB storage).  This is to prevent an unintended DDOS attack, since U-SQL will automatically scale that request across potentially hundreds or thousands of nodes, all running over potentially millions of rows and requesting simultaneously.  Please see Michael Rys' answer <a href=""https://stackoverflow.com/questions/41888772/does-u-sql-allow-custom-code-to-call-external-services"">here</a> for more information.  </p>
</blockquote>
"
"51430708","Is it possible to send messages to an Azure Service Bus Queue or Event Hub from a U-SQL script?","<p>Is it possible for an Azure U-SQL script to put messages on an Azure Service Bus Queue or an Azure Event Hub? Please cite some documentation, if you can find it (since I can't find it).</p>
","<azure><azure-data-factory><azureservicebus><azure-data-lake><azure-eventhub>","2018-07-19 19:48:26","372","1","2","51442350","<p>As stated, this is not allowed.</p>

<p>A possible workaround would be to have the u-sql script output a file with messages to blob storage and have an azure function <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob#trigger"" rel=""nofollow noreferrer"">pick those up</a> and send them to an <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-service-bus#output"" rel=""nofollow noreferrer"">Azure Service Bus Queue</a> or an <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-event-hubs#output"" rel=""nofollow noreferrer"">Azure Event Hub</a>.</p>
"
"51430699","Is there a way to insert a document with a nested array in Azure Data Factory?","<p>I am trying to add documents in CosmosDb that has a nested array. I am using the <code>Copy Activity</code>. </p>

<p>Sample Document:</p>

<pre><code>{
  ""itemNumber"": ""D10001"",      
  ""readings"" : [
                  { ""value"": 25, ""ets"":""100011111""},
                  { ""value"": 35, ""ets"":""100011122""}
               ]
}
</code></pre>

<p>In the source dataset I formatted the readings array as a <code>string</code> in my SQL query, and set the data type in the sink dataset as an <code>Object</code>. The data is copied, but the readings are stringified.</p>

<p>Is there a means to configure the Copy Activity to handle this array?</p>
","<azure-cosmosdb><azure-data-factory>","2018-07-19 19:47:32","1833","2","2","51434278","<p>As I know, no such properties could help you convert string data to array format in adf cosmos db configuration. </p>

<p>Since you are using adf to import data so that you can't use <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/programming#database-pre-triggers"" rel=""nofollow noreferrer"">PreTrigger</a> to change the format of created documents.PreTrigger need to be invoked by code or rest api. </p>

<p>So, as workaround, I suggest you using <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-cosmos-db-triggered-function"" rel=""nofollow noreferrer"">Azure Function Cosmos DB Trigger</a> to process every document when they imported into database. Please refer to my function code:</p>

<pre><code>using System.Collections.Generic;
using Microsoft.Azure.Documents;
using Microsoft.Azure.WebJobs;
using Microsoft.Azure.WebJobs.Host;
using Newtonsoft.Json.Linq;
using System;
using Microsoft.Azure.Documents.Client;

namespace TestADF
{
    public static class Function1
    {
        [FunctionName(""Function1"")]
        public static void Run([CosmosDBTrigger(
            databaseName: ""db"",
            collectionName: ""item"",
            ConnectionStringSetting = ""documentdbstring"",
            LeaseCollectionName = ""leases"")]IReadOnlyList&lt;Document&gt; input, TraceWriter log)
        {
            if (input != null &amp;&amp; input.Count &gt; 0)
            {
                log.Verbose(""Start........."");
                String endpointUrl = ""https://***.documents.azure.com:443/"";
                String authorizationKey = ""key"";
                String databaseId = ""db"";
                String collectionId = ""item"";

                DocumentClient client = new DocumentClient(new Uri(endpointUrl), authorizationKey);

                for (int i = 0; i &lt; input.Count; i++)
                {
                    Document doc = input[i];
                    if ((doc.GetPropertyValue&lt;Boolean&gt;(""alreadyForamt"") == null) || (!doc.GetPropertyValue&lt;Boolean&gt;(""alreadyForamt"")))
                    {                       
                        String readings = doc.GetPropertyValue&lt;String&gt;(""readings"");
                        JArray r = JArray.Parse(readings);

                        doc.SetPropertyValue(""readings"", r);

                        client.ReplaceDocumentAsync(UriFactory.CreateDocumentUri(databaseId, collectionId, doc.Id), doc);

                        log.Verbose(""Update document Id "" + doc.Id);
                    }

                }
            }
        }
    }
}
</code></pre>

<p>Hope it helps you.</p>
"
"51430699","Is there a way to insert a document with a nested array in Azure Data Factory?","<p>I am trying to add documents in CosmosDb that has a nested array. I am using the <code>Copy Activity</code>. </p>

<p>Sample Document:</p>

<pre><code>{
  ""itemNumber"": ""D10001"",      
  ""readings"" : [
                  { ""value"": 25, ""ets"":""100011111""},
                  { ""value"": 35, ""ets"":""100011122""}
               ]
}
</code></pre>

<p>In the source dataset I formatted the readings array as a <code>string</code> in my SQL query, and set the data type in the sink dataset as an <code>Object</code>. The data is copied, but the readings are stringified.</p>

<p>Is there a means to configure the Copy Activity to handle this array?</p>
","<azure-cosmosdb><azure-data-factory>","2018-07-19 19:47:32","1833","2","2","51441530","<p>What is your source? You could copy your data to json files first. And then import it to cosmos DB as is, which means don’t specify the format and structure in your source and sink dataset.</p>
"
"51424192","Update field in SalesForce from Azure DataFactory","<p>I'm looking for a way to update field in SalesForce from Azure DataFactory pipeline. I have standard copy pipeline from SalesForce to SQL server, then I'm executing stored procedure on downloaded data and after that I need to update fields in SalesForce that changed after procedure. Right now I have totally no idea how to even try.</p>
","<azure><salesforce><azure-data-factory>","2018-07-19 13:40:53","1819","0","2","51426010","<p>You could chain your copy activity with a stored procedure activity, and then add another copy activity. You could use ADF V2 UI to help you create the pipeline.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-portal#create-a-pipeline"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-portal#create-a-pipeline</a></p>
"
"51424192","Update field in SalesForce from Azure DataFactory","<p>I'm looking for a way to update field in SalesForce from Azure DataFactory pipeline. I have standard copy pipeline from SalesForce to SQL server, then I'm executing stored procedure on downloaded data and after that I need to update fields in SalesForce that changed after procedure. Right now I have totally no idea how to even try.</p>
","<azure><salesforce><azure-data-factory>","2018-07-19 13:40:53","1819","0","2","51427071","<p>Azure is supporting salesforce as a Source as well as Sink. To copy data to Salesforce, you have to use Salesforce as a sink type.</p>

<p>I don't have much knowledge on Azure, but <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-salesforce#salesforce-as-a-sink-type"" rel=""nofollow noreferrer"">this link</a> will help you to setup salesforce as sink.</p>

<p>You might get difficulty in setting <code>externalIdFieldName</code>. If you already have a field in salesforce containing unique values <code>(except if field)</code>, edit that field and mark <code>External ID</code> as checked. If there is no unique field available then below change needs to do in salesforce (<code>below step is not needed if we map id field of salesforce with database table containing salesforce id value. You can try without adding externalIdFieldName, if not working then use below</code>)  :</p>

<ol>
<li>Create a custom field in salesforce with 'External ID' as checked. e.g. field api name is pipelinecol__c</li>
<li>Copy existing data of id column to newly created field i.e. pipelinecol__c using data loader or code.</li>
</ol>

<p>Sync part will be configured as below in your case:</p>

<pre><code>""sink"": {
                ""type"": ""SalesforceSink"",
                ""writeBehavior"": ""Update"",
                ""externalIdFieldName"": ""pipelinecol__c"",
                ""writeBatchSize"": 10000,
                ""ignoreNullValues"": true
            }
</code></pre>
"
"51420627","Azure Data Factory V2: MDX Query on SAP BW exception Microsoft.DataTransfer.Common.Shared.HybridDeliveryException","<p>I am trying to fetch data from SAP BW system into Azure Data Lake using MDX Query in SAP BW connector. But I am getting exception message in Azure is following :</p>

<pre><code>{
    ""errorCode"": ""2200"",
    ""message"": ""Failure happened on 'Source' side. ErrorCode=UserErrorInvalidDataValue,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Column &amp;apos;[Measures].[SomeMeasure]&amp;apos; contains an invalid value &amp;apos;4.000-2&amp;apos;. Cannot convert &amp;apos;4.000-2&amp;apos; to type &amp;apos;Decimal&amp;apos;.,Source=Microsoft.DataTransfer.Common,''Type=System.InvalidCastException,Message=Specified cast is not valid.,Source=Microsoft.DataTransfer.Common,'"",
    ""failureType"": ""UserError"",
    ""target"": ""Copy1""
}
</code></pre>

<p>From the error, I can understand there are some values in Measures which is not actually numeric value. Changing or correcting value in the SAP system is not in my scope. 
 Is there any option in the Data Factory V2 for SAP BW connection so that I can define the data type of the Measure for input and/or output. or there is any fine-tuning in the MDX query so that I can fetch the data without any error?</p>

<p><strong>This is my MDX Query :</strong></p>

<pre><code>SELECT 
{[Measures].[SomeMeasure]}  ON COLUMNS,
NON EMPTY
{ [0COMP_CODE].[LEVEL01].MEMBERS *
[0COSTELMNT].[LEVEL01].MEMBERS }
ON ROWS
FROM SomeQube
  WHERE {[0FISCPER].[K42015008]}
</code></pre>
","<azure-data-lake><azure-data-factory><mdx-query><sap-bw>","2018-07-19 10:44:41","971","1","2","51421448","<p>If you could skip that incorrect rows, you could set enableSkipIncompatibleRow in as true. Please reference <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-fault-tolerance"" rel=""nofollow noreferrer"">this do</a>c.</p>
"
"51420627","Azure Data Factory V2: MDX Query on SAP BW exception Microsoft.DataTransfer.Common.Shared.HybridDeliveryException","<p>I am trying to fetch data from SAP BW system into Azure Data Lake using MDX Query in SAP BW connector. But I am getting exception message in Azure is following :</p>

<pre><code>{
    ""errorCode"": ""2200"",
    ""message"": ""Failure happened on 'Source' side. ErrorCode=UserErrorInvalidDataValue,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Column &amp;apos;[Measures].[SomeMeasure]&amp;apos; contains an invalid value &amp;apos;4.000-2&amp;apos;. Cannot convert &amp;apos;4.000-2&amp;apos; to type &amp;apos;Decimal&amp;apos;.,Source=Microsoft.DataTransfer.Common,''Type=System.InvalidCastException,Message=Specified cast is not valid.,Source=Microsoft.DataTransfer.Common,'"",
    ""failureType"": ""UserError"",
    ""target"": ""Copy1""
}
</code></pre>

<p>From the error, I can understand there are some values in Measures which is not actually numeric value. Changing or correcting value in the SAP system is not in my scope. 
 Is there any option in the Data Factory V2 for SAP BW connection so that I can define the data type of the Measure for input and/or output. or there is any fine-tuning in the MDX query so that I can fetch the data without any error?</p>

<p><strong>This is my MDX Query :</strong></p>

<pre><code>SELECT 
{[Measures].[SomeMeasure]}  ON COLUMNS,
NON EMPTY
{ [0COMP_CODE].[LEVEL01].MEMBERS *
[0COSTELMNT].[LEVEL01].MEMBERS }
ON ROWS
FROM SomeQube
  WHERE {[0FISCPER].[K42015008]}
</code></pre>
","<azure-data-lake><azure-data-factory><mdx-query><sap-bw>","2018-07-19 10:44:41","971","1","2","54320488","<p>Your filter is incorrect. SAP 0FISCPER dimension is in YYYYMMM format. You need to enter  WHERE {[0FISCPER].[2015008]}. I am almost certain the ""K4"" you entered there is your fiscal year variant, which you should not put there. Objects like this K4 are called compounding infoobject (which your fiscal year/period belongs to if you envision it as hierarchical parent/child structure). But in this specific case, you don't need to specify fiscal year variant. Just remove K4.</p>

<p>I highly advise against skipping ""incorrect rows"". My guidance to you is that you should always fail the job whenever exception happens and investigate the data quality issue. Do not ever put enterprise data integrity at risk. SAP data has to be 100% accurate, not 99.9999%.</p>
"
"51412683","Azure Data factory copy activity failed mapping strings (from csv) to Azure SQL table sink uniqueidentifier field","<p>I have an  Azure data factory (DF) pipeline that consists a Copy activity. The Copy activity uses HTTP connector as source to invoke a REST end-point and returns csv stream that sinks with Azure SQL Database table.</p>

<p>The Copy fails when CSV contains strings (such as <code>40f52caf-e616-4321-8ea3-12ea3cbc54e9</code>) which are mapped to an uniqueIdentifier field in target table with error message <code>The given value of type String from the data source cannot be converted to type uniqueidentifier of the specified target column</code>.  </p>

<p>I have tried to wrapped the source string with <code>{}</code> such as <code>{40f52caf-e616-4321-8ea3-12ea3cbc54e9}</code> with no success.</p>

<p>The Copy activity will work if I modified the target table field from <code>uniqueIdentifier</code> to <code>nvarchar(100)</code>.</p>
","<azure><azure-data-factory>","2018-07-19 00:26:38","6514","1","2","51419543","<p>I reproduce your issue on my side.</p>

<p><a href=""https://i.stack.imgur.com/oq5ub.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oq5ub.png"" alt=""enter image description here""></a></p>

<p>The reason is data types of source and sink are dismatch.You could check the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#data-type-mapping-for-sql-server"" rel=""nofollow noreferrer"">Data type mapping for SQL server</a>.</p>

<p><a href=""https://i.stack.imgur.com/zANft.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zANft.png"" alt=""enter image description here""></a></p>

<p>Your source data type is <code>string</code> which is mapped to nvarchar or varchar, and <code>uniqueidentifier</code> in sql database needs <code>GUID</code> type in azure data factory.</p>

<p>So,please configure sql server <code>stored procedure</code> in your sql server sink as a workaround.</p>

<p>Please follow the steps from this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#invoking-stored-procedure-for-sql-sink"" rel=""nofollow noreferrer"">doc</a>:</p>

<p>Step 1: Configure your Sink dataset:</p>

<p><a href=""https://i.stack.imgur.com/Zrcb2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Zrcb2.png"" alt=""enter image description here""></a></p>

<p>Step 2: Configure Sink section in copy activity as follows:</p>

<p><a href=""https://i.stack.imgur.com/XOQ4b.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XOQ4b.png"" alt=""enter image description here""></a></p>

<p>Step 3: In your database, define the table type with the same name as sqlWriterTableType. Notice that the schema of the table type should be same as the schema returned by your input data.</p>

<pre><code>    CREATE TYPE [dbo].[CsvType] AS TABLE(
    [ID] [varchar](256) NOT NULL
)
</code></pre>

<p>Step 4: In your database, define the stored procedure with the same name as SqlWriterStoredProcedureName. It handles input data from your specified source, and merge into the output table. Notice that the parameter name of the stored procedure should be the same as the ""tableName"" defined in dataset.</p>

<pre><code>Create PROCEDURE convertCsv @ctest [dbo].[CsvType] READONLY
AS
BEGIN
  MERGE [dbo].[adf] AS target
  USING @ctest AS source
  ON (1=1)
  WHEN NOT MATCHED THEN
      INSERT (id)
      VALUES (convert(uniqueidentifier,source.ID));
END
</code></pre>

<p><strong><em>Output:</em></strong></p>

<p><a href=""https://i.stack.imgur.com/UEfkj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UEfkj.png"" alt=""enter image description here""></a></p>

<p>Hope it helps you.Any concern,please free feel to let me know.</p>
"
"51412683","Azure Data factory copy activity failed mapping strings (from csv) to Azure SQL table sink uniqueidentifier field","<p>I have an  Azure data factory (DF) pipeline that consists a Copy activity. The Copy activity uses HTTP connector as source to invoke a REST end-point and returns csv stream that sinks with Azure SQL Database table.</p>

<p>The Copy fails when CSV contains strings (such as <code>40f52caf-e616-4321-8ea3-12ea3cbc54e9</code>) which are mapped to an uniqueIdentifier field in target table with error message <code>The given value of type String from the data source cannot be converted to type uniqueidentifier of the specified target column</code>.  </p>

<p>I have tried to wrapped the source string with <code>{}</code> such as <code>{40f52caf-e616-4321-8ea3-12ea3cbc54e9}</code> with no success.</p>

<p>The Copy activity will work if I modified the target table field from <code>uniqueIdentifier</code> to <code>nvarchar(100)</code>.</p>
","<azure><azure-data-factory>","2018-07-19 00:26:38","6514","1","2","72491309","<p>There is a way to fix guid conversion into <code>uniqueidentifier</code> SQL column type properly via JSON configuration.
Edit the Copy Activity via Code {} button in top right toolbar.</p>
<p>Put:</p>
<pre><code>    &quot;translator&quot;: {
        &quot;type&quot;: &quot;TabularTranslator&quot;,
        &quot;typeConversion&quot;: true
    }
</code></pre>
<p>into <code>typeProperties</code> block of the <code>Copy</code> activity. This will also work if Mapping schema is unspecified / dynamic.</p>
"
"51398187","Azure Data Factory : Invalid URI: The Uri scheme is too long","<p>I am using a Copy activity in Azure Data Factory to copy the data from Http Source to datalake. My Url contains sql query as a query param. Iam getting ""Invalid URI: The Uri scheme is too long"" error
Possible Error conditions:
SQL query with more select fields and date field in where condition - Error is thrown
SQL query with more select fields and string field in where condition - No Error
SQL query with less select fields and date field in where condition - No Error</p>

<p>API looks like the below one:
<a href=""http://example.com/api/query?q=select"" rel=""nofollow noreferrer"">http://example.com/api/query?q=select</a> a,b,c,d........ from table where date >= '' and date &lt; ''
I'd appreciate any help on this. </p>
","<azure><azure-data-factory>","2018-07-18 09:19:20","749","1","1","54895794","<p>This seems to be related to the issue mentioned in the below link. Replacing <code>:</code> with <code>%3A</code> resolved the issue for me.</p>

<p><a href=""https://github.com/OData/odata.net/issues/1318"" rel=""nofollow noreferrer"">https://github.com/OData/odata.net/issues/1318</a></p>
"
"51381759","Data Factory v1 mask some headers as credential in http headers","<p>I have a Data Factory (v1) which downloads some files from an HTTP server.</p>

<p>Within the dataset pointing to the file location on this server we add an API key as an additional header to the HTTP request. We don't want this key to be visible from the portal similar to how Linked Services mask credentials after having been deployed.</p>

<p>The following Json files define the source linked service, the source dataset and the copy activity.</p>

<ul>
<li>HTTP_source_linkedservice.json</li>
</ul>

<p><code>
{
    ""name"": ""HTTPSourceLinkedService"",
    ""properties"": {
        ""hubName"": ""this_is_a_hubname"",
        ""type"": ""Http"",
        ""typeProperties"": {
            ""url"": ""https://website.com"",
            ""authenticationType"": ""Anonymous""
        }
    }
}
</code></p>

<ul>
<li>HTTP_source_dataset</li>
</ul>

<p><code>
{
    ""name"": ""HTTPSourceDataset"",
    ""properties"": {
        ""published"": false,
        ""type"": ""Http"",
        ""linkedServiceName"": ""HTTPSourceLinkedService"",
        ""typeProperties"": {
            ""relativeUrl"": ""/main_file_to_download"",
            ""additionalHeaders"": ""X-api-key: API_KEY_HERE\n""
        },
        ""availability"": {
            ""frequency"": ""Day"",
            ""interval"": 1
        },
        ""external"": true,
        ""policy"": {}
    }
}
</code></p>

<ul>
<li>Copy Activity</li>
</ul>

<p><code>
{
    ""type"": ""Copy"",
    ""typeProperties"": {
         ""source"": {
             ""type"": ""HttpSource""
          },
          ""sink"": {
             ""type"": ""BlobSink"",
             ""writeBatchSize"": 0,
             ""writeBatchTimeout"": ""00:00:00""
           }
     },
     ""inputs"": [
         {
            ""name"": ""HTTPSourceDataset""
         }
      ],
      ""outputs"": [
          {
             ""name"": ""HTTPSinkDataset""
          }
       ],
       ""scheduler"": {
           ""frequency"": ""Day"",
           ""interval"": 1
       },
       ""name"": ""CopyFileFromServer""
}
</code></p>

<p>I know we could use a Custom Activity to make the request itself and fetch the API key from a keyvault but I really want to use the standard Copy Activity.</p>

<p>Is there a way to achieve this ?</p>
","<azure><azure-data-factory>","2018-07-17 12:46:02","389","0","1","51392210","<p>Unfortunately, I think this is not possible.  Header field are defined as string. And in v1, there is even no secure string which is introduced in v2 to indicate a field is credentials.
But I think this can’t be achieved in v2 either. As the model type is fixed.</p>
"
"51376936","Unable to publish Azure Data factory Pipeline changes","<p>I have created a simple data factory pipeline for copying files from azure blob storage to azure data lake. For this i have used one event based trigger. Trigger will automatically run pipeline if new blob will come to the blob storage location. If i am publishing my pipeline with my trigger in stopped state then it is publishing without any error. But, when i am publishing with my trigger in started state then it is giving following error: -</p>

<blockquote>
  <p>Response with status: 403 Forbidden for URL:
  <a href=""https://management.azure.com/subscriptions/subscriptionId/providers/Microsoft.EventGrid/register?api-version=2018-02-01"" rel=""nofollow noreferrer"">https://management.azure.com/subscriptions/subscriptionId/providers/Microsoft.EventGrid/register?api-version=2018-02-01</a></p>
</blockquote>

<p>As event based triggers are latest in ADF, I am unable to get any blogs related to this. Please help me. Any help will be appreciated.</p>
","<azure><azure-data-factory>","2018-07-17 08:47:01","7459","3","1","51377451","<p>I think you will have to stop your trigger first. Tumbling window trigger and schedule trigger also need be stopped and then updated.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger"" rel=""nofollow noreferrer"">Make sure that your subscription is registered with the Event Grid resource provider</a>.</p>
"
"51374568","New instance of pipeline is triggered when first is running in case of hourly schedule Azure data factory v2","<p>We have pipeline scheduled to run every hour. Sometimes the pipeline takes more than an hour to complete. </p>

<p>Currently, another instance of the pipeline starts in the next one hour resulting in 2 instances of the pipeline running in parallel. </p>

<p>How do we avoid this? How to make the next schedule to wait until the prior schedule completes (like SQL Server job agent)?</p>
","<asp.net><sql-server><azure-data-factory>","2018-07-17 06:28:00","159","0","1","51377521","<p>If you are using tumbling window trigger, you could set the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger#tumbling-window-trigger-type-properties"" rel=""nofollow noreferrer"">maxConcurrency</a> to 1. </p>
"
"51352725","Azure Data Factory mapping 2 columns in one column","<p>Can somebody help me solving the error am getting in concatenating the two columns i.e <code>first name</code> and <code>last name</code> from my text file and merging the two columns into one name column in my <strong>Azure SQL database</strong> as a <code>sink</code> in <strong>Azure Data Factory</strong> and another question is that I want to choose the first letter of the column gender that is <code>M</code> or <code>F</code> for <em>male</em> and female respectively from the source text file and changing it to one letter M or F in my gender column in the Azure data factory pipeline <a href=""https://i.stack.imgur.com/TnrVu.png"" rel=""nofollow noreferrer"">enter image description here</a>?</p>

<ul>
<li><strong>Update 1</strong></li>
</ul>

<p>My table name is [dbo].[Contact] and after applying this procedure am getting this error, and my columns names in the text file has space in between them, like First Name and Last Name, does that create a problem too?</p>

<p><a href=""https://i.stack.imgur.com/TZ24f.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<azure><azure-sql-database><azure-blob-storage><azure-data-factory>","2018-07-15 22:38:34","4129","1","1","51354266","<p>Based on the doc: Schema mapping in copy activity, merging columns is supported by schema mapping.</p>

<p>As workaround , I suggest configure sql server <code>stored procedure</code> in your <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#sql-server-as-sink"" rel=""nofollow noreferrer"">sql server sink</a>. It can merge the data being copied with existing data.</p>

<p>Please follow the steps from this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#invoking-stored-procedure-for-sql-sink"" rel=""nofollow noreferrer"">doc</a>:</p>

<p>Step 1: Configure your Output dataset:</p>

<p><a href=""https://i.stack.imgur.com/kT3CR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kT3CR.png"" alt=""enter image description here""></a></p>

<p>Step 2: Configure Sink section in copy activity as follows:</p>

<p><a href=""https://i.stack.imgur.com/u0l5o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u0l5o.png"" alt=""enter image description here""></a></p>

<p>Step 3: In your database, define the table type with the same name as sqlWriterTableType. Notice that the schema of the table type should be same as the schema returned by your input data.</p>

<pre><code>CREATE TYPE [dbo].[MarketingType] AS TABLE(
    [FirstName] [varchar](256) NOT NULL,
    [LastName] [varchar](256) NOT NULL,
    [Gender] [varchar](256) NOT NULL
)
</code></pre>

<p>Step 4: In your database, define the stored procedure with the same name as SqlWriterStoredProcedureName. It handles input data from your specified source, and merge into the output table. Notice that the parameter name of the stored procedure should be the same as the ""tableName"" defined in dataset.</p>

<pre><code>Create PROCEDURE spOverwriteMarketing @Marketing [dbo].[MarketingType] READONLY
AS
BEGIN
  MERGE [dbo].[jay] AS target
  USING @Marketing AS source
  ON (1=1)
  WHEN NOT MATCHED THEN
      INSERT (name, gender)
      VALUES (source.FirstName + ' ' + source.LastName, UPPER(left(source.Gender,1)));
END
</code></pre>

<p><strong><em>Output Screenshot:</em></strong></p>

<p><a href=""https://i.stack.imgur.com/Rqqrl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Rqqrl.png"" alt=""enter image description here""></a></p>

<p>Hope it helps you.</p>
"
"51338326","How to save Data factory stored procedure output","<p>Whenever I execute a stored procedure in the ADFv2, it gives me an output as </p>

<pre><code>{
    ""effectiveIntegrationRuntime"": ""DefaultIntegrationRuntime (Australia Southeast)"",
    ""executionDuration"": 34 
}
</code></pre>

<p>even though I have set 2 variables as output in the procedure. Is there any way to map the output of the stored procedure in the ADFv2? Till now I can map the output of all the other activities but not of Stored procedures.</p>
","<azure-data-factory>","2018-07-14 12:04:08","10512","5","2","51344947","<p>You could use a lookup activity to get the result.
Please reference this post. <a href=""https://social.msdn.microsoft.com/Forums/azure/en-US/82e84ec4-fc40-4bd3-b6d5-b742f3cd1a33/adf-v2-how-to-check-if-stored-procedure-output-is-empty?forum=AzureDataFactory"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/azure/en-US/82e84ec4-fc40-4bd3-b6d5-b742f3cd1a33/adf-v2-how-to-check-if-stored-procedure-output-is-empty?forum=AzureDataFactory</a></p>

<p><em>Update by Gagan</em>:
Instead of getting the output of SP (which is not possible in ADFv2 right now), I stored the output in the table and then apply lookup-foreach to the table to get the value.</p>
"
"51338326","How to save Data factory stored procedure output","<p>Whenever I execute a stored procedure in the ADFv2, it gives me an output as </p>

<pre><code>{
    ""effectiveIntegrationRuntime"": ""DefaultIntegrationRuntime (Australia Southeast)"",
    ""executionDuration"": 34 
}
</code></pre>

<p>even though I have set 2 variables as output in the procedure. Is there any way to map the output of the stored procedure in the ADFv2? Till now I can map the output of all the other activities but not of Stored procedures.</p>
","<azure-data-factory>","2018-07-14 12:04:08","10512","5","2","60093089","<p>Stored procedure call in Data factory (v2) does not capture the result data set. So you cannot use the stored procedure activity to get the result data set and refer it in next activities.
Workaround is to use lookup activity to call exact same stored procedure as lookup will get you the result data set from stored procedure. Replace your Stored procedure activity with lookup and it will work.</p>
"
"51328704","Copying data incrementally (delta data) from an API endpoint by HTTP GET to an Azure SQL DB on Azure Data Factory","<p>I am trying to perform an ETL activity in which data hosted in JSON at an API is incrementally copied into an Azure SQL Database table. The problem I'm having is that I am not sure how to account for the new/changed entries. I don't want to delete everything and do a massive copy each time the pipeline is run... Are there any suggestions? The only help I've been able to find thus far cover scenarios in which an Azure SQL Database table is the source instead of the sink...</p>
","<azure><azure-sql-database><etl><azure-data-factory>","2018-07-13 15:44:36","837","0","1","51329074","<p>[Requires ODATA or filtering capability]</p>

<p>The common solution to this problem is to have one (or two fields):</p>

<p>UpdatedDate/CreatedDate</p>

<p>and fetch filtering by UpdatedDate/CreatedDate >= LastSuccessfulSyncDate</p>

<p>You need to modify UpdatedDate/CreatedDate field every time a row is changed. To achieve that, use database triggers or in your application logic.</p>

<p>You can also look at better communication between your application processes using a message broker like <a href=""https://www.rabbitmq.com/"" rel=""nofollow noreferrer"">RabbitMQ</a> or <a href=""https://azure.microsoft.com/en-us/services/service-bus/?&amp;OCID=AID719825_SEM_LJ3lfjer&amp;lnkd=Google_Azure_Brand&amp;gclid=Cj0KCQjwm6HaBRCbARIsAFDNK-hFc2H712OkudNO4PKgjZsU1BZocjXxdv5_EM-7dtI3gc1Y2_Dzpy8aAvE2EALw_wcB&amp;dclid=CNKZ--K6nNwCFVYUgQodrW4PwA"" rel=""nofollow noreferrer"">Azure Service Bus</a></p>
"
"51318989","Azure Data Factory HTTP connector to parse a webpage","<p>A bit new to using Azure for ETL and Machine Learning.</p>

<p>I want to parse a webpage such as <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/studio/algorithm-choice"" rel=""nofollow noreferrer"">here</a> and convert it into a labeled catalog of structured data, to which I can apply ML transforms.</p>

<p>I was reading up the Azure documentation on <a href=""https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/data-factory/v1/data-factory-http-connector.md"" rel=""nofollow noreferrer"">HTTP Connector</a> but I am unclear on a stepwise process to do so using either the Azure Data factory UI or scripts.</p>

<p>Can Azure Data Factory be used to such a parsing task and if yes, is there clear documentation on how the Azure Data Factory UI can be used to do so?</p>
","<azure><machine-learning><etl><azure-data-factory>","2018-07-13 06:35:51","322","0","1","51637380","<p>I think at this point you should be looking at the v2 of ADF.</p>

<p>Regarding your use case I don't see how the Http Connector would resolve the ""parsing"" of the webpage. That connector can help you take the content of the page (by doing a GET request) and move it to somewhere for storing, for example a blob. And then you can trigger some sort of custom activity with code that has the logic to convert the html of the page to the catalog of structured data that you'll like. Then you can feed that to another pipeline that has the ML transforms that you require.</p>

<p>Basically you will have to implement the logic of the parsing by yourself, IMHO ADF can help you with the orchestrating and the movement of data but not with the ""parsing"" side of things.</p>
"
"51314126","Can the waiting time in Wait Acitivity be variable?","<p>I am asking this, because I need to check whether or not a file has been already copied by some pipe line in a different suscription. Thus,I need to check on time basis this event, I want to increase the value of the waiting activity each time the copy activity fails, because this last one did not find the desired file.</p>
","<azure><azure-data-factory>","2018-07-12 20:42:03","25","1","1","51323629","<p>You could make wait time an expression, and pass different value to it each time.
Please take a look at the example <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-filter-activity#example"" rel=""nofollow noreferrer"">in this doc</a>. </p>
"
"51301491","Triggering a Data Factory Pipeline through javascript","<p>I have made this pipeline in Azure Data Factory that copies data from a Azure Storage Table to a Azure SQL database Table. </p>

<p>The Azure storage table is given data from a Javascript chatbot that records answers and stores them in the table. I would like to trigger the CopyTabletoSQL 
through my javascript app once all of the answers have been recorded.</p>

<p>This is my CopyTableToSQL pipeline object.</p>

<pre><code>{
""name"": ""CopyTabletoSQL"",
""type"": ""Copy"",
""policy"": {
    ""timeout"": ""7.00:00:00"",
    ""retry"": 0,
    ""retryIntervalInSeconds"": 30,
    ""secureOutput"": false
},
""typeProperties"": {
    ""source"": {
        ""type"": ""AzureTableSource""
    },
    ""sink"": {
        ""type"": ""SqlSink"",
        ""writeBatchSize"": 10000
    },
    ""enableStaging"": false,
    ""dataIntegrationUnits"": 0
},
""inputs"": [
    {
        ""referenceName"": ""tableInputDataset"",
        ""type"": ""DatasetReference""
    }
],
""outputs"": [
    {
        ""referenceName"": ""OutputSqlTable1"",
        ""type"": ""DatasetReference""
    }
]
}
</code></pre>

<p>Is there any way to have this execute from a javascript app? The docoumentation (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers</a>) only mentions .net, Powershell, REST API and Python SDK but nothing for node.js</p>
","<javascript><json><azure><azure-table-storage><azure-data-factory>","2018-07-12 09:06:55","2009","1","2","51307749","<p>You could call rest api in JavaScript. </p>
"
"51301491","Triggering a Data Factory Pipeline through javascript","<p>I have made this pipeline in Azure Data Factory that copies data from a Azure Storage Table to a Azure SQL database Table. </p>

<p>The Azure storage table is given data from a Javascript chatbot that records answers and stores them in the table. I would like to trigger the CopyTabletoSQL 
through my javascript app once all of the answers have been recorded.</p>

<p>This is my CopyTableToSQL pipeline object.</p>

<pre><code>{
""name"": ""CopyTabletoSQL"",
""type"": ""Copy"",
""policy"": {
    ""timeout"": ""7.00:00:00"",
    ""retry"": 0,
    ""retryIntervalInSeconds"": 30,
    ""secureOutput"": false
},
""typeProperties"": {
    ""source"": {
        ""type"": ""AzureTableSource""
    },
    ""sink"": {
        ""type"": ""SqlSink"",
        ""writeBatchSize"": 10000
    },
    ""enableStaging"": false,
    ""dataIntegrationUnits"": 0
},
""inputs"": [
    {
        ""referenceName"": ""tableInputDataset"",
        ""type"": ""DatasetReference""
    }
],
""outputs"": [
    {
        ""referenceName"": ""OutputSqlTable1"",
        ""type"": ""DatasetReference""
    }
]
}
</code></pre>

<p>Is there any way to have this execute from a javascript app? The docoumentation (<a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers</a>) only mentions .net, Powershell, REST API and Python SDK but nothing for node.js</p>
","<javascript><json><azure><azure-table-storage><azure-data-factory>","2018-07-12 09:06:55","2009","1","2","51320971","<p>Azure Data Factory nodejs sdk is not supported so far. Based on your description, you already have created pipeline in your adf account.You could use Azure Data Factory <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/pipelines/createrun"" rel=""nofollow noreferrer"">Pipelines - Create Run</a> REST api to execute it.</p>

<p>In this process, you need to generate Authorization token in Headers. You could refer to <a href=""https://github.com/AzureAD/azure-activedirectory-library-for-nodejs#server-to-server-via-client-credentials"" rel=""nofollow noreferrer"">ADAL nodejs sdk</a> to generate token. </p>

<p>Before that, you need to grant adf permission to your ad app. </p>

<p><a href=""https://i.stack.imgur.com/1elie.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1elie.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/qfsAf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qfsAf.png"" alt=""enter image description here""></a></p>

<p>Hope it helps you.</p>
"
"51298910","Multi-character column delimiter not supported in ADF connection","<p>My text file has a delimiter with 2 characters (<code>~*</code>).</p>

<p>However ADF supports column delimiters with only one chararacter. Any help or workaround?</p>

<p>It is a text file present in Azure blob storage which needs to be imported to SQL database.</p>

<p><a href=""https://i.stack.imgur.com/gBC1P.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gBC1P.jpg"" alt=""Screenshot here""></a></p>
","<azure-data-factory>","2018-07-12 06:46:02","1290","2","2","52749315","<p>One workaround would be to go with U-SQL scripts, there you can use any delimiter, I usually prefer non-printable characters like ""\u0001"", for instance I use this as a column delimiter in most of my Azure Data Analytics U-SQL Scripts, and you can run the U-SQL script from ADF.</p>

<h2>Update</h2>

<p>I ended up using the Start of Heading character (<code>\u0001</code>), in fact it is one of the options available in the UI and it's been working so far, there are still some bugs with other delimiters like ('¬') but MS is working on that.</p>

<p><a href=""https://i.stack.imgur.com/2TWLd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2TWLd.png"" alt=""enter image description here""></a></p>
"
"51298910","Multi-character column delimiter not supported in ADF connection","<p>My text file has a delimiter with 2 characters (<code>~*</code>).</p>

<p>However ADF supports column delimiters with only one chararacter. Any help or workaround?</p>

<p>It is a text file present in Azure blob storage which needs to be imported to SQL database.</p>

<p><a href=""https://i.stack.imgur.com/gBC1P.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gBC1P.jpg"" alt=""Screenshot here""></a></p>
","<azure-data-factory>","2018-07-12 06:46:02","1290","2","2","70340443","<p><strong>As of Dec 4th 2021, ADF now has the option for a multiple character delimiter on a copy data activity:</strong></p>
<p><a href=""https://i.stack.imgur.com/uFOWC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uFOWC.png"" alt=""ADF Enhancement"" /></a></p>
<p>See below example for where delimiter goes:</p>
<p><a href=""https://i.stack.imgur.com/W0HGA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W0HGA.png"" alt=""Dataset Example CSV"" /></a></p>
"
"51288793","Azure data factory - Voluminous copy activity - dataware","<p>I'm currently facing to a datafactory problem and can't managed to find a solution:
I've mad a pipeline with a copy activity which send data from a datawarehouse to an oracle database.</p>

<p>The pipeline run fine, but take a lot of time (4h-5h).
I think it's caused by the amount of data (Approximately 12 millions rows)</p>

<p>Do you have any idea for making this pipelin run faster?</p>

<p>Thanks for your help,</p>

<p>Guif</p>
","<performance><copy><azure-data-factory>","2018-07-11 15:03:12","81","0","1","51294918","<p>Try bigger DIU (old name: DMU) and increase the parallelism in the copy activity.</p>

<p>You may also try stage the data into a blob first then followed by a copy to Oracle. I know SQL to SQL would have better performance with staging.</p>

<p>Be sure to use an IR region that is close to your data sources.</p>
"
"51284549","how to run a shell script from Azure data factory","<p>How to run a shell script from Azure Data Factory. Inside the shell script I am trying to execute an hql file like below:</p>
<pre><code>/usr/bin/hive -f &quot;wasbs://hivetest@batstorpdnepetradev01.blob.core.windows.net/hivetest/extracttemp.hql&quot; &gt; 
wasbs://hivetest@batstorpdnepetradev01.blob.core.windows.net/hivetest/extracttemp.csv
</code></pre>
<p>My hql file is stored inside a Blob Storage and I want to execute it and collect the result into  a csv file and store it back to Blob Storage . This entire script is stored in shell script which also in a Blob Storage. NowIi want to execute in a Azure Data Factory in hive activity. Help will be appreciated.</p>
","<azure-data-factory>","2018-07-11 11:38:00","3379","1","1","51285557","<p>You could use Hadoop hive activity in ADF. Please take a look at <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-hadoop-hive"" rel=""nofollow noreferrer"">this doc</a>. And you could build your pipeline with <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-portal"" rel=""nofollow noreferrer"">ADF V2 UI</a>.</p>
"
"51283973","I want to copy a specific row and two specific columns from my Azure Table Storage to my SQL Database Table using Azure Data Factory","<p>I have a chatbot that asks 3 questions then record the answers.</p>

<p>my Storage table looks like this:
<a href=""https://imgur.com/e2PkK3z"" rel=""nofollow noreferrer"">https://imgur.com/e2PkK3z</a></p>

<p>I also have a Azure SQL Database Table like this </p>

<pre><code>CREATE TABLE Answers (
    Timestamp DateTimeOfsett,
    Data varchar(255),
); 
</code></pre>

<p>I have set up a Azure Storage Factory following this guide <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-copy-activity-tutorial-using-azure-portal"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-copy-activity-tutorial-using-azure-portal</a> </p>

<p>I've been able to filter out the columns I don't want using the Schema function in Azure Data Factory in my inputDataSet however I also want to filter out all of the rows excep the one with RowKey ""userData"" however I can't for the life of me figure it out as I do not have any prior experience with Azure, SQL or javascript and have only gotten this far using google and microsoft documentation.</p>
","<azure><azure-sql-database><azure-table-storage><chatbot><azure-data-factory>","2018-07-11 11:08:23","1390","0","1","51288332","<ol>
<li>I would like to suggest you use azure data factory v2 as it is GA now and it has a more advanced UI. Though I think this is also achievable in v1.</li>
<li>You could reference this doc for <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-table-storage"" rel=""nofollow noreferrer"">azure table</a>.</li>
<li>You could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-portal"" rel=""nofollow noreferrer"">ADF V2 UI</a> to build and debug your pipeline. </li>
<li>For ""filter out all of the rows excep the one with RowKey"", You could specify <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-table-storage#copy-activity-properties"" rel=""nofollow noreferrer"">azureTableSourceQuery</a></li>
</ol>
"
"51283419","Connecting storage securely to Azure Data Lake Analytics or Data Factory","<p>I am setting up a new Azure Data Lake Analytics (ADLA) PAAS service to run USQL against some existing data sets in blob storage. The blob storage is firewalled for security and when I try to add the storage account to the data sources in ADLA I get the following error. Similar happens for data factory.</p>

<blockquote>
  <p>InvalidArgument: The Storage account '' or its accessKey
  is invalid.</p>
</blockquote>

<p>If I disable the firewall, the storage account can be successfully added. I have tried to add the relevant <a href=""https://www.microsoft.com/en-gb/download/details.aspx?id=41653"" rel=""nofollow noreferrer"">Azure Data Center IP Address ranges</a> but the connection still fails. I have also ticked the ""Allow trusted Microsoft Services box"" but this <a href=""https://learn.microsoft.com/en-gb/azure/storage/common/storage-network-security#exceptions"" rel=""nofollow noreferrer"">does not seem include data lake or data factory</a>. How do I access my storage account from ADLA but still have it secured? </p>
","<azure-storage><azure-data-lake><azure-data-factory>","2018-07-11 10:41:44","224","1","1","51303548","<p>You could install a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"" rel=""nofollow noreferrer"">selfhosted IR</a> to access your blob storage. Whitelist the IP of the machine hosting your selfhosted IR.</p>
"
"51257746","Azure Data factory: How to update the pipeline parameter (flag) inside the pipeline","<p>I have initialized a pipeline parameter, lets say a flag, with a value ('true'). Now inside the pipeline I want to update the flag value based on some condition.</p>

<p>So lets say I have a web activity inside the pipeline, I want to update the value of that flag to 'false' if i dont get the response from the web activity.</p>

<p>Any idea how to do that?</p>
","<azure-data-factory>","2018-07-10 05:23:55","1084","0","2","51262530","<p>You could try @empty(body('web1'))</p>
"
"51257746","Azure Data factory: How to update the pipeline parameter (flag) inside the pipeline","<p>I have initialized a pipeline parameter, lets say a flag, with a value ('true'). Now inside the pipeline I want to update the flag value based on some condition.</p>

<p>So lets say I have a web activity inside the pipeline, I want to update the value of that flag to 'false' if i dont get the response from the web activity.</p>

<p>Any idea how to do that?</p>
","<azure-data-factory>","2018-07-10 05:23:55","1084","0","2","53466189","<p>So if I understand correctly you want to call a Web activity from your pipeline and if there is no response then set a variable?</p>

<p>In this case, could you not just set a low timeout and use a ""Set Variable"" activity on the Fail condition? So if there was no response it would fail and set the variable</p>
"
"51240111","Azure Data Factory: How to update the condition value in Until activity to exit the loop","<p>I have pipeline parameter, flag, which I am setting as false. And I have put a condition in the Until activity which says that it should loop until the flag turns true. But I am not sure how to update this flag pipeline parameter inside the until activity.</p>

<p>Inside the until activity, I have Web activity and then a stored procedure which is inputing the response of the web activity.</p>

<p>Update: 
Just to give the fuller picture. After GETting the data from the web activity, I am simply upserting it into a table using Stored procedure activity. And then take the 'next-page' link from the response and again GET the data and then upsert. In this way I go till the time I get no 'next-link' in the response. I am basically implementing pagination in the endpoint here.</p>
","<azure-data-factory>","2018-07-09 07:27:02","2895","0","1","61249174","<p>What I do, instead of a parameter I use a variable, which you can set to an init value while declaring it. After reaching your stop condition you can use a ""Set Variable"" activity to set it to a value that you expect your until activity to reach.</p>

<p>Here is the expression I use in the until activity:</p>

<pre><code>@equals(variables('Done'), 'True')
</code></pre>
"
"51222698","Azure Data Factory: For each item() value does not exist for a particular attribute","<p>I have a for each activity which has a stored procedure (SP) wherein I am inputing values using item() evidently.</p>

<p>Now suppose SP's input values are item().a, item().b and item().c</p>

<p>Question: For some of the iteration of foreach, item().b does not exist which is expected. So how should i deal with it in the Stored procedure? Because at this point of time it is giving me an error when it executed SP by saying: </p>

<blockquote>
  <p>""The template language expression 'item().b' cannot be evaluated because property 'b' doesn't
  exist, available properties are 'a, c'</p>
</blockquote>

<p>or how should I overcome this failure in the data factory?</p>

<p>Apparently, data factory has the check for empty() but it does not have the check for exist().</p>
","<azure-data-factory>","2018-07-07 11:23:37","6226","4","2","51230044","<p>I don't think you can solve this in the Data Factory. You could use the String(Item()) to convert it to a Json string in the format:</p>

<pre><code>{
    'a':'value',
    'b':'value',
    'c':'value'
}
</code></pre>

<p>Then you can handle that in your stored procedure with some creative SQL:</p>

<pre><code>DECLARE @jsonParams NVARCHAR(255) = '
    {
        ""a"":""a value"",
        ""c"":""b value""
    }' 


DECLARE @paramA VARCHAR(10) = (SELECT JSON_VALUE(@jsonParams,'$.a'))
DECLARE @paramB VARCHAR(10) = (SELECT JSON_VALUE(@jsonParams,'$.b'))
DECLARE @paramC VARCHAR(10) = (SELECT JSON_VALUE(@jsonParams,'$.c'))
</code></pre>
"
"51222698","Azure Data Factory: For each item() value does not exist for a particular attribute","<p>I have a for each activity which has a stored procedure (SP) wherein I am inputing values using item() evidently.</p>

<p>Now suppose SP's input values are item().a, item().b and item().c</p>

<p>Question: For some of the iteration of foreach, item().b does not exist which is expected. So how should i deal with it in the Stored procedure? Because at this point of time it is giving me an error when it executed SP by saying: </p>

<blockquote>
  <p>""The template language expression 'item().b' cannot be evaluated because property 'b' doesn't
  exist, available properties are 'a, c'</p>
</blockquote>

<p>or how should I overcome this failure in the data factory?</p>

<p>Apparently, data factory has the check for empty() but it does not have the check for exist().</p>
","<azure-data-factory>","2018-07-07 11:23:37","6226","4","2","51237812","<p>You could use “?”. I.e., item()?.b</p>

<p>Please reference <a href=""https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-workflow-definition-language#operators"" rel=""noreferrer"">question mark</a> and a related <a href=""https://social.msdn.microsoft.com/Forums/azure/en-US/c6af1ac9-6c5f-48b8-816e-3294e62c288d/what-does-question-mark-mean?forum=azurelogicapps"" rel=""noreferrer"">post</a>.</p>
"
"51213500","Azure Data Factory: StartDate in Default Parameters","<p>I'm trying to specify a default value for startDate parameter as shown below:</p>

<p><a href=""https://i.stack.imgur.com/JaHAR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JaHAR.png"" alt=""enter image description here""></a></p>

<p>I've been checking how to specify so with functions but so far have I have not find anything.</p>

<p>So far I'm specifying a manual value for startDate, but the idea is to get the current date every time the schedule runs.</p>

<p>I have this defined in a blob storage destination (where it is being used): </p>

<pre><code>@{formatDateTime(pipeline().parameters.windowStart,'yyyy')}@{formatDateTime(pipeline().parameters.windowStart,'MM')}@{formatDateTime(pipeline().parameters.windowStart,'dd')}@{formatDateTime(pipeline().parameters.windowStart,'HH')}@{formatDateTime(pipeline().parameters.windowStart,'mm' 
</code></pre>

<p>Is there a way to replace the fact of calling the parameter and use directly utcnow() ? </p>
","<azure><azure-blob-storage><azure-data-factory>","2018-07-06 15:23:36","3852","2","3","51214127","<p>You can use <code>utcnow()</code> function, 
or if you will define trigger you can use <code>trigger().startTime</code>. 
Other Date functions you can find <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#date-functions"" rel=""nofollow noreferrer"">here</a>.</p>
"
"51213500","Azure Data Factory: StartDate in Default Parameters","<p>I'm trying to specify a default value for startDate parameter as shown below:</p>

<p><a href=""https://i.stack.imgur.com/JaHAR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JaHAR.png"" alt=""enter image description here""></a></p>

<p>I've been checking how to specify so with functions but so far have I have not find anything.</p>

<p>So far I'm specifying a manual value for startDate, but the idea is to get the current date every time the schedule runs.</p>

<p>I have this defined in a blob storage destination (where it is being used): </p>

<pre><code>@{formatDateTime(pipeline().parameters.windowStart,'yyyy')}@{formatDateTime(pipeline().parameters.windowStart,'MM')}@{formatDateTime(pipeline().parameters.windowStart,'dd')}@{formatDateTime(pipeline().parameters.windowStart,'HH')}@{formatDateTime(pipeline().parameters.windowStart,'mm' 
</code></pre>

<p>Is there a way to replace the fact of calling the parameter and use directly utcnow() ? </p>
","<azure><azure-blob-storage><azure-data-factory>","2018-07-06 15:23:36","3852","2","3","51258761","<p>formatDateTime function you use where you want to return a string in specific date format. 
if the format is not imported for you, and you just want the current date, then you can use <code>trigger().startTime</code> or <code>utcnow()</code> in expression field. Don't forget @ sign.</p>

<p>trigger().startTime.utcnow is not valid expression.</p>
"
"51213500","Azure Data Factory: StartDate in Default Parameters","<p>I'm trying to specify a default value for startDate parameter as shown below:</p>

<p><a href=""https://i.stack.imgur.com/JaHAR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JaHAR.png"" alt=""enter image description here""></a></p>

<p>I've been checking how to specify so with functions but so far have I have not find anything.</p>

<p>So far I'm specifying a manual value for startDate, but the idea is to get the current date every time the schedule runs.</p>

<p>I have this defined in a blob storage destination (where it is being used): </p>

<pre><code>@{formatDateTime(pipeline().parameters.windowStart,'yyyy')}@{formatDateTime(pipeline().parameters.windowStart,'MM')}@{formatDateTime(pipeline().parameters.windowStart,'dd')}@{formatDateTime(pipeline().parameters.windowStart,'HH')}@{formatDateTime(pipeline().parameters.windowStart,'mm' 
</code></pre>

<p>Is there a way to replace the fact of calling the parameter and use directly utcnow() ? </p>
","<azure><azure-blob-storage><azure-data-factory>","2018-07-06 15:23:36","3852","2","3","51327092","<p>Finally I was able to fix this by creating the trigger using a JSON code as shown below:</p>

<pre><code>{
    ""name"": ""yourTriggerName"",
    ""properties"": {
        ""runtimeState"": ""Started"",
        ""pipelines"": [
            {
                ""pipelineReference"": {
                    ""referenceName"": ""YourPipelineName"",
                    ""type"": ""PipelineReference""
                },
                ""parameters"": {
                    ""windowStart"": ""@trigger().scheduledTime""
                }
            }
        ],
        ""type"": ""ScheduleTrigger"",
        ""typeProperties"": {
            ""recurrence"": {
                ""frequency"": ""Day"",
                ""interval"": 1,
                ""startTime"": ""2018-07-11T17:00:00Z"",
                ""timeZone"": ""UTC"",
                ""schedule"": {
                    ""minutes"": [
                        20
                    ],
                    ""hours"": [
                        19
                    ]
                }
            }
        }
    }
}
And making emphasis of course, in line below:

""parameters"": {
                        ""windowStart"": ""@trigger().scheduledTime""
</code></pre>

<p>After that, the copy activity started to work as expected.</p>
"
"51210447","Azure Data Factory fails to preview data or import schem from SQL Azure Table","<p>I get an error when trying to create a Dataset in Azure Data Factory V2 against a table in a SQL Azure database.</p>

<p>The table does have a Geography field, otherwise it's a standard table!! Any ideas?</p>

<p>Error reported is:</p>

<p>'Type=,Message=Could not load file or assembly &apos;Microsoft.SqlServer.Types&#44; Version&#61;10.0.0.0&#44; Culture&#61;neutral&#44; PublicKeyToken&#61;89845dcd8080cc91&apos; or one of its dependencies. The system cannot find the file specified.,Source=,'. Activity ID:52dda22e-6328-40cb-b30a-7255117ac37d</p>
","<azure><azure-sql-database><azure-data-factory>","2018-07-06 12:25:20","1563","0","2","51219758","<p>Geography is currently not supported. </p>

<ul>
<li><ol>
<li>You could write a query to exclude this column if you don't need data of this column.</li>
</ol></li>
<li><ol start=""2"">
<li>If you want to copy it to another azure sql or sql server as-is, meaning you don't need specify column mapping (column name between source and sink are well matched), you could skip the preview and schema. </li>
</ol></li>
</ul>
"
"51210447","Azure Data Factory fails to preview data or import schem from SQL Azure Table","<p>I get an error when trying to create a Dataset in Azure Data Factory V2 against a table in a SQL Azure database.</p>

<p>The table does have a Geography field, otherwise it's a standard table!! Any ideas?</p>

<p>Error reported is:</p>

<p>'Type=,Message=Could not load file or assembly &apos;Microsoft.SqlServer.Types&#44; Version&#61;10.0.0.0&#44; Culture&#61;neutral&#44; PublicKeyToken&#61;89845dcd8080cc91&apos; or one of its dependencies. The system cannot find the file specified.,Source=,'. Activity ID:52dda22e-6328-40cb-b30a-7255117ac37d</p>
","<azure><azure-sql-database><azure-data-factory>","2018-07-06 12:25:20","1563","0","2","51367209","<p>I opened a support call with Microsoft and after verifying the issue, they were surprised that Geography fields caused this failure. 
I have been told that they will be working on a fix and await an update of when this will be applied.</p>
"
"51195022","Copying Data from a dynamic URL to SQL DW","<p>I am copying data from a JSON script into my SQLDW. I am using the Copy Activity from DF2 where my datasets are HTTP-Source and  SQLDW-Target</p>

<p>Everything is working fine until I set up a URL parameter for my HTTP LinkedService connection and then I want to feed it with a LookUp Activity.</p>

<p>Is it possible to parameterize LinkedServices?</p>
","<azure-data-factory><sql-data-warehouse>","2018-07-05 15:25:05","45","0","1","51203180","<p>The ‘Linked Service parameterization’ feature is currently under preview.</p>
"
"51168166","Azure Table Storage as Sink in Data Factory Row Key","<p><a href=""https://i.stack.imgur.com/nfvvJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nfvvJ.png"" alt=""enter image description here""></a></p>

<p>I want to pass a value for the row key as a parameter just like the partition key. But the UI only gives me an option to use unique identifier or a source column. Actually I need to use this same entity somewhere else. How will I query this entity if the row key is going to be a random value?</p>
","<azure><azure-table-storage><azure-data-factory><azure-tablequery>","2018-07-04 07:32:01","724","1","1","51170234","<p>Based on the official <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-table-storage#azure-table-as-a-sink-type"" rel=""nofollow noreferrer"">statement</a> , partition key can be set custom value yet row key only could be set column name from source or GUID default value. </p>

<p><a href=""https://i.stack.imgur.com/qeGg9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qeGg9.png"" alt=""enter image description here""></a></p>

<p>I think it's because of guaranteed uniqueness constraint restrictions. So, if you want to control row key value , you could add row key into your source data.</p>

<p>Hope it helps you.</p>
"
"51158909","Azure Data Factory V2 - publish pipeline to specified folder","<p>While developing in Azure Portal, I can organize pipelines, datasets and linked services in folders. JSON file of pipeline will contain folder name:</p>

<pre><code>""folder"": {
  ""name"": ""Business""
}
</code></pre>

<p>But when I publish pipeline (or any other DF V2 object) with PowerShell Cmd-Let, it goes to the root folder:</p>

<pre><code>Set-AzureRmDataFactoryV2Pipeline -DataFactoryName $dataFactoryV2Name -File $filePath `
  -Name $name -ResourceGroupName $resourceGroupName
</code></pre>

<p>Is there a way to force Set-AzureRmDataFactoryV2Pipeline and related Cmd-Lets to honor ""folder"" path and publish object to the specified folder?</p>
","<azure><continuous-deployment><azure-data-factory>","2018-07-03 16:04:35","971","1","2","51190840","<p>Powershell doesn't support it because it ignores properties it can't recognize. And currently, this property is also not supported by SDK. </p>

<p>But you could Restful API to achieve this. And if you have to use powershell, you could use the <a href=""https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.utility/invoke-restmethod?view=powershell-6"" rel=""nofollow noreferrer"">Invoke-RestMethod</a>.</p>
"
"51158909","Azure Data Factory V2 - publish pipeline to specified folder","<p>While developing in Azure Portal, I can organize pipelines, datasets and linked services in folders. JSON file of pipeline will contain folder name:</p>

<pre><code>""folder"": {
  ""name"": ""Business""
}
</code></pre>

<p>But when I publish pipeline (or any other DF V2 object) with PowerShell Cmd-Let, it goes to the root folder:</p>

<pre><code>Set-AzureRmDataFactoryV2Pipeline -DataFactoryName $dataFactoryV2Name -File $filePath `
  -Name $name -ResourceGroupName $resourceGroupName
</code></pre>

<p>Is there a way to force Set-AzureRmDataFactoryV2Pipeline and related Cmd-Lets to honor ""folder"" path and publish object to the specified folder?</p>
","<azure><continuous-deployment><azure-data-factory>","2018-07-03 16:04:35","971","1","2","52843686","<p>Finally Microsoft fixed the issue and in version 6.10.0 Set-AzureRmDataFactoryV2Pipeline properly puts pipeline into the specified folder!</p>
"
"51153512","(Azure) Data Factory to Data warehouse - Dynamically name the landing tables and schemas","<p>I plan to move data from a number of databases periodically using Azure Data Factory (ADF) and i want to move the data into Azure Parallel Data-Warehouse (APDW). However the 'destination' step in the ADF wizard offers me 2 functions; 1- in the case where data is retrieved from a view you are expected to map the columns to an existing table, and 2- when the data comes from a table you are expected to generate a table object in the APDW.</p>

<p>Realistically this is too expensive to maintain and it is possible to erroneously map source data to a landing zone.</p>

<p><strong>What i would like to achieve is an algorithmic approach using variables to name schemas, customer codes and tables.</strong>  </p>

<p>After the source data has landed i will be transforming it using our SSIS Integration Runtime. <strong>I am wondering also whether a SSIS package could request source data instead of an ADF pipeline.</strong></p>

<ol>
<li><em>Are there any resources about connecting to on premises IRs through SSIS objects?</em></li>
<li><em>Can the JSON of an ADF be modified to dynamically generate a schema for each data source?</em></li>
</ol>
","<ssis><azure-data-factory><parallel-data-warehouse>","2018-07-03 11:30:05","230","0","1","51158082","<p>For your question #2, Can the JSON of an ADF be modified to dynamically generate a schema for each data source:</p>

<p>You could put your generate table script in <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse#azure-sql-data-warehouse-as-sink"" rel=""nofollow noreferrer"">precopyscript</a>. </p>
"
"51152741","Query from Azure Comos DB and save to Azure Table Storage using Data Factory","<p>I want to save C._ts+C.ttl as <strong>one entity</strong> in my Azure Table Storage. I do the following query in my Copy Activity:</p>

<pre><code>""typeProperties"": {
        ""source"": {
            ""type"": ""DocumentDbCollectionSource"",
            ""query"": {
                ""value"": ""@concat('SELECT (C.ts+C.ttl) FROM C WHERE (C.ttl+C._ts)&lt;= ', string(pipeline().parameters.bufferdays))"",
                ""type"": ""Expression""
            },
            ""nestingSeparator"": "".""
        },
</code></pre>

<p>I <strong>dont</strong> want to copy all the fields from my source i.e. CosmosDB to my sink i.e. Table Storage. I just want to store the result of this query as <strong>one value</strong>. How can I do that?</p>
","<azure><azure-table-storage><azure-data-factory>","2018-07-03 10:51:05","78","0","1","51302095","<p>According to my test, I presume the <code>null</code> value you queried because of that the collection level <code>ttl</code> affects each document , but will not generate <code>ttl</code> property within document.</p>

<p>So when you execute <code>SELECT c.ttl,c._ts FROM c</code> , just get below result.</p>

<p><a href=""https://i.stack.imgur.com/Vftdn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Vftdn.png"" alt=""enter image description here""></a></p>

<p>Document level <code>ttl</code> is not defined, just follow collection level <code>ttl</code>.</p>

<p><a href=""https://i.stack.imgur.com/chjgV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/chjgV.png"" alt=""enter image description here""></a></p>

<p>You need to bulk add ttl property into per document so that you could transfer <code>_ts+ttl</code> caculator results.</p>

<p>Your Copy Activity settings looks good , just add an alias in SQL, or set the name of the field via column mapping.</p>

<p>Hope it helps you.</p>
"
"51150626","Azure Data Factory v2: fail to create copy activity via c# code","<p>The issue that my inner copy activity from foreach activity isn't published to ADF. I don't see that activity in ADF UI and even after I add it manually I not able not debug, publish and even view activity code =(</p>

<p>When I click on publish I see error 'Publishing: Got error while publishing'
When I'm trying to debug: </p>

<pre><code>Failed
{""__zone_symbol__currentTask"":{""type"":""microTask"",""state"":""notScheduled"",""source"":""Promise.then"",""zone"":""angular"",""cancelFn"":null,""runCount"":0}}
</code></pre>

<p>Clicking on 'Code' button doesn't do any actions.</p>

<p>Anything I missing? Should I add copy activity somewhere else?</p>

<p>Foreach/copy activity in c# code is following :</p>

<pre><code>private Activity CreateDocumentIteratorActivity(
        string lookupDocumentsActivityName,
        string sourceDatasetName,
        string destinationDatasetName,
        string logsServiceName)
    {
        return new ForEachActivity()
        {
            Name = ""ForEachDocument"",
            IsSequential = false,
            BatchCount = BackupSettings.Value.ParallelThreads,
            Items = new Expression { Value = ""@activity('"" + lookupDocumentsActivityName + ""').output.value"" },
            DependsOn = new List&lt;ActivityDependency&gt;() {
                new ActivityDependency() {
                    Activity = lookupDocumentsActivityName,
                    DependencyConditions = new List&lt;string&gt;() {
                        DependencyCondition.Succeeded
                    }
                }
            },
            Activities = new[] {
                CreateCopyDocToFileActivity(sourceDatasetName, destinationDatasetName, logsServiceName, BackupSettings.Value.BackupLogsFolder)
            }
        };
    }

private CopyActivity CreateCopyDocToFileActivity(string sourceDatasetName, string destinationDatasetName, string logsServiceName, string logsPath)
    {
        return new CopyActivity()
        {
            Name = ""CopyDocToFile"",
            Policy = new ActivityPolicy()
            {
                Retry = BackupSettings.Value.RetryPolicyConfiguration.RetryCount,
                RetryIntervalInSeconds = (int)BackupSettings.Value.RetryPolicyConfiguration.DeltaBackOff.TotalSeconds,
                Timeout = BackupSettings.Value.RetryPolicyConfiguration.MaxBackOff
            },
            Source = new DocumentDbCollectionSource()
            {
                Query = new Expression(@""select value c from c where c.id = '@{item().id}'"")
            },
            Inputs = new[] { new DatasetReference() {
                ReferenceName = sourceDatasetName,
                Parameters = new Dictionary&lt;string, object&gt;
                {
                    { ""collectionName"", new Expression(""@pipeline().parameters.collectionName"") }
                }
            }},
            Outputs = new[] { new DatasetReference() {
                ReferenceName = destinationDatasetName,
                Parameters = new Dictionary&lt;string, object&gt; {
                     { ""fileName"", new Expression(""@concat(pipeline().parameters.collectionName,  '/', item().PartitionKey, '/', item().id)"") },
                     { ""backupDateStr"", new Expression(""@pipeline().TriggerTime"") }
                }
            }},
            Sink = new BlobSink()
            {
                CopyBehavior = CopyBehaviorType.PreserveHierarchy,
            },

            ParallelCopies = BackupSettings.Value.ParallelThreads,
            EnableSkipIncompatibleRow = true,
            RedirectIncompatibleRowSettings = new RedirectIncompatibleRowSettings()
            {
                LinkedServiceName = logsServiceName,
                Path = logsPath
            }
        };
    }
</code></pre>

<p>Response from /pipelines api call:</p>

<pre><code>{
""value"": [
{
    ""id"": ""/subscriptions/d2259601-012b-4253-895b-02916ef0f7f7/resourceGroups/Test-Data/providers/Microsoft.DataFactory/factories/backup-data-factory/pipelines/cosmosBackup"",
    ""name"": ""cosmosBackup"",
    ""type"": ""Microsoft.DataFactory/factories/pipelines"",
    ""properties"":
    {
        ""activities"": [
        {
            ""type"": ""Lookup"",
            ""typeProperties"":
            {
                ""source"":
                {
                    ""type"": ""DocumentDbCollectionSource"",
                    ""query"":
                    {
                        ""value"": ""select root.id, root.PartitionKey from root"",
                        ""type"": ""Expression""
                    }
                },
                ""dataset"":
                {
                    ""referenceName"": ""source_cosmosdb_collection"",
                    ""parameters"":
                    {
                        ""collectionName"": ""@pipeline().parameters.collectionName""
                    },
                    ""type"": ""DatasetReference""
                },
                ""firstRowOnly"": false
            },
            ""policy"":
            {
                ""timeout"": ""02:00:00"",
                ""retry"": 3,
                ""retryIntervalInSeconds"": 30
            },
            ""name"": ""GetDocumentsIds""
        },
        {
            ""type"": ""ForEach"",
            ""typeProperties"":
            {
                ""isSequential"": false,
                ""batchCount"": 4,
                ""items"":
                {
                    ""value"": ""@activity('GetDocumentsIds').output.value"",
                    ""type"": ""Expression""
                },
                ""activities"": [
                {
                    ""type"": ""Copy"",
                    ""typeProperties"":
                    {
                        ""source"":
                        {
                            ""type"": ""DocumentDbCollectionSource"",
                            ""query"":
                            {
                                ""value"": ""select value c from c where c.id = '@{item().id}'"",
                                ""type"": ""Expression""
                            }
                        },
                        ""sink"":
                        {
                            ""type"": ""BlobSink"",
                            ""copyBehavior"": ""PreserveHierarchy""
                        },
                        ""parallelCopies"": 4,
                        ""enableSkipIncompatibleRow"": true,
                        ""redirectIncompatibleRowSettings"":
                        {
                            ""linkedServiceName"": ""destination_cosmosdb_collection_service"",
                            ""path"": ""backup/logs""
                        }
                    },
                    ""inputs"": [
                    {
                        ""referenceName"": ""source_cosmosdb_collection"",
                        ""parameters"":
                        {
                            ""collectionName"":
                            {
                                ""value"": ""@pipeline().parameters.collectionName"",
                                ""type"": ""Expression""
                            }
                        },
                        ""type"": ""DatasetReference""
                    }],
                    ""outputs"": [
                    {
                        ""referenceName"": ""destination_cosmosdb_collection"",
                        ""parameters"":
                        {
                            ""fileName"":
                            {
                                ""value"": ""@concat(pipeline().parameters.collectionName,  '/', item().PartitionKey, '/', item().id)"",
                                ""type"": ""Expression""
                            },
                            ""backupDateStr"":
                            {
                                ""value"": ""@pipeline().TriggerTime"",
                                ""type"": ""Expression""
                            }
                        },
                        ""type"": ""DatasetReference""
                    }],
                    ""policy"":
                    {
                        ""timeout"": ""02:00:00"",
                        ""retry"": 3,
                        ""retryIntervalInSeconds"": 30
                    },
                    ""name"": ""CopyDocToFile""
                }]
            },
            ""name"": ""ForEachDocument"",
            ""dependsOn"": [
            {
                ""activity"": ""GetDocumentsIds"",
                ""dependencyConditions"": [""Succeeded""]
            }]
        }],
        ""parameters"":
        {
            ""collectionName"":
            {
                ""type"": ""String""
            }
        }
    },
    ""etag"": ""01006ed4-0000-0000-0000-5b3b38450000""
}]
}
</code></pre>
","<c#><azure><azure-data-factory>","2018-07-03 09:05:42","383","0","1","51157645","<p>Found out the issue. This part is not right.</p>

<pre><code> ""enableSkipIncompatibleRow"": true,
  ""redirectIncompatibleRowSettings"": {
      ""linkedServiceName"": ""destination_cosmosdb_collection_service"",
          ""path"": ""backup/logs""
}
</code></pre>

<p>It shoud be</p>

<pre><code> ""enableSkipIncompatibleRow"": true,
   ""redirectIncompatibleRowSettings"": {
                 ""linkedServiceName"": {
                   ""referenceName"": ""destination_cosmosdb_collection_service"",
                    ""type"": ""LinkedServiceReference""
         },
   ""path"": ""backup/logs""
                            },
</code></pre>
"
"51149857","Azure Data Factory V2: Copy OR Stored Procedure activity for SQL merge","<p>We have number of DB table merge steps in our Azure Data Factory v2 solution. We merge tables in a single instance of Azure SQL Server DB. Source tables and target tables are in different DB schemas. Sources are defined either as a select over single table or as a join of two tables.</p>

<p>My doubt is which one of scenarios described bellow is better from the performance perspective.</p>

<h2>Scenario one (per table)</h2>

<p>Stored Procedure activity invokes a stored procedure that performs all the work.
A Stored Procedure activity in a pipeline invokes that stored procedure. The upserts the target table with all source data. An example of such a stored procedure:</p>

<pre><code>create or alter procedure dwh.fill_lnk_cemvypdet_cemstr2c_table_with_stage_data as
    merge
        dwh.lnk_cemvypdet_cemstr2c as target
    using

        (select
                t.sa_hashkey cemvypdet_hashkey,
                t.sa_timestamp load_date,
                t.sa_source record_source,
                d.sa_hashkey cemstr2c_hashkey
            from
                egje.cemvypdet t
            join
                egje.cemstr2c d
            on
                t.id_mstr = d.id_mstr)
        as source
        on target.cemvypdet_hashkey = source.cemvypdet_hashkey
            and target.cemstr2c_hashkey = source.cemstr2c_hashkey
        when not matched then
            insert(
                cemvypdet_hashkey,
                cemstr2c_hashkey,
                record_source,
                load_date,
                last_seen_date)
            values(
                source.cemvypdet_hashkey,
                source.cemstr2c_hashkey,
                source.record_source,
                source.load_date,
                source.load_date)
        when matched then
            update set last_seen_date = source.load_date;
</code></pre>

<h2>Scenario two (per row)</h2>

<p>A Copy activity declares a stored procedure to invoke in Target tab so that the activity invokes the stored procedure for every single row of the source.</p>

<pre><code>create or alter procedure dwh.fill_lnk_cemvypdet_cemstr2c_subset_table_row_with_stage_data
@lnk_cemvypdet_cemstr2c_subset dwh.lnk_cemvypdet_cemstr2c_subset_type readonly
as
    merge
        dwh.lnk_cemvypdet_cemstr2c_subset as target
    using

    @lnk_cemvypdet_cemstr2c_subset
        as source
        on target.cemvypdet_hashkey = source.cemvypdet_hashkey
            and target.cemstr2c_hashkey = source.cemstr2c_hashkey
        when not matched then
            insert(
                hashkey,
                cemvypdet_hashkey,
                cemstr2c_hashkey,
                record_source,
                load_date,
                last_seen_date)
            values(
                source.hashkey,
                source.cemvypdet_hashkey,
                source.cemstr2c_hashkey,
                source.record_source,
                source.load_date,
                source.load_date)
        when matched then
            update set last_seen_date = source.load_date;
</code></pre>

<p>The type @lnk_cemvypdet_cemstr2c_subset is defined as a table type that follows structure of the target table.</p>
","<azure><stored-procedures><azure-sql-database><azure-data-factory>","2018-07-03 08:24:01","1415","1","1","51155623","<p>Scenario 1 should have better performance but taking the following optimizations in consideration:</p>

<ol>
<li>Create an index on the join columns in the source table that is unique and covering.</li>
<li>Create a unique clustered index on the join columns in the target table.</li>
<li>Parameterize all literal values in the ON clause and in the the WHEN clauses.</li>
<li>Merge subsets of data from the source to the target table by using OFFSET and ROWS FETCH NEXT or by defining views on the source or target that return the filtered rows and reference the view as the source or target table. Furthermore the use of the WITH  clause of the TOP clause to filter out rows from the source or target tables is not recommended because they can generate incorrect results.</li>
<li>To further optimize the merge operation try different batch sizes. <a href=""https://learn.microsoft.com/en-us/azure/sql-database/sql-database-use-batching-to-improve-performance"" rel=""nofollow noreferrer"">Here</a> is the reason.</li>
</ol>
"
"51148257","Azure data factory disaster recovery","<p>How does Azure Data Factory (ADFv2) handle disaster recovery (DR) in the integration runtime? Will it automatically create another integration runtime? Do we have to setup our own DR ""Azure-SSIS Integration Runtime""</p>
","<azure-data-factory>","2018-07-03 06:52:37","6472","1","2","51157374","<p>As far as I know, there is no disaster recovery feature available with Integration runtime. If service stops due to any error, you will have to manually restart the service. </p>

<p>You should ideally setup multiple nodes for Integration Runtime. Link below points to section on High Availability and Scalability, with details on setting up multiple nodes (upto 4). </p>

<p>This avoids having a single point of failure and provides higher throughput, as all nodes are setup as active. </p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#high-availability-and-scalability"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#high-availability-and-scalability</a></p>
"
"51148257","Azure data factory disaster recovery","<p>How does Azure Data Factory (ADFv2) handle disaster recovery (DR) in the integration runtime? Will it automatically create another integration runtime? Do we have to setup our own DR ""Azure-SSIS Integration Runtime""</p>
","<azure-data-factory>","2018-07-03 06:52:37","6472","1","2","51164868","<p>ADF V2 support stopping the SSIS integration runtime in the current region and switch to another region (<a href=""https://learn.microsoft.com/en-us/azure/best-practices-availability-paired-regions"" rel=""nofollow noreferrer"">Paired-region</a> is recommended) to start it again. </p>

<p>To support that,</p>

<ul>
<li>You need to make sure that you have enabled your azure SQL server DR in case the azure sql server has outage at the same time. </li>
<li>You may need to prepare another vNet that could be connected to your azure sql server if you have are using VNet.</li>
<li>You may need to prepare your customer script SAS Uri which could be accessed during outage if you are using custom script.</li>
</ul>

<p>Then you can follow these steps to switch your IR to the new region and start it again.</p>

<ul>
<li>Stop the IR in the original region.</li>
<li><p>Call following command in PowerShell to update the integration runtime</p>

<p><Code>Set-AzureRmDataFactoryV2IntegrationRuntime -Location ""new region"" `
                                       -CatalogServerEndpoint ""SQL Server endpoint"" `
                                       -CatalogAdminCredential ""credential"" `
                                       -VNetId ""new VNet"" `
                                       -Subnet ""new subnet"" `
                                       -SetupScriptContainerSasUri ""new script SAS Uri""</Code></p></li>
<li><p>Start the IR again.</p></li>
</ul>

<p>About how to use PowerShell command, please refer to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-azure-ssis-integration-runtime"" rel=""nofollow noreferrer"">Create the Azure-SSIS integration runtime in Azure Data Factory</a> for more details.</p>
"
"51142783","Using ADF Copy Activity with dynamic schema mapping","<p>I'm trying to drive the columnMapping property from a database configuration table. My first activity in the pipeline pulls in the rows from the config table. My copy activity source is a Json file in Azure blob storage and my sink is an Azure SQL database.</p>

<p>In copy activity I'm setting the mapping using the dynamic content window. The code looks like this:</p>

<pre><code>""translator"": {
    ""value"": ""@json(activity('Lookup1').output.value[0].ColumnMapping)"",
     ""type"": ""Expression""
 }
</code></pre>

<p>My question is, what should the value of activity('Lookup1').output.value[0].ColumnMapping look like?</p>

<p>I've tried several different json formats but the copy activity always seems to ignore it.</p>

<p>For example, I've tried:</p>

<pre><code>{
    ""type"": ""TabularTranslator"",
    ""columnMappings"": {
      ""view.url"": ""url""
    }
}
</code></pre>

<p>and: </p>

<pre><code>""columnMappings"": {
    ""view.url"": ""url""
}
</code></pre>

<p>and:</p>

<pre><code>{
  ""view.url"": ""url""
}
</code></pre>

<p>In this example, view.url is the name of the column in the JSON source, and url is the name of the column in my destination table in Azure SQL database.</p>
","<azure-data-factory>","2018-07-02 19:41:50","13309","3","3","51145793","<p>The issue is due to the dot (.) sign in your column name. </p>

<ol>
<li>To use column mapping, you should also specify structure in your source and sink dataset.</li>
<li>For your source dataset, you need specify your format correctly. And since your column name has dot, you need specify the json path as following.
<a href=""https://i.stack.imgur.com/DxnPa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DxnPa.png"" alt=""enter image description here""></a></li>
<li>You could use ADF UI to setup a copy for a single file first to get the related format, structure and column mapping format. Then change it to lookup.</li>
</ol>

<p>And as my understanding, your first format should be the right format. If it is already in json format, then you may not need use ""json"" function in your expression.</p>
"
"51142783","Using ADF Copy Activity with dynamic schema mapping","<p>I'm trying to drive the columnMapping property from a database configuration table. My first activity in the pipeline pulls in the rows from the config table. My copy activity source is a Json file in Azure blob storage and my sink is an Azure SQL database.</p>

<p>In copy activity I'm setting the mapping using the dynamic content window. The code looks like this:</p>

<pre><code>""translator"": {
    ""value"": ""@json(activity('Lookup1').output.value[0].ColumnMapping)"",
     ""type"": ""Expression""
 }
</code></pre>

<p>My question is, what should the value of activity('Lookup1').output.value[0].ColumnMapping look like?</p>

<p>I've tried several different json formats but the copy activity always seems to ignore it.</p>

<p>For example, I've tried:</p>

<pre><code>{
    ""type"": ""TabularTranslator"",
    ""columnMappings"": {
      ""view.url"": ""url""
    }
}
</code></pre>

<p>and: </p>

<pre><code>""columnMappings"": {
    ""view.url"": ""url""
}
</code></pre>

<p>and:</p>

<pre><code>{
  ""view.url"": ""url""
}
</code></pre>

<p>In this example, view.url is the name of the column in the JSON source, and url is the name of the column in my destination table in Azure SQL database.</p>
","<azure-data-factory>","2018-07-02 19:41:50","13309","3","3","57794683","<p>There seems to be a disconnect between the question and the answer, so I'll hopefully provide a more straightforward answer.</p>

<p>When setting this up, you should have a source dataset with dynamic mapping. The sink doesn't require one, as we're going to specify it in the mapping.</p>

<p>Within the copy activity, format the dynamic json like the following:</p>

<pre><code>    {
      ""structure"": [
        {
          ""name"": ""Address Number""
        },
        {
          ""name"": ""Payment ID""
        },
        {
          ""name"": ""Document Number""
        },
          ...
          ...
      ]
    }
</code></pre>

<p>You would then specify your dynamic mapping like this:</p>

<pre><code>    {
      ""translator"": {
        ""type"": ""TabularTranslator"",
        ""mappings"": [
          {
            ""source"": {
              ""name"": ""Address Number"",
              ""type"": ""Int32""
            },
            ""sink"": {
              ""name"": ""address_number""
            }
          },
          {
            ""source"": {
              ""name"": ""Payment ID"",
              ""type"": ""Int64""
            },
            ""sink"": {
              ""name"": ""payment_id""
            }
          },
          {
            ""source"": {
              ""name"": ""Document Number"",
              ""type"": ""Int32""
            },
            ""sink"": {
              ""name"": ""document_number""
            }
          },
          ...
          ...
        ]
      }
    }
</code></pre>

<p>Assuming these were set in separate variables, you would want to send the source as a string, and the mapping as json:</p>

<p>source: <code>@string(json(variables('str_dyn_structure')).structure)</code></p>

<p>mapping: <code>@json(variables('str_dyn_translator')).translator</code></p>
"
"51142783","Using ADF Copy Activity with dynamic schema mapping","<p>I'm trying to drive the columnMapping property from a database configuration table. My first activity in the pipeline pulls in the rows from the config table. My copy activity source is a Json file in Azure blob storage and my sink is an Azure SQL database.</p>

<p>In copy activity I'm setting the mapping using the dynamic content window. The code looks like this:</p>

<pre><code>""translator"": {
    ""value"": ""@json(activity('Lookup1').output.value[0].ColumnMapping)"",
     ""type"": ""Expression""
 }
</code></pre>

<p>My question is, what should the value of activity('Lookup1').output.value[0].ColumnMapping look like?</p>

<p>I've tried several different json formats but the copy activity always seems to ignore it.</p>

<p>For example, I've tried:</p>

<pre><code>{
    ""type"": ""TabularTranslator"",
    ""columnMappings"": {
      ""view.url"": ""url""
    }
}
</code></pre>

<p>and: </p>

<pre><code>""columnMappings"": {
    ""view.url"": ""url""
}
</code></pre>

<p>and:</p>

<pre><code>{
  ""view.url"": ""url""
}
</code></pre>

<p>In this example, view.url is the name of the column in the JSON source, and url is the name of the column in my destination table in Azure SQL database.</p>
","<azure-data-factory>","2018-07-02 19:41:50","13309","3","3","57858716","<p>VladDrak - You could skip the source dynamic definition by building dynamic mapping like this:</p>

<pre><code>{
  ""translator"": {
    ""type"": ""TabularTranslator"",
    ""mappings"": [
      {
        ""source"": {
          ""type"": ""String"",
          ""ordinal"": ""1""
        },
        ""sink"": {
          ""name"": ""dateOfActivity"",
          ""type"": ""String""
        }
      },
      {
        ""source"": {
          ""type"": ""String"",
          ""ordinal"": ""2""
        },
        ""sink"": {
          ""name"": ""CampaignID"",
          ""type"": ""String""
        }
      }
    ]
  }
}
</code></pre>
"
"51134475","Azure Data Factory: event not starting pipeline","<p>I've set up a Azure Data Factory pipeline containing a copy activity. For testing purposes both source and sink are Azure Blob Storages.</p>

<p>I wan't to execute the pipeline as soon as a new file is created on the source Azure Blob Storage.</p>

<p>I've created a trigger of type BlovEventsTrigger. Blob path begins with has been set to //</p>

<p>I use Cloud Storage Explorer to upload files but it doesn't trigger my pipeline. To get an idea of what is wrong, how can I check if the event is fired? Any idea what could be wrong?</p>

<p>Thanks</p>
","<azure><azure-data-factory>","2018-07-02 10:51:28","5966","3","6","51137361","<p>Please reference this. First, it needs to be a v2 storage. Second, you need register it with event grid.
<a href=""https://social.msdn.microsoft.com/Forums/azure/en-US/db332ac9-2753-4a14-be5f-d23d60ff2164/azure-data-factorys-event-trigger-for-pipeline-not-working-for-blob-creation-deletion-most-of-the?forum=AzureDataFactory"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/azure/en-US/db332ac9-2753-4a14-be5f-d23d60ff2164/azure-data-factorys-event-trigger-for-pipeline-not-working-for-blob-creation-deletion-most-of-the?forum=AzureDataFactory</a></p>
"
"51134475","Azure Data Factory: event not starting pipeline","<p>I've set up a Azure Data Factory pipeline containing a copy activity. For testing purposes both source and sink are Azure Blob Storages.</p>

<p>I wan't to execute the pipeline as soon as a new file is created on the source Azure Blob Storage.</p>

<p>I've created a trigger of type BlovEventsTrigger. Blob path begins with has been set to //</p>

<p>I use Cloud Storage Explorer to upload files but it doesn't trigger my pipeline. To get an idea of what is wrong, how can I check if the event is fired? Any idea what could be wrong?</p>

<p>Thanks</p>
","<azure><azure-data-factory>","2018-07-02 10:51:28","5966","3","6","51160388","<p>Reiterating what others have stated:</p>

<ul>
<li>Must be using a V2 Storage Account</li>
<li>Trigger name must only contain letters, numbers and the '-' character (this restriction will soon be removed)</li>
<li>Must have registered subscription with Event Grid resource provider (this will be done for you via the UX soon)</li>
<li>Trigger makes the following properties available <code>@triggerBody().folderPath</code> and <code>@triggerBody().fileName</code>. To use these in your pipeline your must map them to pipeline paramaters and use them as such: <code>@pipeline().parameters.paramaetername</code>.</li>
</ul>

<p>Finally, based on your configuration setting blob path begins with to <code>//</code> will not match any blob event. The UX will actually show you an error message saying that that value is not valid. Please refer to the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger#examples-of-event-based-triggers"" rel=""nofollow noreferrer"">Event Based Trigger</a> documentation for examples of valid configuration.</p>
"
"51134475","Azure Data Factory: event not starting pipeline","<p>I've set up a Azure Data Factory pipeline containing a copy activity. For testing purposes both source and sink are Azure Blob Storages.</p>

<p>I wan't to execute the pipeline as soon as a new file is created on the source Azure Blob Storage.</p>

<p>I've created a trigger of type BlovEventsTrigger. Blob path begins with has been set to //</p>

<p>I use Cloud Storage Explorer to upload files but it doesn't trigger my pipeline. To get an idea of what is wrong, how can I check if the event is fired? Any idea what could be wrong?</p>

<p>Thanks</p>
","<azure><azure-data-factory>","2018-07-02 10:51:28","5966","3","6","51645811","<p>There seems to be a bug with Blob storage trigger, if you have more than one trigger is allocated to the same blob container, none of the triggers will fire.</p>

<p>For some reasons (another bug, but this time in Data factories?), if you edit several times your trigger in the data factory windows, the data factory seems to loose track of the triggers it creates, and your single trigger may end up creating multiple duplicate triggers on the blob storage. This condition activates the first bug discussed above: the blob storage trigger doesn't trigger anymore. </p>

<p>To fix this, delete the duplicate triggers. For that, navigate to your blob storage resource in the Azure portal. Go to the Events blade. From there you'll see all the triggers that the data factories added to your blob storage. Delete the duplicates.
<a href=""https://i.stack.imgur.com/U8b7F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/U8b7F.png"" alt=""enter image description here""></a></p>
"
"51134475","Azure Data Factory: event not starting pipeline","<p>I've set up a Azure Data Factory pipeline containing a copy activity. For testing purposes both source and sink are Azure Blob Storages.</p>

<p>I wan't to execute the pipeline as soon as a new file is created on the source Azure Blob Storage.</p>

<p>I've created a trigger of type BlovEventsTrigger. Blob path begins with has been set to //</p>

<p>I use Cloud Storage Explorer to upload files but it doesn't trigger my pipeline. To get an idea of what is wrong, how can I check if the event is fired? Any idea what could be wrong?</p>

<p>Thanks</p>
","<azure><azure-data-factory>","2018-07-02 10:51:28","5966","3","6","59793181","<p>If you're creating your trigger via arm template, make sure you're aware of <a href=""https://social.msdn.microsoft.com/Forums/azure/en-US/01246c8f-b1ca-4d39-87b0-9a66742f2f89/unable-to-activate-triggers-in-data-factory-as-part-of-cicd-deployment?forum=AzureDataFactory"" rel=""nofollow noreferrer"">this bug</a>.  The ""runtimeState"" (aka ""Activated"") property of the trigger can only be set as ""Stopped"" via arm template.  The trigger will need to be activated via powershell or the ADF portal.</p>
"
"51134475","Azure Data Factory: event not starting pipeline","<p>I've set up a Azure Data Factory pipeline containing a copy activity. For testing purposes both source and sink are Azure Blob Storages.</p>

<p>I wan't to execute the pipeline as soon as a new file is created on the source Azure Blob Storage.</p>

<p>I've created a trigger of type BlovEventsTrigger. Blob path begins with has been set to //</p>

<p>I use Cloud Storage Explorer to upload files but it doesn't trigger my pipeline. To get an idea of what is wrong, how can I check if the event is fired? Any idea what could be wrong?</p>

<p>Thanks</p>
","<azure><azure-data-factory>","2018-07-02 10:51:28","5966","3","6","68058028","<p>And now, on 20.06.2021, same for me: event trigger is not working, though when editing it's definition in DF, it shows all my files in folder, that matches. But when i add new file to that folder, nothing happens!</p>
"
"51134475","Azure Data Factory: event not starting pipeline","<p>I've set up a Azure Data Factory pipeline containing a copy activity. For testing purposes both source and sink are Azure Blob Storages.</p>

<p>I wan't to execute the pipeline as soon as a new file is created on the source Azure Blob Storage.</p>

<p>I've created a trigger of type BlovEventsTrigger. Blob path begins with has been set to //</p>

<p>I use Cloud Storage Explorer to upload files but it doesn't trigger my pipeline. To get an idea of what is wrong, how can I check if the event is fired? Any idea what could be wrong?</p>

<p>Thanks</p>
","<azure><azure-data-factory>","2018-07-02 10:51:28","5966","3","6","70626350","<p>An event grid resource provider needs to have been registered, within the specific azure subscription.</p>
<p>Also if you use Synapse Studio pipelines instead of Data Factory (like me) make sure the Data Factory resource provider is also registered.</p>
<p>Finally, the user should have both 'owner' and 'storage blob data contributor' on the storage account.</p>
"
"51131786","Data Factory v2 copy from FTP strange fails","<p>I am trying to binary copy a few .ZIP files sequentially from FTP to ADLS. Sometimes its failed, sometimes not, it's really strange for me. I got this type of error only working with this external FTP server. 
Error type: </p>

<blockquote>
  <p>{
      ""errorCode"": ""2200"",
      ""message"": ""Failure happened on 'Sink' side. ErrorCode=UserErrorFailedToReadFtpData,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Failed to read data from ftp: The remote server returned an error: (530) Not logged in.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Net.WebException,Message=The remote server returned an error: (530) Not logged in.,Source=System,'"",
      ""failureType"": ""UserError"",
      ""target"": ""Copy from FTP""
  }</p>
</blockquote>

<p>A connection is good, as I said sometimes it copy files without any errors, this is a simple activity so I don't know what can cause this type of error. 
Sometimes it throws an error after copying 50mb on adls. 
Can it be related to the FTP server? </p>
","<azure-data-factory>","2018-07-02 08:20:25","1305","0","2","51137663","<p>A possible root cause could be :
Your FTP server does not support SSL but you enabled SSL in the FTP linked service. If so, You can disable the SSL in FTP linked service. Check out the FTP properties here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-factory-ftp-connector"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-factory-ftp-connector</a></p>
"
"51131786","Data Factory v2 copy from FTP strange fails","<p>I am trying to binary copy a few .ZIP files sequentially from FTP to ADLS. Sometimes its failed, sometimes not, it's really strange for me. I got this type of error only working with this external FTP server. 
Error type: </p>

<blockquote>
  <p>{
      ""errorCode"": ""2200"",
      ""message"": ""Failure happened on 'Sink' side. ErrorCode=UserErrorFailedToReadFtpData,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Failed to read data from ftp: The remote server returned an error: (530) Not logged in.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Net.WebException,Message=The remote server returned an error: (530) Not logged in.,Source=System,'"",
      ""failureType"": ""UserError"",
      ""target"": ""Copy from FTP""
  }</p>
</blockquote>

<p>A connection is good, as I said sometimes it copy files without any errors, this is a simple activity so I don't know what can cause this type of error. 
Sometimes it throws an error after copying 50mb on adls. 
Can it be related to the FTP server? </p>
","<azure-data-factory>","2018-07-02 08:20:25","1305","0","2","51146459","<p>From telemetry, it shows Copy can sometimes pass or fail with same payload, so it looks like a transient failure. But it is hard to determine the RCA from error message (""530 Not logged in""). What I'm suspecting is Copy hit throttling or similar transient issue from FTP server which will block the read request in the middle. </p>

<p>For further troubleshoot, could you check from FTP server side to see whether there's any detailed failure log. Besides, it will be a great help if I can get a test account to test the FTP server behavior and try to repro the issue. Please let me know if it is possible for you.</p>

<p>Regards,
Gary</p>
"
"51129025","HTTP dataset credential error","<p>In Azure Data Factory V2, I created a new http dataset an added a url and basic login details to connect to a https endpoint, when use the the Test Connection the response is ""Connection successful""</p>

<p>When i try to ""Preview data"" it fails, the message from more is</p>

<p>The credential to read http file is invalid. Activity ID:89ae4de1-e6be-46fd-abb9-39360fe5323b.</p>

<p>How do i find out more about this error?</p>

<p>When I try the same url and basic login details in Postman I get back the expected results.</p>

<p>thanks</p>
","<azure-data-factory>","2018-07-02 04:10:57","1492","1","2","51129777","<p>In Azure data factory, if basic auth type is specified, the http connector honor basic auth protocol, that means：
* It would send out a request without any credential first
* when the http server return 401 response with correct WWW-Authenticate header, it will continue enclose credential into next http request
So if your http server can't handle the request, you will get unauthorize error.</p>

<p>Postman can do it because postman ignored the first http request and send out the second one directly.</p>
"
"51129025","HTTP dataset credential error","<p>In Azure Data Factory V2, I created a new http dataset an added a url and basic login details to connect to a https endpoint, when use the the Test Connection the response is ""Connection successful""</p>

<p>When i try to ""Preview data"" it fails, the message from more is</p>

<p>The credential to read http file is invalid. Activity ID:89ae4de1-e6be-46fd-abb9-39360fe5323b.</p>

<p>How do i find out more about this error?</p>

<p>When I try the same url and basic login details in Postman I get back the expected results.</p>

<p>thanks</p>
","<azure-data-factory>","2018-07-02 04:10:57","1492","1","2","52818957","<p>you can manually add header to your request:
Authorization: Basic ......</p>

<p>If you don't know how to generate it, you can copy it from postman, after passing credentials.
Best,
Pawel</p>
"
"51127506","How to run SQL Script in Azure Data Factory v2?","<p>There is NO Sql Script activity in Azure Data Factory V2. So how can I create a stored proc, a schema in a database? What are my options?</p>
","<azure><azure-data-factory>","2018-07-01 22:44:43","30314","20","4","51129575","<ol>
<li><p>There is a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server#sql-server-as-sink"" rel=""noreferrer"">preCopyScript</a> property. You could put your script there. It will be executed before each run. </p></li>
<li><p>You could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure"" rel=""noreferrer"">store procedure activity</a> as Summit mentioned.</p></li>
<li><p>You could also create a <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""noreferrer"">custom activity</a>.</p></li>
</ol>
"
"51127506","How to run SQL Script in Azure Data Factory v2?","<p>There is NO Sql Script activity in Azure Data Factory V2. So how can I create a stored proc, a schema in a database? What are my options?</p>
","<azure><azure-data-factory>","2018-07-01 22:44:43","30314","20","4","59422740","<p>I agree that the absence of something like &quot;Execute SQL task&quot; of SSIS is bumming. I normally use a &quot;LookUp&quot; activity as I don't like to create procedures for simple tasks which could be a one liner command. The only constraint of lookup activity is that it needs some sort of output result set. So what I do when I need to run an update statement is something like this:</p>
<p><a href=""https://i.stack.imgur.com/P3uOe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P3uOe.png"" alt=""enter image description here"" /></a></p>
<p>The dummy query <code>select 0 id</code> &quot;feeds&quot; the data to the Lookup activity and thus it is able to run the command set on database.</p>
<p><strong>EDIT</strong>: At the time of first writing this, I found no other way than this and the answer came out of my own personal experience. However, Microsoft did add a &quot;Script Activity&quot; (refer Lukasz's answer) later which is surely the better way to go right now.</p>
"
"51127506","How to run SQL Script in Azure Data Factory v2?","<p>There is NO Sql Script activity in Azure Data Factory V2. So how can I create a stored proc, a schema in a database? What are my options?</p>
","<azure><azure-data-factory>","2018-07-01 22:44:43","30314","20","4","70956231","<p>Just to provide another option, I will share how I use to do it.</p>
<ol>
<li>Create a &quot;Stored Procedure&quot; Activity</li>
<li>On Settings at &quot;Stored procedure name&quot;, mark Edit, and type: sp_executesql</li>
<li>Under Stored procedure parameters, add a new parameter called &quot;statement&quot;, and in &quot;Value&quot; put your SQL command.</li>
</ol>
<p>This works with dynamic content as well. Reference about this procedure <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-executesql-transact-sql?view=sql-server-ver15"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Regards</p>
"
"51127506","How to run SQL Script in Azure Data Factory v2?","<p>There is NO Sql Script activity in Azure Data Factory V2. So how can I create a stored proc, a schema in a database? What are my options?</p>
","<azure><azure-data-factory>","2018-07-01 22:44:43","30314","20","4","71339898","<p>It is possible using <strong><a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-script"" rel=""noreferrer"">Script activity</a></strong>:</p>
<blockquote>
<p><strong>Using the script activity, you can execute common operations with Data Manipulation Language (DML), and Data Definition Language (DDL).</strong> DML statements like SELECT, UPDATE, and INSERT let users retrieve, store, modify, delete, insert and update data in the database. DDL statements like CREATE, ALTER and DROP allow a database manager to create, modify, and remove database objects such as tables, indexes, and users.</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/2gfR1.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/2gfR1.png"" alt=""enter image description here"" /></a></p>
<p><sup>Source: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/media/transform-data-using-script/inline-script.png"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/media/transform-data-using-script/inline-script.png</a></sup></p>
<p>Related: <a href=""https://techcommunity.microsoft.com/t5/azure-data-factory-blog/execute-sql-statements-using-the-new-script-activity-in-azure/ba-p/3239969"" rel=""noreferrer"">Execute SQL statements using the new 'Script' activity in Azure Data Factory and Synapse Pipelines</a></p>
"
"51110607","Loading with ADF to DW - Error happened when loading data into SQL Data Warehouse","<p>I created a pipeline with Azure Data Factory that copies tables from Azure sql to Azure DW (with staging blob storage) and one of the copy operations gave me this nice error:</p>

<pre><code>{
    ""errorCode"": ""2200"",
    ""message"": ""ErrorCode=FailedDbOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Error happened when loading data into SQL Data Warehouse.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Data.SqlClient.SqlException,Message=Query aborted-- the maximum reject threshold (0 rows) was reached while reading from an external source: 1 rows rejected out of total 1 rows processed.\n(/f65adec4-55bd-46f9-9d88-957d0f69bb5e/Polybase/[SCHEMA].[TABLE].txt)Column ordinal: 11&amp;#44; Expected data type: DECIMAL(10&amp;#44;4)&amp;#44; Offending value: \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000...,Source=.Net SqlClient Data Provider,SqlErrorNumber=107090,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=107090,State=1,Message=Query aborted-- the maximum reject threshold (0 rows) was reached while reading from an external source: 1 rows rejected out of total 1 rows processed.\n(/f65adec4-55bd-46f9-9d88-957d0f69bb5e/Polybase/[REWARDO].[CAMPAIGN].txt)Column ordinal: 11&amp;#44; Expected data type: DECIMAL(10&amp;#44;4)&amp;#44; Offending value: \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000...,},],'"",
    ""failureType"": ""UserError"",
    ""target"": ""Copy_k1b""
}
</code></pre>

<p>Now, I checked the source table data and the only column with DECIMAL(10,4) data type contains either zero's or decimal values. So WTF?</p>

<p><strong>REQUEST</strong>: Can you please provide at least column name?</p>

<p><strong>UPDATE</strong>: I removed all <code>DECIMAL(10,4) NULL</code> fields and it worked. Please <strong>fix</strong> this! (and don't tell ""failureType"": ""UserError"" to anyone!)</p>
","<azure><azure-data-factory>","2018-06-29 23:48:46","2547","1","1","52595478","<p>weidi is right. Set ""Use Type Default"" to FALSE in the Polybase settings to fix that.</p>
"
"51105550","open ip addresses for redshift and data factory","<p>I'm trying to connect an instance of Azure's Data Factory to Amazon Redshift. I have all the connection details, but the issue is that I need to open the IP address of the Redshift database for it to work. I know this because if I open all IP addresses, I can connect fine. Without them being open though, I get a connection timeout. </p>

<p>For security reasons, I can't leave all IP addresses open. I need to figure out the one Data Factory is operating on, and just open that. </p>

<p>The consensus seems to be, however, that Data Factory does not operate on a static IP address. </p>
","<azure><amazon-redshift><azure-data-factory>","2018-06-29 15:58:53","433","1","1","51111511","<p>You could also setup a selfhosted IR for your redshift. Reference that IR in your redshift linked service. Then you only need open IP address of the machine hosting that selfhosted IR.
Make sure your source and sink reference the same selfhosted IR if sink is also on premises.</p>
"
"51101459","Azure Data Factory using Python","<p>Can someone tell me how we can set general features of Azure pipeline in Azure Data Factory using Python like policy variable (timeout,retry etc)?</p>
","<python><azure><azure-data-factory>","2018-06-29 12:11:25","461","0","1","51130890","<p>Please add <code>policy</code> parameter in your python code:</p>

<pre><code># Create a copy activity
act_name = 'copyBlobtoBlob'
blob_source = BlobSource()
blob_sink = BlobSink()

policy = {""timeout"": ""1.00:00:00"",
          ""concurrency"": 1,
          ""executionPriorityOrder"": ""NewestFirst"",
          ""style"": ""StartOfInterval"",
          ""retry"": 3,
          ""longRetry"": 0,
          ""longRetryInterval"": ""00:00:00""}

copy_activity = CopyActivity(act_name, source=blob_source, sink=blob_sink,
                             policy=policy)

# Create a pipeline with the copy activity
p_name = 'copyPipeline'
params_for_pipeline = {}
p_obj = PipelineResource(activities=[copy_activity], parameters=params_for_pipeline)
p = adf_client.pipelines.create_or_update(rg_name, df_name, p_name, p_obj)
</code></pre>

<p>Test:</p>

<p><a href=""https://i.stack.imgur.com/MUqcK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MUqcK.png"" alt=""enter image description here""></a></p>

<p>Hope it helps you.</p>
"
"51099043","Can't run Azure data factory from Azure function","<p>This is my c# Azure Function script:</p>

<pre><code>#r ""System.Runtime""
#r ""System.Threading.Tasks""
using System;
using System.Net;
using System.Threading.Tasks;
using Newtonsoft.Json;
using Microsoft.Azure;
using Microsoft.Azure.Management.DataFactory;
using Microsoft.Azure.Management.DataFactory.Models;
using Microsoft.IdentityModel.Clients.ActiveDirectory;
using Microsoft.Rest;

public static void Run(TimerInfo myTimer, TraceWriter log)
{
    log.Info($""C# Timer trigger function executed at: {DateTime.Now}"");
    /*...All variable definations...*/


var authenticationContext = new AuthenticationContext(activeDirectoryEndpoint + activeDirectoryTenantld);    
ClientCredential credential = new ClientCredential(clientld, clientSecret);    
AuthenticationResult result = authenticationContext.AcquireTokenAsync(windowsManagementUri, credential).Result; 
ServiceClientCredentials cred = new TokenCredentials(result.AccessToken);
var client = new DataFactoryManagementClient(cred) { SubscriptionId = subscriptionId };

Dictionary&lt;string, object&gt; arguments = new Dictionary&lt;string, object&gt;
            {
                { ""bufferdays"", 10 },
                {""TimeToCopy"", 20 }
            };    
CreateRunResponse runResponse = client.Pipelines.CreateRunWithHttpMessagesAsync(resourceGroupName, dataFactoryName, pipelineName,arguments).Result.Body;

}
</code></pre>

<p>This throws error <strong><em>run.csx(56,130): error CS1503: Argument 4: cannot convert from 'System.Collections.Generic.Dictionary' to 'string'</em></strong>
. But the same thing works fine in Visual Studio. What is wrong here?</p>
","<c#><azure><azure-functions><azure-data-factory>","2018-06-29 09:54:30","485","1","1","51100037","<p>Got it. </p>

<pre><code>I was using {
  ""frameworks"": {
    ""net46"":{
      ""dependencies"": {

        ""Microsoft.Azure.Management.DataFactory"": ""0.8.0"",
</code></pre>

<p>in project.json so automatically it was downloading version 1.0.0. I had to force it to download version 0.8.0 by specifying <code>""Microsoft.Azure.Management.DataFactory"": ""0.8.0-preview"",</code> explicitly.</p>
"
"51084960","Use parameters in place of linked service name in Azure Data Factory V2","<p>My question is slightly similar to this <a href=""https://stackoverflow.com/questions/49814312/change-connection-string-linked-service-in-azure-data-factory-v2"">question</a> however adequately different to merit a new thread.</p>

<p>I have a requirement to extract data from a number of different on-premises SQL Server instances over the internet. I am using Azure Data-Factory 2 and the Integration Runtime to access data from these servers.</p>

<p>The problem is that i will have many pipelines to manage and update. I want to have a single Data Factory process which uses parameters for linked service names.</p>

<p><em>Is it possible to have 1 pipeline which uses a parameter to reference a linked service name which is updated before re-executing the pipeline?</em> </p>

<p>I am struggling to find a useful article on how this can be achieved.</p>
","<azure><parameters><azure-data-factory>","2018-06-28 14:06:02","402","0","1","51111600","<p>Reference name can’t be parametied. But making linked Service support parameters is in the plan as the post you mentioned said.</p>
"
"51084717","CI/CD pipeline for Data factory V1 using VSTS","<p>I am working on creating an entire CI/CD pipeline for my <strong>Data factory V1</strong> project I am using VSTS for the implementation.</p>

<p>I am able to carry out most of my task over VSTS which are required for deployment, However, I am not able to determine if it is possible to completely implement continuous deployment over my project.</p>

<p>I have one common solution file responsible to hold 4 different data factory project &amp; each project holds 4 data flow pipelines each.</p>

<p>The <strong><em>issue</em></strong> is not every time is the case that the entire solution is deployed to a higher environment. we have specific pipelines to be deployed over each deployment.</p>

<p>Is it possible to deploy specific pipelines i.e picking one from each project and deploy them ahead using the release pipeline.</p>

<p>If yes, how are we going to implement it in the vsts release pipeline &amp; will it be incremental?</p>
","<c#><azure><continuous-integration><azure-devops><azure-data-factory>","2018-06-28 13:53:48","392","0","2","51094702","<p>This is a doc for ADF V2 <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">Continuous integration and deployment</a> 
with ADF V2 UI, it will allow you bind a VSTS repository to your Azure Data Factory.</p>
"
"51084717","CI/CD pipeline for Data factory V1 using VSTS","<p>I am working on creating an entire CI/CD pipeline for my <strong>Data factory V1</strong> project I am using VSTS for the implementation.</p>

<p>I am able to carry out most of my task over VSTS which are required for deployment, However, I am not able to determine if it is possible to completely implement continuous deployment over my project.</p>

<p>I have one common solution file responsible to hold 4 different data factory project &amp; each project holds 4 data flow pipelines each.</p>

<p>The <strong><em>issue</em></strong> is not every time is the case that the entire solution is deployed to a higher environment. we have specific pipelines to be deployed over each deployment.</p>

<p>Is it possible to deploy specific pipelines i.e picking one from each project and deploy them ahead using the release pipeline.</p>

<p>If yes, how are we going to implement it in the vsts release pipeline &amp; will it be incremental?</p>
","<c#><azure><continuous-integration><azure-devops><azure-data-factory>","2018-06-28 13:53:48","392","0","2","51099257","<p>Please take a look at <a href=""https://blog.abatishchev.ru/2017/03/28/352"" rel=""nofollow noreferrer"">this blog</a> which maybe helpful. The author is using a comparable method for deployment. Before deploying the JSON files using a PowerShell command, and edit them to insert environment specific values into the Data Factory definitions. You can pass these values as parameters from the VSTS deployment-pipeline. </p>

<p>Also take a look at this blog: <a href=""http://eatcodelive.com/2016/01/18/deploy-azure-data-factory-using-powershell/"" rel=""nofollow noreferrer"">Deploy Azure Data Factory using PowerShell</a></p>

<p>If you want to control the specific pipelines, you could use some 3rd-party extensions such as this one-- <a href=""https://marketplace.visualstudio.com/items?itemName=liprec.vsts-publish-adf"" rel=""nofollow noreferrer"">Azure Data Factory</a></p>

<p>Azure Data Factory Pipelines Management, this release task can be added to a release pipeline to either suspend or resume all pipelines of an Azure Data Factory. </p>

<p><a href=""https://i.stack.imgur.com/HwQL3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HwQL3.png"" alt=""enter image description here""></a></p>
"
"51083674","azure table export Timestamp value is not working","<p>I am trying to set up a simple data factory pipeline with intention of copying Azure table storage to Cosmos DB.
Azure table storage has a system managed field, Timestamp. When flow runs and Cosmos db is populated with the data Timestamp is always   ""Timestamp"": ""1970-01-01T00:00:00Z"". Seems that it is not getting into Cosmos DB correctly.</p>

<p>How to reproduce: create a Azure table, add a few entries. Create Cosmos db instance and a new collection. Create Dat Factory flow. Note how Timestamp looks when exported.</p>

<p>I tried to change Timestamp data type from DateTime to DateTimeOffset as well as exporting it as a string. I also tried to specify the date format. The result is the same.</p>

<p>I suspect that Timestamp is a reserved word in Cosmos DB and somehow it fails to insert correct value. </p>
","<azure-cosmosdb><azure-table-storage><azure-data-factory>","2018-06-28 13:04:41","451","2","1","51097262","<p>I reproduced your issue on my side. I tried to add an property <code>time</code> and set the value as same as <code>Timestamp</code>. It could imported into cosmos db correctly. </p>

<p><a href=""https://i.stack.imgur.com/rmnu2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rmnu2.png"" alt=""enter image description here""></a></p>

<p>So, I think it's related to field name ,not data type or data format. However, I tried to specify target column field name <code>timestamp</code> instead of <code>Timestamp</code> then it does't working. </p>

<p>Based on this doc, <code>_ts</code> is auto-generated by cosmos db which as a number representing the number of elapsed seconds since January 1, 1970. It logs <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/sql-api-resources#system-vs-user-defined-resources"" rel=""nofollow noreferrer"">last updated timestamp of the resource</a> so that it has the same meaning as Timestamp in Azure Table storage.It could be converted <code>UnixDateTimeConverter</code> class. So,you could trace date log via <code>_ts</code> field.</p>

<p>If you do want keep Timestamp, you could add a property as same value sa Timestamp then import it into cosmos db.</p>

<p>Hope it helps you.</p>
"
"51070659","Monitoring not showing all pipeline runs","<p>In my Azure Data Factory, Monitoring appears to only be showing pipeline activity that was kicked off via a schedule. I've executed a number of pipelines manually. Those don't appear to be showing up in Monitoring. Is this by design or am I doing something wrong?</p>
","<azure-data-factory>","2018-06-27 20:09:31","1929","2","1","51082316","<p>Only trigger ones will show out in monitor. Debug runs will just show in output of authoring tab and it can’t be accessed after you close that tab.</p>

<p>Updated:
ADF supports view active debug runs now.
<a href=""https://i.stack.imgur.com/dcdxY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dcdxY.png"" alt=""enter image description here""></a></p>
"
"51070205","how to pass a folder from a data lake store as a parameter to a pipeline?","<p>In data factory, I know you can pass a parameter at the beginning of a pipeline, and then access it later using @pipeline(). If I have a folder in a data lake store, how can I pass that as a parameter and have access to it later (let's say I want to loop a for-each over each file inside it.) Do I pass the path to the folder? Am I passing it as an object? </p>
","<azure><azure-data-factory>","2018-06-27 19:34:54","945","1","2","51072128","<p>Here are the steps that you can use - </p>

<ol>
<li><p>You can use pass folder path as a parameter (string) to the pipeline. </p></li>
<li><p>Use the path and ""Get Metadata"" activity with ""Child Items"". This will return the list of files in JSON Format</p>

<p><a href=""https://i.stack.imgur.com/zONRd.png"" rel=""nofollow noreferrer"">Get Metadata Selection</a></p></li>
<li><p>Loop through using ""Foreach"" activity and perform any action. </p>

<p>Use output from metadata activity as Items in Foreach activity (example below)</p></li>
</ol>

<blockquote>
  <p>@activity('Get List of Files').output</p>
</blockquote>

<p>Hope this helps</p>
"
"51070205","how to pass a folder from a data lake store as a parameter to a pipeline?","<p>In data factory, I know you can pass a parameter at the beginning of a pipeline, and then access it later using @pipeline(). If I have a folder in a data lake store, how can I pass that as a parameter and have access to it later (let's say I want to loop a for-each over each file inside it.) Do I pass the path to the folder? Am I passing it as an object? </p>
","<azure><azure-data-factory>","2018-06-27 19:34:54","945","1","2","51105362","<ol>
<li>First, you need create a data lake store linked service. It will contain the path of azure data lake store. You could use azure data factory UI to create the linked service</li>
<li><p>Then you need create a data lake store dataset reference that linked service in step 2.</p></li>
<li><p>Then you create a getMetaData activity reference dataset in step 2. </p></li>
<li><p>Then following steps provided by summit.</p></li>
</ol>

<p>All of these can be done in UI.<a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-portal#create-a-pipeline"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-portal#create-a-pipeline</a></p>
"
"51061987","Azure Data Factory V1- How to handle multiple Scripts in one pipeline activity","<p>currently I am trying to read and unpivot csv-files with unknown column names on Microsoft Azure. Therefor I am using a Data Factory with two data lake analytics activities: The first activity generates a script to read and unpivot the data and the second avtivity is just the execution of this script. 
My problem is, that sometimes the generated scripts from the first activity are too big     </p>

<blockquote>
  <p>""The provided U-SQL script is 6449969 bytes long, which exceeds the
  size 
        limit of 2097152 bytes.""</p>
</blockquote>

<p>My idea was to split them, but I think that it is not possible to run more than 1 script in 1 activity. Since I dont know in how many party the script would be devided I cannot just add a fix number of activities.</p>

<p>Any suggestions?</p>
","<azure><azure-data-factory><azure-data-lake><u-sql>","2018-06-27 11:45:19","156","0","1","51062184","<p>The only way to work around this limitation at this point is to write a custom extractor. However, you will have to expose the data not as a string but as byte[].</p>

<p>If you use a custom extractor that just reads the byte array, you can go up to 4MB.</p>

<p>In general, if you need to parse your row, it is going to be probably faster, if you write your custom extractor instead of using the built-in extractor and then write another U-SQL transformation or two to parse the data (again).</p>

<p>You can refer to this repo maybe for some insights - <a href=""https://github.com/Azure/usql/tree/mrys-json"" rel=""nofollow noreferrer"">https://github.com/Azure/usql/tree/mrys-json</a></p>
"
"51043814","Azure Data Factory V2 Copy Activity to Data Warehouse from Blob storage","<p>I'm using ADF v2 to import data from CSV source on Blob Storage to Azure SQL Dat Warehouse. My first Activity in the Pipeline is Copy activity from CSV (on hot blob storage) to Azure SQL DWH. If this activity finish successfully the next one is again Copy activity but now from CSV (on hot blob storage) to CSV (on cool blob storage), move process for history and storage pricing optimization. Filename of the CSV file is sent to Pipeline as Pipeline-Parameter.</p>

<p>What I noticed is that if I start the Pipeline with Filename parameter of the csv that does not exists on the blob storage, first Copy activity (CSV -> DWH) finish without error. It does not import any data to DWH, which is normal, but raise no error of missing file. Next Copy activity (CSV hot storage -> CSV cool storage) raise Failed error -> blob is missing. </p>

<p>I would like also Copy to DWH to raise Error, because that way I can add other activity in the pipeline that will handle this situation. Log error somewhere, mark file as missing, etc.</p>

<p>What is specific in Copy activity from Storage to DWH that is finishing successfully without source blob existence? I use polybase checkbox in the Sink part of the activity if that counts for something. </p>

<p>Does anyone knows how to set this Copy activity (Storage CSV -> SQL DWH) to Fail if source blob does not exists?</p>

<p>Thanks</p>
","<azure-data-factory>","2018-06-26 13:13:14","1039","1","1","51083603","<p>yes. 
You can put as many as you want activities in your if activity. 
So you can take details about blob storage with getMetadata activity (check exists property in the documentation, link below). </p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity</a></p>
"
"51043010","not able to create pipeline in microsoft azure using python(2.0) getting the following error","<p>Here is my main function. I am getting the following error shown in the snapshot below.</p>

<pre><code>def main():

    # Azure subscription ID
    subscription_id = ''

    # This program creates this resource group. If it's an existing resource group, comment out the code that creates the resource group
    rg_name = ''

    # The data factory name. It must be globally unique.
    df_name = ''        

    # Specify your Active Directory client ID, client secret, and tenant ID
    credentials = ServicePrincipalCredentials(client_id='', secret='', tenant='')
    resource_client = ResourceManagementClient(credentials, subscription_id)
    adf_client = DataFactoryManagementClient(credentials, subscription_id)

    rg_params = {'location':'eastus'}
    df_params = {'location':'eastus'}

    # create the resource group
    # comment out if the resource group already exits
    #resource_client.resource_groups.create_or_update(rg_name, rg_params)

    # Create a data factory
    #df_resource = Factory(location='eastus')
    #df = adf_client.factories.create_or_update(rg_name, df_name, df_resource)
    #print_item(df)
    #while df.provisioning_state != 'Succeeded':
    #    df = adf_client.factories.get(rg_name, df_name)
    #    time.sleep(1)

    # Create an Azure Storage linked service
    ls_name = ''

    # Specify the name and key of your Azure Storage account
    storage_string = SecureString('DefaultEndpointsProtocol=https;AccountName=;AccountKey=;EndpointSuffix=core.windows.net')
    ls_azure_storage = AzureStorageLinkedService(connection_string=storage_string)
    ls = adf_client.linked_services.create_or_update(rg_name, df_name, ls_name, ls_azure_storage)
    print_item(ls)

    # Create an Azure blob dataset (input)
    ds_name = ''
    ds_ls = LinkedServiceReference(ls_name)
    blob_path= ''
    blob_filename = ''
    ds_azure_blob= AzureBlobDataset(ds_ls, folder_path=blob_path, file_name = blob_filename)
    ds = adf_client.datasets.create_or_update(rg_name, df_name, ds_name, ds_azure_blob)
    print_item(ds)

    # Create an Azure blob dataset (output)
    dsOut_name = ''
    output_blobpath = ''
    dsOut_azure_blob = AzureBlobDataset(ds_ls, folder_path=output_blobpath)
    dsOut = adf_client.datasets.create_or_update(rg_name, df_name, dsOut_name, dsOut_azure_blob)
    print_item(dsOut)

    # Create a copy activity
    act_name =  ''
    blob_source = BlobSource()
    blob_sink = BlobSink()
    dsin_ref = DatasetReference(ds_name)
    dsOut_ref = DatasetReference(dsOut_name)
    copy_activity = CopyActivity(act_name,inputs=[dsin_ref], outputs=[dsOut_ref], source=blob_source, sink=blob_sink)

    # Create a pipeline with the copy activity
    p_name =  ''
    params_for_pipeline = {}
    p_obj = PipelineResource(activities=[copy_activity], parameters=params_for_pipeline)
    p = adf_client.pipelines.create_or_update(rg_name, df_name, p_name, p_obj)
    print_item(p)

    # Create a pipeline run
    run_response = adf_client.pipelines.create_run(rg_name, df_name, p_name,
        {
        }
    )

    # Monitor the pipeilne run
    time.sleep(30)
    pipeline_run = adf_client.pipeline_runs.get(rg_name, df_name, run_response.run_id)
    print(""\n\tPipeline run status: {}"".format(pipeline_run.status))
    activity_runs_paged = list(adf_client.activity_runs.list_by_pipeline_run(rg_name, df_name, pipeline_run.run_id, datetime.now() - timedelta(1),  datetime.now() + timedelta(1)))
    print_activity_run_details(activity_runs_paged[0])
</code></pre>

<hr>

<p>i am getting the following error:-
ErrorResponseException                    Traceback (most recent call last)
 in ()
----> 1 main()</p>

<p> in main()
     37 
     38     ls_azure_storage = AzureStorageLinkedService(connection_string=storage_string)
---> 39     ls = adf_client.linked_services.create_or_update(rg_name, df_name, ls_name, ls_azure_storage)
     40     print_item(ls)
     41 </p>

<p>/usr/local/lib/python2.7/dist-packages/azure/mgmt/datafactory/operations/linked_services_operations.pyc in create_or_update(self, resource_group_name, factory_name, linked_service_name, properties, if_match, custom_headers, raw, **operation_config)
    170 
    171         if response.status_code not in [200]:
--> 172             raise models.ErrorResponseException(self._deserialize, response)
    173 
    174         deserialized = None</p>

<p>ErrorResponseException: Operation returned an invalid status code 'Forbidden'</p>

<p><img src=""https://i.stack.imgur.com/pVdv9.png"" alt=""enter image description here""></p>
","<python><azure><azure-data-factory>","2018-06-26 12:31:40","457","0","1","51065987","<p>Forbidden generally means you don’t have permission. Could you check if you have the write permission of that data factory?</p>

<p>Are you using ADF v1 or v2?
Did you tried create a pipeline in UI since that is an easy way to verify your permission?</p>
"
"51040983","Delete some component in ARM template data factory?","<p>I am trying to delete some components of data factory in ARM template json file. For an example if I have pipeline A,B,C and I would like to only deploy pipeline A into the data factory. Is there any way to accomplish this? I found only command that delete after deploy but I would prefer to delete it before deploying.</p>

<p>I also consider the option of deleting some of json key and value inside ""resource"" key in template file. Anyone ever tried this?</p>
","<azure><azure-data-factory>","2018-06-26 10:45:58","372","0","1","51042151","<p>You could modify the ARM template json file.
Please reference this doc about <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#continuous-integration-lifecycle"" rel=""nofollow noreferrer"">Continuous integration lifecycle</a>.</p>

<p>Please take a look of this post. The other answers (not mine) provided a better answer.
<a href=""https://stackoverflow.com/questions/51084717/ci-cd-pipeline-for-data-factory-v1-using-vsts"">CI/CD pipeline for Data factory V1 using VSTS</a></p>
"
"51035210","Error using data factory for copyactivity from cosmos db as source","<p>Why do I keep getting this error while using cosmos DB as source in copy activity in data factory v2?</p>

<pre><code>[6/26/2018 4:59:37 AM]   ""errorCode"": ""2200"",
[6/26/2018 4:59:37 AM]   ""message"": ""ErrorCode=UserErrorFormatIsRequired,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Format setting is required for file based store(s) in this scenario.,Source=Microsoft.DataTransfer.ClientLibrary,'"",
[6/26/2018 4:59:37 AM]   ""failureType"": ""UserError"",
[6/26/2018 4:59:37 AM]   ""target"": ""CopyFromCosmosDB""
</code></pre>

<p>What does this error mean?</p>
","<azure><azure-cosmosdb><azure-data-factory>","2018-06-26 05:03:43","457","3","1","51042122","<p>It means you need specify format for your sink dataset. If you want to copy it as json or csv or other format. You need specify the format explicitly. You could use ADF V2 UI to author you pipeline.</p>
"
"51030904","ADF V2 - set parameter value from filepath of the blob (csv)","<p>I'm building a solution with ADF V2 which need to import data from CSV files into Azure SQL Data Warehouse. I want to use new Azure Blob Storage trigger (event trigger) to execute pipeline every time new CSV file is added to blob container. CSV files are organized in the following hierarchy (container and virtual folders):</p>

<ul>
<li>FactoryId (1, 2, 3, ..)</li>
<li>--Year (2017, 2018, ...)</li>
<li>----Month (01, 02, ...)</li>
<li>-------Day (01, 02, ... 30)</li>
</ul>

<p>I want to set ADF variable with name FactoryId from the FilePath of the blob that has raised the event trigger (ex. /1/2018/06/25  - FactoryId would be 1). Then I would like to use this value as input value for a column in a table in Azure SQL DWH. The table will have same columns like in the CSV file plus this one FactoryId and I want to populate this value from variable / filepath value.</p>

<p>Is this possible with ADF V2 and if not what would be your other suggestions.</p>

<p>Thanks</p>
","<azure-data-factory>","2018-06-25 19:48:50","2775","0","1","51202367","<ul>
<li>For blob event trigger,</li>
</ul>

<p>When a blob events trigger fires, it makes two variables available to your pipeline: folderPath and fileName. To access these variables, use the @triggerBody().fileName or @triggerBody().folderPath expressions.</p>

<p>Please reference <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger#using-blob-events-trigger-properties"" rel=""nofollow noreferrer"">this link</a>.</p>

<ul>
<li>For Then I would like to use this value as input value for a column in a table in Azure SQL DWH.</li>
</ul>

<p>Currently, there is no easy way to implement this directly. You may use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">custom activity</a>.</p>
"
"51023159","Azure datafactory V2- activity run slices are not visible","<p>I have created pipeline in ADF V2. Pipeline consists of lookup - Copy- Custom.</p>

<p>The pipeline source code is correct and gets validated. But I am not able to see the activity slices in Monitor Pipeline window. What changes to be made?
<a href=""https://i.stack.imgur.com/5Ei0I.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5Ei0I.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/AgJUU.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AgJUU.jpg"" alt=""enter image description here""></a></p>

<p>Also, how to delete pipeline from portal? The delete option deleted the pipeline from the list, but when I tried creating pipeline with same name, it failed</p>

<p><a href=""https://i.stack.imgur.com/q48CP.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q48CP.jpg"" alt=""enter image description here""></a></p>
","<azure-data-factory>","2018-06-25 12:08:25","565","0","1","51023305","<p>For your question about how to delete:
After deleting it, you need click ""publish all"" button to sync your change to your data factory. </p>

<p>For the activity run not visible issue:
Please switch to ""All"" tab. Currently, you are in ""Cancelled"" tab.</p>
"
"51022160","Azure Data Factory - ARM Template","<p>I'm deploying ARM template to Azure Data Factory v2 using PowerShell command </p>

<pre><code>New-AzureRmResourceGroupDeployment -Name MyDeployment -ResourceGroupName myResourceGroup -TemplateFile C:\work\arm_template.json -TemplateParameterFile C:\work\arm_template_parameters.json
</code></pre>

<p>There are two Oracle 12c Linked Services in the factory. The deployment fails with error messages pointing something wrong was with these linked services. The messages provide no details:</p>

<pre><code>Resource Microsoft.DataFactory/factories/linkedServices 'myDataFactory/myLinkedServiceOne' failed with message '{
  ""code"": ""BadRequest"",
  ""message"": ""Error message not found.
</code></pre>

<p>I can see all resources of the Data Factory in Azure Cloud after the deployment fails but those two Linked Services (and pipelines that depend on the Linked Services)</p>

<p>The issue I'm aware of is I didn't know how connection strings should have look like. I didn't find it in MS documentation so I made them up completely. They were as follows (arm-template-parameters.json):</p>

<pre><code>    ""myLinkedServiceOne_connectionString"": {
        ""value"": ""Host=mydatabase.host.net;Port=1536;SID=someSid;User Id=someUser;Password=somePassword;""
    },
</code></pre>

<p>My question is: How Oracle DB connection string should look like in ARM parameters file?</p>
","<oracle><azure><azure-data-factory><azure-resource-manager>","2018-06-25 11:13:44","913","0","1","51023742","<p>This is due to your Integration Runtime is offline. Please restart it and try it again.</p>

<p>You connection string looks fine according to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-oracle#linked-service-properties"" rel=""nofollow noreferrer"">this doc</a>.</p>

<p>But if you are using ODBC connector, you should follow <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-odbc#linked-service-properties"" rel=""nofollow noreferrer"">this doc</a>.</p>
"
"51020446","Azure Data factory to SQL server in AWS","<p>I am new to Azure Data Factory (ADF) and would like to know whether it is technically possible to use ADF to copy data from a source in <strong>AWS</strong> (not Azure) environment and put it to a sink in another <strong>AWS</strong> environment. I am aware that we need Integration Runtime (IR) to connect to the source. Can we achive copying to AWS as well using IR?</p>

<p>According to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-data-movement-activities#supported-data-stores-and-formats"" rel=""nofollow noreferrer"">this document</a> </p>

<p>Data stores with * <strong>can be on-premises or on Azure IaaS</strong>, and require you to install Data Management Gateway on an on-premises/Azure IaaS machine. </p>

<p>But this does not say that we can/cannot transfer to AWS environment.</p>
","<amazon-web-services><azure-data-factory>","2018-06-25 09:41:51","1823","2","1","51023222","<p>You are referencing ADF V1 doc. You could reference <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">ADF V2 doc</a> for ADF V2 support more data store.</p>

<p>Currently, ADF V2 support <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-amazon-marketplace-web-service"" rel=""nofollow noreferrer"">Amazon Marketplace Web Service</a> as Source, but not sink. But you could take a look of <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-odbc"" rel=""nofollow noreferrer"">generic ODBC</a> if you have odbc driver for your aws sql server.</p>
"
"50997104","ADF v2 Copy Activity, Log details about activity to Database Table","<p>Is there an option to Log details of Copy Activity to a Database Table.
I want to log the FileName &amp; PAth that was generate, PipelineID that Generated it, How long it took to copy the File, Rows it copied, size of File Created plus few more.</p>

<p>How can all of these be achieved ?</p>
","<azure-data-factory>","2018-06-23 01:01:47","1900","3","1","51006421","<p>You could reference this two link.
1. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-visually"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/monitor-visually</a>
2. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/monitor-programmatically"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/monitor-programmatically</a></p>

<p>The first one is about what information you can get. The second is to tell you how to get these information with different way.</p>

<p>But currently, I don’t think there is a way to get file name and path directly. But you could leverage user properties. Please reference this post. <a href=""https://social.msdn.microsoft.com/Forums/azure/en-US/8692cd00-307b-4204-a547-bed2030cb762/adfv2-user-property-setting?forum=AzureDataFactory"" rel=""nofollow noreferrer"">https://social.msdn.microsoft.com/Forums/azure/en-US/8692cd00-307b-4204-a547-bed2030cb762/adfv2-user-property-setting?forum=AzureDataFactory</a></p>
"
"50988754","Azure Data Factory v2 portal is slow","<p>This is a generic question regarding development processes and using Azure Data Factory v2.</p>

<p>I am currently using the UI portal to set up and configure pipelines, datasets, activities, triggers, etc. but I am finding the lag time very long. </p>

<p>Is this the same for other users ?</p>

<p>What is the typical workflow for someone not using the portal ? (There seem to be no nuget packages for v2 and only powershell as the alternative to the UI portal.)</p>
","<azure><azure-data-factory>","2018-06-22 13:13:24","1055","0","1","50988873","<p>The publish performance is low when you have large number of recourses (pipelines, datasets, etc). The PROD team is working on improving this.</p>

<p>And if possible, please use Chrome as browser. </p>
"
"50986738","Azure Data Factory v2 using utcnow() as a pipeline parameter","<p>For context, I currently have a Data Factory v2 pipeline with a <strong>ForEach Activity</strong> that calls a <strong>Copy Activity</strong>. The <strong>Copy Activity</strong> simply copies data from an FTP server to a blob storage container.</p>
<p>Here is the pipeline json file :</p>
<pre><code>{
    &quot;name&quot;: &quot;pipeline1&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;ForEach1&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@pipeline().parameters.InputParams&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;Copy1&quot;,
                            &quot;type&quot;: &quot;Copy&quot;,
                            &quot;policy&quot;: {
                                &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                                &quot;retry&quot;: 0,
                                &quot;retryIntervalInSeconds&quot;: 30,
                                &quot;secureOutput&quot;: false
                            },
                            &quot;typeProperties&quot;: {
                                &quot;source&quot;: {
                                    &quot;type&quot;: &quot;FileSystemSource&quot;,
                                    &quot;recursive&quot;: true
                                },
                                &quot;sink&quot;: {
                                    &quot;type&quot;: &quot;BlobSink&quot;
                                },
                                &quot;enableStaging&quot;: false,
                                &quot;cloudDataMovementUnits&quot;: 0
                            },
                            &quot;inputs&quot;: [
                                {
                                    &quot;referenceName&quot;: &quot;FtpDataset&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;,
                                    &quot;parameters&quot;: {
                                        &quot;FtpFileName&quot;: &quot;@item().FtpFileName&quot;,
                                        &quot;FtpFolderPath&quot;: &quot;@item().FtpFolderPath&quot;
                                    }
                                }
                            ],
                            &quot;outputs&quot;: [
                                {
                                    &quot;referenceName&quot;: &quot;BlobDataset&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;,
                                    &quot;parameters&quot;: {
                                        &quot;BlobFileName&quot;: &quot;@item().BlobFileName&quot;,
                                        &quot;BlobFolderPath&quot;: &quot;@item().BlobFolderPath&quot;
                                    }
                                }
                            ]
                        }
                    ]
                }
            }
        ],
        &quot;parameters&quot;: {
            &quot;InputParams&quot;: {
                &quot;type&quot;: &quot;Array&quot;,
                &quot;defaultValue&quot;: [
                    {
                        &quot;FtpFolderPath&quot;: &quot;/Folder1/&quot;,
                        &quot;FtpFileName&quot;: &quot;@concat('File_',formatDateTime(utcnow(), 'yyyyMMdd'), '.txt')&quot;,
                        &quot;BlobFolderPath&quot;: &quot;blobfolderpath&quot;,
                        &quot;BlobFileName&quot;: &quot;blobfile1&quot;
                    },
                    {
                        &quot;FtpFolderPath&quot;: &quot;/Folder2/&quot;,
                        &quot;FtpFileName&quot;: &quot;@concat('File_',formatDateTime(utcnow(), 'yyyyMMdd'), '.txt')&quot;,
                        &quot;BlobFolderPath&quot;: &quot;blobfolderpath&quot;,
                        &quot;BlobFileName&quot;: &quot;blobfile2&quot;
                    }
                ]
            }
        }
    },
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/pipelines&quot;
}
</code></pre>
<p>The issue I am having is that when specifying pipeline parameters, it seems I cannot use system variables and functions the same way I can when for example specifying folder paths for a blob storage dataset.
The consequence of this is that <code>formatDateTime(utcnow(), 'yyyyMMdd')</code> is not being interpreted as function calls but rather the actual string with value <code>formatDateTime(utcnow(), 'yyyyMMdd')</code>.</p>
<p>To counter this I am guessing I should be using a trigger to execute my pipeline and pass the trigger's execution time as a parameter to the pipeline like <code>trigger().startTime</code> but is this the only way? Am I simply doing something wrong in my pipeline's JSON?</p>
","<azure><azure-data-factory>","2018-06-22 11:15:50","31650","6","3","50987825","<p>You can't put a dynamic expression in the default value. You should define this expression and function either when you creating a trigger, or when you define dataset parameters in sink/source in copy activity.
So either you create dataset property FtpFileName with some default value in source dataset, and then in copy activity, you can in source category to specify that dynamic expression. </p>

<p><a href=""https://i.stack.imgur.com/qU4nA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qU4nA.png"" alt=""example""></a></p>

<p>Another way is to define pipeline parameter, and then to add dynamic expression to that pipeline parameter when you are defining a trigger. Hope this is a clear answer to you. :)</p>
"
"50986738","Azure Data Factory v2 using utcnow() as a pipeline parameter","<p>For context, I currently have a Data Factory v2 pipeline with a <strong>ForEach Activity</strong> that calls a <strong>Copy Activity</strong>. The <strong>Copy Activity</strong> simply copies data from an FTP server to a blob storage container.</p>
<p>Here is the pipeline json file :</p>
<pre><code>{
    &quot;name&quot;: &quot;pipeline1&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;ForEach1&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@pipeline().parameters.InputParams&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;Copy1&quot;,
                            &quot;type&quot;: &quot;Copy&quot;,
                            &quot;policy&quot;: {
                                &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                                &quot;retry&quot;: 0,
                                &quot;retryIntervalInSeconds&quot;: 30,
                                &quot;secureOutput&quot;: false
                            },
                            &quot;typeProperties&quot;: {
                                &quot;source&quot;: {
                                    &quot;type&quot;: &quot;FileSystemSource&quot;,
                                    &quot;recursive&quot;: true
                                },
                                &quot;sink&quot;: {
                                    &quot;type&quot;: &quot;BlobSink&quot;
                                },
                                &quot;enableStaging&quot;: false,
                                &quot;cloudDataMovementUnits&quot;: 0
                            },
                            &quot;inputs&quot;: [
                                {
                                    &quot;referenceName&quot;: &quot;FtpDataset&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;,
                                    &quot;parameters&quot;: {
                                        &quot;FtpFileName&quot;: &quot;@item().FtpFileName&quot;,
                                        &quot;FtpFolderPath&quot;: &quot;@item().FtpFolderPath&quot;
                                    }
                                }
                            ],
                            &quot;outputs&quot;: [
                                {
                                    &quot;referenceName&quot;: &quot;BlobDataset&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;,
                                    &quot;parameters&quot;: {
                                        &quot;BlobFileName&quot;: &quot;@item().BlobFileName&quot;,
                                        &quot;BlobFolderPath&quot;: &quot;@item().BlobFolderPath&quot;
                                    }
                                }
                            ]
                        }
                    ]
                }
            }
        ],
        &quot;parameters&quot;: {
            &quot;InputParams&quot;: {
                &quot;type&quot;: &quot;Array&quot;,
                &quot;defaultValue&quot;: [
                    {
                        &quot;FtpFolderPath&quot;: &quot;/Folder1/&quot;,
                        &quot;FtpFileName&quot;: &quot;@concat('File_',formatDateTime(utcnow(), 'yyyyMMdd'), '.txt')&quot;,
                        &quot;BlobFolderPath&quot;: &quot;blobfolderpath&quot;,
                        &quot;BlobFileName&quot;: &quot;blobfile1&quot;
                    },
                    {
                        &quot;FtpFolderPath&quot;: &quot;/Folder2/&quot;,
                        &quot;FtpFileName&quot;: &quot;@concat('File_',formatDateTime(utcnow(), 'yyyyMMdd'), '.txt')&quot;,
                        &quot;BlobFolderPath&quot;: &quot;blobfolderpath&quot;,
                        &quot;BlobFileName&quot;: &quot;blobfile2&quot;
                    }
                ]
            }
        }
    },
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/pipelines&quot;
}
</code></pre>
<p>The issue I am having is that when specifying pipeline parameters, it seems I cannot use system variables and functions the same way I can when for example specifying folder paths for a blob storage dataset.
The consequence of this is that <code>formatDateTime(utcnow(), 'yyyyMMdd')</code> is not being interpreted as function calls but rather the actual string with value <code>formatDateTime(utcnow(), 'yyyyMMdd')</code>.</p>
<p>To counter this I am guessing I should be using a trigger to execute my pipeline and pass the trigger's execution time as a parameter to the pipeline like <code>trigger().startTime</code> but is this the only way? Am I simply doing something wrong in my pipeline's JSON?</p>
","<azure><azure-data-factory>","2018-06-22 11:15:50","31650","6","3","50987841","<p>Default value of parameters cannot be expressions. They must be literal strings. 
You could use trigger to achieve this. Or you could extract the common part of your expressions and just put literal values into the foreach items.</p>
"
"50986738","Azure Data Factory v2 using utcnow() as a pipeline parameter","<p>For context, I currently have a Data Factory v2 pipeline with a <strong>ForEach Activity</strong> that calls a <strong>Copy Activity</strong>. The <strong>Copy Activity</strong> simply copies data from an FTP server to a blob storage container.</p>
<p>Here is the pipeline json file :</p>
<pre><code>{
    &quot;name&quot;: &quot;pipeline1&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;ForEach1&quot;,
                &quot;type&quot;: &quot;ForEach&quot;,
                &quot;typeProperties&quot;: {
                    &quot;items&quot;: {
                        &quot;value&quot;: &quot;@pipeline().parameters.InputParams&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    },
                    &quot;isSequential&quot;: true,
                    &quot;activities&quot;: [
                        {
                            &quot;name&quot;: &quot;Copy1&quot;,
                            &quot;type&quot;: &quot;Copy&quot;,
                            &quot;policy&quot;: {
                                &quot;timeout&quot;: &quot;7.00:00:00&quot;,
                                &quot;retry&quot;: 0,
                                &quot;retryIntervalInSeconds&quot;: 30,
                                &quot;secureOutput&quot;: false
                            },
                            &quot;typeProperties&quot;: {
                                &quot;source&quot;: {
                                    &quot;type&quot;: &quot;FileSystemSource&quot;,
                                    &quot;recursive&quot;: true
                                },
                                &quot;sink&quot;: {
                                    &quot;type&quot;: &quot;BlobSink&quot;
                                },
                                &quot;enableStaging&quot;: false,
                                &quot;cloudDataMovementUnits&quot;: 0
                            },
                            &quot;inputs&quot;: [
                                {
                                    &quot;referenceName&quot;: &quot;FtpDataset&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;,
                                    &quot;parameters&quot;: {
                                        &quot;FtpFileName&quot;: &quot;@item().FtpFileName&quot;,
                                        &quot;FtpFolderPath&quot;: &quot;@item().FtpFolderPath&quot;
                                    }
                                }
                            ],
                            &quot;outputs&quot;: [
                                {
                                    &quot;referenceName&quot;: &quot;BlobDataset&quot;,
                                    &quot;type&quot;: &quot;DatasetReference&quot;,
                                    &quot;parameters&quot;: {
                                        &quot;BlobFileName&quot;: &quot;@item().BlobFileName&quot;,
                                        &quot;BlobFolderPath&quot;: &quot;@item().BlobFolderPath&quot;
                                    }
                                }
                            ]
                        }
                    ]
                }
            }
        ],
        &quot;parameters&quot;: {
            &quot;InputParams&quot;: {
                &quot;type&quot;: &quot;Array&quot;,
                &quot;defaultValue&quot;: [
                    {
                        &quot;FtpFolderPath&quot;: &quot;/Folder1/&quot;,
                        &quot;FtpFileName&quot;: &quot;@concat('File_',formatDateTime(utcnow(), 'yyyyMMdd'), '.txt')&quot;,
                        &quot;BlobFolderPath&quot;: &quot;blobfolderpath&quot;,
                        &quot;BlobFileName&quot;: &quot;blobfile1&quot;
                    },
                    {
                        &quot;FtpFolderPath&quot;: &quot;/Folder2/&quot;,
                        &quot;FtpFileName&quot;: &quot;@concat('File_',formatDateTime(utcnow(), 'yyyyMMdd'), '.txt')&quot;,
                        &quot;BlobFolderPath&quot;: &quot;blobfolderpath&quot;,
                        &quot;BlobFileName&quot;: &quot;blobfile2&quot;
                    }
                ]
            }
        }
    },
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/pipelines&quot;
}
</code></pre>
<p>The issue I am having is that when specifying pipeline parameters, it seems I cannot use system variables and functions the same way I can when for example specifying folder paths for a blob storage dataset.
The consequence of this is that <code>formatDateTime(utcnow(), 'yyyyMMdd')</code> is not being interpreted as function calls but rather the actual string with value <code>formatDateTime(utcnow(), 'yyyyMMdd')</code>.</p>
<p>To counter this I am guessing I should be using a trigger to execute my pipeline and pass the trigger's execution time as a parameter to the pipeline like <code>trigger().startTime</code> but is this the only way? Am I simply doing something wrong in my pipeline's JSON?</p>
","<azure><azure-data-factory>","2018-06-22 11:15:50","31650","6","3","56820529","<p>This should work:
<code>File_@{formatDateTime(utcnow(), 'yyyyMMdd')}</code></p>

<p>Or complex paths as well:</p>

<pre><code>rootfolder/subfolder/@{formatDateTime(utcnow(),'yyyy')}/@{formatDateTime(utcnow(),'MM')}/@{formatDateTime(utcnow(),'dd')}/@{formatDateTime(utcnow(),'HH')}
</code></pre>
"
"50983371","Create Azure Data Factory v2 withot AD app registration","<p>Is there any way to create Azure Data Factory v2 client without create AD app in azure?</p>
","<azure><azure-data-factory>","2018-06-22 08:01:40","490","0","1","50990260","<p>Yes. You could do this with SDK or restful API</p>

<p>Data factory service identity is generated as follows:</p>

<ol>
<li>When creating data factory through Azure portal or PowerShell,
service identity will always be created automatically since ADF V2
public preview.</li>
<li>When creating data factory through SDK, service
identity <strong>will be created only</strong> if you specify ""Identity = new
FactoryIdentity()"" in the factory object for creation. See example
in .NET quickstart - create data factory. </li>
<li>When creating data factory through REST API, service identity <strong>will be created only</strong> if you specify ""identity"" section in request body. See example in REST quickstart - create data factory.</li>
</ol>

<p>Please reference <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-factory-service-identity"" rel=""nofollow noreferrer"">this doc.</a></p>
"
"50968739","Adding additional column in data being saved in cosmosDB by Azure data factory's copy activity","<p>I am using azure data factory's copy activity to copy data from a csv file in blob to CosmosDB(with SQL API). In the Sink's linked service if I do not import any schema , my copy activity on execution reads headers from CSV and then saves the data in json form in cosmosDB. Till here it works fine.</p>

<p>I need to add a batch id column in the data being added in cosmosDB (batch id as GUID / pipelinerunID) so that I can track which all data in a set was copied as batch. </p>

<p>How can I keep all my source columns and add my batch id column in it and save it in my cosmos DB.</p>

<p>The schema is not fixed and can change on each adf pipeline trigger so cannot do import schema and do one o one column mapping in copy activity.</p>
","<copy><azure-cosmosdb><azure-data-factory>","2018-06-21 12:35:53","2547","2","1","50984584","<p>Per my knowledge, you can't add custom column when you transfer data from csv to cosmos db. I suggest you using <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-cosmosdb-v2"" rel=""nofollow noreferrer"">Azure Function Cosmos DB Trigger</a> to add batchId when the document created into database as workaround.</p>

<pre><code>#r ""Microsoft.Azure.Documents.Client""
#r ""Newtonsoft.Json""
#r ""Microsoft.Azure.DocumentDB.Core""
using System;
using System.Collections.Generic;
using Microsoft.Azure.Documents;
using Newtonsoft.Json;
using Newtonsoft.Json.Linq;
using Microsoft.Azure.Documents.Client;a

public static void Run(IReadOnlyList&lt;Document&gt; documents, TraceWriter log)
{
    if (documents != null &amp;&amp; documents.Count &gt; 0)
    {
        private static readonly string endpointUrl = ""https://***.documents.azure.com:443/"";
        private static readonly string authorizationKey = ""***"";
        private static readonly string databaseId = ""db"";
        private static readonly string collectionId = ""coll"";

        private static DocumentClient client;

        documents[0].SetPropertyValue(""batchId"",""123"");

        var document = client.ReplaceDocumentAsync(UriFactory.CreateDocumentUri(databaseId, collectionId, documents[0].id), documents[0]).Result.Resource;
        log.Verbose(""document Id "" + documents[0].Id);
    }
}
</code></pre>

<p>However, it seems that you need to specify the <code>batchId</code> by yourself which can't match the <code>batchId</code> in the azure data factory.</p>

<p>Hope it helps you.</p>
"
"50965280","Copy Data from Azure DWH TO Dynamics 365 in Azure","<p>it is possible to copy CRM data from Dynamics 365 to a DWH with the Azure Datafactory. But is it possible to, for example, create new Data in the Table that was copied from Dynamics 365 and copie that Table back to Dynamics 365? </p>

<p>Export: Dynamics to Blob to DWH 
then create new Data in Table in DWH
Import: DWH to Blob to Dynamics 365 (CRM)</p>
","<azure><dynamics-crm><azure-data-factory>","2018-06-21 09:38:10","121","0","1","50966222","<p>ADF does support upsetting data to Dynamics 365 CRM. 
Please try and let us know if any specific problem you hit.</p>
"
"50955323","Azure data factory SQL Server connection error code 9056","<p>Does anybody now what this error code means? I'm getting this error when trying to connect to an on-premise SQL Server from Azure data factory</p>
","<azure><azure-data-factory>","2018-06-20 19:02:22","745","0","1","50959117","<p>There's an incident around the the time you posted, and it has been mitigated now.</p>

<p>According to the <a href=""https://azure.microsoft.com/en-us/status/history/"" rel=""nofollow noreferrer"">Azure status history</a>, can you manually restart your self-hosted Integration Runtime and try again?</p>
"
"50932479","Error while trying to read file in Data Lake storage","<p>In my Azure Data Lake Store I seek to read a file that I imported using a pipeline in Azure Data Factory 2.</p>
<p>Although I am logged in with the same credentials that I used to create the Data Factory, the App Registration for the Data Factory, and the Data Lake itself, I get the following error message:</p>
<blockquote>
<p>ERROR: AccessControlException</p>
<p>MESSAGE: OPEN failed with error 0x83090aa2 (Forbidden. ACL verification failed. Either the resource does not exist or the user is not authorized to perform the requested operation.). [1a8ca11b-d726-468a-9aeb-d8ef3d93a81d] failed with error 0x83090aa2 (Forbidden. ACL verification failed. Either the resource does not exist or the user is not authorized to perform the requested operation.). [1a8ca11b-d726-468a-9aeb-d8ef3d93a81d][2018-06-19T07:45:23.8686252-07:00]</p>
</blockquote>
<p>My first thought was, this has obviously something to do with access permissions. So just out of curiousity I gave Read, Write and Execute access to 'Everyone else' in the Access page of the folder holding my file. Interestingly enough, the same error occurs.</p>
<p>Why?</p>
<p>The IR I use was autoselected during creation and is called 'AutoResolveIntegrationRuntime'.</p>
","<azure><acl><azure-data-lake><azure-data-factory>","2018-06-19 15:55:44","1607","1","1","50945976","<p>Please also check your fire wall settings.
<a href=""https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-secure-data"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-secure-data</a> (Set IP address range for data access)</p>

<p>Enabled fairwall and Allow access to Azure service ""ON""</p>

<p>You also need allow the IP of your selfhosted IR machine access your ADLS because the copy will happen on your selfhosted IR.</p>
"
"50929665","Azure Data Factory V2 Copy Activity file filters","<p>I am using Data Factory v2 and I currently have a simple copy activity which copies files from an FTP server to blob storage. The file names on this server are of the following form :</p>

<pre><code>File_{Year}{Month}{Day}.zip
</code></pre>

<p>In order to download the most recent file I add this filter to my input dataset json file :</p>

<pre><code>""fileName"": {
    ""value"": ""@concat('File_',formatDateTime(utcnow(), 'yyyyMMdd'), '.zip')"",
    ""type"": ""Expression""
}
</code></pre>

<p>I now want to be able to download yesterday's file which is possible using <code>adddays()</code>.</p>

<p>However I would like to be able to do this in the same copy activity and it seems that Data Factory v2 does not allow me to use the following kind of regular expression logic :</p>

<pre><code>@concat('File_',formatDateTime(utcnow(), 'yyyyMMdd'), '.zip') || @concat('File_', formatDateTime(adddays(utcnow(), -1), 'yyyyMMdd'), '.zip')
</code></pre>

<p>Is this possible or do I need a separate activity ?</p>

<p>It would seem strange to need a second activity since a Copy Activity can only take a single input but if the regex is simple enough, then multiple files are treated as a single input and if not then multiple files are treated as multiple inputs.</p>
","<azure><azure-data-factory>","2018-06-19 13:25:54","1929","0","2","50930720","<p>The '||' won't work since it will be evaluate as a single string.
But I can provided two solutions for this. </p>

<ol>
<li>using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger"" rel=""nofollow noreferrer"">a tumbling window</a> daily trigger and set the start time as yesterday. So it will trigger two pipeline run. </li>
<li>using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">Foreach activity</a> + copy activity. The foreach activity iterate an array to pass the yesterday and today to the copy activity.</li>
</ol>

<p>Btw, you could just use string interpolation expression instead of concat. They are the same.</p>

<pre><code>File_@{formatDateTime(utcnow(), 'yyyyMMdd')}.zip
</code></pre>
"
"50929665","Azure Data Factory V2 Copy Activity file filters","<p>I am using Data Factory v2 and I currently have a simple copy activity which copies files from an FTP server to blob storage. The file names on this server are of the following form :</p>

<pre><code>File_{Year}{Month}{Day}.zip
</code></pre>

<p>In order to download the most recent file I add this filter to my input dataset json file :</p>

<pre><code>""fileName"": {
    ""value"": ""@concat('File_',formatDateTime(utcnow(), 'yyyyMMdd'), '.zip')"",
    ""type"": ""Expression""
}
</code></pre>

<p>I now want to be able to download yesterday's file which is possible using <code>adddays()</code>.</p>

<p>However I would like to be able to do this in the same copy activity and it seems that Data Factory v2 does not allow me to use the following kind of regular expression logic :</p>

<pre><code>@concat('File_',formatDateTime(utcnow(), 'yyyyMMdd'), '.zip') || @concat('File_', formatDateTime(adddays(utcnow(), -1), 'yyyyMMdd'), '.zip')
</code></pre>

<p>Is this possible or do I need a separate activity ?</p>

<p>It would seem strange to need a second activity since a Copy Activity can only take a single input but if the regex is simple enough, then multiple files are treated as a single input and if not then multiple files are treated as multiple inputs.</p>
","<azure><azure-data-factory>","2018-06-19 13:25:54","1929","0","2","50948377","<p>I would suggest you to read about get metadata activity. This can be helpful in your scenario I think.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity</a></p>

<p>You have itemName property, lastModified property, check it out.</p>
"
"50929417","Azure Data Factory - CSV to Parquet - Changing file extension","<p>I have created a Data Factory to convert a CSV file to Parquet format, as I needed to retain the orginial file name I am using the 'Preserve Hierarchy' at the pipeline. The conversion works fine but the output file is generated with the csv extension (an expected output). Is there any out of the box option I could use to generate the file name without the csv extension. I scanned through the system varaible currently supported by ADF and it doesn't list Input File name as an option to mask the file extension - <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables</a>. Is writing a custom component the only option? </p>

<p>Thanks for your inputs.  </p>
","<azure><azure-data-factory>","2018-06-19 13:13:20","5534","1","1","50939135","<p>You could use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">get metadata activity</a> to get the file name of your source dataset and then pass it to both input and output dataset of your copy activity if you are using azure data factory v2.</p>
"
"50925148","Azure Data Factory 2 Dynamic Mapping In Copy Activity UI","<p>I'm trying to create a pipeline in Azure Data Factory 2 that takes a CSV file for example and convert it to JSON format.</p>

<p>The problem is beside converting the file's format I also want to apply some conditions based on other fields.</p>

<p>For example:</p>

<p><strong>CSV:</strong>                                 </p>

<pre><code>Column1, Column2, Column3   
""SomeValue"", ""true"", ""N/A""
</code></pre>

<p><strong>JSON (Schema):</strong></p>

<pre><code>{""firstValue"": null, ""secondValue"": null}
</code></pre>

<p><strong>Now what I'm trying to achieve is the following mapping,</strong></p>

<pre><code>Column1 (CSV) **---&gt;** firstValue (JSON)     
Column1 == true || Column3 != ""N/A"" **---&gt;** secondValue (JSON)
</code></pre>

<p>Hope this example is clear enough, basically, I'm trying to achieve dynamic mapping through the UI, is it possible ?</p>
","<azure-data-factory>","2018-06-19 09:39:27","1551","0","1","50926779","<p>I think there is no easy way to implement this simply with a copy activity.
But you may achieve this with lookup activity + foreach activity + copy activity.
You could take a look of <a href=""https://stackoverflow.com/questions/50872649/how-to-copy-%D0%A1osmosdb-docs-to-blob-storage-each-doc-in-single-json-file-with-az"">this post</a>. You could provide a fake source dataset and just use a query to achieve your map.</p>
"
"50913621","Pipeline Order of Execution","<p>Say I have an ADF with pipelines that look as follows:</p>

<p><a href=""https://i.stack.imgur.com/7QNAc.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7QNAc.jpg"" alt=""enter image description here""></a></p>

<p>Will State, Asset and BranchType all execute in parallel (depending on available resources)?</p>
","<azure-data-factory>","2018-06-18 16:14:56","101","0","1","50920982","<p>Yes.
You could click the debug button. You will find these three activities will be in-progress at the same time.</p>
"
"50902041","Connecting Azure Data Factory with InfluxDB","<p>I'm working on some time series data that I want migrated to cloud. Working in Australia and PIE is stopping me from using Time Series Insights, so I've decided to use InfluxDb as my Time Series database.
I've set up a Grafana VM on Azure and installed InfluxDB in it.
The task where I'm stuck is.
1. Import a csv file (with time series data) to blob storage using Azure Data Factory (Have done this)
<strong>2. Use ADF to transfer the files to InfluxDb. (Need help here)</strong>
3. Do cool stuff on the data (have nice people in the team who're experts in this task)</p>

<p>Need help in point 2. Appreciate you putting your time to help me.</p>

<p>Thanks</p>
","<azure><time-series><influxdb><grafana><azure-data-factory>","2018-06-18 02:31:48","1038","1","1","50903112","<p>Currently, ADF doesn't support influxdb as data source/sink.
Here is <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats"" rel=""nofollow noreferrer"">the list</a> ADF supported.</p>
"
"50900810","Azure Data Factory Disable Triggers On Release","<p>I've been trying to get Data Factory Deployments working through VSTS and am mostly there, but I'm getting a failure due to the triggers needing to be disabled to be overwritten. Error message below:</p>

<pre><code>Remove-AzureRmDataFactoryV2Trigger : HTTP Status Code: BadRequest
Error Code: TriggerEnabledCannotUpdate
Error Message: Cannot update enabled Trigger; it needs to be disabled first.
Request Id: &lt;id number here&gt;
Timestamp (Utc):06/17/2018 21:31:49
At line:1 char:1
+ Remove-AzureRmDataFactoryV2Trigger -ResourceGroupName ""ResourceGroupName"" -Data ...
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : CloseError: (:) [Remove-AzureRmDataFactoryV2Trigger], ErrorResponseException
    + FullyQualifiedErrorId : Microsoft.Azure.Commands.DataFactoryV2.RemoveAzureDataFactoryTriggerCommand
</code></pre>

<p>I get this error when both trying to do a straight deployment, but also when manually running a powershell script to remove the trigger</p>

<pre><code>Remove-AzureRmDataFactoryV2Trigger -ResourceGroupName ""ResourceGroupName"" -DataFactoryName ""DataFactoryName"" -Name ""TriggerName""
</code></pre>

<p>I cant find a way to disable a trigger via powershell or during a release. Can anyone help me find a way around this? Without it I cant do Continuous integration releases with the Data Factory.</p>

<p>Thanks</p>
","<powershell><azure-devops><azure-powershell><azure-data-factory>","2018-06-17 21:59:19","9020","6","1","50901654","<p>Call Stop-AzureRmDataFactoryV2Trigger before removing it.</p>

<p>Iterate round all defined triggers and set to variable</p>

<pre><code>$triggersADF = Get-AzureRmDataFactoryV2Trigger -DataFactoryName &lt;DataFactoryName&gt; -ResourceGroupName &lt;ResourceGroupName&gt;
</code></pre>

<p>Disable all triggers</p>

<pre><code>$triggersADF | ForEach-Object { Stop-AzureRmDataFactoryV2Trigger -ResourceGroupName &lt;ResourceGroupName&gt; -DataFactoryName &lt;DataFactoryName&gt; -Name $_.name -Force }
</code></pre>

<p>re-enable triggers post deployment</p>

<pre><code>$triggersADF | ForEach-Object { Start-AzureRmDataFactoryV2Trigger -ResourceGroupName &lt;ResourceGroupName&gt; -DataFactoryName &lt;DataFactoryName&gt; -Name $_.name -Force }
</code></pre>
"
"50887660","Azure Data lake analysis job failed reading data from Data lake store","<p>I have a CSV file copied from Azure blob to Azure data lake store. The pipe line is established successfully and file copied.</p>

<p>I'm trying to write USQL sample script from here: </p>

<blockquote>
  <p>Home -> datalakeanalysis1->Sample scripts-> New job</p>
</blockquote>

<p>Its showing me default script.</p>

<pre><code>//Define schema of file, must map all columns
@searchlog = 
    EXTRACT UserId          int, 
        Start           DateTime, 
        Region          string, 
        Query           string, 
        Duration        int, 
        Urls            string, 
        ClickedUrls     string
FROM @""/Samples/Data/SearchLog.tsv""
USING Extractors.Tsv();

OUTPUT @searchlog 
TO @""/Samples/Output/SearchLog_output.tsv""
USING Outputters.Tsv();
</code></pre>

<p>Note: my file in data lake store is here:</p>

<blockquote>
  <p>Home->dls1->Data explorer->rdl1</p>
</blockquote>

<p>How can I give the path of my CSV file in the script ( my CSV file is stored in Data Lake Store).</p>

<p>Also, I would like to keep my destination file(output) in Data lake store. </p>

<p>How can I modify my script to refer to the data lake store path?</p>

<p>Edit:</p>

<p>I have changed my script as below:</p>

<pre><code>//Define schema of file, must map all columns
@searchlog = 
EXTRACT ID1          int, 
        ID2           int, 
        Date          DateTime, 
        Rs          string, 
        Rs1        string, 
        Number            string, 
        Direction     string,
        ID3            int
FROM @""adl://rdl1.azuredatalakestore.net/blob1/vehicle1_09142014_JR.csv""
USING Extractors.Csv();

OUTPUT @searchlog 
TO @""adl://rdl1.azuredatalakestore.net/blob1/vehicle1_09142014_JR1.csv""
USING Outputters.Csv();
</code></pre>

<p>However, my job is getting failed with attached error:</p>

<p><a href=""https://i.stack.imgur.com/Wt1xW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Wt1xW.jpg"" alt=""enter image description here""></a></p>

<p>Moreover, I'm attaching the CSV file that I wanted to be used in the job.
<a href=""https://drive.google.com/file/d/1wCW9_QPWrN0a05nxeyeVE-1lFC-p2Tp1/view?usp=sharing"" rel=""nofollow noreferrer"">Sample CSV file</a></p>

<p>Is there anything wrong in the CSV file ? Or in my script??
Please help. Thanks.</p>
","<azure><azure-data-factory><azure-data-lake><u-sql>","2018-06-16 11:51:01","261","0","2","50905106","<p>I followed your steps and reproduce your issue.</p>

<p><a href=""https://i.stack.imgur.com/pk493.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pk493.png"" alt=""enter image description here""></a></p>

<p>My sample data:</p>

<pre><code>ID1,ID2,Date,Rs,Rs1,Number,Direction,ID3
1,1,9/14/2014 0:00,46.81006,-92.08174,51,S,1
1,2,9/14/2014 0:00,46.81006,-92.08174,13,NE,1
1,3,9/14/2014 0:00,46.81006,-92.08174,48,NE,1
1,4,9/14/2014 0:00,46.81006,-92.08174,30,W,1
</code></pre>

<p>Based on the error log, I found it can't parse the title row.So, I removed the title row and everything works fine.</p>

<p>Modified data:</p>

<pre><code>1,1,9/14/2014 0:00,46.81006,-92.08174,51,S,1
1,2,9/14/2014 0:00,46.81006,-92.08174,13,NE,1
1,3,9/14/2014 0:00,46.81006,-92.08174,48,NE,1
1,4,9/14/2014 0:00,46.81006,-92.08174,30,W,1
</code></pre>

<p>Usql script :</p>

<pre><code>//Define schema of file, must map all columns
@searchlog = 
    EXTRACT ID1          int, 
            ID2           int, 
            Date          DateTime, 
            Rs          string, 
            Rs1        string, 
            Number            string, 
            Direction     string,
            ID3            int
    FROM @""/test/data.csv""
    USING Extractors.Csv();

OUTPUT @searchlog 
    TO @""/testOutput/dataOutput.csv""
    USING Outputters.Csv();
</code></pre>

<p>Output:</p>

<p><a href=""https://i.stack.imgur.com/bZl2A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bZl2A.png"" alt=""enter image description here""></a></p>

<p>Hope it helps you.</p>
"
"50887660","Azure Data lake analysis job failed reading data from Data lake store","<p>I have a CSV file copied from Azure blob to Azure data lake store. The pipe line is established successfully and file copied.</p>

<p>I'm trying to write USQL sample script from here: </p>

<blockquote>
  <p>Home -> datalakeanalysis1->Sample scripts-> New job</p>
</blockquote>

<p>Its showing me default script.</p>

<pre><code>//Define schema of file, must map all columns
@searchlog = 
    EXTRACT UserId          int, 
        Start           DateTime, 
        Region          string, 
        Query           string, 
        Duration        int, 
        Urls            string, 
        ClickedUrls     string
FROM @""/Samples/Data/SearchLog.tsv""
USING Extractors.Tsv();

OUTPUT @searchlog 
TO @""/Samples/Output/SearchLog_output.tsv""
USING Outputters.Tsv();
</code></pre>

<p>Note: my file in data lake store is here:</p>

<blockquote>
  <p>Home->dls1->Data explorer->rdl1</p>
</blockquote>

<p>How can I give the path of my CSV file in the script ( my CSV file is stored in Data Lake Store).</p>

<p>Also, I would like to keep my destination file(output) in Data lake store. </p>

<p>How can I modify my script to refer to the data lake store path?</p>

<p>Edit:</p>

<p>I have changed my script as below:</p>

<pre><code>//Define schema of file, must map all columns
@searchlog = 
EXTRACT ID1          int, 
        ID2           int, 
        Date          DateTime, 
        Rs          string, 
        Rs1        string, 
        Number            string, 
        Direction     string,
        ID3            int
FROM @""adl://rdl1.azuredatalakestore.net/blob1/vehicle1_09142014_JR.csv""
USING Extractors.Csv();

OUTPUT @searchlog 
TO @""adl://rdl1.azuredatalakestore.net/blob1/vehicle1_09142014_JR1.csv""
USING Outputters.Csv();
</code></pre>

<p>However, my job is getting failed with attached error:</p>

<p><a href=""https://i.stack.imgur.com/Wt1xW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Wt1xW.jpg"" alt=""enter image description here""></a></p>

<p>Moreover, I'm attaching the CSV file that I wanted to be used in the job.
<a href=""https://drive.google.com/file/d/1wCW9_QPWrN0a05nxeyeVE-1lFC-p2Tp1/view?usp=sharing"" rel=""nofollow noreferrer"">Sample CSV file</a></p>

<p>Is there anything wrong in the CSV file ? Or in my script??
Please help. Thanks.</p>
","<azure><azure-data-factory><azure-data-lake><u-sql>","2018-06-16 11:51:01","261","0","2","50914093","<p>I believe that while extracting data from the file you can pass in some additional parameters to ignore the header row </p>

<blockquote>
  <p><a href=""https://msdn.microsoft.com/en-us/azure/data-lake-analytics/u-sql/extractor-parameters-u-sql#skipFirstNRows"" rel=""nofollow noreferrer"">https://msdn.microsoft.com/en-us/azure/data-lake-analytics/u-sql/extractor-parameters-u-sql#skipFirstNRows</a></p>
</blockquote>

<pre><code>@searchlog = 
EXTRACT ID1          int, 
        ID2           int, 
        Date          DateTime, 
        Rs          string, 
        Rs1        string, 
        Number            string, 
        Direction     string,
        ID3            int
FROM @""adl://rdl1.azuredatalakestore.net/blob1/vehicle1_09142014_JR.csv""
USING Extractors.Csv(skipFirstNRows:1);
</code></pre>

<p>Modifying the input file may or may not be possible in all scenarios specially if the input file is being dropped by stakeholders that you cannot control.</p>
"
"50876717","Data Factory copy activity and ZipDeflate throughput low","<p>I currently have a pipeline comprised of a simple Copy activity which fetches zipped files from a third party's FTP server, unzips the files and copies them to a blob storage container.</p>

<p>The throughput is incredibly low (7 KB/s) and simply copying the files without unzipping does not pose any issue (700 KB/s which is in line with regular download speed from this FTP server using FileZilla for example).</p>

<p>For the Data Factory team the run id is : 825e15a9-aba6-47ed-8656-88c9b6bc3754</p>

<p>Some names in the following datasets have been modified.</p>

<p>Input dataset :
<code>
{
    ""name"": ""InputDataset"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""3PPFtpServer"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""FileShare"",
        ""typeProperties"": {
            ""compression"": {
                ""type"": ""ZipDeflate"",
                ""level"": ""Fastest""
            },
            ""fileName"": ""sample_file.zip"",
            ""folderPath"": ""/dir1/dir2/""
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></p>

<p>Output dataset :
<code>
{
    ""name"": ""OutputDataset"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureStorageLinkedService"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""AzureBlob"",
        ""typeProperties"": {
            ""fileName"": """",
            ""folderPath"": ""test-output/""
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></p>

<p>Am I getting something wrong regarding tuning my decompression ?</p>
","<ftp><azure-blob-storage><azure-data-factory>","2018-06-15 13:34:58","1230","1","1","50899984","<p>Are you accessing the ftp server with a self-hosted integration runtime? Or is it an Azure IR?</p>

<p>If it is self-hosted, chances are that your processing power is not enough to unzip and upload the file faster, so the best plan for this is to upload the zipped file, and then decompress it in the Azure Storage, so that it unzips the file in the cloud. You will have 2 copy activities and 3 datasets:</p>

<p>Activity 1: copy compressed file from ftp to Azure Storage. Datasets will be as you have now but the output dataset will use the same type of compression as the input.
Activity 2: copy decompressed file from Azure Storage to Azure Storage (another folder of course). The input dataset will be the same as Activity1's output, and the output will be the dataset that you are showing in your question.</p>

<p>Hope this helped!</p>
"
"50872649","How to copy CosmosDb docs to Blob storage (each doc in single json file) with Azure Data Factory","<p>I'm trying to backup my Cosmos Db storage using Azure Data Factory(v2). In general, it's doing its job, but I want to have each doc in Cosmos collection to correspond new json file in blobs storage.</p>

<p>With next copying params i'm able to copy all docs in collection into 1 file in azure blob storage:</p>

<pre><code>{
""name"": ""ForEach_mih"",
""type"": ""ForEach"",
""typeProperties"": {
    ""items"": {
        ""value"": ""@pipeline().parameters.cw_items"",
        ""type"": ""Expression""
    },
    ""activities"": [
        {
            ""name"": ""Copy_mih"",
            ""type"": ""Copy"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false
            },
            ""userProperties"": [
                {
                    ""name"": ""Source"",
                    ""value"": ""@{item().source.collectionName}""
                },
                {
                    ""name"": ""Destination"",
                    ""value"": ""cosmos-backup-v2/@{item().destination.fileName}""
                }
            ],
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""DocumentDbCollectionSource"",
                    ""nestingSeparator"": "".""
                },
                ""sink"": {
                    ""type"": ""BlobSink""
                },
                ""enableStaging"": false,
                ""enableSkipIncompatibleRow"": true,
                ""redirectIncompatibleRowSettings"": {
                    ""linkedServiceName"": {
                        ""referenceName"": ""Clear_Test_BlobStorage"",
                        ""type"": ""LinkedServiceReference""
                    },
                    ""path"": ""cosmos-backup-logs""
                },
                ""cloudDataMovementUnits"": 0
            },
            ""inputs"": [
                {
                    ""referenceName"": ""SourceDataset_mih"",
                    ""type"": ""DatasetReference"",
                    ""parameters"": {
                        ""cw_collectionName"": ""@item().source.collectionName""
                    }
                }
            ],
            ""outputs"": [
                {
                    ""referenceName"": ""DestinationDataset_mih"",
                    ""type"": ""DatasetReference"",
                    ""parameters"": {
                        ""cw_fileName"": ""@item().destination.fileName""
                    }
                }
            ]
        }
    ]
}
}
</code></pre>

<p>How I can copy each cosmos doc to separate file and give it name the as {PartitionId}-{docId}?</p>

<p><strong>UPD</strong></p>

<p>Source set code:</p>

<pre><code>{
""name"": ""ClustersData"",
""properties"": {
    ""linkedServiceName"": {
        ""referenceName"": ""Clear_Test_CosmosDb"",
        ""type"": ""LinkedServiceReference""
    },
    ""type"": ""DocumentDbCollection"",
    ""typeProperties"": {
        ""collectionName"": ""directory-clusters""
    }
}
}
</code></pre>

<p>Destination set code: </p>

<pre><code>{
""name"": ""OutputClusters"",
""properties"": {
    ""linkedServiceName"": {
        ""referenceName"": ""Clear_Test_BlobStorage"",
        ""type"": ""LinkedServiceReference""
    },
    ""type"": ""AzureBlob"",
    ""typeProperties"": {
        ""format"": {
            ""type"": ""JsonFormat"",
            ""filePattern"": ""arrayOfObjects""
        },
        ""fileName"": """",
        ""folderPath"": ""cosmos-backup-logs""
    }
}
}
</code></pre>

<p>Pipeline code:</p>

<pre><code>{
""name"": ""copy-clsts"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""LookupClst"",
            ""type"": ""Lookup"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false
            },
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""DocumentDbCollectionSource"",
                    ""nestingSeparator"": "".""
                },
                ""dataset"": {
                    ""referenceName"": ""ClustersData"",
                    ""type"": ""DatasetReference""
                },
                ""firstRowOnly"": false
            }
        },
        {
            ""name"": ""ForEachClst"",
            ""type"": ""ForEach"",
            ""dependsOn"": [
                {
                    ""activity"": ""LookupClst"",
                    ""dependencyConditions"": [
                        ""Succeeded""
                    ]
                }
            ],
            ""typeProperties"": {
                ""items"": {
                    ""value"": ""@activity('LookupClst').output.value"",
                    ""type"": ""Expression""
                },
                ""batchCount"": 8,
                ""activities"": [
                    {
                        ""name"": ""CpyClst"",
                        ""type"": ""Copy"",
                        ""policy"": {
                            ""timeout"": ""7.00:00:00"",
                            ""retry"": 0,
                            ""retryIntervalInSeconds"": 30,
                            ""secureOutput"": false
                        },
                        ""typeProperties"": {
                            ""source"": {
                                ""type"": ""DocumentDbCollectionSource"",
                                ""query"": ""select @{item()}"",
                                ""nestingSeparator"": "".""
                            },
                            ""sink"": {
                                ""type"": ""BlobSink""
                            },
                            ""enableStaging"": false,
                            ""enableSkipIncompatibleRow"": true,
                            ""cloudDataMovementUnits"": 0
                        },
                        ""inputs"": [
                            {
                                ""referenceName"": ""ClustersData"",
                                ""type"": ""DatasetReference""
                            }
                        ],
                        ""outputs"": [
                            {
                                ""referenceName"": ""OutputClusters"",
                                ""type"": ""DatasetReference""
                            }
                        ]
                    }
                ]
            }
        }
    ]
}
}
</code></pre>

<p>Example of doc in input collection (all the same format):</p>

<pre><code>{
   ""$type"": ""Entities.ADCluster"",
    ""DisplayName"": ""TESTNetBIOS"",
    ""OrgId"": ""9b679d2a-42c5-4c9a-a2e2-3ce63c1c3506"",
    ""ClusterId"": ""ab2a242d-f1a5-62ed-b420-31b52e958586"",
    ""AllowLdapLifeCycleSynchronization"": true,
    ""DirectoryServers"": [
        {
            ""$type"": ""Entities.DirectoryServer"",
            ""AddressId"": ""e6a8edbb-ad56-4135-94af-fab50b774256"",
            ""Port"": 389,
            ""Host"": ""192.168.342.234""
        }
    ],
    ""DomainNames"": [
        ""TESTNetBIOS""
    ],
    ""BaseDn"": null,
    ""UseSsl"": false,
    ""RepositoryType"": 1,
    ""DirectoryCustomizations"": null,
    ""_etag"": ""\""140046f2-0000-0000-0000-5ac63a180000\"""",
    ""LastUpdateTime"": ""2018-04-05T15:00:40.243Z"",
    ""id"": ""ab2a242d-f1a5-62ed-b420-31b52e958586"",
    ""PartitionKey"": ""directory-clusters-9b679d2a-42c5-4c9a-a2e2-3ce63c1c3506"",
    ""_rid"": ""kpvxLAs6gkmsCQAAAAAAAA=="",
    ""_self"": ""dbs/kvpxAA==/colls/kpvxLAs6gkk=/docs/kvpxALs6kgmsCQAAAAAAAA==/"",
    ""_attachments"": ""attachments/"",
    ""_ts"": 1522940440
}
</code></pre>
","<azure><azure-cosmosdb><azure-data-factory>","2018-06-15 09:25:02","2989","4","4","50877140","<p>Just take one collection as an example.
<a href=""https://i.stack.imgur.com/54KgB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/54KgB.png"" alt=""enter image description here""></a></p>

<p>And inside the foreach:
<a href=""https://i.stack.imgur.com/RW2gd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RW2gd.png"" alt=""enter image description here""></a></p>

<p>And your lookup and copy activity source dataset reference the same cosmosdb dataset.</p>

<p>If you want to copy your 5 collections, you could put this pipeline into an execute activity. And the master pipeline of the execute activity has a foreach activity.</p>
"
"50872649","How to copy CosmosDb docs to Blob storage (each doc in single json file) with Azure Data Factory","<p>I'm trying to backup my Cosmos Db storage using Azure Data Factory(v2). In general, it's doing its job, but I want to have each doc in Cosmos collection to correspond new json file in blobs storage.</p>

<p>With next copying params i'm able to copy all docs in collection into 1 file in azure blob storage:</p>

<pre><code>{
""name"": ""ForEach_mih"",
""type"": ""ForEach"",
""typeProperties"": {
    ""items"": {
        ""value"": ""@pipeline().parameters.cw_items"",
        ""type"": ""Expression""
    },
    ""activities"": [
        {
            ""name"": ""Copy_mih"",
            ""type"": ""Copy"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false
            },
            ""userProperties"": [
                {
                    ""name"": ""Source"",
                    ""value"": ""@{item().source.collectionName}""
                },
                {
                    ""name"": ""Destination"",
                    ""value"": ""cosmos-backup-v2/@{item().destination.fileName}""
                }
            ],
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""DocumentDbCollectionSource"",
                    ""nestingSeparator"": "".""
                },
                ""sink"": {
                    ""type"": ""BlobSink""
                },
                ""enableStaging"": false,
                ""enableSkipIncompatibleRow"": true,
                ""redirectIncompatibleRowSettings"": {
                    ""linkedServiceName"": {
                        ""referenceName"": ""Clear_Test_BlobStorage"",
                        ""type"": ""LinkedServiceReference""
                    },
                    ""path"": ""cosmos-backup-logs""
                },
                ""cloudDataMovementUnits"": 0
            },
            ""inputs"": [
                {
                    ""referenceName"": ""SourceDataset_mih"",
                    ""type"": ""DatasetReference"",
                    ""parameters"": {
                        ""cw_collectionName"": ""@item().source.collectionName""
                    }
                }
            ],
            ""outputs"": [
                {
                    ""referenceName"": ""DestinationDataset_mih"",
                    ""type"": ""DatasetReference"",
                    ""parameters"": {
                        ""cw_fileName"": ""@item().destination.fileName""
                    }
                }
            ]
        }
    ]
}
}
</code></pre>

<p>How I can copy each cosmos doc to separate file and give it name the as {PartitionId}-{docId}?</p>

<p><strong>UPD</strong></p>

<p>Source set code:</p>

<pre><code>{
""name"": ""ClustersData"",
""properties"": {
    ""linkedServiceName"": {
        ""referenceName"": ""Clear_Test_CosmosDb"",
        ""type"": ""LinkedServiceReference""
    },
    ""type"": ""DocumentDbCollection"",
    ""typeProperties"": {
        ""collectionName"": ""directory-clusters""
    }
}
}
</code></pre>

<p>Destination set code: </p>

<pre><code>{
""name"": ""OutputClusters"",
""properties"": {
    ""linkedServiceName"": {
        ""referenceName"": ""Clear_Test_BlobStorage"",
        ""type"": ""LinkedServiceReference""
    },
    ""type"": ""AzureBlob"",
    ""typeProperties"": {
        ""format"": {
            ""type"": ""JsonFormat"",
            ""filePattern"": ""arrayOfObjects""
        },
        ""fileName"": """",
        ""folderPath"": ""cosmos-backup-logs""
    }
}
}
</code></pre>

<p>Pipeline code:</p>

<pre><code>{
""name"": ""copy-clsts"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""LookupClst"",
            ""type"": ""Lookup"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false
            },
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""DocumentDbCollectionSource"",
                    ""nestingSeparator"": "".""
                },
                ""dataset"": {
                    ""referenceName"": ""ClustersData"",
                    ""type"": ""DatasetReference""
                },
                ""firstRowOnly"": false
            }
        },
        {
            ""name"": ""ForEachClst"",
            ""type"": ""ForEach"",
            ""dependsOn"": [
                {
                    ""activity"": ""LookupClst"",
                    ""dependencyConditions"": [
                        ""Succeeded""
                    ]
                }
            ],
            ""typeProperties"": {
                ""items"": {
                    ""value"": ""@activity('LookupClst').output.value"",
                    ""type"": ""Expression""
                },
                ""batchCount"": 8,
                ""activities"": [
                    {
                        ""name"": ""CpyClst"",
                        ""type"": ""Copy"",
                        ""policy"": {
                            ""timeout"": ""7.00:00:00"",
                            ""retry"": 0,
                            ""retryIntervalInSeconds"": 30,
                            ""secureOutput"": false
                        },
                        ""typeProperties"": {
                            ""source"": {
                                ""type"": ""DocumentDbCollectionSource"",
                                ""query"": ""select @{item()}"",
                                ""nestingSeparator"": "".""
                            },
                            ""sink"": {
                                ""type"": ""BlobSink""
                            },
                            ""enableStaging"": false,
                            ""enableSkipIncompatibleRow"": true,
                            ""cloudDataMovementUnits"": 0
                        },
                        ""inputs"": [
                            {
                                ""referenceName"": ""ClustersData"",
                                ""type"": ""DatasetReference""
                            }
                        ],
                        ""outputs"": [
                            {
                                ""referenceName"": ""OutputClusters"",
                                ""type"": ""DatasetReference""
                            }
                        ]
                    }
                ]
            }
        }
    ]
}
}
</code></pre>

<p>Example of doc in input collection (all the same format):</p>

<pre><code>{
   ""$type"": ""Entities.ADCluster"",
    ""DisplayName"": ""TESTNetBIOS"",
    ""OrgId"": ""9b679d2a-42c5-4c9a-a2e2-3ce63c1c3506"",
    ""ClusterId"": ""ab2a242d-f1a5-62ed-b420-31b52e958586"",
    ""AllowLdapLifeCycleSynchronization"": true,
    ""DirectoryServers"": [
        {
            ""$type"": ""Entities.DirectoryServer"",
            ""AddressId"": ""e6a8edbb-ad56-4135-94af-fab50b774256"",
            ""Port"": 389,
            ""Host"": ""192.168.342.234""
        }
    ],
    ""DomainNames"": [
        ""TESTNetBIOS""
    ],
    ""BaseDn"": null,
    ""UseSsl"": false,
    ""RepositoryType"": 1,
    ""DirectoryCustomizations"": null,
    ""_etag"": ""\""140046f2-0000-0000-0000-5ac63a180000\"""",
    ""LastUpdateTime"": ""2018-04-05T15:00:40.243Z"",
    ""id"": ""ab2a242d-f1a5-62ed-b420-31b52e958586"",
    ""PartitionKey"": ""directory-clusters-9b679d2a-42c5-4c9a-a2e2-3ce63c1c3506"",
    ""_rid"": ""kpvxLAs6gkmsCQAAAAAAAA=="",
    ""_self"": ""dbs/kvpxAA==/colls/kpvxLAs6gkk=/docs/kvpxALs6kgmsCQAAAAAAAA==/"",
    ""_attachments"": ""attachments/"",
    ""_ts"": 1522940440
}
</code></pre>
","<azure><azure-cosmosdb><azure-data-factory>","2018-06-15 09:25:02","2989","4","4","50896340","<p>Since your cosmosdb has array and ADF doesn't support serialize array for cosmos db, this is the workaround I can provide.</p>

<p>First, export all your document to json files with export json as-is (to blob or adls or file systems, any file storage). I think you already knows how to do it. In this way, each collection will have a json file.</p>

<p>Second, handle each json file, to exact each row in the file to a single file. </p>

<p>I only provide pipeline for step 2. You could use execute pipeline activity to chain step 1 and step 2. And you could even handle all the collections in step 2 with a foreach activity.</p>

<p>Pipeline json</p>

<pre><code>{
""name"": ""pipeline27"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""Lookup1"",
            ""type"": ""Lookup"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false
            },
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""BlobSource"",
                    ""recursive"": true
                },
                ""dataset"": {
                    ""referenceName"": ""AzureBlob7"",
                    ""type"": ""DatasetReference""
                },
                ""firstRowOnly"": false
            }
        },
        {
            ""name"": ""ForEach1"",
            ""type"": ""ForEach"",
            ""dependsOn"": [
                {
                    ""activity"": ""Lookup1"",
                    ""dependencyConditions"": [
                        ""Succeeded""
                    ]
                }
            ],
            ""typeProperties"": {
                ""items"": {
                    ""value"": ""@activity('Lookup1').output.value"",
                    ""type"": ""Expression""
                },
                ""activities"": [
                    {
                        ""name"": ""Copy1"",
                        ""type"": ""Copy"",
                        ""policy"": {
                            ""timeout"": ""7.00:00:00"",
                            ""retry"": 0,
                            ""retryIntervalInSeconds"": 30,
                            ""secureOutput"": false
                        },
                        ""typeProperties"": {
                            ""source"": {
                                ""type"": ""DocumentDbCollectionSource"",
                                ""query"": {
                                    ""value"": ""select @{item()}"",
                                    ""type"": ""Expression""
                                },
                                ""nestingSeparator"": "".""
                            },
                            ""sink"": {
                                ""type"": ""BlobSink""
                            },
                            ""enableStaging"": false,
                            ""cloudDataMovementUnits"": 0
                        },
                        ""inputs"": [
                            {
                                ""referenceName"": ""DocumentDbCollection1"",
                                ""type"": ""DatasetReference""
                            }
                        ],
                        ""outputs"": [
                            {
                                ""referenceName"": ""AzureBlob6"",
                                ""type"": ""DatasetReference"",
                                ""parameters"": {
                                    ""id"": {
                                        ""value"": ""@item().id"",
                                        ""type"": ""Expression""
                                    },
                                    ""PartitionKey"": {
                                        ""value"": ""@item().PartitionKey"",
                                        ""type"": ""Expression""
                                    }
                                }
                            }
                        ]
                    }
                ]
            }
        }
    ]
},
""type"": ""Microsoft.DataFactory/factories/pipelines""
</code></pre>

<p>}</p>

<p>dataset json for lookup</p>

<pre><code>   {
""name"": ""AzureBlob7"",
""properties"": {
    ""linkedServiceName"": {
        ""referenceName"": ""bloblinkedservice"",
        ""type"": ""LinkedServiceReference""
    },
    ""type"": ""AzureBlob"",
    ""typeProperties"": {
        ""format"": {
            ""type"": ""JsonFormat"",
            ""filePattern"": ""arrayOfObjects""
        },
        ""fileName"": ""cosmos.json"",
        ""folderPath"": ""aaa""
    }
},
""type"": ""Microsoft.DataFactory/factories/datasets""
</code></pre>

<p>}</p>

<p>Source dataset for copy. Actually, this dataset has no use. Just want to use it to host the query (select @{item()}</p>

<pre><code>{
""name"": ""DocumentDbCollection1"",
""properties"": {
    ""linkedServiceName"": {
        ""referenceName"": ""CosmosDB-r8c"",
        ""type"": ""LinkedServiceReference""
    },
    ""type"": ""DocumentDbCollection"",
    ""typeProperties"": {
        ""collectionName"": ""test""
    }
},
""type"": ""Microsoft.DataFactory/factories/datasets""
</code></pre>

<p>}</p>

<p>Destination dataset. With two parameters, it also addressed your file name request.</p>

<pre><code>{
""name"": ""AzureBlob6"",
""properties"": {
    ""linkedServiceName"": {
        ""referenceName"": ""AzureStorage-eastus"",
        ""type"": ""LinkedServiceReference""
    },
    ""parameters"": {
        ""id"": {
            ""type"": ""String""
        },
        ""PartitionKey"": {
            ""type"": ""String""
        }
    },
    ""type"": ""AzureBlob"",
    ""typeProperties"": {
        ""format"": {
            ""type"": ""JsonFormat"",
            ""filePattern"": ""setOfObjects""
        },
        ""fileName"": {
            ""value"": ""@{dataset().PartitionKey}-@{dataset().id}.json"",
            ""type"": ""Expression""
        },
        ""folderPath"": ""aaacosmos""
    }
},
""type"": ""Microsoft.DataFactory/factories/datasets""
</code></pre>

<p>}</p>

<p>please also note the limitation of Lookup activity:
The following data sources are supported for lookup. The maximum number of rows can be returned by Lookup activity is 5000, and up to 2MB in size. And currently the max duration for Lookup activity before timeout is one hour.</p>
"
"50872649","How to copy CosmosDb docs to Blob storage (each doc in single json file) with Azure Data Factory","<p>I'm trying to backup my Cosmos Db storage using Azure Data Factory(v2). In general, it's doing its job, but I want to have each doc in Cosmos collection to correspond new json file in blobs storage.</p>

<p>With next copying params i'm able to copy all docs in collection into 1 file in azure blob storage:</p>

<pre><code>{
""name"": ""ForEach_mih"",
""type"": ""ForEach"",
""typeProperties"": {
    ""items"": {
        ""value"": ""@pipeline().parameters.cw_items"",
        ""type"": ""Expression""
    },
    ""activities"": [
        {
            ""name"": ""Copy_mih"",
            ""type"": ""Copy"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false
            },
            ""userProperties"": [
                {
                    ""name"": ""Source"",
                    ""value"": ""@{item().source.collectionName}""
                },
                {
                    ""name"": ""Destination"",
                    ""value"": ""cosmos-backup-v2/@{item().destination.fileName}""
                }
            ],
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""DocumentDbCollectionSource"",
                    ""nestingSeparator"": "".""
                },
                ""sink"": {
                    ""type"": ""BlobSink""
                },
                ""enableStaging"": false,
                ""enableSkipIncompatibleRow"": true,
                ""redirectIncompatibleRowSettings"": {
                    ""linkedServiceName"": {
                        ""referenceName"": ""Clear_Test_BlobStorage"",
                        ""type"": ""LinkedServiceReference""
                    },
                    ""path"": ""cosmos-backup-logs""
                },
                ""cloudDataMovementUnits"": 0
            },
            ""inputs"": [
                {
                    ""referenceName"": ""SourceDataset_mih"",
                    ""type"": ""DatasetReference"",
                    ""parameters"": {
                        ""cw_collectionName"": ""@item().source.collectionName""
                    }
                }
            ],
            ""outputs"": [
                {
                    ""referenceName"": ""DestinationDataset_mih"",
                    ""type"": ""DatasetReference"",
                    ""parameters"": {
                        ""cw_fileName"": ""@item().destination.fileName""
                    }
                }
            ]
        }
    ]
}
}
</code></pre>

<p>How I can copy each cosmos doc to separate file and give it name the as {PartitionId}-{docId}?</p>

<p><strong>UPD</strong></p>

<p>Source set code:</p>

<pre><code>{
""name"": ""ClustersData"",
""properties"": {
    ""linkedServiceName"": {
        ""referenceName"": ""Clear_Test_CosmosDb"",
        ""type"": ""LinkedServiceReference""
    },
    ""type"": ""DocumentDbCollection"",
    ""typeProperties"": {
        ""collectionName"": ""directory-clusters""
    }
}
}
</code></pre>

<p>Destination set code: </p>

<pre><code>{
""name"": ""OutputClusters"",
""properties"": {
    ""linkedServiceName"": {
        ""referenceName"": ""Clear_Test_BlobStorage"",
        ""type"": ""LinkedServiceReference""
    },
    ""type"": ""AzureBlob"",
    ""typeProperties"": {
        ""format"": {
            ""type"": ""JsonFormat"",
            ""filePattern"": ""arrayOfObjects""
        },
        ""fileName"": """",
        ""folderPath"": ""cosmos-backup-logs""
    }
}
}
</code></pre>

<p>Pipeline code:</p>

<pre><code>{
""name"": ""copy-clsts"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""LookupClst"",
            ""type"": ""Lookup"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false
            },
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""DocumentDbCollectionSource"",
                    ""nestingSeparator"": "".""
                },
                ""dataset"": {
                    ""referenceName"": ""ClustersData"",
                    ""type"": ""DatasetReference""
                },
                ""firstRowOnly"": false
            }
        },
        {
            ""name"": ""ForEachClst"",
            ""type"": ""ForEach"",
            ""dependsOn"": [
                {
                    ""activity"": ""LookupClst"",
                    ""dependencyConditions"": [
                        ""Succeeded""
                    ]
                }
            ],
            ""typeProperties"": {
                ""items"": {
                    ""value"": ""@activity('LookupClst').output.value"",
                    ""type"": ""Expression""
                },
                ""batchCount"": 8,
                ""activities"": [
                    {
                        ""name"": ""CpyClst"",
                        ""type"": ""Copy"",
                        ""policy"": {
                            ""timeout"": ""7.00:00:00"",
                            ""retry"": 0,
                            ""retryIntervalInSeconds"": 30,
                            ""secureOutput"": false
                        },
                        ""typeProperties"": {
                            ""source"": {
                                ""type"": ""DocumentDbCollectionSource"",
                                ""query"": ""select @{item()}"",
                                ""nestingSeparator"": "".""
                            },
                            ""sink"": {
                                ""type"": ""BlobSink""
                            },
                            ""enableStaging"": false,
                            ""enableSkipIncompatibleRow"": true,
                            ""cloudDataMovementUnits"": 0
                        },
                        ""inputs"": [
                            {
                                ""referenceName"": ""ClustersData"",
                                ""type"": ""DatasetReference""
                            }
                        ],
                        ""outputs"": [
                            {
                                ""referenceName"": ""OutputClusters"",
                                ""type"": ""DatasetReference""
                            }
                        ]
                    }
                ]
            }
        }
    ]
}
}
</code></pre>

<p>Example of doc in input collection (all the same format):</p>

<pre><code>{
   ""$type"": ""Entities.ADCluster"",
    ""DisplayName"": ""TESTNetBIOS"",
    ""OrgId"": ""9b679d2a-42c5-4c9a-a2e2-3ce63c1c3506"",
    ""ClusterId"": ""ab2a242d-f1a5-62ed-b420-31b52e958586"",
    ""AllowLdapLifeCycleSynchronization"": true,
    ""DirectoryServers"": [
        {
            ""$type"": ""Entities.DirectoryServer"",
            ""AddressId"": ""e6a8edbb-ad56-4135-94af-fab50b774256"",
            ""Port"": 389,
            ""Host"": ""192.168.342.234""
        }
    ],
    ""DomainNames"": [
        ""TESTNetBIOS""
    ],
    ""BaseDn"": null,
    ""UseSsl"": false,
    ""RepositoryType"": 1,
    ""DirectoryCustomizations"": null,
    ""_etag"": ""\""140046f2-0000-0000-0000-5ac63a180000\"""",
    ""LastUpdateTime"": ""2018-04-05T15:00:40.243Z"",
    ""id"": ""ab2a242d-f1a5-62ed-b420-31b52e958586"",
    ""PartitionKey"": ""directory-clusters-9b679d2a-42c5-4c9a-a2e2-3ce63c1c3506"",
    ""_rid"": ""kpvxLAs6gkmsCQAAAAAAAA=="",
    ""_self"": ""dbs/kvpxAA==/colls/kpvxLAs6gkk=/docs/kvpxALs6kgmsCQAAAAAAAA==/"",
    ""_attachments"": ""attachments/"",
    ""_ts"": 1522940440
}
</code></pre>
","<azure><azure-cosmosdb><azure-data-factory>","2018-06-15 09:25:02","2989","4","4","50907183","<p>Have you considered implementing this in a different way using Azure Functions? ADF is designed for moving data in bulk from one place to another and only generates a single file per collection.</p>

<p>You could consider having an Azure Function that is triggered when documents are added / updated in your collection and have the Azure Function output the document to blob storage. This should scale well and would be relatively easy to implement.</p>
"
"50872649","How to copy CosmosDb docs to Blob storage (each doc in single json file) with Azure Data Factory","<p>I'm trying to backup my Cosmos Db storage using Azure Data Factory(v2). In general, it's doing its job, but I want to have each doc in Cosmos collection to correspond new json file in blobs storage.</p>

<p>With next copying params i'm able to copy all docs in collection into 1 file in azure blob storage:</p>

<pre><code>{
""name"": ""ForEach_mih"",
""type"": ""ForEach"",
""typeProperties"": {
    ""items"": {
        ""value"": ""@pipeline().parameters.cw_items"",
        ""type"": ""Expression""
    },
    ""activities"": [
        {
            ""name"": ""Copy_mih"",
            ""type"": ""Copy"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false
            },
            ""userProperties"": [
                {
                    ""name"": ""Source"",
                    ""value"": ""@{item().source.collectionName}""
                },
                {
                    ""name"": ""Destination"",
                    ""value"": ""cosmos-backup-v2/@{item().destination.fileName}""
                }
            ],
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""DocumentDbCollectionSource"",
                    ""nestingSeparator"": "".""
                },
                ""sink"": {
                    ""type"": ""BlobSink""
                },
                ""enableStaging"": false,
                ""enableSkipIncompatibleRow"": true,
                ""redirectIncompatibleRowSettings"": {
                    ""linkedServiceName"": {
                        ""referenceName"": ""Clear_Test_BlobStorage"",
                        ""type"": ""LinkedServiceReference""
                    },
                    ""path"": ""cosmos-backup-logs""
                },
                ""cloudDataMovementUnits"": 0
            },
            ""inputs"": [
                {
                    ""referenceName"": ""SourceDataset_mih"",
                    ""type"": ""DatasetReference"",
                    ""parameters"": {
                        ""cw_collectionName"": ""@item().source.collectionName""
                    }
                }
            ],
            ""outputs"": [
                {
                    ""referenceName"": ""DestinationDataset_mih"",
                    ""type"": ""DatasetReference"",
                    ""parameters"": {
                        ""cw_fileName"": ""@item().destination.fileName""
                    }
                }
            ]
        }
    ]
}
}
</code></pre>

<p>How I can copy each cosmos doc to separate file and give it name the as {PartitionId}-{docId}?</p>

<p><strong>UPD</strong></p>

<p>Source set code:</p>

<pre><code>{
""name"": ""ClustersData"",
""properties"": {
    ""linkedServiceName"": {
        ""referenceName"": ""Clear_Test_CosmosDb"",
        ""type"": ""LinkedServiceReference""
    },
    ""type"": ""DocumentDbCollection"",
    ""typeProperties"": {
        ""collectionName"": ""directory-clusters""
    }
}
}
</code></pre>

<p>Destination set code: </p>

<pre><code>{
""name"": ""OutputClusters"",
""properties"": {
    ""linkedServiceName"": {
        ""referenceName"": ""Clear_Test_BlobStorage"",
        ""type"": ""LinkedServiceReference""
    },
    ""type"": ""AzureBlob"",
    ""typeProperties"": {
        ""format"": {
            ""type"": ""JsonFormat"",
            ""filePattern"": ""arrayOfObjects""
        },
        ""fileName"": """",
        ""folderPath"": ""cosmos-backup-logs""
    }
}
}
</code></pre>

<p>Pipeline code:</p>

<pre><code>{
""name"": ""copy-clsts"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""LookupClst"",
            ""type"": ""Lookup"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false
            },
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""DocumentDbCollectionSource"",
                    ""nestingSeparator"": "".""
                },
                ""dataset"": {
                    ""referenceName"": ""ClustersData"",
                    ""type"": ""DatasetReference""
                },
                ""firstRowOnly"": false
            }
        },
        {
            ""name"": ""ForEachClst"",
            ""type"": ""ForEach"",
            ""dependsOn"": [
                {
                    ""activity"": ""LookupClst"",
                    ""dependencyConditions"": [
                        ""Succeeded""
                    ]
                }
            ],
            ""typeProperties"": {
                ""items"": {
                    ""value"": ""@activity('LookupClst').output.value"",
                    ""type"": ""Expression""
                },
                ""batchCount"": 8,
                ""activities"": [
                    {
                        ""name"": ""CpyClst"",
                        ""type"": ""Copy"",
                        ""policy"": {
                            ""timeout"": ""7.00:00:00"",
                            ""retry"": 0,
                            ""retryIntervalInSeconds"": 30,
                            ""secureOutput"": false
                        },
                        ""typeProperties"": {
                            ""source"": {
                                ""type"": ""DocumentDbCollectionSource"",
                                ""query"": ""select @{item()}"",
                                ""nestingSeparator"": "".""
                            },
                            ""sink"": {
                                ""type"": ""BlobSink""
                            },
                            ""enableStaging"": false,
                            ""enableSkipIncompatibleRow"": true,
                            ""cloudDataMovementUnits"": 0
                        },
                        ""inputs"": [
                            {
                                ""referenceName"": ""ClustersData"",
                                ""type"": ""DatasetReference""
                            }
                        ],
                        ""outputs"": [
                            {
                                ""referenceName"": ""OutputClusters"",
                                ""type"": ""DatasetReference""
                            }
                        ]
                    }
                ]
            }
        }
    ]
}
}
</code></pre>

<p>Example of doc in input collection (all the same format):</p>

<pre><code>{
   ""$type"": ""Entities.ADCluster"",
    ""DisplayName"": ""TESTNetBIOS"",
    ""OrgId"": ""9b679d2a-42c5-4c9a-a2e2-3ce63c1c3506"",
    ""ClusterId"": ""ab2a242d-f1a5-62ed-b420-31b52e958586"",
    ""AllowLdapLifeCycleSynchronization"": true,
    ""DirectoryServers"": [
        {
            ""$type"": ""Entities.DirectoryServer"",
            ""AddressId"": ""e6a8edbb-ad56-4135-94af-fab50b774256"",
            ""Port"": 389,
            ""Host"": ""192.168.342.234""
        }
    ],
    ""DomainNames"": [
        ""TESTNetBIOS""
    ],
    ""BaseDn"": null,
    ""UseSsl"": false,
    ""RepositoryType"": 1,
    ""DirectoryCustomizations"": null,
    ""_etag"": ""\""140046f2-0000-0000-0000-5ac63a180000\"""",
    ""LastUpdateTime"": ""2018-04-05T15:00:40.243Z"",
    ""id"": ""ab2a242d-f1a5-62ed-b420-31b52e958586"",
    ""PartitionKey"": ""directory-clusters-9b679d2a-42c5-4c9a-a2e2-3ce63c1c3506"",
    ""_rid"": ""kpvxLAs6gkmsCQAAAAAAAA=="",
    ""_self"": ""dbs/kvpxAA==/colls/kpvxLAs6gkk=/docs/kvpxALs6kgmsCQAAAAAAAA==/"",
    ""_attachments"": ""attachments/"",
    ""_ts"": 1522940440
}
</code></pre>
","<azure><azure-cosmosdb><azure-data-factory>","2018-06-15 09:25:02","2989","4","4","55922867","<p>I also struggled a bit with this, especially getting around the size limits of the Lookup activity, since we have a LOT of data to migrate. I ended up creating a JSON file with a list of timestamps to query the Cosmos data with, then for each of those, getting the document IDs in that range, and then for each of those, getting the full document data and saving it to a path such as <code>PartitionKey/DocumentID</code>. Here's the pipelines I created:</p>

<p>LookupTimestamps - loops through each timestamp range from a <code>times.json</code> file, and for each timestamp, executes the ExportFromCosmos pipeline</p>

<pre><code>{
    ""name"": ""LookupTimestamps"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""LookupTimestamps"",
                ""type"": ""Lookup"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""BlobSource"",
                        ""recursive"": false
                    },
                    ""dataset"": {
                        ""referenceName"": ""BlobStorageTimestamps"",
                        ""type"": ""DatasetReference""
                    },
                    ""firstRowOnly"": false
                }
            },
            {
                ""name"": ""ForEachTimestamp"",
                ""type"": ""ForEach"",
                ""dependsOn"": [
                    {
                        ""activity"": ""LookupTimestamps"",
                        ""dependencyConditions"": [
                            ""Succeeded""
                        ]
                    }
                ],
                ""typeProperties"": {
                    ""items"": {
                        ""value"": ""@activity('LookupTimestamps').output.value"",
                        ""type"": ""Expression""
                    },
                    ""isSequential"": false,
                    ""activities"": [
                        {
                            ""name"": ""Execute Pipeline1"",
                            ""type"": ""ExecutePipeline"",
                            ""typeProperties"": {
                                ""pipeline"": {
                                    ""referenceName"": ""ExportFromCosmos"",
                                    ""type"": ""PipelineReference""
                                },
                                ""waitOnCompletion"": true,
                                ""parameters"": {
                                    ""From"": {
                                        ""value"": ""@{item().From}"",
                                        ""type"": ""Expression""
                                    },
                                    ""To"": {
                                        ""value"": ""@{item().To}"",
                                        ""type"": ""Expression""
                                    }
                                }
                            }
                        }
                    ]
                }
            }
        ]
    },
    ""type"": ""Microsoft.DataFactory/factories/pipelines""
}
</code></pre>

<p>ExportFromCosmos - nested pipeline that's executed from the above pipeline. This is to get around the fact you can't have nested ForEach activities.</p>

<pre><code>{
    ""name"": ""ExportFromCosmos"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""LookupDocuments"",
                ""type"": ""Lookup"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false,
                    ""secureInput"": false
                },
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""DocumentDbCollectionSource"",
                        ""query"": {
                            ""value"": ""select c.id, c.partitionKey from c where c._ts &gt;= @{pipeline().parameters.from} and c._ts &lt;= @{pipeline().parameters.to} order by c._ts desc"",
                            ""type"": ""Expression""
                        },
                        ""nestingSeparator"": "".""
                    },
                    ""dataset"": {
                        ""referenceName"": ""CosmosDb"",
                        ""type"": ""DatasetReference""
                    },
                    ""firstRowOnly"": false
                }
            },
            {
                ""name"": ""ForEachDocument"",
                ""type"": ""ForEach"",
                ""dependsOn"": [
                    {
                        ""activity"": ""LookupDocuments"",
                        ""dependencyConditions"": [
                            ""Succeeded""
                        ]
                    }
                ],
                ""typeProperties"": {
                    ""items"": {
                        ""value"": ""@activity('LookupDocuments').output.value"",
                        ""type"": ""Expression""
                    },
                    ""activities"": [
                        {
                            ""name"": ""Copy1"",
                            ""type"": ""Copy"",
                            ""policy"": {
                                ""timeout"": ""7.00:00:00"",
                                ""retry"": 0,
                                ""retryIntervalInSeconds"": 30,
                                ""secureOutput"": false,
                                ""secureInput"": false
                            },
                            ""typeProperties"": {
                                ""source"": {
                                    ""type"": ""DocumentDbCollectionSource"",
                                    ""query"": {
                                        ""value"": ""select * from c where c.id = \""@{item().id}\"" and c.partitionKey = \""@{item().partitionKey}\"""",
                                        ""type"": ""Expression""
                                    },
                                    ""nestingSeparator"": "".""
                                },
                                ""sink"": {
                                    ""type"": ""BlobSink""
                                },
                                ""enableStaging"": false
                            },
                            ""inputs"": [
                                {
                                    ""referenceName"": ""CosmosDb"",
                                    ""type"": ""DatasetReference""
                                }
                            ],
                            ""outputs"": [
                                {
                                    ""referenceName"": ""BlobStorageDocuments"",
                                    ""type"": ""DatasetReference"",
                                    ""parameters"": {
                                        ""id"": {
                                            ""value"": ""@item().id"",
                                            ""type"": ""Expression""
                                        },
                                        ""partitionKey"": {
                                            ""value"": ""@item().partitionKey"",
                                            ""type"": ""Expression""
                                        }
                                    }
                                }
                            ]
                        }
                    ]
                }
            }
        ],
        ""parameters"": {
            ""from"": {
                ""type"": ""int""
            },
            ""to"": {
                ""type"": ""int""
            }
        }
    }
}
</code></pre>

<p>BlobStorageTimestamps - dataset for the <code>times.json</code> file</p>

<pre><code>{
    ""name"": ""BlobStorageTimestamps"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureBlobStorage1"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""AzureBlob"",
        ""typeProperties"": {
            ""format"": {
                ""type"": ""JsonFormat"",
                ""filePattern"": ""arrayOfObjects""
            },
            ""fileName"": ""times.json"",
            ""folderPath"": ""mycollection""
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p>BlobStorageDocuments - dataset for where the documents will be saved</p>

<pre><code>{
    ""name"": ""BlobStorageDocuments"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""AzureBlobStorage1"",
            ""type"": ""LinkedServiceReference""
        },
        ""parameters"": {
            ""id"": {
                ""type"": ""string""
            },
            ""partitionKey"": {
                ""type"": ""string""
            }
        },
        ""type"": ""AzureBlob"",
        ""typeProperties"": {
            ""format"": {
                ""type"": ""JsonFormat"",
                ""filePattern"": ""arrayOfObjects""
            },
            ""fileName"": {
                ""value"": ""@{dataset().partitionKey}/@{dataset().id}.json"",
                ""type"": ""Expression""
            },
            ""folderPath"": ""mycollection""
        }
    },
    ""type"": ""Microsoft.DataFactory/factories/datasets""
}
</code></pre>

<p>The times.json file it just a list of epoch times and looks like this:</p>

<pre><code>[{
    ""From"": 1556150400,
    ""To"": 1556236799
},
{
    ""From"": 1556236800,
    ""To"": 1556323199
}]
</code></pre>
"
"50870780","Azure Data Factory Debugging Failure","<p>I've been developing a pipeline in ADF with one simple copy activity taking data from SQL Server on premise up to an Azure SQL Database and yesterday came across an issue (pipeline image below)</p>

<p><a href=""https://i.stack.imgur.com/fwsXM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fwsXM.png"" alt=""enter image description here""></a></p>

<p>My pipeline kept failing to debug in the same place with the same error </p>

<pre><code>{
    ""errorCode"": ""BadRequest"",
    ""message"": ""The integration runtime 'Integration-Runtime-Name' under data factory 'Data-Factory-Name' does not exist. "",
    ""failureType"": ""UserError"",
    ""target"": ""PiplineActivity""
}
</code></pre>

<p>The day before it had worked without any issues, and I realised although the debug run failed, if I kicked off a trigger run the pipeline would succeed.</p>

<p>I tested this out with a couple of different runtime environments and a completely new pipeline, but got the same result. I even stripped the one copy task it was trying to do down to a simple test table with one column and one row.</p>

<p>Can anyone else verify they are seeing the same or different behavior? or point out if I'm doing anything wrong. I would like to emphasise it was working fine the day before yesterday.</p>

<p>Many thanks</p>
","<sql-server><azure-sql-database><azure-synapse><azure-data-factory>","2018-06-15 07:15:23","2015","0","1","50879614","<p>The issue has been fixed. Please have a try again. Thanks.</p>
"
"50859359","Multiple failed dependencies in Azure Data Factory activity 'dependsOn'","<p>When there are multiple activity dependencies (""dependsOn"") conditions in an Azure Data Factory control activity do they all need to be true for the activity to run?</p>

<p>For example, if a clean-up activity should run if any other activity fails there can be several dependencies with a ""dependencyCondition"" of ""failed"".</p>

<pre><code>""dependsOn"": [
            {
                ""activity"": ""FirstActivity"",
                ""dependencyConditions"": [
                    ""Failed""
                ]
            },
            {
                ""activity"": ""SecondActivity"",
                ""dependencyConditions"": [
                    ""Failed""
                ]
            }
]
</code></pre>

<p>When there are multiple ""Failed"" dependencies will the activity run if any of them fails or only if they all fail?</p>

<p>I know I can just make each activity dependent on the next but that will may them run synchronously.  I'd like them to be able to run asynchronously and only run the clean-up activity if one or more of the activities it depends on fails.</p>
","<azure-data-factory>","2018-06-14 14:01:52","511","1","1","50860237","<p>ADF dependencies use “and” operator, which means only all of them failed, the activity will be run.</p>

<p>In you scenario, I think you could add a cleanup activity for each activity.</p>
"
"50844550","Why are ADF datasets important?","<p>In Azure Data Factory v2 I've created a number of pipelines. I noticed that each pipeline I create there is a source and destination dataset created. </p>

<p>According to the ADF documentation: <strong><em>A dataset is a named view of data that simply points or references the data you want to use in your activities as inputs and outputs.</em></strong></p>

<p>These datasets are visible within my data factory. I'm curious why I would care about these? These almost seem like 'under the hood' objects ADF creates to move data around. What value are these to me and why would I care about them?</p>
","<azure><azure-data-factory>","2018-06-13 19:07:06","1325","1","3","50844756","<p>You may be seeing those show up in your Factory if you create pipelines via the Copy Wizard Tool. That will create Datasets for your Source &amp; Sink. The Copy Activity is the primary consumer of Datasets in ADF Pipelines.</p>
"
"50844550","Why are ADF datasets important?","<p>In Azure Data Factory v2 I've created a number of pipelines. I noticed that each pipeline I create there is a source and destination dataset created. </p>

<p>According to the ADF documentation: <strong><em>A dataset is a named view of data that simply points or references the data you want to use in your activities as inputs and outputs.</em></strong></p>

<p>These datasets are visible within my data factory. I'm curious why I would care about these? These almost seem like 'under the hood' objects ADF creates to move data around. What value are these to me and why would I care about them?</p>
","<azure><azure-data-factory>","2018-06-13 19:07:06","1325","1","3","50848805","<p>These datasets are entities that can be reused. For example, dataset A can be referenced by many pipelines if those pipelines need the same data (same table or same file).
Linked services can be reused too. I think that's why ADF has these concepts.</p>
"
"50844550","Why are ADF datasets important?","<p>In Azure Data Factory v2 I've created a number of pipelines. I noticed that each pipeline I create there is a source and destination dataset created. </p>

<p>According to the ADF documentation: <strong><em>A dataset is a named view of data that simply points or references the data you want to use in your activities as inputs and outputs.</em></strong></p>

<p>These datasets are visible within my data factory. I'm curious why I would care about these? These almost seem like 'under the hood' objects ADF creates to move data around. What value are these to me and why would I care about them?</p>
","<azure><azure-data-factory>","2018-06-13 19:07:06","1325","1","3","50848975","<p>If you are using ADFv2 to transform data, no DataSet is required. But if you are using ADF copy activity to copy data, DataSet is used to let ADF know the path and name of object to copy from/to.  Once you have one dataset created, it can be used in many pipelines. Could you please help to let me understand more why creating a dataset is a friction to you in your projects? </p>
"
"50835887","Call Azure Function from Data Factory using Managed Service Identity","<p>I want my Data Factory's Web Activity to authenticate to my Azure Function using MSI (Managed Service Identity), so I don't have to include the API key in the Web-URL.</p>

<p>What type of resource string do I need to enter here in the configuration section of the Web Activity:</p>

<p><a href=""https://i.stack.imgur.com/2RZGm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2RZGm.png"" alt=""dialogue""></a></p>

<p>I tried:</p>

<ul>
<li><a href=""https://myfunctionapp.azurewebsites.net/"" rel=""nofollow noreferrer"">https://myfunctionapp.azurewebsites.net/</a></li>
<li><a href=""https://myfunctionapp.azurewebsites.net/api/myfunction"" rel=""nofollow noreferrer"">https://myfunctionapp.azurewebsites.net/api/myfunction</a></li>
</ul>

<p>but none worked. The pipeline activity times out after a few minutes with the error ""Error calling the endpoint"".</p>
","<azure><authentication><azure-functions><azure-data-factory>","2018-06-13 11:17:34","2710","2","1","50846161","<p>I had the same issue, but using <a href=""https://management.azure.com/"" rel=""nofollow noreferrer"">https://management.azure.com/</a> as the resource string worked for me in terms of making MSI function, but after a bunch of testing it seems to me as though the HTTPTrigger still requires a key in either the URI or the header.</p>

<p>Both of these websites are good resources:</p>

<ul>
<li><a href=""https://learn.microsoft.com/en-us/azure/app-service/app-service-managed-service-identity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/app-service/app-service-managed-service-identity</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/active-directory/managed-service-identity/services-support-msi"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/active-directory/managed-service-identity/services-support-msi</a></li>
</ul>
"
"50831915","Nested forEach activity in Azure Data factory V2","<p>I am trying to use two forEach activities to iterate on subfolders of folders with parameters to get metadata of subfolders. I have forEach1 and forEach2 with their own items array. Within the second for loop I need to combine both for loops' item() in a Metada activity to access my dataset like @item1()@item2(). Is this possible? </p>

<p><a href=""https://i.stack.imgur.com/YM5lq.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/YM5lq.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2018-06-13 07:55:56","9392","5","2","50840499","<p>It is possible but the second ForEach activity needs to be inside the first one, not another activity in the pipeline.</p>

<p>As you have it now, the first ForEach will run until completion, then the second one will start and you cannot access the items in the first one.</p>

<p>Hope this helped!</p>
"
"50831915","Nested forEach activity in Azure Data factory V2","<p>I am trying to use two forEach activities to iterate on subfolders of folders with parameters to get metadata of subfolders. I have forEach1 and forEach2 with their own items array. Within the second for loop I need to combine both for loops' item() in a Metada activity to access my dataset like @item1()@item2(). Is this possible? </p>

<p><a href=""https://i.stack.imgur.com/YM5lq.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/YM5lq.png"" alt=""enter image description here""></a></p>
","<azure><azure-data-factory>","2018-06-13 07:55:56","9392","5","2","50875880","<p>Nested foreach activity is not allowed. But you could use an execute pipeline activity inside the foreach activity. And in the nested pipeline, you could have another foreach.</p>
"
"50818948","Cannot access files on FTP server from Azure Data Factory","<p>I currently have access to a third party's FTP server which, upon login, automatically redirects me to a directory that does not contain the files I am trying to download.</p>

<p><a href=""ftp://ftp.fakehost.com"" rel=""nofollow noreferrer"">ftp://ftp.fakehost.com</a> -> <a href=""ftp://ftp.fakehost.com/uselessDir"" rel=""nofollow noreferrer"">ftp://ftp.fakehost.com/uselessDir</a></p>

<p>My files are in <a href=""ftp://ftp.fakehost/usefulDir"" rel=""nofollow noreferrer"">ftp://ftp.fakehost/usefulDir</a>.</p>

<p>This ftp server does not support directory traversal so I cannot get to usefulDir by simply modifying my url. FileZilla works since I can execute specific ftp commands to get to the directory I want.</p>

<p>Can a Data Factory FTP service or dataset be customized to work around this problem since Data Factory cannot access the usefulDir directly ?</p>
","<ftp><azure-data-factory>","2018-06-12 13:53:29","332","1","1","50820359","<p>Please correct me if I doesn't understand your question correctly. Have you tried create a dataset and manually put the usefulDir in folderPath property directly, instead of using the Authoring UI to navigate to that folder (which is not possible based on your description.)</p>
"
"50812577","Make sure the ACL and firewall rule is correctly configured in the Azure Data Lake Store account","<p>I'm coping CSV files from Azure blob to Azure Data Lake using Azure data factory using Copy data tool.
I'm following this link: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-copy-data-tool"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-copy-data-tool</a></p>

<p>Fron Copy data tool my source configuration and test connection successed. However, the destination connection (that is Data lake) is creating problem.</p>

<p>I'm getting error : Make sure the ACL and firewall rule is correctly configured in the Azure Data Lake Store account.</p>

<p>I followed this link for Fairwall setting:  <a href=""https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-secure-data"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-secure-data</a> (Set IP address range for data access)</p>

<p>Enabled fairwall and Allow access to Azure service ""ON""</p>

<p>Still, I'm getting same error. Could any one please suggest. How to fix this?</p>
","<azure><azure-data-factory><azure-data-lake>","2018-06-12 08:27:55","6421","5","3","50814105","<p>Except the firewall setting, please also be sure that your account has necessary permission on the target ADLS account. Please refer to this doc for more details: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-store#linked-service-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-store#linked-service-properties</a></p>
"
"50812577","Make sure the ACL and firewall rule is correctly configured in the Azure Data Lake Store account","<p>I'm coping CSV files from Azure blob to Azure Data Lake using Azure data factory using Copy data tool.
I'm following this link: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-copy-data-tool"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-copy-data-tool</a></p>

<p>Fron Copy data tool my source configuration and test connection successed. However, the destination connection (that is Data lake) is creating problem.</p>

<p>I'm getting error : Make sure the ACL and firewall rule is correctly configured in the Azure Data Lake Store account.</p>

<p>I followed this link for Fairwall setting:  <a href=""https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-secure-data"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-secure-data</a> (Set IP address range for data access)</p>

<p>Enabled fairwall and Allow access to Azure service ""ON""</p>

<p>Still, I'm getting same error. Could any one please suggest. How to fix this?</p>
","<azure><azure-data-factory><azure-data-lake>","2018-06-12 08:27:55","6421","5","3","50814314","<p>Your account and application ADF need to have permission to work on ADLS. Also, see did you gave permission to children folders as well.</p>
"
"50812577","Make sure the ACL and firewall rule is correctly configured in the Azure Data Lake Store account","<p>I'm coping CSV files from Azure blob to Azure Data Lake using Azure data factory using Copy data tool.
I'm following this link: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-copy-data-tool"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-copy-data-tool</a></p>

<p>Fron Copy data tool my source configuration and test connection successed. However, the destination connection (that is Data lake) is creating problem.</p>

<p>I'm getting error : Make sure the ACL and firewall rule is correctly configured in the Azure Data Lake Store account.</p>

<p>I followed this link for Fairwall setting:  <a href=""https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-secure-data"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-secure-data</a> (Set IP address range for data access)</p>

<p>Enabled fairwall and Allow access to Azure service ""ON""</p>

<p>Still, I'm getting same error. Could any one please suggest. How to fix this?</p>
","<azure><azure-data-factory><azure-data-lake>","2018-06-12 08:27:55","6421","5","3","56889746","<p>Get your <em>Managed Identity Application ID</em> from <em>Azure Data Factory properties</em>.</p>

<p>Go to Azure Data Lake Storage and navigate to <code>Data Explorer -&gt; Access -&gt; Add</code> and then provide the ID in the 'Select User or group' field.<br>
It will identify your Azure Data Factory instance/resource and then provide ACLs(R/W/X) as per your requirement.</p>
"
"50810525","Azure: Compare dates in Data factory","<p>Data factory doesn't have a built-in date difference function. I want to compare lastModified date and utcnow in if condition activity. How can I achieve it?</p>

<pre><code>@greaterOrEquals(activity('Get Metadata1').output.lastModified, '2015-03-15T13:27:36Z')
</code></pre>
","<azure><azure-data-factory>","2018-06-12 06:27:24","11458","8","4","50812386","<p>I solved it by converting year and month part of dates to integers and comparing them.</p>
"
"50810525","Azure: Compare dates in Data factory","<p>Data factory doesn't have a built-in date difference function. I want to compare lastModified date and utcnow in if condition activity. How can I achieve it?</p>

<pre><code>@greaterOrEquals(activity('Get Metadata1').output.lastModified, '2015-03-15T13:27:36Z')
</code></pre>
","<azure><azure-data-factory>","2018-06-12 06:27:24","11458","8","4","50884890","<p>You can use the <code>ticks</code> function to convert it to integer.
<a href=""https://learn.microsoft.com/en-us/azure/logic-apps/workflow-definition-language-functions-reference#ticks"" rel=""noreferrer"">Logic Apps function reference for Ticks</a></p>

<p>The ADF UI may show a warning about it not being a recognized function, but ADF is actually using logic app expressions so it will succeed if you debug or trigger the pipeline.</p>
"
"50810525","Azure: Compare dates in Data factory","<p>Data factory doesn't have a built-in date difference function. I want to compare lastModified date and utcnow in if condition activity. How can I achieve it?</p>

<pre><code>@greaterOrEquals(activity('Get Metadata1').output.lastModified, '2015-03-15T13:27:36Z')
</code></pre>
","<azure><azure-data-factory>","2018-06-12 06:27:24","11458","8","4","63170541","<p>I had this problem today where I needed a check to see whether the <code>utcNow()</code> time was greater than 2AM (inside an if block in the Data Factory).</p>
<p>On the above advice, I used the <code>ticks()</code> function. I'm sure its not the most elegant but wasn't sure how to convert the hour section of the datetimes nicely as it seemed that <code>hour()</code> wasnt supported.</p>
<p><code>@if(greaterOrEquals(div(sub(ticks(utcNow()),ticks(startOfDay(utcNow()))),36000000000),2))True,False)</code></p>
<p>with the 36,000,000,000 division converting the 100ns ticks into the number of hours between the <code>utcNow()</code> and the <code>startOfday(utcNow())</code></p>
"
"50810525","Azure: Compare dates in Data factory","<p>Data factory doesn't have a built-in date difference function. I want to compare lastModified date and utcnow in if condition activity. How can I achieve it?</p>

<pre><code>@greaterOrEquals(activity('Get Metadata1').output.lastModified, '2015-03-15T13:27:36Z')
</code></pre>
","<azure><azure-data-factory>","2018-06-12 06:27:24","11458","8","4","69799714","<p>we can get date difference count between 2 dates.
<strong>@string(div(div(div(div(sub(ticks(variables('presentdate')),ticks(variables('pastdate'))),10000000),60),60),24))</strong></p>
"
"50808520","Azure Data Factory Pipeline Intermittent Error 2906","<p>I have four ADF pipelines running on different schedules (1Hr, 2Hr, 6Hr and 1Day). Since yesterday they are having intermittent failures, reporting Error 2906, as follows:</p>

<p>{
    ""errorCode"": ""2906"",
    ""message"": ""Package execution failed."",
    ""failureType"": ""UserError"",
    ""target"": ""%package name%""
}</p>

<p>I'm unclear in the error however given that it has been working just fine up until yesterday, and then yesterday it succeeds and then fails intermittently, is there any advice on how/where to troubleshoot this?</p>
","<azure-data-factory>","2018-06-12 02:38:18","1357","0","1","50829059","<p>I can't say this is the definitive answer, however I suspect the DB was merely under strain performance-wise. The problem appears to have disappeared now.</p>
"
"50800166","Migrating Integration Runtime in Azure","<p>I have an existing Integration Runtime setup for Data Factory v1. Now I want to move it to another Data Factory, which is <strong>v2</strong>. However, I do not have access to the server, where the existing Integration Runtime is hosted. <br/><br/>
So my question is: Is it somehow possible to migrate the runtime from DFv1 to DFv2 without modifying the server? 
I tried copying the .json File of the Integration Runtime to a blank one in v2, but it would always fail...</p>
","<azure><runtime><integration><azure-data-factory>","2018-06-11 14:32:10","500","1","1","50808220","<p>ADF v1 and ADF v2 are using different app model, there is no migration tool between ADF v1 gateway and ADF v2 integration runtime.</p>

<p>On the other hand, v1 gateway has no auto update function, there is no way to upgrade gateway to integration runtime without access to the host machine.</p>

<p>You can create v2 integration runtime under the v2 data factory with same name, however you will need to get access to the host server eventually, or setup another host machine.</p>
"
"50786546","Error in running Azure Data Factory Pipeline. Linked Service Reference not found","<p>I am facing the below issue in creating an Azure Machine Learning Batch Execution activity to execute a scoring ML experiment. Please help:</p>

<p><strong>Please let me know if any other relevant information is needed. I am new to this so, please help</strong></p>

<ol>
<li><p>Created an AzureML Linked Service as below:</p>

<pre><code>{
""name"": ""PredictionAzureML"",
""properties"": {
""typeProperties"": {
""mlEndpoint"": ""https://ussouthcentral.services.azureml.net/workspaces/xxxxx/jobs"",
""apiKey"": ""xxxxxxxx==""
},
""type"": ""AzureML""
}
}
</code></pre></li>
<li><p>Created Pipeline as below:</p>

<pre><code>{
""name"": ""pipeline1"",
""properties"": {
""description"": ""use AzureML model"",
""activities"": [
{
""name"": ""MLActivity"",
""description"": ""description"",
""type"": ""AzureMLBatchExecution"",
""policy"": {
""timeout"": ""02:00:00"",
""retry"": 1,
""retryIntervalInSeconds"": 30
},
""typeProperties"": {
""webServiceInput"": ""PredictionInputDataset"",
""webServiceOutputs"": {
""output1"": ""PredictionOutputDataset""
}
},
""inputs"": [
{
""name"": ""PredictionInputDataset""
}
],
""outputs"": [
{
""name"": ""PredictionOutputDataset""
}
],
""linkedServiceName"": ""PredictionAzureML""
}
]
}
}
</code></pre></li>
<li><p>Getting the below error:</p>

<pre><code> {
""errorCode"": ""2109"",
""message"": ""'linkedservicereference' with reference name 'PredictionAzureML' can't be found."",
""failureType"": ""UserError"",
""target"": ""MLActivity""
}
</code></pre></li>
</ol>
","<azure-machine-learning-service><azure-data-factory>","2018-06-10 17:56:36","4700","1","2","50789646","<p>Please use ""Trigger"" instead of ""Debug"" in the UX. You need publish your pipeline first before click ""Trigger"" Button. 
Please follow <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-machine-learning"" rel=""nofollow noreferrer"">this doc</a> to update your payload. It should look like the following.
<code>
{
    ""name"": ""AzureMLExecutionActivityTemplate"",
    ""description"": ""description"",
    ""type"": ""AzureMLBatchExecution"",
    ""linkedServiceName"": {
        ""referenceName"": ""AzureMLLinkedService"",
        ""type"": ""LinkedServiceReference""
    },
    ""typeProperties"": {
        ""webServiceInputs"": {
            ""&lt;web service input name 1&gt;"": {
                ""LinkedServiceName"":{
                    ""referenceName"": ""AzureStorageLinkedService1"",
                    ""type"": ""LinkedServiceReference""
                }, 
                ""FilePath"":""path1""
            }, 
            ""&lt;web service input name 2&gt;"": {
                ""LinkedServiceName"":{
                    ""referenceName"": ""AzureStorageLinkedService1"",
                    ""type"": ""LinkedServiceReference"" 
                }, 
                ""FilePath"":""path2""
            }<br>
        },
        ""webServiceOutputs"": {
            ""&lt;web service output name 1&gt;"": {
                ""LinkedServiceName"":{
                    ""referenceName"": ""AzureStorageLinkedService2"",
                    ""type"": ""LinkedServiceReference""<br>
                }, 
                ""FilePath"":""path3""
            }, 
            ""&lt;web service output name 2&gt;"": {
                ""LinkedServiceName"":{
                    ""referenceName"": ""AzureStorageLinkedService2"",
                    ""type"": ""LinkedServiceReference""<br>
                }, 
                ""FilePath"":""path4""
            }<br>
        },
        ""globalParameters"": {
            ""&lt;Parameter 1 Name&gt;"": ""&lt;parameter value&gt;"",
            ""&lt;parameter 2 name&gt;"": ""&lt;parameter 2 value&gt;""
        }
    }
}</code></p>
"
"50786546","Error in running Azure Data Factory Pipeline. Linked Service Reference not found","<p>I am facing the below issue in creating an Azure Machine Learning Batch Execution activity to execute a scoring ML experiment. Please help:</p>

<p><strong>Please let me know if any other relevant information is needed. I am new to this so, please help</strong></p>

<ol>
<li><p>Created an AzureML Linked Service as below:</p>

<pre><code>{
""name"": ""PredictionAzureML"",
""properties"": {
""typeProperties"": {
""mlEndpoint"": ""https://ussouthcentral.services.azureml.net/workspaces/xxxxx/jobs"",
""apiKey"": ""xxxxxxxx==""
},
""type"": ""AzureML""
}
}
</code></pre></li>
<li><p>Created Pipeline as below:</p>

<pre><code>{
""name"": ""pipeline1"",
""properties"": {
""description"": ""use AzureML model"",
""activities"": [
{
""name"": ""MLActivity"",
""description"": ""description"",
""type"": ""AzureMLBatchExecution"",
""policy"": {
""timeout"": ""02:00:00"",
""retry"": 1,
""retryIntervalInSeconds"": 30
},
""typeProperties"": {
""webServiceInput"": ""PredictionInputDataset"",
""webServiceOutputs"": {
""output1"": ""PredictionOutputDataset""
}
},
""inputs"": [
{
""name"": ""PredictionInputDataset""
}
],
""outputs"": [
{
""name"": ""PredictionOutputDataset""
}
],
""linkedServiceName"": ""PredictionAzureML""
}
]
}
}
</code></pre></li>
<li><p>Getting the below error:</p>

<pre><code> {
""errorCode"": ""2109"",
""message"": ""'linkedservicereference' with reference name 'PredictionAzureML' can't be found."",
""failureType"": ""UserError"",
""target"": ""MLActivity""
}
</code></pre></li>
</ol>
","<azure-machine-learning-service><azure-data-factory>","2018-06-10 17:56:36","4700","1","2","50789648","<p>I got this working in Data Factory v2, so apologies if you are using v1.</p>

<p>Try putting the <code>linkedServiceName</code> as an object in the JSON outside of the <code>typeProperties</code> and use the following structure:</p>

<pre><code>""linkedServiceName"": {
  ""referenceName"": ""PredictionAzureML"",
  ""type"": ""LinkedServiceReference""
}
</code></pre>

<p>Hope that helps!</p>
"
"50760350","How the Integration Runtime works with S3","<p>Can anyone tell me how the IR of Azure data factory works with S3?
I know that S3 is only supported as a source not sink, then how the IR Gateway works with the S3??</p>
","<azure><amazon-s3><azure-data-factory>","2018-06-08 12:08:53","283","0","1","50857118","<p>Currently, ADF doesn't support S3 as a sink. But if you have s3 odbc driver, you could try ODBC connector of ADF.</p>
"
"50749405","Azure Data Factory - Maximum call stack size exceeded","<p>I created a linked service for creating an HDInsight cluster a few weeks back, was able to modify it after the fact as well for # nodes, cluster type, etc. </p>

<p>When I go to edit the HDInsight linked service today, it throws back ""Maximum call stack size exceeded"". I tried creating a fresh linked service today with the same parameters and I encounter the same error there. </p>

<p>I tried the same behavior on a different data factory under a different subscription and there it was successful, no errors were thrown. Then I tried it with a different linked service there, and saw the same error... Wondering if anybody has encountered this particular scenario and had insights.</p>
","<azure><azure-data-factory><azure-hdinsight>","2018-06-07 20:06:16","345","0","1","50772240","<p>The Issue should have been fixed. Please refresh the UI and try again. Thanks for the patience.</p>
"
"50734984","How to set encoding to ""UTF8 without bom"" in Azure Blob output dataset","<p>I'm currently producing daily .json output with Azure Data Factory. My partners need the encoding to be in UTF8 but without BOM characters. </p>

<p>Is there anyway to do that in the configuration file in AzureDataFactory ?
I found out the encodingName property :</p>

<pre><code>        ""typeProperties"": {
        ""fileName"": ""prod-file-{date}.json"",
        ""folderPath"": ""folder"",
        ""format"": {
            ""type"": ""JsonFormat"",
            ""encodingName"":""""
        },
</code></pre>

<p>but the list of matchings <a href=""https://msdn.microsoft.com/library/system.text.encoding.aspx"" rel=""nofollow noreferrer"">encodingName parameters</a> doesn't seems to correspond to my needs.</p>

<p>Thanks in advance.</p>
","<azure><azure-blob-storage><azure-data-factory>","2018-06-07 07:10:58","1637","2","1","58232516","<p>For me setting the encoding to UTF-8 and having file format text created UTF-8 BOM files. </p>

<p>Leaving the encoding empty used the apparent default of UTF-8 without BOM. </p>

<p>Does not make sense, but hope it works with JSON as well. </p>
"
"50734178","""Value cannot be null.\r\nParameter name: endpoint"" in Azure Data Factory V2","<p>I am getting the following error when I execute Azure ML Batch Execution Activity in ADF V2.</p>

<p><a href=""https://i.stack.imgur.com/JJwWn.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JJwWn.jpg"" alt=""Error in ML Batch Execution Activity""></a></p>

<p>I have written following JSON query in ML Activity</p>

<pre><code>{
    ""name"": ""MLBatchExecution1"",
    ""description"": """",
    ""type"": ""AzureMLBatchExecution"",
    ""linkedServiceName"": {
        ""name"": ""AzureMLLinkedservice2"",
        ""type"": ""AzureML""
    },
    ""typeProperties"": {
        ""webServiceInputs"": {
            ""input1"": {
                ""LinkedServiceName"":{
                    ""name"": ""azureblobstoragelinkedservice"",
                    ""type"": ""AzureStorage""
                }, 
                ""FilePath"":""tutoial/Input/TraiData.csv""
            }, 
            ""input2"": {
                ""LinkedServiceName"":{
                    ""name"": ""azureblobstoragelinkedservice"",
                    ""type"": ""AzureStorage"" 
                }, 
                ""FilePath"":""tutoial/Input/TestData.csv""
            }        
        },
        ""webServiceOutputs"": {
            ""output1"": {
                ""LinkedServiceName"":{
                    ""name"": ""AzureStorageLinkedService2"",
                    ""type"": ""AzureStorage""   
                }, 
                ""FilePath"":""tutoial/Output/Output.csv""
            } 

        }
    }
}
</code></pre>

<p>I have make use of the following link to create linked service &amp; activity:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-machine-learning"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-machine-learning</a></p>

<p>Can anyone help on this pls.
Any help will be appreciated..</p>

<p>Thanks</p>

<p>Deepak</p>
","<azure-data-factory>","2018-06-07 06:20:06","1411","1","1","50876736","<p>Try passing the argument when triggering the pipeline. The error you are receiving is because it enforces a parameter.</p>
"
"50713856","Dataset error message - PdwManagedToNativeInteropException","<p>Currently have a pipeline running in our production environment that has an activity that copies data from an on prem sql database to sql azure database. This pipeline is replicated among the dev and QA environments but don't fail in those environments. Wanted to get a bit more insight as to what this error means.</p>

<p>Message=A database operation failed with the following error: 'PdwManagedToNativeInteropException ErrorNumber: 46724,</p>
","<azure-sql-database><azure-data-factory>","2018-06-06 06:47:27","1374","1","2","50867407","<p>""PDW"" is short for Parallel Data Warehouse and suggests you might be using the MPP product Azure SQL Data Warehouse, rather than a SQL DB as you mentioned. Is that correct? </p>
"
"50713856","Dataset error message - PdwManagedToNativeInteropException","<p>Currently have a pipeline running in our production environment that has an activity that copies data from an on prem sql database to sql azure database. This pipeline is replicated among the dev and QA environments but don't fail in those environments. Wanted to get a bit more insight as to what this error means.</p>

<p>Message=A database operation failed with the following error: 'PdwManagedToNativeInteropException ErrorNumber: 46724,</p>
","<azure-sql-database><azure-data-factory>","2018-06-06 06:47:27","1374","1","2","66297718","<p>This error reflects when your defined size of the column like varchar /int is getting overflown.</p>
<p>Try increasing the size of data types and column and rerun the pipeline.</p>
<p>I recreated it and fixed it in my Data factory.</p>
"
"50713367","Azure: Run a data factory activity when a new file is added to data lake store","<p>I have a large dataset on Azure Data lake store and a few files might be added/updated there daily. How can I process these new files without reading the entire dataset each time? 
I need to copy these new files using Data Factory V1 to SQL server.</p>
","<azure><azure-storage><azure-data-factory>","2018-06-06 06:18:38","213","0","1","50714886","<p>If you could use ADF V2, then you could use get metadata activity to get the lastModifiedDate Properties of each file and then only copy new files. You could reference this doc. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity</a></p>
"
"50703870","M requires a parameter Validation Error","<p>I have a bunch of Copy Pipelines in Azure Data Factory (v2).</p>

<p>They've been working fine up until today. Now whenever I try to publish changes, I am getting the following validation error: M requires a parameter</p>

<p>And of course with a validation error I can't save my changes.</p>

<p>Any dieas as to what I might look for? I have no idea where ""M"" is or what it is.</p>
","<parameters><azure-data-factory>","2018-06-05 15:30:34","267","1","1","50704696","<p>It turned out I had deleted parameters from the main pipeline and it would not let me add them back and save the pipeline. I ended up junking the pipeline and re-creating. It needed editing anyway. This fixed all of the validation issues.</p>
"
"50703762","Data Factory V2 - maxconcurrency is limiting backfilling","<p>I'm finding that the maxConcurrency for a tumbling window trigger is limiting the number of backfilled slices.</p>

<p>e.g. maxConcurrency = 50 (the upper limit), I can only backfill 50 weeks.</p>

<p>I'd expect maxConcurrency to limit the number of in-progress slices, but I'd still expect the other backfilled weeks to be scheduled.</p>

<p>If I remember correctly from the V1 days, backfilling kicks in after a new deployment (and I think other reasons, which I can't seem to find the site describing it now)... Is there a way I can cause / trick the other slices to be scheduled? </p>

<p>Poking around the powershell commands, I only see ways to redefine a trigger not any explicit things around re-scheduling.</p>
","<azure-data-factory>","2018-06-05 15:25:45","170","0","1","50718068","<p>Ooo, correction. Looking again, it looks like the next batch has scheduled approximately 1 hour 1/2 after the first batch. Interestingly not after they completed, as each pipeline run (50 of them) took about 5 minutes.</p>

<p>Is there some internal logic causing the next batch to be scheduled? Or is perhaps one of the other parameters having a knock on impact of the 1 hour 1/2 setting? I'm unsure, I'll update this answer if I discover it.</p>
"
"50698954","Azure Data Factory use result from lookup in filename DataSink","<p>The idea is to use a lookup-activity to select the first mentioned month and year in a table. </p>

<pre><code>select MONTH(regtime) as FirstMonth , YEAR(RegTime) as Year from MyTable where ID = (select MIN(id) from MyTable)
</code></pre>

<p>Then the result for this lookup is used in the copy-activity to select the rows in this table that have a timestamp with the month and year selected in the lookup activity. This can be defining the source in the copy activity as follows:</p>

<pre><code>select * from MyTable where MONTH(RegTime) = '@{activity('LookupFirstMonth').output.firstRow.FirstMonth}' and YEAR(RegTime) = '@{activity('LookupFirstMonth').output.firstRow.Year}'
</code></pre>

<p>The next step should be to copy the rows to a data lake. In the Sink area of the copy-activityit is possible to dynamcly set the filename for the file where the data gets put in to. But I can't find out how to do it the I want it. I want the filename to reflect the month and year, selected in the lookup activity.</p>

<p>I can set the filename of the Sink to <code>@{activity('LookupFirstMonth').output.firstRow.MONTH}</code>, and then the filename will be the monthnumber. But that is not enough. It also should have the YEAR in the filename. When I conifure the filename as:</p>

<pre><code>@CONCAT(@{activity('LookupFirstMonth').output.firstRow.FirstMonth}, @{activity('LookupFirstMonth').output.firstRow.Year}, '.txt')
</code></pre>

<p>I am getting an error:</p>

<pre><code> {""code"":""BadRequest"",""message"":""The expression 'CONCAT(@{activity('LookupFirstMonth').output.firstRow.FirstMonth}, @{activity('LookupFirstMonth').output.firstRow.Year}, '.txt')' is not valid: the string character '@' at position '7' is not expected.\"""",""target"":""pipeline/pipeline1/runid/3cf0a5a9-01df-494e-bdaf-dfc66f406a83""}
</code></pre>

<p>So how do I use the outcome of the lookup activity in a filename for the Sink in the copy activity</p>
","<parameters><copy><lookup><azure-data-factory>","2018-06-05 11:27:06","2948","0","2","50699425","<p>Basically if you start your expression with @concat, everything inside concat function cant start with @. 
so try this: </p>

<pre><code>@CONCAT({activity('LookupFirstMonth').output.firstRow.FirstMonth}, {activity('LookupFirstMonth').output.firstRow.Year}, '.txt')
</code></pre>

<p>example from msdn documentation: </p>

<pre><code>""@concat('Answer is: ', string(pipeline().parameters.myNumber))""
</code></pre>

<p>so you see, before pipeline() there is no @. </p>
"
"50698954","Azure Data Factory use result from lookup in filename DataSink","<p>The idea is to use a lookup-activity to select the first mentioned month and year in a table. </p>

<pre><code>select MONTH(regtime) as FirstMonth , YEAR(RegTime) as Year from MyTable where ID = (select MIN(id) from MyTable)
</code></pre>

<p>Then the result for this lookup is used in the copy-activity to select the rows in this table that have a timestamp with the month and year selected in the lookup activity. This can be defining the source in the copy activity as follows:</p>

<pre><code>select * from MyTable where MONTH(RegTime) = '@{activity('LookupFirstMonth').output.firstRow.FirstMonth}' and YEAR(RegTime) = '@{activity('LookupFirstMonth').output.firstRow.Year}'
</code></pre>

<p>The next step should be to copy the rows to a data lake. In the Sink area of the copy-activityit is possible to dynamcly set the filename for the file where the data gets put in to. But I can't find out how to do it the I want it. I want the filename to reflect the month and year, selected in the lookup activity.</p>

<p>I can set the filename of the Sink to <code>@{activity('LookupFirstMonth').output.firstRow.MONTH}</code>, and then the filename will be the monthnumber. But that is not enough. It also should have the YEAR in the filename. When I conifure the filename as:</p>

<pre><code>@CONCAT(@{activity('LookupFirstMonth').output.firstRow.FirstMonth}, @{activity('LookupFirstMonth').output.firstRow.Year}, '.txt')
</code></pre>

<p>I am getting an error:</p>

<pre><code> {""code"":""BadRequest"",""message"":""The expression 'CONCAT(@{activity('LookupFirstMonth').output.firstRow.FirstMonth}, @{activity('LookupFirstMonth').output.firstRow.Year}, '.txt')' is not valid: the string character '@' at position '7' is not expected.\"""",""target"":""pipeline/pipeline1/runid/3cf0a5a9-01df-494e-bdaf-dfc66f406a83""}
</code></pre>

<p>So how do I use the outcome of the lookup activity in a filename for the Sink in the copy activity</p>
","<parameters><copy><lookup><azure-data-factory>","2018-06-05 11:27:06","2948","0","2","50699659","<p>The right expression should be</p>

<p>@CONCAT(activity('LookupFirstMonth').output.firstRow.FirstMonth, activity('LookupFirstMonth').output.firstRow.Year, '.txt')</p>

<p>Or </p>

<p>@{CONCAT(activity('LookupFirstMonth').output.firstRow.FirstMonth, activity('LookupFirstMonth').output.firstRow.Year, '.txt')}</p>

<p>The first one is a pure expression, while the second one is string interpolation (with {}）</p>
"
"50642734","How to migrate azure data factory to another data center","<p>We have a Azure Data Factory, hosted in <em>EastUS</em> data center. We want to move that to <em>WestUS</em> data center, as our VMs reside in the <em>WestUS</em> datacenter. Moving VMs to <em>EastUS</em> data center is tougher option, as VMs hold databases of size > 4 Tera Bytes. Creating another data factory in <em>WestUS</em> data center will be difficult, as we will lose all our previous pipeline execution logs.</p>

<p>There are options available for move to different resource group, different subscription. But, not sure on how to move a data factory to new data center location. </p>

<p>Please help. </p>
","<azure><migration><azure-data-factory>","2018-06-01 11:47:14","1793","1","1","50676872","<p>Notice how Azure Data Factory (under the Analytics category) is <a href=""https://azure.microsoft.com/en-us/global-infrastructure/services/?"" rel=""nofollow noreferrer"">not available in West US</a> but that Data Movement is. So don't stress about which region Azure Data Factory lives in. The small API calls it makes to execute a stored proc in your database will cross region and incur a tiny amount of egress charge but so little it is not worth worrying about. If Azure Data Factory does any Copy Activities they will occur using Data Movement resources in West US if data is being copied from West US to West US.</p>
<p>If the egress charges on your Azure bill prove otherwise then please clarify what ADF is doing exactly.</p>
"
"50641659","Azure Data Lake Store exposed via OData","<p>Is it possible to expose an Azure Data Lake Store via OData? The main goal is consume this service in Salesforce (via Salesforce Connect).</p>

<p>If so, should it take place through Azure Data Factory?</p>

<p><strong>Update</strong></p>

<p>A little bit more of context:</p>

<p>We have historical data stored in Azure Data Lake Storage (ADLS) that we want to expose via OData (to be visualised in Salesforce via Salesforce Connect / External objects). After digging into the issue and potential solutions, we don't think ADLS is the right service to be used in this particular case. Instead, we'll might need to configure a Data Factory pipeline to copy the data we are interested in to a SQL Database and read the data from there via a simple ASP.Net application using Entity Data Model and WCF Data Services Entity Framework Provider (got some insights from <a href=""https://developer.salesforce.com/blogs/developer-relations/2015/03/accessing-sql-server-azure-database-lightning-connect.html"" rel=""nofollow noreferrer"">this website</a>).</p>
","<azure><odata><azure-data-factory><azure-data-lake>","2018-06-01 10:42:10","1315","1","1","50694776","<p>I don't think OData has a connector for ADLS. However, given OData is basically a REST API, you could probably build an OData API over the existing ADLS REST APIs if they are not providing what you need. I am not sure how ADF would come into this picture?</p>

<p>Maybe it would be useful if you tell us what you want to achieve? </p>
"
"50639420","How to log all incompaitable rows in storage account using ADF V2 copy data tools","<p>I have selected the option of logging all the incompatible rows into the storage account default container, but there have been no logs written inside the storage account, I am wondering why is that not happening?
Is there anything which can be done to make this work?</p>
","<azure><azure-data-factory>","2018-06-01 08:43:29","48","0","1","50695091","<p>It's a regression and we are working on the fix, it's expected to be deployed by end of this week. Please try after that.</p>

<p>Update:
The issue is fixed, can you try again?</p>
"
"50633271","Getting zip and non-zipped files from ftp site via Azure Data Factory - unzipping","<p>I am successfully getting files from an ftp site via Azure Data Factory and unzipping them in the process. I am setting the source dataset for ZipDeflate. However, now I have a similar pipeline where I have to get files from an ftp site where there are a mix of .zip files and other file types like .csv and .xls. </p>

<p>When I try to do ZipDeflate on that Copy activity, it wants to ""deflate"" all of the files (not just the .zip). I kind of expected that . I'm thinking the only want to unzip only the .zip files and NOT unzip the .xls and .csv files is to do them in separate Copy activities (one for files needing unzipping and one for all others)....Is this right or is there another way to do it all in the one activity?</p>

<p>Thanks!!
J</p>
","<azure><unzip><azure-data-factory>","2018-05-31 21:48:00","1287","3","2","50642492","<p>Well yes. You need to have two copy activities. But in that case, you will have two source dataset with different file filter I guess. (one for .zip, and seccond for all others) 
Something that came to my mind is to use metadata activity to take file name. After that, in IF condition you check if the file name contains "".zip"", you proceed to activity for unzipping, and if false you continue to activity without unzipping. This way you use only one dataset that will collect all files, not to have separately. 
Hope this can work and help you. :)</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity</a></p>
"
"50633271","Getting zip and non-zipped files from ftp site via Azure Data Factory - unzipping","<p>I am successfully getting files from an ftp site via Azure Data Factory and unzipping them in the process. I am setting the source dataset for ZipDeflate. However, now I have a similar pipeline where I have to get files from an ftp site where there are a mix of .zip files and other file types like .csv and .xls. </p>

<p>When I try to do ZipDeflate on that Copy activity, it wants to ""deflate"" all of the files (not just the .zip). I kind of expected that . I'm thinking the only want to unzip only the .zip files and NOT unzip the .xls and .csv files is to do them in separate Copy activities (one for files needing unzipping and one for all others)....Is this right or is there another way to do it all in the one activity?</p>

<p>Thanks!!
J</p>
","<azure><unzip><azure-data-factory>","2018-05-31 21:48:00","1287","3","2","59195408","<p>For the moment you could use the GetMetadata+Filter activities to split the files by types and create a separate copy activities for them. Or you could parameterize the dataset, and pass there Zip method like ""ZipDeflate"" or 'None' based on the extension of the file. </p>

<p>If we are talking not only about the native mechanics, you could create a custom .Net activity and create anything you want. </p>
"
"50608834","How to use the pre-copy script from the copy activity to remove records in the sink based on the change tracking table from the source?","<p>I am trying to use change tracking to copy data incrementally from a SQL Server to an Azure SQL Database. I followed the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-change-tracking-feature-portal"" rel=""noreferrer"">tutorial</a> on Microsoft Azure documentation but I ran into some problems when implementing this for a large number of tables. </p>

<p>In the source part of the copy activity I can use a query that gives me a change table of all the records that are updated, inserted or deleted since the last change tracking version. This table will look something like</p>

<pre><code>PersonID   Age    Name   SYS_CHANGE_OPERATION
---------------------------------------------
1          12     John   U
2          15     James  U
3          NULL   NULL   D
4          25     Jane   I
</code></pre>

<p>with PersonID being the primary key for this table.</p>

<p>The problem is that the copy activity can only append the data to the Azure SQL Database so when a record gets updated it gives an error because of a duplicate primary key. I can deal with this problem by letting the copy activity use a stored procedure that merges the data into the table on the Azure SQL Database, but the problem is that I have a large number of tables. </p>

<p>I would like the pre-copy script to delete the deleted and updated records on the Azure SQL Database, but I can't figure out how to do this. Do I need to create separate stored procedures and corresponding table types for each table that I want to copy or is there a way for the pre-copy script to delete records based on the change tracking table? </p>
","<sql-server><azure-sql-database><azure-data-factory>","2018-05-30 15:46:23","20338","8","2","50609849","<p>You have to use a LookUp activity before the Copy Activity. With that LookUp activity you can query the database so that you get the deleted and updated PersonIDs, preferably all in one field, separated by comma (so its easier to use in the pre-copy script). More information here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity</a></p>

<p>Then you can do the following in your pre-copy script:</p>

<pre><code>delete from TableName where PersonID in (@{activity('MyLookUp').output.firstRow.PersonIDs})
</code></pre>

<p>This way you will be deleting all the deleted or updated rows before inserting the new ones.</p>

<p>Hope this helped!</p>
"
"50608834","How to use the pre-copy script from the copy activity to remove records in the sink based on the change tracking table from the source?","<p>I am trying to use change tracking to copy data incrementally from a SQL Server to an Azure SQL Database. I followed the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-change-tracking-feature-portal"" rel=""noreferrer"">tutorial</a> on Microsoft Azure documentation but I ran into some problems when implementing this for a large number of tables. </p>

<p>In the source part of the copy activity I can use a query that gives me a change table of all the records that are updated, inserted or deleted since the last change tracking version. This table will look something like</p>

<pre><code>PersonID   Age    Name   SYS_CHANGE_OPERATION
---------------------------------------------
1          12     John   U
2          15     James  U
3          NULL   NULL   D
4          25     Jane   I
</code></pre>

<p>with PersonID being the primary key for this table.</p>

<p>The problem is that the copy activity can only append the data to the Azure SQL Database so when a record gets updated it gives an error because of a duplicate primary key. I can deal with this problem by letting the copy activity use a stored procedure that merges the data into the table on the Azure SQL Database, but the problem is that I have a large number of tables. </p>

<p>I would like the pre-copy script to delete the deleted and updated records on the Azure SQL Database, but I can't figure out how to do this. Do I need to create separate stored procedures and corresponding table types for each table that I want to copy or is there a way for the pre-copy script to delete records based on the change tracking table? </p>
","<sql-server><azure-sql-database><azure-data-factory>","2018-05-30 15:46:23","20338","8","2","69896947","<p>In the meanwhile the Azure Data Factory provides the meta-data driven copy task. After going through the dialogue driven setup, a metadata table is created, which has one row for each dataset to be synchronized. I solved this UPSERT problem by adding a stored procedure as well as a table type for each dataset to be synchronized. Then I added the relevant information in the metadata table for each row like this</p>
<pre><code>    {
            &quot;preCopyScript&quot;: null,
            &quot;tableOption&quot;: &quot;autoCreate&quot;,
            &quot;storedProcedure&quot;: &quot;schemaname.UPSERT_SHOP_SP&quot;,
            &quot;tableType&quot;: &quot;schemaname.TABLE_TYPE_SHOP&quot;,
            &quot;tableTypeParameterName&quot;: &quot;shops&quot;
    }
</code></pre>
<p>After that you need to adapt the sink properties of the copy task like this (stored procedure, table type, table type parameter name):</p>
<pre><code>@json(item().CopySinkSettings).storedProcedure
@json(item().CopySinkSettings).tableType
@json(item().CopySinkSettings).tableTypeParameterName
</code></pre>
<p>If the destination table does not exist, you need to run the whole task once before adding the above variables, because auto-create of tables works only as long as no stored procedure is given in the sink properties.</p>
"
"50598698","Azure:How to copy partitioned data from from data lake","<p>The folder structure on my data lake is /raw/project/countryName/year/files.json
How can I create a copy activity to read all files in each folder?</p>

<p>In the documentation I could only find partitioned by data example:</p>

<pre><code>""folderPath"": {
      ""value"": ""adfcustomerprofilingsample/logs/marketingcampaigneffectiveness/@{formatDateTime(pipeline().parameters.windowStartTime, 'yyyy/MM/dd')}/"",
      ""type"": ""Expression""
}, 
</code></pre>
","<azure><azure-data-factory>","2018-05-30 07:23:59","848","0","1","50600056","<p>""folderPath"": {
      ""value"": ""/raw/project/countryName/@{formatDateTime(pipeline().parameters.windowStartTime, 'yyyy')}/"",
      ""type"": ""Expression""
}, </p>

<p>Specify your folderPath as above, and don’t specify your fileName.
In your copy source typeProperties, set recursive as true.</p>
"
"50589886","Azure pipeline copy data activity for copying data from Azure MSSQL to Azure MySQL","<p>I'm aware that Azure Pipeline's Copy Data Activity does not support MySQL as sink. But is there any workaround via some other component to do so?</p>
","<mysql><azure><pipeline><azure-data-factory>","2018-05-29 17:30:35","125","0","1","50606748","<p>It depends on the ammount of rows you are trying to copy. If you just need a few tables/rows, you may try with Azure Automation. That way you can just create a runbook with Powershell that connects to Azure Sql Server, queries the server and then sends that data to the Azure MySql Server. Then you can call the runbook from data factory using a webhook :)</p>

<p>If you end up going this route, remember that runbooks have a limitation and cannot run for more than 3 hours. More information here: <a href=""https://learn.microsoft.com/en-us/azure/automation/automation-runbook-execution#fair-share"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/automation/automation-runbook-execution#fair-share</a></p>

<p>Another option would be to create a custom activity for data factory. For this you need an Azure Batch pool. More here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity</a></p>

<p>Hope this helped!</p>
"
"50587416","Converting Avro file to JSONs using JSONOutputer","<p>I have a requirement where I need to extract JSON files from Avro by using U-SQL. Avro files contain a collection of JSON and need to extract individual JSON from Avro. Here I have written code.
I used JsonOuptter() for writing jsons.</p>

<pre><code>OUTPUT @source
TO ""C:/Users/gutthil1/Desktop/ActivityTracker/Single_JSON/schemaType.json""
USING new JsonOutputter();
</code></pre>

<p>I am getting JSON as like below. </p>

<pre><code>[
  {
    ""type"": ""CE"",
    ""cnn"": ""11"",
    ""source"": ""{\r\n  \""vendorCd\"": \""G\"",\r\n  \""brandCd\"": \""4\"",\r\n  \""channelCd\"": \""C\"",\r\n  \""accessTypeCd\"": \""I\""\r\n}"",

  },
  {
    ""type"": ""CE"",
    ""cnn"": ""9"",
    ""source"": ""{\r\n  \""vendorCd\"": \""G\"",\r\n  \""brandCd\"": \""4\"",\r\n  \""channelCd\"": \""C\"",\r\n  \""accessTypeCd\"": \""I\""\r\n}"",

  }
]
</code></pre>

<p>I am expecting JSON as like below.</p>

<pre><code>[
{  
   ""type"":""CE"",
   ""ccn"":""11"",
   ""source"":{  
      ""vendorCd"":""G"",
      ""brandCd"":""4"",
      ""channelCd"":""C"",
      ""accessTypeCd"":""I""
   },
{  
   ""type"":""CE"",
   ""ccn"":""9"",
   ""source"":{  
      ""vendorCd"":""G"",
      ""brandCd"":""4"",
      ""channelCd"":""C"",
      ""accessTypeCd"":""I""
   }
]
</code></pre>

<p>Thanks for your help and support.</p>
","<azure><azure-data-factory><azure-data-lake><u-sql>","2018-05-29 14:55:58","214","0","1","50591574","<p>Your <code>source</code> column is of type <code>string</code> and thus gets inlined as a string value and not a JSON object. You currently have the following two options, both probably requiring you to extend the JSONOutputter code:</p>

<ol>
<li>Add an option that allows you to specify which columns should be inlined as a JSON fragment instead of a string.</li>
<li>Add an option to treat SqlMap as the property set for the column instead of string.</li>
</ol>
"
"50576879","Azure DataFactory unable to check Version using Definition","<p>Unable to check api version in Definition JSON. Below is the definition along with JSON.</p>

<pre><code>    New-AzureRmPolicyDefinition -Policy '{
     ""if"": {
    ""allOf"": [
      {
        ""field"": ""type"",
        ""equals"": ""Microsoft.DataFactory/factories""
      },
      {
        ""field"": ""apiVersion"",
        ""equals"": ""2017-09-01-preview""
      }
    ]
  },
  ""then"": {
    ""effect"": ""deny""
  }
}' -Name 'DataFactoryVersionRestriction'
</code></pre>

<p>When I run above definition, it is throwing error related to alias. Can someone please help?</p>

<p><a href=""https://i.stack.imgur.com/QOXuJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QOXuJ.png"" alt=""enter image description here""></a></p>
","<json><azure><azure-data-factory><azure-powershell><policy>","2018-05-29 05:29:25","202","1","1","59655715","<p>Use the alias Microsoft.DataFactory/factories/version</p>
"
"50545829","Use lookup from one Azure DF Pipeline into another Pipeline","<p>I have a Lookup Activity in a pipeline. Sequential to it i have multiple Execute Pipeline Activities. I want to use that lookup within the execute pipeline activities. Is that possible? Or re-creating the lookup in each the execute pipeline activity is the only option? </p>

<p>I'm using ADF v2.</p>
","<azure><lookup><pipeline><azure-data-factory>","2018-05-26 18:25:00","1222","1","2","50548549","<p>Not quite understand your scenario. If you just need the result of lookup, why don’t just pass the output of lookup to your execute pipeline activities? Why need are-create a lookup activity?</p>
"
"50545829","Use lookup from one Azure DF Pipeline into another Pipeline","<p>I have a Lookup Activity in a pipeline. Sequential to it i have multiple Execute Pipeline Activities. I want to use that lookup within the execute pipeline activities. Is that possible? Or re-creating the lookup in each the execute pipeline activity is the only option? </p>

<p>I'm using ADF v2.</p>
","<azure><lookup><pipeline><azure-data-factory>","2018-05-26 18:25:00","1222","1","2","50558269","<p>To pass the output of lookup activity to an execute pipeline, you need to define a parameter in execute pipeline, and use '@activity('LookupTableList').output.value' to set the value of the pipeline parameter, reference <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-bulk-copy-portal#create-pipelines"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-bulk-copy-portal#create-pipelines</a> to see how <strong>TriggerCopy</strong> execute pipeline use the output of <strong>LookupTableList</strong> lookup activity, which is the exactly same as your scenario.</p>
"
"50533084","Using Azure Data Factory and OData connector to get data from Google-Facebook","<p>I'd like to know if it is possible to get data from Google (e.g. Google Analytics) and Facebook by using Azure Data Factory v1 and the supported OData connector.
Any suggests to me? Thanks</p>
","<azure><azure-data-factory>","2018-05-25 16:03:21","355","-3","1","50567349","<p>I think this would depend on: could you get the odata service url and authentication?</p>
"
"50524501","Exists with GetMetadata always returns true","<p>Using V2, I am trying to find out if a folder exists in an Azure blob storage (I am aware that 'folder' is a pseudo-name, since it is just part of a file url). When I attempt to do this, it always returns true, no matter if the folder exists or not.</p>

<p>Below is an example where it should return false, but returns true. The folderpath ""test/2018/5/25"" does not exist, but Get Metadata returns ""exists: true, itemName: 25"". Other folder paths exists, such as ""test/2018/5/24, but not the one ending in 25, because there is no data for the 25th. </p>

<p>Any ideas?</p>

<p>Pipeline</p>

<pre><code>{
""name"": ""Testing"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""Get Metadata1"",
            ""type"": ""GetMetadata"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false
            },
            ""typeProperties"": {
                ""dataset"": {
                    ""referenceName"": ""metdatatest"",
                    ""type"": ""DatasetReference""
                },
                ""fieldList"": [
                    ""itemName"",
                    ""exists""
                ]
            }
        }
    ]
  }
}
</code></pre>

<p>Dataset</p>

<pre><code>{
""name"": ""metdatatest"",
""properties"": {
    ""linkedServiceName"": {
        ""referenceName"": ""xxx"",
        ""type"": ""LinkedServiceReference""
    },
    ""type"": ""AzureBlob"",
    ""typeProperties"": {
        ""format"": {
            ""type"": ""TextFormat"",
            ""columnDelimiter"": ""\t"",
            ""nullValue"": ""\\N"",
            ""treatEmptyAsNull"": true,
            ""skipLineCount"": 1,
            ""firstRowAsHeader"": false
        },
        ""fileName"": """",
        ""folderPath"": ""test/2018/5/25""
    }
  }
}
</code></pre>
","<azure><azure-data-factory>","2018-05-25 08:12:17","2630","0","1","50526218","<p>I can repro the same, and this is a bug. It currently returns whether its container exists or not, but didn't check the existence of this ""virtual"" folder.</p>

<p>As blob doesn't have a folder concept, in order to know whether such a folder exists, it will need to do a file search to check if any file is under this folder, which could lead to a performance issue. I will circle back to the PROD team. Please stay tuned...</p>
"
"50521792","Azure Data Factory v2 If activity always fails","<p>I'm currently struggling with the Azure Data Factory v2 If activity which always fails with this error message:
<a href=""https://i.stack.imgur.com/EQXiF.png"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p>I've designed two separate pipelines, one takes the full snapshot of the data (1333 records) from the on-premises SQL Server and loads the data into the Azure SQL Database, and another one just takes delta from the same source.</p>

<p>Both pipelines work fine when executed independently.</p>

<p>I then decided to wrap these two pipelines into the one parent pipeline which would do this:
1. 
Execute LookUp activity to check if the target table in Azure SQL Database has any records, basic Select Count(Request_ID) As record_count From target_table - activity works fine, I can preview the returned record count.</p>

<p>2.
Pass the output from the LookUp activity to the If activity with the conditions that if record_count = 0, the parent pipeline would invoke the full load pipeline, otherwise the parent pipeline would invoke the delta load pipeline.</p>

<p>This is the actual expression:<br>
{@activity('lookup_sites_record_count').output.firstRow.record_count}==0""</p>

<p>Whenever I try to execute this parent pipeline, it fails with the above message of ""Activity  failed: Activity failed because an inner activity failed.""</p>

<p>Both inner activities, that is, full load and delta load pipelines, work just fine when triggered independently.</p>

<p>What I'm missing?</p>

<p>Many thanks in advance :).</p>

<p>mikhailg</p>

<p>Pipeline's JSON definition below:</p>

<pre><code>{
""name"": ""pl_remedyreports_load_rs_sites"",
""properties"": {
    ""activities"": [
        {
            ""name"": ""lookup_sites_record_count"",
            ""type"": ""Lookup"",
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""retry"": 0,
                ""retryIntervalInSeconds"": 30,
                ""secureOutput"": false
            },
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""SqlSource"",
                    ""sqlReaderQuery"": ""Select Count(Request_ID) As record_count From mdp.RS_Sites;""
                },
                ""dataset"": {
                    ""referenceName"": ""ds_azure_sql_db_sites"",
                    ""type"": ""DatasetReference""
                }
            }
        },
        {
            ""name"": ""If_check_site_record_count"",
            ""type"": ""IfCondition"",
            ""dependsOn"": [
                {
                    ""activity"": ""lookup_sites_record_count"",
                    ""dependencyConditions"": [
                        ""Succeeded""
                    ]
                }
            ],
            ""typeProperties"": {
                ""expression"": {
                    ""value"": ""{@activity('lookup_sites_record_count').output.firstRow.record_count}==0"",
                    ""type"": ""Expression""
                },
                ""ifFalseActivities"": [
                    {
                        ""name"": ""pl_remedyreports_invoke_load_sites_inc"",
                        ""type"": ""ExecutePipeline"",
                        ""typeProperties"": {
                            ""pipeline"": {
                                ""referenceName"": ""pl_remedyreports_load_sites_inc"",
                                ""type"": ""PipelineReference""
                            }
                        }
                    }
                ],
                ""ifTrueActivities"": [
                    {
                        ""name"": ""pl_remedyreports_invoke_load_sites_full"",
                        ""type"": ""ExecutePipeline"",
                        ""typeProperties"": {
                            ""pipeline"": {
                                ""referenceName"": ""pl_remedyreports_load_sites_full"",
                                ""type"": ""PipelineReference""
                            }
                        }
                    }
                ]
            }
        }
    ],
    ""folder"": {
        ""name"": ""Load Remedy Reference Data""
    }
}
</code></pre>

<p>}</p>
","<azure-data-factory>","2018-05-25 04:56:59","4444","0","1","50523517","<p>Your expression should be:</p>

<pre><code>@equals(activity('lookup_sites_record_count').output.firstRow.record_count,0)
</code></pre>
"
"50518190","Azure Data Factory V2: Optional parameters","<p>I'm using ADFv2 and I would like to use some optional parameters defined at the dataset level. I didn't found if that can be achieved. </p>

<p>If I define a parameter in the dataset (after defining a parameter at the pipeline level and connecting the two so the pipeline passes the value to the parameter at the dataset level) when I run the pipeline manually and don't specify a value I get the error</p>

<pre><code>No value provided for Parameter 'parameter'
</code></pre>
","<azure><azure-data-factory>","2018-05-24 21:07:56","12045","9","8","50520537","<p>What would you expected if you don't specify a value for the parameter? 
If you have a default value for that parameter, you could specify default value when defining a parameter. <code>
""parameters"": {
            ""parameter1"": {
                ""type"": ""String"",
                ""defaultValue"": ""defaultValue1""
            }
        }</code></p>
"
"50518190","Azure Data Factory V2: Optional parameters","<p>I'm using ADFv2 and I would like to use some optional parameters defined at the dataset level. I didn't found if that can be achieved. </p>

<p>If I define a parameter in the dataset (after defining a parameter at the pipeline level and connecting the two so the pipeline passes the value to the parameter at the dataset level) when I run the pipeline manually and don't specify a value I get the error</p>

<pre><code>No value provided for Parameter 'parameter'
</code></pre>
","<azure><azure-data-factory>","2018-05-24 21:07:56","12045","9","8","52726676","<p>Just for anyone that may be looking to do something like this. At least until today's date we can't have optional parameters for a pipeline, this is by design according to what the Product Team of ADF has told me through a support ticket in Azure.</p>
"
"50518190","Azure Data Factory V2: Optional parameters","<p>I'm using ADFv2 and I would like to use some optional parameters defined at the dataset level. I didn't found if that can be achieved. </p>

<p>If I define a parameter in the dataset (after defining a parameter at the pipeline level and connecting the two so the pipeline passes the value to the parameter at the dataset level) when I run the pipeline manually and don't specify a value I get the error</p>

<pre><code>No value provided for Parameter 'parameter'
</code></pre>
","<azure><azure-data-factory>","2018-05-24 21:07:56","12045","9","8","55125422","<p>@fgbaezp, this is a late answer but you can assign an empty string to a parameter by using dynamic content, something like @toLower('')</p>
"
"50518190","Azure Data Factory V2: Optional parameters","<p>I'm using ADFv2 and I would like to use some optional parameters defined at the dataset level. I didn't found if that can be achieved. </p>

<p>If I define a parameter in the dataset (after defining a parameter at the pipeline level and connecting the two so the pipeline passes the value to the parameter at the dataset level) when I run the pipeline manually and don't specify a value I get the error</p>

<pre><code>No value provided for Parameter 'parameter'
</code></pre>
","<azure><azure-data-factory>","2018-05-24 21:07:56","12045","9","8","62234612","<p>We have a dataset for blob csv files.  The dataset has the filename parameterised, as well as the container and directory.</p>

<p><a href=""https://i.stack.imgur.com/GBdRl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GBdRl.png"" alt=""enter image description here""></a></p>

<p>When this dataset is a sink, we might want to specify the file to write to, or leave it blank and let Azure provide data_xxxx.csv filenames for us.</p>

<p>When this dataset is a source, we might want to specify which file to open, or leave it blank to use the wildcard option (all csv in the directory).</p>

<p>In that limited sense, we want the filename parameter to be optional.</p>

<p>But you can't leave the dataset parameter field blank in the UI or the pipeline won't compile (validate).</p>

<p><a href=""https://i.stack.imgur.com/ilhSS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ilhSS.png"" alt=""enter image description here""></a><a href=""https://i.stack.imgur.com/18OLZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/18OLZ.png"" alt=""enter image description here""></a></p>

<p>In those cases where we want the filename to be blank we use the trick suggested above to inject an empty string (we use <code>@coalesce(pipeline().parameters.filename,''</code>) and leave the filename parameter empty when triggering the pipeline.</p>

<p><a href=""https://i.stack.imgur.com/G03AV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G03AV.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/w77jA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w77jA.png"" alt=""enter image description here""></a></p>
"
"50518190","Azure Data Factory V2: Optional parameters","<p>I'm using ADFv2 and I would like to use some optional parameters defined at the dataset level. I didn't found if that can be achieved. </p>

<p>If I define a parameter in the dataset (after defining a parameter at the pipeline level and connecting the two so the pipeline passes the value to the parameter at the dataset level) when I run the pipeline manually and don't specify a value I get the error</p>

<pre><code>No value provided for Parameter 'parameter'
</code></pre>
","<azure><azure-data-factory>","2018-05-24 21:07:56","12045","9","8","68104854","<p>I ran into this issue recently and the answers provided here, didn't quite address my needs. The way that we are using the ADF pipelines relies on the parameter to be set if you need to run it manually or if being run by a trigger, to be empty so that they can be dynamically set to the date/time that the trigger was activated.</p>
<p>In our pipeline we have checks like the following.</p>
<pre><code>@if(empty(string(pipeline().parameters.FolderDate)),
formatDateTime(utcnow(),'yyyyMMdd'),
pipeline().parameters.FolderDate)
</code></pre>
<p>It turns out that if you provide the parameter as a lowercase null, then my pipeline works as expected.</p>
<p><a href=""https://i.stack.imgur.com/8A13f.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/8A13f.png"" alt=""ADF Parameter Setting"" /></a></p>
"
"50518190","Azure Data Factory V2: Optional parameters","<p>I'm using ADFv2 and I would like to use some optional parameters defined at the dataset level. I didn't found if that can be achieved. </p>

<p>If I define a parameter in the dataset (after defining a parameter at the pipeline level and connecting the two so the pipeline passes the value to the parameter at the dataset level) when I run the pipeline manually and don't specify a value I get the error</p>

<pre><code>No value provided for Parameter 'parameter'
</code></pre>
","<azure><azure-data-factory>","2018-05-24 21:07:56","12045","9","8","68243927","<p>Try <code>@coalesce(null)</code> as the expression for the parameter.</p>
"
"50518190","Azure Data Factory V2: Optional parameters","<p>I'm using ADFv2 and I would like to use some optional parameters defined at the dataset level. I didn't found if that can be achieved. </p>

<p>If I define a parameter in the dataset (after defining a parameter at the pipeline level and connecting the two so the pipeline passes the value to the parameter at the dataset level) when I run the pipeline manually and don't specify a value I get the error</p>

<pre><code>No value provided for Parameter 'parameter'
</code></pre>
","<azure><azure-data-factory>","2018-05-24 21:07:56","12045","9","8","71022834","<p>Instead of <code>@coalesce</code> you can use <code>?</code> like <code>@dataset()?.fileName</code> if empty return null.
&quot;defaultValue&quot;: you can fill <code>@null</code> or <code>[]</code> or better <code>@''</code>. Data Factory and Synapse Analytics cross pass parameters need some input to validate pipeline.</p>
<p><a href=""https://i.stack.imgur.com/1mTjI.png"" rel=""nofollow noreferrer"">Null Values Sample</a></p>
<p><a href=""https://i.stack.imgur.com/YYxDo.png"" rel=""nofollow noreferrer"">If Null return null</a></p>
"
"50518190","Azure Data Factory V2: Optional parameters","<p>I'm using ADFv2 and I would like to use some optional parameters defined at the dataset level. I didn't found if that can be achieved. </p>

<p>If I define a parameter in the dataset (after defining a parameter at the pipeline level and connecting the two so the pipeline passes the value to the parameter at the dataset level) when I run the pipeline manually and don't specify a value I get the error</p>

<pre><code>No value provided for Parameter 'parameter'
</code></pre>
","<azure><azure-data-factory>","2018-05-24 21:07:56","12045","9","8","71665537","<p>The workaround for this is using coalesce in dynamic content and leaving the parameter blank.</p>
<p>ex: <strong>@coalesce(pipeline().parameters.parametername,'') in dynamic content</strong></p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#coalesce"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#coalesce</a></p>
<p><a href=""https://i.stack.imgur.com/bY64R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bY64R.png"" alt=""enter image description here"" /></a></p>
"
"50503741","Copy data from private s3 implementation using Azure Data Factory","<p>I'm looking into using azure data factory for copying files from a S3 bucket that is <strong>not</strong> hosted at Amazon. The current solution that we are using includes some azure functions, a logic app and a web app. With azure data factory this should al be much simpler and easier to maintain. </p>

<p>I've looked into the properties that can be given for an Amazon S3 linked service. </p>

<pre><code>""name"": ""AmazonS3LinkedService"",    
""properties"": {
    ""type"": ""AmazonS3"",
    ""typeProperties"": {
        ""accessKeyId"": ""&lt;access key id&gt;"",
        ""secretAccessKey"": {
                ""type"": ""SecureString"",
                ""value"": ""&lt;secret access key&gt;""
        }
    },
    ""connectVia"": {
        ""referenceName"": ""&lt;name of Integration Runtime&gt;"",
        ""type"": ""IntegrationRuntimeReference""
    }
}
</code></pre>

<p>But i do not see a property to set a different host in the documentation.</p>

<p>My question is, is this possible at all?</p>
","<azure><amazon-s3><azure-data-factory>","2018-05-24 07:44:09","463","0","1","50520504","<p>I think it is just a connector for Amazon S3. At least from model layer, there is no property for different host. </p>
"
"50503363","when copying table from AzureDataLakeStore to ASQLServer string fields appear between double quotes in ASQLServer","<p>I have a table in AzureDataLake, and a table defined in AzureSQLServer. 
One of the activities in the pipeline copies form one to the other. 
However, for some reason string fields appear between double quotes 
<a href=""https://i.stack.imgur.com/dUmR0.png"" rel=""nofollow noreferrer"">screenshot of what I get</a></p>

<p>the table looks fine when I do a preview in DataLakseStoreAnalytics, where it's also replicated <a href=""https://i.stack.imgur.com/gYo3Y.png"" rel=""nofollow noreferrer"">see here</a></p>

<p>any clue of what is going on? 
It might be something quite basic, but just got started with Azure</p>
","<azure><azure-data-factory><azure-data-lake><azure-sql-server>","2018-05-24 07:22:51","82","0","1","50504086","<p>What is your source file format? Does it have "" ?
In  your input dataset, in the format section, you could try to add </p>

<pre><code>""typeProperties"": {
        ""format"": {
            ""quoteChar"": ""\""""
        }
</code></pre>

<p>}</p>
"
"50494044","Using an incremental id as watermark for copying data in azure data factory pipeline instead of date time","<p>I'm able to incrementally load data from an source Azure MSSQL DB to a sink Azure MSSQL DB using a timestamp. For some reason i wish to incrementally load the data using an incremental id in the source database instead of a timestamp. Is this possible?</p>

<p>I need to run the Copy Data activity only once a day. So i would also want to store the batch id for each of the copy data activity in a batch_details table.</p>

<p>I'm using ADF v.2</p>

<p>I'm a new to azure, How do i do it?</p>
","<azure><azure-sql-database><pipeline><azure-data-factory>","2018-05-23 17:17:09","1290","0","1","50495122","<p>What you ask is basically what is explained in this tutorial from the official documentation: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-overview"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-overview</a></p>

<p>You can use whatever you want as a watermark, the tutorial uses a datetime value, but an incremental id works too.</p>

<p>To run the pipeline once a day, use a trigger: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#schedule-trigger"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#schedule-trigger</a></p>

<p>Hope this helped!</p>
"
"50491491","Azure Data Factory v2 Pipeline API call for trigger timestamp","<p>I am new to working with azure data factory and I am trying to set up a pipeline that fetches data from an API for a specific time frame. 
Here is what I have been doing so far:
I created a tumbling window pipeline for API call to bring in data as JSON file and store in ADL folder with specific timestamp.
The problem is that I want just data for the current day and hour and store in that specific folder e.g.year/month/day/hour/ json file
How can I tell the URL just to fetch data within the hour the trigger was activated (trigger is set for every full hour and needs to fetch data from e.g. 3am till 3:59.
I was able to get data from specific timestamp but not current time which would be linked to trigger time.
URL code:
?sourceId=1LuOA,9VKZ2,CISG1,D7UIQ,gu4me,hSZGT,K582n,MnkPP,Mxgt4,N4hAZ,PvECt,Qhr1i,uIWnW,vot1K,XAbJ4,XRH1E,ZbKW0,Zjrs3,ZttLo&amp;interval=S10&amp;maxResult=500&amp;startTimestamp=2018-05-16T00:00:00%2B00:00&amp;endTimestamp=2018-05-16T00:59:59%2B00:00</p>

<p>sink Code: </p>

<pre><code>beacon/@{formatDateTime(adddays(utcnow(),0),'yyyy')}/@{formatDateTime(adddays(utcnow(),0),'MM')}/@{formatDateTime(adddays(utcnow(),0),'dd')}
</code></pre>

<p>when I try to add hours I get a weird folder that does not represent hours.</p>
","<api><triggers><pipeline><azure-data-factory>","2018-05-23 14:53:00","934","0","1","50566604","<p>In your pipeline, define a pipeline parameter, say windowStart. Then Pass trigger().outputs.windowStartTime to your pipeline parameter. 
And then if your expected folder name is abc/year/month/day/hour.
You could use<code>abc/@{formatDateTime(pipeline().parameters.windowStart,'yyyy')/@{formatDateTime(pipeline().parameters.windowStart,'MM')/@{formatDateTime(pipeline().parameters.windowStart, 'dd')/@{formatDateTime(pipeline().parameters.windowStart,'HH')}</code></p>

<p>Please reference this for tumbling window.
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger#tumbling-window-trigger-type-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger#tumbling-window-trigger-type-properties</a></p>
"
"50478911","Azure Data Factory Pricing per activity","<p>On this page it says that </p>

<p><a href=""https://azure.microsoft.com/en-us/pricing/details/data-factory/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/pricing/details/data-factory/</a></p>

<pre><code>PRICE: First 50,000 activity runs—$0.55 per 1,000 runs 


Example: copy activity moving data from an Azure blob to an Azure SQL database;
</code></pre>

<p>If i understand this correctly, if for example i make an activity that reads a blob that contains text and then puts that text into sql database, that would cost per 0.55 per 1000 runs? That is very expensive. </p>

<p>Note usually one can have multiple activities in a pipeline.</p>

<p>So if i read a blob from azure storage account, put it in sql azure, then send an e-mail, you already have 3 activities.</p>

<p>In azure function I pay about 0.20 dollars per million executions and $0.000016 per gb per second. ( that means that If i have 1 gb photo in memory for 2 seconds i pay 0.000016 x 2 = 0.000032.</p>

<p>Is the pricing massive or am I missing something?</p>
","<azure><azure-data-factory>","2018-05-23 02:22:07","3467","0","2","50495438","<p>I don't see why would you compare it with Azure Functions, they are different services with different pricing models.</p>

<p>Regarding Data Factory, on top of the activities, you have to consider the data movement cost also.</p>

<p>I don't think its massive as you say, considering its secure, fast, scalable, etc. If you are a big company that is moving its important data to the cloud, you shouldn't be afraid of paying 10-20 dollars a month (even 100!) to get your data to the cloud safely.</p>

<p>Also, if you are using a massive amount of activities and the price is out of control (in the super rare case it does), you are probably not engineering well enough your data movements.</p>

<p>Hope this helped!</p>
"
"50478911","Azure Data Factory Pricing per activity","<p>On this page it says that </p>

<p><a href=""https://azure.microsoft.com/en-us/pricing/details/data-factory/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/pricing/details/data-factory/</a></p>

<pre><code>PRICE: First 50,000 activity runs—$0.55 per 1,000 runs 


Example: copy activity moving data from an Azure blob to an Azure SQL database;
</code></pre>

<p>If i understand this correctly, if for example i make an activity that reads a blob that contains text and then puts that text into sql database, that would cost per 0.55 per 1000 runs? That is very expensive. </p>

<p>Note usually one can have multiple activities in a pipeline.</p>

<p>So if i read a blob from azure storage account, put it in sql azure, then send an e-mail, you already have 3 activities.</p>

<p>In azure function I pay about 0.20 dollars per million executions and $0.000016 per gb per second. ( that means that If i have 1 gb photo in memory for 2 seconds i pay 0.000016 x 2 = 0.000032.</p>

<p>Is the pricing massive or am I missing something?</p>
","<azure><azure-data-factory>","2018-05-23 02:22:07","3467","0","2","50496669","<p>That current pricing link for ADF is during ADF V2 preview and is .55 for every 1000 runs up to 50k runs, .50 / 1000 after that.</p>

<p>So, if you have a copy activity that runs 1000 times, you pay .55 + data movement cost / hr.</p>
"
"50468103","Error while transferring null decimal value to Azure Data Warehouse using polybase enabled in Azure Data Factory v2","<p>We have Azure Data Factory v2 pipeline which transfers data from Azure SQL Database to Azure Data Warehouse using polybase enabled.</p>

<p>Source table contains null decimal values but when ADF tries to process null value it is giving an error :</p>

<blockquote>
  <p>,Errors=[{Class=16,Number=107090,State=1,Message=Query aborted-- the maximum reject threshold (0 rows) was reached while reading from an external source: 1 rows rejected out of total 1 rows processed.\n(/24b40621-2542-4406-8f32-7854fe030292/Polybase/data_24b40621-2542-4406-8f32-7854fe030292_b592f424-dd7b-47af-925f-a2934aea4b67.txt)Column ordinal: 7&#44; Expected data type: DECIMAL(18&#44;3)&#44; Offending value: \u0000\u0000\u0000\u0000\u0000\u0000\u0000\</p>
</blockquote>

<p>We can easily resolve this error by using ISNULL(COLUMNNAME, 0) in Source Query but we don't want to do that. </p>

<p>On Azure official Feedback site, this issue seems under review since long.
<a href=""https://feedback.azure.com/forums/307516-sql-data-warehouse/suggestions/10600192-polybase-fix-file-format-type-default-for-decima"" rel=""nofollow noreferrer"">https://feedback.azure.com/forums/307516-sql-data-warehouse/suggestions/10600192-polybase-fix-file-format-type-default-for-decima</a></p>
","<azure><azure-synapse><azure-data-factory>","2018-05-22 12:46:19","1897","2","1","50470547","<p>It is weird, the documentation talks a lot about nulls when source is data lake store or blob storage, but doesn't say a thing when source is another database. Reading in the polybase documentation shows that Azure SQL database is not supported: <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-guide?view=sql-server-2017"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-guide?view=sql-server-2017</a></p>

<p>I would try disabling polybase to see if this solves the issue. In case it does, the performance will decrease but it may be enough to meet your needs.</p>

<p>Another way to work with polybase is creating a stored procedure in the Sql Data Warehouse that uses polybase to query another database and calling it with Data Factory, this way you will have more control over what polybase is doing.</p>

<p>Hope this helped! :)</p>
"
"50448175","Can we update or delete the VSTS-Git configuration from Azure Data Factory?","<p>We have to configure a vsts git to ADF for the deployment. I want to edit the collaboration branch. How can we do that? Below is the screenshot</p>

<p><a href=""https://i.stack.imgur.com/HAqiB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HAqiB.png"" alt=""enter image description here""></a></p>
","<continuous-integration><azure-devops><azure-pipelines><azure-data-factory>","2018-05-21 11:45:23","1260","0","2","50448779","<p>Yes. You can delete the git and re-add it. Go to the overview page first and then click the git button at the right top corner.</p>

<p><a href=""https://i.stack.imgur.com/Scti1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Scti1.png"" alt=""enter image description here""></a></p>
"
"50448175","Can we update or delete the VSTS-Git configuration from Azure Data Factory?","<p>We have to configure a vsts git to ADF for the deployment. I want to edit the collaboration branch. How can we do that? Below is the screenshot</p>

<p><a href=""https://i.stack.imgur.com/HAqiB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HAqiB.png"" alt=""enter image description here""></a></p>
","<continuous-integration><azure-devops><azure-pipelines><azure-data-factory>","2018-05-21 11:45:23","1260","0","2","50485569","<p>If you want to edit the collaboration branch. There are two options.</p>

<ol>
<li>Remove your git and re-add it again. You have the chance to specify the new collaboration branch during the add process.</li>
<li>You could do this in azure portal. (portal.azure.com) In your data factory tags, remove and re-create the  <strong>gitRepoCollaborationBranch</strong> tag. </li>
</ol>

<p>Currently, author &amp; monitor module doesn't support edit collaboration branch.</p>
"
"50447852","Is it possible to connect ssh enabled MySql database to azure data factory?","<p>Using a virtual machine to get ssh enabled MySql database(on-premise data)and trying to copy on the cloud using Data Factory. Is it possible to connect ssh enabled MySql database to Azure data factory?</p>
","<azure-data-factory>","2018-05-21 11:26:42","789","0","1","55182792","<p>We are not able to connect directly. There is a workaround through which we create a tunnel using ""bitvise"" for connection.You can find complete solution in the link given below.
<a href=""https://www.neenopal.com/ssh_enable_adf_copy.html"" rel=""nofollow noreferrer"">https://www.neenopal.com/ssh_enable_adf_copy.html</a></p>
"
"50428333","Azure Data Factory v2 Data Transformation","<p>I am new to Azure Data Factory. And my question is, I have a requirement to move the data from an on-premise Oracle and on-premise SQL Server to a Blob storage. The data need to be transformed into JSON format. Each row as one JSON file. This will be moved to an Event Hub. How can I achieve this. Any suggestions.</p>
","<json><oracle><azure><azure-data-factory>","2018-05-19 18:34:44","839","0","2","50431836","<p>The <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-hybrid-copy-data-tool"" rel=""nofollow noreferrer"">Data copy tool</a> as part of the azure data factory is an option to copy on premises data to azure. 
the data copy tool comes with a configuration wizard where you do all the required steps like configuring the source, sink, integration pipeline etc. 
In the source you need to write a custom query to fetch data from the tables you require in json format. </p>

<p>In case of SQL server to select json you would use the options <a href=""https://stackoverflow.com/questions/46801908/how-to-convert-rows-to-json-in-tsql-select-query"">OPENJSON, FOR JSON AUTO</a> to convert the rows to json. Supported in SQL 2016. For older versions you need to explore the options available. Worst case you can write a simple console app in C#/java to fetch the rows and then convert them to json file. And then you can upload the file to azure blob storage. If this is an one time activity this option should work and you may not require a data factory. </p>

<p>In case of ORACLE you can use the <a href=""https://docs.oracle.com/en/database/oracle/oracle-database/12.2/adjsn/generation.html#GUID-1084A518-A44A-4654-A796-C1DD4D8EC2AA"" rel=""nofollow noreferrer"">JSON_OBJECT</a> function. </p>
"
"50428333","Azure Data Factory v2 Data Transformation","<p>I am new to Azure Data Factory. And my question is, I have a requirement to move the data from an on-premise Oracle and on-premise SQL Server to a Blob storage. The data need to be transformed into JSON format. Each row as one JSON file. This will be moved to an Event Hub. How can I achieve this. Any suggestions.</p>
","<json><oracle><azure><azure-data-factory>","2018-05-19 18:34:44","839","0","2","50902290","<p>You could use lookup activity + foreach activity. And inside the foreach, there is a copy activity. Please reference this post. <a href=""https://stackoverflow.com/questions/50872649/how-to-copy-%D0%A1osmosdb-docs-to-blob-storage-each-doc-in-single-json-file-with-az"">How to copy СosmosDb docs to Blob storage (each doc in single json file) with Azure Data Factory</a></p>
"
"50426108","DevOPS with Azure Data Factory","<p>I have created Azure Data Factory with Copy Activity using C# and Azure SDK.
How can deploy it using CI/CD ?</p>

<p>Any URL or link will help</p>
","<devops-services><azure-data-factory>","2018-05-19 14:01:30","2219","3","4","52230325","<p>One idea that I got from Microsoft was that using the same Azure SDK you could deserialize the objects and save down the JSON files following the official directory structure into your local GitHub/Git working directory</p>

<p><a href=""https://i.stack.imgur.com/jiozm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jiozm.png"" alt=""Official repository structure""></a></p>

<p>In other words you would have to mimic what the UI Save All/Save button does from the portal.</p>

<p>Then using Git bash, you can just commit and push to your working branch (i.e. develop) and from the UI you can just publish (this will create an adf_publish release branch with the ARM objects)</p>

<p>Official reference for CI using VSTS and the UI Publish feature: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a></p>
"
"50426108","DevOPS with Azure Data Factory","<p>I have created Azure Data Factory with Copy Activity using C# and Azure SDK.
How can deploy it using CI/CD ?</p>

<p>Any URL or link will help</p>
","<devops-services><azure-data-factory>","2018-05-19 14:01:30","2219","3","4","52998316","<p>Data Factory continuous integration and delivery is now possible with directly through the web user interface using ARM Templates or even Git (Github or Azure DevOps).</p>

<p>Just click on ""Set up Code Repository"" and follow the steps.</p>

<p>Check the following link for more information, including a video demostration: <a href=""https://aka.ms/azfr/401/02"" rel=""nofollow noreferrer"">https://aka.ms/azfr/401/02</a> </p>
"
"50426108","DevOPS with Azure Data Factory","<p>I have created Azure Data Factory with Copy Activity using C# and Azure SDK.
How can deploy it using CI/CD ?</p>

<p>Any URL or link will help</p>
","<devops-services><azure-data-factory>","2018-05-19 14:01:30","2219","3","4","56777990","<p>Unfortunately, CI/CD for ADF is not very intuitive at first glance.
Check out this blog post where I'm describing what/how/why step by step:</p>

<p><a href=""https://sqlplayer.net/2019/06/deployment-of-azure-data-factory-with-azure-devops/"" rel=""nofollow noreferrer"">Deployment of Azure Data Factory with Azure DevOps</a></p>

<p>Let me know if you have any questions or concerns and finally - if that works for you.</p>

<p>Good luck!</p>
"
"50426108","DevOPS with Azure Data Factory","<p>I have created Azure Data Factory with Copy Activity using C# and Azure SDK.
How can deploy it using CI/CD ?</p>

<p>Any URL or link will help</p>
","<devops-services><azure-data-factory>","2018-05-19 14:01:30","2219","3","4","58940541","<p>My resources on how to enable CI/CD using Azure DevOps and Data Factory comes from the Microsoft site below:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">Continuous integration and delivery (CI/CD) in Azure Data Factory</a></p>

<p>I am still new to DevOps and CI/CD, but I do know that other departments had this set up and it looks to be working for them.</p>
"
"50422150","How to configure dynamic httpfile source in Azure Data Factory?","<p>I'm trying to pull in data into an Azure SQL DB from an external API.  I will be making about different 50 API each day, each dynamically driven with parameter from the Azure SQL DB.</p>

<p>I started by using Azure Data Factory.  But I can't get beyond setting up a dynamic http source for the API call.</p>

<p>First I created a lookup activity to get a list of keys/param that I will be passing to the API calls.</p>

<p>Then mapped the lookup output to a <code>ForEach</code> items. Inside the <code>ForEach</code>, created a Copy DataFlow activity with the http source and Azure SQL DB as a sink.</p>

<p>How can I inject the <code>ForEach</code> item value into the API call?</p>

<pre><code>domain.com/api/[ForEach Item]/GetData
domain.com/api?item=[ForEach Item] 
</code></pre>

<p>Thank you.</p>
","<azure><azure-data-factory>","2018-05-19 05:36:18","1379","0","2","50444369","<p>in your activity inside foreach you can specified parameters with value 
                                                                           @item().['name of column'] name of column represent item from your foreach activity, <strong>yourparameter = @item().columnName</strong> and after that you can pass this parameter into your api call. check this 
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity</a></p>
"
"50422150","How to configure dynamic httpfile source in Azure Data Factory?","<p>I'm trying to pull in data into an Azure SQL DB from an external API.  I will be making about different 50 API each day, each dynamically driven with parameter from the Azure SQL DB.</p>

<p>I started by using Azure Data Factory.  But I can't get beyond setting up a dynamic http source for the API call.</p>

<p>First I created a lookup activity to get a list of keys/param that I will be passing to the API calls.</p>

<p>Then mapped the lookup output to a <code>ForEach</code> items. Inside the <code>ForEach</code>, created a Copy DataFlow activity with the http source and Azure SQL DB as a sink.</p>

<p>How can I inject the <code>ForEach</code> item value into the API call?</p>

<pre><code>domain.com/api/[ForEach Item]/GetData
domain.com/api?item=[ForEach Item] 
</code></pre>

<p>Thank you.</p>
","<azure><azure-data-factory>","2018-05-19 05:36:18","1379","0","2","51165905","<p>I had success passing the entire Relative URL path from a lookup into the ForEach task, then using @{item().columnName} in the DataSets Relative URL. In your case the value of @{item().columnName} will be a list (for each list) of: domain.com/api/[value]/GetData all combined ahead of time.</p>

<p>In theory you should be able to dynamically create the URL dynamically using @concat, but building the list ahead of time does work. </p>
"
"50406694","Query a specific time range data from cosmos db and store it in sql database via azure data factory","<p>I am using Azure Data Factory V2 and want to copy the json data stored as documents from Azure cosmos db to a azure sql table, using a SQL query.</p>

<p><strong>Situation:</strong>
I want to query just the last hour json files from Cosmos db and store it in sql table.
To do that, I create a new Trigger in my Data factory which recurrence the sql query each 1 hour.</p>

<p><strong>Problem:</strong>
At first I wrote a query that read all data from cosmos db and it work fine, but I want to query just the data which stored within the last hour in cosmos db.</p>

<p><strong>What i need?</strong> 
In Sql, how to write a query to get just the new files which stored within the last hour in the cosmos db ?</p>
","<sql><sql-server><azure><azure-cosmosdb><azure-data-factory>","2018-05-18 08:10:29","2603","2","1","50408328","<p>You could define an udf in your cosmos db collection.</p>

<pre><code>function getLastHourTime(){
    var date = new Date();
    var a = date.setHours(-1);
    return a;
}
</code></pre>

<p>Modify your sql to :</p>

<pre><code>SELECT * FROM c where c.time &gt;= udf.getLastHourTime()
</code></pre>

<p>Note: udf.getLastHourTime() returns <code>Unix time stamp</code> , you need to match the format.</p>

<p>Hope it helps you.</p>
"
"50404107","Connect to Salesforce from Azure Data Factory configuration","<p>as per the documentation at MS, I should be able to configure a custom domain for a salesforce connector through ADF, but i am unable to configure a connection to salesforce.</p>

<p>Doco: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-salesforce"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-salesforce</a></p>

<p>i'm unable to change the environment url ...</p>

<p>how would i go about setting a custom url? or do I have permissions related issues for changing this?</p>
","<salesforce><integration><azure-data-factory>","2018-05-18 05:04:56","1021","-1","1","50579034","<p>ADFv2 visual tool has been updated. You can change the environment url by entering directly in the combo box.</p>

<p><a href=""https://i.stack.imgur.com/bydff.png"" rel=""nofollow noreferrer"">customize your environment url</a></p>
"
"50403463","Azure Data Factory V2 Trigger Pipeline only when current execution finished","<p>Is there a way I can create a trigger in Azure Data Factory that will only kick my pipeline if it's not already running? I basically want to run a pipeline continuously and so for whatever reason it finished (failure / success) I want it to resume as soon as possible.</p>
","<azure><azure-data-factory>","2018-05-18 03:43:23","3634","5","1","57485625","<p>You could try creating a scheduled trigger with a recurrence of 1 minute (this is the minimum possible interval for a schedule trigger) and you can set pipeline Concurrency to 1. What this does is it will start the first run, then the trigger runs again and tries to run the pipeline, but since the first run hasn't finished, it will add it to the queue. After the first run finishes, it will start to run the second one.</p>

<p><a href=""https://i.stack.imgur.com/FXWjq.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/FXWjq.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/xcTsR.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/xcTsR.png"" alt=""enter image description here""></a></p>

<p>Please note that there is a limit on Queue size of 100, so when using pipeline concurrency of 1 could have 1 pipeline run 'InProgress' and 100 in the 'Queue', after that, if they keep triggering they will get error 429.</p>
"
"50402950","Pre-copy script in data factory or on the fly data processing","<p>I am copying data from a source, an API, and copying it into Azure SQL DB. But in one of the column I am getting Json objects. </p>

<p>Any way i can use dynamic parameters (either through Pre-copy script or something else) in the pipeline to only take value of a particular tag from those json objects so that i can have only that value in the column. Only constraint is that I can't change the sink. It has to be Azure SQL DB.</p>

<p>Json object I am getting:
[{""self"":""<a href=""https://xxxxxxxx.jira.com/rest/api/2/customFieldOption/11903"" rel=""nofollow noreferrer"">https://xxxxxxxx.jira.com/rest/api/2/customFieldOption/11903</a>"",""value"":""Yes"",""id"":""11903""}]</p>

<p>And I want only 'value' tag response not the complete json.</p>
","<azure-data-factory>","2018-05-18 02:26:20","7043","2","1","50411527","<p>The pre-copy script is a script that you run against the database before copying new data in, not to modify the data you are ingesting.</p>

<p>What you can do if you cannot change the sink, is store the data in a different table, using a varchar field for the json. Then add another activity in your pipeline, where you take this data and store it in the actual table. In this second activity, you can use t-sql statements to modify the sqlQuery of the copy activity to extract the value you want from it.</p>

<p>This will be useful when designing the query: <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/json/json-data-sql-server?view=sql-server-2017"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sql/relational-databases/json/json-data-sql-server?view=sql-server-2017</a></p>

<p>Hope this helped! :)</p>

<p>PS: in the second activity, use this to get the ""value"" </p>

<pre><code>select JSON_VALUE(fieldWhereYouStoredTheJson, '$[0].value') as jsonValue from temporaryTable
</code></pre>
"
"50363609","JIRA copy custom fields in Azure Data factory","<p>I am aware that JIRA dataset is in beta stage right now in Data factory. But still is there any way to pull the custom fields data?</p>

<p>Right now when i am copying JIRA issues data it is giving me only the standard fields present in any of the Issue. </p>

<p>I want custom fields too. Any way?</p>
","<azure-data-factory>","2018-05-16 06:14:20","228","0","2","50461585","<p>You can try the advanced tab.
For linked service:
<a href=""https://i.stack.imgur.com/jtePh.png"" rel=""nofollow noreferrer"">advanced tab for linked service</a></p>

<p>For dataset:
<a href=""https://i.stack.imgur.com/9z0j2.png"" rel=""nofollow noreferrer"">advanced tab for dataset</a></p>
"
"50363609","JIRA copy custom fields in Azure Data factory","<p>I am aware that JIRA dataset is in beta stage right now in Data factory. But still is there any way to pull the custom fields data?</p>

<p>Right now when i am copying JIRA issues data it is giving me only the standard fields present in any of the Issue. </p>

<p>I want custom fields too. Any way?</p>
","<azure-data-factory>","2018-05-16 06:14:20","228","0","2","50461727","<p>So through hit and trial, i got to know that tables that starts with 'Extras- ' are the ones which has custom fields and that too masked. Again through hit and trial I figured which field value I wanted to extract.</p>
"
"50361262","How to escape a single quote in a dynamic expression with functions?","<p>Need to insert a parameter into a string using the dynamic data function from the pipeline parameters.  Have tried backslash, double backslash, double single quote,@, @@, and other such nonsense.  Just need a simple way to escape a single quote in a string.</p>
","<azure-data-factory>","2018-05-16 01:37:12","21135","25","1","50361364","<p>I actually found out.  It's 2 single quotes apparently.  I forgot to do it in both places, I had a single \ in the second place!</p>
"
"50355494","How to continually migrate data from on-premises SQL Db to Azure SQL Db","<p>As a part of <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/"" rel=""nofollow noreferrer"">Azure Machine Learning</a> process, I need to <code>continually</code> migrate data from on-premises SQL Db to Azure SQL Db using <code>Data Management Gateway</code>.</p>

<p>This Azure official article describes how to: <a href=""https://learn.microsoft.com/en-us/azure/machine-learning/team-data-science-process/move-sql-azure-adf"" rel=""nofollow noreferrer"">Move data from an on-premises SQL server to SQL Azure with Azure Data Factory</a>. But the details are a bit confusing to me. If someone to briefly describe the process, how would you do that. What are 2-3 <code>main</code> steps one needs to perform on <code>on-premises</code> and 2-3 steps on <code>Azure Cloud</code>? No details are needed. <strong>Note</strong>: The solution has to involve using <code>Data Management Gateway</code></p>
","<azure><azure-sql-database><azure-storage><azure-data-factory><azure-machine-learning-service>","2018-05-15 16:50:46","105","1","1","50357832","<p>Based on Azure documentation you can use ""slices"". You can perform a ""delta"" fetch using a timestamp column as mentioned by <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-scheduling-and-execution#data-dependency-and-validation"" rel=""nofollow noreferrer"">this</a> article or using a sequential integer column. To avoid issues about rows not been included on the a slice due to the on-premise server having system date a little behind than Azure system date, is better to use a sequential integer. Below the Input dataset shows how to define slices:</p>

<pre><code>{
  ""name"": ""AzureBlobInput"",
  ""properties"": {
    ""type"": ""AzureBlob"",
    ""linkedServiceName"": ""StorageLinkedService"",
    ""typeProperties"": {
      ""folderPath"": ""mycontainer/myfolder/{Year}/{Month}/{Day}/"",
      ""partitionedBy"": [
        { ""name"": ""Year"", ""value"": {""type"": ""DateTime"",""date"": ""SliceStart"",""format"": ""yyyy""}},
        { ""name"": ""Month"",""value"": {""type"": ""DateTime"",""date"": ""SliceStart"",""format"": ""MM""}},
        { ""name"": ""Day"",""value"": {""type"": ""DateTime"",""date"": ""SliceStart"",""format"": ""dd""}}
      ],
      ""format"": {
        ""type"": ""TextFormat""
      }
    },
    ""external"": true,
    ""availability"": {
      ""frequency"": ""Hour"",
      ""interval"": 1
    }
  }
}
</code></pre>

<p>You can create an activity and use the availability section to specify a schedule for the activity. You can specify ""frequency"" (minute, hour, day, etc.) and ""interval"".</p>

<pre><code>            ""scheduler"": {
                ""frequency"": ""Hour"",
                ""interval"": 1
            }
</code></pre>

<p>Each unit of data consumed or produced by an activity run is called a data slice. The following diagram shows an example of an activity with one input dataset and one output dataset: </p>

<p><a href=""https://i.stack.imgur.com/zGdA7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zGdA7.png"" alt=""enter image description here""></a></p>

<p>The diagram shows the hourly data slices for the input and output dataset. The diagram shows three input slices that are ready for processing. The 10-11 AM activity is in progress, producing the 10-11 AM output slice. </p>

<p>You can access the time interval associated with the current slice in the dataset JSON by using variables: SliceStart and SliceEnd. You can use these variables in your activity JSON to select data from input dataset representing time series data (for example: 8 AM to 9 AM). </p>

<p>You can also set the start date for the pipeline in the past as shown <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-scheduling-and-execution#parallel-processing-of-data-slices"" rel=""nofollow noreferrer"">here</a>. When you do so, Data Factory automatically calculates (back fills) all data slices in the past and begins processing them.</p>
"
"50343749","file arrival in blob storage trigger data factory pipeline","<p>I need to invoke a Data Factory V2 pipeline when a file is placed in a blob container.</p>

<p>I have tried using Powershell to check if the file is present, the issue I have there is that if the file is not there, and tell me its not there, I then place the file in the container and Powershell will still tell me its not there, though perhaps if it reruns the variable will get a fresh value and tell ites there? Maybe there is a way around that? If yes, I can then use the result to invoke the pipeline with the Powershell script. Am I along the right lines here?</p>

<p>Other option will be to write a t-sql query that will give a true/false result if the row condition is met, but I am not sure how I can use this result within/against DFv2. In the IF condition module?</p>

<p>Tried a Logic app but it was kind of useless. It would be great if I could get some suggestions in some ways to trigger the pipeline on the arrival of the file in the blob container, there is more than one way to skin a cat, so open to any and all ideas. Thank you.</p>
","<file><azure-blob-storage><pipeline><azure-data-factory><eventtrigger>","2018-05-15 06:37:55","5054","2","1","51054265","<p>This is now available as an event trigger with ADF V2 as announced in <a href=""https://azure.microsoft.com/en-us/blog/event-trigger-based-scheduling-in-azure-data-factory/"" rel=""nofollow noreferrer"">this bog post</a> on <strong>June 21, 2018</strong>.</p>

<p>Current documentation on how to set it up is available here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger"" rel=""nofollow noreferrer"">Create a trigger that runs a pipeline in response to an event</a>.</p>

<p>From the documentation:</p>

<blockquote>
  <p>As soon as the file arrives in your storage location and the corresponding blob is created, this event triggers and runs your Data Factory pipeline. You can create a trigger that responds to a blob creation event, a blob deletion event, or both events, in your Data Factory pipelines.</p>
</blockquote>

<p>There is a note to be wary of:</p>

<blockquote>
  <p>This integration supports only version 2 Storage accounts (General purpose).</p>
</blockquote>

<p>Event triggers can be one, or both of:</p>

<ul>
<li><code>Microsoft.Storage.BlobCreated</code></li>
<li><code>Microsoft.Storage.BlobDeleted</code></li>
</ul>

<p>With firing conditions from the following:</p>

<ul>
<li><code>blobPathBeginsWith</code></li>
<li><code>blobPathEndsWith</code></li>
</ul>

<p>The documentation also provides the following examples of event trigger firing conditions over blobs:</p>

<ul>
<li><strong>Blob path begins with</strong>('/containername/') – Receives events for any blob in the container.</li>
<li><strong>Blob path begins with</strong>('/containername/foldername') – Receives events for any blobs in the containername container and foldername folder.</li>
<li><strong>Blob path begins with</strong>('/containername/foldername/file.txt') – Receives events for a blob named file.txt in the foldername folder under the containername container.</li>
<li><strong>Blob path ends with</strong>('file.txt') – Receives events for a blob named file.txt at any path.</li>
<li><strong>Blob path ends with</strong>('/containername/file.txt') – Receives events for a blob named file.txt under container containername.</li>
<li><strong>Blob path ends with</strong>('foldername/file.txt') – Receives events for a blob named file.txt in foldername folder under any container.</li>
</ul>
"
"50333804","ADF V1 - Calling a Stored Proc with Parameter based on SQL Query","<p>In ADF V1, in Sproc Activity, is there a way to supply a input parameter based on a SQL query? For example let's say there's a procedure dbo.testProc which expects a parameter param1 and I need to supply the param1 value by looking at a value in a table (select column1 from table_A). How can I specify this? I know in V2 we can use look up activity but in V1 there seems to be no way. Is it impossible in V1? </p>
","<azure><azure-data-factory>","2018-05-14 15:18:38","94","0","1","50615321","<p>Can you make a wrapper-style Stored Proc, e.g.</p>

<p>a) looks up the value with a SELECT statement, stored in a @param1 variable </p>

<p>b) calls your dbo.testProc @param1</p>
"
"50324691","Data factory lookup (dot) in the item() name","<p>I am having lookup wherein salesforce query is there. I am using elements (item()) in subsequent activities. Till now i had item().name or item().email but now i have item().NVMStatsSF__Related_Lead__r.FirstName which has (dot) in the field name.</p>

<p>How should i parse it through body tag so that it reads it correctly?</p>

<p>So I have the following data in item()</p>

<p>{
            ""NVMStatsSF__Related_Lead__c"": ""00QE000egrtgrAK"",
            ""NVMStatsSF__Agent__r.Name"": ""ABC"",
            ""NVMStatsSF__Related_Lead__r.Email"": ""geggegg@gmail.com"",
            ""NVMStatsSF__Related_Lead__r.FirstName"": ""ABC"",
            ""NVMStatsSF__Related_Lead__r.OwnerId"": ""0025434535IIAW""
        }</p>

<p>now when i use item().NVMStatsSF__Agent__r.Name it will not parse because of (dot) after NVMStatsSF__Agent__r. And it is giving me the following error.</p>

<pre><code>'item().NVMStatsSF__Related_Lead__r.Email' cannot be evaluated because property 'NVMStatsSF__Related_Lead__r' doesn't exist, available properties are 'NVMStatsSF__Related_Lead__c, NVMStatsSF__Agent__r.Name, NVMStatsSF__Related_Lead__r.Email, NVMStatsSF__Related_Lead__r.FirstName, NVMStatsSF__Related_Lead__r.OwnerId'."",
    ""failureType"": ""UserError"",
    ""target"": ""WebActivityToAddPerson""
</code></pre>
","<azure-data-factory>","2018-05-14 07:01:55","787","1","3","50342938","<p>this is because ADF uses '.' for object reading.
Could you find a way to rename the field name which contains '.'?</p>
"
"50324691","Data factory lookup (dot) in the item() name","<p>I am having lookup wherein salesforce query is there. I am using elements (item()) in subsequent activities. Till now i had item().name or item().email but now i have item().NVMStatsSF__Related_Lead__r.FirstName which has (dot) in the field name.</p>

<p>How should i parse it through body tag so that it reads it correctly?</p>

<p>So I have the following data in item()</p>

<p>{
            ""NVMStatsSF__Related_Lead__c"": ""00QE000egrtgrAK"",
            ""NVMStatsSF__Agent__r.Name"": ""ABC"",
            ""NVMStatsSF__Related_Lead__r.Email"": ""geggegg@gmail.com"",
            ""NVMStatsSF__Related_Lead__r.FirstName"": ""ABC"",
            ""NVMStatsSF__Related_Lead__r.OwnerId"": ""0025434535IIAW""
        }</p>

<p>now when i use item().NVMStatsSF__Agent__r.Name it will not parse because of (dot) after NVMStatsSF__Agent__r. And it is giving me the following error.</p>

<pre><code>'item().NVMStatsSF__Related_Lead__r.Email' cannot be evaluated because property 'NVMStatsSF__Related_Lead__r' doesn't exist, available properties are 'NVMStatsSF__Related_Lead__c, NVMStatsSF__Agent__r.Name, NVMStatsSF__Related_Lead__r.Email, NVMStatsSF__Related_Lead__r.FirstName, NVMStatsSF__Related_Lead__r.OwnerId'."",
    ""failureType"": ""UserError"",
    ""target"": ""WebActivityToAddPerson""
</code></pre>
","<azure-data-factory>","2018-05-14 07:01:55","787","1","3","50343487","<p>Seems like you need a built-in function to get the value of an object according to the key. Like getValue(item(), 'key.nestkey'). But unfortunately, seems there isn't such a function. You may need handle your key first.</p>
"
"50324691","Data factory lookup (dot) in the item() name","<p>I am having lookup wherein salesforce query is there. I am using elements (item()) in subsequent activities. Till now i had item().name or item().email but now i have item().NVMStatsSF__Related_Lead__r.FirstName which has (dot) in the field name.</p>

<p>How should i parse it through body tag so that it reads it correctly?</p>

<p>So I have the following data in item()</p>

<p>{
            ""NVMStatsSF__Related_Lead__c"": ""00QE000egrtgrAK"",
            ""NVMStatsSF__Agent__r.Name"": ""ABC"",
            ""NVMStatsSF__Related_Lead__r.Email"": ""geggegg@gmail.com"",
            ""NVMStatsSF__Related_Lead__r.FirstName"": ""ABC"",
            ""NVMStatsSF__Related_Lead__r.OwnerId"": ""0025434535IIAW""
        }</p>

<p>now when i use item().NVMStatsSF__Agent__r.Name it will not parse because of (dot) after NVMStatsSF__Agent__r. And it is giving me the following error.</p>

<pre><code>'item().NVMStatsSF__Related_Lead__r.Email' cannot be evaluated because property 'NVMStatsSF__Related_Lead__r' doesn't exist, available properties are 'NVMStatsSF__Related_Lead__c, NVMStatsSF__Agent__r.Name, NVMStatsSF__Related_Lead__r.Email, NVMStatsSF__Related_Lead__r.FirstName, NVMStatsSF__Related_Lead__r.OwnerId'."",
    ""failureType"": ""UserError"",
    ""target"": ""WebActivityToAddPerson""
</code></pre>
","<azure-data-factory>","2018-05-14 07:01:55","787","1","3","50363309","<p>Finally, it worked. I was being silly.</p>

<p>Instead of taking the value from the child table with the help of (dot) operator I just used subquery. Silly see. </p>

<p>And it worked.</p>
"
"50322247","Epoch timestamp in Data factory","<p>The following is working:</p>

<pre><code>responses/desc/50000/1/1525756206/json?X-apikey=542c0603951a208c4be688002d36f48808c40116174
</code></pre>

<p>But not this:</p>

<pre><code>responses/desc/50000/1/@{formatDateTime(adddays(utcnow(), -6), '%a %b %d %H:%M:%S +0000 %Y')}/json?X-apikey=542c0603951a208c4be688002d3c40116174
</code></pre>

<p>Even this is not working:</p>

<pre><code>responses/desc/50000/1/@{formatDateTime(adddays(utcnow(), -6), '%s')}/json?X-apikey=542c0603951a208c4be688002d3c40116174
</code></pre>

<p>So it is 
<code>@{formatDateTime(adddays(utcnow(), -6), '%a %b %d %H:%M:%S +0000 %Y')}</code>
vs
<code>1525756206</code>
vs
<code>@{formatDateTime(adddays(utcnow(), -6), '%s')}</code></p>

<p>Can anybody help me here? I have been stuck here for a couple of days. How to calculate or use seconds in data factory?</p>

<p>Basically when I am using <code>@{formatDateTime(adddays(utcnow(), -6), '%a %b %d %H:%M:%S +0000 %Y')}</code>, It just ignore the filters/tag in the URL.</p>
","<azure-data-factory>","2018-05-14 01:56:27","4919","1","2","50462611","<p>The expression works for me, but some format specifier are not available. You can find supported format specifier in this doc: <a href=""https://learn.microsoft.com/en-us/dotnet/standard/base-types/custom-date-and-time-format-strings"" rel=""nofollow noreferrer"">custom-date-and-time-format-strings</a>.</p>

<p>And <code>formatDateTime(adddays(utcnow(), -6), '%s')</code> will only show the second number, you can try <code>formatDateTime(adddays(utcnow(), -6), 's')</code> if you want to show time in this format: 2018-05-16T07:55:18. (reference: <a href=""https://learn.microsoft.com/en-us/dotnet/standard/base-types/standard-date-and-time-format-strings"" rel=""nofollow noreferrer"">Standard Date and Time Format Strings</a> )</p>
"
"50322247","Epoch timestamp in Data factory","<p>The following is working:</p>

<pre><code>responses/desc/50000/1/1525756206/json?X-apikey=542c0603951a208c4be688002d36f48808c40116174
</code></pre>

<p>But not this:</p>

<pre><code>responses/desc/50000/1/@{formatDateTime(adddays(utcnow(), -6), '%a %b %d %H:%M:%S +0000 %Y')}/json?X-apikey=542c0603951a208c4be688002d3c40116174
</code></pre>

<p>Even this is not working:</p>

<pre><code>responses/desc/50000/1/@{formatDateTime(adddays(utcnow(), -6), '%s')}/json?X-apikey=542c0603951a208c4be688002d3c40116174
</code></pre>

<p>So it is 
<code>@{formatDateTime(adddays(utcnow(), -6), '%a %b %d %H:%M:%S +0000 %Y')}</code>
vs
<code>1525756206</code>
vs
<code>@{formatDateTime(adddays(utcnow(), -6), '%s')}</code></p>

<p>Can anybody help me here? I have been stuck here for a couple of days. How to calculate or use seconds in data factory?</p>

<p>Basically when I am using <code>@{formatDateTime(adddays(utcnow(), -6), '%a %b %d %H:%M:%S +0000 %Y')}</code>, It just ignore the filters/tag in the URL.</p>
","<azure-data-factory>","2018-05-14 01:56:27","4919","1","2","50500159","<p>Finally used the below to compute &amp; use the epoch timestamp as a dynamic parameter.</p>

<pre><code>@{div(sub(ticks(adddays(utcNow(), -1)), ticks('1970-01-01')),10000000)}
</code></pre>

<p>Courtesy: Azure Data factory Product team</p>
"
"50316081","How to handle double pipe delimiter in Azure Data Factory 2","<p>I'm working with a double pipe delimited file which I want to ingest in Azure SQL DB via Data Factory (2):</p>

<pre><code>Column1||Column2||Column3
</code></pre>

<p>In my input dataset, I specified the double pipe delimiter as columnDelimiter in the format section of typeProperties:</p>

<pre><code>        ""format"": {
            ""type"": ""TextFormat"",
            ""rowDelimiter"": ""\n"",
            ""columnDelimiter"": ""||"",
            ""skipLineCount"": 2
        }
</code></pre>

<p>Unfortunately, this is not working. All rows are marked as ""incompatible"" and no data is ingested in my SQL table. Next, I tried a few things. When not specifying any columnDelimiter, the complete row is ingested in the first column of my SQL table, so no delimiter is considered at all. This is not correct, logically each data item must be ingested in a separate column. Scripting the <code>||</code> in Unicode also did not fix it. When converting my input file to single pipe delimited it works fine. However, in the end-state I'm not able to alter the input file so handling the double pipe delimiter is essential.</p>
","<azure><azure-sql-database><delimiter><azure-data-factory>","2018-05-13 12:23:39","2941","2","2","50327024","<p>Based on the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#text-format"" rel=""nofollow noreferrer"">text format allowed rules</a> , only one character is allowed. I even want to use escape delimiter to avoid it, but it also be banned.</p>

<p><a href=""https://i.stack.imgur.com/Kw1Ey.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Kw1Ey.png"" alt=""enter image description here""></a></p>

<p>So, it seems you need to process your data before copying activity.Since ADF supports copy data from <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-http"" rel=""nofollow noreferrer"">HTTP endpoint</a>,so I suggest you following steps: </p>

<ol>
<li>Using Azure Functions to read your file via stream way line by line.</li>
</ol>

<p>2.Replace all the '||' with '|' and set it in the response.(Don't forget '\n')</p>

<p>3.ADF set column delimiter to '|' and copy data from HTTP endpoint.</p>

<p>This solution could process big size data , of course, if you do not want to do such complicate work ,  you could process your data directly and save it as temporary file for ADF.</p>

<p>Hope it helps you.</p>
"
"50316081","How to handle double pipe delimiter in Azure Data Factory 2","<p>I'm working with a double pipe delimited file which I want to ingest in Azure SQL DB via Data Factory (2):</p>

<pre><code>Column1||Column2||Column3
</code></pre>

<p>In my input dataset, I specified the double pipe delimiter as columnDelimiter in the format section of typeProperties:</p>

<pre><code>        ""format"": {
            ""type"": ""TextFormat"",
            ""rowDelimiter"": ""\n"",
            ""columnDelimiter"": ""||"",
            ""skipLineCount"": 2
        }
</code></pre>

<p>Unfortunately, this is not working. All rows are marked as ""incompatible"" and no data is ingested in my SQL table. Next, I tried a few things. When not specifying any columnDelimiter, the complete row is ingested in the first column of my SQL table, so no delimiter is considered at all. This is not correct, logically each data item must be ingested in a separate column. Scripting the <code>||</code> in Unicode also did not fix it. When converting my input file to single pipe delimited it works fine. However, in the end-state I'm not able to alter the input file so handling the double pipe delimiter is essential.</p>
","<azure><azure-sql-database><delimiter><azure-data-factory>","2018-05-13 12:23:39","2941","2","2","50329092","<p>You could either ask the creators of your input file to use a single delimiter (like most people do) or else you have to do some pre-processing.  For example, you could this with a U-SQL step which corrects the file and then import that file into your SQL DB, eg some simple U-SQL:</p>

<pre><code>// Multiple column delimiters
@input =
    EXTRACT 
        col1 string
    FROM ""/input/input91.txt""
    USING Extractors.Text(delimiter:'~', skipFirstNRows:1);


// Do some other processing here?
@output  = SELECT col1.Replace(""||"", ""|"") AS col2 FROM @input;


OUTPUT @output
TO ""/output/output.txt""
USING Outputters.Text(delimiter : '|', quoting : false);
</code></pre>

<p>NB Here I've used a delimiter (tilde ""~"") which I know won't occur in the input file to effectively import all the columns as one column, and then fix it up.</p>

<p>Is there a particular reason you use two delimiters?</p>

<p>Also, if your file is on blob storage, then you can now <code>BULK INSERT</code> into Azure SQL DB, and having just given this a quick test, BULK INSERT can cope with multiple delimiters, eg</p>

<pre><code>DROP TABLE IF EXISTS #tmp

CREATE TABLE #tmp (
    a VARCHAR(50),
    b VARCHAR(50),
    c VARCHAR(50),
)
GO

BULK INSERT #tmp
FROM 'D:\Data Lake\USQLDataRoot\input\input91.txt'
WITH ( FIELDTERMINATOR  = '||', DATA_SOURCE = '&lt;yourDataSource&gt;' );
</code></pre>

<p>See <a href=""https://azure.microsoft.com/en-us/updates/preview-loading-files-from-azure-blob-storage-into-sql-database/"" rel=""nofollow noreferrer"">here</a> for more info on the steps which involve creating an external data source and credential.  You could then get Data Factory to use the Stored Proc task to execute for example.</p>
"
"50298122","Azure data factory | incremental data load from SFTP to Blob","<p>I created a (once run) DF (V2) pipeline to load files (.lta.gz) from a SFTP server into an azure blob to get historical data.
Worked beautifully.
Every day there will be several new files on the SFTP server (which cannot be manipulated or deleted). So I want to create an incremental load pipeline which checks daily for new files - if so ---> copy new files.</p>

<p>Does anyone have any tips for me how to achieve this?</p>
","<azure><azure-data-factory>","2018-05-11 18:20:50","3857","3","2","50302914","<p>Thanks for using Data Factory!</p>

<p>To incrementally load newly generated files on SFTP server, you can leverage the GetMetadata activity to retrieve the LastModifiedDate property:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity</a></p>

<p>Essentially you author a pipeline containing the following activities:</p>

<ul>
<li>getMetadata (return list of files under a given folder)</li>
<li>ForEach (iterate through each file)</li>
<li>getMetadata (return lastModifiedTime for a given file)</li>
<li>IfCondition (compare lastModifiedTime with trigger WindowStartTime)</li>
<li>Copy (copy file from source to destination)</li>
</ul>

<p>Have fun building data integration flows using Data Factory!</p>
"
"50298122","Azure data factory | incremental data load from SFTP to Blob","<p>I created a (once run) DF (V2) pipeline to load files (.lta.gz) from a SFTP server into an azure blob to get historical data.
Worked beautifully.
Every day there will be several new files on the SFTP server (which cannot be manipulated or deleted). So I want to create an incremental load pipeline which checks daily for new files - if so ---> copy new files.</p>

<p>Does anyone have any tips for me how to achieve this?</p>
","<azure><azure-data-factory>","2018-05-11 18:20:50","3857","3","2","54955757","<p>since I posted my previous answer in May last year, many of you contacted me asking for pipeline sample to achieve the incremental file copy scenario using the getMetadata-ForEach-getMetadata-If-Copy pattern.  This has been important feedback that incremental file copy is a common scenario that we want to further optimize.</p>

<p>Today I would like to post an updated answer - we recently released a new feature that allows a much easier and scalability approach to achieve the same goal:</p>

<p>You can now set modifiedDatetimeStart and modifiedDatetimeEnd on SFTP dataset to specify the time range filters to only extract files that were created/modified during that period.  This enables you to achieve the incremental file copy using a single activity:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sftp#dataset-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-sftp#dataset-properties</a></p>

<p>This feature is enabled for these file-based connectors in ADF: AWS S3, Azure Blob Storage, FTP, SFTP, ADLS Gen1, ADLS Gen2, and on-prem file system.  Support for HDFS is coming very soon.</p>

<p>Further, to make it even easier to author an incremental copy pipeline, we now release common pipeline patterns as solution templates.  You can select one of the templates, fill out the linked service and dataset info, and click deploy – it is that simple!
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/solution-templates-introduction"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/solution-templates-introduction</a></p>

<p>You should be able to find the incremental file copy solution in the gallery:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/solution-template-copy-new-files-lastmodifieddate"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/solution-template-copy-new-files-lastmodifieddate</a></p>

<p>Once again, thank you for using ADF and happy coding data integration with ADF!</p>
"
"50289298","Azure Data Factory Pipepline with Multiple Downstream Activity Slow in Downstream Scheduler Starting","<p>I've created an ADF pipeline with two linked activities, the first one to run a stored procedure and the 2nd activity (Copy Data) to copy data from a Azure DW to Azure SQL DB table. To link these two I have put the output dataset of the stored procedure as an input of the Copy Data activity even though that dataset is not used (there's a separate dataset for that).</p>

<p>Now the issue is when I get into ""Monitor and Manage"" and run the 2nd activity with ""Rerun with upstream in Pipeline"" the 1st stored procedure activity runs quickly and then the 2nd activity waits for about 5 mins before changing to In Progress. Why is this happening? Is it due to some time slicing issue? The Pipeline code is as below:</p>

<pre><code>{
""name"": ""RunADLAProc"",
""properties"": {
    ""description"": ""This will run the procedure for ADLA"",
    ""activities"": [
        {
            ""type"": ""SqlServerStoredProcedure"",
            ""typeProperties"": {
                ""storedProcedureName"": ""dbo.BackUpDatabaseLog"",
                ""storedProcedureParameters"": {}
            },
            ""outputs"": [
                {
                    ""name"": ""AzureSQLDatasetOutputforProc""
                }
            ],
            ""policy"": {
                ""timeout"": ""01:00:00"",
                ""concurrency"": 1,
                ""retry"": 3
            },
            ""scheduler"": {
                ""frequency"": ""Day"",
                ""interval"": 1
            },
            ""name"": ""StoredProcedureActivityTemplate""
        },
        {
            ""type"": ""Copy"",
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""SqlDWSource"",
                    ""sqlReaderQuery"": ""select * from dbo.DatabaseLog""
                },
                ""sink"": {
                    ""type"": ""SqlSink"",
                    ""writeBatchSize"": 10000,
                    ""writeBatchTimeout"": ""60.00:00:00""
                }
            },
            ""inputs"": [
                {
                    ""name"": ""AzureSqlDWInput""
                },
                {
                    ""name"": ""AzureSQLDatasetOutputforProc""
                }
            ],
            ""outputs"": [
                {
                    ""name"": ""AzureSQLDatasetOutput""
                }
            ],
            ""policy"": {
                ""timeout"": ""7.00:00:00"",
                ""concurrency"": 1
            },
            ""scheduler"": {
                ""frequency"": ""Day"",
                ""interval"": 1
            },
            ""name"": ""CopyActivityTemplate""
        }
    ],
    ""start"": ""2018-05-09T00:00:00Z"",
    ""end"": ""2018-05-12T00:00:00Z"",
    ""isPaused"": false,
    ""hubName"": ""testdatafactory-v1_hub"",
    ""pipelineMode"": ""Scheduled""
}}
</code></pre>
","<azure><azure-data-factory>","2018-05-11 09:33:23","231","0","1","50344750","<p>You can try ADFv2, it is more easily for debugging, and has a GUI authoring tool.
the UI tool is <a href=""https://adf.azure.com"" rel=""nofollow noreferrer"">https://adf.azure.com</a></p>
"
"50286313","Can I not specify start/end for a paused ADF pipeline?","<p>I have two ADF pipelines using the same dataset. I want the first pipeline to be paused until I want to use it later. I deployed pipeline1 using </p>

<pre><code>""isPaused"" : ""true""
</code></pre>

<p>If I try to deploy it with  <code>""start""</code> and <code>""end""</code> properties, the deploying fails as I get the error </p>

<pre><code>Entity provisioning failed: Cannot set active period Start=05/11/2018 06:00:00, End=12/31/9991 23:59:59 for pipeline 'Pipeline1' due to conflicts on Output: Output1 with Pipeline: Pipeline2, Activity Activity1, Period: Start=05/11/2018 06:00:00, End=01/01/9992 00:00:00 Output: Output2 with Pipeline: Pipeline2, Activity3, Period: Start=05/11/2018 06:00:00, End=01/01/9992 00:00:00 Output: Output3 with Pipeline: Pipeline2, Period: Start=05/11/2018 06:00:00, End=01/01/9992 00:00:00 . Try changing the active period or using autoResolve option when setting the active period.
</code></pre>

<p>I just want to know if skipping the <code>""start""</code> and <code>""end""</code> properties cause any errors anywhere.</p>

<p>I'll have to look for another approach to solve the conflict then.</p>
","<azure><azure-data-factory>","2018-05-11 06:32:21","118","0","1","50345883","<p>Maybe you want to take a look at execute pipeline activity. It could invoke another pipeline inside a pipeline. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-pipeline-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-pipeline-activity</a></p>
"
"50282199","ADF dynamically creating filename for a usql job","<p>Currently getting this error in a dataset that dynamically creates a filename to send back to a usql job. </p>

<p>Error - <code>""errorId"":""E_CSC_USER_DATAPARTITIONEDOUTPUTNOTSUPPORTED"",""severity"":""Error"",""component"":""CSC"",""source"":""USER"",""message"":""Data partitioned output is not supported.""</code></p>
","<azure><dynamic><parameters><azure-data-factory><u-sql>","2018-05-10 22:10:09","163","0","1","50336885","<p>You are trying to use a feature that is currently only available in a private preview. If you would like to use the private preview feature and give us some feedback on it, please send email to [usql] at microsoft dot com.</p>
"
"50261918","What is the relationship between Azure Data Factory and Azure Data Movement","<p>I need some clarity on Data Factory and Data Movement - Data Factory (v1 &amp; v2) are both only available in a select few regions but Data Movement is available in many regions. I'd like to understand the relationship between Azure Data Factory and Azure Data Movement as they relate to the Azure regions (<a href=""https://azure.microsoft.com/en-gb/global-infrastructure/services/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-gb/global-infrastructure/services/</a>) and if they are related or totally different products. </p>

<p>For example, if I have a Data Factory in North Europe, will all data processed by this data factory ALWAYS pass through the North Europe region, even if the source and destinations are both, say, in East US? What I am trying to understand is, does Data Factory do anything clever under the hood to use a Data Movement service in the most appropriate region based on where the data is flowing from and to. </p>

<p>A second abstract example would be data loading from a blob store in Australia to a SQL DB in Australia. I know there are other ways to do this, but say I had to use Data Factory. ADF is not available in Australia, so I would stand it up in somewhere like North Europe, would my data travel from Australia to North Europe and back to Australia? Or would the data movement aspect of ADF be more clever and do that locally?</p>

<p>A further aspect wound be for Integration Runtime - does IR always being data back to the region the ADF is hosted in for processing? </p>

<p>Thanks. </p>
","<azure><azure-data-factory>","2018-05-09 20:56:05","825","0","1","50264606","<p>Data Movement service is part of Data Factory service, which is the real compute env to do data transfer. This means when you are copying data from Australia to Australia, no matter where the Data Factory is (e.g. in East US), it will use the Data Movement service in Australia to finished the Copy. Data Factory region is the region to store your data factory metadata.</p>

<p>For self-hosted IR, data flow won't go back to Data Factory. Self-hosted IR will connect to both source and sink data source to transfer data.(see more details from <a href=""https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#command-flow-and-data-flow"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#command-flow-and-data-flow</a>)</p>

<p>Here're some more details if you are taking care about region: for Cloud Copy, Copy Activity will auto detect the sink data source region and use Data Movement service in that region to finish Copy. When authoring a new pipeline from CopyWizard UI, you will see the region to be used. And when Copy finished, you can also see the region execution region in summary page. </p>

<p>Regards,
Gary</p>
"
"50243988","Disable activity in Azure Data factory pipeline without removing it","<p>So I am testing each of the activities of the pipeline and I want to disable some of the activities in it. Essentially there is an activity of sending emails which I want to disable as I wanted to see the output of prior activities.</p>

<p>Offcourse I dont want to remove the email sending activity because it is in the prod environment and not developed by me.</p>

<p>Is there any way to disable it?</p>
","<azure-data-factory>","2018-05-09 00:44:35","16008","19","5","50252784","<p>You cannot disable one, but what you want to do is possible with the debug option in the editor. Just click on the red circle above any activity and run the debugger, it will run until that activity is complete and stop, allowing you to see the output of those prior to that.</p>

<p>Hope this helped!</p>
"
"50243988","Disable activity in Azure Data factory pipeline without removing it","<p>So I am testing each of the activities of the pipeline and I want to disable some of the activities in it. Essentially there is an activity of sending emails which I want to disable as I wanted to see the output of prior activities.</p>

<p>Offcourse I dont want to remove the email sending activity because it is in the prod environment and not developed by me.</p>

<p>Is there any way to disable it?</p>
","<azure-data-factory>","2018-05-09 00:44:35","16008","19","5","63005404","<p>Just remove from your dataset and add them again. ;-)</p>
"
"50243988","Disable activity in Azure Data factory pipeline without removing it","<p>So I am testing each of the activities of the pipeline and I want to disable some of the activities in it. Essentially there is an activity of sending emails which I want to disable as I wanted to see the output of prior activities.</p>

<p>Offcourse I dont want to remove the email sending activity because it is in the prod environment and not developed by me.</p>

<p>Is there any way to disable it?</p>
","<azure-data-factory>","2018-05-09 00:44:35","16008","19","5","63454255","<p>You could disable the trigger or delay the trigger execution to some date in the future.</p>
"
"50243988","Disable activity in Azure Data factory pipeline without removing it","<p>So I am testing each of the activities of the pipeline and I want to disable some of the activities in it. Essentially there is an activity of sending emails which I want to disable as I wanted to see the output of prior activities.</p>

<p>Offcourse I dont want to remove the email sending activity because it is in the prod environment and not developed by me.</p>

<p>Is there any way to disable it?</p>
","<azure-data-factory>","2018-05-09 00:44:35","16008","19","5","72352114","<p>This builds on Martin Esteban Zurita's answer, if you need to prevent activities from the beginning or from within the middle of your pipeline running, without deleting them altogether:</p>
<p>You can temporarily change the order of the activities and make the ones you want to disable run at the end of the pipeline instead, (closing any gaps you may have created), then click the last activity you do want to run and click the breakpoint circle icon to prevent the &quot;disabled&quot; activities from running.</p>
<p>Then reinstate the order after testing/debugging.</p>
"
"50243988","Disable activity in Azure Data factory pipeline without removing it","<p>So I am testing each of the activities of the pipeline and I want to disable some of the activities in it. Essentially there is an activity of sending emails which I want to disable as I wanted to see the output of prior activities.</p>

<p>Offcourse I dont want to remove the email sending activity because it is in the prod environment and not developed by me.</p>

<p>Is there any way to disable it?</p>
","<azure-data-factory>","2018-05-09 00:44:35","16008","19","5","74285157","<p>I usually copy the activities to another temp pipeline, make your desired debugging and recompose the pipeline in the end if applicable.
Or you can clone the pipeline, delete the desired components and debug it.</p>
"
"50243706","Transform Data using Stored Procedure in Azure Data Factory","<p>I'm trying to write a pipeline using Azure Data Factory v2. The data comes from Azure Table Storage, which I then need to transform by retrieving more data from Azure SQL and then need to send to another SQL Database for Insertion.</p>

<p>What I have in mind is to use:</p>

<p>Lookup from Table Storage -> For Each Row -> Execute SP -> Append Data to Lookup Output -> Execute SP to insert in another SQL.</p>

<p>I am not sure if I what I want to achieve is doable with Data Factory or if i'm even approaching this from the right angle.</p>
","<azure-data-factory>","2018-05-08 23:53:41","153","0","1","50279050","<p>I don't understand what you mean with the step ""Append data to Lookup Output"". You cannot add data to the result of a lookup activity.</p>

<p>What you can do is store that output in a table at the first Azure SQL, and perform another Lookup to grab all the data.</p>

<p>Hope this helped!</p>
"
"50231490","U-SQL get filename of input and use for output","<p>I have a filename of test.csv and I want the output to be test.txt.</p>

<p>I can extract the filename of the input but don't know how to use it for the output?</p>

<pre><code>OUTPUT @result TO ""/output/{filename}.txt""
USING Outputters.Text(outputHeader:false, quoting:false);
</code></pre>

<p>The filename is in the @result.</p>

<p>This feature isn't supported as of yet.</p>

<p>Does anyone have a work around?</p>

<p><a href=""https://stackoverflow.com/questions/40998910/u-sql-how-can-i-get-the-current-filename-being-processed-to-add-to-my-extract-ou"">U-SQL How can I get the current filename being processed to add to my extract output?</a></p>

<p>Ideally I would like dd-mm-yy-test.text?</p>

<p>How do I append the day month and year?</p>

<p>I am using USQL for this.</p>

<p>Thanks</p>
","<u-sql><azure-data-factory>","2018-05-08 10:32:07","415","0","1","50366381","<p>Let me address both issues you're laying out in this question:</p>

<ol>
<li><p>To use the same output name as the input, there would have to be a way to access rowset values into u-sql variables which I'm pretty sure cannot be done, taking into account that the language is built around the necessity to process many files at once.</p></li>
<li><p>To append a date into the output you would only need to declare the current datetime at some point and then use it to write the output file name like this:</p>

<pre><code>DECLARE @now DateTime = DateTime.Now;
OUTPUT @output TO ""/tests/output/"" + @now.ToString(""dd-MM-yyyy"") + ""-output.csv"" USING Outputters.Csv();
</code></pre></li>
</ol>
"
"50217422","Issues creating trigger in Azure DataFactory","<p>I'm working to create a TumblingWindowTrigger that specifies a schedule parameter so that I can execute a backfill of data. After executing the PowerShell command below, in the UI I see a new trigger with the correct name, however, it is of type: ""Trigger"" and not associated with my pipleine named 'appevents_daily'. I don't see any errors or warnings. Any thoughts on what I'm missing ?</p>

<p>PowerShell Commnads:</p>

<pre><code>Login-AzureRmAccount 
$ResourceGroupName=""treasuredata""
$DataFactoryName=""treasured""
$TriggerName=""BackfillAppevents""

Set-AzureRmDataFactoryV2Trigger -ResourceGroupName $ResourceGroupName -DataFactoryName $DataFactoryName -Name $TriggerName -DefinitionFile ""C:\Users\rajesh\Source\datafactory\backfill.json""
</code></pre>

<p>The file backfill.json is defined as:</p>

<pre><code>{
  ""name"": ""BackfillAppevents"",
  ""properties"": {
    ""type"": ""TumblingWindowTrigger"",
    ""typeProperties"": {
      ""frequency"": ""Hour"",
      ""interval"": 24,
      ""startTime"": ""2018-04-09T19:00:00.000Z"",
      ""endTime"": ""2018-05-04T23:26:00.000Z"",
      ""delay"": ""00:00:00"",
      ""maxConcurrency"": 1,
      ""schedule"": {
        ""minutes"": [0,15,30,45]
      },
      ""retryPolicy"": {
        ""count"": 3,
        ""intervalInSeconds"": 120
      }
    },
    ""pipeline"": {
      ""pipelineReference"": {
        ""referenceName"": ""appevents_daily"",
        ""type"": ""PipelineReference""
      },
      ""parameters"": {
        ""windowStart"": ""@trigger().outputs.windowStartTime"",
        ""windowEnd"": ""@trigger().outputs.windowEndTime""
      }
    }
  }
}
</code></pre>
","<azure><azure-data-factory>","2018-05-07 15:13:43","50","1","1","50221619","<p>Found the root issue was an out of date commandlet. FYI -- to install the latest in Powershell:</p>

<pre><code>Install-Module -Name AzureRM.DataFactoryV2 -Force
</code></pre>
"
"50209142","Script task to upload zip file to blob storage with azure datafactory SSIS","<p>I have a azure data factory project. I need to query some data from my Azure SQL Database then load into an xml, zip it and upload to blob sotrage. I don't want to write anything to the file system (because I think the Azure Database doesn't have any lcoal storage) so I am using the Memorystream. </p>

<p>This Script Task is working on my local SSIS database but not on the Azure Datafactory:</p>

<pre><code>using System;
using System.Data;
using Microsoft.SqlServer.Dts.Runtime;
using System.Windows.Forms;
using System.Collections;
using System.Linq;
using System.Data.OleDb;
using System.IO;

using System.IO.Compression;
using System.Data.SqlClient;
using Microsoft.Azure;

using Microsoft.WindowsAzure.Storage;
using Microsoft.WindowsAzure.Storage.Auth;
using Microsoft.WindowsAzure.Storage.Blob;


public void Main()
    {

        CloudStorageAccount storageAccount = null;
        CloudBlobContainer cloudBlobContainer = null;


        try
        {

            DataSet ds = new DataSet(""FullList"");
            OleDbDataAdapter oleDa = new OleDbDataAdapter();

            DataTable dt = new DataTable(""CustomerTable"");
            oleDa.Fill(dt, Dts.Variables[""User::CustomerSelect""].Value);
            ds.Tables.Add(dt);

            DataTable dt_product = new DataTable(""ProductTable"");
            oleDa.Fill(dt_product, Dts.Variables[""User::ProductSelect""].Value);
            ds.Tables.Add(dt_product);



            DataRelation relation = ds.Relations.Add(""relation"", ds.Tables[""CustomerTable""].Columns[""id""], ds.Tables[""ProductTable""].Columns[""id""]);
            relation.Nested = true;


            string connstring = Dts.Connections[""testolgdev""].AcquireConnection(Dts.Transaction).ToString();
            if (CloudStorageAccount.TryParse(connstring, out storageAccount))
            {
                try
                {
                    CloudBlobClient cloudBlobClient = storageAccount.CreateCloudBlobClient();


                    cloudBlobContainer = cloudBlobClient.GetContainerReference(""flat"");

                    string fileName = ""xml"" + DateTime.Now.ToString(""yyyyMMddHHmmssfff"") + "".zip"";
                    var blob = cloudBlobContainer.GetBlockBlobReference(fileName);
                    using (var stream = new ZipArchive(blob.OpenWrite(), ZipArchiveMode.Create))
                    {
                        var entry = stream.CreateEntry(""test_dataset_fullresult_onlymem.xml"");
                        using (var es = entry.Open())
                        {
                            ds.WriteXml(es);
                        }


                    }



                }
                catch (StorageException ex)
                {
                    Console.WriteLine(""Error returned from the service: {0}"", ex.Message);
                }
            }
            else
            {
                Console.WriteLine(""Wrong connection string"");
            }



        }
        catch (TargetInvocationException e)
        {

            throw;
        }

  Dts.TaskResult = (int)ScriptResults.Success;
}
</code></pre>

<p><strong>This is the Azure Datafactory SSIS error when I deploy and execute it:</strong> <br><br>
Script Task 1:Error: Could not load file or assembly 'Microsoft.WindowsAzure.Storage, Version=4.3.0.0, Culture=neutral, PublicKeyToken=31bf3856ad364e35' or one of its dependencies. The located assembly's manifest definition does not match the assembly reference. (Exception from HRESULT: 0x80131040)</p>

<p>Is it possible to fix this? Can I add the missing dll to Azure Datafactory?</p>
","<azure><ssis><azure-sql-database><azure-blob-storage><azure-data-factory>","2018-05-07 07:31:17","1306","2","1","50215318","<p>With this guide I can add the missing dlls to Azure-SSIS IR: <br><a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-configure-azure-ssis-ir-custom-setup"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-configure-azure-ssis-ir-custom-setup.</a></p>

<p>Thanks to <strong>Sandy Winarko(MSFT)</strong> for the answer!</p>
"
"50200021","Populating Azure Search using a Data Factory Pipeline","<p>I'm pretty new to azure search and data factory. I'm wanting to use data factory to populate my azure search index. I have the following:</p>

<p><strong>Source</strong></p>

<p>Azure sql db</p>

<pre><code>{
""name"": ""Drug"",
""properties"": {
    ""published"": false,
    ""type"": ""AzureSqlTable"",
    ""linkedServiceName"": ""AzureSqlLinkedService"",
    ""typeProperties"": {
        ""tableName"": ""[transformed].[drug]""
    },
    ""availability"": {
        ""frequency"": ""Minute"",
        ""interval"": 15
    },
    ""external"": true,
    ""policy"": {}
}
</code></pre>

<p>}</p>

<p>Table structure </p>

<pre><code>[DrugSearchId] [uniqueidentifier] NOT NULL,
[SearchTerm] [nvarchar](max) NOT NULL,
[ProdCode] [int] NULL,
[FormCode] [int] NULL,
[PackCode] [int] NOT NULL,
[PackSort] [int] NULL,
[Active] [nvarchar](25) NULL,
[ActiveUnits] [nvarchar](20) NULL,
[UnitsPerPack] [int] NOT NULL,
[NoOfPacks] [int] NOT NULL,
[NoOfRepeats] [int] NULL,
[PbsCode] [nvarchar](6) NULL,
[PbsDesc] [nvarchar](255) NULL,
[Product] [nvarchar](max) NULL,
[GenericList] [nvarchar](max) NULL,
[ActionList] [nvarchar](max) NULL,
[MedicineInformationType] [nchar](50) NOT NULL
</code></pre>

<p><strong>Target</strong></p>

<pre><code>{
""name"": ""DrugSearchIndex"",
""properties"": {
    ""published"": false,
    ""type"": ""AzureSearchIndex"",
    ""linkedServiceName"": ""AzureSearchLinkedService"",
    ""typeProperties"": {
        ""indexName"": ""drug""
    },
    ""availability"": {
        ""frequency"": ""Minute"",
        ""interval"": 15
    }
}
</code></pre>

<p>}</p>

<p><a href=""https://i.stack.imgur.com/08Ekh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/08Ekh.png"" alt=""enter image description here""></a></p>

<p>I'm getting an error when the pipeline runs with the following:</p>

<p>Source column names don't match with the Azure Search Index field names. Please specify Column Mapping.,Source...</p>

<p>What am I missing?</p>
","<azure-data-factory><azure-cognitive-search>","2018-05-06 13:03:19","217","0","1","50201184","<p>It's possible that the reason for error is case mismatch between the field names in the table and in the search index. All field names in the table are Pascal-case, while index fields are camel-case. </p>
"
"50157976","issue in azure pipeline using azure data factory to pull data from sql-server to azure blob","<blockquote>
  <p>The client 'abc@abc.com' with object id 'abcabcabcabcabc' does not
  have authorization to perform action
  'Microsoft.Resources/deployments/write' over scope
  '/subscriptions/abcabcabc/resourcegroups/abc-01-east/providers/Microsoft.Resources/deployments/publishing-123123123123'</p>
</blockquote>

<p>I was trying to create a pipeline using azure data factory to pull data from sql-server to azure blob, but i am facing the above issue while i was trying to use my integration runtime which already exsist in my azure portal. </p>

<p>At present I have data factory contributor role assigned to me, what other roles should I have to avoid this issue?</p>
","<azure><azure-data-factory>","2018-05-03 14:51:21","1033","1","2","52254904","<p>I had a similar issue being a contributor for an ADF. With this role, you seem to be able to open the ADF UI, but the moment you try to publish anything, you get the above error. Making me a data factory contributor for that ADF didn't help. </p>

<p>What did help was making me a data factory contributor on the resource group level. So go to the resource group that contains the ADF, go to IAM and add you as a data factory contributor. </p>

<p>I also noticed, you need to close the data factory ui before IAM changes take effect. </p>

<p>Azure's roles are a bit of a mystery to me so it would be useful if someone could provide an explanation of how and why.</p>
"
"50157976","issue in azure pipeline using azure data factory to pull data from sql-server to azure blob","<blockquote>
  <p>The client 'abc@abc.com' with object id 'abcabcabcabcabc' does not
  have authorization to perform action
  'Microsoft.Resources/deployments/write' over scope
  '/subscriptions/abcabcabc/resourcegroups/abc-01-east/providers/Microsoft.Resources/deployments/publishing-123123123123'</p>
</blockquote>

<p>I was trying to create a pipeline using azure data factory to pull data from sql-server to azure blob, but i am facing the above issue while i was trying to use my integration runtime which already exsist in my azure portal. </p>

<p>At present I have data factory contributor role assigned to me, what other roles should I have to avoid this issue?</p>
","<azure><azure-data-factory>","2018-05-03 14:51:21","1033","1","2","52410641","<p>Steps</p>

<p>1 - Register an Enterprise APP in your Azure Active Directory</p>

<p>2 - Create a key in the Enterprise APP and save the value somewhere</p>

<p>3 - Go to  your Azure SQL Database through Management Console and </p>

<pre><code>CREATE USER [your application name] FROM EXTERNAL PROVIDER;
</code></pre>

<p>4 - Change the authentication method for Principal and use the application id and key on the form</p>

<p>For more information:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database</a></p>
"
"50154437","How to create Spark activity to run Scala script on HDInsight?","<p>I want to execute Scala script using HDInsight. Below article describes running py script but did not mention abt Scala. I followed the article (instead of py file, uploaded Scala file) but Azure DataFactory failed to execute with error message:</p>

<pre><code>({
    ""errorCode"": ""2312"",
    ""message"": ""Spark job failed, batch id:4"",
    ""failureType"": ""UserError""
}
</code></pre>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-spark"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-spark</a></p>
","<scala><azure><apache-spark><azure-data-factory><azure-hdinsight>","2018-05-03 11:51:39","777","1","2","54981379","<p>To resolve your problem you'd need to</p>

<ol>
<li>Open your pipeline in ADF</li>
<li>Click on your spark activity </li>
<li>Go to script/jar, open Advanced</li>
<li>Set Debug Information for your activity to
Always.</li>
</ol>

<p>Though you be able to view logs after pipeline failed. Adf will provide you url to your logs. It'll be on the linked storage in the <strong>log</strong> dir in the root of your jar. </p>

<p>Examine logs to understand what causes your spark app to fail.
<a href=""https://i.stack.imgur.com/EnB50.png"" rel=""nofollow noreferrer"">See image from ADF</a></p>
"
"50154437","How to create Spark activity to run Scala script on HDInsight?","<p>I want to execute Scala script using HDInsight. Below article describes running py script but did not mention abt Scala. I followed the article (instead of py file, uploaded Scala file) but Azure DataFactory failed to execute with error message:</p>

<pre><code>({
    ""errorCode"": ""2312"",
    ""message"": ""Spark job failed, batch id:4"",
    ""failureType"": ""UserError""
}
</code></pre>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-spark"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-spark</a></p>
","<scala><azure><apache-spark><azure-data-factory><azure-hdinsight>","2018-05-03 11:51:39","777","1","2","55147921","<p>If you want to execute spark code written in scala then you have to call Jar file
. If it is too complicated then you can also use databricks notebook. Let me know in case you face issue </p>
"
"50149745","Lookup and foreach to Web activity in Azure Data factory: @Item() returning null","<p>As mentioned in the below link, I am triggering Lookup first. It gives me Email Ids and then for each of the email id, I am invoking the POST request.</p>

<p><a href=""https://stackoverflow.com/questions/48519589/iterating-through-azure-sql-table-in-azure-data-factory"">Iterating Through azure SQL table in Azure Data Factory</a></p>

<p>I have mentioned @pipeline().parameters.tableList in the items in the settings of the For each. And I after for each I have set an email notification to check the output of @pipeline().parameters.leadList. I am getting it correctly so far so good.</p>

<p>But when i am using @item() it is returning me null.</p>

<blockquote>
  <p>I am confused why @item() giving me null even though
  @pipeline().parameters.leadList in child pipeline giving me correct
  value?</p>
</blockquote>

<p>And I followed this approach: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-bulk-copy-portal"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-bulk-copy-portal</a></p>

<p><strong>Parent pipeline</strong></p>

<pre><code>{
    ""name"": ""LookupToSF"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""LookupToSF"",
                ""description"": ""Retrieve the Lead name and email ids from the Lead table of the Salesforce"",
                ""type"": ""Lookup"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false
                },
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""SalesforceSource"",
                        ""query"": ""select name, email from lead where email='abcd@xxxx.com'"",
                        ""readBehavior"": ""query""
                    },
                    ""dataset"": {
                        ""referenceName"": ""SalesforceObjectToAddPersonAskNicely"",
                        ""type"": ""DatasetReference""
                    },
                    ""firstRowOnly"": false
                }
            },
            {
                ""name"": ""TriggerForEachLead"",
                ""type"": ""ExecutePipeline"",
                ""dependsOn"": [
                    {
                        ""activity"": ""LookupToSF"",
                        ""dependencyConditions"": [
                            ""Succeeded""
                        ]
                    }
                ],
                ""typeProperties"": {
                    ""pipeline"": {
                        ""referenceName"": ""SendSurvey"",
                        ""type"": ""PipelineReference""
                    },
                    ""waitOnCompletion"": true,
                    ""parameters"": {
                        ""leadList"": {
                            ""value"": ""@activity('LookupToSF').output.value"",
                            ""type"": ""Expression""
                        }
                    }
                }
            }
        ]
    }
}
</code></pre>

<p>**</p>

<p><strong>Child pipeline</strong></p>

<pre><code>{
    ""name"": ""SendSurvey"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""ForEachLead"",
                ""type"": ""ForEach"",
                ""typeProperties"": {
                    ""items"": {
                        ""value"": ""@pipeline().parameters.leadList"",
                        ""type"": ""Expression""
                    },
                    ""isSequential"": true,
                    ""activities"": [
                        {
                            ""name"": ""WebActivityToAddPerson"",
                            ""type"": ""WebActivity"",
                            ""policy"": {
                                ""timeout"": ""7.00:00:00"",
                                ""retry"": 0,
                                ""retryIntervalInSeconds"": 30,
                                ""secureOutput"": false
                            },
                            ""typeProperties"": {
                                ""url"": ""https://xxxx.asknice.ly/api/v1/person/trigger"",
                                ""method"": ""POST"",
                                ""headers"": {
                                    ""X-apikey"": ""xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx""
                                },
                                ""data"": {
                                    ""email"": ""@{item().Email}"",
                                    ""addperson"": ""true""
                                }
                            }
                        },
                        {
                            ""name"": ""WebActivityForErrorAddingPerson"",
                            ""type"": ""WebActivity"",
                            ""dependsOn"": [
                                {
                                    ""activity"": ""WebActivityToAddPerson"",
                                    ""dependencyConditions"": [
                                        ""Failed""
                                    ]
                                }
                            ],
                            ""policy"": {
                                ""timeout"": ""7.00:00:00"",
                                ""retry"": 0,
                                ""retryIntervalInSeconds"": 30,
                                ""secureOutput"": false
                            },
                            ""typeProperties"": {
                                ""url"": ""https://prod-07.australiaeast.logic.azure.com:443/workflows/xxxxxxxxxxxxx"",
                                ""method"": ""POST"",
                                ""headers"": {
                                    ""Content-Type"": ""application/json""
                                },
                                ""body"": {
                                    ""Status"": ""Failure"",
                                    ""message"": ""@{activity('WebActivityToAddPerson').output}"",
                                    ""subject"": ""Failure in adding""
                                }
                            }
                        },
                        {
                            ""name"": ""WebActivityToSendSurvey"",
                            ""type"": ""WebActivity"",
                            ""dependsOn"": [
                                {
                                    ""activity"": ""WebActivityToAddPerson"",
                                    ""dependencyConditions"": [
                                        ""Completed""
                                    ]
                                }
                            ],
                            ""policy"": {
                                ""timeout"": ""7.00:00:00"",
                                ""retry"": 0,
                                ""retryIntervalInSeconds"": 30,
                                ""secureOutput"": false
                            },
                            ""typeProperties"": {
                                ""url"": ""https://xxxxxxxx.asknice.ly/api/v1/person/trigger"",
                                ""method"": ""POST"",
                                ""headers"": {
                                    ""X-apikey"": ""xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx""
                                },
                                ""data"": ""Email=@{item().Email}&amp;triggeremail=true ""
                            }
                        },
                        {
                            ""name"": ""WebActivityForErrorSendingSurvey"",
                            ""type"": ""WebActivity"",
                            ""dependsOn"": [
                                {
                                    ""activity"": ""WebActivityToSendSurvey"",
                                    ""dependencyConditions"": [
                                        ""Failed""
                                    ]
                                }
                            ],
                            ""policy"": {
                                ""timeout"": ""7.00:00:00"",
                                ""retry"": 0,
                                ""retryIntervalInSeconds"": 30,
                                ""secureOutput"": false
                            },
                            ""typeProperties"": {
                                ""url"": ""https://prod-07.australiaeast.logic.azure.com:443/workflows/xxxxxxxxxxxxxxxxx"",
                                ""method"": ""POST"",
                                ""headers"": {
                                    ""Content-Type"": ""application/json""
                                },
                                ""body"": {
                                    ""Status"": ""Failure"",
                                    ""message"": ""@{activity('WebActivityToAddPerson').output}"",
                                    ""subject"": ""Failure in adding""
                                }
                            }
                        },
                        {
                            ""name"": ""UserAddingNotification"",
                            ""type"": ""WebActivity"",
                            ""dependsOn"": [
                                {
                                    ""activity"": ""WebActivityToAddPerson"",
                                    ""dependencyConditions"": [
                                        ""Succeeded""
                                    ]
                                }
                            ],
                            ""policy"": {
                                ""timeout"": ""7.00:00:00"",
                                ""retry"": 0,
                                ""retryIntervalInSeconds"": 30,
                                ""secureOutput"": false
                            },
                            ""typeProperties"": {
                                ""url"": ""https://prod-07.australiaeast.logic.azure.com:443/workflows/xxxxxxxxxxxxxxxxxxxxx"",
                                ""method"": ""POST"",
                                ""headers"": {
                                    ""Content-Type"": ""application/json""
                                },
                                ""body"": {
                                    ""Status"": ""Success"",
                                    ""message"": ""@{activity('WebActivityToAddPerson').output}"",
                                    ""subject"": ""User Added/Updated successfully""
                                }
                            }
                        }
                    ]
                }
            },
            {
                ""name"": ""SuccessSurveySentNotification"",
                ""type"": ""WebActivity"",
                ""dependsOn"": [
                    {
                        ""activity"": ""ForEachLead"",
                        ""dependencyConditions"": [
                            ""Succeeded""
                        ]
                    }
                ],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false
                },
                ""typeProperties"": {
                    ""url"": ""https://prod-07.australiaeast.logic.azure.com:443/workflows/xxxxxxxxxxxxxxxxxxxxxx"",
                    ""method"": ""POST"",
                    ""headers"": {
                        ""Content-Type"": ""application/json""
                    },
                    ""body"": {
                        ""Status"": ""Success"",
                        ""message"": ""@{item()}"",
                        ""subject"": ""Survey sent successfully""
                    }
                }
            }
        ],
        ""parameters"": {
            ""leadList"": {
                ""type"": ""Object""
            }
        }
    }
}
</code></pre>
","<azure-data-factory>","2018-05-03 07:55:02","2775","1","1","50267549","<p>So I found the answer. The item() was giving me null because the foreach parameter was not parsing correctly hence there was nothing in item().</p>
"
"50124780","Trigger Azure Data Factory Pipeline from Logic App w/ Parameter","<p>Let me preface that I'm a noob to Logic Apps and Data Factory. Anyways, I'm currently working on an integration and one part of it is that I need to trigger a pipeline in Data Factory from Logic Apps. I've successfully done that, the <i>one</i> part I can't seem to figure out is how to pass parameters to my pipeline. I've tried altering the JSON under both the ""parameters"" &amp; ""triggers"" sections but haven't gotten anything to click so far. The pipeline ends up executing, but only with the default parameters.</p>

<p>Has anybody had any success in doing this yet? Any help is appreciated.</p>
","<azure-data-factory><azure-logic-apps>","2018-05-01 22:35:10","8868","2","3","50345850","<p>as I said in the comment I create a workaround with azure functions. Azure functions and logic app work well together. 
On this link you can see how to create and manage pipelines with .net 
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-dot-net"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-dot-net</a></p>

<p>If you already have ADF and pipeline, you just want to run it (with pipelines) then you can just </p>

<pre><code>Dictionary&lt;string, object&gt; parameters = new Dictionary&lt;string, object&gt;
       {
           {""BoxSerialNumbers"", req.BoxSerialNumbers},
           {""StartDate"", req.StartDate },
           {""EndDate"",req.EndDate },
           {""Recipient"", req.Recipient }
       };//this is how you add initialaze parameters

        var client = Authenticate(); //Authentication with azure
        log.Info(""Creating."");
        CreateRunResponse runResponse = client.Pipelines.CreateRun(resourceGroup, dataFactoryName, ""pipeline1"", parameters);//run pipeline, you can do this async (it's better)
        log.Info(""Created."");
        var response = new HttpResponseMessage();
        if (client.PipelineRuns.Get(resourceGroup, dataFactoryName, runResponse.RunId).Status.Equals(""InProgress""))
        {
            response = new HttpResponseMessage(HttpStatusCode.OK)
            {
                Content = new StringContent(runResponse.RunId, Encoding.UTF8)
            };
        }
        else
        {
            response = new HttpResponseMessage(HttpStatusCode.BadRequest)
            {
                Content = new StringContent(""Pipeline didn't started"", Encoding.UTF8)//just some validation for function
            };
        }
        return response;                                               


    public static DataFactoryManagementClient Authenticate()
    {
        var context = new AuthenticationContext(""https://login.windows.net/"" + tenantID);
        ClientCredential cc = new ClientCredential(applicationID, authenticationKey);
        AuthenticationResult result = context.AcquireTokenAsync(""https://management.azure.com/"", cc).Result;
        ServiceClientCredentials cred = new TokenCredentials(result.AccessToken);
        return new DataFactoryManagementClient(cred) { SubscriptionId = subscriptionID };
    }
</code></pre>

<p>So in the request, you can pass your parameters from a logic app, with runId you can check status. Then in logic app just simple HTTP request to call this function. Hope this help someone.</p>
"
"50124780","Trigger Azure Data Factory Pipeline from Logic App w/ Parameter","<p>Let me preface that I'm a noob to Logic Apps and Data Factory. Anyways, I'm currently working on an integration and one part of it is that I need to trigger a pipeline in Data Factory from Logic Apps. I've successfully done that, the <i>one</i> part I can't seem to figure out is how to pass parameters to my pipeline. I've tried altering the JSON under both the ""parameters"" &amp; ""triggers"" sections but haven't gotten anything to click so far. The pipeline ends up executing, but only with the default parameters.</p>

<p>Has anybody had any success in doing this yet? Any help is appreciated.</p>
","<azure-data-factory><azure-logic-apps>","2018-05-01 22:35:10","8868","2","3","51432606","<p>I used DraganB's solution but the call signature on</p>

<pre><code>CreateRunResponse runResponse = client.Pipelines.CreateRun(resourceGroup, dataFactoryName, ""pipeline1"", parameters);
</code></pre>

<p>has changed.  Minor edits make this work perfectly:</p>

<pre><code>CreateRunResponse runResponse = client.Pipelines.CreateRun(resourceGroup, dataFactoryName, ""pipeline1"", parameters: parameters);
</code></pre>

<p>Here's the function for anyone that needs it.</p>

<pre><code>[FunctionName(""DatafactoryShim"")]
    public async static Task&lt;HttpResponseMessage&gt; Run(
        [HttpTrigger(AuthorizationLevel.Function, ""post"")]
        HttpRequestMessage req,
        ExecutionContext context,
        TraceWriter log
    )
    {
        string messageBody = await req.Content.ReadAsStringAsync();

        BlobToDatalakeFactoryParameters postValues = JsonHelper.ToClass&lt;BlobToDatalakeFactoryParameters&gt;(messageBody);

        Dictionary&lt;string, object&gt; parameters = new Dictionary&lt;string, object&gt;
        {
            {""blobContainer"", postValues.BlobContainer},
            {""blobFolder"", postValues.BlobFolder },
            {""relativeDatalakeFolder"", postValues.RelativeDatalakeFolder },
            {""modelType"", postValues.ModelType }

        }; //this is how you add initialaze parameters

        var client = Authenticate(); //Authentication with azure

        string resourceGroup = ConfigurationManager.AppSettings[""resourceGroup""];
        string dataFactoryName = ConfigurationManager.AppSettings[""dataFactoryName""];
        string pipelineName = ConfigurationManager.AppSettings[""pipelineName""];

        Console.WriteLine(""Creating pipeline run..."");
        CreateRunResponse runResponse = client.Pipelines.CreateRunWithHttpMessagesAsync(
            resourceGroup,
            dataFactoryName,
            pipelineName,
            parameters: parameters).Result.Body;
        Console.WriteLine(""Pipeline run ID: "" + runResponse.RunId);

        var response = new HttpResponseMessage();

        if (client.PipelineRuns.Get(ConfigurationManager.AppSettings[""resourceGroup""],
            ConfigurationManager.AppSettings[""dataFactoryName""], runResponse.RunId).Status.Equals(""InProgress""))
        {
            response = new HttpResponseMessage(HttpStatusCode.OK)
            {
                Content = new StringContent(runResponse.RunId, Encoding.UTF8)
            };
        }
        else
        {
            response = new HttpResponseMessage(HttpStatusCode.BadRequest)
            {
                Content =
                    new StringContent(""Pipeline didn't started"", Encoding.UTF8) //just some validation for function
            };
        }

        return response;
    }
</code></pre>
"
"50124780","Trigger Azure Data Factory Pipeline from Logic App w/ Parameter","<p>Let me preface that I'm a noob to Logic Apps and Data Factory. Anyways, I'm currently working on an integration and one part of it is that I need to trigger a pipeline in Data Factory from Logic Apps. I've successfully done that, the <i>one</i> part I can't seem to figure out is how to pass parameters to my pipeline. I've tried altering the JSON under both the ""parameters"" &amp; ""triggers"" sections but haven't gotten anything to click so far. The pipeline ends up executing, but only with the default parameters.</p>

<p>Has anybody had any success in doing this yet? Any help is appreciated.</p>
","<azure-data-factory><azure-logic-apps>","2018-05-01 22:35:10","8868","2","3","51824880","<p>You can use the body property of the logic app's ""Create a pipeline run"" action to pass parameters to the pipeline. As always, be careful because not only is this action in preview but I also couldn't find this solution in any MS documentation. I just made an educated guess based on how other similar actions are formatted.</p>

<p>Example:</p>

<pre><code>""Run_my_pipeline"": {
  ""inputs"": {
    ""host"": {
      ""connection"": {
        ""name"": ""@parameters('$connections')['azuredatafactory']['connectionId']""
      }
    },
    ""method"": ""post"",
    ""body"": {
      ""param1"": ""myParamValue"",
      ""param2"": ""myParamValue""
    },
    ""path"": ""..."",
    ""queries"": {
      ""x-ms-api-version"": ""2017-09-01-preview""
    },
    ""authentication"": ""@parameters('$authentication')""
  }
}
</code></pre>
"
"50118575","Modify global parameter ADF pipeline","<p>How can I modify the value of a global parameter declared in a pipeline of an ADF?</p>

<p>Let's say I need to check whether or not a file in a ADLS exists. I declare a boolean global parameter, but according to my logic inside a U-SQL activity I need to modify its value. How can I do that?</p>

<p>Thanks!!!</p>
","<u-sql><azure-data-factory>","2018-05-01 14:44:18","1253","0","1","50125948","<p>U-SQL's script parameter model only provides input parameters and no output parameters. If you want to communicate something back, you currently have to do this via a file. E.g., you write the file in the script and then use a Powershell activity to look at the file to set the ADF parameter (I assume that Data Factory has that option).</p>
"
"50112515","Azure data factory: From Salesforce to REST API","<p>I have created data factory source as Salesforce for which i am querying lead and then want to pass each lead email id as an argument (POST request) to a REST endpoint.</p>

<p>I am not sure in the pipeline what sink should i put and if it is HTTP File dataset then how to pass the email id from Source to sink in the argument?</p>
","<azure-data-factory>","2018-05-01 06:58:13","879","0","1","50130369","<p>The only way I know how to surface data in ADF itself is through a lookup activity. You can then iterate through the results of the lookup using a forEach activity. Reference the result of the lookup using in the ForEach items parameter:</p>

<pre><code>@activity('nameoflookupactivity').output.value
</code></pre>

<p>If you need to add multiple IDs at once I think to only way would be to concatenate your IDs in a sink like a SQL database. You would first have to copy your IDs to a table in SQL. Then in Azure SQL DB/SQL server 2017 you could use the following query:</p>

<pre><code>SELECT STRING_AGG([salesforce_Id], ',') AS Ids FROM salesforceLeads; 
</code></pre>

<p>The ADF tutorial for incrementally loading multiple tables discusses the ForEach activity extensively, you can find it here:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-multiple-tables-powershell"" rel=""nofollow noreferrer"">ADF Incrementally Loading multiple Tables Tutorial</a></p>

<p>For more information about the STRING_AGG function check out:<a href=""https://learn.microsoft.com/en-us/sql/t-sql/functions/string-agg-transact-sql?view=sql-server-2017"" rel=""nofollow noreferrer"">STRING_AGG Documentation</a></p>

<p>Thanks,</p>

<p>Jan</p>
"
"50094390","Taking parameters from manual triggers in ADF","<p>Usecase</p>

<p>We have an on-premise Hadoop setup and we are using power BI as a BI visualization tool. What we do currently to get data on Powerbi is as follows.</p>

<ol>
<li>Copy data from on-premise to Azure Blob(Our on-premise schedule does this once the data is ready in Hive)</li>
<li>Data from Azure Blob is then copied to Azure-DataWarehouse/Azure-SQL</li>
<li>Cube refreshes on Azure AAS, AAS pulls data from Azure DataWarehouse/SQL</li>
</ol>

<p>To do the step2 and step3 we are currently running a web server on Azure and the endpoints are configured to take few parameters like the table name, azure file location, cube information and so on. </p>

<p>Sample http request: </p>

<p><a href=""http://azure-web-server-scheduler/copydata?from=blob&amp;to=datawarehouse&amp;fromloc=myblob/data/today.csv&amp;totable=mydb.mytable"" rel=""nofollow noreferrer"">http://azure-web-server-scheduler/copydata?from=blob&amp;to=datawarehouse&amp;fromloc=myblob/data/today.csv&amp;totable=mydb.mytable</a></p>

<p>Here the web servers extract the values from variables(from, fromloc, to, totable) and them does the copy activity. We did this as we had a lot of tables and all could reuse the same function. </p>

<p>Now we have use cases piling up(retries, control flows, email alerts, monitoring) and we are looking for a cloud alternative to do the scheduling job for us, we would still like to hit an HTTP endpoint like the above one. </p>

<p>One of the alternatives we have checked till now is the Azure Data Factory, where are create pipelines to achieve the steps above and trigger the ADF using http endpoints. </p>

<p>Problems</p>

<ol>
<li>How can we take parameters from the http post call and make it available as custom variables[1], this is required within the pipeline so that we can still write a function for each step{2, 3} and the function can take these parameters, we don't want to create an ADF for each table.</li>
<li>How can we detect for failure in ADF steps and send email alerts during failures? </li>
<li>What are the other options apart from ADF to do this in Azure?</li>
</ol>

<p>[1] <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables</a></p>
","<azure><azure-data-factory>","2018-04-30 05:22:41","521","0","1","50200121","<p>You could trigger the copy job from blob to SQL DW via a <a href=""https://learn.microsoft.com/fi-fi/azure/data-factory/control-flow-get-metadata-activity"" rel=""nofollow noreferrer"">Get Metadata Acitivity</a>. It can be used in the following scenarios:
 - Validate the metadata information of any data 
 - Trigger a pipeline when data is ready/ available</p>

<p>For eMail notification you can use a Web Activity calling a LogicApp. See the following <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-control-flow-portal"" rel=""nofollow noreferrer"">tuturial</a> how to send an email notification.</p>
"
"50067281","Azure Data Factory Pipline Run FTP failure","<p>I have a pipeline that fails with the error: </p>

<pre><code>Activity Copy_dbo_hosp_revenuek_lds_100_2016_demo failed: ErrorCode=UserErrorFailedToReadFtpData,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Failed to read data from ftp: The remote server returned an error: (530) Not logged in.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Net.WebException,Message=The remote server returned an error: (530) Not logged in.,Source=System,'
</code></pre>

<p>""... Not logged in...""? This is a little confusing because the FTP just worked when I was setting up the pipeline while following the steps in this article: <a href=""https://www.red-gate.com/simple-talk/cloud/cloud-data/using-copy-wizard-azure-data-factory/"" rel=""nofollow noreferrer"">https://www.red-gate.com/simple-talk/cloud/cloud-data/using-copy-wizard-azure-data-factory/</a></p>

<p>I'm able to connect via command prompt and other FTP applications; why is Azure being difficult?  Did I miss a step?</p>
","<azure><azure-data-factory>","2018-04-27 17:13:06","576","1","1","50344772","<p>Did you enable SSL in your FTP linked service? Does your FTP server support SSL?
If your ftp server doesn't support SSL, you can disable the SSL in FTP linked service. Check out the FTP properties here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-ftp#linked-service-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-ftp#linked-service-properties</a></p>
"
"50056048","Data factory: Trigger time using as a parameter in Relative URI in HTTP file datasets","<p>I have a REST source from where I am ingesting data. I have to trigger pipeline every 2am in the morning. So far so good. I have done this. But there is a parameter in Relative URI in which i have to put 'since time' in it. Basically it will send all the responses after ""since time"" (epoch time) hence its usability is significant.</p>

<p>So suppose if I am triggering it today morning 2am then I have to set ""since time"" in relative URI as (current datetime - 1 day).</p>

<p>Is there any way on how to use parameters of factory to accomplish this?</p>
","<azure-data-factory>","2018-04-27 05:51:22","1067","1","1","50063745","<p>This is accomplished by using data factory's expressions and functions: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions</a></p>

<p>In the dataset, you can combine a date function with the pipeline's trigger time to do the -1 day and format it as follows:</p>

<pre><code>""url"" : ""&lt;HTTP endpoint&gt;/timeSince/@{formatDateTime(adddays(pipeline().TriggerTime, -1), 'yyyy_MM_dd_hh_mm')}""
</code></pre>

<p>Modify it to meet your needs. To format the date see this documentation: <a href=""https://learn.microsoft.com/en-us/dotnet/standard/base-types/standard-date-and-time-format-strings"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/dotnet/standard/base-types/standard-date-and-time-format-strings</a></p>

<p>Hope this helped!</p>
"
"50055813","Azure pipeline mapping I want to add one static field in json while csv import","<p>This is input I want to assign a static value to username so that when this csv will load that static value will be inserted in destination </p>

<pre><code>{
        ""name"": ""Input_dat"",
        ""properties"": {
            ""structure"": [
                {
                    ""name"": ""ServerName"",
                    ""type"": ""String""
                },
                {
                    ""name"": ""DTVer"",
                    ""type"": ""Double""
                },
                {
                    ""name"": ""Ver"",
                    ""type"": ""Double""
                },
                {
                    ""name"": ""UserName"",
                    ""type"": ""String""
                }
            ],
            ""published"": false,
            ""type"": ""AzureBlob"",
            ""linkedServiceName"": ""Source-AzureBlob"",
            ""typeProperties"": {
                ""folderPath"": ""foldwr/folder1/Import/"",
                ""format"": {
                    ""type"": ""TextFormat"",
                    ""rowDelimiter"": ""\n"",
                    ""columnDelimiter"": ""\u0001""
                }
            },
            ""availability"": {
                ""frequency"": ""Day"",
                ""interval"": 1
            },
            ""external"": true,
            ""policy"": {}
        }
    }
</code></pre>

<p>E.g username=""sqladlin""</p>
","<json><azure-data-factory>","2018-04-27 05:32:29","379","0","1","50063976","<p>You cannot do this if your souce is a blob storage. Maybe add the field in the csv before its ingested by data factory, or after.</p>

<p>If you really want to do this with data factory, I think the only option is to go for a custom activity.</p>

<p>Hope this helped!</p>
"
"49996523","Azure Logic apps - Get RefreshId Dynamically for Analysis services","<p>I am trying to Process AAS tabular model using Azure logic app and trying to read status back. I can do this using refreshId manually. </p>

<p><a href=""https://learn.microsoft.com/en-us/azure/analysis-services/analysis-services-async-refresh"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/analysis-services/analysis-services-async-refresh</a></p>

<p>Any Idea How would i retrieve last refreshId dynamically in azure logic app?</p>

<p><a href=""https://i.stack.imgur.com/5X5No.jpg"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<azure-logic-apps><azure-analysis-services><azure-data-factory>","2018-04-24 08:10:06","1097","2","1","50338422","<p>My recommendation would be to simplify your architecture and eliminate the Logic App if you aren't already heavily using Logic Apps. It's not because we don't like Logic Apps. It is just good to simplify the moving parts in an architecture.</p>
<p>It is possible to processing Azure Analysis Services models via ADFv2 with native components. There's no need to use Logic Apps or Azure Batch for custom .NET activities. You can use native activities like Web Activity and Lookup Activity.</p>
<p>You are correct that looking up the refreshID is a bit more complex than it should be. The API to start a refresh asynchronously should return the refreshID in the body of the response and I've provided this feedback <a href=""https://feedback.azure.com/d365community/idea/7d32206c-0625-ec11-b6e6-000d3a4f0858"" rel=""nofollow noreferrer"">here</a>. Please vote for it. In the interim, it's possible to get all the refreshes in the last 30 days and then filter down to the one which started most recently.</p>
<p>I have published an ADFv2 pipeline which uses native activities <a href=""https://github.com/furmangg/automating-azure-analysis-services#processazureas"" rel=""nofollow noreferrer"">here</a>. Please give this a try and see if it meets your needs without a Logic App.</p>
<p><a href=""https://i.stack.imgur.com/lxHuo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lxHuo.png"" alt=""enter image description here"" /></a></p>
"
"49984157","Azure Data Factory v2 set pipeline name dynamically using expression in Execute Pipeline activity","<p>Can we use expression in Execute Pipeline's 'Invoked pipeline' field?</p>

<p>Basically I have list of Pipeline name in SQL Database and I want to execute pipeline in ForEach activity of ADF V2. So if somehow we can set variable and might use it in Invoked Pipeline field! This field is drop down with existing pipeline name.</p>
","<.net-core><azure-data-factory>","2018-04-23 14:57:38","2948","1","2","50345135","<p>Expression is diabled in the property of 'Invoked pipeline'. This field must be static value.
The reason is that this may cause a security issue, and cause an infinite loop.</p>

<p>To work around, you can create several <strong>IF</strong> activity within the <strong>Foreach</strong> activity, and each <strong>IF</strong> activity contains an <strong>Execute Pipeline</strong> activity, once the <strong>IF</strong> condition matches the pipeline name, it will go to the next step.</p>
"
"49984157","Azure Data Factory v2 set pipeline name dynamically using expression in Execute Pipeline activity","<p>Can we use expression in Execute Pipeline's 'Invoked pipeline' field?</p>

<p>Basically I have list of Pipeline name in SQL Database and I want to execute pipeline in ForEach activity of ADF V2. So if somehow we can set variable and might use it in Invoked Pipeline field! This field is drop down with existing pipeline name.</p>
","<.net-core><azure-data-factory>","2018-04-23 14:57:38","2948","1","2","58268435","<p>Take care though, pipelines cannot have more than 40 activities, so this would limit the number of possible invoked pipelines to 20  (assuming a bare setup with 20 IF activities, and one </p>

<p>As per <a href=""https://github.com/MicrosoftDocs/azure-docs/blob/master/includes/azure-data-factory-limits.md"" rel=""nofollow noreferrer"">https://github.com/MicrosoftDocs/azure-docs/blob/master/includes/azure-data-factory-limits.md</a> : </p>

<blockquote>
  <p>Maximum activities per pipeline, which includes inner activities for
  containers: 40</p>
</blockquote>
"
"49979043","Not able to run Azure Data Factory Pipeline using Visual Studio 2015","<p>I have followed the Azure documentation steps to create a simple Copy Data Factory from Blob to SQL.</p>

<p>Now I want to run the pipeline through VS code.
I have checked the authentication keys and Roles assigned are correct.</p>

<p>Below is the code -</p>

<pre><code>var context = new AuthenticationContext(""https://login.windows.net/"" + tenantID);
ClientCredential cc = new ClientCredential(applicationId, authenticationKey);
AuthenticationResult result = context.AcquireTokenAsync(""https://management.azure.com/"", cc).Result;
ServiceClientCredentials cred = new TokenCredentials(result.AccessToken);
var client = new DataFactoryManagementClient(cred) { SubscriptionId = subscriptionId };

Console.WriteLine(""Creating pipeline run..."");
var st = client.Pipelines.Get(resourceGroup, dataFactoryName, pipelineName);
CreateRunResponse runResponse = client.Pipelines.CreateRunWithHttpMessagesAsync(resourceGroup, dataFactoryName, pipelineName).Result.Body;
Console.WriteLine(""Pipeline run ID: "" + runResponse.RunId);
</code></pre>

<p>However, I get <code>Forbidden error</code>.</p>

<blockquote>
  <p>The client 'xxxx' with object id 'xxxx' does not have authorization to perform action 'Microsoft.DataFactory/factories/pipelines/read' over scope '/subscriptions/xxxxx/resourceGroups/'</p>
</blockquote>

<p>How can I fix this?</p>
","<azure><azure-data-factory>","2018-04-23 10:38:50","1085","0","1","49992756","<blockquote>
  <p>How can I fix this?</p>
</blockquote>

<p>According to the exception message that it indicates that you don't assign the corresponding role to application to access the data factory.</p>

<p>I test your code with Azure Datafactory(V2) on my side , it works correctly. The following is my details steps.</p>

<ol>
<li><p><a href=""https://learn.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal#create-an-azure-active-directory-application"" rel=""nofollow noreferrer"">Registry an Azure AD <strong>WebApp</strong> application</a>.</p></li>
<li><p><a href=""https://learn.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal#get-application-id-and-authentication-key"" rel=""nofollow noreferrer"">Get the clientId and clientscret from created Application</a>.</p></li>
<li><p>Assign the role the application to access the datafactory.</p></li>
</ol>

<p><a href=""https://i.stack.imgur.com/DnMVR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DnMVR.png"" alt=""enter image description here""></a></p>

<ol start=""4"">
<li><p>Test code on my side.</p>

<p><a href=""https://i.stack.imgur.com/r4f2f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/r4f2f.png"" alt=""enter image description here""></a></p></li>
</ol>
"
"49973659","NZT On filename output in Azure Data factory","<p>We're attempting to send a file to a system (that's very rigid) from our Datafactory with the New Zealand datetime in the filename (ADF are all UTC)</p>

<p>My output dataset looks like this:</p>

<pre><code>    ""typeProperties"": {
        ""fileName"": ""MasterFile-{fileDateNameVariable}.csv"",
        ""folderPath"": ""something"",
        ""format"": {
            ""type"": ""TextFormat"",
            ""columnDelimiter"": "","",
            ""nullValue"": """",
            ""firstRowAsHeader"": true
        },
        ""partitionedBy"": [
            {
                ""name"": ""fileDateNameVariable"",
                ""value"": {
                    ""type"": ""DateTime"",
                    ""date"": ""$$addhours(SliceStart, 13)"",
                    ""format"": ""yyyyMMdd""
                }
            }
        ]
    }
</code></pre>

<p>As you can see I've tried to add in ""$$addhours(SliceStart, 13)"" but to no avail:</p>

<pre><code>Input is malformed. Reason: inputTable.typeProperties : The date $$addhours(SliceStart, 13) is not a valid variable to partition by. Valid values are SliceStart and SliceEnd..
</code></pre>

<p>Is there a way in which I can create a time variable in the filename without referencing the partitionedBy area? </p>
","<azure><azure-data-factory>","2018-04-23 04:57:51","102","0","1","49983716","<p>If you are using ADFv2, you can use @pipeline.TriggerTime to get the same result without using PartitionedBy.</p>

<p>But it looks like a ADFv1 json, so it should look like this:</p>

<pre><code>""typeProperties"": {
    ""fileName"": ""MasterFile-{Time.AddHours(fileDateNameVariable,13)}.csv"",
    ""folderPath"": ""something"",
    ""format"": {
        ""type"": ""TextFormat"",
        ""columnDelimiter"": "","",
        ""nullValue"": """",
        ""firstRowAsHeader"": true
    },
    ""partitionedBy"": [
        {
            ""name"": ""fileDateNameVariable"",
            ""value"": {
                ""type"": ""DateTime"",
                ""date"": ""SliceStart"",
                ""format"": ""yyyyMMdd""
            }
        }
    ]
}
</code></pre>

<p>Maybe this won't work as-is, I can't test it right now, but that is the correct way to call a function.</p>

<p>Always check here to use functions and system variables <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-functions-variables"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-functions-variables</a></p>

<p>Hope this helped!</p>
"
"49969219","Azure data factory v2 : Customize execute pipeline activity","<p>I am trying to create a master pipeline which will check whether the child pipeline has to be run using the configuration from database. The name of the child pipeline is a parameter to the master pipeline. We would like to choose name of the pipeline for execute pipeline activity using the parameter passed to the project. 
The ultimate goal is that we don't want to get the data from the source again if we have already retrieved it. If there is any error in the whole ETL process, we would like to avoid running the pipeline which has already completed. </p>

<p>I am trying to avoid creating a custom activity since it requires an Azure Batch account. </p>

<p>Any ideas?</p>
","<azure-data-factory>","2018-04-22 18:16:22","525","3","1","49983362","<p>Have you considered calling the pipelines with a powershell script? Install the Azure SDK for powershell from here <a href=""https://azure.microsoft.com/en-us/downloads/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/downloads/</a> and you can make pipelines run with it. If you are somehow getting the name of the pipeline to run, I think its the easiest way to handle what you want to do.</p>

<p>This is how you call a pipeline from powershell (with azure sdk installed):</p>

<pre><code>$SubscriptionName = ""yourSubscName""
$ResourceGroupName = ""your RG name""
$DFName = ""your data factory name""
$PipeName = ""your pipeline name""
Login-AzureRmAccount
Select-AzureRmSubscription -SubscriptionName $SubscriptionName
Invoke-AzureRmDataFactoryV2Pipeline -DataFactoryName $DFName -ResourceGroupName $ResourceGroupName -PipelineName $PipeName
</code></pre>

<p>And that's it! You can run this script from your on-premise pc or with Azure Automation (which is free). For Azure Automation you maybe need to modify it a bit but the basic idea is the same.</p>

<p>Hope this helped!</p>

<p>PS: to run this script you must manually login to azure, but there are ways to automate it that I didn't explain to make this shorter.</p>
"
"49963346","Read container names of an Azure blob in Data Factory V2","<p>My Data Factory V2 pipeline imports CSV files from an Azure blob.</p>

<p>Is there any way in Data Factory V2 to automatically read the names of the containers of which the CSV files are taken from? I search to hand them over to a Microsoft SQL Server and/or save them in the streamed CSV data itself.</p>
","<sql-server><azure><azure-blob-storage><azure-data-factory>","2018-04-22 06:54:07","801","0","1","49975088","<p>Just as which is mentioned in the comments, there is no such thing named folder in Azure Blob Storage. You can only get names of container.</p>

<p>Based on the <a href=""https://learn.microsoft.com/en-us/rest/api/datafactory/datasets/get#examples"" rel=""nofollow noreferrer"">Azure Data Factory REST API</a>, you can get the <code>folderPath</code> from the <code>dataset</code> response body.</p>

<p>sample :</p>

<pre><code>{
  ""id"": ""/subscriptions/12345678-1234-1234-12345678abc/resourceGroups/exampleResourceGroup/providers/Microsoft.DataFactory/factories/exampleFactoryName/datasets/exampleDataset"",
  ""name"": ""exampleDataset"",
  ""properties"": {
    ""type"": ""AzureBlob"",
    ""typeProperties"": {
      ""folderPath"": {
        ""value"": ""@dataset().MyFolderPath"",
        ""type"": ""Expression""
      },
      ""fileName"": {
        ""value"": ""@dataset().MyFileName"",
        ""type"": ""Expression""
      },
      ""format"": {
        ""type"": ""TextFormat""
      }
    },
    ""description"": ""Example description"",
    ""linkedServiceName"": {
      ""referenceName"": ""exampleLinkedService"",
      ""type"": ""LinkedServiceReference""
    },
    ""parameters"": {
      ""MyFolderPath"": {
        ""type"": ""String""
      },
      ""MyFileName"": {
        ""type"": ""String""
      }
    }
  },
  ""etag"": ""280320a7-0000-0000-0000-59b9712a0000""
}
</code></pre>

<p>Or you could get the property in the <a href=""https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.management.datafactory.models.dataset.-ctor?view=azure-dotnet"" rel=""nofollow noreferrer"">Azure Data Factory SDK</a>. Then you could save names into Microsoft SQL Server.</p>

<p>Hope it helps you.</p>
"
"49958536","__spark_conf__.zip: No such file or directory","<p>When I am submitting a Spark activity from Azure Data factory V2 on HDInsight cluster I am getting the error-</p>

<pre><code>Application application_1524230454528_0060 failed 5 times due to AM Container for appattempt_1524230454528_0060_000005 exited with exitCode: -1000
    For more detailed output, check the application tracking page: http://hn1-rbc-cl.tsgjwtochfbexi5wvtaxuipmmg.rx.internal.cloudapp.net:8088/cluster/app/application_1524230454528_0060 Then click on links to logs of each attempt.
    Diagnostics: wasb://rbc-cluster-2018-04-20t13-11-42-670z@rbcdemo.blob.core.windows.net/user/livy/.sparkStaging/application_1524230454528_0060/__spark_conf__.zip: No such file or directory.
    java.io.FileNotFoundException: wasb://rbc-cluster-2018-04-20t13-11-42-670z@rbcdemo.blob.core.windows.net/user/livy/.sparkStaging/application_1524230454528_0060/__spark_conf__.zip: No such file or directory.
    at org.apache.hadoop.fs.azure.NativeAzureFileSystem.getFileStatusInternal(NativeAzureFileSystem.java:2732)
    at org.apache.hadoop.fs.azure.NativeAzureFileSystem.getFileStatus(NativeAzureFileSystem.java:2644)
    at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:253)
    at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:63)
    at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:361)
    at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:359)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)
    at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:359)
    at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:62)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
    Failing this attempt. Failing the application.
</code></pre>

<p>Can someone help me figure out what the issue is or there is any other workaround to run spark jobs via Azure Data factory pipelines. I am stuck on this and could not proceed. As per Azure Data Factory documentation, I tried placing my files in ./files folder in the container where my spark jar is present. But still facing the same issue. </p>
","<azure><apache-spark><hadoop><hadoop-yarn><azure-data-factory>","2018-04-21 17:43:27","446","0","1","49998820","<p>What's your HDI cluster version and the version of Java used to develop your program, please make sure the versions are the same.</p>

<p>BTW, HDI cluster 3.5+ require java 8, while the earlier versions require java 7.</p>
"
"49938899","Azure Data Factory - Integration Runtime for Linux box","<p>I wanted to copy data files from Linux machine to Azure Blob Storage. I am using Azure Data Factory for this(as per the requirement). Can somebody plz help me how to install/ from where to get the Integration Runtime to install on that Linux machine.</p>

<p>Thanks</p>
","<azure><runtime><integration><azure-data-factory>","2018-04-20 09:53:27","6168","1","1","49970380","<p>Azure Data Factory Integration Runtime (self hosted) is currently only available on windows (see system requirements at <a href=""https://www.microsoft.com/en-us/download/details.aspx?id=39717"" rel=""nofollow noreferrer"">https://www.microsoft.com/en-us/download/details.aspx?id=39717</a>). </p>

<p>You could use a Linux file share, see <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system</a> for more details.</p>
"
"49934912","Azure Data Factory: Different Compute environment","<p>There are couple of compute environments that can do transformations for me. I have a REST source from where I am getting responses every day and I have to perform some transformations.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/compute-linked-services"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/compute-linked-services</a></p>

<p>I am confused as to what could be the best way to do it? Or in other words whats the different between all the compute environments as in when should I use Azure Batch, stored procedures, HDInsight, etc?</p>
","<azure-data-factory>","2018-04-20 05:55:11","104","0","1","49943782","<p>It really depends on where you have the data. If you are storing the data in a data lake, you won't use a stored procedure. If you are storing the data in an Azure Sql, you won't use Data Lake Analytics.</p>

<p>Basically its like this:</p>

<pre><code>Data lake -&gt; data lake analytics with u-sql
Azure SQL (warehouse or just sql) -&gt; stored procedure
HDInsight hadoop -&gt; Pig, hive, etc
None of the above -&gt; custom activity with Azure Batch
</code></pre>

<p>Hope this helped!</p>
"
"49934871","Azure data factory: Http file dataset for REST endpoint call","<p>I have set up a dataset of type http file wherein I have setup my endpoint or relative URL. Now the problem is, this URl changes everyday.</p>

<p>Any idea on how can i automate this change without logging into the factory every day?</p>
","<azure-data-factory>","2018-04-20 05:51:46","273","1","1","50267621","<p>So I passed that change as a parameter in the pipeline and it worked.</p>
"
"49923091","Get Metadata Activity ADF V2","<p>Can anyone explain me, what is the use of Get Metadata Activity that is newly introduced in ADF V2?</p>

<p>Actually, the information that is given in learn.microsoft.com isn't enough to understand the uses of this Activity.</p>
","<azure><azure-data-factory>","2018-04-19 14:02:24","4735","2","1","50199878","<p>Main purpose of the Get Metadata Activity is:</p>

<ul>
<li>Validate the metadata information of any data </li>
<li>Trigger a pipeline when data is ready/ available</li>
</ul>

<p>The following example shows how to incrementally load changed files from a folder using the Get Metadata Activity getting filenames and modified Timestamp: </p>

<pre><code>            {
                ""name"": ""IncrementalloadfromSingleFolder"",
                ""properties"": {
                    ""activities"": [
                        {
                            ""name"": ""GetFileList"",
                            ""type"": ""GetMetadata"",
                            ""policy"": {
                                ""timeout"": ""7.00:00:00"",
                                ""retry"": 0,
                                ""retryIntervalInSeconds"": 30,
                                ""secureOutput"": false
                            },
                            ""typeProperties"": {
                                ""dataset"": {
                                    ""referenceName"": ""SrcLocalDir"",
                                    ""type"": ""DatasetReference""
                                },
                                ""fieldList"": [
                                    ""childItems""
                                ]
                            }
                        },
                        {
                            ""name"": ""ForEachFile"",
                            ""type"": ""ForEach"",
                            ""dependsOn"": [
                                {
                                    ""activity"": ""GetFileList"",
                                    ""dependencyConditions"": [
                                        ""Succeeded""
                                    ]
                                }
                            ],
                            ""typeProperties"": {
                                ""items"": {
                                    ""value"": ""@activity('GetFileList').output.childItems"",
                                    ""type"": ""Expression""
                                },
                                ""activities"": [
                                    {
                                        ""name"": ""GetLastModifyfromFile"",
                                        ""type"": ""GetMetadata"",
                                        ""policy"": {
                                            ""timeout"": ""7.00:00:00"",
                                            ""retry"": 0,
                                            ""retryIntervalInSeconds"": 30,
                                            ""secureOutput"": false
                                        },
                                        ""typeProperties"": {
                                            ""dataset"": {
                                                ""referenceName"": ""SrcLocalFile"",
                                                ""type"": ""DatasetReference""
                                            },
                                            ""fieldList"": [
                                                ""lastModified""
                                            ]
                                        }
                                    },
                                    {
                                        ""name"": ""IfNewFile"",
                                        ""type"": ""IfCondition"",
                                        ""dependsOn"": [
                                            {
                                                ""activity"": ""GetLastModifyfromFile"",
                                                ""dependencyConditions"": [
                                                    ""Succeeded""
                                                ]
                                            }
                                        ],
                                        ""typeProperties"": {
                                            ""expression"": {
                                                ""value"": ""@and(less(activity('GetLastModifyfromFile').output.lastModified, pipeline().parameters.current_time), greaterOrEquals(activity('GetLastModifyfromFile').output.lastModified, pipeline().parameters.last_time))"",
                                                ""type"": ""Expression""
                                            },
                                            ""ifTrueActivities"": [
                                                {
                                                    ""name"": ""CopyNewFiles"",
                                                    ""type"": ""Copy"",
                                                    ""policy"": {
                                                        ""timeout"": ""7.00:00:00"",
                                                        ""retry"": 0,
                                                        ""retryIntervalInSeconds"": 30,
                                                        ""secureOutput"": false
                                                    },
                                                    ""typeProperties"": {
                                                        ""source"": {
                                                            ""type"": ""FileSystemSource"",
                                                            ""recursive"": false
                                                        },
                                                        ""sink"": {
                                                            ""type"": ""BlobSink""
                                                        },
                                                        ""enableStaging"": false,
                                                        ""dataIntegrationUnits"": 0
                                                    },
                                                    ""inputs"": [
                                                        {
                                                            ""referenceName"": ""SrcLocalFile"",
                                                            ""type"": ""DatasetReference""
                                                        }
                                                    ],
                                                    ""outputs"": [
                                                        {
                                                            ""referenceName"": ""TgtBooksBlob"",
                                                            ""type"": ""DatasetReference""
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    }
                                ]
                            }
                        }
                    ],
                    ""parameters"": {
                        ""current_time"": {
                            ""type"": ""String"",
                            ""defaultValue"": ""2018-04-01T00:00:00Z""
                        },
                        ""last_time"": {
                            ""type"": ""String"",
                            ""defaultValue"": ""2018-03-01T00:00:00Z""
                        }
                    },
                    ""folder"": {
                        ""name"": ""IncrementalLoadSingleFolder""
                    }
                },
                ""type"": ""Microsoft.DataFactory/factories/pipelines""
            }
</code></pre>

<p>See also recently updated <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity#next-steps"" rel=""nofollow noreferrer"">documentation</a>.</p>
"
"49901490","use spark cassandra package in Azure Data Factory","<p>I created a pyspark script which works fine when I execute it with <code>spark-submit</code>: </p>

<pre><code>spark-submit --packages com.datastax.spark:spark-cassandra-connector_2.11:2.0.6 --conf spark.cassandra.connection.host=12.34.56.68 test_cassandra.py
</code></pre>

<p>As I am working with Azure Data Factory, I would like to execute this job in ADF too. I created the following job : </p>

<pre><code>{
    ""name"": ""spark write to cassandra"",
    ""type"": ""HDInsightSpark"",
    ""policy"": {
        ""timeout"": ""7.00:00:00"",
        ""retry"": 0,
        ""retryIntervalInSeconds"": 30,
        ""secureOutput"": false
    },
    ""typeProperties"": {
        ""rootPath"": ""dev/apps/spikes"",
        ""entryFilePath"": ""test_cassandra.py"",
        ""sparkConfig"": {
            ""packages"": ""datastax:spark-cassandra-connector:2.0.7-s_2.10"",
            ""conf"": ""spark.cassandra.connection.host=12.34.56.78""
        },
        ""sparkJobLinkedService"": {
            ""referenceName"": ""linkedServiceStorageBlobHDI"",
            ""type"": ""LinkedServiceReference""
        }
    },
    ""linkedServiceName"": {
        ""referenceName"": ""linkedServiceHDI"",
        ""type"": ""LinkedServiceReference""
    }
}
</code></pre>

<p>I thought it would be enough, but there is apparently a problem with the package. I received the error : </p>

<blockquote>
  <p>java.lang.ClassNotFoundException: Failed to find data source: org.apache.spark.sql.cassandra. Please find packages at <a href=""https://cwiki.apache.org/confluence/display/SPARK/Third+Party+Projects"" rel=""nofollow noreferrer"">https://cwiki.apache.org/confluence/display/SPARK/Third+Party+Projects</a></p>
</blockquote>

<p>Can you help me to configure properly my activity to make it run ? </p>
","<apache-spark><cassandra><pyspark><azure-data-factory>","2018-04-18 13:51:27","228","0","2","49911847","<p>The failure is caused by the dependency package is not defined correctly. You can follow the document below to include all your dependency jars in the subdirectory ""jars"" of the rootPath of the activity:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-spark#folder-structure"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-spark#folder-structure</a></p>

<p>Or you can use the spark built-in mechanism to manage dependency by customize spark configurations. Please refer to the doc below for details:
<a href=""https://spark.apache.org/docs/latest/configuration.html#available-properties"" rel=""nofollow noreferrer"">https://spark.apache.org/docs/latest/configuration.html#available-properties</a></p>
"
"49901490","use spark cassandra package in Azure Data Factory","<p>I created a pyspark script which works fine when I execute it with <code>spark-submit</code>: </p>

<pre><code>spark-submit --packages com.datastax.spark:spark-cassandra-connector_2.11:2.0.6 --conf spark.cassandra.connection.host=12.34.56.68 test_cassandra.py
</code></pre>

<p>As I am working with Azure Data Factory, I would like to execute this job in ADF too. I created the following job : </p>

<pre><code>{
    ""name"": ""spark write to cassandra"",
    ""type"": ""HDInsightSpark"",
    ""policy"": {
        ""timeout"": ""7.00:00:00"",
        ""retry"": 0,
        ""retryIntervalInSeconds"": 30,
        ""secureOutput"": false
    },
    ""typeProperties"": {
        ""rootPath"": ""dev/apps/spikes"",
        ""entryFilePath"": ""test_cassandra.py"",
        ""sparkConfig"": {
            ""packages"": ""datastax:spark-cassandra-connector:2.0.7-s_2.10"",
            ""conf"": ""spark.cassandra.connection.host=12.34.56.78""
        },
        ""sparkJobLinkedService"": {
            ""referenceName"": ""linkedServiceStorageBlobHDI"",
            ""type"": ""LinkedServiceReference""
        }
    },
    ""linkedServiceName"": {
        ""referenceName"": ""linkedServiceHDI"",
        ""type"": ""LinkedServiceReference""
    }
}
</code></pre>

<p>I thought it would be enough, but there is apparently a problem with the package. I received the error : </p>

<blockquote>
  <p>java.lang.ClassNotFoundException: Failed to find data source: org.apache.spark.sql.cassandra. Please find packages at <a href=""https://cwiki.apache.org/confluence/display/SPARK/Third+Party+Projects"" rel=""nofollow noreferrer"">https://cwiki.apache.org/confluence/display/SPARK/Third+Party+Projects</a></p>
</blockquote>

<p>Can you help me to configure properly my activity to make it run ? </p>
","<apache-spark><cassandra><pyspark><azure-data-factory>","2018-04-18 13:51:27","228","0","2","49917623","

<p>The options as they are in spark-submit are a little bit different in ADF. </p>

<p><code>--packages</code> becomes <code>spark.jars.packages</code><br/>and<br/><code>--conf spark.cassandra.connection.host=12.34.56.78</code> becomes <code>""spark.cassandra.connection.host"": ""12.34.56.78""</code></p>

<p>the final code is : </p>

<pre class=""lang-py prettyprint-override""><code>{
    ""name"": ""spark write to cassandra"",
    ""type"": ""HDInsightSpark"",
    ""policy"": {
        ""timeout"": ""7.00:00:00"",
        ""retry"": 0,
        ""retryIntervalInSeconds"": 30,
        ""secureOutput"": false
    },
    ""typeProperties"": {
        ""rootPath"": ""dev/apps/spikes"",
        ""entryFilePath"": ""test_cassandra.py"",
        ""sparkConfig"": {
            ""spark.jars.packages"": ""datastax:spark-cassandra-connector:2.0.7-s_2.10"",
            ""spark.cassandra.connection.host"": ""12.34.56.78""
        },
        ""sparkJobLinkedService"": {
            ""referenceName"": ""linkedServiceStorageBlobHDI"",
            ""type"": ""LinkedServiceReference""
        }
    },
    ""linkedServiceName"": {
        ""referenceName"": ""linkedServiceHDI"",
        ""type"": ""LinkedServiceReference""
    }
}
</code></pre>
"
"49898829","Azure data factory binary copy from FTP","<p>I have copy activity that does a binary copy of .zip file from FTP to ADLS.
Everything works well, i managed to copy 6 files, and when i needed to copy .zip file which size is 4GB it thorws me an error remote server error 530 - not logged in. Does anybody know what can cause this?</p>

<p><a href=""https://i.stack.imgur.com/T1MnV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T1MnV.png"" alt=""enter image description here""></a></p>
","<azure><ftp><azure-data-factory>","2018-04-18 11:41:23","582","0","1","50225273","<p>Copy Activity level retry will always start from beginning, but there's also retry inner a Copy Activity which will resume from break point. If a Copy finally failed, it means the break point resume also failed.
Could you provide a RunID for troubleshooting?</p>

<p>Regards,
Gary</p>
"
"49890680","Python REST API in Azure Data factory","<p>I have an extra API which I need to access using my Python REST client (OAuth and tokens are everything sorted). But I would like to invoke this python client in my Data Factory as I want to store the results into Azure SQL warehouse.</p>

<p>Any help on how to do it?</p>
","<azure-data-factory>","2018-04-18 03:27:10","1392","1","1","49920812","<p>What you want is a custom activity, this way you can run a Python script with Data Factory. For this you need to create an extra resource, Azure Batch.</p>

<p>More info here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity</a> and here <a href=""https://learn.microsoft.com/en-us/azure/batch/batch-python-tutorial#batch-python-tutorial-code-sample"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/batch/batch-python-tutorial#batch-python-tutorial-code-sample</a></p>

<p>If you dont like that option, you can use a Web activity, that also calls a custom REST api. Documentation: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity</a></p>

<p>Hope this helped!</p>
"
"49854129","Azure Data Factory Decompress","<p>I have a question. I have some .zip files on my FTP location, and i want to decompress and copy them to ADLS. In documentation there is clear explanation for this: </p>

<p>""Read .zip file from FTP server, decompress it to get the files inside, and land those files into Azure Data Lake Store. You define an input FTP dataset with the compression type JSON property as ZipDeflate.""</p>

<p>I tried with this and on my ADLS i get compress file. I tried to put file properties, to define delimiter and stuff, and still getting compressed file on data lake store. 
I think its how i define output dataset. Is there some rules how to define output dataset if input dataset are .zip files from FTP.</p>
","<azure><ftp><azure-data-factory>","2018-04-16 09:39:59","6090","1","1","49856476","<p>You most likely defined your output dataset's compression as ZipDeflate also, thats why you are getting the zipped file on ADLS. Try changing your output dataset (the same where you configure the path in ADLS) so it doesn't use compression. You should have in your copy activity the input dataset where you configure the ftp, with compression and the output dataset, where you configure most stuff for the lake, without compression.</p>

<p>This way you are telling data factory to get a zipped file, and save it unzipped on ADLS.</p>

<p>Hope this helped!</p>
"
"49833919","Long retry in azure datafactory","<p>We have a requirement of retry the particular slice after 1 hour if validation failed.
I gone through the Azure ADF docs <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-create-pipelines"" rel=""nofollow noreferrer"">here</a> and got option of <strong>longRetry</strong> but its not working as expected.</p>

<p><strong>Policy validation in input dataset:-</strong></p>

<pre><code> ""policy"": {
            ""validation"": {
                ""minimumSizeMB"": 0.000005
            }
        }
</code></pre>

<p><strong>Long retry in pipeline:-</strong></p>

<pre><code> ""policy"": {
                    ""concurrency"": 1,
                    ""retry"": 3,
                    ""longRetry"": 2,
                    ""longRetryInterval"": ""01:00:00""
                }
</code></pre>

<p><strong>Summary:-</strong> We want to check validation of particular dataset every hour with pipeline having frequency daily once.</p>

<p>Please let me know if am doing anything wrong here.</p>

<p>Thanks</p>
","<azure><azure-data-factory>","2018-04-14 16:56:14","1309","2","1","49839395","<p>To work with <strong>longRetry</strong> we have to add some more property in dataset as below.</p>

<pre><code>policy"": {
            ""validation"": {
                ""minimumSizeMB"": 0.000005
            },
            ""externalData"": {
                ""retryInterval"": ""01:00:00"",
                ""maximumRetry"": 4
            }
</code></pre>
"
"49824206","ADF Self Hosted Runtime consuming all of the memory","<p>We are running ADF pipelines in V1 and have self hosted integration runtime. The integration runtime is consuming almost all the memory and not releasing it. The machine is getting failed in such cases. 
Is there a way to control memory uses for integration runtime?</p>
","<azure><azure-data-factory>","2018-04-13 19:52:31","758","0","1","50852527","<p>There's continous effort to improve the memory usage of self-hosted integration runtime. Could you please try the latest version which was released on June 6th.</p>
"
"49814312","Change connection string Linked Service in Azure Data Factory v2","<p>I am using Azure Data Factory V2 to integrate data from multiple on-premise mySql database. Is it possible to define just one mysql linked service and then modify the connection string (server name, credential, integration runtime) during runtime. My plan is to use lookup activity to read list of connection strings and then use for-each activity to iterate over that list to pull data from each database using copy activity.</p>

<p>Is it possible to do such things, preferably using the Azure data Factory UI? or Should I use .NET SDK to do that?</p>

<p>Thank You</p>
","<azure><azure-data-factory>","2018-04-13 10:00:33","1351","3","1","49814849","<p>We are working on enabling parameter support for Linked Service so you can accomplish the scenario described today.  Please stay tuned!  Today you still need to use specific connection strings in Linked Service definition.</p>
"
"49802659","Is there any poll interval setting for the self-hosted integration runtime?","<p>I use a combination of lookup and foreach activities to iterate through the set of data ingestion queries and execute them (reasons behind that is a separate broad topic :)). As the data source is connected to the private network, I have provisioned a dedicated VM to run the self-hosted runtime. In most cases everything runs smoothly, I can see worker processes eating the CPU and high overall CPU utilization (which is good).</p>
<p><strong>But:</strong> sometimes, when most work is done, and there are just 2-3 activities standing in line, I can see that the runtime does no processing and CPU usage drops to zero, no new entries appear in the event log. After some time (approximately 10 minutes) I get the 30002 (the example is provided below) and runtime happily completes the work.</p>
<p>Example event message:</p>
<pre>
Job ID: ***-fcab-429a-bb45-***
Task ID: ***-d820-414e-ad8c-***
Queue ID: ***-4f44-4c39-a1c1-***
Log ID: PulledOffNewTask
</pre>
<p><strong>The question</strong>: What could be the root cause of such Azure Data Factory self-hosted integration runtime's behaviour? Can this be fine-tuned?</p>
<hr />
<h3>UPDATE 1</h3>
<p>Errors have been spotted in the application log and warnings have been spotted in the integration runtime log.</p>
<p>Application log contains 3 sets of errors (see below events [1] to [5]) that occured in the time interval of ~2 minutes, shortly after that 8 events (exactly the number of my worker processes) were logged to the integration runtime log (see [6]), straight after that &quot;Windows Error Reporting&quot; events appear. And then we face a &quot;freeze&quot;.</p>
<p>So - looks like a bug :(</p>
<p><strong>&quot;application&quot;</strong> log:</p>
<p>[1]</p>
<pre><code>Application: diawp.exe
Framework Version: v4.0.30319
Description: The process was terminated due to an unhandled exception.
Exception Info: System.NullReferenceException
   at Microsoft.DataTransfer.TransferTask.CopyTaskBase.UpdateJobProgress(System.Object)
   at System.Threading.ExecutionContext.RunInternal(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object, Boolean)
   at System.Threading.ExecutionContext.Run(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object, Boolean)
   at System.Threading.TimerQueueTimer.CallCallback()
   at System.Threading.TimerQueueTimer.Fire()
   at System.Threading.TimerQueue.FireNextTimers()
</code></pre>
<p>[2]</p>
<pre><code>Faulting application name: diawp.exe, version: 3.5.6639.1, time stamp: 0x5aa8cf5f
Faulting module name: unknown, version: 0.0.0.0, time stamp: 0x00000000
Exception code: 0xc0000005
Fault offset: 0x00007ff914402c65
Faulting process id: 0x1bc4
Faulting application start time: 0x01d3d287ef6e34fa
Faulting application path: C:\Program Files\Microsoft Integration Runtime\3.0\Shared\diawp.exe
Faulting module path: unknown
Report Id: 1fe7de4d-5481-478d-b9e7-d542c24ab18a
Faulting package full name: 
Faulting package-relative application ID: 
</code></pre>
<p>[3]: Unable to open the Server service performance object. The first four bytes (DWORD) of the Data section contains the status code.</p>
<p>[4]: The Open Procedure for service &quot;WmiApRpl&quot; in DLL &quot;C:\Windows\system32\wbem\wmiaprpl.dll&quot; failed. Performance data for this service will not be available.</p>
<p><strong>&quot;Integration Runtime&quot;</strong> log:</p>
<p>[6]</p>
<pre><code>'Type=System.InvalidOperationException,Message=Instance &amp;apos;diawp#10&amp;apos; does not exist in the specified Category.,Source=System,StackTrace=   at System.Diagnostics.CounterDefinitionSample.GetInstanceValue(String instanceName)
   at System.Diagnostics.PerformanceCounter.NextSample()
   at System.Diagnostics.PerformanceCounter.NextValue()
   at Microsoft.DataTransfer.TransferTask.FormatedPerfCounter.TryGet(Single&amp;amp; value),'
Job ID: 7b629411-c6cd-42d0-9939-e830e58db015
Log ID: Warning
</code></pre>
","<azure><azure-data-factory>","2018-04-12 17:27:36","1919","2","3","49804888","<p>As far as I know, you don't have a lot of options to tune the Integration Runtime. My bet is a connectivity issue with your private network. Whenever you run the pipeline, open a cmd at the vm and ping the database pc with -t. If the process hangs, take a look at the response time between pings. </p>

<p>Example ping:</p>

<pre><code>ping 192.168.1.1 -t
</code></pre>

<p>Hope this helped!</p>
"
"49802659","Is there any poll interval setting for the self-hosted integration runtime?","<p>I use a combination of lookup and foreach activities to iterate through the set of data ingestion queries and execute them (reasons behind that is a separate broad topic :)). As the data source is connected to the private network, I have provisioned a dedicated VM to run the self-hosted runtime. In most cases everything runs smoothly, I can see worker processes eating the CPU and high overall CPU utilization (which is good).</p>
<p><strong>But:</strong> sometimes, when most work is done, and there are just 2-3 activities standing in line, I can see that the runtime does no processing and CPU usage drops to zero, no new entries appear in the event log. After some time (approximately 10 minutes) I get the 30002 (the example is provided below) and runtime happily completes the work.</p>
<p>Example event message:</p>
<pre>
Job ID: ***-fcab-429a-bb45-***
Task ID: ***-d820-414e-ad8c-***
Queue ID: ***-4f44-4c39-a1c1-***
Log ID: PulledOffNewTask
</pre>
<p><strong>The question</strong>: What could be the root cause of such Azure Data Factory self-hosted integration runtime's behaviour? Can this be fine-tuned?</p>
<hr />
<h3>UPDATE 1</h3>
<p>Errors have been spotted in the application log and warnings have been spotted in the integration runtime log.</p>
<p>Application log contains 3 sets of errors (see below events [1] to [5]) that occured in the time interval of ~2 minutes, shortly after that 8 events (exactly the number of my worker processes) were logged to the integration runtime log (see [6]), straight after that &quot;Windows Error Reporting&quot; events appear. And then we face a &quot;freeze&quot;.</p>
<p>So - looks like a bug :(</p>
<p><strong>&quot;application&quot;</strong> log:</p>
<p>[1]</p>
<pre><code>Application: diawp.exe
Framework Version: v4.0.30319
Description: The process was terminated due to an unhandled exception.
Exception Info: System.NullReferenceException
   at Microsoft.DataTransfer.TransferTask.CopyTaskBase.UpdateJobProgress(System.Object)
   at System.Threading.ExecutionContext.RunInternal(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object, Boolean)
   at System.Threading.ExecutionContext.Run(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object, Boolean)
   at System.Threading.TimerQueueTimer.CallCallback()
   at System.Threading.TimerQueueTimer.Fire()
   at System.Threading.TimerQueue.FireNextTimers()
</code></pre>
<p>[2]</p>
<pre><code>Faulting application name: diawp.exe, version: 3.5.6639.1, time stamp: 0x5aa8cf5f
Faulting module name: unknown, version: 0.0.0.0, time stamp: 0x00000000
Exception code: 0xc0000005
Fault offset: 0x00007ff914402c65
Faulting process id: 0x1bc4
Faulting application start time: 0x01d3d287ef6e34fa
Faulting application path: C:\Program Files\Microsoft Integration Runtime\3.0\Shared\diawp.exe
Faulting module path: unknown
Report Id: 1fe7de4d-5481-478d-b9e7-d542c24ab18a
Faulting package full name: 
Faulting package-relative application ID: 
</code></pre>
<p>[3]: Unable to open the Server service performance object. The first four bytes (DWORD) of the Data section contains the status code.</p>
<p>[4]: The Open Procedure for service &quot;WmiApRpl&quot; in DLL &quot;C:\Windows\system32\wbem\wmiaprpl.dll&quot; failed. Performance data for this service will not be available.</p>
<p><strong>&quot;Integration Runtime&quot;</strong> log:</p>
<p>[6]</p>
<pre><code>'Type=System.InvalidOperationException,Message=Instance &amp;apos;diawp#10&amp;apos; does not exist in the specified Category.,Source=System,StackTrace=   at System.Diagnostics.CounterDefinitionSample.GetInstanceValue(String instanceName)
   at System.Diagnostics.PerformanceCounter.NextSample()
   at System.Diagnostics.PerformanceCounter.NextValue()
   at Microsoft.DataTransfer.TransferTask.FormatedPerfCounter.TryGet(Single&amp;amp; value),'
Job ID: 7b629411-c6cd-42d0-9939-e830e58db015
Log ID: Warning
</code></pre>
","<azure><azure-data-factory>","2018-04-12 17:27:36","1919","2","3","49813170","<p>It looks like caused by worker crash. Could you please check event log from: Windows Log => Application? Any error in the category?</p>
"
"49802659","Is there any poll interval setting for the self-hosted integration runtime?","<p>I use a combination of lookup and foreach activities to iterate through the set of data ingestion queries and execute them (reasons behind that is a separate broad topic :)). As the data source is connected to the private network, I have provisioned a dedicated VM to run the self-hosted runtime. In most cases everything runs smoothly, I can see worker processes eating the CPU and high overall CPU utilization (which is good).</p>
<p><strong>But:</strong> sometimes, when most work is done, and there are just 2-3 activities standing in line, I can see that the runtime does no processing and CPU usage drops to zero, no new entries appear in the event log. After some time (approximately 10 minutes) I get the 30002 (the example is provided below) and runtime happily completes the work.</p>
<p>Example event message:</p>
<pre>
Job ID: ***-fcab-429a-bb45-***
Task ID: ***-d820-414e-ad8c-***
Queue ID: ***-4f44-4c39-a1c1-***
Log ID: PulledOffNewTask
</pre>
<p><strong>The question</strong>: What could be the root cause of such Azure Data Factory self-hosted integration runtime's behaviour? Can this be fine-tuned?</p>
<hr />
<h3>UPDATE 1</h3>
<p>Errors have been spotted in the application log and warnings have been spotted in the integration runtime log.</p>
<p>Application log contains 3 sets of errors (see below events [1] to [5]) that occured in the time interval of ~2 minutes, shortly after that 8 events (exactly the number of my worker processes) were logged to the integration runtime log (see [6]), straight after that &quot;Windows Error Reporting&quot; events appear. And then we face a &quot;freeze&quot;.</p>
<p>So - looks like a bug :(</p>
<p><strong>&quot;application&quot;</strong> log:</p>
<p>[1]</p>
<pre><code>Application: diawp.exe
Framework Version: v4.0.30319
Description: The process was terminated due to an unhandled exception.
Exception Info: System.NullReferenceException
   at Microsoft.DataTransfer.TransferTask.CopyTaskBase.UpdateJobProgress(System.Object)
   at System.Threading.ExecutionContext.RunInternal(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object, Boolean)
   at System.Threading.ExecutionContext.Run(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object, Boolean)
   at System.Threading.TimerQueueTimer.CallCallback()
   at System.Threading.TimerQueueTimer.Fire()
   at System.Threading.TimerQueue.FireNextTimers()
</code></pre>
<p>[2]</p>
<pre><code>Faulting application name: diawp.exe, version: 3.5.6639.1, time stamp: 0x5aa8cf5f
Faulting module name: unknown, version: 0.0.0.0, time stamp: 0x00000000
Exception code: 0xc0000005
Fault offset: 0x00007ff914402c65
Faulting process id: 0x1bc4
Faulting application start time: 0x01d3d287ef6e34fa
Faulting application path: C:\Program Files\Microsoft Integration Runtime\3.0\Shared\diawp.exe
Faulting module path: unknown
Report Id: 1fe7de4d-5481-478d-b9e7-d542c24ab18a
Faulting package full name: 
Faulting package-relative application ID: 
</code></pre>
<p>[3]: Unable to open the Server service performance object. The first four bytes (DWORD) of the Data section contains the status code.</p>
<p>[4]: The Open Procedure for service &quot;WmiApRpl&quot; in DLL &quot;C:\Windows\system32\wbem\wmiaprpl.dll&quot; failed. Performance data for this service will not be available.</p>
<p><strong>&quot;Integration Runtime&quot;</strong> log:</p>
<p>[6]</p>
<pre><code>'Type=System.InvalidOperationException,Message=Instance &amp;apos;diawp#10&amp;apos; does not exist in the specified Category.,Source=System,StackTrace=   at System.Diagnostics.CounterDefinitionSample.GetInstanceValue(String instanceName)
   at System.Diagnostics.PerformanceCounter.NextSample()
   at System.Diagnostics.PerformanceCounter.NextValue()
   at Microsoft.DataTransfer.TransferTask.FormatedPerfCounter.TryGet(Single&amp;amp; value),'
Job ID: 7b629411-c6cd-42d0-9939-e830e58db015
Log ID: Warning
</code></pre>
","<azure><azure-data-factory>","2018-04-12 17:27:36","1919","2","3","49813475","<p>30002 means IntegrationRuntime got new tasks assigned and started execution.
If the 10 minutes ""retry interval"" could constantly be reproduced, then 30002 could further indicate that IntegrationRuntime lost tracks on the previous failed tasks it got assigned and had to go with retry. </p>

<p>You can search the specific JobIds in the eventlogs to verify whether shown up 10 minutes before and any exceptions related to.</p>

<p>Btw, the polling interval in normal happy path is in seconds level.</p>
"
"49800613","Incremental loading of files from On-prem file server to Azure Data Lake","<p>We would like to do incremental loading of files from our on-premises file server to Azure Data Lake using Azure Data Factory v2.</p>

<p>Files are supposed to store on daily basis in the on-prem fileserver and we will have to run the ADFv2 pipeline on regular intervals during the day and only the new un-processed files from the folder should be captured.</p>
","<azure><azure-data-lake><azure-data-factory>","2018-04-12 15:35:24","303","0","2","49808709","<p>Our recommendation is to put the set of files for daily ingestion into /YYYY/MM/DD directories.  You can refer to this example on how to use system variables (@trigger().scheduledTime) to read files from the corresponding directory:</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-read-write-partitioned-data"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/how-to-read-write-partitioned-data</a></p>
"
"49800613","Incremental loading of files from On-prem file server to Azure Data Lake","<p>We would like to do incremental loading of files from our on-premises file server to Azure Data Lake using Azure Data Factory v2.</p>

<p>Files are supposed to store on daily basis in the on-prem fileserver and we will have to run the ADFv2 pipeline on regular intervals during the day and only the new un-processed files from the folder should be captured.</p>
","<azure><azure-data-lake><azure-data-factory>","2018-04-12 15:35:24","303","0","2","49923098","<p>In the source dataset, you can do file filter.You can do that by time for example (calling datetime function in expression language) or something else what will define new file. 
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions</a>
Then with a scheduled trigger, you can execute pipeline n times during the day.</p>
"
"49789859","How to recognize file pattern in azure blob input dataset","<p>This is my file pattern: adm_domain_20180401, adm_domain_20180402, these are from one particular source. same folder also contains adm_agent_20180401, adm_agent_20180402. I want to only copy files from blob to ADL with prefix adm_domain, is there any way to define the file pattern in input data set?</p>

<p>DATASET:</p>

<p>{
    ""name"": ""CgAdmDomain"",
    ""properties"": {
        ""published"": false,
        ""type"": ""AzureBlob"",
        ""linkedServiceName"": ""flk_blob_dev_ls"",
        ""typeProperties"": {
            ""folderPath"": ""incoming/{Date}/"",
            ""format"": {
                ""type"": ""TextFormat""
            },
            ""partitionedBy"": [
                {
                    ""name"": ""Date"",
                    ""value"": {
                        ""type"": ""DateTime"",
                        ""date"": ""SliceStart"",
                        ""format"": ""yyyyMMdd""
                    }
                }
            ]
        },
        ""availability"": {
            ""frequency"": ""Minute"",
            ""interval"": 15
        },
        ""external"": true,
        ""policy"": {}
    }
}</p>
","<azure><azure-data-factory>","2018-04-12 06:52:33","327","0","2","49790435","<p>Are you using ADF V1 or V2?  We are working on adding filename wildcard support in ADF V2.</p>
"
"49789859","How to recognize file pattern in azure blob input dataset","<p>This is my file pattern: adm_domain_20180401, adm_domain_20180402, these are from one particular source. same folder also contains adm_agent_20180401, adm_agent_20180402. I want to only copy files from blob to ADL with prefix adm_domain, is there any way to define the file pattern in input data set?</p>

<p>DATASET:</p>

<p>{
    ""name"": ""CgAdmDomain"",
    ""properties"": {
        ""published"": false,
        ""type"": ""AzureBlob"",
        ""linkedServiceName"": ""flk_blob_dev_ls"",
        ""typeProperties"": {
            ""folderPath"": ""incoming/{Date}/"",
            ""format"": {
                ""type"": ""TextFormat""
            },
            ""partitionedBy"": [
                {
                    ""name"": ""Date"",
                    ""value"": {
                        ""type"": ""DateTime"",
                        ""date"": ""SliceStart"",
                        ""format"": ""yyyyMMdd""
                    }
                }
            ]
        },
        ""availability"": {
            ""frequency"": ""Minute"",
            ""interval"": 15
        },
        ""external"": true,
        ""policy"": {}
    }
}</p>
","<azure><azure-data-factory>","2018-04-12 06:52:33","327","0","2","49798560","<p>The <code>fileFilter</code> is not available for <code>Azure Blob Storage</code>. If you are looking files at on-premise then you will be able to achieve this by specifying a filter to be used to select a subset of files in the <code>folderPath</code> rather than all files - <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-onprem-file-system-connector"" rel=""nofollow noreferrer"">link</a></p>

<p>To solely achieve this for <code>Azure Blob Storage</code> use <code>Azure Data Factory</code> Custom activities. Implement the logic through custom code (.NET) and have it as an activity in the pipeline. More info about how to use custom activites - <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-use-custom-activities"" rel=""nofollow noreferrer"">further reading</a>.</p>
"
"49764307","Azure DataFactory from Blob storage to SFTP server","<p>After hunting through the net I can find lots of examples of retrieving data from SFTP but none to send from Blob storage to SFTP.</p>

<p>Basically I attempted to do this using a Logic App but Azure only supports files less than 50MB (which is really dumb).</p>

<p>All the Azure docs I have read reference pulling but not pushing.</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-sftp-connector"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-sftp-connector</a></p>

<p>etc etc.. </p>

<p>Maybe someone with better googling skills can help me find the docs to help me out.</p>

<p>I'm using DataFactory V1.0 not 2.0 cheers</p>
","<azure><azure-data-factory>","2018-04-10 23:38:28","1047","0","1","49884816","<p>Always check this table to see if a data store is supported as source or sink in a data movement activity. </p>

<p>In this case, SFTP is supported as source but not as sink, this means its possible to extract data from it but not store data on it.</p>

<p>Hope this helped!</p>
"
"49761227","HDinsight hive activity pipeline not creating output in Azure data lake","<p>Trying to run a hive activity using data factory, the pipeline finished fine and table with data is created inside cluster, but output dataset is not creating file in Azure data lake store, is this by intention? </p>

<p>just trying to learn so be gentle.</p>

<p><strong>Input dataset:</strong></p>

<p>Standard input csv file containing the data</p>

<pre><code>{
""name"": ""dlsinput"",
""properties"": {
    ""published"": false,
    ""type"": ""AzureDataLakeStore"",
    ""linkedServiceName"": ""dls"",
    ""typeProperties"": {
        ""fileName"": ""output.csv"",
        ""folderPath"": ""data/output/"",
        ""format"": {
            ""type"": ""TextFormat"",
            ""columnDelimiter"": "",""
        }
    },
    ""availability"": {
        ""frequency"": ""Day"",
        ""interval"": 1
    },
    ""external"": true,
    ""policy"": {}
}
}
</code></pre>

<p><strong>Pipeline:</strong></p>

<p>Pipeline referring to hdinsight cluster </p>

<pre><code>{
""name"": ""HiveActivitySamplePipeline"",
""properties"": {
    ""activities"": [
        {
            ""type"": ""HDInsightHive"",
            ""typeProperties"": {
                ""scriptPath"": ""scripts/hive.hql"",
                ""scriptLinkedService"": ""sta""
            },
            ""inputs"": [
                {
                    ""name"": ""dlsinput""
                }
            ],
            ""outputs"": [
                {
                    ""name"": ""dlsoutput""
                }
            ],
            ""scheduler"": {
                ""frequency"": ""Day"",
                ""interval"": 1
            },
            ""name"": ""HiveActivitySample"",
            ""linkedServiceName"": ""hdi""
        }
    ],
    ""start"": ""2018-04-05T12:20:00Z"",
    ""end"": ""2018-04-10T23:59:59Z"",
    ""isPaused"": false,
    ""hubName"": ""adf"",
    ""pipelineMode"": ""Scheduled""
}
}
</code></pre>

<p><strong>Output:</strong></p>

<p>Output with file i want to have created</p>

<pre><code>{
""name"": ""dlsoutput"",
""properties"": {
    ""published"": false,
    ""type"": ""AzureDataLakeStore"",
    ""linkedServiceName"": ""dls"",
    ""typeProperties"": {
        ""fileName"": ""myfile.csv"",
        ""folderPath"": ""data/output/"",
        ""format"": {
            ""type"": ""TextFormat"",
            ""rowDelimiter"": ""\n"",
            ""columnDelimiter"": "",""
        }
    },
    ""availability"": {
        ""frequency"": ""Day"",
        ""interval"": 1
    }
}
}
</code></pre>

<p>Hive.hql</p>

<pre><code>DROP TABLE IF EXISTS temp;
CREATE EXTERNAL TABLE IF NOT EXISTS temp (
Name STRING,
Road STRING,
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
 LOCATION 'adl://dls.azuredatalakestore.net/data/output/';
</code></pre>
","<azure-data-factory><azure-hdinsight>","2018-04-10 19:07:45","459","0","1","49940376","<p>Looks like it's because the Hive.hql only includes commands to create a table, and there is no data inserted to the table, so you didn't see any data file generated.</p>
"
"49756277","Error when copying data from Azure Blob Storage to SQL Server using Azure Data Factory V2","<p>When triggering my Azure Data Factory V2 pipeline, I receive the following error message:</p>

<pre><code>{
    ""errorCode"": ""2109"",
    ""message"": ""Region detection for linked services with type 'SqlServer' is not supported, please specify location instead."",
    ""failureType"": ""UserError"",
    ""target"": ""Staging""
    }
</code></pre>

<p>My Azure Data Factory V2 has ""westeurope"" as its location, and so does the blob storage with the original data.</p>

<p>All linked services in the Data Factory are validated. When I press 'Validate all', my factory also claims to be without mistakes.</p>

<p>What could the source of my error be and how do I fix it?</p>

<p><a href=""https://i.stack.imgur.com/JPuLX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JPuLX.png"" alt=""Output list of my Copy task""></a></p>
","<azure-blob-storage><azure-data-factory>","2018-04-10 14:30:48","474","1","1","49756905","<p>To address this issue you must create an Azure Integration Runtime and give it a correct location. Official documentation here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#azure-integration-runtime"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#azure-integration-runtime</a> and here <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#integration-runtime-location"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#integration-runtime-location</a></p>

<p>To do this you can use Powershell with Azure SDK, the command is Set-AzureRmDataFactoryv2IntegrationRuntime. It has a lot of parameters but you must make sure you create it with -Type Managed.</p>

<p>Example: </p>

<pre><code>Set-AzureRmDataFactoryV2IntegrationRuntime -DataFactoryName $DataFactoryName -Name ""AzureIR"" -ResourceGroupName $ResourceGroupName -Type Managed -Location ""West Europe""
</code></pre>

<p>After you create this, make sure you reference this in the connectVia property of your linked service for the azure sql.</p>

<p>Hope this helped!</p>
"
"49753834","Does azure data factory uses data catalog services?","<p>I am planning to use Azure Data factory for ETL process, I would like to know if Azure Data factory uses the metamodel that is captured in the Data Catalog. Please advice</p>
","<azure-data-factory><azure-data-catalog>","2018-04-10 12:35:13","757","1","1","49956481","<p>No currently you can' t reuse Metadata stored in Azure Data Catalog in Azure Data Factory directly. You could try to reuse some of the Metadata, retrieving Data Assets via the Rest API (<a href=""https://learn.microsoft.com/en-us/rest/api/datacatalog/data-catalog-data-asset"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/datacatalog/data-catalog-data-asset</a>), but I think that it will be faster doing the setup in Azure Data Factory. Also be aware that main focus of Data Factory is on movement and orchestration. For Big Data transformations, you will use e. g. Databricks activities, for ""classic"" ETL integrate SSIS. </p>
"
"49743936","Copy files from on-prem to azure","<p>I'm new to Azure eco system. I'm doing some research on copying data from on-prem to azure. I found following options: </p>

<ul>
<li>AzCopy </li>
<li>Azure Data Factory (Copy Data Tool) </li>
<li>Data Management Gateway</li>
</ul>

<p>Ours is a Microsoft shop; so, I'm looking for tools that gel with MS platform. Also, down the line, we want to automate the entire thing as much as we can. So, I think, <strong>Azure Storage Explorer</strong> is out of the question.  Is there a preference among the above 3. Or, are there any better tools? </p>
","<azure><azure-sql-database><azure-data-factory><azcopy>","2018-04-10 01:32:32","1635","0","2","49744303","<p>I think you are mixing stuff, Copy Data Tool is just an Azure Data Factory Wizard to make some sample data moving between resources. Azure Data Factory uses the data management gateway to get on premises resources such as files and databases.</p>

<p>What you want to do can be made with Azure Data Factory. I recommend using version 2 (even in its preview version) because its Authoring is easier to understand if you are new to the tool. You can graphically configure linked services, datasets and pipelines from there.</p>

<p>I hope this helped, if you need further help just ask away!</p>
"
"49743936","Copy files from on-prem to azure","<p>I'm new to Azure eco system. I'm doing some research on copying data from on-prem to azure. I found following options: </p>

<ul>
<li>AzCopy </li>
<li>Azure Data Factory (Copy Data Tool) </li>
<li>Data Management Gateway</li>
</ul>

<p>Ours is a Microsoft shop; so, I'm looking for tools that gel with MS platform. Also, down the line, we want to automate the entire thing as much as we can. So, I think, <strong>Azure Storage Explorer</strong> is out of the question.  Is there a preference among the above 3. Or, are there any better tools? </p>
","<azure><azure-sql-database><azure-data-factory><azcopy>","2018-04-10 01:32:32","1635","0","2","49745180","<p>If you're already familiar with SSIS, there's also the option to use SSIS in ADF that enables on-prem data access via VNet.</p>
"
"49732768","Cast values to string in Json Path Expression in Azure Data Factory copy activity","<p>I have an input JSON file where the actual value of the property could be either a numeric value or a string.I extract the value by specifying a json path expression like </p>

<pre><code>""fieldValue"": ""values[*].value""
</code></pre>

<p>in the azure data factory copy activity, connection tab for the source.</p>

<p>Since the actual field value in the JSON could be something like ""X"" or 2.34 it is not able parse it all into strings even though in the schema I specify the fieldValue as string.</p>

<p>So is there a way I could cast it so that it would take the string as is in case the value is ""X"" and if its 2.34 convert it to ""2.34""</p>

<pre><code>""fields"" : ""[{""fieldId"":""fieldName"", ""values"": [{value: 2.34}]},....}]""
</code></pre>
","<azure-data-factory>","2018-04-09 12:15:24","5480","0","1","49746958","<p>You can use expressions in the value field.
Here's <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#conversion-functions"" rel=""nofollow noreferrer"">expression and functions in ADF doc</a>.</p>

<p>Example usage:</p>

<pre><code>""field"": {
    ""value"": ""@string(your_value)"",
    ""type"": ""Expression""
}
</code></pre>

<p>And on the ADF visual tool, there's a ""add dynamic content"" link below each field. Expressions, functions and system variables can be dynamically added there.</p>
"
"49721469","Where to upload console app for Azure DF v2 custom activity?","<p>I am trying to create DF v2 custom activity. I have console application, but i am not sure where to put it.
Should i only build it and upload to blob storage or is there some special publishment?</p>

<p>Thx</p>
","<custom-activity><azure-data-factory>","2018-04-08 18:46:31","44","0","1","49744360","<p>As stated here <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-dotnet-custom-activity</a> you need to upload your code to a Compute Environment which will be an Azure Batch pool.</p>

<blockquote>
  <p>The custom activity runs your customized code logic on an Azure Batch
  pool of virtual machines.</p>
</blockquote>

<p>Then just configure the linked service and the activity as shown in the official documentation.</p>

<p>Hope this helped!</p>
"
"49706549","Look Up Activity in Azure Data Factory V2","<p>How to take oracle as the source dataset in the look up activity in azure data factory V2 ? I have an Oracle dataset 'DS_ORAC'. I would like to take this dataset as the Source Dataset in a look up activity.</p>
","<azure><azure-data-factory>","2018-04-07 10:47:20","831","1","1","49706964","<p>The oracle dataset is now supported as well in Lookup Activity. We will soon update the Supported Capabilities at <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity</a>. You may take a try. The dataset and link service definition would be the same as Copy Activity.</p>
"
"49697148","Azure Data Factory v2 parameters for connection string","<p>I am new to using Azure Data Factory v2 and have a few questions regarding general transforming connection strings / LinkedServices when deploying to multiple environments.</p>

<h1>Coming from SSIS background:</h1>

<p>we used to define connection strings as project parameters. This allowed transforming the connecting string when deploying the artifacts onto different environments.</p>

<p>How can I accomplish the same using Azure Data Factory v2 ?
Is there an easy way to do this ? </p>

<p>I was trying to set up linked services with connection strings as parameters which then could be passed along with the triggers?  Is this feasible ?</p>
","<azure-data-factory>","2018-04-06 16:22:25","1681","0","3","49729660","<p>Relating to <code>SSIS</code> (where we would use configuration files - <code>.dtsconfig</code> for deployment to different deployments), for <code>ADFV2</code> (&amp; <code>ADFV1</code> too) we could look into the option of using <code>ARM</code> templates where for every different environment (dev, test &amp; prod) to deploy the <code>ADF</code> solution that many deployment files(<code>.json</code>) could be made and script the deployments using <code>PowerShell</code>. It is possible to use ARM template parameters to parameterize connections to linked services and other environment specific values. Then there are <code>ADFV2</code> specific <code>PowerShell cmdlets</code> for creation/deployment of <code>ADFV2</code> pipelines. </p>

<p>Also you can use <code>PowerShell</code> to parametrize connections to <code>linked services</code> and other environment specific values.</p>

<p>With the <code>ADFV2 UI</code> the <code>VSTS GIT</code> integration is possible so is the  deployment and integration. <code>VSTS GIT</code> integration allows to choose a feature/development branch or create a new one in the <code>VSTS GIT</code> repository. Once the changes are merged with the master branch it could be published to data factory using <code>ADFV2 UI</code>. </p>
"
"49697148","Azure Data Factory v2 parameters for connection string","<p>I am new to using Azure Data Factory v2 and have a few questions regarding general transforming connection strings / LinkedServices when deploying to multiple environments.</p>

<h1>Coming from SSIS background:</h1>

<p>we used to define connection strings as project parameters. This allowed transforming the connecting string when deploying the artifacts onto different environments.</p>

<p>How can I accomplish the same using Azure Data Factory v2 ?
Is there an easy way to do this ? </p>

<p>I was trying to set up linked services with connection strings as parameters which then could be passed along with the triggers?  Is this feasible ?</p>
","<azure-data-factory>","2018-04-06 16:22:25","1681","0","3","66650474","<p>This feature is now avaialble from URL below.  Are you the one who requested the feature? :)</p>
<p><a href=""https://azure.microsoft.com/en-us/blog/parameterize-connections-to-your-data-stores-in-azure-data-factory/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/blog/parameterize-connections-to-your-data-stores-in-azure-data-factory/</a></p>
"
"49697148","Azure Data Factory v2 parameters for connection string","<p>I am new to using Azure Data Factory v2 and have a few questions regarding general transforming connection strings / LinkedServices when deploying to multiple environments.</p>

<h1>Coming from SSIS background:</h1>

<p>we used to define connection strings as project parameters. This allowed transforming the connecting string when deploying the artifacts onto different environments.</p>

<p>How can I accomplish the same using Azure Data Factory v2 ?
Is there an easy way to do this ? </p>

<p>I was trying to set up linked services with connection strings as parameters which then could be passed along with the triggers?  Is this feasible ?</p>
","<azure-data-factory>","2018-04-06 16:22:25","1681","0","3","66659538","<p>I ended up solving this issue with setting up an azure key vault per environment each having a connection string secret (more details here : <a href=""https://learn.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault</a>)</p>
<pre><code>- dev
    - dev-azure-datafactory
    - dev-key-vault
     - key: db-conn-string
       value: dev-db.windows.net
- qa
    - qa-azure-datafactory
    - qa-key-vault
     - key: db-conn-string
       value: qa-db.windows.net

- production
    - prod-azure-datafactory
    - prod-key-vault
     - key: db-conn-string
       value: prod-db.windows.net
</code></pre>
<p>In Azure Data Factory</p>
<ul>
<li><p>Define an Azure Key Vault linked service</p>
</li>
<li><p>Use the azure key vault linked service while defining connection string(s) for other linked services</p>
</li>
<li><p>This approach removes any changing of parameters in the actual linked service</p>
</li>
<li><p>The connection string with azure key vault linked service can be changed as part of your azure pipeline deployment  (more details here : <a href=""https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment</a>)</p>
</li>
<li><p>Each azure data factory can be given access to its azure key vault using MSI (automated it with terraform in our case)</p>
</li>
</ul>
"
"49693338","Use output from Web Activity call as variable","<p>I'm using ADFv2 to transfer some data. As a part of this operation I need some configuration values to pass into the pipeline.</p>

<p>The config values must be pulled at runtime from a REST service - not as parameters.</p>

<p>I can successfully query the REST service with Web Activity and I can see the output in the debug view.</p>

<p>Now the problem :)</p>

<p>How do I use this output in other activities further in the pipeline?</p>

<p>My Web Activity configuration is like this:</p>

<pre><code>{
""name"": ""Web1"",
""type"": ""WebActivity"",
""policy"": {
   ""timeout"": ""7.00:00:00"",
   ""retry"": 0,
   ""retryIntervalInSeconds"": 30,
   ""secureOutput"": false
},
""typeProperties"": {
   ""url"": ""https://myazurefunction.azurewebsites.net/api/MyFunction"",
   ""method"": ""GET"",
   ""headers"": {
   ""Content-Type"": ""application/json""
   }
}
</code></pre>

<p>I have tried to access the output after is has executed, but it seems empty:</p>

<pre><code>@activity('Web1').Output
@activity('Web1').output
@string(activity('Web1').Output)
</code></pre>

<p>they are all empty. Any suggestions?
Thanks!</p>
","<azure-data-factory>","2018-04-06 12:56:35","13047","2","2","49695600","<p>I set up an ADF2 and try to get a response. </p>

<p>This works for me:</p>

<pre><code>@string(activity('Post').output)
</code></pre>

<p>Have you checked the output in the debugging?</p>

<p>Here is my output:</p>

<pre><code>{
    ""test"": {
        ""value"": 123,
        ""text"": abc
    },
    ""concat"": 123abc
}
</code></pre>

<p>I use the stored procedure to insert the values into the destination table on a Logical Server.</p>
"
"49693338","Use output from Web Activity call as variable","<p>I'm using ADFv2 to transfer some data. As a part of this operation I need some configuration values to pass into the pipeline.</p>

<p>The config values must be pulled at runtime from a REST service - not as parameters.</p>

<p>I can successfully query the REST service with Web Activity and I can see the output in the debug view.</p>

<p>Now the problem :)</p>

<p>How do I use this output in other activities further in the pipeline?</p>

<p>My Web Activity configuration is like this:</p>

<pre><code>{
""name"": ""Web1"",
""type"": ""WebActivity"",
""policy"": {
   ""timeout"": ""7.00:00:00"",
   ""retry"": 0,
   ""retryIntervalInSeconds"": 30,
   ""secureOutput"": false
},
""typeProperties"": {
   ""url"": ""https://myazurefunction.azurewebsites.net/api/MyFunction"",
   ""method"": ""GET"",
   ""headers"": {
   ""Content-Type"": ""application/json""
   }
}
</code></pre>

<p>I have tried to access the output after is has executed, but it seems empty:</p>

<pre><code>@activity('Web1').Output
@activity('Web1').output
@string(activity('Web1').Output)
</code></pre>

<p>they are all empty. Any suggestions?
Thanks!</p>
","<azure-data-factory>","2018-04-06 12:56:35","13047","2","2","52971621","<p>In ADFv2, you access the output of previous activities using <code>@activity('ActivityName').output</code>.</p>

<p>For the web activity defined, the response from your function should be in JSON format, so you would reference specific JSON values using their attribute names in the response. For example, your defined web activity, named Web1, calls a function that returns a response of:</p>

<pre><code>{
  ""foo"": ""bar"",
  ""some"": ""value""
}
</code></pre>

<p>To use the value of <code>foo</code> in a subsequent ADF activity, you would reference <code>@activity('Web1').output.foo</code>. ADFv2 provides multiple type conversion functions, should you need the returned value converted to another type. </p>

<p>If your function is returning an empty JSON response back, you may want to inspect the response from your function using Postman or another tool to ensure you are returning a properly formatted response, and that your function isn't failing for another reason.</p>

<p>Inside your Azure function code, you should be returning a JSON object, along with a success code, similar to <code>return req.CreateResponse(HttpStatusCode.OK, json);</code>.</p>

<p>Also note that if you reference a property of the response and it does not exist, ADF will fail at that point, so you can use an If Condition activity to check for the required values to better handle failures in ADFv2.</p>
"
"49655984","Azure Data Factory failed while copying big data files","<p>I am using <strong>Azure Data Factory</strong> from copying data from <strong>REST API</strong> to <strong>Azure Data Lake Store</strong>. Following is the JSON of my activity</p>

<pre><code>{
    ""name"": ""CopyDataFromGraphAPI"",
    ""type"": ""Copy"",
    ""policy"": {
        ""timeout"": ""7.00:00:00"",
        ""retry"": 0,
        ""retryIntervalInSeconds"": 30,
        ""secureOutput"": false
    },
    ""typeProperties"": {
        ""source"": {
            ""type"": ""HttpSource"",
            ""httpRequestTimeout"": ""00:30:40""
        },
        ""sink"": {
            ""type"": ""AzureDataLakeStoreSink""
        },
        ""enableStaging"": false,
        ""cloudDataMovementUnits"": 0,
        ""translator"": {
            ""type"": ""TabularTranslator"",
            ""columnMappings"": ""id: id, name: name, email: email, administrator: administrator""
        }
    },
    ""inputs"": [
        {
            ""referenceName"": ""MembersHttpFile"",
            ""type"": ""DatasetReference""
        }
    ],
    ""outputs"": [
        {
            ""referenceName"": ""MembersDataLakeSink"",
            ""type"": ""DatasetReference""
        }
    ]
}
</code></pre>

<p>The REST API is created by me. First for testing purpose I am returning just 2500 rows and my Pipeline was working fine. It copied the data from REST API call to Azure Data Lake Store. </p>

<p>After testing I update the REST API and now It is returning 125000 rows. I tested that API in REST client and its working fine. But in <strong>Azure Data Factory's Copy Activity</strong> It is giving following error while copying data to Azure Data Lake Store. </p>

<pre><code>{
    ""errorCode"": ""2200"",
    ""message"": ""Failure happened on 'Sink' side. ErrorCode=UserErrorFailedToReadHttpFile,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Failed to read data from http source file.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Net.WebException,Message=The remote server returned an error: (500) Internal Server Error.,Source=System,'"",
    ""failureType"": ""UserError"",
    ""target"": ""CopyDataFromGraphAPI""
}
</code></pre>

<p>The sink side is Azure Data Lake Store. Is there any limit of content size which I am copying from REST call to Azure Data Lake Store. </p>

<p>I also retested the pipeline by updating REST API call(2500 rows)  and It worked fine and when I updated API Call and It returns 125000 rows. My pipeline starts giving the same above mentioned error.</p>

<p>My source DataSet in Copy Activity is </p>

<pre><code>{
    ""name"": ""MembersHttpFile"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""WM_GBS_LinikedService"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""HttpFile"",
        ""structure"": [
            {
                ""name"": ""id"",
                ""type"": ""String""
            },
            {
                ""name"": ""name"",
                ""type"": ""String""
            },
            {
                ""name"": ""email"",
                ""type"": ""String""
            },
            {
                ""name"": ""administrator"",
                ""type"": ""Boolean""
            }
        ],
        ""typeProperties"": {
            ""format"": {
                ""type"": ""JsonFormat"",
                ""filePattern"": ""arrayOfObjects"",
                ""jsonPathDefinition"": {
                    ""id"": ""$.['id']"",
                    ""name"": ""$.['name']"",
                    ""email"": ""$.['email']"",
                    ""administrator"": ""$.['administrator']""
                }
            },
            ""relativeUrl"": ""api/workplace/members"",
            ""requestMethod"": ""Get""
        }
    }
}
</code></pre>

<p><strong>Sink Data Set is</strong> </p>

<pre><code>{
    ""name"": ""MembersDataLakeSink"",
    ""properties"": {
        ""linkedServiceName"": {
            ""referenceName"": ""DataLakeLinkService"",
            ""type"": ""LinkedServiceReference""
        },
        ""type"": ""AzureDataLakeStoreFile"",
        ""structure"": [
            {
                ""name"": ""id"",
                ""type"": ""String""
            },
            {
                ""name"": ""name"",
                ""type"": ""String""
            },
            {
                ""name"": ""email"",
                ""type"": ""String""
            },
            {
                ""name"": ""administrator"",
                ""type"": ""Boolean""
            }
        ],
        ""typeProperties"": {
            ""format"": {
                ""type"": ""JsonFormat"",
                ""filePattern"": ""arrayOfObjects"",
                ""jsonPathDefinition"": {
                    ""id"": ""$.['id']"",
                    ""name"": ""$.['name']"",
                    ""email"": ""$.['email']"",
                    ""administrator"": ""$.['administrator']""
                }
            },
            ""fileName"": ""WorkplaceMembers.json"",
            ""folderPath"": ""rawSources""
        }
    }
}
</code></pre>
","<azure-data-lake><u-sql><azure-data-factory>","2018-04-04 16:27:28","4186","0","1","49658293","<p>As far as I know, there is no limit to file size. I've had a 10 gb csv with millions of rows and the data lake doesn't care. </p>

<p>What I can see is that while the error says ""sink"" side, the error code is UserErrorFailedToReadHttpFile so I think the issue may be solved if you change the httpRequestTimeout on your source, as of now it is ""00:30:40"" and maybe the row transferring is being interrupted because of it. 30 minutes is a lot of time for 2500 rows, but maybe 125k doesn't fit there.</p>

<p>Hope this helped!</p>
"
"49645796","Multiple schedules in Azure Data Factory V1 Pipeline","<p>Is there any way to create two triggers/Schedules in ADF pipeline, one with Month Start Date and another with Every Monday in ADF v1.</p>
","<azure><azure-data-factory>","2018-04-04 08:05:45","492","0","1","49657461","<p>I explained how to ""trigger"" a pipeline in Data Factory v1 here: <a href=""https://stackoverflow.com/questions/49167400/azure-data-factory-v1/49195167#49195167"">Azure Data Factory V1</a></p>

<p>If this works for you, you can create a runbook in Azure Automation and schedule it to run whenever you need. This way it is much more flexible, but may not work if you use WindowStart and WindowEnd a lot.</p>

<p>Hope this helped!</p>
"
"49639932","Trouble using key phrase extractor and sentiment analysis with U-SQL through Azure data lake analytics on Azure data lake","<p>I am trying to use microsoft's cognitive services with data lake and have run into a problem while trying to get key phrases and sentiment from the text in a column of a CSV file.</p>

<p>I have checked to make sure that the file is formatted correctly and is being read correctly (I have done a few basics, like copying, to make sure it is workable).</p>

<p>I have also made sure that the column I am interested in the CSV file (Description) contains just text(string) when it is extracted by itself.</p>

<p>The input file and output folder are in my Azure data lake and I am running the script from my data lake analytics on Azure. I have <em>not</em> tried to run this locally in Visual Studio.</p>

<p>I used <a href=""https://msdn.microsoft.com/en-us/azure/data-lake-analytics/u-sql/key-phrases-extraction-u-sql"" rel=""nofollow noreferrer"">Key Phrases Extraction (U-SQL)</a> and <a href=""https://msdn.microsoft.com/en-us/azure/data-lake-analytics/u-sql/sentiment-analysis-u-sql"" rel=""nofollow noreferrer"">Sentiment Analysis (U-SQL)</a> as my reference and followed the directions there, including getting the plugins.</p>

<p>In each case when I submit the job I get an error that I cannot seem to find a way round. Below I have shown the code that I have used for each and the error that I get when running it.</p>

<p><strong>Key Phrase Code</strong></p>

<pre><code>REFERENCE ASSEMBLY [TextSentiment];
REFERENCE ASSEMBLY [TextKeyPhrase];

@myinput =
EXTRACT 
    Modified_On string,
    _Name string,
    Description string,
    Customer string,
    Category string,
    Target_Market string,
    Person_Responsible string,
    Status string,
    _Region string,
    Modified_On_2 string,
    Created_On string,
    _Site string,
    _Team string    
FROM ""/userData/fromSharepoint/Game_Plans""
USING Extractors.Csv(skipFirstNRows:1);

@keyphrase =
PROCESS @myinput
PRODUCE 
    Description,
    KeyPhrase string
READONLY
    Description
USING new Cognition.Text.KeyPhraseExtractor();

OUTPUT @keyphrase
    TO ""/userData/testingCognitive/tesing1.csv""
    USING Outputters.Csv();
</code></pre>

<p><strong>Key Phrase Error Message</strong></p>

<p><a href=""https://i.stack.imgur.com/BgLvz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BgLvz.png"" alt=""enter image description here""></a></p>

<p><strong>Sentiment Code</strong></p>

<pre><code>REFERENCE ASSEMBLY [TextSentiment];
REFERENCE ASSEMBLY [TextKeyPhrase];

@myinput =
EXTRACT 
    Modified_On string,
    _Name string,
    Description string,
    Customer string,
    Category string,
    Target_Market string,
    Person_Responsible string,
    Status string,
    _Region string,
    Modified_On_2 string,
    Created_On string,
    _Site string,
    _Team string    
FROM ""/userData/fromSharepoint/Game_Plans""
USING Extractors.Csv(skipFirstNRows:1);

@sentiment =
PROCESS @myinput
PRODUCE 
    Description,
    sentiment string,
    conf double
READONLY
    Description
USING new Cognition.Text.SentimentAnalyzer(true);

OUTPUT @sentiment
    TO ""/userData/testingCognitive/tesing1.csv""
    USING Outputters.Csv();
</code></pre>

<p><strong>Sentiment Error Message</strong></p>

<p><a href=""https://i.stack.imgur.com/GqP3R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GqP3R.png"" alt=""enter image description here""></a></p>

<p>Any assistance on how to solve this would be much appreciated.</p>

<p>Alternatively if anyone has got these functions working and can provide some scripts to test with and links to input files to download that would be awesome.</p>
","<bigdata><azure-cognitive-services><azure-data-lake><u-sql><azure-data-factory>","2018-04-03 22:18:03","191","2","1","49640564","<p>I can't reproduce your exact error (can you post some simple sample data?) but I can get these libraries to work.  I think the KeyPhraseExtractor by default expects columns called <code>Text</code> and <code>KeyPhrase</code> so if you are going to change them then you have to pass your column names in as arguments, eg</p>

<pre><code>@keyphrase =
    PROCESS @myinput
    PRODUCE Description,
            KeyPhrase string
    READONLY Description
    USING new Cognition.Text.KeyPhraseExtractor(""Description"", ""KeyPhrase"");
</code></pre>

<p>UPDATE:  There are some invalid characters in your sample file, just after the word ""Bass"".  This is a non-breaking space (U+00A0) and I don't <em>think</em> you'll be able to import them - happy to be corrected.  I removed these manually and was able to import the file.  You could pre-process them in some manner.</p>

<p><a href=""https://i.stack.imgur.com/pXgkl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pXgkl.png"" alt=""Invalid characters""></a></p>
"
"49637233","Change job names in ADLA when using pipelines","<p>I have jobs running in ADLA using U-SQL scripts that are scheduled with data factory pipelines. The job names appear in ADLA as some id for e.g""ADF-das783bb-9dsb-4123-9420-ddaklsdlm65c1"". Is there a way to rename these jobs so that I can differentiate between them?</p>
","<azure><azure-data-factory><azure-data-lake>","2018-04-03 18:53:17","45","0","1","49659901","<p>Well, if you are running these from azure data factory, these jobs should be displaying the azure data factory activity name, by default. </p>

<p>Something like:</p>

<p>act_adla_usql_AccountingData-2018-04-04T18:57:28Z</p>

<p>where [act_adla_usql_AccountingData] is the ADF activity name.</p>

<p>Are you running these via ADF? and are you naming these?</p>
"
"49621990","NULLS in File output are \N and I want them to be empty","<p>I have a datafactory that reads from a table and stores the output as a CSV to Blob Storage.</p>

<p>I have noticed that instead of leaving a NULL field blank it inserts the NULL character \N.. Now the external system that is ingesting this can't handle \N.</p>

<p>Is there anyway in my dataset where I can say leave nulls blank. </p>

<p>Below is my dataset properties:</p>

<pre><code>  ""typeProperties"": {
        ""fileName"": ""MasterFile-{fileDateNameVariable}.csv"",
        ""folderPath"": ""master-file-landing"",
        ""format"": {
            ""type"": ""TextFormat"",
            ""columnDelimiter"": "","",
            ""firstRowAsHeader"": true
        },
        ""partitionedBy"": [
            {
                ""name"": ""fileDateNameVariable"",
                ""value"": {
                    ""type"": ""DateTime"",
                    ""date"": ""SliceStart"",
                    ""format"": ""yyyyMMdd""
                }
            }
        ]
    },
</code></pre>

<p>Thanks in advance.</p>
","<azure><azure-data-factory>","2018-04-03 03:54:56","1482","3","1","49624402","<p>You could set the <code>Null value</code> to <code>""""</code> when you set your <code>dataset</code>. Please refer to my test.</p>

<p><strong><em>Table data:</em></strong></p>

<p><a href=""https://i.stack.imgur.com/cThEB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cThEB.png"" alt=""enter image description here""></a></p>

<p><strong><em>Output Dataset:</em></strong></p>

<p><a href=""https://i.stack.imgur.com/Gj37d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Gj37d.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/2T8Hz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2T8Hz.png"" alt=""enter image description here""></a></p>

<p><strong><em>Generate csv file:</em></strong></p>

<p><a href=""https://i.stack.imgur.com/4E7WF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4E7WF.png"" alt=""enter image description here""></a></p>

<p>Hope it helps you.</p>
"
"49617968","Fetch Paging Data using Azure Data Factory","<p>I created a Pipeline in Azure Data Factory V2. It will copy the data from Rest API and save this data in the form of JSON file in Azure Data Lake. Then I transform that JSON file using U-SQL and Copy that data into another folder in .csv format. My Pipeline. See the following Image of Pipeline. </p>

<p><a href=""https://i.stack.imgur.com/4ONHU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4ONHU.png"" alt=""enter image description here""></a></p>

<p>The Rest API Return Data into JSON Format lie.</p>

<pre><code>{
    ""data"": [
        {
            ""id"": ""100024937598765"",
            ""name"": ""Sebastian Martinelli"",
            ""email"": ""sebastian.martinelli@abc.com"",
            ""administrator"": false
        },
        {
            ""id"": ""100024909012916"",
            ""name"": ""Diego Juarez"",
            ""email"": ""diego.juarez@abc.com"",
            ""administrator"": false
        },
        {
            ""id"": ""100025002270557"",
            ""name"": ""Jose Lopez"",
            ""email"": ""jose.lopez@abc.com"",
            ""administrator"": false
        },
        {
            ""id"": ""100024553664067"",
            ""name"": ""Valentin Montemarani"",
            ""email"": ""valentin.montemarani@abc.com"",
            ""administrator"": false
        }
    ],
    ""paging"": {
        ""cursors"": {
            ""before"": ""QVFIUmU1QnBOYThYTnJiQlNqVzItMFdoTVprSHh3cWZA4LXF2ZAE5nSjIxVWZAOWUc1ZAjdLZAjN2em1SazRYVno4TGE4aFBaOFdMaS1NMDdkeEduVkRsOTVhN3Jn"",
            ""after"": ""QVFIUjhWdm5EOTk3amJaWHVYR3p1OEZAZAQ0ZAoeTR5TDBwblE0Mmx3dC1zRXhPM2VLZAWdqR0RWQndUVnhpTGc3RkIzVkNIY21EcXFTQU93NHVxRFcxVW12dTNB""
        },
        ""next"": ""https://graph.facebook.com/v2.12/1528385107457405/members?access_token=%2Cemail&amp;limit=25&amp;after=QVFIUjhWdm5EOTk3amJaWHVYR3p1OEZAZAQ0ZAoeTR5TDBwblE0Mmx3dC1zRXhPM2VLZAWdqR0RWQndUVnhpTGc3RkIzVkNIY21EcXFTQU93NHVxRFcxVW12dTNB""
    }
}
</code></pre>

<p>This API is not returning <strong>full data</strong> it returns data using <strong>next</strong> link by hitting that next link we can fetch <strong>next set of records</strong>. How can I fetch this type of data using <strong>Iteration</strong> in <strong>Data Factory V2</strong>? I tried other activities like <strong>For each, Until and If</strong> but unable to repeat that copy activity to fetch complete data. </p>

<p>The Base URL is like</p>

<p><a href=""https://graph.facebook.com/Community/groups?fields=privacy,name,purpose&amp;limit=5&amp;access_token=value"" rel=""nofollow noreferrer"">https://graph.facebook.com/Community/groups?fields=privacy,name,purpose&amp;limit=5&amp;access_token=value</a> </p>

<p>Now the next URL will be the same and have 1 more parameter after in it like </p>

<p><a href=""https://graph.facebook.com/v2.12/1528385107457405/groups?access_token=value&amp;pretty=1&amp;fields=privacy,name,purpose&amp;limit=5&amp;after=QVF"" rel=""nofollow noreferrer"">https://graph.facebook.com/v2.12/1528385107457405/groups?access_token=value&amp;pretty=1&amp;fields=privacy,name,purpose&amp;limit=5&amp;after=QVF</a></p>

<p><strong>Is there any way to do this?</strong></p>
","<azure-data-lake><u-sql><azure-data-factory>","2018-04-02 20:00:15","2921","2","1","59514571","<ol>
<li><p>Copy activity has a built-in support for <strong>Pagination</strong>. In a case of @Waqas Idrees the pagination rule should be set as:</p>

<ul>
<li>Name: <code>AbsoluteUrl</code></li>
<li>Value: <code>$.data.paging.next</code></li>
</ul>

<p>Such setting can be found on Source tab in a Copy activity:</p></li>
</ol>

<p><a href=""https://i.stack.imgur.com/5F6HC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5F6HC.png"" alt=""enter image description here""></a></p>

<ol start=""2"">
<li>There is no need to use U-SQL to transform hierarchical to a tabular format. Copy activity <strong>Mapping</strong> can do it on a fly</li>
</ol>

<p>Therefore answer to the question of OP:</p>

<blockquote>
  <p>How can I fetch this type of data using Iteration in Data Factory V2?</p>
</blockquote>

<p>No need to use Iterations. Consider to use a Copy activity functionality.</p>

<p><strong>P.S.</strong> Because exploration of such topic took quite considerable amount of my time and I was not able to find a similar guidance during my research, I've created a blog post about it and described the process step-by-step: <a href=""https://www.alexvolok.com/2019/adfv2-rest-api-part3-mapping-pagination/"" rel=""nofollow noreferrer"">Azure Data Factory and REST APIs - Mapping and Pagination</a></p>

<p><strong>P.P.S.</strong> Because of pace of development of ADF, I accept that in 2018 April built in pagination could not be part of the product yet. In this case it explains why the question was not answered that time. However, I hope that my findings will help someone who faced similar challenge these days.</p>
"
"49554806","Data Factory v2 - Generate a json file per row","<p>I'm using Data Factory v2. I have a copy activity that has an Azure SQL dataset as input and a Azure Storage Blob as output. I want to write each row in my SQL dataset as a separate blob, but I don't see how I can do this.</p>
<p>I see a copyBehavior in the copy activity, but that only works from a file based source.</p>
<p>Another possible setting is the filePattern in my dataset:</p>
<blockquote>
<p>Indicate the pattern of data stored in each JSON file. Allowed values
are: setOfObjects and arrayOfObjects.</p>
<p>setOfObjects - Each file contains single object, or line-delimited/concatenated multiple objects. When this option is chosen in an output dataset, copy activity produces a single JSON file with each object per line (line-delimited).</p>
<p>arrayOfObjects - Each file contains an array of objects.</p>
</blockquote>
<p>The description talks about &quot;each file&quot; so initially I thought it would be possible, but now I've tested them it seems that setOfObjects creates a line separated file, where each row is written to a new line. The setOfObjects setting creates a file with a json array and adds each line as a new element of the array.</p>
<p>I'm wondering if I'm missing a configuration somewhere, or is it just not possible?</p>
","<json><azure-data-factory>","2018-03-29 11:21:34","3896","1","1","49752944","<p>What I did for now is to load the rows in to a SQL table and run a foreach for each record in the table. The I use a Lookup activity to have an array to loop in a Foreach activity. The foreach activity writes each row to a blob store.</p>

<p>For Olga's documentDb question, it would look like this:
<a href=""https://i.stack.imgur.com/u82AM.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u82AM.jpg"" alt=""Pipeline""></a></p>

<p>In the lookup, you get a list of the documentid's you want to copy:
<a href=""https://i.stack.imgur.com/CILpd.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CILpd.jpg"" alt=""enter image description here""></a></p>

<p>You use that set in your foreach activity
<a href=""https://i.stack.imgur.com/fHkd5.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fHkd5.jpg"" alt=""enter image description here""></a></p>

<p>Then you copy the files using a copy activity within the foreach activity. You query a single document in your source:
<a href=""https://i.stack.imgur.com/pmqhW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pmqhW.jpg"" alt=""enter image description here""></a></p>

<p>And you can use the id to dynamically name your file in the sink. (you'll have to define the param in your dataset too):
<a href=""https://i.stack.imgur.com/Rd51q.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Rd51q.jpg"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/MxcXM.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MxcXM.jpg"" alt=""enter image description here""></a></p>
"
"49526319","Azure Data Factory failed on Build server The remote name could not be resolved: 'datafactories.schema.management.azure.com'","<p>I have Data Factory project in my solution. It compiled successfully on my machine. But when I triggered it from my tfs build server DataFactory throws an error at ParameterConfig.json</p>

<blockquote>
  <p>MsBuild\1.0\DataFactory.targets (35): Failed to download JSON schema
  <a href=""http://datafactories.schema.management.azure.com/internalschemas/2015-01-01-preview/Microsoft.DataFactory.Table.json"" rel=""nofollow noreferrer"">http://datafactories.schema.management.azure.com/internalschemas/2015-01-01-preview/Microsoft.DataFactory.Table.json</a>
  with exception message The remote name could not be resolved:
  'datafactories.schema.management.azure.com'</p>
</blockquote>

<p>I have added $schema in all the datasets, pipeline, and ParameterConfig files. This issue is <strong>intermittent</strong>. Someday it fails someday it works.</p>
","<azure><tfsbuild><azure-data-factory>","2018-03-28 04:49:58","1597","0","1","49751790","<p>After lot more brainstorming I come to know that its totally intermittent issue. 
I got related issues in Keyvault stuff also. Same error saying ""Remote name could not be resolved"". 
What I observed is the URL mentioned above ""<a href=""http://datafactories.schema.management.azure.com"" rel=""nofollow noreferrer"">http://datafactories.schema.management.azure.com</a>"" is not accessible from my build server account. So I used the trick and it works for me.
I have added ""<a href=""http://datafactories.schema.management.azure.com"" rel=""nofollow noreferrer"">http://datafactories.schema.management.azure.com</a>"" site as a trusted site on my build server Internet settings.
It works! I didn't get any issues till now. My build is succeeded all the time.</p>
"
"49509563","List files in a blob storage container using spark activity in Azure Data Factory V2","<p>I would like to know how to connect to and list what files are available in a blob storage container using an activity preferably pyspark in Azure Data Factory V2</p>
","<azure><pyspark><azure-blob-storage><azure-data-factory>","2018-03-27 09:46:04","3032","0","2","49580733","<p>There a few ways which could help you:</p>

<blockquote>
  <p>When you are using HDInsight Hadoop or Spark clusters in Azure, they are automatically pre-configured to access Azure Storage Blobs via the hadoop-azure module that implements the standard Hadoop FilesSystem interface. You can learn more about how HDInsight uses blob storage at <strong><a href=""https://azure.microsoft.com/en-us/documentation/articles/hdinsight-hadoop-use-blob-storage/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-us/documentation/articles/hdinsight-hadoop-use-blob-storage/</a></strong></p>
</blockquote>

<p>A detailed guide can be found in this Blog post:<a href=""https://blogs.msdn.microsoft.com/arsen/2016/07/13/accessing-azure-storage-blobs-from-spark-1-6-that-is-running-locally/"" rel=""nofollow noreferrer""><strong>https://blogs.msdn.microsoft.com/arsen/2016/07/13/accessing-azure-storage-blobs-from-spark-1-6-that-is-running-locally/</strong></a></p>

<p>Another source which shows the integration of Storage API usage with Spark can be found in this <strong>slide:<a href=""https://www.slideshare.net/BrajaDas/azure-blob-storage-api-for-scala-and-spark"" rel=""nofollow noreferrer"">https://www.slideshare.net/BrajaDas/azure-blob-storage-api-for-scala-and-spark</a></strong></p>
"
"49509563","List files in a blob storage container using spark activity in Azure Data Factory V2","<p>I would like to know how to connect to and list what files are available in a blob storage container using an activity preferably pyspark in Azure Data Factory V2</p>
","<azure><pyspark><azure-blob-storage><azure-data-factory>","2018-03-27 09:46:04","3032","0","2","49769498","<p>This python script allows access to the blobs via a pyspark script run using Azure Datafactory V2.  </p>

<p><a href=""https://github.com/Azure-Samples/storage-blobs-python-quickstart/blob/master/example.py"" rel=""nofollow noreferrer"">https://github.com/Azure-Samples/storage-blobs-python-quickstart/blob/master/example.py</a></p>

<p>However I had to use </p>

<pre><code>from azure.storage.blob import BlobService
</code></pre>

<p>instead of the suggested </p>

<pre><code>from azure.storage.blob import BlockBlobService
</code></pre>
"
"49502637","azure data factory dependencies","<p>I have two activities in my azure data factory.</p>

<p>Activity A1 = a stored proc on a sql db. Input=none, output = DB (output1). The stored proc targets the output dataset.</p>

<p>Activity A2 = an azure copy activity (""type"": ""Copy"") which copies from blob to same sql db. Input = blob, Output = DB (output2)</p>

<p>i need to run activity A1 before A2 and i cant for the world figure out what dependencies to put between them.</p>

<p>i tried to mark A2 as having two inputs - the blob + the DB (output1). if i do this, the copy activity doesn't throw error, but it does NOT copy the blob to db (i think it silently uses the DB as the source of copy, instead of blob as source of copy and somehow does nothing). </p>

<p>if i remove the DB input (output1) on A2 it can successfully copy blob to DB but i no longer have the dependency chain that A1 needs to run before A2</p>

<p>thanks!</p>
","<azure-data-factory>","2018-03-27 00:27:58","106","1","1","49502938","<p>I figured this out - I was able to keep two dependencies on A2, but just needed to make sure of the ordering of the 2 inputs. Weird. Looks like the Copy activity just acts on the FIRST input - so when i moved the blob as the first input it worked! :) (earlier i had the DB output1 as first input and it silently did nothing)</p>

<pre><code> ""activities"": [
        {
            ""type"": ""Copy"",
            ""typeProperties"": {
                ""source"": {
                    ""type"": ""BlobSource""
                },
                ""sink"": {
                    ""type"": ""SqlSink"",
                    ""writeBatchSize"": 0,
                    ""writeBatchTimeout"": ""00:00:00""
                }
            },
            ""inputs"": [
                {
                    ""name"": ""MyBlobInput""
                },
                {
                    ""name"": ""MyDBOutput1""
                }
            ],
            ""outputs"": [
                {
                    ""name"": ""MyDBOutput2""
                }
            ],
            ""policy"": {
                ""timeout"": ""01:00:00"",
                ""concurrency"": 3,
                ""retry"": 3
            },
            ""scheduler"": {
                ""frequency"": ""Day"",
                ""interval"": 1
            },
            ""name"": ""AzureBlobtoSQL"",
            ""description"": ""Copy Activity""
        }
    ],
</code></pre>
"
"49486379","How to force an empty output file with Azure Stream Analytics","<p>I have configured a Stream Analytics Jobs so that input data goes to an Azure Data Lake repository every hour.</p>

<p>Sometimes there is no event to track, so no output. But my Data Factory goes in error because the file doesn't exist.</p>

<p>I wonder if exist a way to force empty file out from Stream Analytics?</p>

<p>Many thanks!</p>
","<azure-data-factory><azure-data-lake><azure-stream-analytics>","2018-03-26 07:46:25","354","0","3","49500796","<p>You can look at our <a href=""https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns#query-example-fill-missing-values"" rel=""nofollow noreferrer"">common query patterns here</a>. In particular I think you can use the one named ""fill missing values"" to generate some events regularly, even when there is no input.
Let me know if it works for you.</p>

<p>Thanks!</p>

<p>JS</p>
"
"49486379","How to force an empty output file with Azure Stream Analytics","<p>I have configured a Stream Analytics Jobs so that input data goes to an Azure Data Lake repository every hour.</p>

<p>Sometimes there is no event to track, so no output. But my Data Factory goes in error because the file doesn't exist.</p>

<p>I wonder if exist a way to force empty file out from Stream Analytics?</p>

<p>Many thanks!</p>
","<azure-data-factory><azure-data-lake><azure-stream-analytics>","2018-03-26 07:46:25","354","0","3","49506066","<p>Are you using ADF v2?</p>

<p>I didn't find anything inbuilt in ADF to come up with it. 
But I can see few workarounds - starting from simplest one:</p>

<ol>
<li>In your ASA query, you can use <a href=""https://msdn.microsoft.com/en-us/azure/stream-analytics/reference/with-azure-stream-analytics"" rel=""nofollow noreferrer"">WITH</a> statement and union your input with a fake empty message. - Then there will be always output</li>
<li>As a second output in ASA job you can store in some DB info whenever a file was produced. Then in ADF you can check whenever there are files and run copy conditionally. </li>
<li>In ADF run web activity e.g. LogicApp/FunctionApp to get info whenever files in container exist. </li>
</ol>
"
"49486379","How to force an empty output file with Azure Stream Analytics","<p>I have configured a Stream Analytics Jobs so that input data goes to an Azure Data Lake repository every hour.</p>

<p>Sometimes there is no event to track, so no output. But my Data Factory goes in error because the file doesn't exist.</p>

<p>I wonder if exist a way to force empty file out from Stream Analytics?</p>

<p>Many thanks!</p>
","<azure-data-factory><azure-data-lake><azure-stream-analytics>","2018-03-26 07:46:25","354","0","3","50208467","<p>Find the way to do it... </p>

<p>I had an activity using the data lake analytics, what I do is to run an U-SQL than read data with no transformation and write it to the output with headers. </p>

<p>In that way the activity always write an output file!</p>

<p>Very easy!</p>
"
"49484646","Custom date in azure blob folder path","<p>I have looked at some posts and documentation on how to specify custom folder paths while creating an azure blob (using the azure data factories).</p>

<p>Official documentation: </p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-azure-blob-connector#using-partitionedBy-property"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-azure-blob-connector#using-partitionedBy-property</a></p>

<p>Forums posts: </p>

<p><a href=""https://dba.stackexchange.com/questions/180487/datafactory-tutorial-blob-does-not-exist"">https://dba.stackexchange.com/questions/180487/datafactory-tutorial-blob-does-not-exist</a></p>

<p>I am successfully able to put into date indexed folders, however what I am not able to do is put into incremented/decremented date folders.</p>

<p>I tried using $$Text.Format (like below) but it gives a compile error --> Text.Format is not a valid blob path .</p>

<pre><code>    ""folderPath"": ""$$Text.Format('MyRoot/{0:yyyy/MM/dd}/', Date.AddDays(SliceEnd,-2))"",
</code></pre>

<p>I tried using the PartitionedBy section (like below) but it too gives a compile error --> Only SliceStart and SliceEnd are valid options for ""date""</p>

<pre><code>    {
""name"": ""MyBlob"",
""properties"": {
    ""published"": false,
    ""type"": ""AzureBlob"",
    ""linkedServiceName"": ""MyLinkedService"",
    ""typeProperties"": {
        ""fileName"": ""MyTsv.tsv"",
        ""folderPath"": ""MyRoot/{Year}/{Month}/{Day}/"",
        ""format"": {
            ""type"": ""TextFormat"",
            ""rowDelimiter"": ""\n"",
            ""columnDelimiter"": ""\t"",
            ""nullValue"": """"
        },
        ""partitionedBy"": [
            {
                ""name"": ""Year"",
                ""value"": {
                    ""type"": ""DateTime"",
                    ""date"": ""Date.AddDays(SliceEnd,-2)"",
                    ""format"": ""yyyy""
                }
            },
            {
                ""name"": ""Month"",
                ""value"": {
                    ""type"": ""DateTime"",
                    ""date"": ""Date.AddDays(SliceEnd,-2)"",
                    ""format"": ""MM""
                }
            },
            {
                ""name"": ""Day"",
                ""value"": {
                    ""type"": ""DateTime"",
                    ""date"": ""Date.AddDays(SliceEnd,-2)"",
                    ""format"": ""dd""
                }
            }
        ]
    },
    ""availability"": {
        ""frequency"": ""Day"",
        ""interval"": 1
    },
    ""external"": false,
    ""policy"": {}
}
</code></pre>

<p>Any pointers are appreciated!</p>

<h1>EDIT for response from Adam:</h1>

<p>I also used folder structure directly in FileName as per suggestion from Adam as per below forum post:</p>

<p><a href=""https://stackoverflow.com/questions/2619007/windows-azure-how-to-create-sub-directory-in-a-blob-container"">Windows Azure: How to create sub directory in a blob container</a></p>

<p>I used it like in below sample. </p>

<pre><code>     ""typeProperties"": {
        ""fileName"": ""$$Text.Format('{0:yyyy/MM/dd}/MyBlob.tsv', Date.AddDays(SliceEnd,-2))"",
        ""folderPath"": ""MyRoot/"",
        ""format"": {
            ""type"": ""TextFormat"",
            ""rowDelimiter"": ""\n"",
            ""columnDelimiter"": ""\t"",
            ""nullValue"": """"
        },        
</code></pre>

<p>It gives no compile error and also no error during deployment. But it throws an error during execution!!      </p>

<p>Runtime Error is ---> Error in Activity: ScopeJobManager:PrepareScopeScript, Unsupported unstructured stream format '.adddays(sliceend,-2))', can't convert to unstructured stream.</p>

<p>I think the problem is that FileName can be used to create folders but not dynamic folder names, only static ones.                                                                                                                                                                                                                                          </p>
","<azure><blob><azure-data-factory><azure-blob-storage>","2018-03-26 05:41:48","1618","0","1","49497284","<p>you should create a blob using the following convention: ""foldername/myfile.txt"" , so you could also append additional blobs under that foldername. I'd recommend checking this thread: <a href=""https://stackoverflow.com/questions/2619007/windows-azure-how-to-create-sub-directory-in-a-blob-container"">Windows Azure: How to create sub directory in a blob container</a> , It may help you resolve this case.</p>
"
"49468317","Copy activity - sink stored procedure parameter value ignored","<p>After adding a <code>copy activity</code>, and specifying a stored procedure to use - I want to add a stored procedure parameter:</p>

<p><a href=""https://i.stack.imgur.com/7znNc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7znNc.png"" alt=""enter image description here""></a></p>

<p>Running the activity, I get an error saying that there is no value specified for the stored procedure parameter <code>projectId</code>.</p>

<p>Looking at the generated JSON code for the <code>copy activity</code>, I can indeed see that there is no value set for the parameter:</p>

<pre><code>""sink"": {
    ""type"": ""SqlSink"",
    ""writeBatchSize"": 10000,
    ""sqlWriterStoredProcedureName"": ""[dbo].[CreateAllocation]"",
    ""sqlWriterTableType"": ""AllocationType"",
    ""storedProcedureParameters"": {
        ""projectId"": {
            ""type"": ""String""
        }
    }
},
""enableStaging"": false,
""cloudDataMovementUnits"": 0
</code></pre>

<p>Setting the value of the parameter by clicking the <code>Advanced</code> tab on the <code>copy activity</code>, and pasting the following JSON makes it work:</p>

<pre><code>""sink"": {
    ""type"": ""SqlSink"",
    ""writeBatchSize"": 10000,
    ""sqlWriterStoredProcedureName"": ""[dbo].[CreateAllocation]"",
    ""sqlWriterTableType"": ""AllocationType"",
    ""storedProcedureParameters"": {
        ""projectId"": {
            ""type"": ""String"",
            ""value"": ""200""
        }
    }
},
""enableStaging"": false,
""cloudDataMovementUnits"": 0
</code></pre>

<p>I've also tried to connect the data factory to git, and making a commit that sets the value of the stored procedure parameter - but no luck. As soon as the copy activity has loaded, it seems like it strips out the value set for the parameter.</p>

<p>How come that I can't set the value of the stored procedure parameter, without using the <code>Advanced</code> tab to override the activity template?</p>
","<azure><azure-data-factory>","2018-03-24 18:26:17","316","1","1","50772573","<p>This issue has already been fixed. With the lasted UI, you won't hit this issue.</p>
"
"49456110","How to create a HDInsightOnDemand LinkedService with a script action in Data Factory?","<p>We are creating a DataFactory for running a pySpark job, that uses a HDInsight on demand cluster.</p>

<p>The problem is that we need to use additional python dependencies for running this job, such as numpy, that are not installed.</p>

<p>We believe that the way of doing so is configuring a Script Action for the HDInsightOnDemandLinkedService, but we cannot find this option on DataFactory or LikedServices.</p>

<p>Is there an alternative for automating the HDInsightOnDemand installation of the dependencies?</p>
","<azure><pyspark><pip><azure-data-factory><azure-hdinsight>","2018-03-23 18:20:30","97","1","1","50326026","<p>Currently the Script Actions for HDInsightOnDemandLinkedService are not supported. You can use Azure Automation to run a PowerShell script that does the following:</p>

<ol>
<li>create HDInsight cluster</li>
<li>execute Script Action</li>
<li>run pipeline in your DataFactory</li>
<li>delete the cluster.</li>
</ol>
"
"49446121","Iterate through json custom object","<p>I'm trying to perform some config transformations on JSON files using PowerShell.
For this there's several input json files and a transform one (one for each environment).</p>

<p>Input sample (<code>AzureStorage.json</code>):</p>

<pre class=""lang-json prettyprint-override""><code>{
""$schema"": ""http://datafactories.schema.management.azure.com/schemas/2015-09-01/Microsoft.DataFactory.LinkedService.json"",
""name"": ""AzureStorage"",
""properties"": {
""type"": ""AzureStorage"",
  ""typeProperties"": {
    ""connectionString"": ""My Connection String here""
  } } }
</code></pre>

<p>Transform:</p>

<pre class=""lang-json prettyprint-override""><code>{
  ""$schema"": ""http://datafactories.schema.management.azure.com/vsschemas/V1/Microsoft.DataFactory.Config.json"",
  ""AzureStorage"": [
    {
      ""name"": ""$.properties.typeProperties.connectionString"",
      ""value"": ""DefaultEndpointsProtocol=https;AccountName=mytestaccount;AccountKey=d;lasfjdalfdjfldjfdsfds;EndpointSuffix=core.windows.net""
    }
  ],
  ""DataLakeStore"": [
    {
      ""name"": ""$.properties.typeProperties.dataLakeStoreUri"",
      ""value"": ""https://mydatalake.azuredatalakestore.net/webhdfs/v1""
    }
  ]
}
</code></pre>

<p>What I need to do, is to load the transform file, then traverse it, finding the names of the input files I need to transform (in this example <code>AzureStorage.json</code> and <code>DataLakeStore.json</code>).</p>

<p>Next, I need to replace the properties accordingly.  I'm trying to do it by loading the transform file into a variable using <code>ConvertFrom-Json</code>, but I not sure how to traverse it afterwards.</p>
","<json><powershell><azure-data-factory>","2018-03-23 09:20:45","5598","3","1","49447921","<p>I don't know hat exactly you need.  I'm guessing access to the information within the JSON file.</p>
<p>What about this approach?</p>
<pre><code>$json_object = Get-Content -Raw -Path '&lt;your_path&gt;\transform.json' | ConvertFrom-Json
$azure_storage = @('AzureStorage'; 'DataLakeStore')

ForEach ($azure in $json_object) {
    ForEach ($storage in $azure_storage) {
        Write-Output $azure.$storage.name
        Write-Output $azure.$storage.value
    }
}
</code></pre>
<p><strong>Edit</strong> Due to edit I got it.  You need a generic access.</p>
<p>Here you go:</p>
<pre><code>$json_object = (Get-Content -Path '&lt;your_path&gt;\transform.json') -join &quot;`n&quot;  | ConvertFrom-Json

ForEach ($object in $json_object.PsObject.Properties) {
    Write-Output $object.name
    Write-Output $object.value
}
</code></pre>
<p>Explanation:
<code>(Get-Content -Path '&lt;your_path&gt;\transform.json') -join &quot;</code>n&quot;` is a <a href=""https://learn.microsoft.com/en-us/powershell/module/Microsoft.PowerShell.Utility/ConvertFrom-Json?view=powershell-5.1"" rel=""nofollow noreferrer"">Microsoft's MSDN</a> recommended way to read json files.</p>
<p>You need to find out where the values are.  The object you are using is a &quot;Windows PowerShell custom object&quot; - PsObject.  To access the you need to use <code>.Properties.value</code>.  Then you have the Strings you want and you can access them using <code>.name</code> and <code>.value</code>.</p>
"
"49431850","Execute a stored procedure in Azure Data Factory V2 if two pipelines are succesfull executed","<p><a href=""https://i.stack.imgur.com/4W8Yf.png"" rel=""nofollow noreferrer"">Azure Data Factory IF condition Image</a></p>

<p>What I'm trying to do is executing a stored produre, however it should only be executed if the two preceding ""Pipelines"" are successfully executed. <em>See Image URL above.</em></p>

<p>I'm struggling with the correct expression in the ""IF condition"".
I'm trying to accomplish something like:
<strong>IF</strong> <em>TriggerCopyAX</em> is <em>succesfull</em> AND <em>TriggerCopyNav</em> is <em>succesfull</em> Continue..</p>
","<azure-data-factory>","2018-03-22 14:56:34","279","2","2","49432090","<p>Execute pipeline is a type of activity, what you want to do is chain activities, not pipelines. You can configure activity dependency with the dependsOn property. See here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities#control-activity"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities#control-activity</a></p>

<p>Example:</p>

<pre><code>""dependsOn"": [ 
    { 
        ""activity"": ""YourActivityName"", 
        ""dependencyConditions"": [ ""Succeeded"" ] 
    }
]
</code></pre>

<p>Hope this helped!</p>
"
"49431850","Execute a stored procedure in Azure Data Factory V2 if two pipelines are succesfull executed","<p><a href=""https://i.stack.imgur.com/4W8Yf.png"" rel=""nofollow noreferrer"">Azure Data Factory IF condition Image</a></p>

<p>What I'm trying to do is executing a stored produre, however it should only be executed if the two preceding ""Pipelines"" are successfully executed. <em>See Image URL above.</em></p>

<p>I'm struggling with the correct expression in the ""IF condition"".
I'm trying to accomplish something like:
<strong>IF</strong> <em>TriggerCopyAX</em> is <em>succesfull</em> AND <em>TriggerCopyNav</em> is <em>succesfull</em> Continue..</p>
","<azure-data-factory>","2018-03-22 14:56:34","279","2","2","49434006","<p>Yes that works.
So basically what I did:
1. Did not use the ""If condition"".
2. I dragged the two ""execute pipelines""  directly to the stored procedure.
3. When I looked at the code behind the stored procedure element I saw the ""dependsOn"" property is automatically used.</p>

<pre><code>""dependsOn"": [
        {
            ""activity"": ""TriggerCopy_AX"",
            ""dependencyConditions"": [
                ""Succeeded""
            ]
        },
        {
            ""activity"": ""TriggerCopy_NAV"",
            ""dependencyConditions"": [
                ""Succeeded""
            ]
        }
    ],
</code></pre>
"
"49428893","How do i Re-run pipeline with only failed activities/Dataset in Azure Data Factory V2?","<p>I am running a pipeline where i am looping through all the tables in INFORMATION.SCHEMA.TABLES and copying it onto Azure Data lake store.My question is how do i run this pipeline for the failed tables only if any of the table fails to copy? </p>

<p><img src=""https://i.stack.imgur.com/D08iJ.png"" alt=""enter image description here""></p>
","<azure-data-factory>","2018-03-22 12:37:37","3837","0","1","49516909","<p>Best approach I’ve found is to code your process to:</p>

<pre><code>0. Yes, root cause the failure and identify if it is something wrong with the pipeline or if it is a “feature” of your dependency you have to code around.

1. Be idempotent.  If your process ensures a clean state as the very first step, similar to Command Design pattern’s undo (but more naive), then your process can re-execute. 

    * with #1, you can safely use “retry” in your pipeline activities, along with sufficient time between retries.
    * this is an ADFv1 or v2 compatible approach

2. If ADFv2, then you have more options and can have more complex logic to handle errors:

    * for the activity that is failing, wrap this in an until-success loop, and be sure to include a bound on execution.  
    * you can add more activities in the loop to handle failure and log, notify, or resolve known failure conditions due to externalities out of your control.

3. You can also use asynchronous communication to future process executions that save success to a central store.  Then later executions “if” I already was successful then stop processing before the activity.  

    * this is powerful for more generalized pipelines, since you can choose where to begin

4. Last resort I know (and I would love to learn new ways to handle) is manual re-execution of failed activities.  
</code></pre>

<p>Hope this helps,
J</p>
"
"49415895","U-SQL Activity is not running on Azure Data Factory","<p>I am using Azure Data Factory to transfer on-premises data to Azure Data Lake store. After copying the data I am running a U-SQL script on the uploaded file to convert it to new .csv file. My U-SQL job is running fine if run it from Visual Studio or Directly from Data Lake Analytics. </p>

<p>But If add and an activity in Azure Data Factory. After copying the data this U-SQL Script activity immediately fails. I tried many ways but unable to resolve the issues. It gives me the following error.</p>

<p><a href=""https://i.stack.imgur.com/BfCiB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BfCiB.png"" alt=""enter image description here""></a></p>

<p>JSON Definition of my U-SQL Activity is  </p>

<pre><code>{
    ""name"": ""Transform Data"",
    ""description"": ""This will transform work space data."",
    ""type"": ""DataLakeAnalyticsU-SQL"",
    ""dependsOn"": [
        {
            ""activity"": ""Copy_workplace_groups_info_2018_03_19_09_32_csv"",
            ""dependencyConditions"": [
                ""Completed""
            ]
        }
    ],
    ""policy"": {
        ""timeout"": ""7.00:00:00"",
        ""retry"": 0,
        ""retryIntervalInSeconds"": 30,
        ""secureOutput"": false
    },
    ""typeProperties"": {
        ""scriptPath"": ""Scripts/Script.usql"",
        ""scriptLinkedService"": {
            ""referenceName"": ""Destination_DataLakeStore_lc0"",
            ""type"": ""LinkedServiceReference""
        }
    },
    ""linkedServiceName"": {
        ""referenceName"": ""AzureDataLakeAnalyticsForDF"",
        ""type"": ""LinkedServiceReference""
    }
}
</code></pre>

<p><strong>JSON of entire pipeline is</strong></p>

<pre><code>{
    ""name"": ""CopyPipeline_d26"",
    ""properties"": {
        ""activities"": [
            {
                ""name"": ""Copy_workplace_groups_info_2018_03_19_09_32_csv"",
                ""type"": ""Copy"",
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false
                },
                ""typeProperties"": {
                    ""source"": {
                        ""type"": ""FileSystemSource"",
                        ""recursive"": false
                    },
                    ""sink"": {
                        ""type"": ""AzureDataLakeStoreSink"",
                        ""copyBehavior"": ""MergeFiles""
                    },
                    ""enableStaging"": false,
                    ""cloudDataMovementUnits"": 0,
                    ""enableSkipIncompatibleRow"": true
                },
                ""inputs"": [
                    {
                        ""referenceName"": ""workplace_groups_info_2018_03_19_09_32_csv_i_lc0"",
                        ""type"": ""DatasetReference""
                    }
                ],
                ""outputs"": [
                    {
                        ""referenceName"": ""workplace_groups_info_2018_03_19_09_32_csv_o_lc0"",
                        ""type"": ""DatasetReference""
                    }
                ]
            },
            {
                ""name"": ""Transform Data"",
                ""description"": ""This will transform work space data."",
                ""type"": ""DataLakeAnalyticsU-SQL"",
                ""dependsOn"": [
                    {
                        ""activity"": ""Copy_workplace_groups_info_2018_03_19_09_32_csv"",
                        ""dependencyConditions"": [
                            ""Completed""
                        ]
                    }
                ],
                ""policy"": {
                    ""timeout"": ""7.00:00:00"",
                    ""retry"": 0,
                    ""retryIntervalInSeconds"": 30,
                    ""secureOutput"": false
                },
                ""typeProperties"": {
                    ""scriptPath"": ""Scripts/Script.usql"",
                    ""scriptLinkedService"": {
                        ""referenceName"": ""Destination_DataLakeStore_lc0"",
                        ""type"": ""LinkedServiceReference""
                    }
                },
                ""linkedServiceName"": {
                    ""referenceName"": ""AzureDataLakeAnalyticsForDF"",
                    ""type"": ""LinkedServiceReference""
                }
            }
        ],
        ""parameters"": {
            ""windowStart"": {
                ""type"": ""String""
            },
            ""windowEnd"": {
                ""type"": ""String""
            }
        }
    }
}
</code></pre>
","<azure-data-lake><u-sql><azure-data-factory>","2018-03-21 20:25:20","535","1","1","49617734","<p>I resolved the issue by creating a runtime using AppService. I followed the following steps.</p>

<ol>
<li>I created a WebApp in Active Directory.</li>
<li>Assign Azure Data Lake Permission to that WebApp too.</li>
<li>Create a public Key in that App and note it. It will never show again.</li>
<li>Note Application Id of that WebApp.</li>
<li>Open the Azure Data Lake Analytics and assign contributor role to created WebApp in Active Directory.</li>
<li>Use Application Id as <em>Service Principal Id</em> and Public Key as <em>Service Principal Key</em> of the WebApp while creating run time. </li>
</ol>

<p>It works fine. :)</p>
"
"49409644","Azure Data Facory, encoding issues","<p>I'm currently working on a project on Azure data Factory and i have some issues with, I think, encoding.
I did'nt managed to find anything on the documentation, you have maybe the answer?</p>

<p>I'm trying to make a flow of data between SQL Server and Oracle.
The fields in my source table on SQL Server are nvarchar(255).</p>

<p>In order to make the flow working, i have to configure the target field in Varchar2(255). If i try to put nvarchar(255) the flow fall with error :</p>

<pre><code>ERROR [22001] [Microsoft][ODBC Oracle Wire Protocol driver]String data&amp;#44; right truncated.
</code></pre>

<p>Otherwise, with Varchar2(255) it work fine.</p>

<p>But! My issues come with spécial characters:</p>

<p>if in my data source, there is an accent like ('é','è','à'), the flow fall with error:</p>

<pre><code>ERROR [22001] [Microsoft][ODBC Oracle Wire Protocol driver]String data&amp;#44; right truncated.
</code></pre>

<p>I try to make the target field longer, to add a substring to my selection request but it doesn't work. I can't transfer the spécial characters...</p>

<p>(btw i tried to do an ""insert into"" with the the values of my request directly in the oracle data base and it works fine).</p>

<p>The SQL Server is configure with SQL_Latin1_General_CP1_CI_AS
THe oracle database is configure in AL16UTF16  with version 11.2.0.4.0.</p>

<p>Did you have any idea or suggestions to solve that issue?
Have a great day!</p>
","<sql-server><oracle><azure><encoding><azure-data-factory>","2018-03-21 14:56:33","504","-1","1","51941190","<p>It's a known issue in Azure Data Factory Integration Runtime, and has been fixed in the latest version.  You may want to upgrade your Integration Runtime and have a retry.</p>
"
"49405267","How to get Pipeline RunID","<p>I am new to Azure and I need to find the Latest Activity RunID of a pipeline in Azure using c#. I don't want to invoke the pipeline instance, simply want to get the last activity RunID. When I tried it is possible to invoke the pipeline and get the ID. Can anyone help me in this?.</p>
","<c#><azure-data-factory>","2018-03-21 11:33:59","1489","2","1","49406413","<p>The way I found to do this is by querying the data factory instance asking for runs between dates, and then look for the pipeline name in the response list. In c# it would be something like this:</p>

<pre><code>        DateTime today = DateTime.Now;
        DateTime lastWeek = DateTime.Now.AddDays(-7);
        PipelineRunFilterParameters prfp = new PipelineRunFilterParameters(lastWeek, today);

        var x = client.PipelineRuns.QueryByFactory(""yourResourceGroupName"", ""DatafactoryInstanceName"", prfp);
        var enumerator = x.Value.GetEnumerator();
        PipelineRun pipeRun;
        string runId;
        string pipeName = ""theNameOfYourPipeline"";
        while (enumerator.MoveNext())
        {
            pipeRun = enumerator.Current;
            if(pipeRun.PipelineName == pipeName)
            {
                runId = pipeRun.RunId;
                break;
            }
        }
</code></pre>

<p>This code assumes your pipeline runs on a weekly basis, modify it to meet your needs.</p>

<p>Hope this helped!!</p>
"
"49397290","Build single dataset from multiple tables in Azure Data Factory","<p>I have an ask which requires me to use views in the query from the data store. The built-in copy utility doesn't seem to like my query with inner joins. I know the query works because I tested it in my local SSIS. How can I build a dataset that joins 2 or more tables in ADF? I tried going to the advanced tab and referencing SqlDataReader with my query, but I can't figure out how to see a preview of the results. Please assist.</p>
","<azure><azure-data-factory>","2018-03-21 02:46:23","1579","1","1","50772616","<p>Sorry for the late response. Hope you have figured out how to do this. 
The authoring UI has been updated and you don't need to use Advanced tab now. 
Go to the copy activity source tab, put your query in the query input box, and then press the Preview button, you will see the query results.<a href=""https://i.stack.imgur.com/JoWmW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JoWmW.png"" alt=""enter image description here""></a></p>
"
"49396051","ADF V2 - ADLS to SQL Server On prem, does column order matter?","<p>I've been trying to move data from a flat file in ADLS (Azure Data Lake Store) to a SQL Server On Prem, using purely ADLS and SQL Server Datasets, I thought that the TabularTranslator Property was going to help to map the columns and ignore the order of how they are in the flat file. doing something like this</p>

<pre><code>{
    ""source"": {
        ""type"": ""AzureDataLakeStoreSource""
    },
    ""sink"": {
        ""type"": ""SqlSink"",
        ""writeBatchSize"": 10000
    },
    ""enableStaging"": false,
    ""cloudDataMovementUnits"": 0,
    ""translator"": {
        ""type"": ""TabularTranslator"",
        ""columnMappings"": ""ColA: ColA, ColB:ColB""
    }
}
</code></pre>

<p>However it looks like the order matters, and TabularTranslator might only work for SQL Server and Azure SQL databases, is there anyway to map these columns without having to change the column order of my target?</p>

<p>The error I'm getting is a truncation error because it is trying to insert ColB data into ColA in my destination</p>

<p><strong>More details</strong>
In other words, I have the following source and target...</p>

<p><strong>Source</strong>: File.tsv</p>

<p>ColA ColB</p>

<p>""codeA""    1</p>

<p>""codeB""    2</p>

<p>""codeC""    3</p>

<p><strong>Target</strong>: SQL Server table</p>

<pre><code>|---------------------|------------------|
|      ColB           |     ColB         |
|---------------------|------------------|
|                     |                  |
|---------------------|------------------|
</code></pre>

<p>Looks like the data coming from ColA tries to get inserted into ColB and then I got the following:</p>

<p><strong><em>""ErrorCode=UserErrorInvalidDataValue,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Column &apos;ColB&apos; contains an invalid value 'codeA';. Cannot convert 'codeA'; to type 'Double';.,Source=Microsoft.DataTransfer.Common,''Type=System.FormatException,Message=Input string was not in a correct format.,Source=mscorlib,'""</em></strong></p>
","<azure-data-factory>","2018-03-21 00:00:39","482","0","1","49406569","<p>Using a tabular translator is not needed, I've only used it when moving data from ADLS to a warehouse using Polybase, but I think this is not supported for on-premises databases.</p>

<p>Take a look at the documentation when using a data lake as a source: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-store#copy-activity-properties"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-store#copy-activity-properties</a></p>

<p>It should work completely fine without a tabular translator in your case.</p>

<p>Hope this helped!!</p>
"
"49390651","Key vault for data factory V1?","<p>I would like to know if there is a way to use Key Vault for Data Factory V1 ? I don't seem to find an answer on the web . </p>

<p>thanks , Adam .</p>
","<azure><azure-data-factory><azure-keyvault>","2018-03-20 17:29:55","177","0","1","49397654","<p>Ability to store Linked Service credentials is only available in ADF V2:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault</a></p>

<p>We don't have plans to add it to V1.</p>
"
"49360676","Data Slices in ADF do not execute sequentially in time","<p>I have multiple activities in a pipeline and they have dependencies among themselves. When I set my pipeline start date in the past, my data slices set do not execute sequentially in time. To elaborate:</p>

<p>Let's say I have two activities A1 &amp; A2. A2 has a dependency on A1 and takes much longer to execute than A1. Let's say I upload my pipeline today with a start date of 3 days back in time (d-3). I'd like my activities to follow the sequence as:</p>

<p>A1(d-3) -> A2(d-3) -> A1(d-2) -> A2(d-2) -> A1(d-1) -> A2(d-1)  -></p>

<p>But in practice I see that all data slices of A1 have been executed whereas A2(d-3) is still in progress. 
How to enforce that all data slices for a particular date are completed before slices of the next day are taken up?</p>
","<azure><azure-data-factory>","2018-03-19 10:28:23","46","0","1","49362771","<p>I don't think there is a way to accomplish this, because every data slice is independent on others and should not depend on previous executions. If you make you query correctly, the execution order should not matter.</p>
"
"49324935","How to read a setting with an expression from file in data factory v2?","<p>I'm reading a setting from a json file with an expression, but the expression doesn't work.</p>

<p>The setting is outputPath -> @activity('GetSet').output.value[0].subs.outputPath</p>

<p>The file has the expression:</p>

<p>""outputPath"": ""/subs/@{formatDateTime(utcnow(), 'yyyy')}/subs.json""</p>

<p>The result should be /subs/2018/subs.json but appear the same has written in the file. If I put the expression in the Settings directly, it works.</p>

<p>json of my pipeline </p>

<pre><code>{
    ""name"": ""subscription experience"",
    ""type"": ""ExecutePipeline"",
    ""dependsOn"": [
        {
            ""activity"": ""GetSet"",
            ""dependencyConditions"": [
                ""Succeeded""
            ]
        }
    ],
    ""typeProperties"": {
        ""pipeline"": {
            ""referenceName"": ""Client"",
            ""type"": ""PipelineReference""
        },
        ""waitOnCompletion"": true,
        ""parameters"": {
            ""outputPath"": ""@activity('GetSet').output.value[0].subs.outputPath"",
        }
    }
}
</code></pre>

<p>my file is:</p>

<pre><code>{
  ""test"": ""my teste""
  ""subs"": {
    ""outputPath"": ""/subs/@{formatDateTime(utcnow(), 'yyyy')}/subscriptions.json"",
}
</code></pre>
","<azure><azure-data-factory>","2018-03-16 15:57:34","292","0","2","49363024","<p>Remember that the expressions must be between @{}. Try with </p>

<pre><code>@{activity('GetSet').output.value[0].subs.outputPath}
</code></pre>

<p>And remember that the activity 'GetSet' must have finished previously for this expression to have a value, so add a condition for it if you dont have it already.</p>

<p>Hope this helps!</p>
"
"49324935","How to read a setting with an expression from file in data factory v2?","<p>I'm reading a setting from a json file with an expression, but the expression doesn't work.</p>

<p>The setting is outputPath -> @activity('GetSet').output.value[0].subs.outputPath</p>

<p>The file has the expression:</p>

<p>""outputPath"": ""/subs/@{formatDateTime(utcnow(), 'yyyy')}/subs.json""</p>

<p>The result should be /subs/2018/subs.json but appear the same has written in the file. If I put the expression in the Settings directly, it works.</p>

<p>json of my pipeline </p>

<pre><code>{
    ""name"": ""subscription experience"",
    ""type"": ""ExecutePipeline"",
    ""dependsOn"": [
        {
            ""activity"": ""GetSet"",
            ""dependencyConditions"": [
                ""Succeeded""
            ]
        }
    ],
    ""typeProperties"": {
        ""pipeline"": {
            ""referenceName"": ""Client"",
            ""type"": ""PipelineReference""
        },
        ""waitOnCompletion"": true,
        ""parameters"": {
            ""outputPath"": ""@activity('GetSet').output.value[0].subs.outputPath"",
        }
    }
}
</code></pre>

<p>my file is:</p>

<pre><code>{
  ""test"": ""my teste""
  ""subs"": {
    ""outputPath"": ""/subs/@{formatDateTime(utcnow(), 'yyyy')}/subscriptions.json"",
}
</code></pre>
","<azure><azure-data-factory>","2018-03-16 15:57:34","292","0","2","49535744","<p>We have to use concat like this:</p>

<p>File is:</p>

<pre><code>{
 ""test"": ""my teste""
""subs"": {
""outputPath"": ""/subs/"",
""outputPath2"": subscriptions.json"",
}
</code></pre>

<p>And in parameter put:</p>

<pre><code>@concat(activity('GetSettings').output.value[0].subs.outputPath,formatDateTime(utcnow(), 'yyyy'),activity('GetSettings').output.value[0].subs.outputPath2)
</code></pre>
"
"49309191","Is there any need of Data Warehouse when using Azure Data Lake?","<p>I am exploring Azure Data Lake and I am new to this field. I explored many things and read many articles. Basically I have to develop <strong>Power BI dashboard</strong> from data of different sources. </p>

<p>In classic SQL Server stack I can write an ETL (Extract, Transform, Load) process to bring the data from my system databases into the Data Warehouse database. Then use that Data Warehouse with Power BI by using SSAS etc. </p>

<p>But I want to use Azure Data Lake and I explored Azure Data Lake Store and Azure Data Lake Analytic(U-SQL). I draw following architecture diagram.  </p>

<p><a href=""https://i.stack.imgur.com/ujYnV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ujYnV.png"" alt=""enter image description here""></a></p>

<ol>
<li>Is there any thing which I am missing in current flow of the
application? </li>
<li>I can get data directly from Azure Data Lake using
Power BI so there is no need of Data Warehouse. Am I right?</li>
<li>I can create a database in Azure Data Lake is that will be my Data Warehouse?</li>
<li>What will be the best format for the Output data from Original file in Azure Data Lake e.g .csv?</li>
</ol>
","<azure><data-warehouse><azure-data-factory><azure-data-lake>","2018-03-15 21:06:02","1666","3","2","49310773","<p><strong>1 &amp; 2)</strong> Currently ADLS only has limited support for allowing PowerBI to query directly over it.  If your data is too large (greater than about 10GB I believe), then PowerBI cannot work directly over data in your ADLS account.  In this case, I would recommend either moving your processed data in ADLS to a SQL Database or SQL Data Warehouse, as this allows for PowerBI to operate over larger amounts of data. You can use Azure Data Factory to move your data, or Polybase if moving data into SQL DW.   </p>

<p><strong>3)</strong> A data lake is still distinct from a data warehouse, and they have separate strengths and weaknesses. The data lake is best for storing your raw or slightly processed data, which may have a variety of formats and schemas. After you process and filter this data using Azure Data Lake Analytics, you can move that data into SQL DW for interactive analytics and data management (but at the cost of inflexibility of schema). </p>

<p><strong>4)</strong> Depends on your use case.  If you plan on continuing to process the data in ADLS, I recommend you output into an ADLS table for greater performance. However, if you need to pass this data into another service, then CSV is a good choice. You can find more outputters on our <a href=""https://github.com/Azure/usql/tree/master/Examples/DataFormats"" rel=""nofollow noreferrer"">GitHub</a> such as JSON and XML.</p>
"
"49309191","Is there any need of Data Warehouse when using Azure Data Lake?","<p>I am exploring Azure Data Lake and I am new to this field. I explored many things and read many articles. Basically I have to develop <strong>Power BI dashboard</strong> from data of different sources. </p>

<p>In classic SQL Server stack I can write an ETL (Extract, Transform, Load) process to bring the data from my system databases into the Data Warehouse database. Then use that Data Warehouse with Power BI by using SSAS etc. </p>

<p>But I want to use Azure Data Lake and I explored Azure Data Lake Store and Azure Data Lake Analytic(U-SQL). I draw following architecture diagram.  </p>

<p><a href=""https://i.stack.imgur.com/ujYnV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ujYnV.png"" alt=""enter image description here""></a></p>

<ol>
<li>Is there any thing which I am missing in current flow of the
application? </li>
<li>I can get data directly from Azure Data Lake using
Power BI so there is no need of Data Warehouse. Am I right?</li>
<li>I can create a database in Azure Data Lake is that will be my Data Warehouse?</li>
<li>What will be the best format for the Output data from Original file in Azure Data Lake e.g .csv?</li>
</ol>
","<azure><data-warehouse><azure-data-factory><azure-data-lake>","2018-03-15 21:06:02","1666","3","2","52248035","<p>This answer may not be timely, but what I've tried that is more similar to your prior experience is spin up an instance of Azure Analysis Service. You can create a tabular model or mdx model, shove a ton of data into memory and connect to it from power bi. The ""only"" catch is that it can get pricey quick. One great thing about AAS is that the interface to build a tabular model nearly follows power query and uses dax.</p>

<p>Also I believe these days adla store is basically gone in favor of using blob storage directly, so basically you'd go data --> blob --> dla --> aas --> pbi.</p>
"
"49302973","passing dynamic parameter to USQL script using data factory v2 USQL activity","<h2>USQL</h2>

<p>I have multiple stored procedures defined in USQL database with the following signatures. </p>

<ul>
<li>MyDatabase.dbo.DoThing01(@ticketNumber int)</li>
<li>MyDatabase.dbo.DoThing02(@ticketNumber int)</li>
<li>MyDatabase.dbo.DoThing03(@ticketNumber int)</li>
</ul>

<p>These stored procedures work when I call them from the azure analytics job UI in the portal manually.</p>

<blockquote>
  <p>I have data factory v2 pipeline setup, with an USQL activity that calls a script file located on azure datalake store which tries to pass a parameter value as @ticketNumber</p>
</blockquote>

<h1>What I have tried</h1>

<h3>Datafactory Activity</h3>

<pre><code>                      ""typeProperties"": {
                            ""scriptPath"": ""Script.txt"",
                            ""degreeOfParallelism"": 10,
                            ""scriptLinkedService"": {
                                ""referenceName"": ""CampuslabsDatalakeStore"",
                                ""type"": ""LinkedServiceReference""
                            },
                            ""parameters"": {
                                ""ticketNumber"": ""@item().InstitutionId""
                            }
                        }                           
</code></pre>

<h3>Scenario 1 : USQL Script</h3>

<pre><code>DECLARE @ticketNumber int = -1;

MyDatabase.dbo.DoThing01(@ticketNumber);
MyDatabase.dbo.DoThing02(@ticketNumber);
MyDatabase.dbo.DoThing03(@ticketNumber);
</code></pre>

<p>ERROR : the script gets executed with default value of -1.</p>

<h3>Scenario 2 : USQL Script</h3>

<pre><code>DECLARE @ticketNumber int;

MyDatabase.dbo.DoThing01(@ticketNumber);
MyDatabase.dbo.DoThing02(@ticketNumber);
MyDatabase.dbo.DoThing03(@ticketNumber);
</code></pre>

<p>ERROR : I get a compilation error that declaration of variable is wrong.</p>

<h3>Scenario 3 : USQL Script</h3>

<pre><code>MyDatabase.dbo.DoThing01(@ticketNumber);
MyDatabase.dbo.DoThing02(@ticketNumber);
MyDatabase.dbo.DoThing03(@ticketNumber);
</code></pre>

<p>ERROR : I get a compilation error that variable is not defined.</p>

<blockquote>
  <p>My question is : How do you pass a dynamic value parameter to the USQL script from a v2 USQL activity</p>
</blockquote>
","<azure-data-factory><u-sql>","2018-03-15 15:08:08","1528","2","1","49310999","<p>When passing in parameters from ADF to a U-SQL script, you should declare a default value for the parameter in your script using <a href=""https://msdn.microsoft.com/en-us/azure/data-lake-analytics/u-sql/declare-variables-u-sql?f=255&amp;MSPPError=-2147217396"" rel=""nofollow noreferrer"">DECLARE EXTERNAL</a>.  ADF v2 passes in parameters to U-SQL by writing in DECLARE statements at the very top of your script.  However, multiple DECLARE statements for the same variable cause a compilation error - but having a DECLARE EXTERNAL and DECLARE statement won't cause an error, because DECLARE EXTERNAL's use case is to be overwritten by the DECLARE statement above it (for this very use case).  </p>

<p>U-SQL requires that a variable has a value when it is defined, which is your Scenario 2 error.  </p>

<p>Because Scenario 1 runs with the value declared in the script and Scenario 3 doesn't compile means your variables aren't getting passed in at all, even though your JSON script looks correct. My hunch is the error is in your dynamic parameter definition. Where are you passing the InstitutionId from? If it's a parameter defined for the whole pipeline, the value should be ""@Pipeline().parameters.InstitutionId"". It would be useful to see your whole ADF pipeline if possible (or at least where you're getting the item().InstituionId from).  </p>
"
"49300291","How to migrate data from on premise database to Azure hosted database using ADF?","<p>Question looks fairly simple but in my case it has some intricacies involved in it. So here is the deal: </p>

<ul>
<li>There is one sql database hosted on-premise and one sql Azure hosted database. We need to keep both databases in sync. These databases contain  50 tables.</li>
<li><p>Azure database will not be updated by any application but on premise database will keep on updating frequently. So we need modified/inserted data from on premise database to be moved to Azure database. We are using <em>Azure Data Factory (ADF)</em> for this.</p></li>
<li><p>All database tables contain one column called <em>LastModifiedDate</em> indicating when was record modified.</p></li>
<li>Currently we have created staging tables corresponding to all 50 tables. We are maintaining one watermark table which contains table name and it's highest <em>LastModifiedDate</em>. </li>
<li>There is one activity in ADF job which executes stored procedure which takes records from all tables having <em>LastModifiedDate</em> > corresponding <em>LastModifiedDate</em> in watermark table and dumps them into staging table.</li>
<li>When execution of this stored procedure completes, all data from staging table is synced with Azure database tables. Finally watermark table is updated 
<em>LastModifiedDate</em> for each table. All staging tables  are then flushed.</li>
<li>This process will keep on repeating periodically, so that whenever data from on-premise is updated, Azure database will also update.</li>
</ul>

<p><strong>Problems with current approach:</strong></p>

<p>Creating staging table correspoding to each table doesn't look like a good idea. If number of tables in database increases, we need those many corresponding staging table. </p>

<p><strong>Question:</strong> </p>

<p>Is there any better approach to handle this scenario <strong>with using ADF</strong> and without creating huge number of staging tables?</p>
","<sql-server><azure><azure-sql-database><azure-data-factory>","2018-03-15 13:07:49","288","0","1","49300739","<p>You can try using <a href=""https://learn.microsoft.com/en-us/azure/sql-database/sql-database-get-started-sql-data-sync"" rel=""nofollow noreferrer"">SQL Data Sync</a> instead and make SQL Data Sync just to sync in one direction, from on-premise to Azure SQL Database. When configuring SQL Data Sync choose ""To the hub"" on ""Sync Directions"" as shown on below image.</p>

<p><a href=""https://i.stack.imgur.com/wYmwb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wYmwb.png"" alt=""enter image description here""></a></p>
"
"49298084","Best way to transform data values in Azure Data Factory","<p>What is the best way to replace specific values using a Azure Data factory?<br>
The case, for example: Need to lead to a single value brand ""ssang yong"" and model ""ceed"" for brand ""kia"". <br>
Data source:</p>

<pre><code>{
    id: 1
    brand: ""ssang yong"",
    model: ""rexton""
},
{
    id: 2
    brand: ""ssang_yong"",
    model: ""rexton""
},
{
    id: 3
    brand: ""ssangyong"",
    model: ""rexton""
},
{
    id: 4
    brand: ""kia"",
    model: ""ceed""
},
{
    id: 5
    brand: ""kia"",
    model: ""c'eed""
}
</code></pre>

<p><strong>Pattern:</strong></p>

<pre><code> { 
        target: ""brand"",
        common_value: ""ssang yong"",
        condition: {
            brand: [""ssang-yong"", ""ssangyong""]
        }
    },
    {
        target: ""model"",
        common_value: ""ceed"",
         condition: {
            brand: [""kia""],
            model: [""c'eed""]
        }
    }
</code></pre>
","<azure><azure-data-factory>","2018-03-15 11:18:43","760","0","1","49361190","<p>ADF is mostly used to move data from one place to another and to manage ELT process. 
So my use case in this scenario would be:
1) copy raw data with ADF to ADLS from sources
2) perform transformations with Azure data lake analytics and save output to the new file
3) import file into power bi (if you do not have analysis service to create tabular model)</p>
"
"49274338","Azure Data factory: How to read username and password in C# from Http Connector","<p>I am using Azure Data factory HTTP connector as a linked service to read data from the REST endpoint using basic authentication.
<code>{
    ""name"": ""LS_HTTP"",
    ""properties"": {
        ""hubName"": ""Hub name"",
        ""type"": ""Http"",
        ""typeProperties"": {
            ""url"": ""http://*****.azurewebsites.net"",
            ""authenticationType"": ""Basic"",
            ""gatewayName"": """",
            ""encryptedCredential"": """",
            ""username"": ""test"",
            ""password"": ""**********"",
            ""enableServerCertificateValidation"": true
        }
    }
}</code></p>

<p>Following code snippet is written to fetch the username and password from the headerin my web API</p>

<pre><code>string authHeader = context.Request.Headers[""Authorization""];
if (authHeader != null &amp;&amp; authHeader.StartsWith(""Basic""))
{
    //Extract credentials   
    string encodedUsernamePassword = authHeader.Substring(""Basic "".Length).Trim();  
    Encoding encoding = Encoding.GetEncoding(""iso-8859-1"");
    string usernamePassword = encoding.GetString(Convert.FromBase64String(encodedUsernamePassword));    
    int seperatorIndex = usernamePassword.IndexOf(':');
    var username = usernamePassword.Substring(0, seperatorIndex);
    var password = usernamePassword.Substring(seperatorIndex + 1);  

    if (username == ""test"" &amp;&amp; password == ""****"")
    {
        await _next.Invoke(context);
    }
    else
    {
        context.Response.StatusCode = 401; //Unauthorized
        return;
    }
}
else
{
    // no authorization header
    context.Response.StatusCode = 401; //Unauthorized
    return;
}
</code></pre>

<p>When I run Azure data factory pipeline with this setup, I am not able to get username and password from the request header in the web api, basically Authorization header itself is null. 
Help me to fetch the username and password passed from my ADF connector service in my web API.</p>
","<c#><rest><azure-data-factory><basic-authentication>","2018-03-14 09:44:39","1456","0","1","49385427","<p>Looking at your definition, you're working with Data Factory v1. I see you configure some properties that are not required for basic authentication.</p>
<p>encryptedCredential, is not required. <a href=""https://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-http-connector#linked-service-properties"" rel=""nofollow noreferrer"">The documentation states</a>:</p>
<blockquote>
<p>Description: Encrypted credential to access the HTTP endpoint. Auto-generated when you configure the authentication information in copy wizard or the ClickOnce popup dialog.</p>
<p>Required: No. Apply only when copying data from an on-premises HTTP server.</p>
</blockquote>
<p>gatewayName, is not required since you're not using an on-premises HTTP server</p>
<blockquote>
<p>Description: Name of the Data Management Gateway to connect to an on-premises HTTP source.</p>
</blockquote>
<p>enableServerCertificateValidation, already defaults to true</p>
<p>Documentation gives this basic example:</p>
<pre><code>{
&quot;name&quot;: &quot;HttpLinkedService&quot;,
&quot;properties&quot;:
{
    &quot;type&quot;: &quot;Http&quot;,
    &quot;typeProperties&quot;:
    {
        &quot;authenticationType&quot;: &quot;basic&quot;,
        &quot;url&quot; : &quot;https://en.wikipedia.org/wiki/&quot;,
        &quot;userName&quot;: &quot;user name&quot;,
        &quot;password&quot;: &quot;password&quot;
    }
}
}
</code></pre>
"
"49273165","Setting name of usql job invoked from data factory","<p>Is it possible to set name of USQL job when invoking it using Data factory ? </p>

<p>Currently when my USQL jobs are invoked using Data Factory, Data Lake Analytics job view shows just GUID as a name for those jobs. My goal is to easily spot what the job is about and GUID doesn't help unless I open the job and see its code.</p>
","<azure-data-factory><u-sql>","2018-03-14 08:47:22","107","1","1","49285851","<p>Are you trying to set the name of the job automatically based on certain parameters? And are you using ADF v2 or v1? The below answer is for v2:</p>

<p>Currently you can set the name of your U-SQL job in the JSON field of the U-SQL activity (see <a href=""https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-data-lake-analytics"" rel=""nofollow noreferrer"">here</a> under ""Data Lake Analytics U-SQL Activity""), or by clicking on the U-SQL activity and changing the Name field under the General tab (if you're using the Visual Tools). Changing the name in either of those fields should result in a job name in ADLA of ""-.  If you're not getting that even with changing the name, or you're trying to customize the name via parameters, we can debug further.</p>
"
"49240814","Azure Data Factory copy pipeline: get a file with date of yesterday that arrived today","<p>I am getting a txt file (on todays date) with the date of yesterday in it and I want dynamically get this filename in my data factory pipeline. </p>

<p>The file is placed automatically on a file system and I want to copy this file to the blob store In my example below I am simulating this by copying from blob to blob. </p>

<p><strong>For example:</strong>
filename_2018-02-11.txt arrived today (2018-03-12) with the date of yesterday(2018-02-11). How can I pick this file up on today's date? 
Yesterday's slice did run but there was not a file yet.</p>

<p>Here is my example:</p>

<pre><code> {""$schema"": ""http://datafactories.schema.management.azure.com/schemas/2015-09-01/Microsoft.DataFactory.Pipeline.json"",
""name"": ""CopyPipeline-fromBlobToBlob"",
""properties"": {
""activities"": [
  {
    ""type"": ""Copy"",
    ""typeProperties"": {
      ""source"": {
        ""type"": ""BlobSource"",
        ""recursive"": true
      },
      ""sink"": {
        ""type"": ""BlobSink"",
        ""copyBehavior"": """",
        ""writeBatchSize"": 0,
        ""writeBatchTimeout"": ""00:00:00""
      },
      ""enableSkipIncompatibleRow"": true
    },
    ""inputs"": [
      {
        ""name"": ""InputDataset-1""
      }
    ],
    ""outputs"": [
      {
        ""name"": ""OutputDataset-1""
      }
    ],
    ""policy"": {
      ""timeout"": ""1.00:00:00"",
      ""concurrency"": 1,
      ""executionPriorityOrder"": ""NewestFirst"",
      ""style"": ""StartOfInterval"",
      ""retry"": 3,
      ""longRetry"": 0,
      ""longRetryInterval"": ""00:00:00""
    },
    ""scheduler"": {
      ""frequency"": ""Day"",
      ""interval"": 1,
      ""offset"": ""05:00:00""
    },
    ""name"": ""activity_00""
  }
],
""start"": ""2018-03-07T00:00:00Z"",
""end"": ""2020-03-08T00:00:00Z"",
""isPaused"": false,
""pipelineMode"": ""Scheduled""

}
}
</code></pre>
","<azure><azure-data-factory>","2018-03-12 17:05:54","5543","0","2","49244273","<p>You can use <code>EndOfInterval</code> instead of <code>StartOfInterval</code> in the policy. That will use the end of the day instead of the start of the day to do the execution. You may also want to set the appropriate offset if the file is not available at midnight.</p>
"
"49240814","Azure Data Factory copy pipeline: get a file with date of yesterday that arrived today","<p>I am getting a txt file (on todays date) with the date of yesterday in it and I want dynamically get this filename in my data factory pipeline. </p>

<p>The file is placed automatically on a file system and I want to copy this file to the blob store In my example below I am simulating this by copying from blob to blob. </p>

<p><strong>For example:</strong>
filename_2018-02-11.txt arrived today (2018-03-12) with the date of yesterday(2018-02-11). How can I pick this file up on today's date? 
Yesterday's slice did run but there was not a file yet.</p>

<p>Here is my example:</p>

<pre><code> {""$schema"": ""http://datafactories.schema.management.azure.com/schemas/2015-09-01/Microsoft.DataFactory.Pipeline.json"",
""name"": ""CopyPipeline-fromBlobToBlob"",
""properties"": {
""activities"": [
  {
    ""type"": ""Copy"",
    ""typeProperties"": {
      ""source"": {
        ""type"": ""BlobSource"",
        ""recursive"": true
      },
      ""sink"": {
        ""type"": ""BlobSink"",
        ""copyBehavior"": """",
        ""writeBatchSize"": 0,
        ""writeBatchTimeout"": ""00:00:00""
      },
      ""enableSkipIncompatibleRow"": true
    },
    ""inputs"": [
      {
        ""name"": ""InputDataset-1""
      }
    ],
    ""outputs"": [
      {
        ""name"": ""OutputDataset-1""
      }
    ],
    ""policy"": {
      ""timeout"": ""1.00:00:00"",
      ""concurrency"": 1,
      ""executionPriorityOrder"": ""NewestFirst"",
      ""style"": ""StartOfInterval"",
      ""retry"": 3,
      ""longRetry"": 0,
      ""longRetryInterval"": ""00:00:00""
    },
    ""scheduler"": {
      ""frequency"": ""Day"",
      ""interval"": 1,
      ""offset"": ""05:00:00""
    },
    ""name"": ""activity_00""
  }
],
""start"": ""2018-03-07T00:00:00Z"",
""end"": ""2020-03-08T00:00:00Z"",
""isPaused"": false,
""pipelineMode"": ""Scheduled""

}
}
</code></pre>
","<azure><azure-data-factory>","2018-03-12 17:05:54","5543","0","2","49256072","<p>In ADF v2 you can use inbuilt variables (<code>@pipeline().TriggerTime</code>):</p>

<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables</a></p>

<p>And in a source data set (InputDataset-1) put file path/file name as something like this:</p>

<p><code>@concat('YOUR BASE PATH IN BLOB/', 'filename_', 
    addhours(pipeline().TriggerTime, -1, 'yyyy'), '-', 
    addhours(pipeline().TriggerTime, -1, 'MM'), '-', 
    addhours(pipeline().TriggerTime, -1, 'dd'), '.txt'))</code></p>

<p>You can also use <code>@trigger().scheduledTime</code>
To have always the same date when e.g. pipeline will fail.
But remember that it is only available in trigger scope. 
In my tests it was only evaluated for Schedule trigger.</p>
"
"49235767","copy and decompress .tar file with Azure Data Factory","<p>I m trying to copy and decompress .tar file from FTP to Azure Data Lake Store.
.tar file contains HTML files. In the copy activity, on a dataset, i select Compression type GZipDeflate, but I wonder what file format do I need to use? Is it supported to do such I thing without custom activity?</p>
","<azure><azure-data-factory>","2018-03-12 12:53:34","2302","0","2","49235986","<p>Unfortunately, Data factory doesn't support decompression of .tar files. The supported types for ftp are GZip, Deflate, BZip2, and ZipDeflate. (as seen here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#compression-support"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/supported-file-formats-and-compression-codecs#compression-support</a>).</p>

<p>A solution may be to save the files in one of the supported formats, or try a custom activity as was explained here, although I'm not sure if it was for data factory v1 or v2: <a href=""https://stackoverflow.com/questions/41949630/import-tar-file-using-azure-data-factory"">Import .tar file using Azure Data Factory</a></p>

<p>Hope this helped!</p>
"
"49235767","copy and decompress .tar file with Azure Data Factory","<p>I m trying to copy and decompress .tar file from FTP to Azure Data Lake Store.
.tar file contains HTML files. In the copy activity, on a dataset, i select Compression type GZipDeflate, but I wonder what file format do I need to use? Is it supported to do such I thing without custom activity?</p>
","<azure><azure-data-factory>","2018-03-12 12:53:34","2302","0","2","49281426","<p>So its true that there is no way just to decompress .tar files with ADF or ADL Analytics, but there is an option to take a content from every file in .tar file and save as an output in U-SQL. 
I have a scenario that I need to take content from html files inside the .tar file, so i just created html extractor that will take stream content of each html file in .tar file and save in a U-SQL output variable.
Maybe this can help someone who has a similar use case.
I used SharpCompress.dll for extracting and looping over .tar files in c#.</p>
"
"49190085","ADF activity is in progress state for a longer period of time","<p>I have a daily schedule ADF pipeline that consist of couple of activities. The first activity basically invokes a stored procedure that takes less than 2 second to run on DB side but the activity is running continuously(<strong>In Progress state</strong> for longer period of time). Initially I thought it was due to the blocking on SQL server side. But the problem is there is NO SQL SERVER hit from ADF side. The strange thing is running on Dev environment but getting blocked in QA.</p>

<p><a href=""https://i.stack.imgur.com/RKxlv.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RKxlv.jpg"" alt=""enter image description here""></a> </p>

<p>Here is the RunId for the same:</p>

<p><a href=""https://i.stack.imgur.com/8Bq4p.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8Bq4p.jpg"" alt=""enter image description here""></a></p>

<p>Thanks!</p>
","<azure-data-factory><azure-data-lake>","2018-03-09 09:04:52","1064","0","1","49193855","<p>Sometimes you can invoke a stored procedure with a copy activity, changing the SqlReaderQuery property to something like ""exec 'sp_name'"". I explained this here: <a href=""https://stackoverflow.com/questions/47903441/execute-storedprocedure-from-azure-datafactory/47905719#47905719"">Execute storedProcedure from azure datafactory</a></p>

<p>Your problem looks like the integration runtime cannot reach the database, try opening the integration runtime, and connecting to the database from the diagnostic tab. Once you make it work there, make sure the linked service is configured with those credentials.</p>

<p>Hope this helped!</p>
"
"49177818","azure data factory syntax for scheduled slice activity","<p>I'm using Azure Data Factory(V2) to schedule a copy pipeline activity - the requirement is that every day the job should run and select everything from a table, from the last 5 days. I have scheduled the copy and tried the following syntax in the source dataset:</p>

<p>select * from [dbo].[aTable] where [aDate] >= '@{formatDateTime(adddays(pipeline().parameters.windowStart, 'yyyy-MM-dd HH:mm' ),-5)}' </p>

<p>But this doesn't work, I'm getting an error stating that adddays is expecting an int for it's second parameter but is receiving a string.</p>

<p>Can anybody advise on the proper way to nest this??</p>

<p>Thanks</p>
","<azure><azure-data-factory>","2018-03-08 16:22:05","230","0","1","49194092","<p>I cant test this right now, so I'll risk a possible answer just by looking at your query. I think it should be like this:</p>

<pre><code>select * from [dbo].[aTable] where [aDate] &gt;= '@{formatDateTime(adddays(pipeline().parameters.windowStart, -5), 'yyyy-MM-dd HH:mm')}'
</code></pre>

<p>Hope this helps!</p>
"
"49167400","Azure Data Factory V1","<p>Is it possible to trigger a pipeline in ADF v1 using Powershell script?</p>

<p>I found this command ""Resume-AzureRmDataFactoryPipeline"" to trigger the pipeline, but it does not really start the pipeline.. </p>

<p>Please advise. </p>
","<azure><azure-data-factory>","2018-03-08 07:14:56","664","0","3","49169259","<p><code>Resume-AzureRmDataFactoryPipeline</code> will work only on those pipelines which are suspended as this only </p>

<blockquote>
  <p>resumes a suspended pipeline in Data Factory. <a href=""https://learn.microsoft.com/en-us/powershell/module/azurerm.datafactories/resume-azurermdatafactorypipeline?view=azurermps-5.4.0"" rel=""nofollow noreferrer"">Link</a>. </p>
</blockquote>

<p>Now, if you want to start a pipeline then start with - </p>

<p><code>New-AzureRmDataFactoryPipeline</code> which would create a pipeline for you and if the pipeline already exists then it would ask for confirmation to replace the existing one. </p>

<p>Once successfully done then you can use <code>Set-AzureRmDataFactoryPipelineActivePeriod</code> to configure active period for the data slices. So, this basically means after you create the pipeline, you specify the period in which data processing occurs by specifying the active period for the pipeline in which the data slices are processed. These cmdlets would run only when the data factory is already created. </p>

<p>You could also choose to run <code>Set-AzureRmDataFactoryPipelineActivePeriod</code> independently to define the active periods of the pipeline and run your data factory.</p>
"
"49167400","Azure Data Factory V1","<p>Is it possible to trigger a pipeline in ADF v1 using Powershell script?</p>

<p>I found this command ""Resume-AzureRmDataFactoryPipeline"" to trigger the pipeline, but it does not really start the pipeline.. </p>

<p>Please advise. </p>
","<azure><azure-data-factory>","2018-03-08 07:14:56","664","0","3","49195167","<p>It really depends on what your pipeline does, but an alternative method is setting the status of a slice to waiting, with the following powershell cmdlet:</p>

<pre><code>$StartDateTime = (Get-Date).AddDays(-7)
$ResourceGroupName = ""YourRGName""
$DSName = ""YourDatasetName""
$DataFactoryV1Name = ""YourDFv1Name""
Set-AzureRmDataFactorySliceStatus -DataFactoryName $DataFactoryV1Name -DatasetName $DSName -ResourceGroupName $ResourceGroupName -StartDateTime $StartDateTime -Status Waiting
</code></pre>

<p>Replace with your values and run after being logged in and selecting a subscription. What this does is sets some slices to Waiting, and if their startdatetime is in the past, data factory will run them immediately.</p>

<p>Hope this helped!</p>
"
"49167400","Azure Data Factory V1","<p>Is it possible to trigger a pipeline in ADF v1 using Powershell script?</p>

<p>I found this command ""Resume-AzureRmDataFactoryPipeline"" to trigger the pipeline, but it does not really start the pipeline.. </p>

<p>Please advise. </p>
","<azure><azure-data-factory>","2018-03-08 07:14:56","664","0","3","49236892","<p>You can use this command <code>Set-AzureRmDataFactorySliceStatus</code>. Through this, you can reset the slice to ""Pending Execution"" state. You also get the option to set the same status for Upstream slices so that the entire pipeline can re-run.</p>

<p>See this for reference <a href=""https://learn.microsoft.com/en-us/powershell/module/azurerm.datafactories/set-azurermdatafactoryslicestatus?view=azurermps-5.4.0"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/powershell/module/azurerm.datafactories/set-azurermdatafactoryslicestatus?view=azurermps-5.4.0</a></p>
"
"49158784","How do I pass a parameter with Expression as value in ADFv2?","<p>In Azure Data Factory v2 (ADFv2) I am having trouble passing a parameter whose value is an expression that needs evaluated at runtime.  </p>

<p>For example, I have a set of extracts I want to download daily from the same LinkedService/Connection.  I want a pipeline with a Foreach to be able to input a JSON pipeline parameter with a list of configuration for each report type (this I can do).  ""but"" when I have one of those configuration KVPairs with value that is an expression, the expression does not seem to be evaluated.</p>

<p>here is an example of a Foreach parameter set that works for an SFTP LinkedService :</p>

<pre><code>[ { ""dirPath"" : ""/dirPath"" ,""fileFilter"" : ""this_works_fine_20180307*.txt"" } ]
</code></pre>

<p>here is an example of a Foreach parameter set that does not match the files I need to get.<br>
(assume utcnow('yyyyMMdd') returns 20180307</p>

<p>[ { ""dirPath"" : ""/dirPath"" ,""fileFilter"" : ""this_does_NOT_work_@{utcnow('yyyyMMdd')}*.txt"" } ]</p>

<p>This assumes that in the underlying Copy activity I am passing the dataset parameter fileFilter as </p>

<pre><code>@item().fileFilter
</code></pre>

<p>...and in the dataset, the value of the fileFilter is an expression with value</p>

<pre><code>@dataset().fileFilter
</code></pre>

<p>...I have also tried to wrap the argument completely as:</p>

<pre><code>[ { ""dirPath"" : ""/dirPath"" ,""fileFilter"" : ""@toLower(concat(string('this_does_NOT_work_'),string(utcnow('yyyyMMdd')),string('*.txt') )))"" } ]
</code></pre>

<p>...If you have suggestions/guidance, please let me know.</p>

<p>Thanks,
J</p>
","<dataset><parameter-passing><dynamic-programming><sftp><azure-data-factory>","2018-03-07 18:20:08","1140","3","1","49325929","<p>Try to put the fileFilter parameter directly in pipeline parameter.</p>

<p>Something like this will work:</p>

<p>[ { ""dirPath"" : ""/dirPath"" ,""fileFilter"" : ""this_works_fine_@{formatDateTime(utcnow(), 'yyyy')}@{formatDateTime(utcnow(), 'MM')}@{formatDateTime(utcnow(), 'dd')}*.txt"" } ]</p>
"