{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9df784b6-eb3a-416d-ac97-3c181e173509",
   "metadata": {},
   "source": [
    "Project: NLP for Requirements Engineering Interview Transcripts\n",
    "\n",
    "Sixth SQL Query: Data Gouvernance Tags and Titles\n",
    " **Project: NLP for Requirements Engineering Interview Transcripts**\n",
    " \n",
    " Second SQL Query: Requirements Tags and Titles including scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd2e73-f38b-4e00-8f7b-4925edb472e4",
   "metadata": {},
   "source": [
    "Query\n",
    "\n",
    "SELECT TOP 500 q.Id AS QuestionId, q.Title AS QuestionTitle, q.Body AS QuestionBody, q.Tags AS QuestionTags, q.CreationDate AS Date, q.ViewCount AS ViewCount, q.Score AS Score, q.AnswerCount AS NumberOfAnswers, a.Id AS AnswerId, a.Body AS AnswerBody FROM Posts q INNER JOIN Posts a ON a.ParentId = q.Id WHERE q.PostTypeId = 1 AND ( q.Tags LIKE '%%' OR q.Tags LIKE'%%' OR q.Tags LIKE'%%' OR q.Tags LIKE'%%' OR q.Tags LIKE'%%' OR q.Tags LIKE'%%' OR q.Tags LIKE'%%' OR q.Title LIKE'%data synchronization%' OR q.Title LIKE'%data governance%' OR q.Title LIKE'%data quality%' OR q.Title LIKE'%single source of truth%' OR q.Title LIKE'%data standards%' OR q.Title LIKE'%data cleansing%' OR q.Title LIKE'%data architecture%' ) AND q.CreationDate >= '2018-01-01' ORDER BY q.CreationDate DESC;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096e3eb9-ec8e-42f9-922f-9c0bde84ef5d",
   "metadata": {},
   "source": [
    "**Combine data sets into a single one with 1000 rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea24977d-124c-4de5-9c65-243b52e11986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets into pandas DataFrames\n",
    "df = pd.read_csv('6query_data_governance.csv')\n",
    "# Join the DataFrames\n",
    "#data1 = pd.concat([data1, data2], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "#data1.to_csv('df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aadd3547-3127-4703-95d1-09b4792bc46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad7f4bf-aa99-429c-b8c7-0b6724516347",
   "metadata": {},
   "source": [
    "**Initial Exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7bb160a-faa3-48bb-b6b0-c1d32b60ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates based on the 'QuestionId' and 'AnswerId' columns\n",
    "df_no_duplicates = df.drop_duplicates(subset=['QuestionId', 'AnswerId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89400b79-ac6f-43e7-9c73-f601cff12601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_no_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40d9e8a0-350f-435f-bc0f-21cc7bf28d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>QuestionTitle</th>\n",
       "      <th>QuestionBody</th>\n",
       "      <th>QuestionTags</th>\n",
       "      <th>Date</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Score</th>\n",
       "      <th>NumberOfAnswers</th>\n",
       "      <th>AnswerId</th>\n",
       "      <th>AnswerBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75961897</td>\n",
       "      <td>How to sync in realtime a legacy system with a...</td>\n",
       "      <td>&lt;p&gt;I have a legacy grails based system with a ...</td>\n",
       "      <td>&lt;mongodb&gt;&lt;spring-boot&gt;&lt;data-synchronization&gt;</td>\n",
       "      <td>2023-04-07 20:58:30</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>75994127</td>\n",
       "      <td>&lt;p&gt;I have resolved my problem with AOP instead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75916054</td>\n",
       "      <td>Insufficient CPU quota when running data quali...</td>\n",
       "      <td>&lt;p&gt;I try to follow the below guide from GCP to...</td>\n",
       "      <td>&lt;google-cloud-platform&gt;&lt;google-cloud-dataproc&gt;...</td>\n",
       "      <td>2023-04-03 04:33:41</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>75919323</td>\n",
       "      <td>&lt;p&gt;It won't help you, but even if you have act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75916054</td>\n",
       "      <td>Insufficient CPU quota when running data quali...</td>\n",
       "      <td>&lt;p&gt;I try to follow the below guide from GCP to...</td>\n",
       "      <td>&lt;google-cloud-platform&gt;&lt;google-cloud-dataproc&gt;...</td>\n",
       "      <td>2023-04-03 04:33:41</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>75920854</td>\n",
       "      <td>&lt;p&gt;As mentioned by @guillaume, there is set a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   QuestionId                                      QuestionTitle   \n",
       "0    75961897  How to sync in realtime a legacy system with a...  \\\n",
       "1    75916054  Insufficient CPU quota when running data quali...   \n",
       "2    75916054  Insufficient CPU quota when running data quali...   \n",
       "\n",
       "                                        QuestionBody   \n",
       "0  <p>I have a legacy grails based system with a ...  \\\n",
       "1  <p>I try to follow the below guide from GCP to...   \n",
       "2  <p>I try to follow the below guide from GCP to...   \n",
       "\n",
       "                                        QuestionTags                 Date   \n",
       "0       <mongodb><spring-boot><data-synchronization>  2023-04-07 20:58:30  \\\n",
       "1  <google-cloud-platform><google-cloud-dataproc>...  2023-04-03 04:33:41   \n",
       "2  <google-cloud-platform><google-cloud-dataproc>...  2023-04-03 04:33:41   \n",
       "\n",
       "   ViewCount  Score  NumberOfAnswers  AnswerId   \n",
       "0         19      0                1  75994127  \\\n",
       "1         76      1                2  75919323   \n",
       "2         76      1                2  75920854   \n",
       "\n",
       "                                          AnswerBody  \n",
       "0  <p>I have resolved my problem with AOP instead...  \n",
       "1  <p>It won't help you, but even if you have act...  \n",
       "2  <p>As mentioned by @guillaume, there is set a ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de7ca358-6e37-4926-88d5-3ba1c7f9a8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>QuestionTitle</th>\n",
       "      <th>QuestionBody</th>\n",
       "      <th>QuestionTags</th>\n",
       "      <th>Date</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Score</th>\n",
       "      <th>NumberOfAnswers</th>\n",
       "      <th>AnswerId</th>\n",
       "      <th>AnswerBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>48463910</td>\n",
       "      <td>CouchDB - Does synchronization copy a database...</td>\n",
       "      <td>&lt;p&gt;I was looking for various information on &lt;s...</td>\n",
       "      <td>&lt;database&gt;&lt;couchdb&gt;&lt;indexeddb&gt;&lt;pouchdb&gt;&lt;data-s...</td>\n",
       "      <td>2018-01-26 14:57:03</td>\n",
       "      <td>901</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>48466158</td>\n",
       "      <td>&lt;p&gt;PouchDB can be used in a many different con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>48379097</td>\n",
       "      <td>Is there a way to implement a lock-free soluti...</td>\n",
       "      <td>&lt;p&gt;I am running multiple instances of a java a...</td>\n",
       "      <td>&lt;java&gt;&lt;mysql&gt;&lt;locking&gt;&lt;data-synchronization&gt;</td>\n",
       "      <td>2018-01-22 10:12:04</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>48379432</td>\n",
       "      <td>&lt;p&gt;Assuming that you're maintaining the ACIDit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>48064589</td>\n",
       "      <td>Firebase function in firebase function</td>\n",
       "      <td>&lt;p&gt;The following function has to return all of...</td>\n",
       "      <td>&lt;swift&gt;&lt;firebase&gt;&lt;asynchronous&gt;&lt;firebase-realt...</td>\n",
       "      <td>2018-01-02 16:35:17</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>48065939</td>\n",
       "      <td>&lt;pre&gt;&lt;code&gt;///retrieves all of user's friends\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     QuestionId                                      QuestionTitle   \n",
       "259    48463910  CouchDB - Does synchronization copy a database...  \\\n",
       "260    48379097  Is there a way to implement a lock-free soluti...   \n",
       "261    48064589             Firebase function in firebase function   \n",
       "\n",
       "                                          QuestionBody   \n",
       "259  <p>I was looking for various information on <s...  \\\n",
       "260  <p>I am running multiple instances of a java a...   \n",
       "261  <p>The following function has to return all of...   \n",
       "\n",
       "                                          QuestionTags                 Date   \n",
       "259  <database><couchdb><indexeddb><pouchdb><data-s...  2018-01-26 14:57:03  \\\n",
       "260       <java><mysql><locking><data-synchronization>  2018-01-22 10:12:04   \n",
       "261  <swift><firebase><asynchronous><firebase-realt...  2018-01-02 16:35:17   \n",
       "\n",
       "     ViewCount  Score  NumberOfAnswers  AnswerId   \n",
       "259        901      4                1  48466158  \\\n",
       "260         72      0                1  48379432   \n",
       "261        115      0                1  48065939   \n",
       "\n",
       "                                            AnswerBody  \n",
       "259  <p>PouchDB can be used in a many different con...  \n",
       "260  <p>Assuming that you're maintaining the ACIDit...  \n",
       "261  <pre><code>///retrieves all of user's friends\\...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "162b9b4f-c253-416b-8bf1-9faa2780f226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionId         0\n",
       "QuestionTitle      0\n",
       "QuestionBody       0\n",
       "QuestionTags       0\n",
       "Date               0\n",
       "ViewCount          0\n",
       "Score              0\n",
       "NumberOfAnswers    0\n",
       "AnswerId           0\n",
       "AnswerBody         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify missing values:\n",
    "df.isnull().sum()\n",
    "\n",
    "# Entferne NaN_Werte falls zutreffend: df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23b64ceb-da6c-4b1d-85a8-b837315d18c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Date' column to a datetime object to allow for better manipulation:\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f37d5a7-df43-4693-9fa9-9f5a427e2de3",
   "metadata": {},
   "source": [
    "**Explore the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e10b133-a2bd-4100-bbfe-3ed9d855dbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 196 unique question tags in the dataset.\n"
     ]
    }
   ],
   "source": [
    "unique_question_tags = df['QuestionTags'].nunique()\n",
    "\n",
    "print(f\"There are {unique_question_tags} unique question tags in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d309ae5-3f4d-4a7f-99f1-17f1c1fd3cfa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<mongodb><spring-boot><data-synchronization>',\n",
       "       '<google-cloud-platform><google-cloud-dataproc><google-dataplex>',\n",
       "       '<azure-databricks><spark-structured-streaming><great-expectations><data-quality><databricks-autoloader>',\n",
       "       '<r><filter><tidyverse><data-quality>',\n",
       "       '<google-sheets><google-sheets-formula><data-quality>',\n",
       "       '<flutter><graph><cloud><local><data-synchronization>',\n",
       "       '<r><shiny><dt><data-synchronization>',\n",
       "       '<azure><azure-databricks><azure-purview><databricks-unity-catalog><data-governance>',\n",
       "       '<databricks><delta-lake><great-expectations><data-quality>',\n",
       "       '<python><tuples><where-clause>',\n",
       "       '<python><c#><synchronization><data-synchronization>',\n",
       "       '<python><data-quality><data-profiling>',\n",
       "       '<databricks><azure-databricks><data-ingestion><delta-live-tables>',\n",
       "       '<python><pandas><sorting><pivot><data-governance>', '<hbase>',\n",
       "       '<python><pandas><pandas-profiling><data-quality><data-profiling>',\n",
       "       '<python><data-quality><great-expectations>',\n",
       "       '<python><data-quality><amazon-deequ>',\n",
       "       '<databricks><delta-live-tables>',\n",
       "       '<amazon-web-services><amazon-sagemaker>',\n",
       "       '<snowflake-cloud-data-platform><data-governance>',\n",
       "       '<google-cloud-platform><google-bigquery><google-api-python-client><google-data-catalog><data-governance>',\n",
       "       '<sql><validation><snowflake-cloud-data-platform><unpivot><data-quality>',\n",
       "       '<airflow><data-quality><great-expectations>',\n",
       "       '<java><jdbc><talend><data-quality>',\n",
       "       '<oracle><types><triggers><data-synchronization>',\n",
       "       '<databricks><catalog><databricks-unity-catalog><data-governance>',\n",
       "       '<synchronization><data-synchronization><objectbox>',\n",
       "       '<winapi><synchronization><semaphore><data-synchronization>',\n",
       "       '<reactjs><database><api><symfony><data-synchronization>',\n",
       "       '<python><pyspark><apache-spark-sql><data-quality><great-expectations>',\n",
       "       '<python><databricks><azure-databricks><delta-live-tables>',\n",
       "       '<c++><multithreading>',\n",
       "       '<database><sqlite><xamarin.forms><azure-sql-database><data-synchronization>',\n",
       "       '<azure-databricks><azure-purview><data-governance>',\n",
       "       '<apache-spark><data-quality><great-expectations><amazon-deequ>',\n",
       "       '<c><parallel-processing><openmp>',\n",
       "       '<firebase><time><mongodb-atlas><data-synchronization>',\n",
       "       '<sql><database><tsql><data-quality>',\n",
       "       '<azure><azure-purview><data-governance>',\n",
       "       '<dataframe><pyspark><apache-spark-sql><data-quality>',\n",
       "       '<pyspark><apache-spark-sql><data-quality>',\n",
       "       '<python><dask><data-quality>',\n",
       "       '<database><devops><domain-driven-design><business-logic>',\n",
       "       '<database><tableau-api><data-synchronization><oracle-cdc>',\n",
       "       '<python><amazon-web-services><amazon-sagemaker>',\n",
       "       '<python><google-bigquery><data-quality><great-expectations>',\n",
       "       '<data-quality><apache-dolphinscheduler>',\n",
       "       '<r><dataframe><function><recursion><data-quality>',\n",
       "       '<rest><architecture><openapi><openapi-generator><system-design>',\n",
       "       '<salesforce><data-synchronization><sanity>',\n",
       "       '<mongodb><postgresql><database-design><architecture><distributed-system>',\n",
       "       '<android><view><viewmodel><android-jetpack-compose><android-jetpack-datastore>',\n",
       "       '<python><pre-commit-hook><python-poetry><pre-commit.com>',\n",
       "       '<javascript><node.js><mongodb><data-synchronization>',\n",
       "       '<tsql><dynamic-sql><data-quality>', '<bash><shell>',\n",
       "       '<web-applications><couchdb><offline><pouchdb><data-synchronization>',\n",
       "       '<java><volatile><data-synchronization>',\n",
       "       '<watson-knowledge-catalog><data-governance>',\n",
       "       '<android><rsync><data-synchronization><lineageos>',\n",
       "       '<ios><swift><core-data><icloud><cloudkit>',\n",
       "       '<python><pandas><for-loop><data-quality>',\n",
       "       '<c#><sql><.net><data-synchronization>',\n",
       "       '<import><google-bigquery><google-cloud-sql><data-synchronization><database-link>',\n",
       "       '<python><monitoring><data-quality>',\n",
       "       '<google-drive-api><collaboration><data-synchronization><google-drive-shared-drive>',\n",
       "       '<postgresql><docker><docker-volume><data-synchronization>',\n",
       "       '<python><pandas><missing-data><data-quality>',\n",
       "       '<r><data-quality>',\n",
       "       '<python><apache-spark><pyspark><data-quality><amazon-deequ>',\n",
       "       '<java><google-cloud-platform><google-cloud-data-fusion><cdap><data-governance>',\n",
       "       '<data-processing><data-quality>',\n",
       "       '<oracle><amazon-s3><data-synchronization><oracle-cloud-infrastructure>',\n",
       "       '<ios><multithreading><grand-central-dispatch><data-synchronization><barrier>',\n",
       "       '<validation><pyspark><data-quality>',\n",
       "       '<cpu-architecture><handshake><data-synchronization>',\n",
       "       '<asp.net-core><authentication><odata><syncfusion><data-synchronization>',\n",
       "       '<python><unit-testing><tdd><python-unittest><parallel-testing>',\n",
       "       '<java><database><synchronization><primary-key><data-synchronization>',\n",
       "       '<sql><azure><data-synchronization><azure-data-sync>',\n",
       "       '<javascript><jquery><ajax><api><asynchronous>',\n",
       "       '<java><synchronization>', '<php><json>',\n",
       "       '<c#><design-patterns><azure-functions>',\n",
       "       '<dynamics-crm><crm><application-design><data-quality>',\n",
       "       '<caching><redis><data-synchronization>',\n",
       "       '<scala><data-quality><amazon-deequ>', '<azure><informatica>',\n",
       "       '<mysql><database><laravel><data-synchronization>',\n",
       "       '<c#><sockets><unity3d><udp><multicastsocket>',\n",
       "       '<protocol-buffers><avro><thrift-protocol><data-security>',\n",
       "       '<javascript><reactjs><url><browser-history>',\n",
       "       '<synchronization><couchdb><replication><pouchdb>',\n",
       "       '<c><embedded><task><data-synchronization>',\n",
       "       '<reactjs><react-hooks><data-synchronization>',\n",
       "       '<python><pandas><events><data-quality>',\n",
       "       '<architecture><etl><data-ingestion><data-lake><data-governance>',\n",
       "       '<storage><data-synchronization><server-farm>',\n",
       "       '<r><data-science>', '<postgresql><airflow><data-quality>',\n",
       "       '<android><rx-java2>', '<sql><data-quality>', '<python><pandas>',\n",
       "       '<swiftui><combine>', '<sql><sql-server>',\n",
       "       '<javascript><c#><angular><typescript><synchronization>',\n",
       "       '<validation><user-interface><google-sheets><user-input><user-experience>',\n",
       "       '<apache-kafka><microservices><data-synchronization>',\n",
       "       '<sas><difference><data-quality>',\n",
       "       '<python><data-analysis><data-cleaning><data-quality>',\n",
       "       '<sap-commerce-cloud><data-synchronization><backoffice>',\n",
       "       '<reactjs><react-native>',\n",
       "       '<mongodb><typescript><elasticsearch><synchronization><loopback4>',\n",
       "       '<reactjs><react-hooks><use-reducer>',\n",
       "       '<java><performance><apache-kafka><relational-database><data-synchronization>',\n",
       "       '<python><pandas><airflow><standard-deviation><data-quality>',\n",
       "       '<sql><amazon-s3><metadata><amazon-athena><data-quality>',\n",
       "       '<azure><azure-sql-database><localdb><data-synchronization><azure-data-sync>',\n",
       "       '<mysql><mongodb><apache-kafka><etl><data-synchronization>',\n",
       "       '<scala><apache-spark><delta-lake><data-quality>',\n",
       "       '<android><multithreading><background>',\n",
       "       '<firebase><google-cloud-firestore>',\n",
       "       '<android><wear-os><synchronous><data-synchronization>',\n",
       "       '<sql><sql-server><join>',\n",
       "       '<android><timestamp><client-server><data-synchronization><sql-timestamp>',\n",
       "       '<c#><sql-server><database><data-synchronization>',\n",
       "       '<python><c++><ubuntu><message-queue><data-synchronization>',\n",
       "       '<informatica><informatica-powercenter><data-quality>',\n",
       "       '<sql><jdbc><snowflake-cloud-data-platform><data-governance>',\n",
       "       '<mongodb><asynchronous><replication><data-synchronization>',\n",
       "       '<informatica><data-quality>', '<python><pandas><data-cleaning>',\n",
       "       '<ssis><data-quality>', '<node.js><typescript><jsonschema>',\n",
       "       '<android><json><server><httpurlconnection><data-synchronization>',\n",
       "       '<testing><tfs-2015><alm><data-synchronization><hp-quality-center>',\n",
       "       '<mongodb><meteor><data-synchronization><minimongo>',\n",
       "       '<java><android><api><data-synchronization><offline-mode>',\n",
       "       '<validation><aws-glue><data-quality>', '<angular><google-maps>',\n",
       "       '<python><pandas><data-quality>',\n",
       "       '<android><firebase><google-cloud-firestore><broadcastreceiver><android-8.0-oreo>',\n",
       "       '<task-parallel-library><reactive-programming><system.reactive><tpl-dataflow><rx.net>',\n",
       "       '<ruby-on-rails><web-services><associations><data-synchronization>',\n",
       "       '<ibm-infosphere><data-governance><ibm-governance>',\n",
       "       '<sql-server><azure><azure-data-factory><data-synchronization><temporal-tables>',\n",
       "       '<sql-server><microservices><cqrs><data-synchronization>',\n",
       "       '<rest><api>', '<firebase><flutter><google-cloud-firestore>',\n",
       "       '<php><arrays><data-synchronization>',\n",
       "       '<database><distributed-system><data-synchronization><cap-theorem>',\n",
       "       '<database><architecture><iot><data-synchronization>',\n",
       "       '<mysql><data-synchronization><azure-data-sync>',\n",
       "       '<python><mysql><django><database><data-synchronization>',\n",
       "       '<microsoft-graph-api><office365><microsoft-graph-security><data-governance>',\n",
       "       '<multi-tenant><data-synchronization>',\n",
       "       '<python><regex><pandas><dataframe>',\n",
       "       '<sql-server><sql-server-2008>', '<delphi><indy10>',\n",
       "       '<active-directory><google-workspace><data-synchronization>',\n",
       "       '<c><linux><data-synchronization>', '<redux><react-redux>',\n",
       "       '<function><batch-file><registry><data-synchronization>',\n",
       "       '<database><mobile><data-synchronization>',\n",
       "       '<unicode><character-encoding><utf-16><data-synchronization>',\n",
       "       '<sql>',\n",
       "       '<docker><authentication><user-permissions><docker-registry><data-synchronization>',\n",
       "       '<python><n-gram>',\n",
       "       '<sql><sql-server><sql-server-2008><data-synchronization>',\n",
       "       '<java><data-synchronization>',\n",
       "       '<scala><apache-spark><apache-spark-sql><data-governance>',\n",
       "       '<visual-studio><axapta><x++><data-synchronization><dynamics-365-operations>',\n",
       "       '<batch-file><informatica><informatica-powercenter><data-quality>',\n",
       "       '<postgresql><rest><data-synchronization>',\n",
       "       '<ionic-framework><synchronization><ionic3><data-synchronization>',\n",
       "       '<database><oracle><postgresql><data-synchronization>',\n",
       "       '<ibm-midrange><data-synchronization><symmetricds>',\n",
       "       '<ddl><distributed-database><tidb>',\n",
       "       '<database><data-synchronization>',\n",
       "       '<sql><oracle><duplicates><data-quality>',\n",
       "       '<outlook><gmail><icalendar><data-synchronization><zimbra>',\n",
       "       '<python-3.x><pandas>',\n",
       "       '<mysql><database-replication><data-synchronization>',\n",
       "       '<database><asynchronous><synchronization><client-server><message-queue>',\n",
       "       '<c#><signalr><real-time><microsoft-sync-framework>',\n",
       "       '<database><data-synchronization><symmetricds>',\n",
       "       '<python><python-3.x>',\n",
       "       '<sql-server><sql-update><ssms><remote-server><data-synchronization>',\n",
       "       '<swift><multithreading><grand-central-dispatch><data-synchronization><barrier>',\n",
       "       '<xampp><cpanel><data-synchronization>',\n",
       "       '<mysql><wordpress><wordpress-rest-api><data-synchronization><offlineapps>',\n",
       "       '<c#><database-replication><data-synchronization><merge-replication><xp-cmdshell>',\n",
       "       '<database><couchdb><indexeddb><pouchdb><data-synchronization>',\n",
       "       '<java><mysql><locking><data-synchronization>',\n",
       "       '<swift><firebase><asynchronous><firebase-realtime-database><data-synchronization>'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show unique question tags\n",
    "df['QuestionTags'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "700c35d6-e9cc-4222-931f-c3a6b1d49cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionTags\n",
       "<informatica><informatica-powercenter><data-quality>                                   4\n",
       "<c++><multithreading>                                                                  4\n",
       "<python><python-3.x>                                                                   4\n",
       "<bash><shell>                                                                          4\n",
       "<java><data-synchronization>                                                           3\n",
       "<c><linux><data-synchronization>                                                       3\n",
       "<python><regex><pandas><dataframe>                                                     3\n",
       "<azure><azure-databricks><azure-purview><databricks-unity-catalog><data-governance>    3\n",
       "<database><oracle><postgresql><data-synchronization>                                   3\n",
       "<swift><multithreading><grand-central-dispatch><data-synchronization><barrier>         3\n",
       "<sas><difference><data-quality>                                                        3\n",
       "<node.js><typescript><jsonschema>                                                      3\n",
       "<java><volatile><data-synchronization>                                                 2\n",
       "<ios><swift><core-data><icloud><cloudkit>                                              2\n",
       "<watson-knowledge-catalog><data-governance>                                            2\n",
       "<storage><data-synchronization><server-farm>                                           2\n",
       "<sql-server><microservices><cqrs><data-synchronization>                                2\n",
       "<python><pre-commit-hook><python-poetry><pre-commit.com>                               2\n",
       "<sql-server><sql-server-2008>                                                          2\n",
       "<python><pandas><for-loop><data-quality>                                               2\n",
       "<r><data-quality>                                                                      2\n",
       "<sql-server><azure><azure-data-factory><data-synchronization><temporal-tables>         2\n",
       "<ruby-on-rails><web-services><associations><data-synchronization>                      2\n",
       "<sql>                                                                                  2\n",
       "<python><pandas><data-quality>                                                         2\n",
       "<angular><google-maps>                                                                 2\n",
       "<validation><aws-glue><data-quality>                                                   2\n",
       "<mongodb><meteor><data-synchronization><minimongo>                                     2\n",
       "<informatica><data-quality>                                                            2\n",
       "<mysql><database><laravel><data-synchronization>                                       2\n",
       "<reactjs><react-hooks><data-synchronization>                                           2\n",
       "<architecture><etl><data-ingestion><data-lake><data-governance>                        2\n",
       "<google-cloud-platform><google-cloud-dataproc><google-dataplex>                        2\n",
       "<python><pandas>                                                                       2\n",
       "<sql><sql-server>                                                                      2\n",
       "<dataframe><pyspark><apache-spark-sql><data-quality>                                   2\n",
       "<python><data-analysis><data-cleaning><data-quality>                                   2\n",
       "<python><pandas><pandas-profiling><data-quality><data-profiling>                       2\n",
       "<visual-studio><axapta><x++><data-synchronization><dynamics-365-operations>            2\n",
       "<ionic-framework><synchronization><ionic3><data-synchronization>                       2\n",
       "<database><asynchronous><synchronization><client-server><message-queue>                2\n",
       "<python><n-gram>                                                                       2\n",
       "<python><data-quality><great-expectations>                                             2\n",
       "<c#><signalr><real-time><microsoft-sync-framework>                                     2\n",
       "<python-3.x><pandas>                                                                   2\n",
       "<python><databricks><azure-databricks><delta-live-tables>                              2\n",
       "<python><pyspark><apache-spark-sql><data-quality><great-expectations>                  2\n",
       "<sql><oracle><duplicates><data-quality>                                                2\n",
       "<google-sheets><google-sheets-formula><data-quality>                                   2\n",
       "<winapi><synchronization><semaphore><data-synchronization>                             2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['QuestionTags'].value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2da299-a724-4ed5-9558-47f0b6584127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['QuestionTags'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3a5509-eac6-4241-a5d8-1b12b9aa9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique questions in the dataset\n",
    "unique_questions_count = df['QuestionId'].nunique()\n",
    "\n",
    "print(f\"There are {unique_questions_count} unique questions in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd6d531-3e1c-43fa-8282-50d2ecd89fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_answers_count = df['AnswerId'].nunique()\n",
    "print(f'There are {unique_answers_count} unique answers in the dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951cc502-d17d-419d-a0a5-f8f0fdcd6e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the basic statistics of the dataset:\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad80b8da-180c-401f-9006-41e9bcc4035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the most common tags:\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "tags = df['QuestionTags'].str.findall('<(.*?)>').explode().values\n",
    "tag_counts = Counter(tags)\n",
    "most_common_tags = tag_counts.most_common(20)\n",
    "\n",
    "plt.barh([tag for tag, _ in most_common_tags], [count for _, count in most_common_tags])\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Tags')\n",
    "plt.title('Top 20 Most Common Tags')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc034c-95df-4c76-bc24-19e4a36910d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of ViewCount, Score, and NumberOfAnswers:\n",
    "#!pip install seaborn\n",
    "\n",
    "sns.histplot(data=df, x='ViewCount', bins=30)\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(data=df, x='Score', bins=30)\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(data=df, x='NumberOfAnswers', bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b701ba9-bd33-45dc-b25b-e9ad5936431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the distribution of questions over time:\n",
    "\n",
    "df['YearMonth'] = df['Date'].dt.to_period('M')\n",
    "\n",
    "question_count_by_month = df.groupby('YearMonth')['QuestionId'].nunique()\n",
    "question_count_by_month.plot()\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Number of Questions')\n",
    "plt.title('Number of Questions by Month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581a7737-25e8-4733-ae24-a0ce3e68661e",
   "metadata": {},
   "source": [
    "**NLP-Pre-Processing**\n",
    "\n",
    "**_always:_** lowercasing, removal of non-alphanumeric characters\n",
    "\n",
    "**_depends_** on task and corpus:* stopwords removal, lemmatization (for spaCy stopwords have to be removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2425707e-5674-48cf-afb0-3f9cd3e48cdf",
   "metadata": {},
   "source": [
    "Using BeautifulSoup to clean the question and answer body is helpful because it helps remove any HTML tags present in the text. The Stack Overflow dataset often contains text with HTML tags. \n",
    "\n",
    "These tags don't provide any valuable information for most NLP tasks and can be considered noise in the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8769fcd-761e-492a-96f6-83c1a492c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "df['CleanedQuestionBody'] = df['QuestionBody'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "df['CleanedAnswerBody'] = df['AnswerBody'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e454bb91-b716-4713-95b9-60f0b33e5761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any unnecessary characters, such as punctuation, numbers, or extra whitespace, \n",
    "# and convert the text to lowercase:\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove non-word characters\n",
    "    text = re.sub(r'\\d+', ' ', text)  # Remove digits\n",
    "    text = text.lower().strip()  # Convert to lowercase and remove leading/trailing whitespace\n",
    "    text = text.replace('<code>', '').replace('</code>', '')  # Remove <code> tags\n",
    "    return text\n",
    "\n",
    "df['CleanedQuestionBody'] = df['CleanedQuestionBody'].apply(clean_text)\n",
    "df['CleanedAnswerBody'] = df['CleanedAnswerBody'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165c8368-fdfd-4db2-a8e1-ef35182aff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tags from 'QuestionTags' column and store them in extra column:\n",
    "df['Tags'] = df['QuestionTags'].str.findall('<(.*?)>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ee353-1b11-4420-86a0-a681fc4e3e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb9d773-86ad-48a9-aed9-34e66b63140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = df.columns\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7866837c-222d-49ef-bcc7-14f537b9d681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization (split text into individual words):\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df['TokenizedQuestionBody'] = df['CleanedQuestionBody'].apply(word_tokenize)\n",
    "df['TokenizedAnswerBody'] = df['CleanedAnswerBody'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca781f49-2a4b-4e9f-8c25-e5b242ef5588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "df['LemmatizedQuestionBody'] = df['TokenizedQuestionBody'].apply(lemmatize_tokens)\n",
    "df['LemmatizedAnswerBody'] = df['TokenizedAnswerBody'].apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90703da-6837-4563-8d7c-b51520f9e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords from the 'LemmatizedQuestionBody' column:\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "df['LemmatizedQuestionBody'] = df['LemmatizedQuestionBody'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063b2cdb-93da-42a3-875f-d0b782fa32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show if new columns were added\n",
    "column_names = df.columns\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42824996-9281-4a25-9693-85bf43500484",
   "metadata": {},
   "source": [
    "**Preprocess and analyze with spaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7039defd-e807-40c8-afce-720a8448298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove <code> and </code> tags, which are commonly used \n",
    "# in Stack Overflow posts to denote code snippets. \n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)  # Convert the list to a string by joining its elements\n",
    "        #text = re.sub(r'\\W+', ' ', text)  # Remove non-word characters\n",
    "        #text = re.sub(r'\\d+', ' ', text)  # Remove digits\n",
    "        #text = text.lower().strip()  # Convert to lowercase and remove leading/trailing whitespace\n",
    "        #text = text.replace('<code>', '').replace('</code>', '')  # Remove <code> tags\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8562e26d-512f-4442-90e3-1355e3eabd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code uses spaCy to analyze the text and extract entities, nouns, and verbs. Finally, it \n",
    "# displays the top 10 most common entities, nouns, and verbs found in the dataset.\n",
    "\n",
    "# You can further customize this analysis by extracting more specific information \n",
    "# relevant to your domain or by using different NLP techniques, such as topic \n",
    "# modeling, sentiment analysis, or clustering.\n",
    "\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Analyze the text and extract entities, nouns, and verbs\n",
    "entities_counter = defaultdict(int)\n",
    "nouns_counter = defaultdict(int)\n",
    "verbs_counter = defaultdict(int)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    question_body = preprocess_text(row['LemmatizedQuestionBody'])\n",
    "    \n",
    "    for text in [question_body]:\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            entities_counter[ent.text.lower()] += 1\n",
    "        \n",
    "        for token in doc:\n",
    "            if token.pos_ == 'NOUN' and token.is_alpha:\n",
    "                nouns_counter[token.text.lower()] += 1\n",
    "            elif token.pos_ == 'VERB' and token.is_alpha:\n",
    "                verbs_counter[token.text.lower()] += 1\n",
    "\n",
    "# Display the top 10 most common entities, nouns, and verbs\n",
    "def display_top_n(counter, n, title):\n",
    "    print(title)\n",
    "    for item, count in sorted(counter.items(), key=lambda x: x[1], reverse=True)[:n]:\n",
    "        print(f\"{item}: {count}\")\n",
    "    print()\n",
    "\n",
    "display_top_n(entities_counter, 10, \"Top 10 Entities\")\n",
    "display_top_n(nouns_counter, 10, \"Top 10 Nouns\")\n",
    "display_top_n(verbs_counter, 10, \"Top 10 Verbs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b967f02b-118b-4a48-962f-84b57fb6881f",
   "metadata": {},
   "source": [
    "**GPT4 Prompt** Discuss the results of the spaCy extractions from each extraction individually: (insert code and results from above)\n",
    "\n",
    "**GPT4 Results:** The spaCy extraction results for the entities, nouns, and verbs are as follows:\n",
    "\n",
    "    Top 10 Entities:\n",
    "        two: 70\n",
    "        one: 66\n",
    "        first: 39\n",
    "        second: 23\n",
    "        max: 12\n",
    "        three: 12\n",
    "        dest_df: 8\n",
    "        to_date: 7\n",
    "        third: 7\n",
    "        custom_sql: 6\n",
    "\n",
    "The entities extracted are mainly related to ordinal and cardinal numbers, as well as specific technical terms such as \"dest_df\", \"to_date\", and \"custom_sql\".\n",
    "\n",
    "    Top 10 Nouns:\n",
    "        data: 545\n",
    "        table: 223\n",
    "        server: 221\n",
    "        value: 218\n",
    "        column: 165\n",
    "        person: 150\n",
    "        time: 142\n",
    "        database: 136\n",
    "        file: 130\n",
    "        user: 124\n",
    "\n",
    "The nouns extracted are primarily associated with data-related concepts, such as \"data\", \"table\", \"server\", \"value\", \"column\", \"database\", and \"file\". Other nouns like \"person\", \"time\", and \"user\" also indicate that the dataset deals with people and time aspects in data management.\n",
    "\n",
    "    Top 10 Verbs:\n",
    "        using: 145\n",
    "        get: 128\n",
    "        want: 85\n",
    "        let: 64\n",
    "        following: 63\n",
    "        need: 56\n",
    "        trying: 55\n",
    "        know: 54\n",
    "        make: 53\n",
    "        wa: 51 (likely short for \"was\" or \"were\")\n",
    "\n",
    "The verbs extracted show that the dataset contains questions related to various activities, such as using tools or methods, getting information or data, wanting or needing help, trying different approaches, and making changes. The presence of verbs like \"know\" and \"following\" suggests that users are seeking knowledge and understanding in their questions.\n",
    "\n",
    "These extractions provide an insight into the common topics and activities discussed in the dataset. They can be used to further analyze the data, identify patterns, or even inform the development of domain-specific resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05bb0bf-a26f-42c6-a8c1-626d9275553e",
   "metadata": {},
   "source": [
    "**Latent Dirichlet Allocation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71554c40-881b-44e2-b3e0-7726067688d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set the random seed for numpy and python to ensure the same results each time the allocation re-runs\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Prepare the input data by using the lemmatized tokens from the 'LemmatizedQuestionBody' column:\n",
    "documents = df['LemmatizedQuestionBody'].tolist()\n",
    "\n",
    "# Create a dictionary and a bag of words corpus from the documents:\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [dictionary.doc2bow(document) for document in documents]\n",
    "\n",
    "# Set the seed for the random number generator used by the LDA model to a fixed value. \n",
    "# Ensures that the same sequence of random numbers is generated every time the model is trained \n",
    "# with the same input data and configuration.\n",
    "random_state_seed = 42 \n",
    "gensim.models.ldamodel.LdaModel.seed = random_state_seed\n",
    "\n",
    "# Train the LDA model. You can choose the number of topics by setting the num_topics parameter:\n",
    "num_topics = 5  # Change this value to the desired number of topics\n",
    "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15) # Set the number of passes\n",
    "\n",
    "# Display the top words for each topic\n",
    "words_per_topic = 10  # Specify the number of top words per topic\n",
    "for topic in range(num_topics):\n",
    "    print(f\"Topic {topic + 1}:\")\n",
    "    top_words = lda_model.show_topic(topic, topn=words_per_topic)\n",
    "    for word, weight in top_words:\n",
    "        print(f\"  {word}: {weight:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c1dd2-e5b9-4fba-ad9b-0770981239d5",
   "metadata": {},
   "source": [
    "**Correction!!**\n",
    "\n",
    "**GPT4 Prompt:** Summarize the results for the LDA on this data set. Give a title to each topic and discuss each topic individually: (insert code and results from above)\n",
    "\n",
    "**GPT4 Results:** The LDA model was trained with 5 topics, and the top 10 keywords for each topic are displayed. Here is a summary of each topic with a suggested title based on the top keywords:\n",
    "\n",
    "    Topic 1: \"Data Manipulation and File Processing\"\n",
    "        This topic seems to be related to data manipulation, file processing, and working with data frames (df). It involves keywords like data, table, df, c, like, need, string, would, value, and file.\n",
    "\n",
    "    Topic 2: \"Cloud Data Solutions and Governance\"\n",
    "        This topic appears to focus on cloud data solutions and data governance. The keywords include data, file, col, azure, governance, value, sql, purview, work, and databricks.\n",
    "\n",
    "    Topic 3: \"Server and Client Data Management\"\n",
    "        This topic is likely about server and client-side data management. The keywords are value, server, data, id, n, client, user, time, b, and modified.\n",
    "\n",
    "    Topic 4: \"Personal Data and Privacy\"\n",
    "        This topic seems to be related to personal data and privacy concerns. The keywords include person, column, public, name, data, self, table, string, store, and var.\n",
    "\n",
    "    Topic 5: \"Database Systems and User Applications\"\n",
    "        This topic appears to focus on database systems and user applications, with keywords such as data, server, medium, lite, database, user, app, client, file, and want.\n",
    "\n",
    "These topics provide an overview of the main themes discussed in the dataset. They can be useful for understanding the content, organizing information, and identifying areas of interest for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01610ab9-aa65-4dfa-b30b-0c0f69461a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get the document-topic matrix and pad with zeros if necessary \n",
    "# (lists inside doc_topic_matrix have different lengths.)\n",
    "doc_topic_matrix = [np.pad([topic_prob for _, topic_prob in lda_model.get_document_topics(bow)], (0, lda_model.num_topics - len(lda_model.get_document_topics(bow))), mode='constant') for bow in corpus]\n",
    "\n",
    "# Create a new column in the dataset to store the topic names\n",
    "df['LDATopicName'] = ''\n",
    "\n",
    "# Define a function that returns the topic name based on the index\n",
    "def get_topic_name(topic_index):\n",
    "    topic_names = {\n",
    "        0: 'Data Manipulation and File Processing',\n",
    "        1: 'Cloud Data Solutions and Governance',\n",
    "        2: 'Server and Client Data Management',\n",
    "        3: 'Personal Data and Privacy',\n",
    "        4: 'Database Systems and User Applications'\n",
    "    }\n",
    "    return topic_names.get(topic_index, '')\n",
    "\n",
    "# Iterate through the document-topic matrix and assign the topic names\n",
    "for i, row in enumerate(doc_topic_matrix):\n",
    "    topic_index = np.argmax(row)\n",
    "    df.at[i, 'LDATopicName'] = get_topic_name(topic_index)\n",
    "\n",
    "# This code block assumes that you have the `lda_model`, `corpus`, and `df` \n",
    "# variables defined in your environment. After running this code, your DataFrame \n",
    "# `df` should have a new column called `TopicName` with the assigned \n",
    "# topic names for each document.\n",
    "\n",
    "#In this code, I've added a padding step using np.pad() to make all lists inside doc_topic_matrix \n",
    "#have the same length. This should resolve the warning, and the code should now work correctly in your \n",
    "#Jupyter Notebook.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b065677-634d-40e5-8073-187aa54a8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each question to its most relevant topic:\n",
    "def assign_topic(document):\n",
    "    bow = dictionary.doc2bow(document)\n",
    "    topic_distribution = lda_model.get_document_topics(bow)\n",
    "    most_relevant_topic = max(topic_distribution, key=lambda x: x[1])[0]\n",
    "    return most_relevant_topic\n",
    "\n",
    "df['LDATopic'] = df['LemmatizedQuestionBody'].apply(assign_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35806aa9-7d16-44a9-8ae9-f74f65b31636",
   "metadata": {},
   "source": [
    "**Perform Sentiment Analysis on each topic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85d634e-d786-4fc3-bb20-74f0bdfbfefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the VADER Sentiment Analzer it is reccommendet to use raw, unproecessed text the way it coems out of socail media.\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to calculate the sentiment scores\n",
    "def get_sentiment_score(text):\n",
    "    return sia.polarity_scores(text)\n",
    "\n",
    "# Apply the function to the 'AnswerBody' column and store the sentiment scores in a new column\n",
    "df['VaderSentimentScores'] = df['QuestionBody'].apply(get_sentiment_score)\n",
    "\n",
    "# Calculate the average sentiment score for each topic\n",
    "avg_sentiment_by_topic = df.groupby('LDATopicName')['VaderSentimentScores'].apply(lambda scores: pd.DataFrame(scores.tolist()).mean())\n",
    "\n",
    "print(avg_sentiment_by_topic)\n",
    "\n",
    "# Note: The VADER sentiment analyzer is designed for social media texts and might not be the best choice \n",
    "# for your specific dataset. If you find that the sentiment analysis results are not accurate enough, \n",
    "# you can consider using other sentiment analysis libraries or training your own sentiment analysis model \n",
    "# using machine learning techniques.\n",
    "#\n",
    "# Some alternative sentiment analysis libraries include TextBlob and spaCy. You can also explore using\n",
    "# pre-trained models for sentiment analysis from popular NLP libraries like Hugging Face's Transformers.\n",
    "\n",
    "# This code will apply the VADER sentiment analyzer to each answer in the DataFrame, store the sentiment \n",
    "# scores in a new column called SentimentScores, and calculate the average sentiment score for each topic. \n",
    "# The limitations and suggestions for alternative approaches are provided as comments within the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b1b7b0-1db9-4d77-899d-0de6801765c2",
   "metadata": {},
   "source": [
    "**GPT4 Prompt:** Summarize the results of the vader sentiment analysis and discuss the results for each topic individually: (insert code and results from above)\n",
    " \n",
    "**GPT4 Results:** The VADER sentiment analysis was applied to the QuestionBody column of the DataFrame, and the average sentiment scores for each topic were calculated. Here is a summary of the results:\n",
    "\n",
    "    Topic 1: \"Data Manipulation and File Processing\"\n",
    "        Negative Sentiment: 0.0293\n",
    "        Neutral Sentiment: 0.9050\n",
    "        Positive Sentiment: 0.0657\n",
    "        Compound Sentiment: 0.3879\n",
    "\n",
    "    Topic 2: \"Cloud Data Solutions and Governance\"\n",
    "        Negative Sentiment: 0.0135\n",
    "        Neutral Sentiment: 0.9081\n",
    "        Positive Sentiment: 0.0786\n",
    "        Compound Sentiment: 0.5542\n",
    "\n",
    "    Topic 3: \"Server and Client Data Management\"\n",
    "        Negative Sentiment: 0.0000\n",
    "        Neutral Sentiment: 0.9550\n",
    "        Positive Sentiment: 0.0450\n",
    "        Compound Sentiment: 0.1763\n",
    "\n",
    "    Topic 4: \"Personal Data and Privacy\"\n",
    "        Negative Sentiment: 0.0000\n",
    "        Neutral Sentiment: 0.8815\n",
    "        Positive Sentiment: 0.1185\n",
    "        Compound Sentiment: 0.3014\n",
    "\n",
    "    Topic 5: \"Database Systems and User Applications\"\n",
    "        Negative Sentiment: 0.0000\n",
    "        Neutral Sentiment: 0.9270\n",
    "        Positive Sentiment: 0.0730\n",
    "        Compound Sentiment: 0.2522\n",
    "\n",
    "From the analysis, we can observe that the sentiment across all topics is mostly neutral, with varying degrees of positive sentiment. Topic 2 (Cloud Data Solutions and Governance) has the highest compound sentiment score (0.5542), indicating a more positive overall sentiment compared to the other topics. Topic 3 (Server and Client Data Management) has the lowest compound sentiment score (0.1763), suggesting a more neutral sentiment. Topic 4 (Personal Data and Privacy) has the highest positive sentiment (0.1185) and no negative sentiment, indicating a positive outlook on the subject matter.\n",
    "\n",
    "It is important to note that VADER is designed for analyzing social media texts and might not be the best choice for your specific dataset. If you find the sentiment analysis results are not accurate enough, you can consider using other sentiment analysis libraries or training your own sentiment analysis model using machine learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cdb2be-b4d4-4923-a0f5-4560989c1c8e",
   "metadata": {},
   "source": [
    "**Sentiment Analysis with TextBlob**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a93d9df-4472-4e35-b4f9-11f97da7d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "def textblob_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "df['TextBlobSentiment'] = df['QuestionBody'].apply(textblob_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f0065c-32e3-42d6-aa6f-a70516b0937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_topic(document):\n",
    "    bow = dictionary.doc2bow(document)\n",
    "    topic_scores = lda_model[bow]\n",
    "    most_probable_topic = max(topic_scores, key=lambda x: x[1])[0]\n",
    "    return most_probable_topic\n",
    "\n",
    "df['BlobSourceTopicfromLDA'] = df['LemmatizedQuestionBody'].apply(assign_topic)\n",
    "\n",
    "topic_names = {\n",
    "        0: 'Data Manipulation and File Processing',\n",
    "        1: 'Cloud Data Solutions and Governance',\n",
    "        2: 'Server and Client Data Management',\n",
    "        3: 'Personal Data and Privacy',\n",
    "        4: 'Database Systems and User Applications'\n",
    "    }\n",
    "df[\"BlobTopicName\"] = df[\"BlobSourceTopicfromLDA\"].map(topic_names)\n",
    "\n",
    "avg_textblob_sentiment_by_topic = df.groupby('BlobTopicName')['TextBlobSentiment'].mean()\n",
    "print(avg_textblob_sentiment_by_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d22eb4-0f90-4311-92f0-cb2cd7e9b880",
   "metadata": {},
   "source": [
    "**GPT4 Prompt:** Summarize the results of the TextBlob analysis and discuss the results for each topic individually. Afterwards contrast the findings of vader with the findings of TextBlob: (insert code with results from above)\n",
    "\n",
    "**GPT4 Results:** The TextBlob sentiment analysis results for each topic are as follows:\n",
    "\n",
    "    Data Manipulation and File Processing: The average sentiment polarity is 0.07, indicating a slightly positive sentiment.\n",
    "\n",
    "    Cloud Data Solutions and Governance: The average sentiment polarity is 0.11, indicating a positive sentiment.\n",
    "\n",
    "    Server and Client Data Management: The average sentiment polarity is 0.07, indicating a slightly positive sentiment.\n",
    "\n",
    "    Personal Data and Privacy: The average sentiment polarity is 0.03, indicating a neutral sentiment.\n",
    "\n",
    "    Database Systems and User Applications: The average sentiment polarity is 0.07, indicating a slightly positive sentiment.\n",
    "\n",
    "When comparing the results of TextBlob with VADER, it can be seen that both sentiment analysis tools have similar findings in terms of the ranking of the topics by sentiment polarity. However, there are differences in the actual sentiment polarity values assigned to each topic. For example, while VADER assigned a relatively high positive sentiment to the topic of Cloud Data Solutions and Governance, TextBlob assigned an even higher positive sentiment to this topic. Similarly, while VADER assigned a slightly positive sentiment to most of the topics, TextBlob assigned slightly higher sentiment polarities to the topics of Cloud Data Solutions and Governance and Data Manipulation and File Processing. These differences can be attributed to the different algorithms and lexicons used by the two sentiment analysis tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d07fdd-824c-41bc-8e46-fbb218e7c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c5e580-7723-4f06-ac34-c111093f97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7763a2d5-af0a-4653-a138-220dd2298a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['QueryName'] = '6query'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4429bd8-8d46-4655-9327-a142e16e0f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab9c6b-e9b4-4ff4-aa2c-c37acfd3c94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('6query_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6042ac7f-89ac-4daa-ae17-3581bb098c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
