QuestionId,QuestionTitle,QuestionBody,QuestionTags,Date,ViewCount,Score,NumberOfAnswers,AnswerId,AnswerBody
"67427250","How can I replicate an existing data warehouse on Azure?","<p>I am new to Azure and have no prior experience or knowledge regarding working with Azure data warehouse systems (now Azure Synapse Analytics Framework)</p>
<p>I have access to a &quot;read only&quot; data warehouse (not in Azure) that looks like this:</p>
<p><a href=""https://i.stack.imgur.com/jKdh9.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jKdh9.jpg"" alt=""enter image description here"" /></a></p>
<p>I want to replicate this data warehouse as it is on Azure cloud. Can anyone point me to the right direction (video tutorials or documentation) and the number of steps involved in this process? There are around 40 databases in this warehouse. And what if I wanted to replicated only specific ones?</p>
","<sql-server><azure><data-warehouse>","2021-05-06 23:38:10","300","-1","1","67428524","<p>We can't do that you only have the read only permisson. No matter which data warehouse, we all need the server admin or database owner permission to do the database replicate.</p>
<p>You can easily get this from the all documents relate to the database backup/migrate/replicate, for example: <a href=""https://learn.microsoft.com/en-us/sql/t-sql/statements/backup-transact-sql?view=sql-server-ver15#permissions"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sql/t-sql/statements/backup-transact-sql?view=sql-server-ver15#permissions</a>,</p>
<p>If you have enough permission then you can to that. But for Azure SQL datawarehouse, now we called SQL pool (formerly SQL DW), we can't replicate other from on-premise datawarehouse to Azure directly.</p>
<p>The official document provide a way import the data into to Azure SQL pool((formerly SQL DW)):</p>
<ul>
<li>Once your dedicated SQL pool is created, you can import big data with
simple <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-guide?toc=/azure/synapse-analytics/sql-data-warehouse/toc.json&amp;bc=/azure/synapse-analytics/sql-data-warehouse/breadcrumb/toc.json&amp;view=azure-sqldw-latest&amp;preserve-view=true"" rel=""nofollow noreferrer"">PolyBase T-SQL queries</a>, and then use the power of the
distributed query engine to run high-performance analytics.</li>
</ul>
<p>You also could use other ETL tool to achieve the data migration from on-premise datawarehouse to Azure. For example using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/introduction"" rel=""nofollow noreferrer"">Data Factory</a>, combine these two tutorials:</p>
<ol>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server"" rel=""nofollow noreferrer"">Copy data to and from SQL Server by using Azure Data Factory</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse"" rel=""nofollow noreferrer"">Copy and transform data in Azure Synapse Analytics by using Azure
Data Factory</a></li>
</ol>
"
"67384676","How to prevent locks in redshift. ( Shared lock stopping a write job)","<p>I have a data warehouse which are used by multiple downstream users. They read the data from the redshift table. When they read the data, there is a shared lock enforced on the table. At that time, my daily job which is supposed to write on the table does not write as it cannot put an exclusive lock until the shared lock is clear.</p>
<p>Ideally my write job should take priority over any other read job. Can I enforce this is some way?</p>
","<sql><locking><amazon-redshift><data-warehouse><database-locking>","2021-05-04 12:22:43","668","0","1","67387264","<p>Usually this is done by your update process not requiring an exclusive lock or managing the need for locks so that the update process isn't blocked.</p>
<p>Can you describe your update process and which steps are requiring the exclusive locks?</p>
<p>Look at the locks and statements causing them when things are making forward progress. Reworking these parts should allow you to keep you updates moving while these read sessions are acting on the versions of data they started with.</p>
<p>It is also important to not have user transactions that hang around for days on end.  This can happen when interactive sessions are just left open mid transaction.  The also prevents errors due to some sessions seeing very old versions of data.</p>
"
"67313466","Combining count tables with different row numbers to matrix - file names are column names","<p>I often have to import multiple text files in R and combine them to a matrix for further analysis. Usually I deal with count data from microbiological studies with items in the first column and counts in the second column. The number of rows in the samples is not the same usually.</p>
<p>I would like to combine those single files to one matrix, where the rownames are the items of column 1 and columns display the counts of column 2. The column names of the final table should be the file names of the original single files.</p>
<p>Here is a minimal example:
File 1: Sample1.csv</p>
<pre><code>    [1] [2]
[1]  A , 3
[2]  B , 5
[3]  C , 7
[4]  D , 2
[5]. E , 1
</code></pre>
<p>File2: Sample2.csv</p>
<pre><code>    [1] [2]
[1]  B , 1
[2]  C , 3
[3]  F , 4
[4]  G , 7
</code></pre>
<p>Final wanted output</p>
<pre><code>  [Sample1] [Sample2]
[A]  3       NA
[B]  5       1
[C]  7       3
[D]  2       NA
[E]  1       NA
[F]  NA      4
[G]  NA      7
</code></pre>
<p>I understood that in this case you would have to start loading your files in R as a list</p>
<pre><code>all_paths &lt;-list.files(path = &quot;/path/to/directory/with/.csv&quot;, pattern = &quot;*.csv&quot;,full.names = TRUE)

all_content &lt;-
  all_paths %&gt;%
  lapply(read.table,
         header = FALSE, row.names =1,
         sep = &quot;,&quot;,
         encoding = &quot;UTF-8&quot;)
</code></pre>
<p>which gives me a list of dataframes with the items as row names and counts in the first column</p>
<p>I struggle now to combine this list to the above shown pattern. I tried rbind.fill (dpylr) and other functions from different posts but never succeeded.</p>
<p>Any help is highly appreciated!</p>
","<r><data-manipulation><data-management>","2021-04-29 08:12:17","255","0","1","67313547","<p>Don't put the items as rownames, read them as first column. Put the data in a list and do a full join with <code>Reduce</code>.</p>
<pre><code>all_paths &lt;-list.files(path = &quot;/path/to/directory/with/.csv&quot;, pattern = &quot;*.csv&quot;,full.names = TRUE)
all_content &lt;-lapply(all_paths, read.table,sep = &quot;,&quot;, encoding = &quot;UTF-8&quot;)
result &lt;- Reduce(function(x, y) merge(x, y, by = 'V1', all = TRUE), all_content)
</code></pre>
"
"67309612","where is OLAP in powerBI","<p>I am new to Business Inteligence and powerBI
I understand the need of warehouses and dimension modeling</p>
<p>My question is once the datawarehouse is created (in my case using oracle) i can connect it to powerBI and start creating reports with the datasets imported from my warehouse</p>
<p>In this case what is the difference between the data imported in powerBI and the OLAP cubes , is it the same thing ? (does powerBI creates the cubes implicitly before beeing able to create reports for example)</p>
<p>For instance, I can have my datawarehouse created with usual sql and sgbd oracle (no multidimensional database , MDX or any other concept/technology) so I need to understand the utility of OLAP in this case</p>
<p>In case it is useful to use OLAP and or MDX , should we redesign the warehouse from scratch with other technologies or it is just a layer to add on top of oracle database?</p>
<p>Thank you</p>
","<powerbi><data-warehouse><business-intelligence><olap><olap-cube>","2021-04-29 00:40:31","3652","0","1","67312364","<p>Ques: The difference between the data imported in powerBI and the OLAP
cubes , is it the same thing ?
answer: <strong>NO.</strong></p>
<ol>
<li><p>What's the need for OLAP cube when we have datawarehouse?</p>
<p>The main purpose of a <code>cube</code> is to do the precalculations and store it in analysis (in general ssas) database. Instead of running the calculations in the rdbms or data warehouse we create a OLAP (or say a SSAS) cube.
So to rephrase it output of the calculated formula get stored in cube. So <code>center of the cube is a fact table and dimensions are faces of the cube.</code>
Now there are numerous advantages of creating a OLAP cube but on a high level it can be the understood as with cube we can avoid contention of resources while doing analysis with source system and that we can have data from different data sources for analytic processing (OLAP).</p>
</li>
<li><p>What's the need for OLAP cube when we can directly import data in PowerBI from datawarehouse?</p>
<p>As OLAP cube hold the precalculated data for each query in PowerBI there is a minimum latency.So <code>OLAP offloads Relationships, Aggregations, Drill downs and business Logic from PowerBI and do them at cube level</code>.</p>
</li>
</ol>
"
"67276882","Data Warehouse: Who initiate extract data from multiple source to staging area, is it the source or the data warehouse?","<p>I have a question regarding ETL (especially the extract one) in the data warehouse.</p>
<p>From this <a href=""https://data-warehouses.net/architecture/staging.html"" rel=""nofollow noreferrer"">article</a>, a staging area is mainly for timing reasons and some don't even need a staging area.</p>
<p>Let say I have 3 microservice servers, each one has its own database, 2 operational servers, and 1 data warehouse server.</p>
<ol>
<li>Server 1: data is scheduled to be extracted daily</li>
<li>Server 2: data is needed to be real-time available at the data warehouse</li>
</ol>
<p>Who will initiate extract data? my thought is</p>
<ol>
<li>Server 1: data extracting is initiated by the data warehouse server, extract directly from Server 1's database, scheduled daily</li>
<li>Server 2: the timing is cannot be scheduled, so the data warehouse server provides API for use by Server 2, and Server 2 initiates data extraction or rather sent data to the data warehouse server through provided API every time data is available.</li>
</ol>
<p>But I'm not sure about it, should I provides API to all operational servers and let them decide when to send data to the warehouse? Or is it all the data warehouse server job to extract data directly from the operational database? If so, how to extract the real-time one?</p>
","<database><microservices><etl><data-warehouse>","2021-04-27 04:10:50","71","0","1","67289114","<p>Your proposed approach (you ping server 1 daily but receive data from server 2 in near real time) is what I've seen in the past. A few things you might want to consider, though, are:</p>
<ul>
<li>Sometimes Server 2 can't be configured to automatically ping an API, frequently because its database won't support triggers. If this is the case, then you might need to ping server 2 periodically from the DW, like every five minutes.</li>
<li>Typically, you would query server 1 every night. That said, I could see how it might actually be better for server 1 to just tell you when it's ready to be read. In that second case your DW doesn't have to worry about synchronizing with server 1's schedule; if server 1 has processing delays or its batch window runs long, your DW will still pick up its data when that data is ready.</li>
</ul>
"
"67257232","SQL - load data warehouse dimension table without business IDs?","<p>just wanted to see if any others have dealt with this issue and how.</p>
<p>I am building a star schema DW and currently in the process of structuring my dimension tables. Most of them are fine as the descriptions have IDs, for example I have an “action type description” and an “action type ID” which I can use to build my dimActionType dimension.</p>
<p>My issue is some of the description fields don’t have a corresponding business key - so for example I have a “group role” description but no business ID. What would be the best practice in putting this into a DimGroup table? I can select distinct group role into a table with auto increment but how would I then join my fact table to this?</p>
<p>Appreciate any thoughts/ideas</p>
","<sql><data-warehouse><star-schema-datawarehouse><kimball>","2021-04-25 19:05:36","268","0","1","67257751","<p>If the &quot;group role description&quot; uniquely identifies a group role then it is the business key - so just treat it as you would any other business key.</p>
"
"67237836","Star Schema from multiple source tables","<p>I am struggling in figuring out how to create a star schema from multiple source tables.  I work at a trading firm so the data is related to user trading activity.  The issue I am having is that our datasets do not have primary ids for every field that could be a dimension. Instead, we usually relate our data together using the combination of date and account number.  Here is an example of 3 source tables...</p>
<p><a href=""https://i.stack.imgur.com/Df7JX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Df7JX.png"" alt=""enter image description here"" /></a></p>
<p>I would like to turn this into a star schema, something that looks like ...</p>
<p><a href=""https://i.stack.imgur.com/JEdLA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JEdLA.png"" alt=""enter image description here"" /></a></p>
<p>Is my only option to denormalize my source tables into one wide table (joining trades to position on account number and date, and joining the users table on account number), create keys for each dimension, then re normalizing it into the star schema? Are star schema's ever built from multiple source tables?</p>
","<data-modeling><data-warehouse><denormalization><star-schema><fact-table>","2021-04-23 23:18:47","1123","0","2","67242628","<p>Star schemas are almost always created from multiple source tables.</p>
<p>The normal process is:</p>
<ol>
<li>Populate your dimension tables</li>
<li>Create a temporary/virtual fact record using your source data</li>
<li>Using this fact record, look up the relevant dimension keys</li>
<li>Write the actual fact record to your target fact table</li>
</ol>
"
"67237836","Star Schema from multiple source tables","<p>I am struggling in figuring out how to create a star schema from multiple source tables.  I work at a trading firm so the data is related to user trading activity.  The issue I am having is that our datasets do not have primary ids for every field that could be a dimension. Instead, we usually relate our data together using the combination of date and account number.  Here is an example of 3 source tables...</p>
<p><a href=""https://i.stack.imgur.com/Df7JX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Df7JX.png"" alt=""enter image description here"" /></a></p>
<p>I would like to turn this into a star schema, something that looks like ...</p>
<p><a href=""https://i.stack.imgur.com/JEdLA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JEdLA.png"" alt=""enter image description here"" /></a></p>
<p>Is my only option to denormalize my source tables into one wide table (joining trades to position on account number and date, and joining the users table on account number), create keys for each dimension, then re normalizing it into the star schema? Are star schema's ever built from multiple source tables?</p>
","<data-modeling><data-warehouse><denormalization><star-schema><fact-table>","2021-04-23 23:18:47","1123","0","2","67288570","<p>Data-warehousing is about query speed.  The data-warehouse should not be concerned with data integrity.  IT SHOULD NOT CLEAN OR CORRECT BAD DATA.  It only needs to gather all the data together into a single record to present to the model for analysis.  Denormalizing the data is how this is done.</p>
<p>In a star schema, dimensions do not know about each other and have no relationships with other dimensions.  In a snowflake, dimensions are related to other dimensions.  That is the primary difference between star and snowflake.</p>
<p>All the metadata options for events are rolled up into dimensions and used for slicing/filtering.  All the measurable/calculation data for an event are in the event fact, along with a reference to the dimension(s) containing the relevant metadata.  The Metadata/Dimension is reused across multiple fact records.</p>
<p>Based on the limited example you've provided, I'd suggest you research degenerate dimensions and junk dimensions.  Your Trade and Position data may need to be turned into a fact and a dimension (degenerate), and some of your flag attributes may be best placed into a junk dimension.</p>
<p>You should also make sure your dimension keys are clear.  You should not have multiple paths to a dimension (accountnumber: trade -&gt; position -&gt; user &amp; trade -&gt; user ) as that will cause inconsistent results when querying depending on which relationship you traverse.</p>
"
"67236246","How to standardize city names inserted by user","<p>I need to write a small ETL pipeline because I need to move some data from a source database to a target database (a datawarehouse) to perform some analysis on data.</p>
<p>Among those data, I need to clean and conform the name of cities. Cities are inserted manually by international users, conseguently for a single city I can have multiple names (for example London or Londra).
In my source database I do not have only big cities but I have also small villages.</p>
<p>Well, if I do not standardize city names, our analysis could be nonsensical.</p>
<p>Which is the best practices to standardize cities in my target database? Have any idea or suggestion I can undertake?</p>
<p>Thank you</p>
","<geolocation><etl><data-warehouse><standardization>","2021-04-23 20:05:09","140","0","1","67242725","<p>The only reliable way to do this is to use commercial address validation software - preferably in your source system when the data is being created but it could be integrated into your data pipeline processes.</p>
<p>Assuming you can't afford/justify the use of commercial software, the only other solution is to create your own translation table i.e. a table that holds the values that are entered and what value you want them to be translated to.</p>
<p>While you can build this table based on historic data, there will always be new values that are not in the table, so you would need a process to identify these, add the new record to your translation data and then fix the affected records. You would also need to accept that there would be un-cleansed data in your warehouse for a period of time after each data load</p>
"
"67178469","How to get segmentation data from Snowflake Table in API efficiently and cost-effectively?","<p>I have a segmentation project I am working on for my company and we have to create a pipeline to gather data from our app users and when they fit a segment then the app will receive that information and do something with it (not in my scope). So currently, the client connects and authenticates to an endpoint that allows their client to send JSON data to an Elasticsearch cluster (app started, level completed, etc). I'm then using an Azure Function to grab the live data every 5 minutes and store it in an Azure Blob Storage which then creates a queue that Snowflake reads and ingests the JSON files. We'd then use Snowflake to run a task per segment (that will be decided by the analysts or executives) and the data will be outputted to a table like the one below:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>AccountID</th>
<th>Game</th>
<th>SegmentID</th>
<th>CreatedAt</th>
<th>DeletedAt</th>
</tr>
</thead>
<tbody>
<tr>
<td>123456789</td>
<td>Game 1</td>
<td>1</td>
<td>2021-04-20</td>
<td>2021-04-21</td>
</tr>
<tr>
<td>123456789</td>
<td>Game 1</td>
<td>2</td>
<td>2021-04-20</td>
<td></td>
</tr>
<tr>
<td>123456789</td>
<td>Game 1</td>
<td>3</td>
<td>2021-04-20</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>Where <strong>SegmentID</strong> can represent something like</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>SegmentID</th>
<th>SegmentType</th>
<th>SegmentDescription</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>5 Day Streak</td>
<td>User played for 5 consecutive days</td>
</tr>
<tr>
<td>2</td>
<td>10 Day Streak</td>
<td>User played for 10 consecutive days</td>
</tr>
<tr>
<td>3</td>
<td>15 Day Streak</td>
<td>User played for 15 consecutive days</td>
</tr>
</tbody>
</table>
</div>
<p>In the next step of the pipeline, the same API the user authenticated with should post a request when the game boots up to grab all the segments that the user matches. The dev team will then decide where, when in the session and how to use the information to personalize content. Something like:</p>
<pre><code>select
  SegmentID
from
  SegmentTable
where
  AccountID='{AccountID the App authenticated with}' and
  Game='{Game the App authenticated with}' and
  DeletedAt is null
</code></pre>
<p>Response:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>SegmentID</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
</tr>
<tr>
<td>3</td>
</tr>
</tbody>
</table>
</div>
<p>Serialised:</p>
<pre><code>{&quot;SegmentID&quot;: [2,3]}
</code></pre>
<p>We expect to have about 300K-500K users per day. My question would be, what would be the most efficient and cost-effective way to get this information from Snowflake back to the client so that this amount of users wouldn't have issues when querying the same endpoint and it won't be costly.</p>
","<sql><api><rest><snowflake-cloud-data-platform><data-warehouse>","2021-04-20 11:56:45","129","0","1","68197622","<p>OK, so a bit of a workaround, but I created an external function on Snowflake (using Azure Functions) that upserts data in a local MongoDB cluster. So the API connects to the MongoDB instance which can handle the large volume of concurrent connections and since it is on a local server it is quite cheap. The only cost is the data transfer from Snowflake to MongoDB and the running of the App Service Plan on Azure Functions (could not use consumption-based as to send data to our internal server I needed to create a VNET, NAT Gateway and a Static Outbound IP Address in Azure) and the API Management Service I had to create in Azure.</p>
<p>So how it works? For each stored procedure in Snowflake, at the end I am collecting the segments which have changed (New row or DELETED_AT not null) and triggering the external function which upserts the data in MongoDB using the pymongo client.</p>
"
"67154576","Data Quality Process - defining rules","<p>I am working on a <strong>Data Quality Monitoring</strong> project which is new me.
I started with a <strong>Data Profiling</strong> to analyse my data and have a global view of it.
Next, i thought about defining some data quality rules, but i'm a little bit confused about how to implement these rules.
If u guys can guide me a little bit as i'm totally new to this.</p>
","<python><monitoring><data-quality>","2021-04-19 00:12:42","257","1","1","71782114","<p>This is quite ambiguous question but I try to guess a few tips how to start. Since you are a new to data quality and want already implementation hints, lets start from that.</p>
<p>Purpose: Data quality monitoring system wants to a) recognize error and b) trigger next step how to handle it.</p>
<p>First, build a data quality rule for your data set. The rule can be attribute, record, table or cross-table rule. Lets start with attribute level rule. Implement a rule that recognizes that attribute content does not have '@' in it. Run it to email attributes and create an error record for each row that does not have '@' in email attribute. Error record should have these attributes:</p>
<p>ErrorInstanceID; ErrorName; ErrorCategory; ErrorRule; ErrorLevel; ErrorReaction; ErrorScript; SourceSystem; SourceTable; SourceRecord; SourceAttribute; ErrorDate;</p>
<p>&quot;asd2321sa1&quot;; &quot;Email Format Invalid&quot;; &quot;AttributeError&quot;; &quot;Does not contain @&quot;; &quot;Warning|Alert&quot;; &quot;Request new email at next login&quot;; &quot;ScriptID x&quot;; &quot;Excel1&quot;; &quot;Sheet1&quot;; &quot;RowID=34&quot;; &quot;Column=Email&quot;; &quot;1.1.2022&quot;</p>
<p>MONITORING SYSTEM</p>
<p>You need to make above scripts configurable so that you can change systems, tables and columns as well as rules easily. When ran on top of data sets, they will all populate error records to the same structures resulting in a consistent and historical storage of all errors. You should be able to build reports about existing errors in specific systems, trends of errors appearing or getting fixed and so on.</p>
<p>Next, you need to start building a full-sale data quality metadata repository with a proper data model and design a suitable historical versioning for the above information. You need to store information like which rules were ran and when, which systems and tables they checked, and so on. To detect which systems have bee included in monitoring and also to recognize if systems are not monitored with correct rules. In practice, quality monitoring for data quality monitoring system. You should have statistics which systems are monitored with specific rules, when they were ran last time, aggregates of inspected tables, records and errors.</p>
<p>Typically, its more important to focus on errors that need immediate attention and &quot;alert&quot; an end-user to go fix the issue or triggers a new workflow or flag in source system. For example, invalid emails might be categorized as alerts and be just aggregate statistics. We have 2134223 invalid emails. Nobody cares. However, it might be more important to recognize invalid email of a person who has ordered his bills as digital invoices to his email. Alert. That kind of error (Invalid Email AND Email Invoicing) should trigger an alert and set up a flag in CRM for end users to try get email fixed. There should not be any error records for this error. But this kind of rule should be ran on top of all systems that store customer contact and billind preferences.</p>
<p>For a technical person, I could recommend this book. It's a good book that goes deeper in technical and logical issues of data quality assessment and monitoring systems. There is also a small metadata model for data quality metadata structures. <a href=""https://rads.stackoverflow.com/amzn/click/com/0977140024"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">https://www.amazon.com/Data-Quality-Assessment-Arkady-Maydanchik/dp/0977140024/</a></p>
"
"67140562","SSIS: How do I query for IDs from 1st database and then select data with only those IDs on 2nd database/data warehouse?","<p>SSIS: How do I query for IDs from 1st database and then select data with only those IDs on 2nd database/data warehouse? There are only 12,000 rows I need data on in the 2nd database/data warehouse but the problem is it has 3M+ rows. I need the second SQL query to constrain it to have only the IDs from the 1st query. TIA!</p>
<ol>
<li>1st database query: select distinct ID from tablename1</li>
<li>2nd database/datawarehouse query: select * from tablename2 where ID in (IDs from 1st database query)</li>
</ol>
","<ssis>","2021-04-17 16:41:27","85","0","1","67143214","<p>Here's my solution, it runs in just about 2 minutes now instead of 40 mins without filtering to the id's I need:</p>
<ol>
<li><p>Execute SQL Task: Creates a table (table1) that holds all of the distinct ID's I want to use to query the data warehouse/DB #2. Also creates another table (table2) that will act as the destination table within the 1st DB that holds the distinct ID's, and the columns from the data warehouse I needed.</p>
</li>
<li><p>(Data Flow Task) OLE DB SOURCE (selects all from table1 which holds the distinct ID's)</p>
</li>
<li><p>(Data Flow Task) LOOKUP: Used a lookup task set to full cache mode and ignoring unmatched entries. Finds rows in the data warehouse that match the ID from table1 (one to one relationship) and adds the columns I selected.</p>
</li>
<li><p>(Data flow task) OLE DB DESTINATION: Loads matched output into table2 which is within my destination DB</p>
</li>
</ol>
"
"67136216","an issue due to miscalculation with the monthly revenue","<p>We have a data warehouse which contain dimensional table and sales fact table. One day, there is an issue due to miscalculation with the monthly revenue. We found the root cause is due to missing one customer in source file. How we can we improve the system to handle this issue and make the total monthly revenue still correct? Please give your assumption for the existing system and the solution to handle missing customer data.</p>
","<database><data-warehouse>","2021-04-17 08:34:03","22","-3","1","67289527","<p>The missing revenue should be associated with the &quot;Unknown&quot; customer so it is surfaced in the model and revealed for the source to fix it.  The data warehouse should NEVER FIX BAD DATA, just reveal it.</p>
"
"67114618","Fact and Dimension Data-warehouse Datamart","<p>I have two fact tables (purchase and defect )
and both of them  are linked to the same dimension.
So my question can i combine the two fact to a single fact table (put all the KPI in a single Fact table ).</p>
<p>Thank you.</p>
","<data-warehouse><datamart>","2021-04-15 19:03:57","127","0","1","67114846","<p>A fact table must have a grain defined for it and all measures must adhere to that grain. So if you have two fact tables with the same grain then, technically, you could combine them into a single fact table. Whether that make sense from a usage perspective is a different question.</p>
<p>I assume that your fact tables are linked to more than one dimension each? The fact that they have a dimension in common doesn't mean you can combine them.</p>
"
"67053792","Can you extract defective rows using AssertR in R?","<p>The example below is a simple one which tries to assert the column y is always positive (y&gt;0). How can I extract  the errored data (row 3 with the negative value,into a dataframe maybe, or any convenient object) while allowing the workflow to continue with &quot;cleaned&quot; data?</p>

<pre class=""lang-r prettyprint-override""><code>library(tidyverse)
library(assertr)
#&gt; Warning: package 'assertr' was built under R version 4.0.5

test = tribble(
  ~x,~y,
  1,4,
  3,8,
  1,-2,
  3,1
)

test %&gt;% 
  assert(within_bounds(0,100),y) %&gt;% 
  group_by(x) %&gt;% 
  summarise(avg = mean(y))
#&gt; Column 'y' violates assertion 'within_bounds(0, 100)' 1 time
#&gt;     verb redux_fn             predicate column index value
#&gt; 1 assert       NA within_bounds(0, 100)      y     3    -2
#&gt; Error: assertr stopped execution
</code></pre>
<p><sup>Created on 2021-04-12 by the <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex package</a> (v0.3.0)</sup></p>
<p><strong>UPDATE</strong></p>
<p>After trying @Sirius solution, it worked for me the first time I tried it, but consequent trials all failed with the message shown in this reprex below:</p>

<pre class=""lang-r prettyprint-override""><code>library(tidyverse)
library(assertr)
#&gt; Warning: package 'assertr' was built under R version 4.0.5

test = tribble(
  ~x,~y,
  1,4,
  3,8,
  1,-2,
  3,1,
  5,81
)


my_error_fun = function( ... ) {
  args = list(...)
  
  do.call( just_warn, args )
  
  bad.data = args[[1]][[1]]$error_df
  
  these.failed.rows = args$data %&gt;% 
    slice( bad.data$index )
  
  if(!exists(&quot;my.failed.rows&quot;, inherits=TRUE)) {
    my.failed.rows = NULL
  }
  my.failed.rows = rbind( my.failed.rows, these.failed.rows )
  assign( &quot;my.failed.rows&quot;, my.failed.rows, envir=parent.frame(n=3) )
  
  good.rows = args$data %&gt;% slice(-bad.data$index)
  
  return(good.rows)
  
}

my.result &lt;- test %&gt;%
  assert(within_bounds(0,100),y, error_fun = my_error_fun) %&gt;% 
  group_by(x) %&gt;%
  summarise(avg = mean(y))
#&gt; Column 'y' violates assertion 'within_bounds(0, 100)' 1 time
#&gt;     verb redux_fn             predicate column index value
#&gt; 1 assert       NA within_bounds(0, 100)      y     3    -2
#&gt; Warning: assertr encountered errors
#&gt; `summarise()` ungrouping output (override with `.groups` argument)

print(my.result)
#&gt; # A tibble: 3 x 2
#&gt;       x   avg
#&gt;   &lt;dbl&gt; &lt;dbl&gt;
#&gt; 1     1   4  
#&gt; 2     3   4.5
#&gt; 3     5  81

print(my.failed.rows)
#&gt; Error in print(my.failed.rows): object 'my.failed.rows' not found
</code></pre>
<p><sup>Created on 2021-05-02 by the <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex package</a> (v0.3.0)</sup></p>
<pre class=""lang-r prettyprint-override""><code>devtools::session_info()
#&gt; - Session info ---------------------------------------------------------------
#&gt;  setting  value                       
#&gt;  version  R version 4.0.2 (2020-06-22)
#&gt;  os       Windows 10 x64              
#&gt;  system   x86_64, mingw32             
#&gt;  ui       RTerm                       
#&gt;  language (EN)                        
#&gt;  collate  English_United States.1252  
#&gt;  ctype    English_United States.1252  
#&gt;  tz       Africa/Nairobi              
#&gt;  date     2021-05-02                  
#&gt; 
#&gt; - Packages -------------------------------------------------------------------
#&gt;  package     * version date       lib source        
#&gt;  assertr     * 2.8     2021-01-25 [1] CRAN (R 4.0.5)
#&gt;  assertthat    0.2.1   2019-03-21 [1] CRAN (R 4.0.2)
#&gt;  backports     1.1.9   2020-08-24 [1] CRAN (R 4.0.2)
#&gt;  blob          1.2.1   2020-01-20 [1] CRAN (R 4.0.2)
#&gt;  broom         0.7.0   2020-07-09 [1] CRAN (R 4.0.2)
#&gt;  callr         3.4.4   2020-09-07 [1] CRAN (R 4.0.2)
#&gt;  cellranger    1.1.0   2016-07-27 [1] CRAN (R 4.0.2)
#&gt;  cli           2.0.2   2020-02-28 [1] CRAN (R 4.0.2)
#&gt;  colorspace    1.4-1   2019-03-18 [1] CRAN (R 4.0.2)
#&gt;  crayon        1.3.4   2017-09-16 [1] CRAN (R 4.0.2)
#&gt;  DBI           1.1.0   2019-12-15 [1] CRAN (R 4.0.2)
#&gt;  dbplyr        1.4.4   2020-05-27 [1] CRAN (R 4.0.2)
#&gt;  desc          1.2.0   2018-05-01 [1] CRAN (R 4.0.2)
#&gt;  devtools      2.3.2   2020-09-18 [1] CRAN (R 4.0.3)
#&gt;  digest        0.6.25  2020-02-23 [1] CRAN (R 4.0.2)
#&gt;  dplyr       * 1.0.2   2020-08-18 [1] CRAN (R 4.0.2)
#&gt;  ellipsis      0.3.1   2020-05-15 [1] CRAN (R 4.0.2)
#&gt;  evaluate      0.14    2019-05-28 [1] CRAN (R 4.0.2)
#&gt;  fansi         0.4.1   2020-01-08 [1] CRAN (R 4.0.2)
#&gt;  forcats     * 0.5.0   2020-03-01 [1] CRAN (R 4.0.2)
#&gt;  fs            1.5.0   2020-07-31 [1] CRAN (R 4.0.2)
#&gt;  generics      0.0.2   2018-11-29 [1] CRAN (R 4.0.2)
#&gt;  ggplot2     * 3.3.2   2020-06-19 [1] CRAN (R 4.0.2)
#&gt;  glue          1.4.2   2020-08-27 [1] CRAN (R 4.0.2)
#&gt;  gtable        0.3.0   2019-03-25 [1] CRAN (R 4.0.2)
#&gt;  haven         2.3.1   2020-06-01 [1] CRAN (R 4.0.3)
#&gt;  highr         0.8     2019-03-20 [1] CRAN (R 4.0.2)
#&gt;  hms           0.5.3   2020-01-08 [1] CRAN (R 4.0.2)
#&gt;  htmltools     0.5.0   2020-06-16 [1] CRAN (R 4.0.2)
#&gt;  httr          1.4.2   2020-07-20 [1] CRAN (R 4.0.2)
#&gt;  jsonlite      1.7.1   2020-09-07 [1] CRAN (R 4.0.2)
#&gt;  knitr         1.29    2020-06-23 [1] CRAN (R 4.0.2)
#&gt;  lifecycle     0.2.0   2020-03-06 [1] CRAN (R 4.0.2)
#&gt;  lubridate     1.7.9   2020-06-08 [1] CRAN (R 4.0.2)
#&gt;  magrittr      1.5     2014-11-22 [1] CRAN (R 4.0.2)
#&gt;  memoise       1.1.0   2017-04-21 [1] CRAN (R 4.0.3)
#&gt;  modelr        0.1.8   2020-05-19 [1] CRAN (R 4.0.2)
#&gt;  munsell       0.5.0   2018-06-12 [1] CRAN (R 4.0.2)
#&gt;  pillar        1.4.6   2020-07-10 [1] CRAN (R 4.0.2)
#&gt;  pkgbuild      1.1.0   2020-07-13 [1] CRAN (R 4.0.2)
#&gt;  pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.0.2)
#&gt;  pkgload       1.1.0   2020-05-29 [1] CRAN (R 4.0.2)
#&gt;  prettyunits   1.1.1   2020-01-24 [1] CRAN (R 4.0.2)
#&gt;  processx      3.4.4   2020-09-03 [1] CRAN (R 4.0.2)
#&gt;  ps            1.3.4   2020-08-11 [1] CRAN (R 4.0.2)
#&gt;  purrr       * 0.3.4   2020-04-17 [1] CRAN (R 4.0.2)
#&gt;  R6            2.4.1   2019-11-12 [1] CRAN (R 4.0.2)
#&gt;  Rcpp          1.0.5   2020-07-06 [1] CRAN (R 4.0.2)
#&gt;  readr       * 1.3.1   2018-12-21 [1] CRAN (R 4.0.2)
#&gt;  readxl        1.3.1   2019-03-13 [1] CRAN (R 4.0.2)
#&gt;  remotes       2.2.0   2020-07-21 [1] CRAN (R 4.0.3)
#&gt;  reprex        0.3.0   2019-05-16 [1] CRAN (R 4.0.2)
#&gt;  rlang         0.4.7   2020-07-09 [1] CRAN (R 4.0.2)
#&gt;  rmarkdown     2.7     2021-02-19 [1] CRAN (R 4.0.2)
#&gt;  rprojroot     2.0.2   2020-11-15 [1] CRAN (R 4.0.4)
#&gt;  rvest         0.3.6   2020-07-25 [1] CRAN (R 4.0.2)
#&gt;  scales        1.1.1   2020-05-11 [1] CRAN (R 4.0.2)
#&gt;  sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 4.0.3)
#&gt;  stringi       1.5.3   2020-09-09 [1] CRAN (R 4.0.2)
#&gt;  stringr     * 1.4.0   2019-02-10 [1] CRAN (R 4.0.2)
#&gt;  testthat      2.3.2   2020-03-02 [1] CRAN (R 4.0.2)
#&gt;  tibble      * 3.0.3   2020-07-10 [1] CRAN (R 4.0.2)
#&gt;  tidyr       * 1.1.2   2020-08-27 [1] CRAN (R 4.0.2)
#&gt;  tidyselect    1.1.0   2020-05-11 [1] CRAN (R 4.0.2)
#&gt;  tidyverse   * 1.3.0   2019-11-21 [1] CRAN (R 4.0.2)
#&gt;  usethis       1.6.3   2020-09-17 [1] CRAN (R 4.0.3)
#&gt;  utf8          1.1.4   2018-05-24 [1] CRAN (R 4.0.2)
#&gt;  vctrs         0.3.4   2020-08-29 [1] CRAN (R 4.0.2)
#&gt;  withr         2.2.0   2020-04-20 [1] CRAN (R 4.0.2)
#&gt;  xfun          0.16    2020-07-24 [1] CRAN (R 4.0.2)
#&gt;  xml2          1.3.2   2020-04-23 [1] CRAN (R 4.0.2)
#&gt;  yaml          2.2.1   2020-02-01 [1] CRAN (R 4.0.2)
#&gt; 
#&gt; [1] C:/Users/tnkil/OneDrive/Documents/R/win-library/4.0
#&gt; [2] C:/Program Files/R/R-4.0.2/library
</code></pre>
","<r><assert><assertion><data-management>","2021-04-12 06:51:48","192","0","1","67055866","<p>This is tricky, and the answer below doesn't solve this 100%. Now there are a number of different ways assertr lets you handle errors/stops, just see ?error_stop (which is the default).</p>
<p>You need to not only filter out rows that fail, but also collect them (all) for later inspection.</p>
<p>Below I wrote my own error handler. It fetches those rows that fail, filter them away, and stores them in the <em>global environment</em> under the varibale <code>my.failed.rows</code>.</p>
<pre class=""lang-r prettyprint-override""><code>
library(tidyverse)
library(assertr)
#&gt; Warning: package 'assertr' was built under R version 4.0.5

test = tribble(
  ~x,~y,
  1,4,
  3,8,
  1,-2,
  3,1
)

my_error_fun &lt;- function( ... ) {
    
    args &lt;- list(...)

    do.call( just_warn, args )
    
    bad.data &lt;- args[[1]][[1]]$error_df
    these.failed.rows &lt;- args$data %&gt;% slice( bad.data$index )

    if(!exists(&quot;my.failed.rows&quot;, inherits=TRUE)) {
        my.failed.rows &lt;- NULL
    }
    my.failed.rows &lt;- rbind( my.failed.rows, these.failed.rows )
    assign( &quot;my.failed.rows&quot;, my.failed.rows, envir=parent.frame(n=3) )
    
    good.rows &lt;- args$data %&gt;% slice( -bad.data$index )
    
    return( good.rows )
    
}

my.result &lt;- test %&gt;%
     assert(within_bounds(0,100),y, error_fun = my_error_fun ) %&gt;% 
     group_by(x) %&gt;%
     summarise(avg = mean(y))

print(my.result)

print(my.failed.rows)

</code></pre>
<p>Output:</p>
<pre><code>&gt; print(my.result)
# A tibble: 2 x 2
      x   avg
  &lt;dbl&gt; &lt;dbl&gt;
1     1   4  
2     3   4.5
&gt; print(my.failed.rows)
# A tibble: 1 x 2
      x     y
  &lt;dbl&gt; &lt;dbl&gt;
1     1    -2
</code></pre>
<p>Now next time, it will keep appending to <code>my.failed.rows</code>, so it's up to you to truncate or remove it when you're done inspecting it. I havent' figured out a way to automate this. Essentially a way to detect that a new chained dplyr operation is going on.</p>
"
"66966682","Data quality - Missing values (Pandas)","<p>I'm working on a data quality project. I'm trying to generate a data quality report using pandas-profiling profileReport but when i verify the report it says that i have no missing values while i do have empty cells.
Or do you have any other suggestion
<a href=""https://i.stack.imgur.com/w7BxC.png"" rel=""nofollow noreferrer"">Result</a></p>
<pre><code>df = pd.read_excel('D:/SDAD/PFE/bi_bpcustomer.xls')
print(df.dtypes)
reportCl=ProfileReport(df)
reportCl.to_file(output_file='rapportClient.html')
</code></pre>
<p>Here's a part of my xls file showing missing cells:
<a href=""https://i.stack.imgur.com/pcCk0.png"" rel=""nofollow noreferrer"">xls file</a></p>
","<python><pandas><missing-data><data-quality>","2021-04-06 10:11:08","357","1","1","66974533","<p>It's fine, i found a way by adding this to my code:</p>
<pre><code>missing_values = [&quot;&quot;,&quot; &quot;]
client= pd.read_excel('D:/SDAD/PFE/bi_bpcustomer.xls',na_values = 
missing_values)
</code></pre>
"
"66892108","Data quality - check if all values in a character column are numbers in R","<p>I am looking to perform data quality on numerous system generated tables. One of the checks is to see if all values in a character column are only numbers. I am looking to know the number columns where this check is true. Using the following table as an example I would want to identify that  two columns (code and age) are character columns that consist of only numeric values.</p>
<p><strong>Table Structure</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Column Name</th>
<th>Data Type</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>character</td>
</tr>
<tr>
<td>type</td>
<td>character</td>
</tr>
<tr>
<td>code</td>
<td>character</td>
</tr>
<tr>
<td>member_id</td>
<td>integer</td>
</tr>
<tr>
<td>collection_date</td>
<td>date</td>
</tr>
<tr>
<td>age</td>
<td>character</td>
</tr>
<tr>
<td>height</td>
<td>double</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Table Values</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Column Name</th>
<th>Column Values</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>only letters</td>
</tr>
<tr>
<td>type</td>
<td>only letters</td>
</tr>
<tr>
<td>code</td>
<td>only numbers</td>
</tr>
<tr>
<td>member_id</td>
<td>only numbers</td>
</tr>
<tr>
<td>collection_date</td>
<td>only dates</td>
</tr>
<tr>
<td>age</td>
<td>only numbers</td>
</tr>
<tr>
<td>height</td>
<td>only numbers</td>
</tr>
</tbody>
</table>
</div>
<p>I am having issues thinking of the logic that is required to do this. What I have done thus far is:</p>
<p>To select only columns that are character data types</p>
<pre><code>df %&gt;%
  dplyr::select_if(is.character)
</code></pre>
<p>To validate that all values in the column are numeric (or null, which is fine)</p>
<pre><code>sum(varhandle::check.numeric(df$code)) == nrow(df)
</code></pre>
<p>I am hoping to build a function that performs this across all columns and stores the number where the check is true into a column (i.e. dplyr::mutate), but I am not sure how to structure this; is it an across, an apply, or something else. Or is there any existing function/package that would perform this task? Any help is appreciated.</p>
","<r><data-quality>","2021-03-31 16:58:13","910","1","1","66892136","<p>We could construct the condition within <code>select</code> itself</p>
<pre><code>library(dplyr)
iris %&gt;%
   select(where(~ all(varhandle::check.numeric(.)))) 
</code></pre>
<hr />
<p>It is not clear whether <code>numeric</code> columns with mismatched type or columns having some character elements and thus got converted to <code>class</code> <code>character</code>.  If it is the former, then add a <code>type.convert</code> before the <code>select</code> and then get only the numeric columns</p>
<pre><code>df %&gt;%
   type.convert(as.is = TRUE) %&gt;%
   select(where(is.numeric))
</code></pre>
"
"66887070","StarSchema - Entity Framework Core - Migration","<p>I am trying to build a datawarehouse (DWH), using the code-first approach (star-schema):</p>
<p>Fact-/dimension classes:</p>
<pre><code>    [Table(&quot;FactBase&quot;)]
    public class FactBase
    {
        [Key]
        public Guid Id { get; set; }

        [ForeignKey(&quot;DimTest1&quot;)]
        public string DimDigitalesBuchISBN { get; set; }

        public virtual DimTest1 DimTest1 { get; set; }  
    }


    [Table(&quot;DimTest1&quot;)]
    public class DimTest1
    {
        [Key]
        public string ISBN { get; set; }

        public string Bla { get; set; }
    }
</code></pre>
<p>Context:</p>
<pre><code>public class XDBContextDWH : DbContext
{      
    public DbSet&lt;FactBase&gt; FactBase { get; set; }
    public DbSet&lt;DimTest1&gt; DimTest1 { get; set; }

    protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder)
    {
        optionsBuilder.UseSqlServer(new string(&quot;connection string&quot;));
    }
}   
</code></pre>
<p>After using migration the schema looks like this:</p>
<p><a href=""https://i.stack.imgur.com/qddE6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qddE6.png"" alt=""Here"" /></a></p>
<p>Based on that <a href=""https://www.etl-tools.com/images/Articles/Star%20Schema.png"" rel=""nofollow noreferrer"">star schema</a>, shouldn't be the relationship (here: <a href=""https://justpaste.it/8w9tk"" rel=""nofollow noreferrer"">SQL</a>) like that?</p>
<p><a href=""https://i.stack.imgur.com/tWs5B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tWs5B.png"" alt=""here"" /></a></p>
","<c#><entity-framework-core><entity-relationship><data-warehouse><star-schema>","2021-03-31 11:34:27","362","1","2","66910252","<p>When you specify the attribute <code>[ForeignKey(&quot;DimTest1&quot;)]</code>, you're telling EF to use <code>DimTest1</code> as the navigation property of the <code>FactBase</code> class, not pointing to the <code>DimTest1</code> class.</p>
<p>But since that property does not exist, it does not create the relationship.</p>
<p>Change your class to:</p>
<pre><code>[Table(&quot;FactBase&quot;)]
public class FactBase
{
    [Key]
    public Guid Id { get; set; }

    [ForeignKey(&quot;DimTest1&quot;)]
    public string DimDigitalesBuchISBN { get; set; }
    public virtual DimTest1 DimTest1 { get; set; } // navigation property
}
</code></pre>
<p>This should make it work as intended.</p>
"
"66887070","StarSchema - Entity Framework Core - Migration","<p>I am trying to build a datawarehouse (DWH), using the code-first approach (star-schema):</p>
<p>Fact-/dimension classes:</p>
<pre><code>    [Table(&quot;FactBase&quot;)]
    public class FactBase
    {
        [Key]
        public Guid Id { get; set; }

        [ForeignKey(&quot;DimTest1&quot;)]
        public string DimDigitalesBuchISBN { get; set; }

        public virtual DimTest1 DimTest1 { get; set; }  
    }


    [Table(&quot;DimTest1&quot;)]
    public class DimTest1
    {
        [Key]
        public string ISBN { get; set; }

        public string Bla { get; set; }
    }
</code></pre>
<p>Context:</p>
<pre><code>public class XDBContextDWH : DbContext
{      
    public DbSet&lt;FactBase&gt; FactBase { get; set; }
    public DbSet&lt;DimTest1&gt; DimTest1 { get; set; }

    protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder)
    {
        optionsBuilder.UseSqlServer(new string(&quot;connection string&quot;));
    }
}   
</code></pre>
<p>After using migration the schema looks like this:</p>
<p><a href=""https://i.stack.imgur.com/qddE6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qddE6.png"" alt=""Here"" /></a></p>
<p>Based on that <a href=""https://www.etl-tools.com/images/Articles/Star%20Schema.png"" rel=""nofollow noreferrer"">star schema</a>, shouldn't be the relationship (here: <a href=""https://justpaste.it/8w9tk"" rel=""nofollow noreferrer"">SQL</a>) like that?</p>
<p><a href=""https://i.stack.imgur.com/tWs5B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tWs5B.png"" alt=""here"" /></a></p>
","<c#><entity-framework-core><entity-relationship><data-warehouse><star-schema>","2021-03-31 11:34:27","362","1","2","68667789","<p>As you imply is your question, the star schema Fact table should use a composite key made up of the foreign keys it's referencing.
So I would say there are a couple issues with your situation that should be addressed.
First, a fact table probably shouldn't have a a column called Id, though it's not really going to hurt anything, it probably wouldn't ever be used to query by, so you are just adding extra data taking up disk space.
Second, and probably the answer you are looking for is, if you want a composite primary key on your fact table, you need to specify that in the Database Context.</p>
<pre><code>
protected override void OnModelCreating(ModelBuilder modelBuilder)
{
    modelBuilder.Entity&lt;FactBase&gt;()
        .HasKey(x =&gt; new { x.Id, x.DimDigitalesBuchISBN });
}

</code></pre>
<p>As I mentioned, you probably don't want to include the Fact.Id column in your PK but instead you would refer to more than one dimension like this:</p>
<pre><code>
protected override void OnModelCreating(ModelBuilder modelBuilder)
{
    modelBuilder.Entity&lt;FactBase&gt;()
        .HasKey(x =&gt; new { x.Dim1Id, x.Dim2Id, x.Dim3Id});
}

</code></pre>
<p>Where Dim1Id, Dim2Id and Dim3Id are the primary keys of your dimensions.
I should also mention that you need to remove the [Key] attribute from the Id field of your FactBase class.</p>
<p>refer to: <a href=""https://learn.microsoft.com/en-us/ef/core/modeling/keys?tabs=data-annotations"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/ef/core/modeling/keys?tabs=data-annotations</a></p>
"
"66870732","How to use with open to filter datafiles in python and create new file?","<p>i have huge csv and i tried to filter data using with open.</p>
<p>I know i can use FINDSTR on command line but i would like to use python to create a new file filtered or i would like to create a  pandas dataframe as output.</p>
<p>here is my code:</p>
<pre><code>outfile = open('my_file2.csv', 'a')
with open('my_file1.csv', 'r') as f:
 for lines in f:
         if '31/10/2018' in lines:
            print(lines)  
         outfile.write(lines)
</code></pre>
<p>The problem is that the output file generated is = input file and there is no filter(and the size of file is the same)</p>
<p>Thanks to all</p>
","<python><pandas><csv><bigdata><data-warehouse>","2021-03-30 12:12:45","121","1","1","66870789","<p>The problem with your code is the indentation of the last line. It should be within the if-statement, so only lines that contain <code>'31/10/2018'</code> get written.</p>
<pre><code>outfile = open('my_file2.csv', 'a')
with open('my_file1.csv', 'r') as f:
 for lines in f:
         if '31/10/2018' in lines:
            print(lines)  
            outfile.write(lines)
</code></pre>
<p>To filter using Pandas and creating a DataFrame, do something along the lines of:</p>
<pre><code>import pandas as pd
import datetime

# I assume here that the date is in a seperate column, named 'Date'
df = pd.read_csv('my_file1.csv', parse_dates=['Date']) 

# Filter on October 31st 2018
df_filter = df[df['Date'].dt.date == datetime.date(2018, 10, 31)]

# Output to csv
df_filter.to_csv('my_file2.csv', index=False)
</code></pre>
<p>(For very large csv's, look at the <code>pd.read_csv()</code> argument 'chunksize')</p>
<p>To use <code>with open(....) as f:</code>, you could do something like:</p>
<pre><code>import pandas as pd

filtered_list = []
with open('my_file1.csv', 'r') as f:
    for lines in f:
        if '31/10/2018' in lines:
            print(lines)
            # Split line by comma into list
            line_data = lines.split(',')
            filtered_list.append(line_data)

# Convert to dataframe and export as csv
df = pd.DataFrame(filtered_list)
df_filter.to_csv('my_file2.csv', index=False)
</code></pre>
"
"66865311","Data Factory cannot copy `csv` with comma after last column to sql data warehouse","<p>I have <code>CSV</code> files that I want to copy from a blob to DW, the <code>CSV</code> files have comma after the last column (see example below). Using ADF, I tried to copy csv files to a SQL table in DW. However, I got this error, which I think it's because of the last comma (as I have 15 columns):</p>
<p><a href=""https://i.stack.imgur.com/umIQt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/umIQt.png"" alt=""enter image description here"" /></a></p>
<p>few rows of csv file:</p>
<pre><code>Code,Last Trading Date,Bid Price,Bid Size,Ask Price,Ask Size,Last Price,Traded Volume,Open Price,High Price,Low Price,Settlement Price,Settlement Date,Implied Volatility,Last Trade Time,
BNH2021F,31/03/2021,37.750000,1,38.000000,1,,0,,,,37.750000,29/03/2021,,,
BNM2021F,30/06/2021,44.500000,6,44.700000,2,44.400000,4,44.300000,44.400000,44.300000,44.500000,29/03/2021,,15-55-47.000,
BNU2021F,30/09/2021,46.250000,2,47.000000,1,47.490000,2,47.490000,47.490000,47.490000,46.920000,29/03/2021,,15-59-10.000,
</code></pre>
<p>Note that <code>CSV</code>s are the original files and I can't change them. I also tried different <em>Quote</em> and <em>Escape</em> characters in the dataset and it didn't work.
Also I want to do this using ADF, not azure functions.</p>
<p>I couldn't find any solution to that, please help.</p>
<p><strong>Update</strong>:
It's interesting that the dataset preview works:
<a href=""https://i.stack.imgur.com/VZQ6j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VZQ6j.png"" alt=""enter image description here"" /></a></p>
","<azure><csv><azure-data-factory>","2021-03-30 05:35:45","1178","3","3","66867659","<p>I think you can use data flow to achieve that.</p>
<ol>
<li><p>Azure data factory will interpret last comma as a column with null value. So we can use Select activity to filter last column.
<a href=""https://i.stack.imgur.com/Z8Rwn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Z8Rwn.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Set mapping manually at <strong>sink</strong>.
<a href=""https://i.stack.imgur.com/HzA9u.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HzA9u.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Then we can sink to our DW or SQL table.
<a href=""https://i.stack.imgur.com/mz7qF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mz7qF.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>
"
"66865311","Data Factory cannot copy `csv` with comma after last column to sql data warehouse","<p>I have <code>CSV</code> files that I want to copy from a blob to DW, the <code>CSV</code> files have comma after the last column (see example below). Using ADF, I tried to copy csv files to a SQL table in DW. However, I got this error, which I think it's because of the last comma (as I have 15 columns):</p>
<p><a href=""https://i.stack.imgur.com/umIQt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/umIQt.png"" alt=""enter image description here"" /></a></p>
<p>few rows of csv file:</p>
<pre><code>Code,Last Trading Date,Bid Price,Bid Size,Ask Price,Ask Size,Last Price,Traded Volume,Open Price,High Price,Low Price,Settlement Price,Settlement Date,Implied Volatility,Last Trade Time,
BNH2021F,31/03/2021,37.750000,1,38.000000,1,,0,,,,37.750000,29/03/2021,,,
BNM2021F,30/06/2021,44.500000,6,44.700000,2,44.400000,4,44.300000,44.400000,44.300000,44.500000,29/03/2021,,15-55-47.000,
BNU2021F,30/09/2021,46.250000,2,47.000000,1,47.490000,2,47.490000,47.490000,47.490000,46.920000,29/03/2021,,15-59-10.000,
</code></pre>
<p>Note that <code>CSV</code>s are the original files and I can't change them. I also tried different <em>Quote</em> and <em>Escape</em> characters in the dataset and it didn't work.
Also I want to do this using ADF, not azure functions.</p>
<p>I couldn't find any solution to that, please help.</p>
<p><strong>Update</strong>:
It's interesting that the dataset preview works:
<a href=""https://i.stack.imgur.com/VZQ6j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VZQ6j.png"" alt=""enter image description here"" /></a></p>
","<azure><csv><azure-data-factory>","2021-03-30 05:35:45","1178","3","3","66868090","<p>You are using 15 columns and your destination is expecting 16. Add another column to your CSV or modify your DW.</p>
"
"66865311","Data Factory cannot copy `csv` with comma after last column to sql data warehouse","<p>I have <code>CSV</code> files that I want to copy from a blob to DW, the <code>CSV</code> files have comma after the last column (see example below). Using ADF, I tried to copy csv files to a SQL table in DW. However, I got this error, which I think it's because of the last comma (as I have 15 columns):</p>
<p><a href=""https://i.stack.imgur.com/umIQt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/umIQt.png"" alt=""enter image description here"" /></a></p>
<p>few rows of csv file:</p>
<pre><code>Code,Last Trading Date,Bid Price,Bid Size,Ask Price,Ask Size,Last Price,Traded Volume,Open Price,High Price,Low Price,Settlement Price,Settlement Date,Implied Volatility,Last Trade Time,
BNH2021F,31/03/2021,37.750000,1,38.000000,1,,0,,,,37.750000,29/03/2021,,,
BNM2021F,30/06/2021,44.500000,6,44.700000,2,44.400000,4,44.300000,44.400000,44.300000,44.500000,29/03/2021,,15-55-47.000,
BNU2021F,30/09/2021,46.250000,2,47.000000,1,47.490000,2,47.490000,47.490000,47.490000,46.920000,29/03/2021,,15-59-10.000,
</code></pre>
<p>Note that <code>CSV</code>s are the original files and I can't change them. I also tried different <em>Quote</em> and <em>Escape</em> characters in the dataset and it didn't work.
Also I want to do this using ADF, not azure functions.</p>
<p>I couldn't find any solution to that, please help.</p>
<p><strong>Update</strong>:
It's interesting that the dataset preview works:
<a href=""https://i.stack.imgur.com/VZQ6j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VZQ6j.png"" alt=""enter image description here"" /></a></p>
","<azure><csv><azure-data-factory>","2021-03-30 05:35:45","1178","3","3","66911237","<p>There is a simple solution to this.</p>
<p><strong>Step 1:</strong></p>
<p>Uncheck the &quot;First Row as header&quot; option in your source dataset
<img src=""https://i.stack.imgur.com/Fh6E3.png"" alt=""enter image description here"" /></p>
<p><strong>Step 2:</strong> Sink it first to another CSV file. in the sink csv dataset import schema like below. Copy activity will create a new CSV file with all clean 15 columns i.e. last extra comma will not be present in new csv file.</p>
<p><img src=""https://i.stack.imgur.com/FhdVa.png"" alt=""Click here to see image of mapping setting"" /></p>
<p><strong>Step 3:</strong> Copy from the newly created csv file with &quot;First row as header&quot;  checked and sick it to DW.</p>
"
"66861075","How to use hasUniqueness check in PyDeequ?","<p>I'm using <a href=""https://github.com/awslabs/python-deequ"" rel=""nofollow noreferrer"">PyDeequ</a> for data quality and I want to check the uniqueness of a set of columns. There is a Check method <code>hasUniqueness</code> but I can't figure how to use it.</p>
<p>I'm trying:</p>
<pre><code>check.hasUniqueness([col1, col2], ????) 
</code></pre>
<p>But what should we use here for the assertion function in place of <code>????</code>?</p>
<p>Has anyone tried the check <code>hasUniqueness</code> for a combination of columns?</p>
","<python><apache-spark><pyspark><data-quality><amazon-deequ>","2021-03-29 20:18:00","1250","1","1","66861855","<p><a href=""https://github.com/awslabs/python-deequ/blob/e358c835172a3a74c33a246550c2783be5df6123/pydeequ/checks.py#L242"" rel=""nofollow noreferrer""><code>hasUniqueness</code></a> takes a function that accepts an in/float parameter and returns a boolean :</p>
<blockquote>
<p>Creates a constraint that asserts any uniqueness in a single or
combined set of key columns. Uniqueness is the fraction of unique
values of a column(s)  values that occur exactly once.</p>
</blockquote>
<p>Here's an example of usage :</p>
<pre><code>df.show()
#+---+---+
#|  a|  b|
#+---+---+
#|foo|  1|
#|bar|  0|
#|baz|  1|
#|bar|  0|
#+---+---+
</code></pre>
<p>In this dataframe, the combination of columns <code>a</code> and <code>b</code> has 2 values that occur exactly once <code>(foo, 1)</code> and <code>(baz, 1)</code> so <code>Uniqueness = 0.5</code> here. Let's verify it using the check constraint :</p>
<pre><code>from pydeequ.checks import CheckLevel, Check
from pydeequ.verification import VerificationResult, VerificationSuite

result = VerificationSuite(spark).onData(df).addCheck(
    Check(spark, CheckLevel.Warning, &quot;test hasUniqueness&quot;)
        .hasUniqueness([&quot;a&quot;, &quot;b&quot;], lambda x: x == 0.5)
).run()

result_df = VerificationResult.checkResultsAsDataFrame(spark, result)

result_df.select(&quot;constraint_status&quot;).show()

#+-----------------+
#|constraint_status|
#+-----------------+
#|          Success|
#+-----------------+
</code></pre>
"
"66849754","Is it a good practice to use Microsoft power bi for visualizations of a retail data warehouse","<p>I completed my ETL part in SSIS. Now for data visualization i installed Power BI for dashboards and reports. Also i read research papers and I didn't find anyone related to power Bi. Lastly, Do i need to implement SSAS and SSRS package as well.</p>
","<ssis><powerbi><ssrs-2008><ssas><data-warehouse>","2021-03-29 06:48:32","72","-1","1","66851631","<p>Power BI's strength is data visualisation, and it is likely to be well suited for for using on top of you retail data warehouse.</p>
<p>I'm not sure which research paper you are referring to, but Microsoft has been topping Gartner's Magic Quadrant for Analytics and Business Intelligence Platform for several years now, followed by Tableau and Qlik.  If you are interested in reading further around the various platforms, you can download from <a href=""https://info.microsoft.com/ww-Landing-2021-Gartner-MQ-for-Analytics-and-Business-Intelligence-Power-BI.html?LCID=EN-US"" rel=""nofollow noreferrer"">https://info.microsoft.com/ww-Landing-2021-Gartner-MQ-for-Analytics-and-Business-Intelligence-Power-BI.html?LCID=EN-US</a></p>
<p>Power BI does not require SSAS or SSRS to run.  If you already have SSAS, Power BI can use SSAS as a data source, and it works very well with a live connection, alternatively you can model the semantic layer directly within Power BI itself.  Power BI, especially now Paginated reports are included is seen as a cloud based alternative to SQL Server Reporting Server</p>
"
"66845290","How to handle a MySQL table which is keep increasing quickly?","<p>I have a table named <code>transactions</code>, something like this:</p>
<pre><code>id | user_id | business_id | amount | tracking_code | status | created_at | updated_at
</code></pre>
<p>As you can see, this is a table which keeps all transactions. Currently it has over 50M rows and every day about 4k new rows get added to it. I'm worried about one or two next years that the business scaled up and I will end up with a really huge table.</p>
<p>Currently we have two indexes on this table for a better search performance. Also the engine is <code>innodb</code>.</p>
<p>Any idea how it should be handled generally?</p>
<p>In the side of hardware resources, I'm completely ok to increase the server hardware when needed. But I guess, the issue will be managing the data in the future and not the resources.</p>
","<mysql><database><performance><cluster-computing><data-management>","2021-03-28 19:18:21","98","0","3","66845345","<p>Disk space is cheap.  You have, what, 3.2 GB in data and some factor on top of that in indexes.  If the applications doesn't need all the data to be online, then you have the option of archive old data (dump then delete from table).  You can look into <a href=""https://dev.mysql.com/doc/refman/8.0/en/innodb-compression.html"" rel=""nofollow noreferrer"">compression</a> as an option.  Possibly in combination with some of the <a href=""https://dev.mysql.com/doc/refman/8.0/en/storage-engines.html"" rel=""nofollow noreferrer"">alternative storage engines</a></p>
"
"66845290","How to handle a MySQL table which is keep increasing quickly?","<p>I have a table named <code>transactions</code>, something like this:</p>
<pre><code>id | user_id | business_id | amount | tracking_code | status | created_at | updated_at
</code></pre>
<p>As you can see, this is a table which keeps all transactions. Currently it has over 50M rows and every day about 4k new rows get added to it. I'm worried about one or two next years that the business scaled up and I will end up with a really huge table.</p>
<p>Currently we have two indexes on this table for a better search performance. Also the engine is <code>innodb</code>.</p>
<p>Any idea how it should be handled generally?</p>
<p>In the side of hardware resources, I'm completely ok to increase the server hardware when needed. But I guess, the issue will be managing the data in the future and not the resources.</p>
","<mysql><database><performance><cluster-computing><data-management>","2021-03-28 19:18:21","98","0","3","66845371","<p>I would not worry about application performance with 1 billion rows on a machine that can keep the indexes in memory.</p>
<p>However, I would suggest making the id columns as BigInt if you know that the table is growing at a fast pace as a 32-bit integer will be limited to 2^31-1= 2,147,483,647 rows</p>
<p>The performance of your table and search engine depends on:</p>
<ol>
<li>How many join those queries do on this particular table?</li>
<li>How well your indexes are set up? Apparently good</li>
<li>How much RAM is in the machine hosting the DB?</li>
<li>Speed and number of processors related to it?</li>
<li>Size of the row/amount of data returned in the queries.</li>
</ol>
"
"66845290","How to handle a MySQL table which is keep increasing quickly?","<p>I have a table named <code>transactions</code>, something like this:</p>
<pre><code>id | user_id | business_id | amount | tracking_code | status | created_at | updated_at
</code></pre>
<p>As you can see, this is a table which keeps all transactions. Currently it has over 50M rows and every day about 4k new rows get added to it. I'm worried about one or two next years that the business scaled up and I will end up with a really huge table.</p>
<p>Currently we have two indexes on this table for a better search performance. Also the engine is <code>innodb</code>.</p>
<p>Any idea how it should be handled generally?</p>
<p>In the side of hardware resources, I'm completely ok to increase the server hardware when needed. But I guess, the issue will be managing the data in the future and not the resources.</p>
","<mysql><database><performance><cluster-computing><data-management>","2021-03-28 19:18:21","98","0","3","66849044","<p>Don't worry about how many CPUs and their speed.  That is rarely the limiting factor in MySQL.</p>
<p>Do you need either <code>created_at</code> or <code>updated_at</code>?</p>
<p>Don't bother with compression; it is unlikely to be worth it.</p>
<p>Do use smaller datatypes where practical (and conservative).</p>
<p>Stick with InnoDB.  There are many reasons; I won't repeat them here.</p>
<p>Please show us <code>SHOW CREATE TABLE</code> and some of the critical queries.  We may have more tips.</p>
"
"66821389","Improve speed of SQL view joining multiple tables, all with primary keys","<p>I have a fact table that contains 3 mln records. The view takes about 8 minutes to run and joins to about 9 other tables (all have PK and joins are done on PKs) and there is also a small date exclusion.</p>
<pre><code>SELECT k.Field
,l.Field 
--...
,z.field 
FROM dbo.Fact f
join DimRegion dr on dr.RegionKey = f.RegionKey AND dr.RegionName = 'UK' 
join DimDate dd on dd.DateKey = f.DateKey and dd.Date &gt;= '2020-07-01'
join DimTable dt on dt.TKey = f.TKey and dt.isExcluded = 0
left join DimK dk on f.KKey = dk.KKey
--...
left join DimZ dz on f.ZKey = dz.ZKey
</code></pre>
<p>The query takes 90 seconds if I comment out all LEFT JOINs, and with each adding about 45 seconds time to run. All the joins are made using integer values.</p>
<p>I have rebuilt all the indexes that were fragmented, created index on the Date but this didn't help at all.
Here is the execution plan with the highest numbers:
<a href=""https://i.stack.imgur.com/H72aJ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H72aJ.jpg"" alt=""enter image description here"" /></a></p>
<p>Is there anything else I could do to speed it up or is the volume of data too large and that's why it takes so long?</p>
<p>here's the full plan (thanks @eshirvana):
<a href=""https://www.brentozar.com/pastetheplan/?id=S1tphqoEu"" rel=""nofollow noreferrer"">https://www.brentozar.com/pastetheplan/?id=S1tphqoEu</a></p>
","<sql><query-optimization><data-warehouse>","2021-03-26 17:08:33","360","-1","2","66835822","<p>Why are you trying to optimize a query that returns 2.3M rows to the client?</p>
<p>You should be filtering and aggregating the results on the server.  This query spent 3 minutes just to send all the results to the client, in addition to the 1.5 minutes of IO waits and 1 minute of CPU time.</p>
<pre><code>        &lt;WaitStats&gt;
          &lt;Wait WaitType=&quot;ASYNC_NETWORK_IO&quot; WaitTimeMs=&quot;171313&quot; WaitCount=&quot;189426&quot; /&gt;
          &lt;Wait WaitType=&quot;IO_COMPLETION&quot; WaitTimeMs=&quot;66082&quot; WaitCount=&quot;16620&quot; /&gt;
          &lt;Wait WaitType=&quot;IO_QUEUE_LIMIT&quot; WaitTimeMs=&quot;49006&quot; WaitCount=&quot;3433&quot; /&gt;
          &lt;Wait WaitType=&quot;RESERVED_MEMORY_ALLOCATION_EXT&quot; WaitTimeMs=&quot;889&quot; WaitCount=&quot;496442&quot; /&gt;
          &lt;Wait WaitType=&quot;SOS_SCHEDULER_YIELD&quot; WaitTimeMs=&quot;430&quot; WaitCount=&quot;5574&quot; /&gt;
          &lt;Wait WaitType=&quot;RESOURCE_GOVERNOR_IDLE&quot; WaitTimeMs=&quot;162&quot; WaitCount=&quot;19&quot; /&gt;
          &lt;Wait WaitType=&quot;MEMORY_ALLOCATION_EXT&quot; WaitTimeMs=&quot;80&quot; WaitCount=&quot;44521&quot; /&gt;
        &lt;/WaitStats&gt;
        &lt;QueryTimeStats CpuTime=&quot;75783&quot; ElapsedTime=&quot;481785&quot; /&gt;
</code></pre>
<p>And why is your fact table a clustered index table instead of a <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-overview?view=sql-server-ver15#:%7E:text=Clustered%20columnstore%20index.%20A%20clustered%20columnstore%20index%20is,a%20btree%20list%20of%20IDs%20for%20deleted%20rows."" rel=""nofollow noreferrer"">clustered columnstore</a>?  Once you stop returning all the rows and columns to the client, a clustered columnstore should perform much better.</p>
"
"66821389","Improve speed of SQL view joining multiple tables, all with primary keys","<p>I have a fact table that contains 3 mln records. The view takes about 8 minutes to run and joins to about 9 other tables (all have PK and joins are done on PKs) and there is also a small date exclusion.</p>
<pre><code>SELECT k.Field
,l.Field 
--...
,z.field 
FROM dbo.Fact f
join DimRegion dr on dr.RegionKey = f.RegionKey AND dr.RegionName = 'UK' 
join DimDate dd on dd.DateKey = f.DateKey and dd.Date &gt;= '2020-07-01'
join DimTable dt on dt.TKey = f.TKey and dt.isExcluded = 0
left join DimK dk on f.KKey = dk.KKey
--...
left join DimZ dz on f.ZKey = dz.ZKey
</code></pre>
<p>The query takes 90 seconds if I comment out all LEFT JOINs, and with each adding about 45 seconds time to run. All the joins are made using integer values.</p>
<p>I have rebuilt all the indexes that were fragmented, created index on the Date but this didn't help at all.
Here is the execution plan with the highest numbers:
<a href=""https://i.stack.imgur.com/H72aJ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H72aJ.jpg"" alt=""enter image description here"" /></a></p>
<p>Is there anything else I could do to speed it up or is the volume of data too large and that's why it takes so long?</p>
<p>here's the full plan (thanks @eshirvana):
<a href=""https://www.brentozar.com/pastetheplan/?id=S1tphqoEu"" rel=""nofollow noreferrer"">https://www.brentozar.com/pastetheplan/?id=S1tphqoEu</a></p>
","<sql><query-optimization><data-warehouse>","2021-03-26 17:08:33","360","-1","2","66868402","<p>thanks for all your help and ideas!
I have rebuilt the stats, and focused on indexes - especially the one to DimDate.
What has helped was changing this</p>
<pre><code>...f.DateKey and dd.Date &gt;= '2020-07-01'
</code></pre>
<p>to</p>
<pre><code>...f.DateKey and f.DateKey &gt;= 20200701
</code></pre>
<p>I have realised that after @Stu's comment to use <em>force order</em> (I've managed to figure out which index was failing me) it took a minute and half off, but fully skipping that index took half time (3.5 mins less) and the remaining time, as mentioned by @David Browne 3 mins are spent on sending full data to the client, I'm happy with that as most of the time it will be filtered by customer and dates so it actually takes seconds now.
Thank you once again.</p>
"
"66816023","Common format to represent a data mapping allowing duplicated column/key names","<p><strong>Context:</strong><br>
I actually have an extracting task in <strong>PHP</strong> which executes a <strong>SQL</strong> query and generates a <strong>CSV</strong> file. The CSV columns names are based on the query column names.<br>
The CSV must contain a lot of columns and <strong>some column names appear multiple times</strong> (2 or 3 times). The team processing this extract need all of the columns because the value can change from a duplicate column to an other, and the order is important.</p>
<p><strong>From SQL to CSV:</strong><br>
I can set aliases in the SQL query <code>SELECT</code> statement with duplicated names:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT
    col_a AS 'duplicate',
    col_b AS 'some_col',
    '' AS 'duplicate',
    col_c AS 'other_col',
...
</code></pre>
<p>The query is executed with <strong>PDO</strong> and fetched with a classic <code>PDO::FETCH_ASSOC</code> which returns an associative array. As the keys of an array are unique, I lose the duplicated columns:</p>
<pre class=""lang-php prettyprint-override""><code>[
    'duplicate' =&gt; 'something',
    'some_col'  =&gt; 'value',
    'other_col' =&gt; 'other value'
    ...
]
</code></pre>
<p><strong>Failing solutions:</strong><br>
Using the <code>PDO::FETCH_NAMED</code> fetch mode would not work because I would lose the order of the columns, requiring much more work to make it work in a reusable way (the task is a generic SQL/CSV extract).</p>
<p>I can rename the aliases in the <code>SELECT</code> clause to be unique but then I wouldn't be able to use the column names for the CSV.<br>
<strong>Except if I have a mapping of the SQL columns to the CSV columns.</strong></p>
<p>I thought about a format like JSON but this won't work as JSON data <em>should not</em> contain duplicated keys, and I would lose them while reading them in PHP anyway:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;CSV_COL&quot;: &quot;SQL_COL&quot;,
    &quot;OTHER_CSV&quot;: &quot;OTHER_SQL&quot;,
    &quot;CSV_COL&quot;: &quot;something_else&quot;
}
</code></pre>
<p>Is there a simple format that could allow for duplicate column/key names that PHP would not delete while reading it ?</p>
","<php><duplicates><export-to-csv><data-representation>","2021-03-26 11:25:29","78","-1","1","67088667","<p>I solved this with a custom format. This is not the ideal solution as I would prefer a more standard one but it works. Here are the details:</p>
<h3>Custom format</h3>
<p>First, a form field accepts text input like the following:</p>
<pre><code>csv_column:sql_column

Name of CSV column   :   &quot;some string&quot;
 csv_column :
</code></pre>
<p>Rules of the format:</p>
<ul>
<li>each line maps a <strong>CSV column</strong> to a <strong>SQL column / value</strong></li>
<li>blank lines are ignored but are fine</li>
<li><strong>CSV</strong> columns are separated from <strong>SQL</strong> columns with a <code>:</code></li>
<li>you can add as many whitespaces as you want around column names</li>
<li>you can define a default string value for the <strong>SQL</strong> side by enclosing the value with <code>'</code> or <code>&quot;</code></li>
<li>if a <strong>CSV</strong> column must be empty, the <strong>SQL</strong> part is optional</li>
</ul>
<h3>Format parsing</h3>
<p>Then I wrote a function which will transform this string into a multidimensionnal array:</p>
<pre class=""lang-php prettyprint-override""><code>/**
 * Parse CSV/SQL column mapping.
 * @param string $textMapping column mapping in text format
 * @return string[][]|null each array has a &quot;CSV&quot; &amp; &quot;SQL&quot; key with corresponding column/value and a &quot;type&quot; key to
 *                         differentiate SQL columns from SQL values
 */
function getColumnMapping(string $textMapping): ?array
{
    // No mapping
    if (trim($textMapping) === '') {
        return null;
    }

    /*
     * RegEx explanation:
     *      \h*(.*\S)   optional whitepaces following by a CSV column name
     *                  (anything but does not end with whitespace)
     *      \h*:\h*     a colon optionally surrounded by whitespaces
     *      ([^\v]*)\h* an optional SQL column/value, optionally followed by whitespaces
     *                  (anything but line breaks)
     *      (?:\R|$)    a line break or the end of the string
     *
     * CSV parts are at index 1 and SQL parts are at index 2
     */
    preg_match_all('~\h*(.*\S)\h*:\h*([^\v]*)\h*(?:\R|$)~', $textMapping, $matches);

    // No mapping found
    if ([] === $matches[0]) {
        return null;
    }

    $mapping = [];

    for ($i = 0; $i &lt; count($matches[0]); $i++) {
        $mappingItem = [
            'CSV' =&gt; $matches[1][$i],
            'SQL' =&gt; $matches[2][$i],
            'type' =&gt; 'column',
        ];

        if ('' === $mappingItem['SQL']) {
            // Empty: empty value
            $mappingItem['type'] = 'value';
        } elseif (1 === preg_match('~^(\'|&quot;)(.*)\1$~', $mappingItem['SQL'], $match)) {
            // Enclosed by quotes: string value
            $mappingItem['SQL'] = $match[2];
            $mappingItem['type'] = 'value';
        }

        $mapping[] = $mappingItem;
    }

    return $mapping;
}
</code></pre>
<p>The previous example would result in the following array:</p>
<pre class=""lang-php prettyprint-override""><code>[
    [
        'CSV'  =&gt; 'csv_column',
        'SQL'  =&gt; 'sql_column',
        'type' =&gt; 'column',
    ],
    [
        'CSV'  =&gt; 'Name of CSV column',
        'SQL'  =&gt; 'some string',
        'type' =&gt; 'value',
    ],
    [
        'CSV'  =&gt; 'csv_column',
        'SQL'  =&gt; '',
        'type' =&gt; 'value',
    ],
]
</code></pre>
<h3>Building the CSV file</h3>
<p>Finally, after executing the SQL query, each result is passed to a function to build the data row:</p>
<pre class=""lang-php prettyprint-override""><code>/**
 * Build a CSV data line based on a column mapping
 * @param array $resultRow SQL result row
 * @param array $mapping CSV/SQL column mapping
 * @return array formatted data
 */
function buildRowFromColumnMapping(array $resultRow, array $mapping): array
{
    $row = [];

    foreach ($mapping as $mappingItem) {
        if ($mappingItem['type'] === 'value') {
            $row[] = $mappingItem['SQL'];
        } elseif (false === array_key_exists($mappingItem['SQL'], $resultRow)) {
            throw new \LogicException(sprintf('Column &quot;%s&quot; not found.', $mappingItem['SQL']));
        } else {
            $row[] = $resultRow[$mappingItem['SQL']];
        }
    }

    return $row;
}
</code></pre>
<p>The result is a unidimensionnal array you can send to anything you want (<a href=""https://www.php.net/manual/en/function.fputcsv"" rel=""nofollow noreferrer"">fputcsv()</a>, specific lib, ...).</p>
"
"66812964","How to match chinese characters in Clickhouse","<p>I tried to extract chinese characters from a string using <a href=""https://clickhouse.tech/docs/en/"" rel=""nofollow noreferrer"">clickhouse</a> SQL.</p>
<p>I use:</p>
<pre class=""lang-sql prettyprint-override""><code>select extractAll('dkfdfjsd1234中文字符串', '[a-zA-Z]')
</code></pre>
<p>It could successfully returns:</p>
<pre><code>['d','k','f','d','f','j','s','d']
</code></pre>
<p>Now I want to extract chinese like that, I tried:</p>
<pre class=""lang-sql prettyprint-override""><code>select extractAll('dkfdfjsd1234中文字符串', '[\u4e00-\u9fa5]')
</code></pre>
<p>It returns error.</p>
<pre><code>Code: 427, e.displayText() = DB::Exception: OptimizedRegularExpression: cannot compile re2: [\u4e00-\u9fa5], error: invalid escape sequence: \u. Look at https://github.com/google/re2/wiki/Syntax for reference. Please note that if you specify regex as an SQL string literal, the slashes have to be additionally escaped. For example, to match an opening brace, write '\(' -- the first slash is for SQL and the second one is for regex (version 20.8.14.4 (official build))
</code></pre>
","<sql><database><data-warehouse><clickhouse>","2021-03-26 07:57:34","638","0","1","66819509","<p>To match Unicode point use <a href=""https://www.regular-expressions.info/unicode.html"" rel=""nofollow noreferrer"">\x{FFFF}</a>:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT extractAll('dkfdfjsd1234中文字符串', '[\\x{4e00}-\\x{9fa5}]') AS result

/*
┌─result─────────────────────┐
│ ['中','文','字','符','串'] │
└────────────────────────────┘
*/
</code></pre>
"
"66791896","Is it possible to Grant Roles to Users in a Stored Procedure?","<p>I am working on DB2 Data Warehouse. I am trying to write a stored procedure that involves Granting a input user a role.</p>
<pre><code>CREATE OR REPLACE MYSCHEMA.PROCEDURE SP_CHECKIN_USER (
IN userid varchar(9),
IN groupid varchar(10)
) LANGUAGE SQL
BEGIN

-- do some work here
GRANT ROLE groupid TO USER userid;

END
</code></pre>
<p>When I try to run this I get the error:</p>
<pre><code>&quot;GROUPID&quot; is undefined name .. SQLCODE=-204, SQLSTATE=42704, DRIVER=...
Run of routine failed.
</code></pre>
","<db2><data-warehouse>","2021-03-25 01:43:22","126","0","1","66794962","<p>Yes it is possible if you use dynamic SQL for the <code>grant role</code> inside the strored procedure.</p>
<p>For example:</p>
<pre><code>   declare v_sql varchar(1024);
   set v_sql = 'grant role '||trim(p_groupid)||' to user '||trim(p_userid)||' ';
   execute immediate v_sql;
</code></pre>
<p>Remember that all other rules still apply, the role must pre-exist, the user must pre-exist, the authid must have all relevant permissions etc.</p>
"
"66763756","Remove duplicates from fact table to calculate measure correctly","<p>I'm very new to data warehousing and dimensional modelling. For a uni project I started out with a database that I need to turn into a data warehouse for analyses. To end up with a clean star schema, I had to denormalize a few tables into 1 fact table. The downside to this is the amount of redundancy.</p>
<p>Below is a part of the data from the fact table:</p>
<p><a href=""https://i.stack.imgur.com/SIneR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SIneR.png"" alt=""enter image description here"" /></a></p>
<p>A voyage consists of multiple shipments, and a shipment can consist of multiple different items. In this example, containers 1-2000 of shipment 1 contain item 3, and containers 2001-5000 contain item 1. The total amount of containers for this shipment is 5000, obviously. However, for data analysis purposes, I need to calculate a measure for the total amount of containers per voyage. This presents a problem with the current fact table, because I have a record for each different item. So for voyage 1, the actual total amount should be 9200, but because of the duplication I'll end up with 19400, leading to an incorrect measure.</p>
<p>I need to find a way to get rid of the duplicates in my calculation, but I can't find a way to do so. Any help would be much appreciated.</p>
","<sql><tsql><data-warehouse><cube>","2021-03-23 13:05:43","624","0","1","66764047","<p>What you'll need to do is group by your shipments (<a href=""https://learn.microsoft.com/en-us/sql/t-sql/queries/with-common-table-expression-transact-sql?view=sql-server-ver15"" rel=""nofollow noreferrer"">CTE</a>, <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/performance/subqueries?view=sql-server-ver15"" rel=""nofollow noreferrer"">inner query</a>, <a href=""https://learn.microsoft.com/en-us/sql/t-sql/queries/select-into-clause-transact-sql?view=sql-server-ver15"" rel=""nofollow noreferrer"">temp table</a>, etc) to get the number of containers per shipment, then group by your voyages to get the number of containers per voyage.</p>
<p>Here's an example with an inner query:</p>
<pre><code>SELECT voyage_id, SUM(num_ship_containers) AS num_voyage_containers
FROM (
  SELECT voyage_id, shipment_id, MAX(container_end) AS num_ship_containers
  FROM ShippingWarehouse
  GROUP BY voyage_id, shipment_id
) AS ship_data
GROUP BY voyage_id;
</code></pre>
<blockquote>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>voyage_id</th>
<th>num_voyage_containers</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>9200</td>
</tr>
</tbody>
</table>
</div></blockquote>
<p><a href=""http://sqlfiddle.com/#!18/0f87f/6"" rel=""nofollow noreferrer"">Try it out!</a></p>
"
"66702353","Is it possible to get lineage metadata from the pipeline in my Data Fusion Action plugin?","<p>I'm trying to get data lineage metadata like data source/schema and data target/schema in a custom Action plugin which gets executed after the successful run of the other steps in the pipeline.</p>
<p>I have a basic Action plugin that executes but I'm having trouble finding a way to get the metadata I'm after.</p>
<p>The use case I'm working on is pushing data lineage into a third party data governance tool.</p>
<p>I would very much appreciate if someone could point me in the right direction!</p>
","<java><google-cloud-platform><google-cloud-data-fusion><cdap><data-governance>","2021-03-19 04:02:38","430","1","1","66852007","<p>As was suggested in my comment, you might consider to use CDAP <em><a href=""https://cdap.atlassian.net/wiki/spaces/DOCS/pages/480346522/System+Metadata"" rel=""nofollow noreferrer"">system metadata</a></em> inventory to extract the particular property for the desired entity via CDAP existed RESTfull API methods by sending appropriate HTTP request as explained in CDAP <a href=""https://cdap.atlassian.net/wiki/spaces/DOCS/pages/477692187/Metadata+Microservices"" rel=""nofollow noreferrer""><em>Metadata Microservices</em></a> documentation. Said this entity properties can also depict <a href=""https://cdap.atlassian.net/wiki/spaces/DOCS/pages/477692187/Metadata+Microservices#Field-Lineage-Summary"" rel=""nofollow noreferrer"">lineage</a> of dataset fields returning the result in JSON format.</p>
<p>However, adjusting appropriate HTTP method mostly depends on the particular use case, therefore feel free to further contribute and share your further discovering.</p>
"
"66700458","How to connect DbVisualizer to OAC ADW","<p><strong>OAC ADW</strong> = Oracle Analytics Cloud Autonomous Data Warehouse. Our OAC ADW admin provided us <strong>.zip wallet files</strong> for connection, however this is not natively supported by <strong>DbVisualizer</strong>. How can I make the connection?</p>
","<sql><data-warehouse><dbvisualizer><oracle-analytics>","2021-03-18 23:18:17","250","0","1","66700459","<p>Below are the notes I developed after getting help from multiple sources and jumping through all the hoops:</p>
<pre><code>1. Install [DbVisualizer](https://www.dbvis.com/)
    a. Need admin access on the machine
    b. This guide uses DbVisualizer Pro 12.0.2 (Build #3211) with Java version 1.8.0_275 included

2. Add Oracle drivers
    a. Visit [Oracle download page](https://www.oracle.com/database/technologies/jdbc-ucp-122-downloads.html) to get three Java packages (Oracle account required): oraclepki.jar, osdt_cert.jar, and osdt_core.jar
    b. Save to DbVisualizer Oracle driver path (e.g. C:\Program Files\DbVisualizer\jdbc\oracle)

3. Configure [Oracle connection files](https://www.orafaq.com/wiki/Sqlnet.ora)
    a. Get .zip wallet files from OAC ADW admin
    b. Unzip and save to a safe path (e.g. C:\ADW): cwallet.sso, ewallet.p12, keystore.jks, ojdbc.properties, sqlnet.ora, tnsnames.ora, truststore.jks
    c. Edit &quot;DIRECTORY&quot; part in sqlnet.ora (Oracle database configuration file) to match the wallet file directory:
        WALLET_LOCATION = (SOURCE = (METHOD = file) (METHOD_DATA = (DIRECTORY=&quot;C:\ADW&quot;)))
        SSL_SERVER_DN_MATCH=yes
    d. Edit system environment variable to set TNS_ADMIN = &quot;C:\ADW&quot; to tell DbVisualizer where to find tnsnames.ora file (configuration file that defines Oracle database address)
    e. Note: if you machine already has TNS_ADMIN set, check if that path already contains a tnsnames.ora (&quot;remote tnsnames.ora file&quot;). If so
        i. If your OAC ADW admin allows you to edit that file, copy content of the tnsnames.ora file in your wallet file path (&quot;local tnsnames.ora file&quot;) and append to the end of that remote file. Don't change your machine's TNS_ADMIN
        ii. If you can't edit that remote file, copy its content and append to the local file. Then proceed to set TNS_ADMIN = &quot;C:\ADW&quot;
    f. Repeat the process for every OAC ADW environment (prod, test, etc)

4. Configure DbVisualizer
    a. Launch DbVisualizer. If the JDBC Driver Finder opens automatically, also close it (you already manually saved the Oracle drivers you need for now)
    b. Create a new connection to OAC ADW using the Connection Wizard
        i. Select Database Driver = &quot;Oracle Thin&quot;, Service = &quot;TNS&quot;
        ii. You should be able to pick &quot;TNS Alias&quot; from a dropdown list that contains the OAC ADW you're looking for. If not, check Step 3. Note: if you have DbVisualizer open before Step 3, you must re-launch it to have the new TNS_ADMIN in effect. Also after setting TNS_ADMIN you must click &quot;OK&quot; to apply the change
        iii. Enter Database Userid and Password your OAC ADW admin provided, &quot;Finish&quot;
        iv. Note: before you try the connection, you must go to &quot;Properties&quot; &gt; &quot;Driver Properties&quot;, scroll to parameter &quot;oracle.net.wallet_location&quot;, enter &quot;C:\ADW&quot;, and click &quot;Apply&quot;
        v. If using the filepath alone does not work, use the following string instead: 
            (SOURCE=(METHOD=file)(METHOD_DATA=(DIRECTORY=&quot;C:\ADW&quot;)))

5. Now you should be all set. If not, check all steps and a few other things:
    a. You have updated to the newest version of DbVisualizer
    b. If mixed Oracle drivers are installed, uninstall/remove other ones
    c. Check if environment variable ORACLE_HOME is defined. This is because tnsnames.ora file could be found in two places: path defined by TNS_ADMIN, as well as %ORACLE_HOME%\network\admin
</code></pre>
"
"66670804","Question about MongoDB data modelling with one-to-many relationship","<p>I am designing a course review system and I have Review documents that refer to a review made for a course by a user.</p>
<p>I also have course documents and I am having trouble designing a data model that satisfies my needs.</p>
<p>The relation between course and review is one to many.</p>
<p>I have 2 options:</p>
<ol>
<li>Embed Course in their Review objects which are many:</li>
</ol>
<p>In this case, course objects do not exist on their own but I have to allow my users to search through courses so in that case, I would need to run a query on Review objects to search for courses.</p>
<ol start=""2"">
<li>If I store courses in a separate collection and reference through has_many: reviews</li>
</ol>
<p>I also need to find the reviews for a course when the user clicks on a course after searching and with this design, I will need to run a query when retrieving reviews for a course and also when I am displaying the review, I will need to display course too so I would need to run another query.</p>
<p>What would be the best design in this case? I thought if I could find a way to keep Courses as a separate entity and still embed it inside Reviews as well.</p>
<p>Edit: I have decided to embed reviews inside courses as suggested but I have some new questions now:</p>
<p>For following questions please assume that I have embedded reviews inside course.</p>
<ol>
<li>When inserting reviews, should I do it in ReviewController by finding its course by id and inserting inside its reviews array?</li>
<li>When a user searches for a course, I would like to return last 10 reviews with the course information instead of all reviews because it may slow down fetching the search results. How can I achieve this after putting all reviews inside courses as you mentioned?</li>
<li>I also have users who enter the reviews (one to many again), I am planning to show recent reviews with usernames, is there a way to embed only username field of user collection inside review?</li>
<li>To find a certain users reviews I will need to iterate over all courses, right? It is not a very common query but is there a way to make it faster with an index?</li>
</ol>
","<database><mongodb><mongoose><mongodb-query><data-modeling>","2021-03-17 10:01:15","347","0","1","66674604","<p>Modeling suggestions for - <em>course has reviews and reviews are made by users</em>.</p>
<blockquote>
<p>I have decided to embed reviews inside courses as suggested but I have
some new questions now:</p>
</blockquote>
<br>
<blockquote>
<ol>
<li>When inserting reviews, should I do it in ReviewController by finding its course by id and inserting inside its reviews array?</li>
</ol>
</blockquote>
<p>You are <em>updating</em> a course collection document. The <a href=""https://docs.mongodb.com/manual/reference/method/db.collection.update/index.html"" rel=""nofollow noreferrer"">update</a> query filter will be by the course id (or name) - and you will <code>$push</code> (<a href=""https://docs.mongodb.com/manual/reference/operator/update/push/index.html"" rel=""nofollow noreferrer"">$push</a> is an update operator) a review sub-document (or embedded document) into the course document's <code>reviews</code> <em>array</em> field.</p>
<p>The <code>course</code> collection document can be like this:</p>
<pre><code>{
  _id: &lt;ObjectId&gt;,
  name: &lt;string&gt;,
  description: &lt;string&gt;,
  reviews: [
     { _id: &lt;some id&gt;, date: &lt;date&gt;, content: &lt;string&gt;, user: &lt;...&gt; },
     { _id: &lt;some id&gt;, date: &lt;date&gt;, content: &lt;string&gt;, user: &lt;...&gt; },
      ...
  ]
}
</code></pre>
<p>The <code>reviews</code> sub-document can have both user's name and id <em>or</em> just one of them.</p>
<br>
<blockquote>
<ol start=""2"">
<li>When a user searches for a course, I would like to return last 10 reviews with the course information instead of all reviews because it
may slow down fetching the search results. How can I achieve this
after putting all reviews inside courses as you mentioned?</li>
</ol>
</blockquote>
<p>You can make this an Aggregation query. For example,</p>
<pre><code>db.course.aggregate([
  { $match: { _id: &lt;some course id&gt;  } },    // or, this can be filter by course name field
  { $addFields: {
       latestTenReviews: {
           // use $function aggregation operator to sort the reviews by the date field descending and 
           // limit to first ten array elements
       }
    },
])
</code></pre>
<p>The <code>$match</code> stage can use index defined upon the <code>_id</code> (it has by default a unique index) <em>or</em> define an index on the course''s <code>name</code> field.</p>
<br>
<blockquote>
<ol start=""3"">
<li>I also have users who enter the reviews (one to many again), I am planning to show recent reviews with usernames, is there a way to
embed only username field of user collection inside review?</li>
</ol>
</blockquote>
<p>Yes, you store user information in a review as shown in the previous point (2). You can store only the id <em>or</em> name <em>or</em> both, depending upon your need. When the reviews are queried for a course, the user names will show, if the names are stored. In case names are not stored, you may have to use the <a href=""https://docs.mongodb.com/manual/reference/operator/aggregation/lookup/index.html"" rel=""nofollow noreferrer"">$lookup</a> aggregation stage to make a &quot;join&quot; operation to get the user details, like name.</p>
<br>
<blockquote>
<ol start=""4"">
<li>To find a certain users reviews I will need to iterate over all courses, right? It is not a very common query but is there a way to
make it faster with an index?</li>
</ol>
</blockquote>
<p>You can define an index on the user field of the <code>reviews</code> array field's sub-document. Indexes on array fields are called as <a href=""https://docs.mongodb.com/manual/core/index-multikey/index.html"" rel=""nofollow noreferrer"">Multikey indexes</a>. The query with user field as filter criteria will benefit from this index.</p>
"
"66654812","Can Stats per partition prevent parameter sniffing problem when data varies by wide margin in partitions?","<p>Currently we have a Datawarehouse that is holding data from multiple tenants. SQL server is on version 2019. Same schema for all the tenant databases and the data from all the tenants is consolidated in the Datawarehouse. Data is partitioned in the datawarehouse on Tenant basis. We have parameter sniffing problem with the new dashboard as the data varies a lot between the tenants. Some tenants have data less than 10000 rows and a couple of tenants have data ranging up to 5 million rows. Due to this, dashboard performance is bad for large tenants if the execution plan is built based on a smaller tenant.</p>
<p>Suggestions on the internet are available asking to use Recompile hint or Optimize for hint etc. But I have a doubt on the basics of this parameter sniffing. As statistics are maintained by the SQL server at partition level, is this statistics information not used to see if the plan built is right for a new run time value? Before executing, are stats ever compared for the plans built on compile time and run time to see if they are valid and the associated plan is valid?</p>
<p>Kindly advise.</p>
","<sql-server><data-warehouse><database-partitioning><parameter-sniffing>","2021-03-16 11:59:39","57","0","1","66835607","<ol>
<li>Embed the Partition number or the TenantID key in the query text</li>
</ol>
<p>Parameters are for when you want shared, reused query plans.  Hard-coding the criteria that cause query plans to vary is the basic right answer here.</p>
<p>And even though &quot;As much as possible, we are refraining from using Dynamic SQL in the code&quot;, you should make an exception here.</p>
<ol start=""2"">
<li>Use OPTION RECOMPILE</li>
</ol>
<p>If you don't end up spending too much time in query optimization, this is almost as good.  Or</p>
<ol start=""3"">
<li>Add a comment into the query that varies by tenant or tenant size to get a partitioned plan cache.  This is also useful for correlating queries to the code paths that generate them.  eg</li>
</ol>
<pre><code>/* Dashboard: Sales Overview
   Visual: Total Sales
   TenantID: 12345    */   
select sum(Sales) TotalSales
from T
where TenantId = @tenantId
</code></pre>
"
"66609204","How to leave only rows that meet a specific condition in R","<p>I have a data frame that contains around 700 cases with 1800 examinations. Some cases underwent several different modalities. I want to leave only one examination result based on the specific condition of the modality.</p>
<p>Here is a dummy data frame:</p>
<pre><code>df &lt;- data.frame (ID = c(&quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;4&quot;, &quot;5&quot;, &quot;5&quot;),
                  c1 = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;A&quot;, &quot;C&quot;, &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;),
                  x1 = c(5, 4, 5, 3, 1, 3, 4, 2, 3, 5),
                  x2 = c(4, 3, 7, 9, 1, 2, 4, 7, 5, 0))
</code></pre>
<p>There are five cases with 10 exams. [c1] is the exam modality (condition), and the results are x1 and x2.</p>
<p>I want to leave only one row based on the following condition:</p>
<blockquote>
<p>C &gt; B &gt; A</p>
</blockquote>
<p>I want to leave the row with C first; if not, leave the row with B; If C and B are absent, leave the row with A.</p>
<p>Desired output:</p>
<pre><code>output &lt;- data.frame (ID = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;),
                      c1 = c(&quot;C&quot;, &quot;C&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;),
                      x1 = c(5, 1, 3, 2, 5),
                      x2 = c(7, 1, 2, 7, 0))
</code></pre>
","<r><dataframe><conditional-statements><data-management>","2021-03-13 01:14:39","42","0","2","66609353","<p>Here is one approach:</p>
<pre><code>df.srt &lt;- df[order(df$c1, decreasing=TRUE), ]
df.spl &lt;- split(df.srt, df.srt$ID)
first &lt;- lapply(df.spl, head, n=1)
result &lt;- do.call(rbind, first)
result
#   ID c1 x1 x2
# 1  1  C  5  7
# 2  2  C  1  1
# 3  3  A  3  2
# 4  4  B  2  7
# 5  5  C  5  0
</code></pre>
"
"66609204","How to leave only rows that meet a specific condition in R","<p>I have a data frame that contains around 700 cases with 1800 examinations. Some cases underwent several different modalities. I want to leave only one examination result based on the specific condition of the modality.</p>
<p>Here is a dummy data frame:</p>
<pre><code>df &lt;- data.frame (ID = c(&quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;4&quot;, &quot;5&quot;, &quot;5&quot;),
                  c1 = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;A&quot;, &quot;C&quot;, &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;),
                  x1 = c(5, 4, 5, 3, 1, 3, 4, 2, 3, 5),
                  x2 = c(4, 3, 7, 9, 1, 2, 4, 7, 5, 0))
</code></pre>
<p>There are five cases with 10 exams. [c1] is the exam modality (condition), and the results are x1 and x2.</p>
<p>I want to leave only one row based on the following condition:</p>
<blockquote>
<p>C &gt; B &gt; A</p>
</blockquote>
<p>I want to leave the row with C first; if not, leave the row with B; If C and B are absent, leave the row with A.</p>
<p>Desired output:</p>
<pre><code>output &lt;- data.frame (ID = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;),
                      c1 = c(&quot;C&quot;, &quot;C&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;),
                      x1 = c(5, 1, 3, 2, 5),
                      x2 = c(7, 1, 2, 7, 0))
</code></pre>
","<r><dataframe><conditional-statements><data-management>","2021-03-13 01:14:39","42","0","2","66611113","<p>You can <code>arrange</code> the data based on required correct order and for each <code>ID</code> select it's 1st row.</p>
<pre><code>library(dplyr)

req_order &lt;- c('C', 'B', 'A')

df %&gt;%
  arrange(ID, match(c1, req_order)) %&gt;%
  distinct(ID, .keep_all = TRUE)

#  ID    c1       x1    x2
#  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;
#1 1     C         5     7
#2 2     C         1     1
#3 3     A         3     2
#4 4     B         2     7
#5 5     C         5     0
</code></pre>
<p>In base R, this can be written as :</p>
<pre><code>df1 &lt;- df[order(match(df$c1, req_order)), ]
df1[!duplicated(df1$ID), ]
</code></pre>
"
"66589209","What causes data tables in Hadoop servers (and in general) to become ""corrupted""?","<p>Today my coworker and I discovered we were unable to access a table on our Hadoop server - we couldn't perform queries on the table in Hive SQL or extract the table to our RStudio server.</p>
<p>Our data management person said sometimes this happens, tables become &quot;corrupted&quot;.</p>
<p>Is it purely random (a stray cosmic ray?) or are there engineering or user related issues to blame for corrupted tables?</p>
","<hadoop><corruption><data-management>","2021-03-11 19:25:43","118","0","1","66606741","<p>While the question is generic, the cause of corruption can vary with different use case. Some of the reasons that I've come across are,</p>
<ul>
<li><p>Metadata of the table in Hive Metastore DB gets overwritten, edited or go missing due to any operations performed on the DB. This essentialy means, we cannot perform any query on a specific or group of tables.</p>
</li>
<li><p>The actual data of the Hive table in HDFS gets corrupted. While the chance of hitting this very low (due to replica of data in Cluster), it is still possible if a 128 MB block of the table's file gets corrupted, then the data can't be read. This will result in partial or no data returned by the query. This can be hardware specific.</p>
</li>
</ul>
"
"66578572","Snowflake OR condition inside the ON clause is only working on one field even when the other exists","<p>I am getting data from dynamic source, where 2 fields could exists within the data or only one of them.</p>
<p>Those fields are <code>FACILITY_ID</code> and <code>BLOCK</code>.</p>
<p>Some data has only BLOCK, and some has both, and I need to do a <code>MERGE INTO</code> to add what is available into <code>HUB_LOCATION</code>.</p>
<p>Some data will come like that:</p>
<p><a href=""https://i.stack.imgur.com/07fZ8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/07fZ8.png"" alt=""enter image description here"" /></a></p>
<p>And some would have only <code>BLOCK</code> and some would have both:</p>
<p><a href=""https://i.stack.imgur.com/JRvrO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JRvrO.png"" alt=""enter image description here"" /></a></p>
<p>I tried this query:</p>
<pre><code>MERGE INTO HUB_LOCATION HL
  USING (SELECT md5(CONCAT_WS('', 'FACILITY', OBJECT_CONSTRUCT(*):FACILITY_ID)) AS FACILITY_HKEY,
     md5(CONCAT_WS('', 'BLOCK', OBJECT_CONSTRUCT(*):BLOCK)) AS BLOCK_HKEY, OBJECT_CONSTRUCT(*):FACILITY_ID AS FACILITY_ID, OBJECT_CONSTRUCT(*):BLOCK AS BLOCK FROM TEMP_TABLE ) ST 
  ON (md5(CONCAT_WS('', 'FACILITY', ST.FACILITY_ID)) = HL.LOCATION_HKEY OR md5(CONCAT_WS('', 'BLOCK', ST.BLOCK)) = HL.LOCATION_HKEY ) 
  WHEN NOT MATCHED AND ST.FACILITY_HKEY IS NOT NULL THEN 
      INSERT (LOCATION_HKEY, LOAD_DT, RECORD_SRC, LOCATION_VALUE, LOCATION_TYPE) 
      VALUES (md5(CONCAT_WS('', 'FACILITY', ST.FACILITY_ID)), CURRENT_TIMESTAMP(), 'ONA', ST.FACILITY_ID, 'FACILITY') 
  WHEN NOT MATCHED AND ST.BLOCK IS NOT NULL THEN
      INSERT (LOCATION_HKEY, LOAD_DT, RECORD_SRC, LOCATION_VALUE, LOCATION_TYPE) 
      VALUES (md5(CONCAT_WS('', 'BLOCK', ST.BLOCK)), CURRENT_TIMESTAMP(), 'ONA', ST.BLOCK, 'BLOCK');
</code></pre>
<p>This merge statement will only work when there is only one of both fields.</p>
<p>Within the <code>ON</code> clause I tried to change <code>OR</code> to <code>AND</code> but the script ran non stop.</p>
","<sql><merge><snowflake-cloud-data-platform><data-warehouse>","2021-03-11 08:09:13","256","0","1","66602659","<p>I reproduced what it appears you're doing and the merge is working. The issue is the test for the first table and second table are using the same values that get matched. That inserts 0 rows from the <code>when not matched</code> clause. If you make a trivial change to the values, the merge works. If you need to update them if matched, you can add a clause for that. Here's all I did with the change to the second table to make the changes not match:</p>
<pre><code>create or replace table HUB_LOCATION(LOCATION_HKEY string, LOAD_DT date, RECORD_SRC string, LOCATION_VALUE string, LOCATION_TYPE string);

create or replace table TEMP_TABLE(id int, facility_id string, &quot;name&quot; string);
insert into TEMP_TABLE (id, facility_id, &quot;name&quot;) values (1, '1A', 'xyz'), (2, '1A', 'abc'), (3, '2C', 'ac');

create or replace table TEMP_TABLE(id int, facility_id string, &quot;name&quot; string, block string);
insert into TEMP_TABLE (id, facility_id, &quot;name&quot;, block) values (1, '1Ax', 'xyz', 'B1'), (2, '1Ay', 'abc', 'B2'), (3, '2Cz', 'ac', 'B3');


MERGE INTO HUB_LOCATION HL
  USING (SELECT md5(CONCAT_WS('', 'FACILITY', OBJECT_CONSTRUCT(*):FACILITY_ID)) AS FACILITY_HKEY,
     md5(CONCAT_WS('', 'BLOCK', OBJECT_CONSTRUCT(*):BLOCK)) AS BLOCK_HKEY, OBJECT_CONSTRUCT(*):FACILITY_ID AS FACILITY_ID, OBJECT_CONSTRUCT(*):BLOCK AS BLOCK FROM TEMP_TABLE ) ST 
  ON (md5(CONCAT_WS('', 'FACILITY', ST.FACILITY_ID)) = HL.LOCATION_HKEY OR md5(CONCAT_WS('', 'BLOCK', ST.BLOCK)) = HL.LOCATION_HKEY ) 
  WHEN NOT MATCHED AND ST.FACILITY_HKEY IS NOT NULL THEN 
      INSERT (LOCATION_HKEY, LOAD_DT, RECORD_SRC, LOCATION_VALUE, LOCATION_TYPE) 
      VALUES (md5(CONCAT_WS('', 'FACILITY', ST.FACILITY_ID)), CURRENT_TIMESTAMP(), 'ONA', ST.FACILITY_ID, 'FACILITY') 
  WHEN NOT MATCHED AND ST.BLOCK IS NOT NULL THEN
      INSERT (LOCATION_HKEY, LOAD_DT, RECORD_SRC, LOCATION_VALUE, LOCATION_TYPE) 
      VALUES (md5(CONCAT_WS('', 'BLOCK', ST.BLOCK)), CURRENT_TIMESTAMP(), 'ONA', ST.BLOCK, 'BLOCK');
      
select * from HUB_LOCATION;
</code></pre>
"
"66550402","How to accommodate two dependent fact tables in the same star schema?","<p>In a data warehouse how to store facts that are dependent on each other (one fact is a child of the other).?</p>
<p>Ex:</p>
<p>Transaction/Sale is a fact table in my warehouse.
But, I also have &quot;comments/remarks&quot; on each transaction/sale, which could be 0 - N  (usually N is never &gt; 3 and 80% of the time 0)) per each sale/transaction.</p>
<p>Do I have to create a new fact for the &quot;comments&quot; table?</p>
","<data-warehouse><star-schema>","2021-03-09 16:00:42","956","1","3","66551144","<p>If facts have the same grain then they can (but don't have to) be stored in the same fact table; if they have a different grain then they cannot be stored in the same fact table.</p>
"
"66550402","How to accommodate two dependent fact tables in the same star schema?","<p>In a data warehouse how to store facts that are dependent on each other (one fact is a child of the other).?</p>
<p>Ex:</p>
<p>Transaction/Sale is a fact table in my warehouse.
But, I also have &quot;comments/remarks&quot; on each transaction/sale, which could be 0 - N  (usually N is never &gt; 3 and 80% of the time 0)) per each sale/transaction.</p>
<p>Do I have to create a new fact for the &quot;comments&quot; table?</p>
","<data-warehouse><star-schema>","2021-03-09 16:00:42","956","1","3","66551193","<p>Yes, you would create a second fact table, sometimes called &quot;factless fact table&quot; or &quot;bridge table&quot;. This is a many-to-many-relationship table connecting several dimension tables, but not containing any fact itself, only relationships.</p>
<p>Another typical example for this type of table in data warehouse literature is the case of a bank account having potentially more than one owner.</p>
"
"66550402","How to accommodate two dependent fact tables in the same star schema?","<p>In a data warehouse how to store facts that are dependent on each other (one fact is a child of the other).?</p>
<p>Ex:</p>
<p>Transaction/Sale is a fact table in my warehouse.
But, I also have &quot;comments/remarks&quot; on each transaction/sale, which could be 0 - N  (usually N is never &gt; 3 and 80% of the time 0)) per each sale/transaction.</p>
<p>Do I have to create a new fact for the &quot;comments&quot; table?</p>
","<data-warehouse><star-schema>","2021-03-09 16:00:42","956","1","3","66553670","<p>The first approach for the detail entity is to set up an <strong>ordinary fact table</strong>, with foreign key to all available dimensions (e.g. if the comments have their own posting dates) <em>plus</em> a foreign key towards the master fact table.</p>
<p>If the both fact tables are accessed often in common you will profit from an identical <em>partitioning schema</em>, e.g. range partitioning based on the sales day. This will help also to implement a unique rolling window concept for both fact tables.</p>
<p>In your example though you will possibly realize that a <em>free text comment</em> is not very valuable in dimensional modelling schema, so you may want to apply some sort of <em>feature selection</em> from the comments. If you manage to define a fixed number of such features, you may fall back to a one fact table schema simple adding those features as attributes.</p>
"
"66493420","Can I create a many-to-many relationship in my data warehouse / tabular model on different fields?","<p>Imagine the following situation:</p>
<p>I am designing a data warehouse / Tabular cube/model for a broker company.
This means that when the broker creates a Contract, there is a Buyer and a Seller.</p>
<p>So when I create my Fact_Contracts it looks something like this:</p>
<pre><code>CREATE TABLE [dbo].[Fact_Contracts](
    [FactContractId] [INT] IDENTITY(1,1) NOT NULL,
    [ContractBuyer_Client_Key] [INT] NOT NULL,
    [ContractSeller_Client_Key] [INT] NOT NULL,
    [ContractDate_Key] [INT] NOT NULL,
    [ContractAmount] [DECIMAL](18, 2) NOT NULL,
)
</code></pre>
<p>Now I have my Clients Dimension:</p>
<pre><code>CREATE TABLE [dbo].[Dim_Clients](
    [ClientKey] [int] IDENTITY(1,1) NOT NULL,
    [ClientBK] [int] NOT NULL,
    [ClientName] [nvarchar](20) NOT NULL,
)
</code></pre>
<p>Let's say I have 1 contract where ContractBuyer_Client_Key = 1 and ContractSeller_Client_Key = 2 and the ContractAmount = 1000.</p>
<p>Now when I look in my Tabular Cube, I would like to create a report, showing me details about my customers and my ContractAmount. The output should be like this:</p>
<pre><code>FactContractId  Client_Key   ClientName   ContractAmount
10              1            Abraham      1000
10              2            Björk        1000
--------------------------------------------------------
Total                                     1000
--------------------------------------------------------
</code></pre>
<p>Is it possible to create such a reciprocity report where I combine two different columns from my Fact-table?</p>
<p>Please note: A solution where I divide the Total by 2 is not an option. There are other issues which makes this undoable: Sometimes I have a third party as part of the contract which has not been shown on this simplistic outline of my tables. So my total needs to be the sum of all unique Contracts.</p>
","<many-to-many><data-warehouse><tabular><ssas-tabular>","2021-03-05 13:26:06","29","0","2","66494528","<p>This is your fact table</p>
<pre><code>select * from contract;

  BUYER_ID  SELLER_ID     AMOUNT
---------- ---------- ----------
         1          2       1000
</code></pre>
<p>Create a <em>role</em> view on a top of it</p>
<pre><code>create view contract_role as         
select 'Buyer' role_id, BUYER_ID client_id, amount from contract
union all
select 'Seller' role_id, SELLER_ID client_id, amount from contract;

select * from contract_role;

ROLE_I  CLIENT_ID     AMOUNT
------ ---------- ----------
Buyer           1       1000
Seller          2       1000
</code></pre>
<p>Now you may report all <em>roles</em>, but consider the <em>total</em> only from one of the <em>roles</em></p>
<pre><code>select CLIENT_ID, AMOUNT,
sum(case when ROLE_ID = 'Buyer' then AMOUNT end) over ( ) as total_amount
from contract_role

 CLIENT_ID     AMOUNT TOTAL_AMOUNT
---------- ---------- ------------
         1       1000         1000
         2       1000         1000
</code></pre>
"
"66493420","Can I create a many-to-many relationship in my data warehouse / tabular model on different fields?","<p>Imagine the following situation:</p>
<p>I am designing a data warehouse / Tabular cube/model for a broker company.
This means that when the broker creates a Contract, there is a Buyer and a Seller.</p>
<p>So when I create my Fact_Contracts it looks something like this:</p>
<pre><code>CREATE TABLE [dbo].[Fact_Contracts](
    [FactContractId] [INT] IDENTITY(1,1) NOT NULL,
    [ContractBuyer_Client_Key] [INT] NOT NULL,
    [ContractSeller_Client_Key] [INT] NOT NULL,
    [ContractDate_Key] [INT] NOT NULL,
    [ContractAmount] [DECIMAL](18, 2) NOT NULL,
)
</code></pre>
<p>Now I have my Clients Dimension:</p>
<pre><code>CREATE TABLE [dbo].[Dim_Clients](
    [ClientKey] [int] IDENTITY(1,1) NOT NULL,
    [ClientBK] [int] NOT NULL,
    [ClientName] [nvarchar](20) NOT NULL,
)
</code></pre>
<p>Let's say I have 1 contract where ContractBuyer_Client_Key = 1 and ContractSeller_Client_Key = 2 and the ContractAmount = 1000.</p>
<p>Now when I look in my Tabular Cube, I would like to create a report, showing me details about my customers and my ContractAmount. The output should be like this:</p>
<pre><code>FactContractId  Client_Key   ClientName   ContractAmount
10              1            Abraham      1000
10              2            Björk        1000
--------------------------------------------------------
Total                                     1000
--------------------------------------------------------
</code></pre>
<p>Is it possible to create such a reciprocity report where I combine two different columns from my Fact-table?</p>
<p>Please note: A solution where I divide the Total by 2 is not an option. There are other issues which makes this undoable: Sometimes I have a third party as part of the contract which has not been shown on this simplistic outline of my tables. So my total needs to be the sum of all unique Contracts.</p>
","<many-to-many><data-warehouse><tabular><ssas-tabular>","2021-03-05 13:26:06","29","0","2","66511727","<p>Marmite Bomber did put me in the right direction, but the answer was not 100% correct.</p>
<p>Creating a contract_role view as mentioned is correct. But instead of just adding data from one role, I have instead done it in dax, using  a SUMMARIZE:</p>
<pre><code>SumOfAmount := 
CALCULATE(
     SUM(Fact_Contracts[ContractAmount]),
     SUMMARIZE(
         contract_role,
         Fact_Contract[FactContractId]
    )
)
</code></pre>
<p>This seems to work even if I have more roles than just a Buyer and a Seller.</p>
"
"66480592","Writing DAX in Power BI for Facts and Dimensions","<p>Let's say in Power BI I have the following set up:</p>
<pre><code>d_date (this is a date dimension)
d_customers (this is a customer dimension)
f_orders (this is a fact table)
</code></pre>
<p><code>f_orders</code> connects to <code>d_date</code> on <code>date_id</code> and connects to <code>d_customers</code> on <code>customer_id</code>.</p>
<p>I want to create a dax measure that shows the list of distinct customers, along with their name, for each date.</p>
<p>How could I do so?</p>
<p>(I am a novice at Dax and somewhat new to Power BI)</p>
<p>Expected output</p>
<pre><code>Table 1
    order date     Customer Name
    2020-01-01     John Doe
    2020-01-01     James Simpson
    2020-01-03     Emilia Clarke
    ...
    2020-12-31     Jamie Lanister


Table 2

    order date     distinct_customer_count
    2020-01-01     2
    2020-01-03     1
    ...
    2020-12-31     1
</code></pre>
<p><strong>My goal is not so much to produce the output as to see a simple example of using DAX to produce output from 2 tables.</strong></p>
","<powerbi><dax><data-warehouse><powerbi-desktop>","2021-03-04 18:03:04","467","0","2","66481632","<p>Create a measure that counts distinct customers in f_orders. If you use that measure in a visual that groups/filters by date, then that measure will show the correct values.</p>
<p>For example a chart with date on the X axis and the distinct customer count as the value will then show the distinct count for each day.</p>
<p>In Power BI/DAX, you don't need to pre-calculate all kinds of scenarios because the measure will always get evaluated in the context of the filters of the page/visual.</p>
"
"66480592","Writing DAX in Power BI for Facts and Dimensions","<p>Let's say in Power BI I have the following set up:</p>
<pre><code>d_date (this is a date dimension)
d_customers (this is a customer dimension)
f_orders (this is a fact table)
</code></pre>
<p><code>f_orders</code> connects to <code>d_date</code> on <code>date_id</code> and connects to <code>d_customers</code> on <code>customer_id</code>.</p>
<p>I want to create a dax measure that shows the list of distinct customers, along with their name, for each date.</p>
<p>How could I do so?</p>
<p>(I am a novice at Dax and somewhat new to Power BI)</p>
<p>Expected output</p>
<pre><code>Table 1
    order date     Customer Name
    2020-01-01     John Doe
    2020-01-01     James Simpson
    2020-01-03     Emilia Clarke
    ...
    2020-12-31     Jamie Lanister


Table 2

    order date     distinct_customer_count
    2020-01-01     2
    2020-01-03     1
    ...
    2020-12-31     1
</code></pre>
<p><strong>My goal is not so much to produce the output as to see a simple example of using DAX to produce output from 2 tables.</strong></p>
","<powerbi><dax><data-warehouse><powerbi-desktop>","2021-03-04 18:03:04","467","0","2","66491387","<p>You are asking for a measure returning a list. This cannot be done directly, since measures must return a scalar, unless you mean to build a string with all the customers. (this could be done using CONCATENATEX(), but doesn't work well when more than few customers are to he shown)</p>
<p>From your desired output I see that you would like to see a table visual with date and customer who placed an order for that date. To do so it suffice to create a customers count measure like follows</p>
<pre><code># Customers = DISTINCTCOUNT( f_orderes[customer_id] )
</code></pre>
<p>and to create a table visual with d_date[date], d_customer[name] and [# Customers]</p>
<p>using only d_date[date], and [# Customers] you should get the Table 2</p>
<p>to see some code using both tables, you might compute calculated tables, like for instance, moving to a real dataset with customers, date and sales, an easy implementation of table 1 is</p>
<pre><code>SUMMARIZE ( Sales, 'Date'[Date], Customer[Name] )
</code></pre>
<p>and here is the same code to be tested on <a href=""https://dax.do/xozTYuRxs6TNtf/"" rel=""nofollow noreferrer"">dax.do</a></p>
<p>and for table 2</p>
<pre><code>FILTER (
    ADDCOLUMNS (
        ALL ( 'Date'[Date] ),
        &quot;# Customers&quot;, CALCULATE ( DISTINCTCOUNT ( Sales[CustomerKey] ) )
    ),
    NOT ISBLANK ( [# Customers] )
)
</code></pre>
<p>the FILTER on NOT ISBLANK() is needed to remove the dates with no sales</p>
<p>here is the code on <a href=""https://dax.do/6bspQkyT7U5Vey/"" rel=""nofollow noreferrer"">dax.do</a></p>
<p>it's also possible to use the SUMMARIZECOLUMNS, but SUMMARIZECOLUMNS is a more advanced function that cannot be used in DAX measures.</p>
<pre><code>SUMMARIZECOLUMNS (
    'Date'[Date],
    &quot;# Customers&quot;, DISTINCTCOUNT ( Sales[CustomerKey] )
)
</code></pre>
<p>here is the <a href=""https://dax.do/Gmkjt6BgDeuykN/"" rel=""nofollow noreferrer"">dax.do link for this last code snippet</a></p>
"
"66478931","Forge BIM360 data management","<p>I'm trying to repeat and comprehend the following solution for copying files from BIM360 by Augusto Goncalves:
<a href=""https://github.com/Developer-Autodesk/data.management-csharp-a360sync.git"" rel=""nofollow noreferrer"">https://github.com/Developer-Autodesk/data.management-csharp-a360sync.git</a></p>
<p><a href=""https://i.stack.imgur.com/NGCfX.png"" rel=""nofollow noreferrer"">screen</a></p>
<p>After filling FORGE_ID and FORGE_SECRET as <a href=""https://youtu.be/4Pgg05tLW-M"" rel=""nofollow noreferrer"">shown</a> it says that &quot;Page exired&quot;. What I'm doind wrong?</p>
","<autodesk-forge><autodesk-bim360>","2021-03-04 16:16:57","38","0","1","66479312","<p>I will suggest making your callback url - http://localhost:58966/api/forge/callback/oauth it needs to return you the code for authentication.</p>
"
"66467313","Graph database vs. RDB with link/bridge tables","<p>I work in the fraud/AML (anti-money laundering) field, and we are exploring using a graph database to unearth hidden connections and links. I've read a fair amount abut graph databases lately (mostly neo4j, but I think the concepts are similar across different products?), and from what I can tell, they seem to be well-suited to this domain. The issue is that I'm having a hard time getting buy-in from tech management, as they seem to think that we can do the same things with our existing data reporting model, which is in Hadoop, and is essentially a data warehouse which has specific tables that provide many-to-many link tables between the core tables (I believe Kimball calls them 'bridge' tables?).</p>
<p>In a way, they seem to provide the same functionality as the relationship tables in a graph DB. Given that we have already constructed the link tablesin Hadoop, would a graph database provide any performance advantage for the kinds of things we may want to do (e.g. How is Customer A connected to Customer B), or have we largely negated any performance advantage of a graph DB by building all of the link tables?</p>
","<database><database-design><neo4j><graph-databases><data-warehouse>","2021-03-04 01:31:38","153","0","1","66477597","<p>On similar hardware platforms, a relational database will never be able to keep up with a well constructed graph database when performing &quot;path-between&quot; queries. Never.</p>
<p>Every graph database product has its own internal storage representation, but they are all fundamentally designed to store nodes and edges and support navigational queries across those nodes and edges. Without the addition of new graph-support features, relational database will struggle to provide graph-like capabilities.</p>
<p>The other advantage of using a native graph database is that the graph query languages are specifically designed to support path-between queries. In Objectivity/DB, a massively scalable and distributable object/graph database, we can use the DO query language to find all of the paths between two entities up to a specified number of degrees apart in milliseconds or seconds. A DO query might look like the following:</p>
<pre><code>Match p = (:Account { accountId = &quot;1234&quot;})
          -[*..100]-&gt;
          (:Account { accountId = &quot;5678&quot;})
          return p;
</code></pre>
<p>Here, we are saying: Find all paths (p) from Account 1234 to Account 5678, where they are between 1 and 100 degrees apart.</p>
<p>To create and execute this same query in a relational database would be much more complicated (without the addition of graph features to the database) and the execution of a query like this in a relational database would be much more resource intensive (memory, cpu, I/O).</p>
<p>If you have the opportunity to explore graph database for your project, make sure you understand your scalability and data distribution requirements. That information will be key to selecting the correct product.</p>
<p><em>Disclaimer: I am the Director of Field Operations for Objectivity.</em></p>
"
"66466865","Power BI Tables and DAX - Import and Write DAX or do everything in the Source","<p>Let's assume I have a data warehouse environment.</p>
<p>Would it be a better to practice to...</p>
<p>Write a SQL query that gathers everything you need via the fact and dimension tables to minimize the amount of DAX and M that you need to use?</p>
<p>OR</p>
<p>Import the fact table and the dimension tables separately and write some DAX and use M to get everything that you need to get as far as columns go?</p>
<p>Please help me understand the best practice.</p>
<p><strong>Edit #1</strong></p>
<p>This seems to suggest it's better to import everything into facts and dimensions separately vs combining everything:</p>
<p><a href=""https://community.powerbi.com/t5/Desktop/Facts-and-Dimensions-still-useful-with-Power-BI/td-p/402218"" rel=""nofollow noreferrer"">https://community.powerbi.com/t5/Desktop/Facts-and-Dimensions-still-useful-with-Power-BI/td-p/402218</a></p>
","<powerbi><dax><data-warehouse><powerbi-desktop><m>","2021-03-04 00:24:08","121","0","1","66467243","<blockquote>
<p>Import the fact table and the dimension tables separately and write some DAX and use M to get everything that you need to get as far as columns go?</p>
</blockquote>
<p>This.  The dimensional model (facts and dimensions) serves the same function in Power BI as it does in a Data Mart: It reduces data duplication and separates the &quot;things that we measure&quot; from the &quot;things that we filter, pivot, drill, and sort by&quot;.</p>
<p>See for instance: <a href=""https://learn.microsoft.com/en-us/learn/modules/design-model-power-bi/"" rel=""nofollow noreferrer"">Design a data model in Power BI</a></p>
<p>Note that your Power BI data model might not be the <em>same</em> as the Data Warehouse dimensional model, but you'll normally bring in the relevant dimensions as-is.  You might bring in the facts at a different grain than they are in the data warehouse.</p>
"
"66443233","Question regarding role-playing dimension","<p>I hope you can be helpful in answering one question in regards to role-playing dimensions.</p>
<p>When using views for a role playing dimension, Does it then matter which view is referred to later in the analysis. Especially, when sorting on the role playing dimension, can this be done no matter which view is used?</p>
<p>Hope the question is clear enough. If not, let me know and I will elaborate.</p>
<p>Thanks in advance.</p>
","<data-warehouse><dimensional-modeling>","2021-03-02 16:26:25","51","0","1","66461007","<p>Do you mean you have created a view similar to &quot;SELECT * FROM DIM&quot; for each role the Dim plays? If that's all you've done then you could use any of these views in a subsequent SQL statement that joins the DIM to a FACT table - but obviously if you use the &quot;wrong&quot; view it's going to be very confusing for anyone trying to read your SQL (or you trying to understand what've you've written in 3 months time!)</p>
<p>For example, if you have a fact table with keys OrderDate and ShipDate that both reference your DateDim then you could create vwOrderDate and vwShipDate. You could then join FACT.OrderDate to vwShipDate and FACT.ShipDate to vwOrderDate and it will make no difference to the actual resultset your query produces (apart from, possibly, column names).</p>
<p>However, unless the applicable attributes are very different for different roles, I really wouldn't bother creating views for role-playing Dims as it's an unnecessary overhead and just going to cause confusion to anyone you've given access to at this level of the DB (who presumably have pretty strong SQL skills to be given this level of access?).</p>
<p>If you are trying to make life easier for end-users then either create these types of &quot;views&quot; in the models of the BI tool(s) they are using - and not directly in the DB - or, if they are being given access to the DB, then create View(s) across the Fact(s) and all their joined Dimensions</p>
"
"66440215","Dimensional Modeling - Outrigger dimension for geography","<p>Currently, I'm working on dimensional modeling and have a question in regards to an outrigger dimension.
The company is trading and acts as a broker between customer and supplier.</p>
<p>For a fact table, &quot;Fact Trades&quot;, we include dimCustomer and dimSupplier.
Each of these dimensions have an address.</p>
<p>My question is if it is correct to do outrigger dimensions that refer to geography. This way we can measure how much we have delivered from an origin and delivered to a city.
<a href=""https://i.stack.imgur.com/YEJRr.png"" rel=""nofollow noreferrer"">dimensional model</a></p>
<p>I am curious to what is best practice. I hope you can help to explain how this should be modelled correctly and why.</p>
<p>Hope my question was clear and that I have posted it the correct place.</p>
<p>Thanks in advance.</p>
","<data-warehouse><dimensional-modeling>","2021-03-02 13:19:15","166","0","1","66442172","<p>I can think of at least 3 possible options; your particular circumstances will determine which is best for you:</p>
<ol>
<li>If you often filter your fact by geography but without needing company/person information (i.e. how many trades where between London and New York?) then I would create a standalone geography dimension and link it directly to your fact (twice - for customer and supplier). This doesn't also stop you having geographic attributes in your customer/supplier Dims, as a dimensional model is not normalised</li>
<li>If geographic attributes change at a significantly more frequent rate than the customer/supplier attributes, and the customer/supplier Dims have a lot of attributes, then it may be worth creating an outrigger dim for the geographical attributes - as this reduces the maintenance required for the customer/supplier Dims. However, given that most companies/people rarely change their address, this is probably unlikely</li>
<li>Keep the geographical attributes in the customer/supplier Dims. I would probably do this anyway even if also picking option 1 above</li>
</ol>
<p>Just out of interest - do customer and supplier have significantly different sets of attributes (I assume they are both companies or people)? Is it necessary to create separate Dims for them?</p>
"
"66427561","How can I load data into snowflake from S3 whilst specifying data types","<p>I'm aware that its possible to load data from files in S3 (e.g. csv, parquet or json) into snowflake by creating an external stage with file format type <code>csv</code> and then loading it into a table with 1 column of type <code>VARIANT</code>. But this needs some manual step to cast this data into the correct types to create a view which can be used for analysis.</p>
<p>Is there a way to automate this loading process from S3 so the table column data types is either inferred from the CSV file or specified elsewhere by some other means? (similar to how a table can be created in Google BigQuery from csv files in GCS with inferred table schema)</p>
","<amazon-s3><snowflake-cloud-data-platform><data-warehouse>","2021-03-01 18:26:17","389","1","1","66429678","<p>As of today, the single Variant column solution you are adopting is the closest you can get with Snowflake out-of-the-box tools to achieve your goal which, as I understand from your question, is to let the loading process infer the source file structure.</p>
<p>In fact, the COPY command needs to know the structure of the expected file that it is going to load data from, through FILE_FORMAT.</p>
<p>More details: <a href=""https://docs.snowflake.com/en/user-guide/data-load-s3-copy.html#loading-your-data"" rel=""nofollow noreferrer"">https://docs.snowflake.com/en/user-guide/data-load-s3-copy.html#loading-your-data</a></p>
"
"66420233","Why do i got minus with filter rows age > 0 in pentaho?","<p>Hello i'm currently learning pentaho, and i used a filter rows
And when i'm using the filter rows condition like this : Age &gt; 0
What i got was only the minus in the preview. I thought Age &gt; 0 it should've been a more than 1 right?
When i used Age &lt; 0, it only got positives numbers not minus. Why ?</p>
<p><a href=""https://i.stack.imgur.com/Hy4YZ.png"" rel=""nofollow noreferrer"">job picture</a></p>
<p><a href=""https://i.stack.imgur.com/o8S2f.png"" rel=""nofollow noreferrer"">data preview</a></p>
","<pentaho><data-warehouse><pentaho-spoon><pentaho-data-integration>","2021-03-01 10:17:50","150","0","1","66420431","<p>&quot;Filter Rows&quot; step has 2 option. One is what do you want if condition is &quot;<strong>true</strong>&quot; and another is what do you want if condition is &quot;<strong>false</strong>&quot;.</p>
<p><a href=""https://i.stack.imgur.com/aHvsy.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aHvsy.jpg"" alt=""enter image description here"" /></a></p>
<p>In your transformation, you have choose option 'false'. That's why all rows are selected which are &lt;0. You need to select step &quot;Calculator&quot; in 'Send <strong>true</strong> data to step' box as you want result if condition true.</p>
<p>You can see sample from <a href=""https://drive.google.com/drive/folders/10rmWDDWN1LVNR8i0g3K5CaxZqCh9dGbG?usp=sharing"" rel=""nofollow noreferrer"">HERE</a> in Logging preview.</p>
"
"66411276","What is the diff between Full table vs Delta table vs Incremental in dwh oracle","<p>I am trying to understand the concept Full Table vs Incremental Table vs Delta table and in principle in order to simplify (faster loading process) the daily ETL loads is it a good practice to use Incremental table</p>
<p><strong>FULL TABLE</strong></p>
<p><strong>INCREMENTAL TABLE</strong></p>
<p><strong>DELTA TABLE</strong></p>
<p>i have read some where</p>
<p><em>Using incremental loads to move data can shorten the run times of your ETL processes and reduce the risk when something goes wrong</em></p>
<p>can some one please help me understanding the concept ?</p>
","<sql><oracle><data-warehouse><sql-data-warehouse><star-schema-datawarehouse>","2021-02-28 16:28:06","3475","0","2","66411330","<ul>
<li>full, as its name says, loads <em>everything</em>, the whole source data file</li>
<li>incremental - or <em>delta</em> (those are synonyms, not two different types) - mean that you load only data which you haven't loaded yet. It usually means that time of the last loading has been recorded. The next loading session loads data created after the last successful loading timestamp</li>
</ul>
<p>As of</p>
<ul>
<li>&quot;shortening run times&quot;: obviously, if you don't have to load everything but just what's missing, it takes less time</li>
<li>&quot;reducing the risk&quot;: you don't mess up with data already loaded, it stays in the database. If something goes wrong, it goes wrong with current loading session so you can discard changes you've made and start over</li>
</ul>
"
"66411276","What is the diff between Full table vs Delta table vs Incremental in dwh oracle","<p>I am trying to understand the concept Full Table vs Incremental Table vs Delta table and in principle in order to simplify (faster loading process) the daily ETL loads is it a good practice to use Incremental table</p>
<p><strong>FULL TABLE</strong></p>
<p><strong>INCREMENTAL TABLE</strong></p>
<p><strong>DELTA TABLE</strong></p>
<p>i have read some where</p>
<p><em>Using incremental loads to move data can shorten the run times of your ETL processes and reduce the risk when something goes wrong</em></p>
<p>can some one please help me understanding the concept ?</p>
","<sql><oracle><data-warehouse><sql-data-warehouse><star-schema-datawarehouse>","2021-02-28 16:28:06","3475","0","2","66412117","<p>Well you did not provide the reference for your quote, but in my experience it is only 50% correct.</p>
<p>I'd read it:</p>
<p><em>Using incremental loads to move data can shorten the run times of your ETL processes</em>  <strong>but increase</strong> <em>the risk that something goes wrong</em>.</p>
<p>The problem is in the <strong>error accumulation</strong>. If you get corrupt or incomplete data in <em>full load</em> you through them away on the next load and there is a good chance that the new load is valid.</p>
<p>On contrary with <em>delta load</em> the errors remain and can <em>accumulate within the time</em>.</p>
<p>Therefor is a <em>good practice</em> while implementing a <em>delta load</em> is to perform periodical check (daily, monthly etc.) that the complete snashot in the source and target are identical.</p>
<p>My <em>rule of thumb</em> is - choose <em>delta load</em> only if the <em>full load</em> is not feasible (i.e. for transaction tables and large dimensions).</p>
"
"66409283","updating data in dvc registry from other projects","<p>I have a couple of projects that are using and updating the same data sources. I recently learned about <a href=""https://dvc.org/doc/use-cases/data-registries"" rel=""nofollow noreferrer"">dvc's data registries</a>, which sound like a great way of versioning data across these different projects (e.g. scrapers, computational pipelines).</p>
<p>I have put all of the relevant data into <code>data-registry</code> and then I imported the relevant files into the scraper project with:</p>
<pre class=""lang-sh prettyprint-override""><code>$ poetry run dvc import https://github.com/username/data-registry raw
</code></pre>
<p>where <code>raw</code> is a directory that stores the scraped data. This seems to have worked properly, but then when I went to build <a href=""https://dvc.org/doc/start/data-pipelines"" rel=""nofollow noreferrer"">a dvc pipeline</a> that <em>outputted</em> data into a file that was already tracked by dvc, I got an error:</p>
<pre class=""lang-sh prettyprint-override""><code>$ dvc run -n menu_items -d src/ -o raw/menu_items/restaurant.jsonl scrapy crawl restaurant
ERROR: Paths for outs:                                                
'raw'('raw.dvc')
'raw/menu_items/restaurant.jsonl'('menu_items')
overlap. To avoid unpredictable behaviour, rerun command with non overlapping outs paths.
</code></pre>
<p>Can someone help me understand what is going on here? <strong>What is the best way to use data registries to share and update data across projects?</strong></p>
<p>I would ideally like to update the data-registry with new data from the scraper project and then allow other dependent projects to update their data when they are ready to do so.</p>
","<data-management><dvc>","2021-02-28 12:51:53","513","4","1","66412578","<p>When you <code>import</code> (or <code>add</code>) something into your project, a .dvc file is created with that lists that something (in this case the <code>raw/</code> dir) as an &quot;output&quot;.</p>
<p>DVC doesn't allow overlapping outputs among .dvc files or dvc.yaml stages, meaning that your &quot;menu_items&quot; stage shouldn't write to <code>raw/</code> since it's already under the control of <code>raw.dvc</code>.</p>
<p>Can you make a separate directory for the pipeline outputs? E.g. use <code>processed/menu_items/restaurant.jsonl</code></p>
"
"66396556","Inserting new records based on the date in pentaho","<p>i wanted to asks if this is possible in pentaho</p>
<p>I have a data looks like this yesterday 26/02/2021 , START_DATE TAKEN FROM SYSDATE</p>
<pre><code>ID|CODE|NAME|MARKS|START_DATE|END_DATE
1|A01|KUMAR|90|26/02/2021|NULL
2|A02|RAJESH|80|26/02/2021|NULL
3|A03|SINGH|70|26/02/2021|NULL
</code></pre>
<p>So let's say that SINGH with CODE A03, he get a better marks(90) on 27/02/2021, so it will look like this</p>
<pre><code>ID|CODE|NAME|MARKS|START_DATE|END_DATE
1|A01|KUMAR|90|26/02/2021|NULL
2|A02|RAJESH|80|26/02/2021|NULL
3|A03|SINGH|70|26/02/2021|27/02/2021
4|A03|SINGH|90|27/02/2021|NULL
</code></pre>
<p>As SINGH has different values in his MARKS, the old record of SINGH will meet it's END_DATE and SINGH new record with his latest MARKS will has START_DATE on the day the record being inserted and his END_DATE will still be a null.So the job will see if there are any differences in it's values, then if there are a some differences,it means the row has an 'updated' values, and the latest ones from the source that will be insert.
Can I implement this in pentaho?</p>
<p><a href=""https://filebin.net/5rwbu4t0gvdmhr44"" rel=""nofollow noreferrer"">My Pentaho Jobs</a></p>
","<etl><pentaho><data-warehouse><pentaho-spoon><pentaho-data-integration>","2021-02-27 07:29:29","371","1","1","66397578","<p>You can do this easily using 'Update' and 'Insert/Update' step in only one transformation in Pentaho data integration.</p>
<p><a href=""https://i.stack.imgur.com/0iWWo.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0iWWo.jpg"" alt=""enter image description here"" /></a></p>
<p>You can see the sample from <a href=""https://drive.google.com/drive/folders/18hbhVWFdjIggsYMNYfOUdiBjKn3KTMIv?usp=sharing"" rel=""nofollow noreferrer"">Here</a></p>
"
"66342700","How do I design a data warehouse model that allows me to dynamically query for total action count, unique user count, and a count of total users","<p>Currently facing a problem where I am trying to create a login utilization report for a web application. To describe the report a bit, users in our system are tagged with different metadata about the user. For example, I could be tagged with &quot;New York City&quot; and &quot;Software Engineer&quot;, while other users may be tagged with different locations and job titles. The utilization report is essentially the following:</p>
<ol>
<li>Time period (quarterly)</li>
<li>Total number of logins</li>
<li>Unique logins</li>
<li>Total users</li>
<li>&quot;Engagement percentage&quot; (Unique logins / Total users)</li>
</ol>
<p>The catch is, the report needs to be a bit dynamic. I need to be able to be apply any combination of job titles and locations and have each of the numbers reflect the applied metadata. The time period also needs to be able to be easily adjusted to support weekly, monthly, and yearly as well. Ideally, I can create a view in Redshift that allows our BI software users to run this report whenever they see fit.</p>
<p>My question is, what is an ideal strategy to design a data model to support this report? I currently have an atomic fact table that contains all logins with this schema:</p>
<ul>
<li>User ID</li>
<li>Login ID</li>
<li>Login Timestamp</li>
<li>Job Title Group ID (MD5 hash of job titles to support multi valued)</li>
<li>Location Group ID (MD5 hash of locations to support multi valued)</li>
</ul>
<p>The fact table allows me to easily write a query to aggregate on total (count of login id) and unique (distinct count of user id).</p>
<p>How can I supplement the data I have to include a count of total users? Is what I currently have the best approach?</p>
","<amazon-redshift><data-modeling><data-warehouse><amazon-quicksight><star-schema-datawarehouse>","2021-02-23 23:48:22","337","0","1","66424258","<p>Hierarchical, fixed-depth many-to-one (M:1) relationships between attributes are typically denormalized or collapsed into a flattened dimension table. If you’ve spent most of your career designing entity-relationship models for transaction processing systems, you’ll need to resist your instinctive tendency to normalize or snowflake a M:1 relationship into smaller subdimensions; dimension denormalization is the name of the game in dimensional modeling.</p>
<p>It is relatively common to have multiple M:1 relationships represented in a single dimension table. One-to-one relationships, like a unique product description associated with a product code, are also handled in a dimension table. Occasionally many-to-one relationships are resolved in the fact table, such as the case when the detailed dimension table has millions of rows and its roll-up attributes are frequently changing. However, using the fact table to resolve M:1 relationships should be done sparingly.</p>
<p>In your case I recommend you to have this following design as a solution :</p>
<p><a href=""https://i.stack.imgur.com/nS7Qc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nS7Qc.png"" alt=""enter image description here"" /></a></p>
"
"66331786","CHARINDEX function to fetch string of digits until there is '.'","<p>How to fetch string of letters up to a digit before <code>.</code> using <code>charindex</code>, for example:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
</tr>
</thead>
<tbody>
<tr>
<td>1022786.12</td>
</tr>
<tr>
<td>1203384</td>
</tr>
<tr>
<td>1226757.23</td>
</tr>
<tr>
<td>22343445</td>
</tr>
<tr>
<td>23434533</td>
</tr>
</tbody>
</table>
</div>
<p>and I want to fetch only those which do not have <code>.</code>  using <code>charindex</code> function.</p>
<p>we can do this using <code>like</code> condition:</p>
<pre><code>where ID not like '%.%'
</code></pre>
<p>But I want to use <code>charindex</code> function</p>
<p>After using <code>charindex</code>, the result should be:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>1203384</td>
</tr>
<tr>
<td>22343445</td>
</tr>
<tr>
<td>23434533</td>
</tr>
</tbody>
</table>
</div>
<p>How do I do this?</p>
","<sql-server><azure><data-warehouse><charindex>","2021-02-23 10:56:22","70","0","1","66332218","<p>use <strong>charindex('.', number) &lt; 1</strong></p>
<pre><code>declare @t table(number varchar(20))
insert @t select '1022786.12'
insert @t select '1203384'
insert @t select '1226757.23'
insert @t select '22343445'
insert @t select '23434533'

select * 
from @t
where charindex('.', number) &lt; 1
</code></pre>
"
"66325001","Azure Data Factory: Inconsistent ordering of results after copy activity inside foreach activity","<p>I am trying to populate a fact table for a data warehouse in Azure Data Factory.</p>
<p>In the process, I am using the lookup activity which looks up a database table and outputs each row one by one to the foreach activity. The input to the foreach activity looks like:</p>
<p><img src=""https://i.stack.imgur.com/Hk2A9.png"" alt=""Pipeline"" /></p>
<p>Inside the foreach activity, I have a copy activity. In this copy activity, using the values of <code>Model</code> and <code>Make</code> for each row from the lookup activity, I use the query below to search my dimension table for the primary key <code>Car_ID</code>:</p>
<p><img src=""https://i.stack.imgur.com/fAszu.png"" alt=""Query used inside the copy activity to search for primary key:"" /></p>
<p>I get the correct <code>Car_ID</code> from the dimension table and copy it to a blank table, which I intend to copy to my fact table. I am able to get this to work perfectly but the only issue is that the order gets jumbled up in my <code>sink</code> table, which is the blank table. The source table is the dimension table I have mentioned earlier. Debugging the pipeline run tells me the input to the foreach is in the desired row-wise order and the corresponding <code>Car_ID</code> primary key obtained from searching is also correct; however, while copying the <code>Car_ID</code> to <code>sink</code>, I lose the order, which is necessary to maintain as I want to copy this resulting column to my fact table.</p>
<p><strong>Output from lookup/Input to foreach:</strong></p>
<p><img src=""https://i.stack.imgur.com/XQwGT.png"" alt=""Output from lookup/Input to foreach"" /></p>
<p>I would really appreciate it if anyone could point out why it is happening or suggest a way to fix this.</p>
","<sql><data-warehouse><azure-data-factory><azure-synapse>","2021-02-22 23:35:59","554","0","1","66326798","<p>Please click the <strong>Sequential</strong> in For Each active settings:
<a href=""https://i.stack.imgur.com/nX1v6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nX1v6.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/nsHKi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nsHKi.png"" alt=""enter image description here"" /></a></p>
<p>I tried and here's the result comparation:
<a href=""https://i.stack.imgur.com/ahTZa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ahTZa.png"" alt=""enter image description here"" /></a></p>
<p>HTH.</p>
"
"66312569","Should I use Data Warehouse or database or something else?","<p>On current project we have a webapp with analytics module. The users select some filters and based on those filters table or graph is shown. We want the module to be responsive, so when the users select the filters it can get data in matters of seconds.</p>
<p>User filters are querying a large table ~1,000,000,000 rows and 20 columns (for a few years it should grow 2x/year in rows). 18 out of 20 columns are filtrable. And mostly there will be SELECT + WHERE queries.</p>
<p>We are not sure, should we use Data Warehouses or classical DBs.
Current reasearch suggests we should discuss between Clickhouse, DynamoDB, Snowflake, BigQuery or Redshift. Has anyone had similar use cases and which database solution would you recommend?</p>
","<amazon-web-services><google-cloud-platform><google-bigquery><amazon-redshift><data-warehouse>","2021-02-22 08:52:05","119","0","1","66312608","<p>Since you are using the database for analytics purposes, it is recommended to use a OLAP ( Redshift)..
an OLAP database is designed to process large datasets quickly to answer questions about data.
You can compare the pricing here</p>
<p><a href=""https://medium.com/2359media/redshift-vs-bigquery-vs-snowflake-a-comparison-of-the-most-popular-data-warehouse-for-data-driven-cb1c10ac8555"" rel=""nofollow noreferrer"">https://medium.com/2359media/redshift-vs-bigquery-vs-snowflake-a-comparison-of-the-most-popular-data-warehouse-for-data-driven-cb1c10ac8555</a></p>
"
"66215184","Create Glue data catalog via Athena SDK","<p>I would like to use Athena to run queries on data in an S3 bucket in another AWS account. I am using Javascript SDK. Reading through the <a href=""https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Athena.html"" rel=""nofollow noreferrer"">documentation</a>, I understand that I must first create a data catalog that will point Athena to the correct S3 location.</p>
<p>I think that I have to call the <a href=""https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Athena.html#createDataCatalog-property"" rel=""nofollow noreferrer"">createDataCatalog</a> method. Most of the arguments for this method are self-explanatory, except for the &quot;parameters&quot; argument, which seems to contain information about how the data catalog will be created. But I am unable to find anywhere how these parameters should look.</p>
<p>So my questions are:</p>
<ol>
<li>What are the parameters to provide to here?</li>
<li>Is this the right way to create a glue data catalog (including database and table)?</li>
<li>Once done, will this allow me to run Athena queries on the data catalog?</li>
</ol>
","<amazon-web-services><amazon-athena><aws-sdk-js><aws-glue-data-catalog>","2021-02-15 20:47:47","264","1","1","66217442","<p>For a simple use case with static S3 data,</p>
<ul>
<li>We first need to create Glue Table using Glue <a href=""https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Glue.html#createTable-property"" rel=""nofollow noreferrer"">createTable</a> API pointing to S3 location. Few Examples in cli <a href=""https://awscli.amazonaws.com/v2/documentation/api/latest/reference/glue/create-table.html"" rel=""nofollow noreferrer"">documentation</a>.</li>
<li>Run queries against this Glue Table from <a href=""https://github.com/awsdocs/aws-doc-sdk-examples/blob/master/javascript/example_code/athena/index.js"" rel=""nofollow noreferrer"">Athena</a></li>
</ul>
<p>Here is an example to create Glue Database and Table</p>
<pre><code>const AWS = require(&quot;aws-sdk&quot;);
AWS.config.update({ region: &quot;us-east-1&quot; });

const glue = new AWS.Glue();
const dbName = &quot;test-db&quot;;
glue.createDatabase(
  {
    DatabaseInput: {
      Name: dbName,
    },
  },
  function (dbCrtErr, dbRsp) {
    if (dbCrtErr.message === &quot;Database already exists.&quot; || dbRsp) {
      console.log(&quot;dbRsp&quot;, dbRsp);
      glue.createTable(
        {
          DatabaseName: dbName,
          TableInput: {
            Name: &quot;my-table&quot;,
            Parameters: {
              classification: &quot;json&quot;,
              compressionType: &quot;none&quot;,
            },
            TableType: &quot;EXTERNAL_TABLE&quot;,
            StorageDescriptor: {
              Location: &quot;s3://my-s3-bucket-with-events/&quot;,
              InputFormat:
                &quot;org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat&quot;,
              OutputFormat:
                &quot;org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat&quot;,
              Columns: [
                {
                  Name: &quot;id&quot;,
                  Type: &quot;string&quot;,
                },
                {
                  Name: &quot;name&quot;,
                  Type: &quot;string&quot;,
                },
              ],
            },
          },
        },
        function (error, response) {
          console.log(&quot;error&quot;, error, &quot;response&quot;, response);
        }
      );
    } else {
      console.log(&quot;dbCrtErr&quot;, dbCrtErr);
    }
  }
);
</code></pre>
"
"66191397","How do I add new dimensions to a fact table without having to rebuild the table?","<p>I'm new to data warehousing so apologies if this is super basic, but I'm curious about this concept.</p>
<p>Example: say I have a table that stores aggregated analytics for a website, (e.g. the total pageviews for a url on a given date).</p>
<p><strong>dbo.PageFacts</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Date</th>
<th>Url_Id</th>
<th>Page_Views</th>
</tr>
</thead>
<tbody>
<tr>
<td>2020-01-01</td>
<td>1</td>
<td>280</td>
</tr>
<tr>
<td>2020-01-01</td>
<td>2</td>
<td>50</td>
</tr>
<tr>
<td>2020-01-02</td>
<td>3</td>
<td>10</td>
</tr>
</tbody>
</table>
</div>
<p>Now let's say I'm tasked with adding a new &quot;Device_Id&quot; column to this table (foreign keyed to its own dimension table), and now the table is split out like this...</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Date</th>
<th>Url_Id</th>
<th>Device_Id</th>
<th>Page_Views</th>
</tr>
</thead>
<tbody>
<tr>
<td>2020-01-01</td>
<td>1</td>
<td>101</td>
<td>180</td>
</tr>
<tr>
<td>2020-01-01</td>
<td>1</td>
<td>102</td>
<td>100</td>
</tr>
<tr>
<td>2020-01-01</td>
<td>2</td>
<td>101</td>
<td>50</td>
</tr>
<tr>
<td>2020-01-02</td>
<td>3</td>
<td>101</td>
<td>10</td>
</tr>
</tbody>
</table>
</div>
<p>How do most people go about doing this for a fact table with millions of records? I'm assuming you don't want to rebuild it every time new dimensions are added in like, especially when it requires updating the aggregated value.</p>
<p>I was thinking of creating a surrogate key on the fact table and then creating a separate dimension table with percent breakdowns (e.g. 0.75, 0.25, etc) and then just building a VIEW that joins them together and computes the new values? Something like this...</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE PageFacts (
    PageSurKey INT PRIMARY KEY,
    Date DATE,
    Url_Id INT,
    Page_Views INT
);
CREATE TABLE Device_Pct (
    Id INT PRIMARY KEY,
    PageSurKey INT FOREIGN KEY REFERENCES PageFacts(PageSurKey),
    Device_Id INT,
    Percentage FLOAT
);


CREATE VIEW Device_PageFacts AS
SELECT
    pf.Date,
    pf.Url_Id,
    d.Device_Id,
    SUM(pf.Page_Views * d.Percentage) as Page_Views
FROM PageFacts pf
JOIN Device_Pct d on d.PageSurkey = pf.PageSurkey
GROUP BY pf.Date, pf.Url_Id, d.Device_Id;
</code></pre>
<p>This seems like the way to go (since adding in new dimensions which further slice the data only requires knowing the ratio to split by), but I don't know if there's a better practice. Any insight would help. Thanks in advance.</p>
","<sql><sql-server><database><database-design><data-warehouse>","2021-02-14 00:42:47","527","1","1","66191490","<p>Normally you will rebuild the fact table if you change its grain.  That's a pretty significant design change to the dimensional model, and I don't see any benefit to complicating the schema to avoid rebuilding the fact table.</p>
<p>If you want, you could introduce this as an <em>additional</em> fact table, but it wouldn't have &quot;percentages&quot; just a normal fact table:</p>
<pre><code>CREATE TABLE PageDeviceFacts 
(
    Date DATE references DimDate,
    Url_Id INT references DimUrl,
    Device_ID INT references DimDevice,
    Page_Views INT,
    constraint pk_PageDeviceFacts
      primary key (Date,Url_Id,Device_ID)
);
</code></pre>
<p>And if it's a big table, make the PK non-clustered and create a clustered columnstore index</p>
<pre><code>create clustered columnstore index cci_PageDeviceFacts on PageDeviceFacts
</code></pre>
<p>And the old PageFacts table becomes an aggregate fact in the model.</p>
"
"66146742","Greenplum vs Citus for Data Warehouse","<p>I'm trying to evaluate Citus and Greenplum in terms of using them as a Data Warehouse. The general idea is that data from multiple OLTP systems will be integrated in real time via Kafka Connect in a central warehouse for analytical queries.</p>
<p>How does Citus compare to Greenplum in this respect? I have read that Citus has some SQL limitations, e.g. correlated subqueries are not supported if the correlation is not on the distribution column, does Greenplum have similar SQL limitations? Will Greenplum work well if data is being streamed into it (as opposed to batch updates)? I'm just having this feeling that Greenplum is more analytics-focused and can sacrifice some OLTP-specific things, which Citus cannot afford since they position themselves as HTAP (not OLAP). Citus also positions itself as a solution for sub second query times, which is not necessary for my use case - several seconds (up to 5) per query will be satisfactory.</p>
","<data-warehouse><greenplum><citus>","2021-02-10 23:35:38","1499","1","1","66152709","<p>I am not aware of any SQL limitations for Greenplum, like the one you mention above. In some cases, i.e. <code>CUBE</code> or <code>percentile_*</code> window functions (ordered-set aggregate functions) GPORCA, the Greenplum database query optimiser, will fall back to the PostgreSQL query optimiser and these queries won't be as performant as GPORCA-enabled queries - but you would still get a response to your query.</p>
<p>I'd say getting streaming data in vs. batch updates is one thing - using Kafka Connection with JDBC, would work out-of-the-box but won't be taking any advantage of the parallel distributed nature of Greenplum as all your data would have to pass through the coordinator.</p>
<p>What would be optimal is to use something like the Greenplum Streaming Server (GPSS) which would write the data delivered from the client directly into the segments of the Greenplum Database cluster and would allow maximum parallelism and best stream loading performance.</p>
"
"66143603","Add delimiters while writing into a text file","<p>I am writing an array into the text file, which I later use to read in excel for plotting.
The data file is split in such a way that after 1000000 steps (approximately), the file closes and starts writing to another file.</p>
<p>However, my text file is writing data as a big chunk of values without any separators.
Please refer the code below and let me know where I am going wrong.</p>
<pre><code>counter = 1
for i in range(0, len(abc_value), 1000000):
    with open(f&quot;abc{counter}.txt&quot;, &quot;w&quot;) as file:
        for val in abc_value[i:i + 1000000]:
            file.write(str(val))
        file.close()
        counter += 1
</code></pre>
<p>Thank you!</p>
","<python><large-data><large-data-volumes><data-management>","2021-02-10 19:20:04","2074","-1","2","66143877","<p>Please see example below for using <a href=""https://docs.python.org/3/library/csv.html#csv.writer"" rel=""nofollow noreferrer"">csv writer</a>.</p>
<pre><code>import csv
with open('abc.txt', 'w') as csv_file:
    wrtr = csv.writer(csv_file)
    for val in range(10):
        wrtr.writerow(str(val))
                                   
</code></pre>
"
"66143603","Add delimiters while writing into a text file","<p>I am writing an array into the text file, which I later use to read in excel for plotting.
The data file is split in such a way that after 1000000 steps (approximately), the file closes and starts writing to another file.</p>
<p>However, my text file is writing data as a big chunk of values without any separators.
Please refer the code below and let me know where I am going wrong.</p>
<pre><code>counter = 1
for i in range(0, len(abc_value), 1000000):
    with open(f&quot;abc{counter}.txt&quot;, &quot;w&quot;) as file:
        for val in abc_value[i:i + 1000000]:
            file.write(str(val))
        file.close()
        counter += 1
</code></pre>
<p>Thank you!</p>
","<python><large-data><large-data-volumes><data-management>","2021-02-10 19:20:04","2074","-1","2","66143961","<p>The idea is correct but you are reading the values ​​of the array and they do not have a comma. You have to add the comma when passing the value to string.
A simple way to do it would be:</p>
<pre><code>counter = 1
for i in range(0, len(abc_value), 1000000):
    with open(f&quot;hola.txt&quot;, &quot;w&quot;) as file:
        file.write(str(abc_value[i]))
        for val in abc_value[i+1:i + 1000000]:
            file.write(&quot;, &quot; + str(val))
        file.close()
        counter += 1
</code></pre>
"
"66142021","Can Azure SQL data warehouse (Synapse Analytics) be installed in on-premise?","<p>To store and analyze private data, can Azure Synapse Analytics be installed in their bare metal? i.e. an Azure SQL data warehouse installed in private infrastructure.</p>
","<azure-synapse>","2021-02-10 17:34:04","1128","0","2","66145639","<p>No.</p>
<hr />
<p>I tried to find some source to quote, but hard to prove a negative. Only things you would find that you can download and install are tools/SDKs to connect to DB/Workspaces in cloud for development, not DB itself.</p>
"
"66142021","Can Azure SQL data warehouse (Synapse Analytics) be installed in on-premise?","<p>To store and analyze private data, can Azure Synapse Analytics be installed in their bare metal? i.e. an Azure SQL data warehouse installed in private infrastructure.</p>
","<azure-synapse>","2021-02-10 17:34:04","1128","0","2","66148553","<p>Short answer is NO.  Azure is not just VMs running in the cloud so how would you expect to run a PaaS system on-premises?</p>
<p>In saying that, you do have options.  The on-premises version of a MPP data engine is <a href=""https://www.microsoft.com/en-au/sql-server/analytics-platform-system?WT.mc_id=DP-MVP-5001259"" rel=""nofollow noreferrer"">Microsoft APS</a>.  This requires specific hardware and is expensive, but gives the same MPP experience as Synapse Dedicated Pools.</p>
<p><a href=""https://learn.microsoft.com/en-us/sql/big-data-cluster/big-data-cluster-overview?WT.mc_id=DP-MVP-5001259&amp;view=sql-server-ver15"" rel=""nofollow noreferrer"">Big Data Clusters</a> may also help to implement what you need. This runs on a Kubernetes cluster which you can easily run on-premises.</p>
<p>Finally - the true way to run Azure PaaS services on-premises WILL be <a href=""https://learn.microsoft.com/en-au/azure/azure-arc/data/overview?WT.mc_id=DP-MVP-5001259"" rel=""nofollow noreferrer"">Azure Arc</a>.  I say <strong>WILL BE</strong> as it is still preview and currently only enables Azure SQL Managed Instance and PostgreSQL Hyperscale. There is no guarantee that Synapse will ever be enabled on this platform and if it does come it is still a long way off.</p>
<p>If you are <strong>only</strong> concerned with the privacy of data then this can be crafted with attention paid to encryption, data masking, use of Azure Managed Identities and RBAC roles, and the use of <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/security/how-to-connect-to-workspace-with-private-links?WT.mc_id=DP-MVP-5001259"" rel=""nofollow noreferrer"">Private Links</a> to restrict access to specific VNETs</p>
"
"66134334","RoomDB SQL Query / data modelling problem","<p>I'm using RoomDB to store entities in my app, so far I've only needed relatively trivial queries such as <code>SELECT * FROM table</code> and <code>DELETE * FROM table</code> as well as slightly more complex things such as a join of 2 tables to return composite models, like <code>SELECT * from tableA INNER JOIN ON tableB tableA.tableBId = tableB.tableBId</code>.</p>
<p>So far, so good. Now, I have something I've not dealt with before, and I'm not sure whether I'm A) modelling my data wrong (so no query will be possible to create these models) or B) I don't know parts of SQL that are required to make this work.</p>
<p>I'm far from a SQL expert, but have used it on and off for a few years.</p>
<p>Below are the simplest example of the models (marked with @Entity such that RoomDB tables are generated) of what I'm trying to achieve:</p>
<pre><code>data class FooPopulated(
    val fooId: Long,
    @Embedded
    val barOne: Bar,
    @Embedded
    val barTwo: Bar,
    @Embedded
    val barThree: Bar
)

@Entity(primaryKeys = [&quot;fooId&quot;])
data class FooUnpopulated(
    val fooId: Long,
    val barOneId: Long,
    val barTwoId: Long,
    val barThreeId: Long
)

@Entity(primaryKeys = [&quot;barId&quot;])
data class Bar(
    val barId: Long,
    val name: String
)
</code></pre>
<p>So there will be 2 tables; <code>Bar</code> and <code>FooUnpopulated</code> and I want to run a query that will &quot;join&quot; the <code>FooUnpopulated</code> table entries with the <code>Bar</code> table entries, in such a way that I get back a collection of <code>FooPopulated</code>.</p>
<p>I am stumped, can anyone advise? Is my <code>FooPopulated</code> model just &quot;wrong&quot; in that it doesn't define a valid relationship which can be satisfied by a SQL query, or, am I missing that magic SQL keywords/understanding to make this happen?</p>
","<android><sql><kotlin><android-room>","2021-02-10 09:38:33","28","0","1","66134767","<p>You can use one to one relation and can define FooPopulated like:</p>
<pre><code>data class FooPopulated{
 @Embeded
 val fooUnpopulated: FooUnpopulated,
 @Relation(parentColumn = &quot;barOneId&quot;, childColumn = &quot;barId&quot;)
 val barOne: Bar,
 @Relation( parentColumn = &quot;barTwoId&quot;, childColumn = &quot;barId&quot;)
 val barTwo: Bar,
 @Relation( parentColumn = &quot;barThreeId&quot;, childColumn = &quot;barId&quot;)
 val barThree: Bar
}
</code></pre>
<p>Now you can define a function in your DAO to get this object:</p>
<pre><code>@Transaction
@Query(&quot;YOUR QUERY HERE&quot;)
fun getFooPopulated(): List&lt;FooPopulated&gt;
</code></pre>
<p>More details here:
<a href=""https://developer.android.com/training/data-storage/room/relationships#one-to-one"" rel=""nofollow noreferrer"">https://developer.android.com/training/data-storage/room/relationships#one-to-one</a></p>
"
"66127434","Splitting large data file in python","<p>I output a large array into a text file in python. I then read it in excel to plot the data.
Currently, the file I am writing is too large to read in excel.</p>
<p>I use file open and close functions and write the data in array ( please refer the code):</p>
<pre class=""lang-py prettyprint-override""><code>with open(&quot;abc.txt&quot;, &quot;w&quot;) as file:
    file.write(str(abc_value))
    file.close()
</code></pre>
<p>Question: How can I split the data file so that after 1000000 steps (approximately), the file closes and starts writing to another file.</p>
<p>At the end, there should be multiple data files which I can read in excel separately.</p>
<p>Any leads much appreciated!</p>
","<python><excel><large-data><large-data-volumes><data-management>","2021-02-09 21:28:57","136","1","1","66128157","<p>I am not sure what is type(abc_value) originally, but if you can submit in in a form of array this code should work:</p>
<pre><code>counter = 1
for i in range(0, len(abc_value), 1000000):
    with open(f&quot;abc{counter}.txt&quot;, &quot;w&quot;) as file:
        for val in abc_value[i:i + 1000000]:
            file.write(str(val))
        file.close()
        counter += 1
</code></pre>
<p>The main idea is just to split your original data and then create and open deferent files in for loop.</p>
<p>Output files should be &quot;abc1.txt&quot;,&quot;abc2.txt&quot;,...</p>
<p>Hope I understood your question correctly and this answers it.</p>
"
"66102886","Great Expectations: base_directory must be an absolute path if root_directory is not provided","<p>This is about Great Expectations module in python primarily used for data quality checks (I found their documentation to be inadequate). So I've been trying to set up the data context on my notebook (using a local datasource) - as mentioned in:</p>
<p><a href=""https://docs.greatexpectations.io/en/latest/guides/how_to_guides/configuring_data_contexts/how_to_instantiate_a_data_context_without_a_yml_file.html#how-to-guides-configuring-data-contexts-how-to-instantiate-a-data-context-without-a-yml-file"" rel=""nofollow noreferrer"">https://docs.greatexpectations.io/en/latest/guides/how_to_guides/configuring_data_contexts/how_to_instantiate_a_data_context_without_a_yml_file.html#how-to-guides-configuring-data-contexts-how-to-instantiate-a-data-context-without-a-yml-file</a></p>
<p>Following is my code :</p>
<pre><code>from great_expectations.data_context.types.base import DataContextConfig
from great_expectations.data_context.types.base import DatasourceConfig
from great_expectations.data_context.types.base import FilesystemStoreBackendDefaults
from great_expectations.data_context import BaseDataContext

data_context_config = DataContextConfig(
    datasources={
        &quot;debaprc_test&quot;: DatasourceConfig(
            class_name=&quot;PandasDatasource&quot;,
            batch_kwargs_generators={
                &quot;subdir_reader&quot;: {
                    &quot;class_name&quot;: &quot;SubdirReaderBatchKwargsGenerator&quot;,
                    &quot;base_directory&quot;: &quot;/Users/debaprc/Downloads&quot;              
                }
            },
        )
    },
    store_backend_defaults=FilesystemStoreBackendDefaults(root_directory=&quot;/Users/debaprc/GE_Test/New/&quot;)
)

context = BaseDataContext(project_config=data_context_config)
</code></pre>
<p>And this is the error I get:</p>
<p><code>base_directory must be an absolute path if root_directory is not provided</code></p>
<p>What am I doing wrong?</p>
","<python><data-quality><great-expectations>","2021-02-08 13:45:59","2394","4","1","66105460","<p>Thank you so much for using Great Expectations. That is a known issue with our latest upgrade of the Checkpoints feature, which was fixed on our develop branch. Please install from the develop branch or wait until our next release 0.13.9 coming this week.</p>
"
"66074276","ETL / Replication from Azure SQL Database to on-premises SQL Server data warehouse","<p>I'm trying to build daily ETL process from Azure SQL Database to on-premises SQL Server data warehouse.</p>
<p>I have experience in ETL between on-premises SQL Servers but when it comes to Azure I'm not sure the best practise. I did some researches and got the options on ETL and ADP.</p>
<p>Would anyone here have similar experience before, could shed me some lights? Any comment, example or  tutorial are much appreciated.</p>
","<sql-server><azure><etl>","2021-02-06 06:44:00","169","-1","1","66097995","<p>There are many ways can help you build daily ETL process from Azure SQL Database to on-premises SQL Server data warehouse.</p>
<p>I would suggest you think about <a href=""https://learn.microsoft.com/en-us/azure/data-factory/introduction"" rel=""nofollow noreferrer"">Azure Data Factory</a> or Azure SQL database <a href=""https://learn.microsoft.com/en-us/azure/azure-sql/database/sql-data-sync-data-sql-server-sql-database"" rel=""nofollow noreferrer"">Data Sync</a>.</p>
<p>Data Factory can help you build a pipeline to copy the data between Azure SQL database and On-premise SQL Server. You could create a time trigger to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#trigger-execution"" rel=""nofollow noreferrer"">trigger</a> the copy daily. Ref these documents:</p>
<ol>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database"" rel=""nofollow noreferrer"">Copy and transform data in Azure SQL Database by using Azure Data
Factory</a></li>
<li><a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server"" rel=""nofollow noreferrer"">Copy data to and from SQL Server by using Azure Data Factory</a></li>
</ol>
<p>Data Sync is a feature of Azure SQL database, you can sync the data between SQL database and on-premise SQL Server automatically. I think that's the easiest and cheapest way which can achieve your request in some way. Just note the <a href=""https://learn.microsoft.com/en-us/azure/azure-sql/database/sql-data-sync-data-sql-server-sql-database#sync-req-lim"" rel=""nofollow noreferrer"">requirements and limitations</a>.</p>
<p>And like @Nick said, Azure SQL database doesn't support windows authentication, please use SQL authentication or AD.</p>
<p>HTH.</p>
"
"66071767","Theoretically, are DATE and TIME two different variables?","<p>I’m curious to find out if, in terms of tidy data principles, a column containing “date and time” ( 1/1/21 11:31) would be considered as a single variable or tow separate ones?</p>
","<data-processing><data-quality>","2021-02-05 23:03:00","28","-1","1","66071796","<p>Timestamp:</p>
<p>An important difference is that DATETIME represents a date (as found in a calendar) and a time (as can be observed on a wall clock), while TIMESTAMP represents a well defined point in time.</p>
"
"66060470","will powerBI reports need to be republished as part of migrating on prem data warehouse to Azure","<p>My organisation will be moving from a on premise sql server data warehouse to azure sql database. </p>
<p>Currently, we use a gateway to allow connectivity to on premise Data warehouse</p>
<p>The azure database will be renamed as part of the migration. </p>
<p>Does this mean that all PowerBI reports will need to be republised pointing to the new azure DWH?</p>
","<powerbi><powerbi-desktop><powerbi-datasource>","2021-02-05 09:10:59","177","0","1","66062643","<p>If the database schema remains the same, then you can simply use <a href=""https://learn.microsoft.com/en-us/rest/api/power-bi/"" rel=""nofollow noreferrer"">Power BI REST API</a> to <a href=""https://learn.microsoft.com/en-us/rest/api/power-bi/datasets/updatedatasourcesingroup"" rel=""nofollow noreferrer"">change the datasource</a> and <a href=""https://learn.microsoft.com/en-us/rest/api/power-bi/gateways/updatedatasource"" rel=""nofollow noreferrer"">patch the credentials</a>. This can be done for example with PowerShell using <a href=""https://powerbi.microsoft.com/en-us/blog/announcing-apis-and-powershell-cmdlets-for-power-bi-administrators/"" rel=""nofollow noreferrer"">Power BI Management CmdLets</a>. First, start PowerShell as administrator and install the CmdLets by executing the following script:</p>
<pre><code>Install-Module MicrosoftPowerBIMgmt
</code></pre>
<p>If you don't have admin rights on the computer, or for some other reason do not want to install these for all users, you can install them for the current user only:</p>
<pre><code>Install-Module MicrosoftPowerBIMgmt -Scope CurrentUser
</code></pre>
<p>Update the values in the first few lines to point the Azure SQL database and published report, then execute the script to change the data source:</p>
<pre><code># Fill these ###################################################
$workspaceName = &quot;The name of the workspace where the report is published&quot;
$datasetName = &quot;The name of the report&quot;
$sqlDatabaseServer = &quot;servername.database.windows.net&quot;
$sqlDatabaseName = &quot;Database name&quot;
$username = &quot;powerbiuser@yourcompany.com&quot;
$password = &quot;the password of the power bi user&quot; | ConvertTo-SecureString -asPlainText -Force
################################################################

Import-Module MicrosoftPowerBIMgmt

Clear-Host

$credential = New-Object System.Management.Automation.PSCredential($username, $password)

Connect-PowerBIServiceAccount -Credential $credential | Out-Null

$workspace = Get-PowerBIWorkspace -Name $workspaceName

$dataset = Get-PowerBIDataset -WorkspaceId $workspace.Id -Name $datasetName

$datasource = Get-PowerBIDatasource -WorkspaceId $workspace.Id -DatasetId $dataset.Id

# Construct url
$workspaceId = $workspace.Id
$datasetId = $dataset.Id
$datasourceUrl = &quot;groups/$workspaceId/datasets/$datasetId/datasources&quot;

# Call the REST API to get gateway Id, datasource Id and current connection details
$datasourcesResult = Invoke-PowerBIRestMethod -Method Get -Url $datasourceUrl | ConvertFrom-Json

# Parse the response
$datasource = $datasourcesResult.value[0]
$gatewayId = $datasource.gatewayId
$datasourceId = $datasource.datasourceId
$sqlDatabaseServerCurrent = $datasource.connectionDetails.server
$sqlDatabaseNameCurrent = $datasource.connectionDetails.database

# Construct url for update
$datasourePatchUrl = &quot;groups/$workspaceId/datasets/$datasetId/Default.UpdateDatasources&quot;

# create HTTP request body to update datasource connection details
$postBody = @{
  &quot;updateDetails&quot; = @(
   @{
    &quot;connectionDetails&quot; = @{
      &quot;server&quot; = &quot;$sqlDatabaseServer&quot;
      &quot;database&quot; = &quot;$sqlDatabaseName&quot;
    }
    &quot;datasourceSelector&quot; = @{
      &quot;datasourceType&quot; = &quot;Sql&quot;
      &quot;connectionDetails&quot; = @{
        &quot;server&quot; = &quot;$sqlDatabaseServerCurrent&quot;
        &quot;database&quot; = &quot;$sqlDatabaseNameCurrent&quot;
      }
      &quot;gatewayId&quot; = &quot;$gatewayId&quot;
      &quot;datasourceId&quot; = &quot;$datasourceId&quot;
    }
  })
}

$postBodyJson = ConvertTo-Json -InputObject $postBody -Depth 6 -Compress

# Execute POST operation to update datasource connection details
Invoke-PowerBIRestMethod -Method Post -Url $datasourePatchUrl -Body $postBodyJson

# NOTE: dataset credentials must be reset after updating connection details
</code></pre>
<p>Now update the values in the beginning of the following script and execute it to patch the credentials for your production database:</p>
<pre><code># Fill these ###################################################
$workspaceName = &quot;The name of the workspace where the report is published&quot;
$reportName = &quot;The name of the report&quot;
$sqlUserName = &quot;user name&quot;
$sqlUserPassword = &quot;password&quot;
$username = &quot;powerbiuser@yourcompany.com&quot;
$password = &quot;the password of the power bi user&quot; | ConvertTo-SecureString -asPlainText -Force
################################################################

Import-Module MicrosoftPowerBIMgmt

Clear-Host

$credential = New-Object System.Management.Automation.PSCredential($username, $password)

Connect-PowerBIServiceAccount -Credential $credential | Out-Null

$workspace = Get-PowerBIWorkspace -Name $workspaceName

$dataset = Get-PowerBIDataset -WorkspaceId $workspace.Id -Name $reportName

$workspaceId = $workspace.Id
$datasetId = $dataset.Id

$datasources = Get-PowerBIDatasource -WorkspaceId $workspaceId -DatasetId $datasetId

foreach($datasource in $datasources) {

  $gatewayId = $datasource.gatewayId
  $datasourceId = $datasource.datasourceId
  $datasourePatchUrl = &quot;gateways/$gatewayId/datasources/$datasourceId&quot;

  Write-Host &quot;Patching credentials for $datasourceId&quot;

  # HTTP request body to patch datasource credentials
  $userNameJson = &quot;{&quot;&quot;name&quot;&quot;:&quot;&quot;username&quot;&quot;,&quot;&quot;value&quot;&quot;:&quot;&quot;$sqlUserName&quot;&quot;}&quot;
  $passwordJson = &quot;{&quot;&quot;name&quot;&quot;:&quot;&quot;password&quot;&quot;,&quot;&quot;value&quot;&quot;:&quot;&quot;$sqlUserPassword&quot;&quot;}&quot;

  $patchBody = @{
    &quot;credentialDetails&quot; = @{
      &quot;credentials&quot; = &quot;{&quot;&quot;credentialData&quot;&quot;:[ $userNameJson, $passwordJson ]}&quot;
      &quot;credentialType&quot; = &quot;Basic&quot;
      &quot;encryptedConnection&quot; =  &quot;NotEncrypted&quot;
      &quot;encryptionAlgorithm&quot; = &quot;None&quot;
      &quot;privacyLevel&quot; = &quot;Organizational&quot;
    }
  }

  # Convert body contents to JSON
  $patchBodyJson = ConvertTo-Json -InputObject $patchBody -Depth 6 -Compress

  # Execute PATCH operation to set datasource credentials
  Invoke-PowerBIRestMethod -Method Patch -Url $datasourePatchUrl -Body $patchBodyJson
}

$datasetRefreshUrl = &quot;groups/$workspaceId/datasets/$datasetId/refreshes&quot;

Write-Host &quot;Refreshing...&quot;

Invoke-PowerBIRestMethod -Method Post -Url $datasetRefreshUrl 
</code></pre>
<p>Of course, these scripts can be extended to automatically process all reports in a workspace, or to enumerate all workspaces, etc. but this depends on the deployment in your organization, for which we do not know the details.</p>
"
"66036461","Get the FIRST_VALUE from the current partition since stage has changed","<p>I want to use the FIRST_VALUE() function in order to have the first &quot;updated_at&quot; value. But what I want is to have the first value since the stage has changed.</p>
<pre><code>with values as (

    select 1 as deal, 2 as stage, '2020-11-10' as updated_at, '2020-11-10' as first_updated_at
    union all
    select 1 as deal, 2 as stage, '2020-11-11' as updated_at, '2020-11-10' as first_updated_at 
    union all
    select 1 as deal, 3 as stage, '2020-11-12' as updated_at, '2020-11-12' as first_updated_at
    union all 
    select 1 as deal, 4 as stage, '2020-11-13' as updated_at, '2020-11-13' as first_updated_at
    union all 
    select 1 as deal, 4 as stage, '2020-11-14' as updated_at, '2020-11-13' as first_updated_at
    union all 
    select 1 as deal, 2 as stage, '2020-11-15' as updated_at, '2020-11-15' as first_updated_at
    union all 
    select 1 as deal, 2 as stage, '2020-11-16' as updated_at, '2020-11-15' as first_updated_at

)
select * from values
</code></pre>
<p>I am trying to use the FIRST VALUE function like this:</p>
<p><code>FIRST_VALUE(updated_at) OVER(PARTITION BY deal, stageORDER BY updated_at ASC </code></p>
<p>Am I missing something here or what I want is not possible?</p>
<p>Thanks in advance</p>
","<sql><google-bigquery><data-warehouse>","2021-02-03 22:10:04","113","1","2","66036507","<p>You can do this using window functions.  First, look at the previous stage to see if it is <em>different</em> from the previous row.  Then use a cumulative max to take the <code>updated_at</code> when this occurs:</p>
<pre><code>with values as (

    select 1 as deal, 2 as stage, '2020-11-10' as updated_at, '2020-11-10' as first_updated_at
    union all
    select 1 as deal, 2 as stage, '2020-11-11' as updated_at, '2020-11-10' as first_updated_at 
    union all
    select 1 as deal, 3 as stage, '2020-11-12' as updated_at, '2020-11-12' as first_updated_at
    union all 
    select 1 as deal, 4 as stage, '2020-11-13' as updated_at, '2020-11-13' as first_updated_at
    union all 
    select 1 as deal, 4 as stage, '2020-11-14' as updated_at, '2020-11-13' as first_updated_at
    union all 
    select 1 as deal, 2 as stage, '2020-11-15' as updated_at, '2020-11-15' as first_updated_at
    union all 
    select 1 as deal, 2 as stage, '2020-11-16' as updated_at, '2020-11-15' as first_updated_at

)
select v.*,
       max(case when stage &lt;&gt; prev_stage or prev_stage is null then updated_at end) over (partition by deal order by updated_at) as imputed_first_updated_at
from (select v.*,
             lag(stage) over (partition by deal order by updated_at) as prev_stage
      from values v
     ) v
</code></pre>
"
"66036461","Get the FIRST_VALUE from the current partition since stage has changed","<p>I want to use the FIRST_VALUE() function in order to have the first &quot;updated_at&quot; value. But what I want is to have the first value since the stage has changed.</p>
<pre><code>with values as (

    select 1 as deal, 2 as stage, '2020-11-10' as updated_at, '2020-11-10' as first_updated_at
    union all
    select 1 as deal, 2 as stage, '2020-11-11' as updated_at, '2020-11-10' as first_updated_at 
    union all
    select 1 as deal, 3 as stage, '2020-11-12' as updated_at, '2020-11-12' as first_updated_at
    union all 
    select 1 as deal, 4 as stage, '2020-11-13' as updated_at, '2020-11-13' as first_updated_at
    union all 
    select 1 as deal, 4 as stage, '2020-11-14' as updated_at, '2020-11-13' as first_updated_at
    union all 
    select 1 as deal, 2 as stage, '2020-11-15' as updated_at, '2020-11-15' as first_updated_at
    union all 
    select 1 as deal, 2 as stage, '2020-11-16' as updated_at, '2020-11-15' as first_updated_at

)
select * from values
</code></pre>
<p>I am trying to use the FIRST VALUE function like this:</p>
<p><code>FIRST_VALUE(updated_at) OVER(PARTITION BY deal, stageORDER BY updated_at ASC </code></p>
<p>Am I missing something here or what I want is not possible?</p>
<p>Thanks in advance</p>
","<sql><google-bigquery><data-warehouse>","2021-02-03 22:10:04","113","1","2","66038095","<blockquote>
<p>I am trying to use the FIRST VALUE function ...</p>
</blockquote>
<p>Consider below option</p>
<pre><code>select * except(updated_at_on_change), 
  ifnull(updated_at_on_change, first_value(updated_at ignore nulls) over win) as first_updated_at
from (
  select *,
    if(stage != ifnull(lag(stage) over win, stage - 1), updated_at, null) updated_at_on_change
  from values
  window win as (partition by deal order by updated_at)
)
window win as (partition by deal order by updated_at desc rows between 1 following and unbounded following )
# order by updated_at   
</code></pre>
<p>If applied to sample data in your question - output is</p>
<p><a href=""https://i.stack.imgur.com/mano3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mano3.png"" alt=""enter image description here"" /></a></p>
"
"66027672","How to query 3 tables in PSQL?","<p>I am new to databases, I am not sure how to query these 3 tables.</p>
<p>I am trying to find out the total deaths(Table COVID - TotalDeathUK) of each strain(Table Strain) for the last 1 year(Table - Time?)</p>
<p>Table COVID</p>
<pre><code> centerid | patientid | dateid | strainid | totaldeathsuk | 
----------+-----------+--------+----------+---------------+
   100000 |    100000 | 100000 |   100000 |         31369 |       
   100000 |    100000 | 100000 |   100000 |         95382 |       
   100004 |    100004 | 100004 |   100000 |         46031 |

----------------------------------------------------------------
</code></pre>
<p>Table Strain</p>
<pre><code> strainid |   name    | risk 
----------+-----------+------
   100000 | Fixflex   |   50
   100001 | Cardguard |   85
   100002 | Zaam-Dox  |   53
</code></pre>
<p>Table Time</p>
<pre><code> dateid | year |    month     | quater | week | dayofweek |   hour   
--------+------+--------------+--------+------+-----------+----------
 100007 | 2039 | Thin Ice     |      2 |    3 |         7 | 11:54:00
 100008 | 2109 | Parineeta    |      4 |    1 |         6 | 20:12:00
 100011 | 2096 | Crawlspace   |      4 |    2 |         7 | 19:03:00
</code></pre>
<p>Link to Image of the Database Schema</p>
<p><a href=""https://i.stack.imgur.com/gXafr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gXafr.png"" alt=""enter image description here"" /></a></p>
","<postgresql><select><data-warehouse><querying>","2021-02-03 12:34:37","80","-4","1","66054271","
<blockquote>
<pre><code>CREATE TABLE Covid(CenterID int,PatientID int, DateID int,StrainID int,TotalDeathUK int);
CREATE TABLE Strain(StrainID int, Name varchar(20),Risk int);
CREATE TABLE Time(DateID int,Year int,Month varchar(20),Quarter int,Week int,DayOfWeek int,Hour varchar(8))
</code></pre>
</blockquote>

<blockquote>
<pre><code>INSERT INTO Covid(CenterID,PatientID, DateID,StrainID,TotalDeathUK)
VALUES(100000,100000,100000,100000,31369),
(100000,100000,100008,100000,95382),
(100004,100004,100011,100000,46031);
INSERT INTO Strain(StrainID, Name,Risk)
VALUES( 100000,'Fixflex ',50),( 100001,'Cardguard ',85)
,( 100002,'Zaam-Dox ',53);
INSERT INTO Time(DateID,Year,Month,Week,Quarter,DayOfWeek,Hour)VALUES(100000,2020,'Thin Ice',2,3,7,'11:54:00'),
(100008,2019,'Parineeta',4,1,6,'20:12:00'),
(100011,2018,'Crawlspace',4,2,7,'19:03:00');
</code></pre>
</blockquote>

<blockquote>
<pre><code>SELECT T.Year,S.StrainID,C.TotalDeathUK FROM Covid AS C
LEFT JOIN Strain AS S ON S.StrainID=C.StrainID
LEFT JOIN Time AS T ON T.DateID=C.DateID
WHERE T.Year=date_part('year', (SELECT current_timestamp))-1
</code></pre>
</blockquote>
<blockquote>
<pre>
year | strainid | totaldeathuk
---: | -------: | -----------:
2020 |   100000 |        31369
</pre>
</blockquote>
<p><em>db&lt;&gt;fiddle <a href=""https://dbfiddle.uk/?rdbms=postgres_13&amp;fiddle=2e108686adbd1d64fa763c750a60868b"" rel=""nofollow noreferrer"">here</a></em></p>
"
"66018449","T-SQL logic for roll up and group by","<p>I have a question to collapse or roll up data based on the logic below.
How can I implement it?</p>
<p>The logic that allows episodes to be condensed into a single continuous care episode is a discharge code of 22 followed by an admission code of 4 on the same day.</p>
<p><img src=""https://i.stack.imgur.com/SK2Uk.png"" alt=""continuous care implementation "" /><a href=""https://i.stack.imgur.com/SK2Uk.png"" rel=""nofollow noreferrer"">continuous care implementation update</a></p>
<p>EPN--is a business_key.</p>
<p>episode_continuous_care_key is an artificial key that can be a row number function.</p>
<p>Below is the table structure.</p>
<pre><code>            drop table #source
            CREATE TABLE #source(patidid varchar(20),epn int,preadmitdate datetime,adminttime varchar(10),
            admitcode varchar(10),datedischarge datetime,disctime varchar(10),disccode varchar(10))
            INSERT INTO #source VALUES
             (1849,1,'4/23/2020','7:29',1,'7/31/2020','9:03',22)
            ,(1849,2,'7/31/2020','11:00',4,'7/31/2020','12:09',22)
            ,(1849,3,'7/31/2020','13:10',4,'8/24/2020','10:36',10)
            ,(1849,4,'8/26/2020','12:25',2,null,null,null)
            ,(1850,1,'4/23/2020','7:33',1,'6/29/2020','7:30',22)
            ,(1850,2,'6/29/2020','9:35',4,'7/8/2020','10:51',7)
            ,(1850,3,'7/10/2020','11:51',3,'7/29/2020','9:12',7)
            ,(1850,4,'7/31/2020','11:00',2,'8/6/2020','10:24',22)
            ,(1850,5,'8/6/2020','12:26',4,null,null,null)
            ,(1851,1,'4/23/2020','7:35',1,'6/24/2020','13:45',22)
            ,(1851,2,'6/24/2020','15:06',4,'9/24/2020','15:00',2)
            ,(1851,3,'12/4/2020','8:59',0,null,null,null)
            ,(1852,1,'4/23/2020','7:37',1,'7/6/2020','11:15',20)
            ,(1852,2,'7/8/2020','10:56',0,'7/10/2020','11:46',2)
            ,(1852,3,'7/10/2020','11:47',2,'7/28/2020','13:16',22)
            ,(1852,4,'7/28/2020','15:17',4,'8/4/2020','11:37',22)
            ,(1852,5,'8/4/2020','13:40',4,'11/18/2020','15:43',2)
            ,(1852,6,'12/2/2020','15:23',2,null,null,null)
            ,(1853,1,'4/23/2020','7:40',1,'7/1/2020','8:30',22)
            ,(1853,2,'7/1/2020','14:57',4,'12/4/2020','12:55',7)
            ,(1854,1,'4/23/2020','7:44',1,'7/31/2020','13:07',20)
            ,(1854,2,'8/3/2020','16:30',0,'8/5/2020','9:32',2)
            ,(1854,3,'8/5/2020','10:34',2,'8/24/2020','8:15',22)
            ,(1854,4,'8/24/2020','10:33',4,'12/4/2020','7:30',22)
            ,(1854,5,'12/4/2020','9:13',4,null,null,null)
</code></pre>
","<sql-server><stored-procedures><data-warehouse>","2021-02-02 22:19:53","109","-2","2","66028226","<p>That Excel sheet image says little about your database design so I invented my own version that more or less resembles your image. With a proper database design the first step of the solution should not be required...</p>
<ol>
<li>Unpivot timestamp information so that admission timestamp and discharge timestamps become one column.<br>
<em>I used a common table expression <code>Log1</code> for this action.</em></li>
<li>Use the codes to filter out the start of the continuous care periods. Those are the admissions, marked with <code>Code.IsAdmission = 1</code> in my database design.<br>
Also add the next period start as another column by using the <code>lead()</code> function.<br>
<em>These are all the actions from <code>Log2</code>.</em></li>
<li>Add a row number as continuous care key.<br>
Using the next period start date, find the current continuous period end date with a <code>cross apply</code>.<br>
Replace empty period end dates with the current date using the <code>coalesce()</code> function.<br>
Calculate the difference as the continuous care period duration with the <code>datediff()</code> function.</li>
</ol>
<p><strong>Sample data</strong></p>
<pre><code>create table Codes
(
  Code int,
  Description nvarchar(50),
  IsAdmission bit
);

insert into Codes (Code, Description, IsAdmission) values
( 1, 'First admission', 1),
( 2, 'Re-admission', 1),
( 4, 'Campus transfer IN', 0),
(10, 'Trial visit', 0),
(22, 'Campus transfer OUT', 0);

create table PatientLogs
(
  PatientId int,
  AdmitDateTime smalldatetime,
  AdmitCode int,
  DischargeDateTime smalldatetime,
  DischargeCode int
);

insert into PatientLogs (PatientId, AdmitDateTime, AdmitCode, DischargeDateTime, DischargeCode) values
(1849, '2020-04-23 07:29', 1, '2020-07-31 09:03', 22),
(1849, '2020-07-31 11:00', 4, '2020-07-31 12:09', 22),
(1849, '2020-07-31 13:10', 4, '2020-08-24 10:36', 10),
(1849, '2020-08-26 12:25', 2, null, null);
</code></pre>
<p><strong>Solution</strong></p>
<pre><code>with Log1 as
(
  select updt.PatientId,
         case updt.DateTimeType
           when 'AdmitDateTime' then updt.AdmitCode
           when 'DischargeDateTime' then updt.DischargeCode
         end as Code,
         updt.LogDateTime,
         updt.DateTimeType
  from PatientLogs pl
  unpivot (LogDateTime for DateTimeType in (AdmitDateTime, DischargeDateTime)) updt
),
Log2 as (
  select l.PatientId,
         l.Code,
         l.LogDateTime,
         lead(l.LogDateTime) over(partition by l.PatientId order by l.LogDateTime) as LogDateTimeNext
  from Log1 l
  join Codes c
    on c.Code = l.Code
  where c.IsAdmission = 1
)
select la.PatientId,
       row_number() over(partition by la.PatientId order by la.LogDateTime) as ContCareKey,
       la.LogDateTime as AdmitDateTime,
       coalesce(ld.LogDateTime, convert(smalldatetime, getdate())) as DischargeDateTime,
       datediff(day, la.LogDateTime, coalesce(ld.LogDateTime, convert(smalldatetime, getdate()))) as ContStay
from Log2 la -- log admission
outer apply ( select top 1 l1.LogDateTime
              from Log1 l1
              where l1.PatientId = la.PatientId
                and l1.LogDateTime &lt; la.LogDateTimeNext
              order by l1.LogDateTime desc ) ld -- log discharge
order by la.PatientId,
         la.LogDateTime;
</code></pre>
<p><strong>Result</strong></p>
<pre><code>PatientId  ContCareKey  AdmitDateTime     DischargeDateTime  ContStay
---------  -----------  ----------------  -----------------  --------
1849       1            2020-04-23 07:29  2020-08-24 10:36   123
1849       2            2020-08-26 12:25  2021-02-03 12:49   161
</code></pre>
<p><a href=""https://dbfiddle.uk/?rdbms=sqlserver_2019l&amp;fiddle=8e748cb924375943260885fc7abef888"" rel=""nofollow noreferrer"">Fiddle</a> to see things in action with intermediate results.</p>
"
"66018449","T-SQL logic for roll up and group by","<p>I have a question to collapse or roll up data based on the logic below.
How can I implement it?</p>
<p>The logic that allows episodes to be condensed into a single continuous care episode is a discharge code of 22 followed by an admission code of 4 on the same day.</p>
<p><img src=""https://i.stack.imgur.com/SK2Uk.png"" alt=""continuous care implementation "" /><a href=""https://i.stack.imgur.com/SK2Uk.png"" rel=""nofollow noreferrer"">continuous care implementation update</a></p>
<p>EPN--is a business_key.</p>
<p>episode_continuous_care_key is an artificial key that can be a row number function.</p>
<p>Below is the table structure.</p>
<pre><code>            drop table #source
            CREATE TABLE #source(patidid varchar(20),epn int,preadmitdate datetime,adminttime varchar(10),
            admitcode varchar(10),datedischarge datetime,disctime varchar(10),disccode varchar(10))
            INSERT INTO #source VALUES
             (1849,1,'4/23/2020','7:29',1,'7/31/2020','9:03',22)
            ,(1849,2,'7/31/2020','11:00',4,'7/31/2020','12:09',22)
            ,(1849,3,'7/31/2020','13:10',4,'8/24/2020','10:36',10)
            ,(1849,4,'8/26/2020','12:25',2,null,null,null)
            ,(1850,1,'4/23/2020','7:33',1,'6/29/2020','7:30',22)
            ,(1850,2,'6/29/2020','9:35',4,'7/8/2020','10:51',7)
            ,(1850,3,'7/10/2020','11:51',3,'7/29/2020','9:12',7)
            ,(1850,4,'7/31/2020','11:00',2,'8/6/2020','10:24',22)
            ,(1850,5,'8/6/2020','12:26',4,null,null,null)
            ,(1851,1,'4/23/2020','7:35',1,'6/24/2020','13:45',22)
            ,(1851,2,'6/24/2020','15:06',4,'9/24/2020','15:00',2)
            ,(1851,3,'12/4/2020','8:59',0,null,null,null)
            ,(1852,1,'4/23/2020','7:37',1,'7/6/2020','11:15',20)
            ,(1852,2,'7/8/2020','10:56',0,'7/10/2020','11:46',2)
            ,(1852,3,'7/10/2020','11:47',2,'7/28/2020','13:16',22)
            ,(1852,4,'7/28/2020','15:17',4,'8/4/2020','11:37',22)
            ,(1852,5,'8/4/2020','13:40',4,'11/18/2020','15:43',2)
            ,(1852,6,'12/2/2020','15:23',2,null,null,null)
            ,(1853,1,'4/23/2020','7:40',1,'7/1/2020','8:30',22)
            ,(1853,2,'7/1/2020','14:57',4,'12/4/2020','12:55',7)
            ,(1854,1,'4/23/2020','7:44',1,'7/31/2020','13:07',20)
            ,(1854,2,'8/3/2020','16:30',0,'8/5/2020','9:32',2)
            ,(1854,3,'8/5/2020','10:34',2,'8/24/2020','8:15',22)
            ,(1854,4,'8/24/2020','10:33',4,'12/4/2020','7:30',22)
            ,(1854,5,'12/4/2020','9:13',4,null,null,null)
</code></pre>
","<sql-server><stored-procedures><data-warehouse>","2021-02-02 22:19:53","109","-2","2","66048724","<p>Here is a T-SQL solution that contains primary and foreign key relationships.</p>
<p>To make it a bit more realistic, I added a simple &quot;Patient&quot; table.</p>
<p>I put all your &quot;codes&quot; into a single table which should make it easier to manage the codes.</p>
<p>I do not understand the purpose of your concept of &quot;continuous care&quot; so I just added an &quot;is first&quot; binary column to the Admission table.
You might also consider adding something about the medical condition for which the patient is being treated.</p>
<p><a href=""https://i.stack.imgur.com/k4V5s.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/k4V5s.jpg"" alt=""Codes schema"" /></a></p>
<pre><code>    CREATE SCHEMA Codes
GO

GO


CREATE TABLE dbo.Code
(
    codeNr int NOT NULL,
    description nvarchar(50),
    CONSTRAINT Code_PK PRIMARY KEY(codeNr)
)
GO


CREATE TABLE dbo.Patient
(
    patientNr int NOT NULL,
    birthDate date NOT NULL,
    firstName nvarchar(max) NOT NULL,
    lastName nvarchar(max) NOT NULL,
    CONSTRAINT Patient_PK PRIMARY KEY(patientNr)
)
GO


CREATE TABLE dbo.Admission
(
    admitDateTime time NOT NULL,
    patientNr int NOT NULL,
    admitCode int,
    isFirst bit,
    CONSTRAINT Admission_PK PRIMARY KEY(patientNr, admitDateTime)
)
GO


CREATE TABLE dbo.Discharge
(
    dischargeDateTime time NOT NULL,
    patientNr int NOT NULL,
    dischargeCode int NOT NULL,
    CONSTRAINT Discharge_PK PRIMARY KEY(patientNr, dischargeDateTime)
)
GO


ALTER TABLE dbo.Admission ADD CONSTRAINT Admission_FK1 FOREIGN KEY (patientNr) REFERENCES dbo.Patient (patientNr) ON DELETE NO ACTION ON UPDATE NO ACTION
GO


ALTER TABLE dbo.Admission ADD CONSTRAINT Admission_FK2 FOREIGN KEY (admitCode) REFERENCES dbo.Code (codeNr) ON DELETE NO ACTION ON UPDATE NO ACTION
GO


ALTER TABLE dbo.Discharge ADD CONSTRAINT Discharge_FK1 FOREIGN KEY (patientNr) REFERENCES dbo.Patient (patientNr) ON DELETE NO ACTION ON UPDATE NO ACTION
GO


ALTER TABLE dbo.Discharge ADD CONSTRAINT Discharge_FK2 FOREIGN KEY (dischargeCode) REFERENCES dbo.Code (codeNr) ON DELETE NO ACTION ON UPDATE NO ACTION
GO


GO

</code></pre>
"
"66013805","Checking Slowly Changing Dimension 2","<p>I have a table that looks like this:</p>
<p><img src=""https://i.stack.imgur.com/jjHOm.jpg"" alt=""enter image description here"" /></p>
<p>A slowly changing dimension type 2, according to Kimball.</p>
<p>Key is just a surrogate key, a key to make rows unique.</p>
<p>As you can see there are three rows for product A.
Timelines for this product are ok. During time the description of the product changes.
From 1-1-2020 up until 4-1-2020 the description of this product was ProdA1.
From 5-1-2020 up until 12-2-2020 the description of this product was ProdA2 etc.</p>
<p>If you look at product B, you see there are gaps in the timeline.</p>
<p>We use DB2 V12 z/Os. How can I check if there are gaps in the timelines for each and every product?</p>
<p>Tried this, but doesn't work</p>
<pre><code>with selectie (key, tel) as
  (select product, count(*)
    from PROD_TAB
   group by product
    having count(*) &gt; 1)
Select * from
PROD_TAB A
inner join selectie B
on A.product = B.product
Where not exists
(SELECT 1 from PROD_TAB C
WHERE A.product = C.product
AND A.END_DATE + 1 DAY = C.START_DATE
)
</code></pre>
<p>Does anyone know the answer?</p>
","<db2><data-warehouse>","2021-02-02 16:33:35","55","0","2","66023678","<p>The following query returns all gaps for all products.<br />
The idea is to enumerate (RN column) all periods inside each product by START_DATE and join each record with its next period record.</p>
<pre><code>WITH
/*
  MYTAB (PRODUCT, DESCRIPTION, START_DATE, END_DATE) AS
(
            SELECT 'A', 'ProdA1', DATE('2020-01-01'), DATE('2020-01-04') FROM SYSIBM.SYSDUMMY1
  UNION ALL SELECT 'A', 'ProdA2', DATE('2020-01-05'), DATE('2020-02-12') FROM SYSIBM.SYSDUMMY1
  UNION ALL SELECT 'A', 'ProdA3', DATE('2020-02-13'), DATE('2020-12-31') FROM SYSIBM.SYSDUMMY1
  UNION ALL SELECT 'B', 'ProdB1', DATE('2020-01-05'), DATE('2020-01-09') FROM SYSIBM.SYSDUMMY1
  UNION ALL SELECT 'B', 'ProdB2', DATE('2020-01-12'), DATE('2020-03-14') FROM SYSIBM.SYSDUMMY1
  UNION ALL SELECT 'B', 'ProdB3', DATE('2020-03-15'), DATE('2020-04-18') FROM SYSIBM.SYSDUMMY1
  UNION ALL SELECT 'B', 'ProdB4', DATE('2020-04-16'), DATE('2020-05-03') FROM SYSIBM.SYSDUMMY1
)
,
*/
MYTAB_ENUM AS 
(
  SELECT 
    T.*
  , ROWNUMBER() OVER (PARTITION BY PRODUCT ORDER BY START_DATE) RN
  FROM MYTAB T
)
SELECT A.PRODUCT, A.END_DATE + 1 START_DT, B.START_DATE - 1 END_DT
FROM MYTAB_ENUM A
JOIN MYTAB_ENUM B ON B.PRODUCT = A.PRODUCT AND B.RN = A.RN + 1
WHERE A.END_DATE + 1 &lt;&gt; B.START_DATE
AND A.END_DATE &lt; B.START_DATE;  
</code></pre>
<p>The result is:</p>
<pre><code>|PRODUCT|START_DT  |END_DT    |
|-------|----------|----------|
|B      |2020-01-10|2020-01-11|
</code></pre>
<p>May be more efficient way:</p>
<pre><code>WITH MYTAB2 AS
(
SELECT 
  T.*
, LAG(END_DATE) OVER (PARTITION BY PRODUCT ORDER BY START_DATE) END_DATE_PREV
FROM MYTAB T
)
SELECT PRODUCT, END_DATE_PREV + 1 START_DATE, START_DATE - 1 END_DATE
FROM MYTAB2
WHERE END_DATE_PREV + 1 &lt;&gt; START_DATE
AND END_DATE_PREV &lt; START_DATE;
</code></pre>
"
"66013805","Checking Slowly Changing Dimension 2","<p>I have a table that looks like this:</p>
<p><img src=""https://i.stack.imgur.com/jjHOm.jpg"" alt=""enter image description here"" /></p>
<p>A slowly changing dimension type 2, according to Kimball.</p>
<p>Key is just a surrogate key, a key to make rows unique.</p>
<p>As you can see there are three rows for product A.
Timelines for this product are ok. During time the description of the product changes.
From 1-1-2020 up until 4-1-2020 the description of this product was ProdA1.
From 5-1-2020 up until 12-2-2020 the description of this product was ProdA2 etc.</p>
<p>If you look at product B, you see there are gaps in the timeline.</p>
<p>We use DB2 V12 z/Os. How can I check if there are gaps in the timelines for each and every product?</p>
<p>Tried this, but doesn't work</p>
<pre><code>with selectie (key, tel) as
  (select product, count(*)
    from PROD_TAB
   group by product
    having count(*) &gt; 1)
Select * from
PROD_TAB A
inner join selectie B
on A.product = B.product
Where not exists
(SELECT 1 from PROD_TAB C
WHERE A.product = C.product
AND A.END_DATE + 1 DAY = C.START_DATE
)
</code></pre>
<p>Does anyone know the answer?</p>
","<db2><data-warehouse>","2021-02-02 16:33:35","55","0","2","66043081","<p>Thnx Mark, will try this one of these days.
Never heard of LAG in DB2 V12 for z/Os
Will read about it</p>
<p>Thnx</p>
"
"66009165","Link fact tables at different granularity levels of a dimension","<p>New to data warehouse design. I have a denormalised dimension table representing geographies (e.g. suburb, city, state). This is a slowly changing dimension.</p>
<p>Also have multiple fact tables, each at different grain levels.</p>
<p>Is it possible to model this so the fact tables use surrogate keys, whilst maintaining a denormalised dimension table?</p>
<p><a href=""https://i.stack.imgur.com/EeUrI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EeUrI.png"" alt=""data model"" /></a></p>
","<data-modeling><data-warehouse>","2021-02-02 11:50:17","1451","2","1","66012511","<p>If you have effectively the same dimensional data but at different grains then you handle this by creating &quot;aggregate&quot; dimensions. In your example, copy the dim_geo table definition (not the data), name the dim to something like dim_geo_city and drop all the columns at a lower granularity than city (e.g. suburb_id, suburb). If you have facts at the state level then you would create dim_geo_state in the same way - and so on for any further levels of aggregation.</p>
<p>Fact_population will continue to reference dim_geo but fact_housing should reference dim_geo_city.</p>
<p>The easiest way to populate aggregate dims is to run a SELECT DISTINCT on the base dim (dim_geo) and only include the columns that exist in the target dim (dim_geo_city) - you then take the resulting data and apply the appropriate SCD logic to insert/update it into the target dim.</p>
<p><a href=""https://i.stack.imgur.com/GuUlj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GuUlj.png"" alt=""enter image description here"" /></a></p>
"
"66000517","Populating Fact Tables(Data Warehouse) and Querying","<p>I am not sure how to query my fact tables(covid and vaccinations), I populated the dimensions with dummy data, I am supposed to leave the fact tables empty? As far as I know, they would get populated when I write the queries.</p>
<p>I am not sure how to query the tables I have tried different things, but I get an empty result.
Below is a link to the schema.
I want to find out the &quot;TotalDeathsUK&quot;(fact table COVID) for the last year caused by each &quot;Strain&quot;(my strain table has 3 strain in total.</p>
<p><a href=""https://i.stack.imgur.com/7GuIw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7GuIw.png"" alt=""Snowflake Schema Link"" /></a></p>
","<psql><data-warehouse><querying><fact-table>","2021-02-01 21:38:48","322","0","1","66065781","<p>You can use <a href=""https://www.postgresql.org/message-id/attachment/23520/sql-merge.html"" rel=""nofollow noreferrer"">MERGE</a> to poulate your fact table <code>COVIDFact</code> :</p>
<pre><code>MERGE 
INTO         factcovid 
using        ( 
                    SELECT centerid, 
                           dateid, 
                           patientid, 
                           strainid 
                    FROM   yourstagingfacttable ) AS f 
ON factcovid.centerid = f.centerid AND factcovid.dateid=f.dateid... //the join columns
WHEN matched THEN 
do nothing WHEN NOT matched THEN 
INSERT VALUES 
       ( 
              f.centerid, 
              f.dateid, 
              f.patientid, 
              f.strainid 
       )
</code></pre>
<p>And for <code>VaccinationsFact</code> :</p>
<pre><code>MERGE 
INTO         vaccinations 
using        ( 
                    SELECT centerid, 
                           dateid, 
                           patientid, 
                           vaccineid 
                    FROM   yourstagingfacttable ) AS f 
ON factcovid.centerid = f.centerid //join condition(s) 
WHEN matched THEN 
do nothing WHEN NOT matched THEN 
INSERT VALUES 
       ( 
              f.centerid, 
              f.dateid, 
              f.patientid, 
              f.vaccineid 
       )
</code></pre>
<p>For the <code>TotalDeathUK</code> measure :</p>
<pre><code>SELECT S.[Name] AS Strain, COUNT(CF.PatientID) AS [Count of Deaths] FROM CovidFact AS CF
    LEFT JOIN Strain AS S ON S.StrainID=CF.StrainID
    LEFT JOIN Time AS T ON CF.DateID=T.DateID
    LEFT JOIN TreatmentCenter AS TR ON TR.CenterID=CF.CenterID
    LEFT JOIN City AS C ON C.CityID = TR.CityID
    WHERE C.Country LIKE 'UK' AND T.Year=2020
    AND Result LIKE 'Death' // you should add a Result column to check if the Patient survived or died
    GROUP BY S.[Name]
</code></pre>
"
"65932737","After copying all our SQL, NoSQL data into Snowflake, is there a way to detect ""relationships"" across the hundreds of Tables, Jsons, other data?","<p>Apologies for this somewhat bizarre question...</p>
<p>Data from multiple transactional, operational, and event sources have successfully been ingested into Snowflake. However, most of our analytics and data science use cases involve:</p>
<ul>
<li>denormalizing data</li>
<li>building models across multiple structured and semi-structured sources i.e. understand how data &quot;joins&quot; within and across sources, esp where there is no consistent naming convention naming convention across sources</li>
</ul>
<p>Is there a way in Snowflake (either directly or via other tools) to automatically detect relationships across the data without requiring us to write multiple joins? Do any of the other cloud data warehouses offer this (directly or via 3rd party add-ons)?</p>
","<sql><snowflake-cloud-data-platform><data-modeling><data-warehouse>","2021-01-28 07:32:26","109","4","1","66371728","<p>This is a great question.</p>
<p>Snowflake does maintain metadata about tables, columns, etc - but nothing that infers a relationship.  That said, you could use the metadata to see how many overlaps exist between two objects.  For example:</p>
<pre><code>-- Find all tables that share n column names
select c1.table_catalog || '.' || c1.table_schema || '.' || c1.table_name object1, 
       c2.table_catalog || '.' || c2.table_schema || '.' || c2.table_name object2,
       count(1) overlapping_column_names
from snowflake.account_usage.columns c1,
     snowflake.account_usage.columns c2
where upper(c1.column_name) = upper(c2.column_name)
group by 1, 2
order by 3 desc;
</code></pre>
<p>For semi-structured, it's a bit more complex but there are functions in Snowflake to extract the different keys in the variant data.  The SQL below will identify the unique keys observed in a 10% sample of the variant data:</p>
<pre><code>with c1 as (select distinct object_keys(json_data) keys 
             from customer_interactions sample(10)
           )
select distinct 'TABLE1' table_name, upper(value::string) keyname
from c1, lateral flatten(c1.keys)
;
</code></pre>
<p>You could compare this to other variant data sets to see how many keys overlap between the two.</p>
"
"65923518","String delimiter present in string not permitted in Polybase?","<p>I'm creating an external table using a CSV stored in an Azure Data Lake Storage and populating the table using Polybase in SQL Server.</p>
<p>However, I ran into this problem and figured it may be due to the fact that in one particular column there are double quotes present within the string, and the string delimiter has been specified as <code>&quot;</code> in Polybase <code>(STRING_DELIMITER = '&quot;')</code>.</p>
<pre><code>HdfsBridge::recordReaderFillBuffer - Unexpected error encountered filling record reader buffer: HadoopExecutionException: Could not find a delimiter after string delimiter
</code></pre>
<p>Example:</p>
<p><a href=""https://i.stack.imgur.com/kThlF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kThlF.png"" alt=""enter image description here"" /></a></p>
<p>I have done quite an extensive research in this and found that this issue has been around for years but yet to see any solutions given.</p>
<p>Any help will be appreciated.</p>
","<sql-server><data-warehouse><polybase>","2021-01-27 16:41:57","838","2","2","65945588","<p>I think the easiest way to fix this up because you are in charge of the .csv creation is to use a delimiter which is not a comma and leave off the string delimiter.  Use a separator which you know will not appear in the file.  I've used a pipe in my example, and I clean up the string once it is imported in to the database.</p>
<p>A simple example:</p>
<pre><code>IF EXISTS ( SELECT * FROM sys.external_tables WHERE name = 'delimiterWorking' )
DROP EXTERNAL TABLE delimiterWorking
GO

IF EXISTS ( SELECT * FROM sys.tables WHERE name = 'cleanedData' )
DROP TABLE cleanedData
GO



IF EXISTS ( SELECT * FROM sys.external_file_formats WHERE name = 'ff_delimiterWorking' )
DROP EXTERNAL FILE FORMAT ff_delimiterWorking
GO

CREATE EXTERNAL FILE FORMAT ff_delimiterWorking
WITH (
    FORMAT_TYPE = DELIMITEDTEXT,
    FORMAT_OPTIONS (
        FIELD_TERMINATOR = '|',
        --STRING_DELIMITER = '&quot;',
        FIRST_ROW = 2,
        ENCODING = 'UTF8'
        )
);
GO


CREATE EXTERNAL TABLE delimiterWorking (
    id                  INT NOT NULL,
    body                VARCHAR(8000) NULL
)
WITH (
    LOCATION = 'yourLake/someFolder/delimiterTest6.txt',
    DATA_SOURCE = ds_azureDataLakeStore,
    FILE_FORMAT = ff_delimiterWorking,
    REJECT_TYPE = VALUE,
    REJECT_VALUE = 0
);
GO


SELECT *
FROM delimiterWorking
GO



-- Fix up the data
CREATE TABLE cleanedData
WITH (
    CLUSTERED COLUMNSTORE INDEX,
    DISTRIBUTION = ROUND_ROBIN
    )
AS
SELECT
    id,
    body AS originalCol,
    SUBSTRING ( body, 2, LEN(body) - 2 ) cleanBody
FROM delimiterWorking
GO



SELECT *
FROM cleanedData
</code></pre>
<p>My results:</p>
<p><a href=""https://i.stack.imgur.com/ryiyw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ryiyw.png"" alt=""My results"" /></a></p>
"
"65923518","String delimiter present in string not permitted in Polybase?","<p>I'm creating an external table using a CSV stored in an Azure Data Lake Storage and populating the table using Polybase in SQL Server.</p>
<p>However, I ran into this problem and figured it may be due to the fact that in one particular column there are double quotes present within the string, and the string delimiter has been specified as <code>&quot;</code> in Polybase <code>(STRING_DELIMITER = '&quot;')</code>.</p>
<pre><code>HdfsBridge::recordReaderFillBuffer - Unexpected error encountered filling record reader buffer: HadoopExecutionException: Could not find a delimiter after string delimiter
</code></pre>
<p>Example:</p>
<p><a href=""https://i.stack.imgur.com/kThlF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kThlF.png"" alt=""enter image description here"" /></a></p>
<p>I have done quite an extensive research in this and found that this issue has been around for years but yet to see any solutions given.</p>
<p>Any help will be appreciated.</p>
","<sql-server><data-warehouse><polybase>","2021-01-27 16:41:57","838","2","2","65997746","<p>String Delimiter issue can be avoided if you have the Data lake flat file converted to Parquet format.</p>
<p>Input:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>&quot;ID&quot;</th>
<th>&quot;NAME&quot;</th>
<th>&quot;COMMENTS&quot;</th>
</tr>
</thead>
<tbody>
<tr>
<td>&quot;1&quot;</td>
<td>&quot;DAVE&quot;</td>
<td>&quot;Hi &quot;I am Dave&quot; from&quot;</td>
</tr>
<tr>
<td>&quot;2&quot;</td>
<td>&quot;AARO&quot;</td>
<td>&quot;AARO&quot;</td>
</tr>
</tbody>
</table>
</div><h3>Steps:</h3>
<p>1 Convert Flat file to Parquet format <em>[Using Azure Data factory]</em></p>
<p>2 Create External File format in Data Lake <em>[Assuming Master key, Scope credentials available]</em></p>
<pre><code>CREATE EXTERNAL FILE FORMAT PARQUET_CONV
WITH (FORMAT_TYPE = PARQUET, 
DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'
);
</code></pre>
<p>3 Create External Table with FILE_FORMAT = PARQUET_CONV</p>
<blockquote>
<p>Output:</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/Voj4V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Voj4V.png"" alt=""enter image description here"" /></a></p>
<p>I believe this is the best option as Microsoft don't have an solution currently to handle this string delimiter occurring with in the data for External table</p>
"
"65914509","Where should we install SSIS feature? on Data warehouse server or source DB Server","<p>We need to install SSIS on one server to use it for ETL.</p>
<p>We have two SQL Servers, one will be source data and the second one will be destination data.</p>
<p>Where should we install SSIS? on source DB server or destination DB server?</p>
","<sql-server><ssis><data-warehouse>","2021-01-27 07:19:45","80","-1","1","65915481","<p>I think the question is based on your current IT infrastructure.</p>
<p>Better practice are three environments: Developer, User Acceptance(UAT) and Production. Production environment is usually on a separate server.</p>
<p>For SSIS, each of these environment has its own data-sources and destination. Developers usually develops SSIS packages on Dev environment and after necessary testing, those are <a href=""https://learn.microsoft.com/en-us/sql/integration-services/packages/deploy-integration-services-ssis-projects-and-packages?view=sql-server-ver15"" rel=""nofollow noreferrer"">DEPLOYED</a> to UAT/ Prod SQL server catalog DB's. You can also deploy as file.</p>
<p>These packages can be scheduled to run on specific times, and usually it will be on out-of-office hours (night time) or off-peak hours. This will help with performance.</p>
<p>Still it depends on your server resources and data complexities.</p>
"
"65900731","Does anybody use Karate DSL as a test data management tool?","<p>i am glad with the use of Karate DSL for API testing. But i was wondering if it is suited for test data generation. Some of our UI tests need some particular data load which I think could be generated by calling the API (Rest and SOAP) through Karate DSL.</p>
<p>Is Karate adequate for this or would you use other specific data generation tool?</p>
<p>Thanks in advance for your help.</p>
","<api><testing><karate>","2021-01-26 11:43:38","126","1","1","65911821","<p>I suggest you start looking at the new RC version (0.9.9.RC3) you can use a function to <a href=""https://twitter.com/KarateDSL/status/1343201222396821504"" rel=""nofollow noreferrer"">generate data</a>: <a href=""https://github.com/intuit/karate/tree/develop#json-function-data-source"" rel=""nofollow noreferrer"">https://github.com/intuit/karate/tree/develop#json-function-data-source</a></p>
<pre><code>* def generator = function(i){ if (i == 20) return null; return { name: 'cat' + i, age: i } }
</code></pre>
"
"65900415","How do I add a directory of .wav files to the Kedro data catalogue?","<p>This is my first time trying to use the <a href=""https://kedro.readthedocs.io/en/stable/index.html"" rel=""nofollow noreferrer"">Kedro</a> package.</p>
<p>I have a list of .wav files in an s3 bucket, and I'm keen to know how I can have them available within the Kedro data catalog.</p>
<p>Any thoughts?</p>
","<amazon-s3><kedro>","2021-01-26 11:22:32","393","2","2","65900738","<p>I don't believe there's currently a dataset format that handles <code>.wav</code> files. You'll need to build a <a href=""https://kedro.readthedocs.io/en/stable/07_extend_kedro/03_custom_datasets.html"" rel=""nofollow noreferrer"">custom dataset</a> that uses something like <a href=""https://docs.python.org/3/library/wave.html"" rel=""nofollow noreferrer"">Wave</a> - not as much work as it sounds!</p>
<p>This will enable you to do something like this in your catalog:</p>
<pre class=""lang-yaml prettyprint-override""><code>dataset:
  type: my_custom_path.WaveDataSet
  filepath: path/to/individual/wav_file.wav # this can be a s3://url
</code></pre>
<p>and you can then access your WAV data natively within your Kedro pipeline. You can do this for each <code>.wav</code> file you have.</p>
<p>If you wanted to be able to access a whole folders worth of wav files, you might want to explore the notion of a &quot;wrapper&quot; dataset like the <a href=""https://kedro.readthedocs.io/en/stable/kedro.io.PartitionedDataSet.html"" rel=""nofollow noreferrer"">PartitionedDataSet</a> whose <a href=""https://kedro.readthedocs.io/en/stable/05_data/02_kedro_io.html#partitioned-dataset-definition"" rel=""nofollow noreferrer"">usage guide</a> can be found in the documentation.</p>
"
"65900415","How do I add a directory of .wav files to the Kedro data catalogue?","<p>This is my first time trying to use the <a href=""https://kedro.readthedocs.io/en/stable/index.html"" rel=""nofollow noreferrer"">Kedro</a> package.</p>
<p>I have a list of .wav files in an s3 bucket, and I'm keen to know how I can have them available within the Kedro data catalog.</p>
<p>Any thoughts?</p>
","<amazon-s3><kedro>","2021-01-26 11:22:32","393","2","2","65901842","<p>This worked:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

from pathlib import Path, PurePosixPath
from kedro.io import AbstractDataSet


class WavFile(AbstractDataSet):
    '''Used to load a .wav file'''
    
    def __init__(self, filepath):
        self._filepath = PurePosixPath(filepath)

    def _load(self) -&gt; pd.DataFrame:
        df = pd.DataFrame({'file': [self._filepath],
                           'data': [load_wav(self._filepath)]})     
        return df
    

    def _save(self, df: pd.DataFrame) -&gt; None:
        df.to_csv(str(self._filepath))

    def _exists(self) -&gt; bool:
        return Path(self._filepath.as_posix()).exists()

    def _describe(self):
        return dict(filepath=self._filepath)
    
    
class WavFiles(PartitionedDataSet):
    '''Replaces the PartitionedDataSet.load() method to return a DataFrame.'''

    def load(self)-&gt;pd.DataFrame:
        '''Returns dataframe'''
        dict_of_data = super().load()
        
        df = pd.concat(
            [delayed() for delayed in dict_of_data.values()]
        )
        
        return df
    
    
my_partitioned_dataset = WavFiles(
    path=&quot;path/to/folder/of/wav/files/&quot;,
    dataset=WavFile,
)
     
my_partitioned_dataset.load()

</code></pre>
"
"65891245","How to implement dimension table design when having different values for the same dimension according to specific criteria?","<p>If I have a <code>Dimension Date table</code> like this:</p>
<pre><code>CREATE TABLE [Dimension].[Date](
    [Date Key] [int] IDENTITY(1,1) NOT NULL,
    [Date] [date] NOT NULL,
    [Day] [tinyint] NOT NULL,
    [Day Suffix] [char](2) NOT NULL,
    [Weekday] [tinyint] NOT NULL,
    [Weekday English Name] [varchar](10) NOT NULL,
    [Weekday English Name Short] [char](3) NOT NULL,
    [Weekday English Name FirstLetter] [char](1) NOT NULL,
    [Weekday Arabic Name] [nvarchar](15) NOT NULL,
    [Weekday Arabic Name FirstLetter] [nchar](1) NOT NULL,
    [Day Of Year] [smallint] NOT NULL,
    [Week Of Month] [tinyint] NOT NULL,
    [Week Of Year] [tinyint] NOT NULL,
    [Month] [tinyint] NOT NULL,
    [Month English Name] [varchar](10) NOT NULL,
    [Month English Name Short] [char](3) NOT NULL,
    [Month English Name FirstLetter] [char](1) NOT NULL,
    [Month Arabic Name] [nvarchar](15) NOT NULL,
    [Month Arabic Name FirstLetter] [nchar](1) NOT NULL,
    [Quarter] [tinyint] NOT NULL,
    [Quarter Name] [varchar](6) NOT NULL,
    [Year] [int] NOT NULL,
    [MMYYYY] [char](6) NOT NULL,
    [Month Year] [char](7) NOT NULL,
    [Is Weekend] [bit] NOT NULL,
    [Is Holiday] [bit] NOT NULL,
    [Holiday Name] [nvarchar](50) NOT NULL,
    [Special Day] [nvarchar](50) NOT NULL,
    [First Date Of Year] [date] NULL,
    [Last Date Of Year] [date] NULL,
    [First Date Of Quater] [date] NULL,
    [Last Date Of Quater] [date] NULL,
    [First Date Of Month] [date] NULL,
    [Last Date Of Month] [date] NULL,
    [First Date Of Week] [date] NULL,
    [Last Date Of Week] [date] NULL,
    [Lineage Key] [int] NULL,
 CONSTRAINT [PK__Date__B7A341C5SWWC2006D] PRIMARY KEY CLUSTERED 
(
    [Date Key] ASC
)WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON, OPTIMIZE_FOR_SEQUENTIAL_KEY = OFF) ON [PRIMARY]
) ON [PRIMARY]
GO
</code></pre>
<hr />
<p>I face the following problem:</p>
<p>Not all <code>weekends and holidays</code> are the same for all the organizational departments, There are default weekends(<code>Fri,Sat</code>) for example but some departments have different schedules so they have different weekend days. How to handle this situation when I have different values for the same dimension according to specific criteria? Should I create multiple versions for the same Dim? How to use Date Dim as a <code>conformed dimension</code> in an enterprise data warehouse in this case?</p>
","<sql><sql-server><etl><data-warehouse><business-intelligence>","2021-01-25 19:31:55","140","1","1","65900741","<p>Having the Date dimension as a conformed table is adavntageous when  :</p>
<ul>
<li>One central location to update by ETL jobs</li>
<li>Easy to implement “Single Source of Truth” because the same copy of
data is visible across organization.</li>
<li>Less data footprint due to removal of unnecessary copies of the same
data.</li>
</ul>
<p>In your case, you don't need all the columns so you can have the DimDate table in your database but pointing to it with a simple view to get only columns you need.
Also, you can have a bridge table between the DimDate view and the DimDepartment like below :</p>
<p><a href=""https://i.stack.imgur.com/4BwRQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4BwRQ.png"" alt=""enter image description here"" /></a></p>
<p>A logical data model may contain one or more many-to-many relationships. Physical data modelling techniques transform a many-to-many many-relationships into one-to many-relationships by adding additional tables. These are referred to as bridge tables.</p>
"
"65883718","Data warehouse modelling","<p>Let’s say I have 6 databases, multiple industries, mostly similar schema.</p>
<p>Currently we have dozens of excel files that connect to each DB and query live data (work orders, invoices etc).</p>
<p>As I understand. Creating a separate DW database would be of benefit performance wise, but also when remodelled, would eliminate complex joins currently required by our QA people.</p>
<p>Would it stand to reason, that I would have a denormalized table, called ‘WorkOrder’ which was also a merger of all work orders across 5-6 systems? How would I handle the primary keys of each workorder, when they overlap? I assume a distinct column for each with a unique prefix to designate the origin database?</p>
<p>Should the workorder table contain only common fields, or would all fields make more sense, nulling out those fields where data didn’t exist in the original?</p>
<p>This denormalized table would be easier to query from the QA standpoint, no doubt. But seems to contradict what I’ve read about DW star or snowflake modelling with facts etc?!?</p>
<p>It’s very likely I just don’t get the fundamentals of data warehousing either :)</p>
","<etl><data-warehouse><business-intelligence>","2021-01-25 11:17:52","101","1","1","65884929","<p>Having decided that you need a Data Warehouse, the first decision you need to take is what type of design/database you are going to use. There are quite a few options (Kimball, Inmon, Data Vault, NoSQL, Graph, etc.) but the vast majority of data warehouses follow the basic Kimball Methodology of dimensional modelling e.g. Facts and Dimensions.</p>
<p>If you are going to build a Kimball-style data warehouse (or follow any other methodology) then my first recommendation would be to employ someone with experience who can lead the work. It is very easy to make mistakes when designing a DW but very hard to correct them once people are using it, have built reports against it, etc.</p>
<p>If you're not going to employ someone who knows what they are doing then the next best option is to go on a course and/or read books on the subject. For Kimball, there are really 2 books that should be required reading:</p>
<ol>
<li><a href=""https://www.amazon.co.uk/gp/product/0471255475/ref=dbs_a_def_rwt_bibl_vppi_i4"" rel=""nofollow noreferrer"">The Data Warehouse Lifecycle Toolkit</a> : this talks you through all the components involved and the steps to follow in order to deliver a robust data warehouse</li>
<li><a href=""https://www.amazon.co.uk/gp/product/B00DRZX6XS/ref=dbs_a_def_rwt_hsch_vapi_tkin_p1_i0"" rel=""nofollow noreferrer"">The Data Warehouse Toolkit</a> : this goes through the steps to design a dimensional model</li>
</ol>
<p>Once you have read and understood these 2 books you will be better placed to understand the terminology and ask specific, focussed questions about any parts of the methodology (or your specific circumstances) that you don't understand.</p>
<p>This is absolutely not meant to be a criticism but from your questions it is very clear that you don't (yet) have the knowledge or experience to be designing and building a data warehouse - and you're not going to be able gain that experience by asking questions on this (or any other) forum.</p>
"
"65875735","When do we consider snowflaking a star schema?","<p>In the below dimensional model, all the dimension table follows the star schema except the prod table, which is snowflaked and has prod_sub table as a child table. What is the strategical point to consider snowflaking a star schema?</p>
<pre><code>Sales Table - Fact Table
Prod_id  (fk)
Promo_id (fk)
store_id (fk)

Promo Table - Dimension Table
promo_id (pk)

Store Table - Dimension Table
store_id (pk)

Product Table - Dimension Table
Prod_id (pk)
prod_class_id (FK)
Prod_name
Prod_brand

Prod_sub Table- Dimension Table (snow flaked)
prod_class_id,
prod_class
prod_sub_class
prod_family
</code></pre>
","<etl><data-modeling><data-warehouse><star-schema><snowflake-schema>","2021-01-24 20:42:01","99","2","1","65883036","<p>According to <a href=""https://www.kimballgroup.com/1997/08/a-dimensional-modeling-manifesto/"" rel=""nofollow noreferrer"">Kimball</a> : Star Schema is the best way of designing a data model for reporting, You will get the best performance and also flexibility using such a model.
So no need to snowflake it.</p>
<p>Then your Product Table Dimension Table becomes :</p>
<pre><code>Product Table - Dimension Table
Prod_id (pk)
Prod_name
Prod_brand
prod_class
prod_sub_class
prod_family
</code></pre>
"
"65853724","Dashboards with multiple warehouses","<p>I'm looking for feedback on possible approaches. I'm trying to put together a least a dozen different dashboards that look at a wide variety of topics with overlapping dimensions. The issue I'm having is the data that is requested is stored across at least 4 very large data warehouse all with different levels of normalization and data standardization. Almost all the data in each of these warehouses pretty much use the same data from the same operational data store and are just stored, standardized or aggregated in different ways.</p>
<p>The question I have is would it make sense to stand up a new data warehouse that combines the necessary data from these warehouses and stores it specifically for use in Dozen dashboards I need to create? Or would a better approach be to rely on the dashboard applications blending tool and have each dashboard connect to each of the already established data warehouses?</p>
<p>Thanks for the feedback!</p>
","<sql-server><data-warehouse><dashboard><business-intelligence>","2021-01-22 22:42:25","65","0","1","65855423","<p>Unless your new datawarehouse is going to add a new dimension over the existing data that the other stores are already aggregating, or your visualisation layer cannot easily access the other stores, then a new data warehouse is going to add another layer of management and complexity to your solution that might be simpler to achieve in your dashboard directly.</p>
<p>Approach this pragmatically, do what works with the least amount of effort. If the data is available and you can access multiple stores from your dashboards, then do so, that is often the power that we expect from dashboards and BI visualisation layers, we don't have to aggregate the data into a single store first.</p>
<blockquote>
<p>Take PowerBI as an example or proof of the concept, the visualization layer in that product is heavily focused on creating many data connections to disparate data sources and even allows you to perform analytics from data across these connections.</p>
</blockquote>
<p>If you can make it work, down the track you might be able to identify optimisations that could be realised by adding another layer in the form of a new data warehouse, but that decision would largely depend on the platform or framework that you choose to use for your visualisations and how many people need to collaborate on the design of these dashboards.</p>
"
"65839265","Firestore data modelling for travel planner","<p>I'm building an apps that involved travel planning using flutter. this app will help people plan their travel by providing few options for them to choose from either cheapest, fastest, shortest etc.</p>
<p>I'm quite new with firebase and i need some advice with the data structure, I was thinking of having public transportation such as <code>train</code>. this train will have it's own schedule. What is the best way to structure this schedule inside the firestore. so that i could create a view that will display train schedule.</p>
","<firebase><google-cloud-firestore>","2021-01-22 04:23:13","119","0","1","65843502","<p>If it were me, I would want to map out how I intend to interact with the application. I usually draw this out by hand as I find it much quicker than trying to model something electronically. I review other similar apps at the same time and try and identify data that are missing. Start simple and build up the various data points that you want to include.</p>
<p>The main thing to note is that Cloud Firestore is a NoSQL, document-oriented database; there are no tables or rows. You store data in documents, which are organised into collections that contain a set of key-value pairs.</p>
<p>Once you have a basic structure, you can opt for one of the <a href=""https://firebase.google.com/docs/firestore/data-model"" rel=""nofollow noreferrer"">Firestore data structures</a> and test it out:</p>
<ul>
<li>Documents</li>
<li>Multiple collections</li>
<li>Subcollections within documents</li>
</ul>
<p><strong>Nested data</strong></p>
<p>A typical use case for this might involve a  chat app, and you want to store a user's three most recently visited chat rooms as a nested list in their profile.</p>
<p><strong>Subcollections</strong></p>
<p>You can create collections within documents when you have data that might expand over time; for example, you might create a collection of users or messages within chat room documents.</p>
<p><strong>Root-level collections</strong></p>
<p>You can create collections at the root level to organise distinct data sets for users and another for rooms and messages.</p>
"
"65795618","Azure Data Factory - different copy data mappings views in the same pipeline?","<p>I am trying to set up a pipeline with copy data activity in Azure Data Factory and I am confused by the different view of mapping in the copy activity. I have created the pipeline from the template &quot;Copy data from on premise SQL Server to SQL Azure&quot; and I am cloning the activity so there shouldn't be any differences. The source is the same in both activities and I use query against the source database.</p>
<p>Here's how I see it:</p>
<p>Original copy activity:</p>
<p><a href=""https://i.stack.imgur.com/q6rJd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q6rJd.png"" alt=""enter image description here"" /></a></p>
<p>Cloned copy activity:</p>
<p><a href=""https://i.stack.imgur.com/801Jk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/801Jk.png"" alt=""enter image description here"" /></a></p>
<p>I would like to understand why I see different views of mapping.</p>
<p>Thanks in advance!</p>
","<azure><azure-data-factory>","2021-01-19 16:31:04","124","0","1","65822226","<p>I don't that's an problem. When we clone a copy active, before we debug or run the pipeline, we need to check all the settings manually.</p>
<p>From your screenshots, original copy activity miss the source schema during mapping. Just import schema will be solved it.
<a href=""https://i.stack.imgur.com/5H0Sj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5H0Sj.png"" alt=""enter image description here"" /></a></p>
<p>And the cloned copy active seams import schema automatically. Not sure if all the columns are mapped(I think not). Some suggestions:</p>
<ol>
<li>Please import the schema in source dataset firstly, and fully set
one copy active.</li>
<li>Then clone the copy active which may avoid the problem.</li>
</ol>
<p>Data Factory may not very smartly and even we clone active, we still should check all the settings in each actives.</p>
<p><strong>Update:</strong></p>
<p>Like @JeffRamos said, sinks are different then the mapping will be different.</p>
<p>We are glad to hear that you have figured it out:</p>
<ul>
<li>&quot;I have figured it out - I was using the query that contained the
&quot;count(*)&quot; aggregate. Removing it and the &quot;group by&quot; clause made the
mapping view the same as for the original Copy activity.&quot;</li>
</ul>
<p>Thanks for @JeffRamos's useful comment again.</p>
<p>HTH.</p>
"
"65784974","Dimension Tables: is it worth creating a dimension with only 2 values?","<p>Is it worth creating a dimension that has only 2 values?</p>
<p>The values are literally &quot;Yes&quot; and &quot;No&quot;.</p>
<p>Or should I combine it with other values and turn it into a junk dimension?</p>
<p><strong>Edit:</strong></p>
<p>Not sure why this was closed as it's not opinion based, it's based on the number of records, etc.</p>
<p>Query performance does not seem to be much impacted if you are joining with a table that has just a couple rows, but rather than having a bunch of joins with tables that will likely contain no more than 2-3 values, it's better to just create a junk dimension and do 1 join.</p>
<p>Seems the best solution for my problem was to just turn it into a junk dimension.</p>
","<data-warehouse><business-intelligence>","2021-01-19 03:06:51","750","-1","2","65787531","<p>You can use both Static Dimension or Junk dimension :</p>
<p><strong>Static Dimensions</strong></p>
<p>Static dimensions are not extracted from the original data source, but are created within the context of the data warehouse. A static dimension can be loaded manually — for example with status codes — or it can be generated by a procedure, such as a date or time dimension. Also, status codes dimension is mostly a static dimension.</p>
<p><strong>Junk Dimensions</strong></p>
<p>There are some attributes that have really low cardinality (distinct possible values). Neither this attributes specifically belong to a dimension table nor they are important enough to create an individual dimension for them. A junk dimension creates combinations of all the distinct values of such attributes and stores it in a single junk dimension table. The surrogate keys for these rows are inserted in the fact table. So now the fact table instead of having multiple foreign keys for each of these attributes, has a single foreign key for each row in the junk dimension table</p>
<p>Example-: Status( pass, fail) Income category( lower, middle, high)</p>
<p>Note: Be careful, another dimension = more joins = complex queries</p>
"
"65784974","Dimension Tables: is it worth creating a dimension with only 2 values?","<p>Is it worth creating a dimension that has only 2 values?</p>
<p>The values are literally &quot;Yes&quot; and &quot;No&quot;.</p>
<p>Or should I combine it with other values and turn it into a junk dimension?</p>
<p><strong>Edit:</strong></p>
<p>Not sure why this was closed as it's not opinion based, it's based on the number of records, etc.</p>
<p>Query performance does not seem to be much impacted if you are joining with a table that has just a couple rows, but rather than having a bunch of joins with tables that will likely contain no more than 2-3 values, it's better to just create a junk dimension and do 1 join.</p>
<p>Seems the best solution for my problem was to just turn it into a junk dimension.</p>
","<data-warehouse><business-intelligence>","2021-01-19 03:06:51","750","-1","2","65831268","<p>Short answer, Yes.  It is worth creating a dimension containing all the possible values for an attribute.</p>
<p>The purpose of a junk dimension is to combine those very tiny dimensions into a larger table to eliminate unnecessary joins.  Even if this single Yes/No value is the only attribute with such a small set, it is still worth creating a junk dimension because...</p>
<p>Warehouses grow.  You will eventually find other attributes that are great candidates for including in the junk dimension (sex/gender, marital status, deceased, etc). Start your warehouse on a good path by doing it right the first time.</p>
<p>It might seem like overkill, but the system doesn't care much about joining to a two row table. It is much easier to add new attributes to an existing junk dimension versus rewriting your architecture when needed later.  Also, once you publish the design for use, your users will have queries leveraging those attributes.  That makes change extremely difficult, if not impossible.</p>
"
"65765002","Data Modeling with a Time Dimension","<p>Would it be better to create a time dimension with hh:mm:ss altogether or would it be better to split them up into 3 dimensions since separately they will take up less space?</p>
<p>What would you recommend and why?</p>
","<data-modeling><data-warehouse><business-intelligence>","2021-01-17 19:22:26","673","3","3","65765176","<p>Why not a Date Dimension ?
The Date Dimension is a key dimension in data warehousing as it allows to analyze data in different aspects of date. Apart from the standard date attributes like year, quarter, month, day , hour, min ..., the date dimension can be extended to richer analysis</p>
<p>Working with star schema model is a best practice. Date dimension is called <a href=""https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/conformed-dimension/"" rel=""nofollow noreferrer"">a conformed dimension</a>.</p>
<p>Spliting up in 3 dimension = more joins = complex queries.</p>
"
"65765002","Data Modeling with a Time Dimension","<p>Would it be better to create a time dimension with hh:mm:ss altogether or would it be better to split them up into 3 dimensions since separately they will take up less space?</p>
<p>What would you recommend and why?</p>
","<data-modeling><data-warehouse><business-intelligence>","2021-01-17 19:22:26","673","3","3","65816640","<p>Why not create a time dimension with the following (and maybe more) columns on the dimension table?</p>
<ul>
<li>HH</li>
<li>MM</li>
<li>SS</li>
<li>AM PM Indicator</li>
<li>HH:MM:SS</li>
<li>Nickname (e.g. 'noon')</li>
<li>Daypart (e.g. 'morning')</li>
<li>HH Military (here you might put 14 for 2 pm)</li>
</ul>
<p>The key could be interesting here. Perhaps it's a sequence but, alternatively, you could use HHMMSS as the key (with leading zeros). I don't think performance will differ significantly.</p>
"
"65765002","Data Modeling with a Time Dimension","<p>Would it be better to create a time dimension with hh:mm:ss altogether or would it be better to split them up into 3 dimensions since separately they will take up less space?</p>
<p>What would you recommend and why?</p>
","<data-modeling><data-warehouse><business-intelligence>","2021-01-17 19:22:26","673","3","3","65819888","<p>What I ended up doing was just creating a separate time dimension with a row for each possible combination of hour, minute, and second.</p>
<p>I did this vs creating 3 separate tables for hours, minutes, and seconds each to keep the number of joins down and not make the database too clustered with tables.</p>
<p>I kept it separate from my date table to keep the number of rows down and making performance slower.</p>
<p>Seems to work pretty well, thanks for the feedback everyone.</p>
"
"65763310","Datamart exercise on a Data warehouse","<p>This is the text of the exercise:</p>
<p>Let us consider the monitoring of production machines, e.g., in the manufacturing industry. A machine, identified by a unique ID, operates under a given range of temperature, humidity, electric voltage, lubricant level, and other measurement types. Machines are installed with sensors that read the value of measures once a minute.
In case of failures, human operators document the type of problem, and the time needed for repairing the machine (downtime).
A company with several factories will collect the flow of data from sensors and operators and populate a data warehouse for analysis of failures.</p>
<p>The following are some examples of business questions collected during the user interviews.</p>
<p>(a) Number of failures by month and by the factory.</p>
<p>(b) Average downtime by failure type.</p>
<p>(c) Average, minimum, and maximum measurement value by measurement types.</p>
<p>(d) Average lubricant level in 1hour before a failure by machine ID.</p>
<p>Concerning the above business scenario, answer the following questions design a conceptual schema for the data mart to support the business questions.
Your schema should at least be able to satisfy the above mentioned analysis requirements.
You may motivate other suitable attributes for the dimensions.</p>
<p>I think that the solution is a datamart with two fact tables: one for machines and one for failures. I would like to know if there are other solutions.</p>
","<database><data-science><data-warehouse>","2021-01-17 16:44:08","96","0","1","65771421","<p>The Dim Machine will be a <a href=""https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/conformed-dimension/"" rel=""nofollow noreferrer"">conformed dimension</a> can exist as a single dimension table that relates to multiple fact tables within the same data warehouse, or as identical dimension tables in separate data marts.</p>
<p>In your case, the <strong>Multidimensional Schema</strong> used is <strong>Galaxy Schema</strong> since there are two fact tables <strong>Fact Monitoring</strong> and <strong>Fact Failure</strong> that share the <strong>Dim Machine</strong> between them. It is also called <strong>Fact Constellation Schema</strong>. The schema is viewed as a collection of stars hence the name Galaxy Schema.</p>
<p><a href=""https://i.stack.imgur.com/wzv28.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wzv28.png"" alt=""enter image description here"" /></a></p>
"
"65759429","Designing a star / snowflake schema database","<p>I have to design and build a star / snowflake schema database that will keep data about employees in a company - especially the rates that are payed to the employees. This is the first time I am experimenting with this schema type and I'm not sure about which parts of the fact tables should be separate dimension tables.</p>
<p>I don't exactly understand the practical upsides of having this schema, is it actually that much easier to perform queries on this type of database? Or is it only about the performance?</p>
<p>Below I am attaching the project of the schema of my database. I would like to know what should I modify for this to be the best possible version for this database. I also have a question about two things:</p>
<ol>
<li>Should the <code>rate</code> column be just a value in the fact table? Or should it be a foreign key to a dim_rate table?</li>
<li>What about date dimensions? Should they just be values in specific tables? Or should they always be foreign keys? If they should be foreign keys, should there be one <code>dim_date</code> table or a table for each type of date?</li>
</ol>
<p>As an example for question 2 lets takie the dim_employee table and the <code>employment_date</code> and <code>end_of_employment</code> columns. I have these dates as values in the <code>dim_employee</code> table but I can think of 2 other versions of how to handle this data: either foreign keys to a <code>dim_date</code> table or seperate fact tables for <code>fact_start_of_employment</code> and <code>fact_end_of_deployment</code>. I know I will need different kinds of report for example reports showing how many people started work and left the company for different date intervals (eg. in december of 2020). Honestly at this point I have no idea which option would be best and easiest to work with in the future.</p>
<p>Also as I said - I would love any constructive criticism of this schema, even if it means completely redesigning it.</p>
<p><a href=""https://i.stack.imgur.com/agJQo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/agJQo.png"" alt=""enter image description here"" /></a></p>
","<database><postgresql><database-design><architecture><data-warehouse>","2021-01-17 10:18:52","197","-2","2","65761054","<p>I would merge both fact tables because I think there is a strong relation between rate and position. But that's how I look at this data without knowing all the details.</p>
<p>I would also create a date dimension and a form_of_employment dimension.</p>
<p>That would result in 4 dimensions:</p>
<ul>
<li>dim_employee</li>
<li>dim_date</li>
<li>dim_position</li>
<li>dim_form_of_employment</li>
</ul>
<p>And a single fact table with these columns:
fact_assignment</p>
<ul>
<li>employee_id</li>
<li>date_id</li>
<li>position_id</li>
<li>form_of_employment_id</li>
<li>rate</li>
<li>student</li>
</ul>
<p>This setup results in a proper star and very simpel SQL for your reports</p>
"
"65759429","Designing a star / snowflake schema database","<p>I have to design and build a star / snowflake schema database that will keep data about employees in a company - especially the rates that are payed to the employees. This is the first time I am experimenting with this schema type and I'm not sure about which parts of the fact tables should be separate dimension tables.</p>
<p>I don't exactly understand the practical upsides of having this schema, is it actually that much easier to perform queries on this type of database? Or is it only about the performance?</p>
<p>Below I am attaching the project of the schema of my database. I would like to know what should I modify for this to be the best possible version for this database. I also have a question about two things:</p>
<ol>
<li>Should the <code>rate</code> column be just a value in the fact table? Or should it be a foreign key to a dim_rate table?</li>
<li>What about date dimensions? Should they just be values in specific tables? Or should they always be foreign keys? If they should be foreign keys, should there be one <code>dim_date</code> table or a table for each type of date?</li>
</ol>
<p>As an example for question 2 lets takie the dim_employee table and the <code>employment_date</code> and <code>end_of_employment</code> columns. I have these dates as values in the <code>dim_employee</code> table but I can think of 2 other versions of how to handle this data: either foreign keys to a <code>dim_date</code> table or seperate fact tables for <code>fact_start_of_employment</code> and <code>fact_end_of_deployment</code>. I know I will need different kinds of report for example reports showing how many people started work and left the company for different date intervals (eg. in december of 2020). Honestly at this point I have no idea which option would be best and easiest to work with in the future.</p>
<p>Also as I said - I would love any constructive criticism of this schema, even if it means completely redesigning it.</p>
<p><a href=""https://i.stack.imgur.com/agJQo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/agJQo.png"" alt=""enter image description here"" /></a></p>
","<database><postgresql><database-design><architecture><data-warehouse>","2021-01-17 10:18:52","197","-2","2","65776354","<p>For every BI or reporting system, you have a process of designing your tables and building them based on that design. This process is called dimensional modeling. Some others call it data warehouse design, which is the same thing. Dimensional modeling is the process of thinking and designing the data model including tables and their relationships. As you see, there is no technology involved in the process of dimensional modeling, It is all happening on your head and ends up with sketching diagrams on the paper. Dimensional modeling is not the diagram in which tables are connected to each other, it is the process of doing that.</p>
<p>Star Schema is the best way of designing a data model for reporting, You will get the best performance and also flexibility using such a model.</p>
<p>In this case the Employee Dimension will be a <a href=""https://www.datawarehouse4u.info/SCD-Slowly-Changing-Dimensions.html"" rel=""nofollow noreferrer"">Historical Dimension or Slowly Changing Dimension</a> :</p>
<p><a href=""https://i.stack.imgur.com/6gexA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6gexA.png"" alt=""enter image description here"" /></a></p>
<p>You can use a <a href=""https://www.kimballgroup.com/2014/05/design-tip-166-potential-bridge-table-detours/"" rel=""nofollow noreferrer"">bridge table</a>.
In a classic dimensional schema, each dimension attached to a fact table has a single value consistent with the fact table’s grain. But there are a number of situations in which a dimension is legitimately multivalued.
Like in your example, an employee can have many positions :</p>
<p><a href=""https://i.stack.imgur.com/0aIWM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0aIWM.png"" alt=""enter image description here"" /></a></p>
"
"65759115","How to design star schema in a restaurant scenario where customer place food orders and each order contains multiple food items","<p>Here I am having a FACT table, <strong>FACT_Orders</strong> and which has fields <strong>customer_id, order_id, total_cost, created_date, updated_date</strong>.</p>
<p>Also I have one more FACT table, <strong>FACT_Order_List</strong> to keep the items in the order which has fields order_list_id, order_id, item_id</p>
<p>Also I have the dimension tables <strong>DIM_Item</strong> and <strong>DIM_Customer</strong> to keep the arrtibutes of Item and Customer</p>
<p>From the FACT tables, I need to compute the most popular item and least popular item sold</p>
<p>Is this good design having two FACT tables and joining FACT tables for the computations?</p>
","<sql><database-design><data-modeling><data-warehouse><star-schema-datawarehouse>","2021-01-17 09:36:58","786","-1","1","65765952","<p>Assuming that :</p>
<blockquote>
<p>An item has many orders and an order contains many items.</p>
</blockquote>
<p>you can have a fact record like below : CustomerId=5 ordered 2 items</p>
<pre><code>OrderId OrderListId ItemId CustomerId ItemPrice   TotalCost   CreatedDate
1       1           12     5          100         350         17-01-2021
1       1           15     5          250         350         17-01-2021
</code></pre>
<p>You don't join fact tables. Facts are combined by summarizing each set of measures individually by common dimensions then joining the summarized sets on those dimensions.</p>
<p>Your model can be like below:</p>
<p><a href=""https://i.stack.imgur.com/efWeh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/efWeh.png"" alt=""enter image description here"" /></a></p>
<blockquote>
<p>Regading the  updated_date in the fact :</p>
</blockquote>
<p>Fact tables should not be updated. There are several really good reasons why facts measure values should almost never be updated. ... Fact Table Processing: Preforming updates on tables can be a very costly operation on the SQL Server data warehouse database server! Fact tables can and should be very large.</p>
"
"65757824","Database Architecture Question - Dimension Creation for Non-Atomic Data","<p>I am looking at an Excel file that will be imported into Power BI. I am not allowed to have access to the database itself due to employment reasons, so they gave me an Excel file to work with that I will then upload into Power BI.</p>
<p>On one of the fact &quot;tables&quot;, they have data that looks like this</p>
<pre><code>s-ID     success%     late%     on-time%     schedule
1        10%          2%        5%           calculus-1;algebra-2
1        5%           10%       27%          Calculus-1
1        5%           3%        80%          algebra-2
2        33%          50%       3%           null
5        5%           34%       8%           English-1;English-10;theatre;art
</code></pre>
<p>I realize the numbers do not make any sense, but that's basically how the data is structure wise. There are also roughly 100,000 records in this fact &quot;table&quot;.</p>
<p>I have a dimension for courses, but I'm not sure how to handle this schedule column. If I split the column vertically, the measure columns will be double counted.</p>
<p>How can I model this and put the schedule into a dimension intelligently in Power-BI?</p>
<p>My goal is as model the data as follows:</p>
<ul>
<li><p>Be able to split the schedule into separate rows, but simultaneously not double count all of the values.</p>
</li>
<li><p>I also want to show that the s-ID records have the student taking a
class that has both the calculus-1 and algebra together.</p>
</li>
</ul>
<p>Sometimes the professors schedule 2 classes together into 1 class whenever they are talking about topics that apply to both. There could be 2 classes together, there could be as many as 8 classes together or anything in between.</p>
<p>Is this a scenario where a bridge table would be appropriate?</p>
","<powerbi><data-modeling><data-warehouse><business-intelligence><powerbi-desktop>","2021-01-17 06:05:44","68","1","1","65777075","<p>You can use a <a href=""https://www.kimballgroup.com/2014/05/design-tip-166-potential-bridge-table-detours/"" rel=""nofollow noreferrer"">bridge table</a>. In a classic dimensional schema, each dimension attached to a fact table has a single value consistent with the fact table’s grain. But there are a number of situations in which a dimension is legitimately multivalued. Like in your example, a student can enroll many courses :</p>
<p><a href=""https://i.stack.imgur.com/5WKMW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5WKMW.png"" alt=""enter image description here"" /></a></p>
"
"65651825","Using Identity or sequence in data warehouse","<p>I'm new to data warehouse, So I try to follow the best practice, mimicking some implementation details from the Microsoft Demo DB <a href=""https://learn.microsoft.com/en-us/sql/samples/wide-world-importers-dw-install-configure?view=sql-server-ver15"" rel=""nofollow noreferrer"">WideWorldImportersDW</a>, One of the things that I have noticed is using <code>Sequence</code> as default value for PK over <code>Identity</code>.</p>
<hr />
<p>Could I ask, If it's preferable to use <code>Sequence</code> over <code>Identity</code> in data warehouse in general and Which one is more convenient especially during ETL process?.</p>
","<sql><sql-server><etl><data-warehouse><sql-server-2019>","2021-01-10 09:34:59","812","1","3","65653660","<p>A sequence has more guarantees than an identity column.  In particular, each call to a sequence is guaranteed to produce the <em>next</em> value for the sequence.</p>
<p>However, an <code>identity</code> column can have gaps and other inconsistencies.  This is all documented <a href=""https://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-transact-sql-identity-property?view=sql-server-ver15"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Because of the additional guarantees on sequences, I suspect that they are slower.  In particular, I suspect that the database cannot preallocate values in batch.  That means that in a multi-threaded environments, sequences would impose serialization on transactions, slowing things down.</p>
<p>In general, I see <code>identity</code> used for identifying columns in tables.  And although there is probably a performance comparison, I haven't seen one.  But I suspect that sequences are a wee bit slower in some circumstances.</p>
"
"65651825","Using Identity or sequence in data warehouse","<p>I'm new to data warehouse, So I try to follow the best practice, mimicking some implementation details from the Microsoft Demo DB <a href=""https://learn.microsoft.com/en-us/sql/samples/wide-world-importers-dw-install-configure?view=sql-server-ver15"" rel=""nofollow noreferrer"">WideWorldImportersDW</a>, One of the things that I have noticed is using <code>Sequence</code> as default value for PK over <code>Identity</code>.</p>
<hr />
<p>Could I ask, If it's preferable to use <code>Sequence</code> over <code>Identity</code> in data warehouse in general and Which one is more convenient especially during ETL process?.</p>
","<sql><sql-server><etl><data-warehouse><sql-server-2019>","2021-01-10 09:34:59","812","1","3","65658585","<p>Both <em>Sequence</em> and <em>Identity</em> are  designed for <code>OLTP</code> tables to enable effective assignment of unique keys in multi-session environment.</p>
<p>Important thing to realize is that in data warehouse environment you often have a different setup and there is only <em>one job</em> that populates a specific table.</p>
<p>In a <em>single user environment</em> you do not need the above features at all and you <em>can</em> simple assign  the keys <em>manually</em> starting with <code>max(id) +1</code> and increment by one for each row.</p>
<p>The <em>general rule of data warehouse</em> is that you should not search for <em>silver bullet recommendation</em> but check the functionality and preformance in your onw test.</p>
<p>If you make some research on <strong>SQL Server Identity vs Sequence</strong> e.g. <a href=""https://stackoverflow.com/questions/10062328/sequence-vs-identity"">here</a> or <a href=""https://www.sqlservercentral.com/blogs/sql-sequence-vs-identity-column"" rel=""nofollow noreferrer"">here</a> you get various result partly prefering the former partly the latter feature.</p>
<p>My recomendation is therefore to perform a test with the <em>manually</em> assigned <code>ID</code>s  (i.e. with no overhead) simple to get a baseline for the expectation.</p>
<p>Than repeat it with both <em>identity</em> and <em>sequence</em> - compare and choose.</p>
<p>The <code>sequence</code> in SQL Server was added later and is based on Oracle Sequence, so I would not expect it has some basic problem.</p>
<p>The experience from Oracle tells us, you need to have a large enought <code>cache</code> in the sequence to support effective bulk insert.</p>
<p>In the meantime the <code>identity</code> can also be defined as <a href=""https://learn.microsoft.com/en-us/sql/t-sql/statements/alter-database-scoped-configuration-transact-sql?view=sql-server-ver15"" rel=""nofollow noreferrer"">cached</a>, (IDENTITY_CACHE = { ON | OFF }) so once again, try all three posibilities (sequence, identity, nothing) and choose the best one.</p>
"
"65651825","Using Identity or sequence in data warehouse","<p>I'm new to data warehouse, So I try to follow the best practice, mimicking some implementation details from the Microsoft Demo DB <a href=""https://learn.microsoft.com/en-us/sql/samples/wide-world-importers-dw-install-configure?view=sql-server-ver15"" rel=""nofollow noreferrer"">WideWorldImportersDW</a>, One of the things that I have noticed is using <code>Sequence</code> as default value for PK over <code>Identity</code>.</p>
<hr />
<p>Could I ask, If it's preferable to use <code>Sequence</code> over <code>Identity</code> in data warehouse in general and Which one is more convenient especially during ETL process?.</p>
","<sql><sql-server><etl><data-warehouse><sql-server-2019>","2021-01-10 09:34:59","812","1","3","65668986","<p>Identity is scoped to a single table, is part of the table definition (DDL) and is reset on a truncate.  Identity is unique within the table.  Each table has its own identity value when configured and cannot be shared across tables.  In general usage, the &quot;next&quot; value is consumed by SQL Server when an Insert occurs on the table.+</p>
<p>Sequence is a first class object, scoped to the database.  The &quot;next&quot; value is consumed when the Sequence is used (NEXT VALUE FOR).</p>
<p>Sequences are most effectively used when you need a person readable unique identifier stored across multiple tables.  For example a ticketing system that stores ticket types in different tables may use a sequence to ensure no ticket receives the same number, regardless of the table in which it is stored, and that a person can reasonably refer to the number (not GUID).</p>
<p>In data warehousing, the dimension table needs a row identifier unique within the table.  In general, the OLTP primary key is not sufficient as it may be duplicated within the dimension table depending on the type of dimension, and you don't want to risk assigning additional context to the OLTP PK as that can cause challenges when the source data changes.  The dimension row identifier should only have meaning to the non-measure fact columns associated with it.  Fact columns are not joined across different dimensions.++</p>
<p><strong>Since the scope of the dimension table identifier is limited to the dimension table, an identity key is the ideal row identifier.  It is simple to create, compact to store, and is meaningless outside the dimension. You won't use the dimension identity on a report.  (Really, please don't be that developer.)</strong></p>
<p><em>+ Its rare you'll need to know the next value without needing to assign to a row.  Might be a red flag if you are trying to manipulate the identity value prior to assignment</em></p>
<p><em>++ a dimension view may union different tables to feed the OLAP cube, in which case a persistent repeatable key should be generated from the underlying data, usually by concatenating a string literal with each table key in a normalized format.</em></p>
"
"65605358","Raw number data management in python","<p>As we know python has a grammar system for recognize different types of data, specifically for number this is an integer: <strong>87665</strong> and this is a float: <strong>667.03</strong></p>
<p>My problem is that I have equipment raw data in the following format(example):</p>
<pre><code> 22.5946120.0000  9.2703108.3784 30.2703 35.4054  4.8378 74.3243
 51.8919 96.4865145.6757 63.7838  6.1892 12.4054 58.1081145.4054
 61.3514 52.1622 16.0000 38.6486 38.1081 75.9459 59.1892  9.8649
 45.6757 38.3784 45.9459 47.0270 51.8919 36.4865  4.6757 42.4324
 58.6486 28.1081 32.9730 58.1081  8.7568 45.1351 59.4595 30.5405
</code></pre>
<p>as you can see they would be floats on python, every &quot;number&quot;  takes 8 spaces including the dot, is like this: <strong>3 spaces before the dot/dot/4 spaces after the dot</strong>, so for example I could have the number &quot;0.01&quot; but in the raw data it will appear as &quot;--0.01--&quot;, or I could have &quot;562.4001&quot;, originally this was supposed to be processed with Fortran, but I prefer python.</p>
<p>I tried to stablish a rule for processing the data, like:</p>
<pre><code>in data replace &quot;  &quot; for &quot;, &quot;
</code></pre>
<p>being &quot;data&quot; a string of the raw number (I just converted the entire number's block into a string and the treated it like that) but sometimes there's just 1 space between 2 different number and sometimes there is no space at all. So basically I want to python to learn that in that string, every 8 spaces is a new float, I think that is possible with some slice command but I haven't got it quite yet... help...</p>
","<python><slice><raw>","2021-01-07 01:09:21","74","0","1","65605496","<p>Split the line into chunks of 8 characters. Strip the spaces from around each chunk, then call <code>float()</code> to convert them to floats.</p>
<pre><code>def line_to_floats(line, width):
    return list(map(float, [line[i:i+width].strip() for i in range(0, len(line), width)]))

text = ''' 22.5946120.0000  9.2703108.3784 30.2703 35.4054  4.8378 74.3243
 51.8919 96.4865145.6757 63.7838  6.1892 12.4054 58.1081145.4054
 61.3514 52.1622 16.0000 38.6486 38.1081 75.9459 59.1892  9.8649
 45.6757 38.3784 45.9459 47.0270 51.8919 36.4865  4.6757 42.4324
 58.6486 28.1081 32.9730 58.1081  8.7568 45.1351 59.4595 30.5405'''
for line in text.split('\n'):
    print(line_to_floats(line, 8))
</code></pre>
"
"65601231","Populate surrogate Datekey based on Date in a column","<p>I have created a table</p>
<pre><code>CREATE TABLE myTable(
DateID INT PRIMARY KEY NOT NULL,
myDate DATE)
</code></pre>
<p>Then I want to populate the table with data from a staging table using SSIS. The problem is that I don't understand how to generate DateID based on the incoming value from a staging table. For example:
after the staging table inserted 2020-12-12, I want my DateID to become 20201212 and so on.</p>
<p>I tried to google this issue but didn't find anything related to my case. (But I don't deny that I did it badly). How can I do it?</p>
","<sql><sql-server><ssis><etl><data-warehouse>","2021-01-06 18:18:11","33","1","1","65601280","<p>You can use a persisted computed column:</p>
<pre><code>CREATE TABLE myTable (
     DateID AS (YEAR(myDate) * 10000 + MONTH(myDate) * 100 + DAY(myDate)) PERSISTED PRIMARY KEY,
     myDate DATE
);
</code></pre>
<p><a href=""https://dbfiddle.uk/?rdbms=sqlserver_2019&amp;fiddle=dfb3947e145379a514a9aafd8e39729c"" rel=""nofollow noreferrer"">Here</a> is a db&lt;&gt;fiddle.</p>
"
"65593538","How to decrypt DB2 data in another data warehouse platform","<p>We need to send data from db2 (db2 for AS400) to another data warehouse platform (Hive). But we need to encrypt data in DB2 first. And then target server will connect to DB2 to export data and decrypt data in target server.</p>
<p>SQL to encrypt data in DB2:</p>
<pre><code>INSERT INTO TESTAES
SELECT ENCRYPT_AES( ACCOUNT, '1234567890') FROM TESTPLAIN;
</code></pre>
<p>I know the DECRYPT_CHAR function in DB2:</p>
<pre><code>SELECT DECRYPT_CHAR( ACCOUNT, '1234567890') FROM TESTAES
</code></pre>
<p>But after we load this table to another platform, we don't know how to decrypt the data. We don't know the DB2 decryption algorithm. </p>
<p>The way I thinks may works:</p>
<p>(1) Get the decryption algorithm of ENCRYPT_AES in DB2 and we can write a program in targer server to decrypt the data. But IBM shared this in any documents. I searched it in IBM DB2 document, just told us</p>
<blockquote>
<p>Encryption algorithm: The internal encryption algorithm used is from the CLiC Toolkit from IBM Research. The 128-bit encryption key is derived from the password using a SHA1 message digest?https://www.ibm.com/support/knowledgecenter/ssw_ibm_i_73/db2/rbafzscaencryptaes.htm</p>
</blockquote>
<p>(2) Get the decryption algorithm package of ENCRYPT_AES in DB2 and we can import this package in targer server to decrypt the data. Did IBM have such package?</p>
<p>(3) Use another open-source/common function/package to encrypt data in DB2. And we know the algorithm and we can write program/or use the same algorithm package to decrypt the data. But I don't how can we encrypt data in AS400 db2 except ENCRYPT_AES. Maybe write a java program or something else?</p>
<p>Would anyone share this experience in encrypted data migration to another platform.</p>
","<algorithm><encryption><db2>","2021-01-06 09:52:13","678","0","2","65608588","<p>it is standard AES algorithm, but the default CCSID in AS400 is EBCDIC.</p>
<p>do you need to convert DATA to UTF-8 after decryption.</p>
"
"65593538","How to decrypt DB2 data in another data warehouse platform","<p>We need to send data from db2 (db2 for AS400) to another data warehouse platform (Hive). But we need to encrypt data in DB2 first. And then target server will connect to DB2 to export data and decrypt data in target server.</p>
<p>SQL to encrypt data in DB2:</p>
<pre><code>INSERT INTO TESTAES
SELECT ENCRYPT_AES( ACCOUNT, '1234567890') FROM TESTPLAIN;
</code></pre>
<p>I know the DECRYPT_CHAR function in DB2:</p>
<pre><code>SELECT DECRYPT_CHAR( ACCOUNT, '1234567890') FROM TESTAES
</code></pre>
<p>But after we load this table to another platform, we don't know how to decrypt the data. We don't know the DB2 decryption algorithm. </p>
<p>The way I thinks may works:</p>
<p>(1) Get the decryption algorithm of ENCRYPT_AES in DB2 and we can write a program in targer server to decrypt the data. But IBM shared this in any documents. I searched it in IBM DB2 document, just told us</p>
<blockquote>
<p>Encryption algorithm: The internal encryption algorithm used is from the CLiC Toolkit from IBM Research. The 128-bit encryption key is derived from the password using a SHA1 message digest?https://www.ibm.com/support/knowledgecenter/ssw_ibm_i_73/db2/rbafzscaencryptaes.htm</p>
</blockquote>
<p>(2) Get the decryption algorithm package of ENCRYPT_AES in DB2 and we can import this package in targer server to decrypt the data. Did IBM have such package?</p>
<p>(3) Use another open-source/common function/package to encrypt data in DB2. And we know the algorithm and we can write program/or use the same algorithm package to decrypt the data. But I don't how can we encrypt data in AS400 db2 except ENCRYPT_AES. Maybe write a java program or something else?</p>
<p>Would anyone share this experience in encrypted data migration to another platform.</p>
","<algorithm><encryption><db2>","2021-01-06 09:52:13","678","0","2","65625526","<p>I think this question can be closed. Becuase</p>
<p>(1) I asked IBM Technical Support, he suggest us to write encryption/decryption function by ourselves. Maybe the algorithm of DB2 encryption function is a secret.</p>
<p>(2) I created an DB2 UDF to call Java Program in AS400. Finally, it works. I can encrypt data in DB2 and after other database get encrypted data and it can be decrypted in other database.</p>
"
"65584693","How model a dimension with composite key?","<p>I have a fact named <code>sales</code> which have FKs to dimensions <code>product</code> and <code>store</code>. Each of these dimensions have information about that dimension alone, but I have some information about a <code>product</code> in a specific <code>store</code> like where a product is in that store.</p>
<p>I am tempted to model a dimension where the primary key is a combination of product and store, it is ok to do that or some better alternative exists?</p>
","<data-warehouse><dimensional-modeling>","2021-01-05 18:38:27","417","0","2","65611959","<p>my thoughts...</p>
<ol>
<li>Having a 3rd dimension for location is definitely a viable option. You could also include store details within this Dim (but still have the location as its level of granularity) and have a Location &gt; Store hierarchy</li>
<li>You won't find references to a dimension having a PK with multiple columns because that would break one of the fundamental design principles of dimensional modelling</li>
<li>I'm confused/surprised by your statement that your source system is generating surrogate keys? Given that surrogate keys (in this context) are entirely an artefact within a data warehouse, it seems unlikely that a source system would be generating them</li>
</ol>
"
"65584693","How model a dimension with composite key?","<p>I have a fact named <code>sales</code> which have FKs to dimensions <code>product</code> and <code>store</code>. Each of these dimensions have information about that dimension alone, but I have some information about a <code>product</code> in a specific <code>store</code> like where a product is in that store.</p>
<p>I am tempted to model a dimension where the primary key is a combination of product and store, it is ok to do that or some better alternative exists?</p>
","<data-warehouse><dimensional-modeling>","2021-01-05 18:38:27","417","0","2","65776801","<p>Be careful another dimension = more joins = complex queries.</p>
<p>You can stick to a simple modeling :
<a href=""https://i.stack.imgur.com/PCvE8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PCvE8.png"" alt=""enter image description here"" /></a></p>
"
"65582399","How do I replace ""?"" with NULL in SSIS expression?","<p>There are ? in my data columns and I have to replace them with NULL. In my SSIS package.</p>
","<ssis><data-warehouse><sql-data-warehouse>","2021-01-05 16:05:07","213","-1","1","65584274","<p>if a question mark is in the column already it must be some kind of string.</p>
<p>Use a derived column and a if function:</p>
<pre><code>[colName] == &quot;?&quot; ? NULL(DT_STR, «length», 1252) : [colName]
</code></pre>
<p>or</p>
<pre><code>[colName] == &quot;?&quot; ? NULL(DT_WSTR, «length») : [colName]
</code></pre>
"
"65562790","Periodic snapshot fact table - Design question","<p>I'm working on the design of a new periodic snapshot fact table. I'm looking into health insurance claims and the amount of money people owe to the insurance company and the amount they've already paid. Data in the table will look like this.</p>
<pre><code>CLAIM_ID    TIME_KEY   AMOUNT_OWED     PAID
123        31.1.2000          1000     0
123        28.2.2000           900     100
123        31.3.2000           800     200
123        30.4.2000             0     1000
123        31.5.2000             0     1000
123        30.6.2000             0     1000
123        31.7.2000             0     1000
123        31.8.2000             0     1000
...
</code></pre>
<p>As you can see after 30.4.2000 it doesn't make sense to insert new data for claim_id 123 as it no longer changes (there is a reasonable degree of certainty this won't happen). Is it a good idea to stop inserting data for this claim or should I do so till the end of time :)?</p>
<p>I'm mainly concerned about sticking to best practices when designing Data Warehouse tables.</p>
<p>Thanks for any answer!</p>
","<database><data-warehouse><fact><fact-table>","2021-01-04 12:31:47","351","0","2","65563404","<p>just a few thoughts...</p>
<ol>
<li>Unless you can have multiple payments in a day against a claim (and potentially other transactions e.g. interest that increases the amount owed), then what you have shown is not really a snapshot fact, it is a transactional fact. The normal example given is a bank account where you have multiple in/out transactions per day and then a snapshot of the end-of-day (or end-of-month) position. Obviously I don't know your business model but it seems unlikely that there would be multiple transactions per day against a single claim</li>
<li>If there have been no changes to a claim since the last fact record was created there seems little point creating a new fact record</li>
</ol>
"
"65562790","Periodic snapshot fact table - Design question","<p>I'm working on the design of a new periodic snapshot fact table. I'm looking into health insurance claims and the amount of money people owe to the insurance company and the amount they've already paid. Data in the table will look like this.</p>
<pre><code>CLAIM_ID    TIME_KEY   AMOUNT_OWED     PAID
123        31.1.2000          1000     0
123        28.2.2000           900     100
123        31.3.2000           800     200
123        30.4.2000             0     1000
123        31.5.2000             0     1000
123        30.6.2000             0     1000
123        31.7.2000             0     1000
123        31.8.2000             0     1000
...
</code></pre>
<p>As you can see after 30.4.2000 it doesn't make sense to insert new data for claim_id 123 as it no longer changes (there is a reasonable degree of certainty this won't happen). Is it a good idea to stop inserting data for this claim or should I do so till the end of time :)?</p>
<p>I'm mainly concerned about sticking to best practices when designing Data Warehouse tables.</p>
<p>Thanks for any answer!</p>
","<database><data-warehouse><fact><fact-table>","2021-01-04 12:31:47","351","0","2","65569107","<p>Typically you choose a periodic snapshot if you have</p>
<p>a) a large number of transactions and</p>
<p>b) you need an effective access to the data at some point of time (end of the month in your case)</p>
<p>If you have say 50 claim transactions per month and the claim is active one year on average, you will profit from this design even if you will hold the inactive claims for 50 years (which you will probably will not do;)</p>
<p>Your doubts suggest that you have not so much transactions per claim life cycle. It that case you should consider a <em>fact table</em> storing each transaction.</p>
<p>You will have definitively no overhead for inactive claims, but to get a snapshot information at a specific time you'll have to read the whole table.</p>
<p>On the contrary the periodic snapshot is typically partitioned on the snapshot time, so the access is very affective.
get no <em>free lunch</em> with <strong>saving space and  an effective access</strong>.</p>
"
"65487344","Reshaping Data in Stata","<p>I have a dataset that is composed of research studies.  Within some of the studies are multiple data points (DP).  My data is structured so that each row is a separate data point.  Additionally, I have a separate variable that denotes the specific research article.</p>
<p>I need to obtain summary statistics from the data relative to the research studies (not DPs).  In other words, I need for every row to become research studies with the DPs becoming counts.</p>
<p>I have tried the code below using <code>contract</code>.  It works for the <code>list</code> command.  However, I need summary statistics as well as I'd like to get summaries for multiple variables and combine them into one table once the data is organized.</p>
<pre><code>contract study nation
drop _freq study
contract nation
list
</code></pre>
<p>EXAMPLE:</p>
<p>Raw Data:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Study</th>
<th>DP</th>
<th>Year</th>
<th>Nation</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>2005</td>
<td>Brazil</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>2005</td>
<td>Brazil</td>
</tr>
<tr>
<td>1</td>
<td>3</td>
<td>2005</td>
<td>Brazil</td>
</tr>
<tr>
<td>1</td>
<td>4</td>
<td>2005</td>
<td>France</td>
</tr>
<tr>
<td>2</td>
<td>5</td>
<td>2006</td>
<td>Brazil</td>
</tr>
<tr>
<td>2</td>
<td>6</td>
<td>2006</td>
<td>Italy</td>
</tr>
<tr>
<td>3</td>
<td>7</td>
<td>2010</td>
<td>Brazil</td>
</tr>
<tr>
<td>3</td>
<td>8</td>
<td>2010</td>
<td>Canada</td>
</tr>
<tr>
<td>4</td>
<td>9</td>
<td>2011</td>
<td>Canada</td>
</tr>
<tr>
<td>5</td>
<td>10</td>
<td>2015</td>
<td>Brazil</td>
</tr>
<tr>
<td>6</td>
<td>11</td>
<td>2015</td>
<td>Canada</td>
</tr>
</tbody>
</table>
</div>
<p>What I need:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Year</th>
<th>f (of studies)</th>
</tr>
</thead>
<tbody>
<tr>
<td>2005</td>
<td>1</td>
</tr>
<tr>
<td>2006</td>
<td>1</td>
</tr>
<tr>
<td>2010</td>
<td>1</td>
</tr>
<tr>
<td>2011</td>
<td>1</td>
</tr>
<tr>
<td>2015</td>
<td>2</td>
</tr>
</tbody>
</table>
</div>
<p>And I also need a histogram of the above table.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Nation</th>
<th>f (of studies)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Brazil</td>
<td>4</td>
</tr>
<tr>
<td>Canada</td>
<td>3</td>
</tr>
<tr>
<td>France</td>
<td>1</td>
</tr>
<tr>
<td>Italy</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>I have more variables that will need this.  And they will need more than frequencies (e.g. mean, sd, var).  So whatever solution is given needs to work for summarizing variables as well.</p>
","<stata><data-management>","2020-12-29 04:37:25","61","0","1","65490398","<p><code>egen</code> will help with summary statistics and graphs. Its <code>tag()</code> function will let you tag each country just once.</p>
<p>Note here that <code>dataex</code> in Stata is a better way to give a code example, as explained in the Statalist FAQ and here at the Stata tag.</p>
<pre><code>* Example generated by -dataex-. To install: ssc install dataex
clear
input byte(Study DP) int Year str6 Nation
1  1 2005 &quot;Brazil&quot;
1  2 2005 &quot;Brazil&quot;
1  3 2005 &quot;Brazil&quot;
1  4 2005 &quot;France&quot;
2  5 2006 &quot;Brazil&quot;
2  6 2006 &quot;Italy&quot; 
3  7 2010 &quot;Brazil&quot;
3  8 2010 &quot;Canada&quot;
4  9 2011 &quot;Canada&quot;
5 10 2015 &quot;Brazil&quot;
6 11 2015 &quot;Canada&quot;
end

egen tag = tag(Nation)

egen count = count(DP) , by(Nation)

histogram count if tag, discrete freq width(1) xla(1/6)
</code></pre>
"
"65473180","Stata - Show grouped frequencies","<p>I have a dataset with nations, studies, and data collections. I'm wanting to create a frequency distribution where it shows the total number of studies conducted in each nation.</p>
<p>The problem I'm running into is that my data is organized by data collections.  Some studies have multiple data collection sites and measure different parts of a nation (we'll say 3 areas).  Therefore, one study can have multiple rows.  However, for the table I need, I essentially want it to group the studies together, and report with a yes or no (0 or 1) of whether that nation was included in the study. Essentially, I don't care how many places in a nation were observed in each individual study. I just want to know whether or not a nation is included in a study. The example below may help.</p>
<p>Current data:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Study</th>
<th>Data Coll</th>
<th>Nation</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>Brazil</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>Brazil</td>
</tr>
<tr>
<td>1</td>
<td>3</td>
<td>Brazil</td>
</tr>
<tr>
<td>1</td>
<td>4</td>
<td>France</td>
</tr>
<tr>
<td>2</td>
<td>5</td>
<td>Brazil</td>
</tr>
<tr>
<td>2</td>
<td>6</td>
<td>India</td>
</tr>
<tr>
<td>3</td>
<td>7</td>
<td>Brazil</td>
</tr>
<tr>
<td>4</td>
<td>8</td>
<td>France</td>
</tr>
</tbody>
</table>
</div>
<p>Desired Table:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Nation</th>
<th>f (of studies)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Brazil</td>
<td>3</td>
</tr>
<tr>
<td>France</td>
<td>2</td>
</tr>
<tr>
<td>India</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>As you can see, in the top example, Study #1 observed 3 different areas of Brazil.  This nation was also observed in Studies #2 and #3.  The table I need to create just has a &quot;3&quot; by Brazil, as that is how many studies Brazil was included in.</p>
","<grouping><stata><data-management>","2020-12-28 04:59:42","149","0","2","65474616","<pre><code>* Example generated by -dataex-. To install: ssc install dataex
clear
input byte(study data_coll) str6 nation
1 1 &quot;Brazil&quot;
1 2 &quot;Brazil&quot;
1 3 &quot;Brazil&quot;
1 4 &quot;France&quot;
2 5 &quot;Brazil&quot;
2 6 &quot;India&quot; 
3 7 &quot;Brazil&quot;
4 8 &quot;France&quot;
end

contract study nation
drop _freq study
contract nation
list
</code></pre>
<p>Result:</p>
<pre><code>. list

     +----------------+
     | nation   _freq |
     |----------------|
  1. | Brazil       3 |
  2. | France       2 |
  3. |  India       1 |
     +----------------+
</code></pre>
"
"65473180","Stata - Show grouped frequencies","<p>I have a dataset with nations, studies, and data collections. I'm wanting to create a frequency distribution where it shows the total number of studies conducted in each nation.</p>
<p>The problem I'm running into is that my data is organized by data collections.  Some studies have multiple data collection sites and measure different parts of a nation (we'll say 3 areas).  Therefore, one study can have multiple rows.  However, for the table I need, I essentially want it to group the studies together, and report with a yes or no (0 or 1) of whether that nation was included in the study. Essentially, I don't care how many places in a nation were observed in each individual study. I just want to know whether or not a nation is included in a study. The example below may help.</p>
<p>Current data:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Study</th>
<th>Data Coll</th>
<th>Nation</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>Brazil</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>Brazil</td>
</tr>
<tr>
<td>1</td>
<td>3</td>
<td>Brazil</td>
</tr>
<tr>
<td>1</td>
<td>4</td>
<td>France</td>
</tr>
<tr>
<td>2</td>
<td>5</td>
<td>Brazil</td>
</tr>
<tr>
<td>2</td>
<td>6</td>
<td>India</td>
</tr>
<tr>
<td>3</td>
<td>7</td>
<td>Brazil</td>
</tr>
<tr>
<td>4</td>
<td>8</td>
<td>France</td>
</tr>
</tbody>
</table>
</div>
<p>Desired Table:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Nation</th>
<th>f (of studies)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Brazil</td>
<td>3</td>
</tr>
<tr>
<td>France</td>
<td>2</td>
</tr>
<tr>
<td>India</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>As you can see, in the top example, Study #1 observed 3 different areas of Brazil.  This nation was also observed in Studies #2 and #3.  The table I need to create just has a &quot;3&quot; by Brazil, as that is how many studies Brazil was included in.</p>
","<grouping><stata><data-management>","2020-12-28 04:59:42","149","0","2","65490912","<p>Note that if you wish to keep the original data structure, you can avoid using <code>contract</code>.</p>
<pre><code>egen tag = tag(Nation)
egen wanted = total(tag), by(Study)
tabdisp Nation, c(wanted)
</code></pre>
"
"65471153","Why would I not used Databricks as my data mart?","<p>I'm trying to get my head around Databricks.</p>
<p>I've found documentation stepping through importing data from S3 or Azure Datalake, and then outputting into Azure Synapse Analytics or another Data Warehouse solution.</p>
<p>After a quick play, I've recognised that you can simply save a table in Databricks, access it using SQL, and even pull it into PowerBI as a source.</p>
<p>So my question: for a small Datamart (10 dims, 5 facts), why would I choose to pay for an additional database solution like Azure SQL, Synapse, RDS or other when I could simply leave the data in a table in Databricks and then access it directly from my reporting tool from there?</p>
<p>Thank you in advance.</p>
<p>Andy</p>
","<databricks><data-warehouse><datamart>","2020-12-27 22:40:26","471","3","1","65588534","<p>Yes this is very much possible . Just to let you know that  SQL Azure and Synapse may be a Microsoft offering but they are for different purpose , Synapse supports MPP and so it more big data implementation . Also its not only how many dimension and fact table you have , how much data you have , what kind of aggregation it has etc becomes decisive .</p>
"
"65468423","Put hierarchy in its own separate dimension or set it as a part of related dimension?","<p>I'm new to <code>the dimensional model</code> in dataware house design, and I face some confusion in my first design.</p>
<p>I take a simple business process (<code>Vacation Request</code>), and I want to ask which of these two designs is accurate and near to be effective and applicable, I will be grateful if I get a detailed answer please? (My question about the dimension design mainly)</p>
<pre><code>1- 

Dimension.Employee           Fact.Vacation
[Employee Key]               [Employee Key] FK PK
[_source key]                [Vacation Transaction]PK DD
[Employee Name]              ...
....                         ...
[Campus Code]
[Campus Name]
[Department Code]
[Department Name]
[Section code]
[Section Name]
.... 
</code></pre>
<hr />
<pre><code>2- 

Dimension.Employee           Dimension.Section         Fact.Vacation
[Employee Key]               [Section Key]             [Employee Key] FK PK
[_source key]                [_source key]             [Vacation Transaction]PK DD
[Employee Name]              [Department Code]         [Section Key]FK
....                         [Department Name]         ...
....                         [Campus Code]
                             [Campus Name]
</code></pre>
<hr />
<p>Where the hierarchy is like this:</p>
<pre><code>Campus Contains --&gt; 
Many Departments and each department contains --&gt; 
many sections and each section contains many employees
</code></pre>
","<sql><sql-server><data-warehouse><business-intelligence><dimensional-modeling>","2020-12-27 17:24:54","771","2","2","65470804","<p>The second.</p>
<p>Employees are a WHO, departments are a WHERE.</p>
"
"65468423","Put hierarchy in its own separate dimension or set it as a part of related dimension?","<p>I'm new to <code>the dimensional model</code> in dataware house design, and I face some confusion in my first design.</p>
<p>I take a simple business process (<code>Vacation Request</code>), and I want to ask which of these two designs is accurate and near to be effective and applicable, I will be grateful if I get a detailed answer please? (My question about the dimension design mainly)</p>
<pre><code>1- 

Dimension.Employee           Fact.Vacation
[Employee Key]               [Employee Key] FK PK
[_source key]                [Vacation Transaction]PK DD
[Employee Name]              ...
....                         ...
[Campus Code]
[Campus Name]
[Department Code]
[Department Name]
[Section code]
[Section Name]
.... 
</code></pre>
<hr />
<pre><code>2- 

Dimension.Employee           Dimension.Section         Fact.Vacation
[Employee Key]               [Section Key]             [Employee Key] FK PK
[_source key]                [_source key]             [Vacation Transaction]PK DD
[Employee Name]              [Department Code]         [Section Key]FK
....                         [Department Name]         ...
....                         [Campus Code]
                             [Campus Name]
</code></pre>
<hr />
<p>Where the hierarchy is like this:</p>
<pre><code>Campus Contains --&gt; 
Many Departments and each department contains --&gt; 
many sections and each section contains many employees
</code></pre>
","<sql><sql-server><data-warehouse><business-intelligence><dimensional-modeling>","2020-12-27 17:24:54","771","2","2","65524967","<p>Good question! I've faced this situation a number of times myself. I'm afraid this is going to get a bit confusing and the final answer will be, &quot;it depends&quot;, but here are some things to consider...</p>
<ul>
<li>WHAT A STAR SCHEMA REALLY IS: While people think of a data warehouse as a reporting database, it actually performs two functions: data integration and data distribution. It turns out that the best data structures for integration are not great for distribution (here's a <a href=""https://dataspace.com/big-data-strategy/star-schema-data-mart-yes-data-warehouse-no/"" rel=""nofollow noreferrer"">blog post</a> I wrote about this some years ago). Star schemas are really about data distribution - getting data out of the system quickly and easily. The best data structures for this have no joins, i.e. they are akin to flat files (yes, I realize there are some DB buffering considerations that might affect this a bit but, in a general sense, indexed flat files do avoid all joins).</li>
</ul>
<p>The star schema takes that flat file and normalizes it a little, largely to save disk space (it's a huge space waster when you have to write out every attribute of each dimension on every record). So, when people say a star schema is denormalized, they are partially incorrect. The dimension tables are denormalized (snowflake schemas normalize these) but the fact table is normalized - it's got a bunch of attributes dependent on a unique primary key.</p>
<p>So, this concept would point to minimizing the number of dimensions in order to minimize the number of joins you need to make. Point for putting them into <strong>one dimension</strong>.</p>
<ul>
<li><p>FACT TABLE SHOWS RELATIONSHIPS: Your fact table shows the relationship between otherwise unrelated dimension elements. For example, in the absence of a sale, there is no relationship between a product and a customer. A sale creates that relationship and the sale fact record models it. In your case, there is a natural relationship between section and employee (I assume, at least). You don't need a fact table to model this relationship and, therefore, they should both be in one dimension table. Another point for putting them into <strong>one dimension</strong>.</p>
</li>
<li><p>CAN AN EMPLOYEE BE IN MULTIPLE SECTIONS SIMULTANEOUSLY?: If an employee can be in multiple sections at the same time then you probably do need the fact table to model this relationship (otherwise, each employee would need two active records in the employee dimension table). Point for putting them into <strong>separate dimensions</strong>.</p>
</li>
<li><p>DO EMPLOYEES CHANGE SECTIONS FREQUENTLY?: If so, and you only have one dimension, you'll end up having to constantly be modifying the employee record in your employee dimension - this leads to a longer than needed dimension table if this is a type two slowly changing dimension (i.e. one where you're tracking the history of changes to dimension elements). Point for putting them into <strong>separate dimensions</strong>.</p>
</li>
<li><p>DO YOU NEED TO AGGREGATE BY SECTION?: If you have a lot of sections and frequently report at the section level, you may need to create a fact table aggregated at the section level. In this case, if you're a staunch believer in having your DB enforce your relationships (I am), you'll need a section table to which your fact table can relate. In this case you'll need a section table. Point for putting them into <strong>separate dimensions</strong>.</p>
</li>
<li><p>WILL THE SECTION DIMENSION BE USED BY OTHER FACT TABLES?: One tough situation with star schemas occurs when you're using conformed dimensions (i.e. dimensions that are shared by multiple fact tables). The problem occurs when the different fact tables are defined at different levels in the dimension hierarchy. In your case, imagine there is a different fact table, say one modeling equipment purchases and it only makes sense at the section, not the employee, level. In this case, you'd probably split the section dimension into its own table so it can be shared by both fact tables, your current one and that future, equipment one. This, BTW, is similar to the consideration related to aggregate tables mentioned earlier. Point for putting them into <strong>separate dimensions</strong>.</p>
</li>
</ul>
<p>Anyhow, that's off the top of my head. So, I guess the answer is, &quot;it depends&quot;. Both will work, it just depends on other factors you're trying to optimize for. I'll try to edit my answer if anything else comes to mind. Best of luck with this!</p>
"
"65442617","Convert csv data catalogue to word2vec object","<p>I found a <a href=""https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv"" rel=""nofollow noreferrer"">Spotify csv data catalogue</a>, but I want to convert it into word2vec (specifically the track title and artist name) and show the vector in t-SNE. I saw that it's common to save your word2vec results TO csv, but is it possible to convert csv files to word2vec object?</p>
","<python><scikit-learn><data-visualization><word2vec>","2020-12-24 19:35:43","175","0","1","65444601","<p>It is not common to save word-vectors to CSV.</p>
<p>And, that file you've linked isn't typical dense high-dimensional word-vector embedding data.</p>
<p>Rather, there are a few named, meaningful scalar values from Spotify's analysis. (I see things like <code>danceability</code>, <code>energy</code>, <code>speechiness</code>, etc.)</p>
<p>An actual word2vec model <em>doesn't</em> usually label the values with such easily-interpretable names.</p>
<p>This might still be interesting multi-dimensional data for clustering/plotting, but you wouldn't at any point convert it to a &quot;word2vec object&quot; to do that.</p>
<p>(And, if you did either train some word-vectors from the artist/title info alone, or use external word-vectors to convert the artist/title to word-vector dimensions, the results might be disappointing - those words may not characterize the underlying patterns very well, except in some really coarse ways that are obvious from shared words (like love-songs having 'love' in the title, or those with 'remix' in the title being a little more dancey, etc).</p>
<p>What is your real ultimate goal in working with this data?</p>
"
"65412083","How to connect spyder to a postgres database?","<p>I am using Python with spyder and would like to directly connect to the our postgres datawarehouse to directly query data from there for further analysis.</p>
<p>This is what I found under the following link:
[https://www.psycopg.org/docs/]</p>
<p><code>import psycopg</code></p>
<p>Connect to your postgres DB
<code>conn = psycopg.connect(&quot;dbname=*** user=***&quot;)</code></p>
<p>Open a cursor to perform database operations
<code>cur = conn.cursor()</code></p>
<p>Execute a query
<code>cur.execute(&quot;SELECT * FROM my_data&quot;)</code></p>
<p>Retrieve query results
<code>records = cur.fetchall()</code></p>
<p>However, I get the following error message:</p>
<p><em>runfile('C:/Users/.../.spyder-py3/untitled2.py', wdir='C:/Users/.../.spyder-py3')
Traceback (most recent call last):
File &quot;C:\Users....spyder-py3\untitled2.py&quot;, line 9, in 
import psycopg
ModuleNotFoundError: No module named 'psycopg'</em></p>
","<python><postgresql><connection><spyder><data-warehouse>","2020-12-22 16:13:42","1207","0","2","65412236","<p><a href=""https://www.psycopg.org/docs/"" rel=""nofollow noreferrer"">https://www.psycopg.org/docs/</a> -&gt; this is what you are looking for.
Next step might be using pandas read_sql() to load the sql table into to your variable exploirer if it is not too big.</p>
<p>Does this help?</p>
"
"65412083","How to connect spyder to a postgres database?","<p>I am using Python with spyder and would like to directly connect to the our postgres datawarehouse to directly query data from there for further analysis.</p>
<p>This is what I found under the following link:
[https://www.psycopg.org/docs/]</p>
<p><code>import psycopg</code></p>
<p>Connect to your postgres DB
<code>conn = psycopg.connect(&quot;dbname=*** user=***&quot;)</code></p>
<p>Open a cursor to perform database operations
<code>cur = conn.cursor()</code></p>
<p>Execute a query
<code>cur.execute(&quot;SELECT * FROM my_data&quot;)</code></p>
<p>Retrieve query results
<code>records = cur.fetchall()</code></p>
<p>However, I get the following error message:</p>
<p><em>runfile('C:/Users/.../.spyder-py3/untitled2.py', wdir='C:/Users/.../.spyder-py3')
Traceback (most recent call last):
File &quot;C:\Users....spyder-py3\untitled2.py&quot;, line 9, in 
import psycopg
ModuleNotFoundError: No module named 'psycopg'</em></p>
","<python><postgresql><connection><spyder><data-warehouse>","2020-12-22 16:13:42","1207","0","2","65424797","<p>Posting this in the Spyder Editor actually worked to establish a connection with the PostgreSQL Database:</p>
<pre><code>import psycopg2  
connection = psycopg2.connect(user=&quot;...&quot;,                                   
password=&quot;...&quot;,                                   
host=&quot;...&quot;,                                   
port=&quot;...&quot;,                                   
database=&quot;...&quot;) 
cursor = connection.cursor() cursor.execute(&quot;SELECT * FROM data.table25 limit 5&quot;)
</code></pre>
<p>After running the above command you can check for your rows with:</p>
<pre><code>rows=cur.fetchall()
</code></pre>
"
"65283470","Dimensional Modeling: app session or activity measures","<p>I am trying to answer the below question given by the business (The business generates revenue from multiple apps through customer pay model) The business is interested in the below questions</p>
<ol>
<li>new users (trend with respect to previous months)</li>
<li>daily active users</li>
<li>Day 1 retention
I came up with the below DM</li>
</ol>
<p>Dimension: users, app, deviceid, useractions, plan, date</p>
<p>Fact: fact_activity(userid, appid,deviceid, actionid)</p>
<p>Actions could be: app installed, app launch, registered, completed purchase, postedcomments, playgame etc</p>
<p>The questions I have is</p>
<ol>
<li>should the fact table contain action_type instead of the actionid into the fact (to avoid join with useractions)</li>
<li>Definition of day 1 retention: No of apps installed/ app launches next day how do to avoid multiple counting of single user using multiple devices</li>
<li>Would it be advisable to have device details in the user dimension
or separate.</li>
<li>If I need to measure average session duration, should I use another fact at session level or tweak the activity fact?</li>
</ol>
","<data-warehouse><dimensional-modeling><star-schema-datawarehouse>","2020-12-14 04:08:55","152","0","1","65291865","<p>your questions are really unanswerable without significant more information about your business processes, data definitions, etc. In effect, you are asking someone to design a dimensional model for you before they can answer your questions - which is obviously not going to happen.</p>
<p>However, I can give you some very generic pointers that may help you:</p>
<h2>Dimensions</h2>
<p>A Dimension describes an entity so if attributes can't be described as belonging to the same entity then they shouldn't be in the same dimension. In your case, I assume a Device and a User are not the same thing and therefore they need to be separate dimensions</p>
<h2>Facts</h2>
<p>You need to define your measures i.e. what precisely are the things you are going to want to aggregate (count, sum, avg, etc) and how are they defined/calculated.</p>
<p>For each measure, you also need to define its grain i.e. what is the minimum set of dimensions that uniquely identify it. Once you have the grain defined, if multiple measures have the same grain then they can be held in the same fact table and if they don't then they can't</p>
"
"65266154","Cosmos DB data modelling to optimize search","<p>I watched <a href=""https://azure.microsoft.com/en-us/resources/videos/build-2019-best-practices-for-azure-cosmos-db-data-modeling-partitioning-and-rus/"" rel=""nofollow noreferrer"">this video</a> on data modelling in Cosmos DB.</p>
<p>In the video, it is explained that if you can model your data such that your most common queries are in partition queries, then you can minimize RUs, which in turn minimizes cost and maximizes performance.</p>
<p>The example used in the video is a blogging system. They showed that by moving things around such that Blog Posts and Comments are stored as separate entities in the same collection all partitioned by <code>blogId</code> they could achieve a low RU for a common query.</p>
<p>They then showed that searching for all blog posts by a specific user, being a cross partition query, is very expensive. So they then duplicate all blog post data and add each blog post as a separate entity to the users collection, which is already partitioned by <code>userId</code>. Searching for posts by a user is now cheap. The argument is storage is much cheaper than CPU time so this is a fine thing to do.</p>
<p>My question is: do I continue to follow this pattern when I want to make more things efficiently searchable? For example, I want to be able to search on blog topic (of which there could be many per blog post), a discrete blog rating, and so on.</p>
<p>I feel like extending this pattern for each search term is unsustainable. In these cases, do I just have to live with high RU searches or is there some clever way of making things efficient?</p>
","<azure-cosmosdb><data-modeling>","2020-12-12 14:39:06","122","0","1","65281011","<p>The essentially comes down to knowing whether the cost of using change feed to copy data from one container to another is less than the cost of doing cross-partition queries. This requires knowing the access patterns of your application and also requires measuring the average cost of these queries versus the cost of using change feed to make another copy. Change feed consumes 2 RU/s when it polls the container, then 1 RU for each 1Kb or less read from the source container and ~8 RU for each 1Kb or less insert on target container depending on your index policy. Multiply that by the rate at which data is inserted or updated. Then calculate this per day or per month to compare cost.</p>
<p>If what you're looking for is to do free text search on your data, you may want to look at using Azure Search. This is simpler than using the approach using change feed, but Azure Search can be quite expensive as well.</p>
"
"65255012","Workaround for oreplace in Teradata exceeding limit","<p>I’m trying to update a table using <strong>oreplace</strong> but it won’t let me do it saying it exceeds the length.
I have tried breaking it into multiple oreplace statements asin the first approach, and multiple update statements as the second approach but it still doesn’t work or give me the desired results.</p>
<p>Apparently oreplace can only return a maximum of 8000 characters? If yes, what is the solution/workaround for this?</p>
<p><strong>First</strong> approach:</p>
<pre><code>sel
oreplace (substr(text_val,1,8000),'CIM_OUTPUT','DD_CIM_OUTPUT')
|| oreplace (substr(text_val,8001,16000),'CIM_OUTPUT','DD_CIM_OUTPUT')
|| oreplace (substr(text_val,16001,18000),'CIM_OUTPUT','DD_CIM_OUTPUT')
from DB.TABLENAME ;
</code></pre>
<p><strong>Second</strong> approach:</p>
<p>The problem with this is that text_val is truncated after the first update and the rest of the updates do not apply on the complete value of text_Val thus rendering them as useles.</p>
<pre><code>update DB.TABLENAME set text_val = oreplace (substr(text_val,1,8000),'CIM_OUTPUT','DD_CIM_OUTPUT');

update DB.TABLENAME set text_val = text_val||oreplace (substr(text_val,8001,16000),'CIM_OUTPUT','DD_CIM_OUTPUT');

update DB.TABLENAME set text_val = text_val || oreplace (substr(text_val,16001,18000),'CIM_OUTPUT','DD_CIM_OUTPUT');
</code></pre>
","<sql><database><teradata><data-warehouse><teradatasql>","2020-12-11 16:23:52","743","1","1","65255438","<p>oReplace is limited to 8000 chars (maybe because it's based on Oracle). REGEXP_REPLACE has the same limit for VarChar input, but works on CLOBs, too. This this should work:</p>
<pre><code>SET TEXT_VAL=cast(regexp_replace(cast(text_Val as CLOB),'cim_output','DD_cim_ouput') as varchar(18000));
</code></pre>
"
"65238806","I got bigquery - DML cannot update view 'mydateset.myview'","<p>I would like to make a query on google bigquery and schedule it as a simple pipeline. What I've made is I first made a query with a couple of CTEs and save the view, then modify the query as</p>
<pre><code>INSERT INTO mydataset.myview
With t1 AS (...),
t2 AS (...),
...
big_t AS (...)
SELECT * FROM big_t t2
WHERE t2.date &gt; (SELECT MAX(date) FROM mydataset.myview)
</code></pre>
<p>but, when I click the <code>run_query</code> button, I got the prompt warning tells</p>
<p>DML cannot update view 'mydateset.myview'</p>
<p>I don't know why, because I think there is nothing to do with the syntax</p>
<p>is that to do with the way I save the view or something else?</p>
","<sql><google-bigquery><data-warehouse>","2020-12-10 16:49:16","347","0","3","65240075","<p>I figured out that it is about the table vs view manipulation. So, the INSERT/SELECT only works on table, when I change the query to be create table first the INSERT/SELECT, it works.</p>
"
"65238806","I got bigquery - DML cannot update view 'mydateset.myview'","<p>I would like to make a query on google bigquery and schedule it as a simple pipeline. What I've made is I first made a query with a couple of CTEs and save the view, then modify the query as</p>
<pre><code>INSERT INTO mydataset.myview
With t1 AS (...),
t2 AS (...),
...
big_t AS (...)
SELECT * FROM big_t t2
WHERE t2.date &gt; (SELECT MAX(date) FROM mydataset.myview)
</code></pre>
<p>but, when I click the <code>run_query</code> button, I got the prompt warning tells</p>
<p>DML cannot update view 'mydateset.myview'</p>
<p>I don't know why, because I think there is nothing to do with the syntax</p>
<p>is that to do with the way I save the view or something else?</p>
","<sql><google-bigquery><data-warehouse>","2020-12-10 16:49:16","347","0","3","65240986","<p>To update view you need to use <a href=""https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_view_statement"" rel=""nofollow noreferrer""><code>CREATE OR REPLACE VIEW</code></a> DDL</p>
"
"65238806","I got bigquery - DML cannot update view 'mydateset.myview'","<p>I would like to make a query on google bigquery and schedule it as a simple pipeline. What I've made is I first made a query with a couple of CTEs and save the view, then modify the query as</p>
<pre><code>INSERT INTO mydataset.myview
With t1 AS (...),
t2 AS (...),
...
big_t AS (...)
SELECT * FROM big_t t2
WHERE t2.date &gt; (SELECT MAX(date) FROM mydataset.myview)
</code></pre>
<p>but, when I click the <code>run_query</code> button, I got the prompt warning tells</p>
<p>DML cannot update view 'mydateset.myview'</p>
<p>I don't know why, because I think there is nothing to do with the syntax</p>
<p>is that to do with the way I save the view or something else?</p>
","<sql><google-bigquery><data-warehouse>","2020-12-10 16:49:16","347","0","3","65244852","<p>You can update data in a table only or a materialized view but not in a view.</p>
"
"65225551","Reorganizing Data in a data table","<p>I have data that is formatted this way when I pull it for the web data base.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">site</th>
<th style=""text-align: left;"">date</th>
<th style=""text-align: left;"">time</th>
<th style=""text-align: left;"">parameter_code</th>
<th style=""text-align: left;"">remark_code</th>
<th style=""text-align: left;"">result</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">001</td>
<td style=""text-align: left;"">01-01-2020</td>
<td style=""text-align: left;"">15:10</td>
<td style=""text-align: left;"">00010</td>
<td style=""text-align: left;"">NA</td>
<td style=""text-align: left;"">16.6</td>
</tr>
<tr>
<td style=""text-align: left;"">001</td>
<td style=""text-align: left;"">01-01-2020</td>
<td style=""text-align: left;"">15:10</td>
<td style=""text-align: left;"">00095</td>
<td style=""text-align: left;"">NA</td>
<td style=""text-align: left;"">521.0</td>
</tr>
<tr>
<td style=""text-align: left;"">001</td>
<td style=""text-align: left;"">01-01-2020</td>
<td style=""text-align: left;"">15:10</td>
<td style=""text-align: left;"">00300</td>
<td style=""text-align: left;"">NA</td>
<td style=""text-align: left;"">5.6</td>
</tr>
<tr>
<td style=""text-align: left;"">001</td>
<td style=""text-align: left;"">01-01-2020</td>
<td style=""text-align: left;"">15:10</td>
<td style=""text-align: left;"">34475</td>
<td style=""text-align: left;"">&lt;</td>
<td style=""text-align: left;"">1.0</td>
</tr>
<tr>
<td style=""text-align: left;"">001</td>
<td style=""text-align: left;"">03-30-2020</td>
<td style=""text-align: left;"">09:45</td>
<td style=""text-align: left;"">00010</td>
<td style=""text-align: left;"">NA</td>
<td style=""text-align: left;"">18.0</td>
</tr>
<tr>
<td style=""text-align: left;"">001</td>
<td style=""text-align: left;"">03-30-2020</td>
<td style=""text-align: left;"">09:45</td>
<td style=""text-align: left;"">00095</td>
<td style=""text-align: left;"">NA</td>
<td style=""text-align: left;"">546.0</td>
</tr>
<tr>
<td style=""text-align: left;"">001</td>
<td style=""text-align: left;"">03-30-2020</td>
<td style=""text-align: left;"">09:45</td>
<td style=""text-align: left;"">00300</td>
<td style=""text-align: left;"">NA</td>
<td style=""text-align: left;"">3.7</td>
</tr>
<tr>
<td style=""text-align: left;"">001</td>
<td style=""text-align: left;"">03-30-2020</td>
<td style=""text-align: left;"">09:45</td>
<td style=""text-align: left;"">34475</td>
<td style=""text-align: left;"">NA</td>
<td style=""text-align: left;"">2.3</td>
</tr>
</tbody>
</table>
</div>
<p>I want to format it as shown below.
I know that I need to combine the columns of Remark_code and Result first, but I'm not sure how to parse out the Parameter_code column into individual columns with the Result filling in the &quot;body&quot; of the table.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">site</th>
<th style=""text-align: left;"">date</th>
<th style=""text-align: left;"">time</th>
<th style=""text-align: left;"">00010</th>
<th style=""text-align: left;"">00095</th>
<th style=""text-align: left;"">00300</th>
<th style=""text-align: left;"">34475</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">001</td>
<td style=""text-align: left;"">01-01-2020</td>
<td style=""text-align: left;"">15:10</td>
<td style=""text-align: left;"">16.6</td>
<td style=""text-align: left;"">521.0</td>
<td style=""text-align: left;"">5.6</td>
<td style=""text-align: left;"">&lt;1.0</td>
</tr>
<tr>
<td style=""text-align: left;"">001</td>
<td style=""text-align: left;"">03-30-2020</td>
<td style=""text-align: left;"">15:10</td>
<td style=""text-align: left;"">18.0</td>
<td style=""text-align: left;"">546.0</td>
<td style=""text-align: left;"">3.7</td>
<td style=""text-align: left;"">2.3</td>
</tr>
</tbody>
</table>
</div>
<p>I don't necessarily need all of the code on how to do this, but just direction on what functions to use. I've been struggling with this since I'm not even sure what key words to look up or what this type of transformation(?) this would be called. Any help would be appreciated.</p>
","<r><dataframe><data-manipulation><data-management>","2020-12-09 22:00:16","73","1","2","65225586","<p>We can use <code>pivot_wider</code> after <code>unite</code>ing the columns 'REMARK_CODE' and 'RESULT'</p>
<pre><code>library(dplyr)
library(tidyr)
df1 %&gt;%
  unite(RESULT, REMARK_CODE, RESULT, sep=&quot;&quot;, na.rm = TRUE) %&gt;%
   pivot_wider(names_from = PARAMETER_CODE, values_from = RESULT)
</code></pre>
<p>-output</p>
<pre><code># A tibble: 2 x 7
#   SITE  DATE      TIME  `10`  `95`  `300` `34475`   
#   &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  
# 1 001   1/1/2020  15:10 16.6  521   5.6   &lt;1     
#   2 001   3/30/2020 9:45  18    546   3.7   2.3    
</code></pre>
<h3>data</h3>
<pre><code>df1 &lt;- structure(list(SITE = c(&quot;001&quot;, &quot;001&quot;, &quot;001&quot;, &quot;001&quot;, &quot;001&quot;, &quot;001&quot;, 
&quot;001&quot;, &quot;001&quot;), DATE = c(&quot;1/1/2020&quot;, &quot;1/1/2020&quot;, &quot;1/1/2020&quot;, &quot;1/1/2020&quot;, 
&quot;3/30/2020&quot;, &quot;3/30/2020&quot;, &quot;3/30/2020&quot;, &quot;3/30/2020&quot;), TIME = c(&quot;15:10&quot;, 
&quot;15:10&quot;, &quot;15:10&quot;, &quot;15:10&quot;, &quot;9:45&quot;, &quot;9:45&quot;, &quot;9:45&quot;, &quot;9:45&quot;), PARAMETER_CODE = c(10L, 
95L, 300L, 34475L, 10L, 95L, 300L, 34475L), REMARK_CODE = c(NA, 
NA, NA, &quot;&lt;&quot;, NA, NA, NA, NA), RESULT = c(16.6, 521, 5.6, 1, 18, 
546, 3.7, 2.3)), row.names = c(NA, -8L), class = &quot;data.frame&quot;)
</code></pre>
"
"65225551","Reorganizing Data in a data table","<p>I have data that is formatted this way when I pull it for the web data base.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">site</th>
<th style=""text-align: left;"">date</th>
<th style=""text-align: left;"">time</th>
<th style=""text-align: left;"">parameter_code</th>
<th style=""text-align: left;"">remark_code</th>
<th style=""text-align: left;"">result</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">001</td>
<td style=""text-align: left;"">01-01-2020</td>
<td style=""text-align: left;"">15:10</td>
<td style=""text-align: left;"">00010</td>
<td style=""text-align: left;"">NA</td>
<td style=""text-align: left;"">16.6</td>
</tr>
<tr>
<td style=""text-align: left;"">001</td>
<td style=""text-align: left;"">01-01-2020</td>
<td style=""text-align: left;"">15:10</td>
<td style=""text-align: left;"">00095</td>
<td style=""text-align: left;"">NA</td>
<td style=""text-align: left;"">521.0</td>
</tr>
<tr>
<td style=""text-align: left;"">001</td>
<td style=""text-align: left;"">01-01-2020</td>
<td style=""text-align: left;"">15:10</td>
<td style=""text-align: left;"">00300</td>
<td style=""text-align: left;"">NA</td>
<td style=""text-align: left;"">5.6</td>
</tr>
<tr>
<td style=""text-align: left;"">001</td>
<td style=""text-align: left;"">01-01-2020</td>
<td style=""text-align: left;"">15:10</td>
<td style=""text-align: left;"">34475</td>
<td style=""text-align: left;"">&lt;</td>
<td style=""text-align: left;"">1.0</td>
</tr>
<tr>
<td style=""text-align: left;"">001</td>
<td style=""text-align: left;"">03-30-2020</td>
<td style=""text-align: left;"">09:45</td>
<td style=""text-align: left;"">00010</td>
<td style=""text-align: left;"">NA</td>
<td style=""text-align: left;"">18.0</td>
</tr>
<tr>
<td style=""text-align: left;"">001</td>
<td style=""text-align: left;"">03-30-2020</td>
<td style=""text-align: left;"">09:45</td>
<td style=""text-align: left;"">00095</td>
<td style=""text-align: left;"">NA</td>
<td style=""text-align: left;"">546.0</td>
</tr>
<tr>
<td style=""text-align: left;"">001</td>
<td style=""text-align: left;"">03-30-2020</td>
<td style=""text-align: left;"">09:45</td>
<td style=""text-align: left;"">00300</td>
<td style=""text-align: left;"">NA</td>
<td style=""text-align: left;"">3.7</td>
</tr>
<tr>
<td style=""text-align: left;"">001</td>
<td style=""text-align: left;"">03-30-2020</td>
<td style=""text-align: left;"">09:45</td>
<td style=""text-align: left;"">34475</td>
<td style=""text-align: left;"">NA</td>
<td style=""text-align: left;"">2.3</td>
</tr>
</tbody>
</table>
</div>
<p>I want to format it as shown below.
I know that I need to combine the columns of Remark_code and Result first, but I'm not sure how to parse out the Parameter_code column into individual columns with the Result filling in the &quot;body&quot; of the table.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">site</th>
<th style=""text-align: left;"">date</th>
<th style=""text-align: left;"">time</th>
<th style=""text-align: left;"">00010</th>
<th style=""text-align: left;"">00095</th>
<th style=""text-align: left;"">00300</th>
<th style=""text-align: left;"">34475</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">001</td>
<td style=""text-align: left;"">01-01-2020</td>
<td style=""text-align: left;"">15:10</td>
<td style=""text-align: left;"">16.6</td>
<td style=""text-align: left;"">521.0</td>
<td style=""text-align: left;"">5.6</td>
<td style=""text-align: left;"">&lt;1.0</td>
</tr>
<tr>
<td style=""text-align: left;"">001</td>
<td style=""text-align: left;"">03-30-2020</td>
<td style=""text-align: left;"">15:10</td>
<td style=""text-align: left;"">18.0</td>
<td style=""text-align: left;"">546.0</td>
<td style=""text-align: left;"">3.7</td>
<td style=""text-align: left;"">2.3</td>
</tr>
</tbody>
</table>
</div>
<p>I don't necessarily need all of the code on how to do this, but just direction on what functions to use. I've been struggling with this since I'm not even sure what key words to look up or what this type of transformation(?) this would be called. Any help would be appreciated.</p>
","<r><dataframe><data-manipulation><data-management>","2020-12-09 22:00:16","73","1","2","65225668","<p>Another approach can be:</p>
<pre><code>library(dplyr)
library(tidyr)
#Code
new &lt;- df %&gt;% mutate(RESULT=ifelse(is.na(REMARK_CODE),paste0('',RESULT),
                            paste0(REMARK_CODE,RESULT))) %&gt;%
  select(-REMARK_CODE) %&gt;%
  pivot_wider(names_from = PARAMETER_CODE,values_from=RESULT)
</code></pre>
<p>Output:</p>
<pre><code># A tibble: 2 x 7
  SITE  DATE      TIME  `10`  `95`  `300` `34475`
  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  
1 001   1/1/2020  15:10 16.6  521   5.6   &lt;1     
2 001   3/30/2020 9:45  18    546   3.7   2.3    
</code></pre>
<p>Some data used:</p>
<pre><code>#Data
df &lt;- structure(list(SITE = c(&quot;001&quot;, &quot;001&quot;, &quot;001&quot;, &quot;001&quot;, &quot;001&quot;, &quot;001&quot;, 
&quot;001&quot;, &quot;001&quot;), DATE = c(&quot;1/1/2020&quot;, &quot;1/1/2020&quot;, &quot;1/1/2020&quot;, &quot;1/1/2020&quot;, 
&quot;3/30/2020&quot;, &quot;3/30/2020&quot;, &quot;3/30/2020&quot;, &quot;3/30/2020&quot;), TIME = c(&quot;15:10&quot;, 
&quot;15:10&quot;, &quot;15:10&quot;, &quot;15:10&quot;, &quot;9:45&quot;, &quot;9:45&quot;, &quot;9:45&quot;, &quot;9:45&quot;), PARAMETER_CODE = c(10L, 
95L, 300L, 34475L, 10L, 95L, 300L, 34475L), REMARK_CODE = c(NA, 
NA, NA, &quot;&lt;&quot;, NA, NA, NA, NA), RESULT = c(16.6, 521, 5.6, 1, 18, 
546, 3.7, 2.3)), row.names = c(NA, -8L), class = &quot;data.frame&quot;)
</code></pre>
"
"65173777","What's the use of a dimension that is a subset of a fact table?","<p>In the book: 'Hands-On SQL Server 2019 Analysis Services'; the author presents this model.</p>
<p>In the center I see Sales and InvoiceSales as fact tables... My question is regarding the Invoice dimension, it only has 2 columns which are already present in InvoiceSales, why did he add it?</p>
<p><a href=""https://i.stack.imgur.com/dv5ew.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dv5ew.png"" alt=""enter image description here"" /></a></p>
<p>note: the InvoiceSales fact table has the InvoiceDateKey column.</p>
","<data-warehouse><business-intelligence><dimensional-modeling>","2020-12-06 22:33:09","207","1","1","65775004","<p>This may be a business need as a <code>Snapshot Fact Tables</code>:</p>
<blockquote>
<p>Snapshot Fact Tables</p>
<p>(Periodic) Snapshot fact tables capture the state of the measures
based on the occurrence of a status event or at a specified
point-in-time or over specified time intervals (week, month, quarter,
year, etc.).</p>
</blockquote>
"
"65138766","Data Warehouse Fact Tables - MERGE OR DROP & INSERT","<p>I am building specific Fact Table for a Kimball-based EDW.</p>
<p>While loading fact table I came upon the realization on the different approaches.</p>
<p>Below the steps taken:</p>
<p><strong>The Source Data</strong></p>
<ul>
<li><p>Learner Absenteeism data.</p>
</li>
<li><p>Learners are sometimes absent and these absenteeism's are capture again a learner.</p>
</li>
<li><p>It can happen that the reason for absenteeism can change a few days later.</p>
</li>
<li><p>It can also happen that the record is completely deleted a few days later.</p>
</li>
<li><p>Source system has no form of Change Control features to indicate changed/deleted records.</p>
</li>
</ul>
<p><strong>Sourcing Data</strong></p>
<ul>
<li><p>When sourcing the data only the current year's data is sourced.</p>
</li>
<li><p>And no other data is kept in stage.</p>
</li>
</ul>
<p><strong>Lookup Surrogates</strong></p>
<ul>
<li>Lookups are performed to replace natural keys with surrogate keys of the relating dimensions.</li>
</ul>
<p><strong>Load Data in Fact Table</strong></p>
<ul>
<li>So now a clean conformed data set exist that need to to be loaded into the final Fact Table</li>
</ul>
<p>Option One - Fresh Batch</p>
<pre><code>DELETE FROM FactLearnerAbsenteeism WHERE SchoolYear = @CurrentYear

INSERT INTO FactLearnerAbsenteeism SELECT * FROM #Conform
</code></pre>
<p>Option Two - The Merge</p>
<pre><code>MERGE dbo.FactLearnerAbsenteeism as DST

USING #Conform as SRC

*UPDATE CHANGES*

*INSERT NEW RECORDS*
</code></pre>
<p><strong>My concern is:</strong> How I can i ensure records that no longer exist this year (deleted records) are removed. I prefer using the merge but using the DELETE option in MERGE can remove records from the previous years as they wont exist in the #Conform dataset.</p>
<p>Should I the rather source ALL data?
Should I keep the previous years data in STAGE and only source the current years data?</p>
","<merge><data-warehouse><fact-table>","2020-12-04 06:10:48","465","0","1","73442395","<p>I would propose that you add a technical column &quot;IsValid&quot; in your fact table, set by default to 0. Then your loading process could be as follows :</p>
<p><strong>Step 1</strong>
UPDATE FactLearnerAbsenteism SET IsValid = 0 WHERE SchoolYear = @CurrentYear</p>
<p><strong>Step 2</strong>
Perform the Merge, and set Isvalid to 1 for each updated or inserted record.</p>
<p><strong>Step 3</strong>
Decide what you want to do with the invalidated records : either you delete them in a last pass, or you keep them as it can represent an interesting piece of information enabling you to track the deletions in the source systems.</p>
<p>Disadvantage is that analysts need to be aware of this isValid parameter in their queries, to avoid overestimating the absentees, but I tend to prefer this as it captures the maximum information.</p>
"
"65138693","What's wrong with this simple SQL-based ETL solution?","<p>We use Informatica PowerCenter to move data mostly from other relational databases into our data warehouse. I've come
up with a simple algorithm to move data into a star schema or any materialized view using change data capture, and
would like to know what's wrong with it so I can better understand the appeal of tools such as Informatica.</p>
<p>Assumptions:</p>
<ul>
<li>We have a CDC message channel through which individual insert/update/delete events arrive.</li>
<li>We've defined the materialized view of interest as an aggregate SQL query against the source tables. In other words, it's
a SQL query that contains a GROUP BY expression. Therefore, the materialized view is guaranteed to have a consistent
granularity, and the contents of the GROUP BY expression can be treated as its primary key.</li>
</ul>
<p>The algorithm:</p>
<ul>
<li><p>A CDC message comes over the channel. A source table row was changed:</p>
<p>We loop through any queries that depend on this table. For every query, we determine whether the table is 'primary'
or 'non-primary' for this particular query - e.g. whether a value from this table was used in the query's GROUP BY
expression. In other words, we determine whether the PK of the materialized view depends on this table directly.</p>
<ul>
<li>A primary table was changed:
<ul>
<li><p>A primary table row was inserted with the value P=p:</p>
<ul>
<li>Insert the raw row into our mirror of the source table.</li>
<li>Run the query of interest with the filter WHERE P=p and find the affected rows.</li>
<li>Insert the affected rows into our materialized view table.</li>
</ul>
</li>
<li><p>A primary table row was updated with the value P=p1 changing to P=p2:</p>
<p>We split this into two messages: DROP P=p1 (see below) and INSERT P=p2 (see above).</p>
</li>
<li><p>A primary table row was with the value P=p was deleted:</p>
<ul>
<li>Run the query of interest with selecting just the PK values and the filter WHERE P=p to find the
affected PK values (pk1, pk2, ..., pkn).</li>
<li>Drop the raw row of our mirror of the source table.</li>
<li>Drop the rows WHERE PK IN (pk1, pk2, ..., pkn) from the materialized view table.</li>
</ul>
</li>
</ul>
</li>
<li>A non-primary table was changed:
<ul>
<li><p>A non-primary table row was inserted with the relevant value V=v:</p>
<ul>
<li>Insert the raw row into our mirror of the source table.</li>
<li>Run the query of interest with the filter WHERE V=v and find the affected PK values (pk1, pk2, ..., pkn).</li>
<li>Run the query of interest with the filter WHERE PK in (pk1, pk2, ..., pkn) and update these rows
in the materialized view table.</li>
</ul>
</li>
<li><p>A non-primary table row was updated with the relevant value changed from V=v1 to V=v2:</p>
<p>We split this into two messages: DROP V=v1 (see below) and INSERT V=v2 (see above).</p>
</li>
<li><p>A non-primary table row with the relevant value V=v was deleted:</p>
<ul>
<li>Run the query of interest with selecting just the PK values and the filter WHERE V=v to find the
affected PK values (pk1, pk2, ..., pkn).</li>
<li>Drop the raw row from our mirror of the source table.</li>
<li>Run the query of interest with the filter WHERE PK in (pk1, pk2, ..., pkn) and update these rows
in the materialized view table.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Making it even better:</p>
<ul>
<li>Any updates of the materialized view table can themselves be converted into CDC messages. This would allow for
recursive updates to materialized views that depend on other materialized views.</li>
</ul>
<p>If such a solution could actually work and be fast and scalable, it could solve 80% of our ETL problems. However,
I can't be the first person to think of such a naive approach, so please do tell me why this can't work.</p>
","<relational-database><etl><data-warehouse>","2020-12-04 06:02:07","68","0","1","65223937","<p>For anyone looking at this in the future, <a href=""https://github.com/timelydataflow/differential-dataflow/"" rel=""nofollow noreferrer"">differential dataflow</a> is a much better solution than what I posted. There's an open-source SQL-like implementation of differential dataflow called <a href=""https://github.com/MaterializeInc/materialize"" rel=""nofollow noreferrer"">Materialize</a>.</p>
"
"65131019","How data is fetched from all shards in Elastic search","<p>I have 4 nodes,3 shards and 3 replicas in Elastic search cluster. I understands that data is split and stored into the shards. <strong>When query is requested, How data is fetched from all three shards?</strong></p>
<h2>Storage Structure :</h2>
<p>Node 1: Shard1(Primary), Shard2(replica),Shard3(replica) <br>
Node 2: Shard2(Primary), Shard1(replica)<br>
Node 3: Shard3(Primary), Shard1(replica)<br>
Node 4: Shard2(replica), Shard3(replica)<br></p>
<p>Thanks,<br>
Kumar.</p>
","<elasticsearch><search><parallel-processing><data-warehouse>","2020-12-03 17:26:50","614","2","1","65136887","<p>For search query, data can be searched in a primary shard or its replica shard, as Elastic never assigns the replica shard of a primary on the same data node, replica improves the high-availability also as replica contains the copy so data can be searched from replica shard as well.</p>
<p>But as your data is divided into 3 primary shards, so for search query at least it needs to search 3 shards(primary or primary's shard).</p>
"
"65070016","How to deal with historicization data in a data lake vs data warehouse?","<p>It is possible (or even a core functionality) having data historicized within a classic data warehouse. Data will be added to the data warehouse over time and it is possible to move in time over the data.</p>
<p>If I just want to use the data lake and to have also data historicization for the business user, would this be possible? And if yes, how would a possible approach look like?</p>
","<data-warehouse><data-lake>","2020-11-30 08:15:14","289","0","2","65073785","<p>Yes - you can do it. If you just do inserts of data then you will have, by default, a full history of all your data.</p>
<p>The possible approaches would be entirely dependent on the technology you were running to support your data lake, how you have structured your data in the data lake, the tools your business users were using to access the data, etc. So without much more information from you it's not possible to give you an answer - other than the generic &quot;yes, it is possible to hold historic data in a data lake&quot;</p>
"
"65070016","How to deal with historicization data in a data lake vs data warehouse?","<p>It is possible (or even a core functionality) having data historicized within a classic data warehouse. Data will be added to the data warehouse over time and it is possible to move in time over the data.</p>
<p>If I just want to use the data lake and to have also data historicization for the business user, would this be possible? And if yes, how would a possible approach look like?</p>
","<data-warehouse><data-lake>","2020-11-30 08:15:14","289","0","2","65075541","<p>Your classic data warehouse will bring data together, modelled with time series at the centre.</p>
<p>Data lakes hold the raw data in the original format, which typically will not be stored with time series in mind.  You are able to store your data so that the time series and historical changes can be worked out, but a data lake will be missing the pre modelled, easily accessible time series aspect of a data warehouse.</p>
"
"65033677","Define Data Quality Rules for Big Data","<p>Is there any way to define Data quality rules that can be applied over Dataframes.
The template to define the rule should be easy enough for any lay man to define and then we can take these rules and convert them to pyspark codes and run them over the data.</p>
<p>I was thinking in line as below.</p>
<pre><code>ID  ProjectID   RuleID  Attribute1  Value1          Condition1  Attribute2  Value2          Condition2  Type    ModifyAttribute ModificationLogic   CustomUDF
1   1           1       SerialNum   6               EQUAL                                               MODIFY  SerialNum   SUBSTR(serialNum,1,6)   
2   1           2       DriverName  ['A','B','C']   VALUEMATCH  Source      ['D','E','F']   IN          REJECT  
</code></pre>
<p>If there is any tools or Domain specific language to define the same it would help.
If there is any template to define rules which can be applied cross attribute and across multiple tables (join, example country lookup) is also helpful.</p>
","<validation><pyspark><data-quality>","2020-11-27 07:47:25","209","1","1","65662430","<p>Surprised no one gave a shot at answering this yet. Typically, for a use case like this, I would use <a href=""https://docs.python.org/3/library/configparser.html"" rel=""nofollow noreferrer"">ConfigParser</a>. Based on what your architecture is, you can define sections and rules which can easily be read and executed. But that's something a developer would find easy to use rather than a normal user.</p>
<p>Now that's out of the way, for your use case, as python is a scripting language with a lot of flexibility, you can simply create an excel in the format you have given which will dictate the flow of your data manipulation. I hope this helps in some way. Let me know if you need more info.</p>
"
"64993317","Table Design for Data with multiple Granularity","<p>I have been trying to understand data modelling and warehousing, Given the best practices of having only one type of granularity in the table can I have one table store both the low granular data with the aggregate data.</p>
<p>2 Table Structure</p>
<p>Table 1</p>
<pre><code>TransactionID   Transaction_Dt  ProductID   Items   Cost_Total
11111           1/1/2020        1           10      100
11111           1/1/2020        2           5       200
11111           1/1/2020        3           4       400
11111           1/1/2020        4           5       100
11111           1/1/2020        5           6       600
11111           1/1/2020        6           10      100
</code></pre>
<p>Table 2 (Aggregated)</p>
<pre><code>TransactionID   Transaction_Dt  Total_Items Cost_Total
11111           1/1/2020        40          1500
</code></pre>
<p>One table Structure</p>
<p>Aggregate Data in the table</p>
<pre><code>TransactionID   Transaction_Dt  ProductID   Items   Cost_Total  Type
11111           1/1/2020        1           10      100         ind_Item
11111           1/1/2020        2           5       200         ind_Item
11111           1/1/2020        3           4       400         ind_Item
11111           1/1/2020        4           5       100         ind_Item
11111           1/1/2020        5           6       600         ind_Item
11111           1/1/2020        6           10      100         ind_Item
**11111         1/1/2020        ALL         40      1500        all_Item**
</code></pre>
<p>Here we have one record for the entire transaction with the sum of all items and sum of all cost.</p>
<p>Can anyone help me on the Cons of the 2nd approach where we have aggregated data in the same table</p>
","<data-modeling><data-warehouse>","2020-11-24 19:12:41","441","1","1","65013452","<p>Some thoughts about this:</p>
<ol>
<li>I am not a fan of storing data at multiple levels of aggregation in one table for the reason that @Marmite Bomber suggested - if you do a select sum and don't filter out the aggregates, you'll get a multiple of the answer you're looking for.</li>
<li>If you do still want to put everything into one table, I'd add another column, perhaps called agg_level, and indicate the aggregation level of the row in that table. (you are already kind of doing this with your 'Type' column although Type is a very ambiguous term).</li>
<li>I'd recommend against changing the TransactionID value (you propose adding some asterisks to indicate that it's an aggregate). Modifying it will make it harder to search for the one you want and users will have to understand your notation to get the right records. If you do add an agg_level column and leave the TransactionIDs in their original form, you could put an easily recognizable term in the agg_level column. For example, a record could say &quot;raw&quot;, or &quot;Transaction Total&quot;, or &quot;Monthly Aggregate&quot;...</li>
<li>If you have to put your aggregates into your base data table, like you've shown, you should consider creating views on top of the table, each view providing only detail at one level of aggregation. You likely would give users access only to these views, and not to the base data. In this way, you store everyone in one table but, to users, it looks like you have multiple tables and you needn't worry about a user accidentally misforming a query that brings back duplicate totals.</li>
</ol>
<p>It's a good question, Snehasish, and it shows that you've been giving it some thought. Best of luck as you navigate the need going forward!</p>
"
"64992798","Cannot create a new dimension","<p>I have problems with creating a new dimension in Visual Studio. I want to use an existing table. After setting main table, I can't click next. Also, name columns option is blocked. What should I do to succesfully create a new dimension?</p>
<p><a href=""https://i.stack.imgur.com/EafrZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EafrZ.png"" alt=""enter image description here"" /></a></p>
","<visual-studio-2019><data-warehouse><business-intelligence><analysis><wizard>","2020-11-24 18:37:34","104","0","1","65791490","<p>It's a bug. As I recall, I clicked on main table or data source view once again, and then next option was enabled.</p>
"
"64973659","Where is a dimension structure tab in Visual Studio?","<p>Where can I find a dimension structure tab in Microsoft Visual Studio 2019? Is there a shortcut for it?</p>
","<structure><visual-studio-2019><data-warehouse><business-intelligence><dimension>","2020-11-23 17:52:45","120","-1","1","64974006","<p>I think I found an answer. You just have to simply double click on one of your created dimensions in the solution explorer.</p>
"
"64943373","DWH: Why build cubes, when you already have a DB star schema?","<p>my etl process collects and transforms data into a DB base with facts and dimensions. so why should I build cubes out of this? Is there more than just a the speed benefit of queries and the pre aggregiation of values?</p>
<p>Thank you for helping</p>
","<database><data-warehouse>","2020-11-21 13:04:30","111","1","1","64953737","<p>Facts and dimensions are tables that can be built in almost any relational database. To improve performance you can build aggregate fact tables in the same relational database. These tables are generic in that, with very little effort, you could move these tables from Oracle to SQL Server, as an example.</p>
<p>At the risk of over simplifying, a cube is a type of aggregate fact table but is built in a multi-dimensional database and is, normally, specific to that flavour of database. So if you build a cube in SSAS you couldn't move it to Hyperion Essbase.</p>
<p>For a simple query, such as sum transaction amount by date, cubes would not give you much/any benefit over facts. For complex queries, the performance tends to be significantly better than with facts.</p>
<p>Cubes normally support their own query language (e.g. SSAS and DAX) that allow much more complex queries than can normally be written in SQL (without a lot of effort)</p>
<p>So whether you should build cubes depends on a lot of factors, such as:</p>
<ul>
<li>Are you running a lot of complex queries that would perform better in a cube?</li>
<li>Would the improved performance be worth the effort/cost involved?</li>
<li>Is there a cost/benefit case for deploying a MOLAP (cube) database as well as your dimensional database? Cubes are normally populated from facts/dimensions so they are an addition to, rather than a replacement for, facts/dimensions</li>
</ul>
"
"64916806","What is the difference between data modelling and dimensional modelling?","<p>I've been learning about Data warehousing concepts and I found these 2 topics little confusing. I've read multiple blog posts and I understood that data modelling consists of three steps</p>
<ol>
<li>Conceptual Data Model</li>
<li>Logical Data Model</li>
<li>Physical Data Model</li>
</ol>
<p>and in data warehousing we need to perform certain steps:</p>
<p>Step 1: Identify the dimensions</p>
<p>Step 2: Identify the measures</p>
<p>Step 3: Identify the attributes or properties of dimensions</p>
<p>Step 4: Identify the granularity of the measures</p>
<p>Are these modelling techniques related to each other? If yes, how are this related.
If someone asks, how to design a data warehouse, what should be the correct answer. Where does these modelling techniques comes in while designing a data warehouse.</p>
<p>It would be really helpful, if someone could provide me any link/resource about data modelling and dimensional modelling scenarios.</p>
","<database><data-modeling><data-warehouse><olap><dimensional-modeling>","2020-11-19 17:23:00","570","0","1","64928587","<p>As the name suggests, a conceptual model is very high level and does not correspond directly to what actually gets built. Logical/physical models do correspond to what you are actually going to build - the difference between the two is that a logical model is system-independent while a physical model is tied to the platform/DB where it is going to be deployed. However they are fundamentally identical in that most modelling tools can automatically generate a physical model from a logical one (and vice versa).</p>
<p>A dimensional model is a type of logical/physical model, in the same way that OLTP, Inmon, Data Vault, etc. are types of logical/physical model. There are normally best practices defined for the steps required to design each of these model types - and you have listed the steps specific to designing a Dimensional model.</p>
<p>So for a given data domain (e.g. a Sales organisation), you would normally have a single Conceptual model and then multiple logical/physical models. Usually these would be one transactional model and one analytical model; the transactional model could be OLTP or NoSQL (or whatever suits your requirements/technology the best); the analytical model could be Dimensional, Inmon, Graph, etc. - again whatever suits your data/analytical requirements the best.</p>
"
"64910831","If columns match, take value from another column and assign it to a new variable","<p>I have a data set that contains a group an id variable and a pair id variable. Each observation is paired with another observation. I want to create a new variable that takes the id variable of its pair-partner and assign it as a new 'partner' variable.</p>
<p>Below I have an example data:</p>
<pre><code>id &lt;- 1:10
pair_id &lt;- rep(1:5,2)

df &lt;- cbind(id, pair_id)

&gt; df
      id pair_id
 [1,]  1       1
 [2,]  2       2
 [3,]  3       3
 [4,]  4       4
 [5,]  5       5
 [6,]  6       1
 [7,]  7       2
 [8,]  8       3
 [9,]  9       4
[10,] 10       5
</code></pre>
<p>As I said above, I want to add a variable indicating on each <code>id</code> partner's <code>id</code>, where the partnership is identified with the <code>pair_id</code>. As an example, observations with <code>id == 1</code> has <code>pair_id == 1</code>, which makes it partner to be the observation with <code>id == 6</code>, since they share <code> pair_id</code>.</p>
<p>So the end result should look like this:</p>
<pre><code>     id pair_id partner_id
 [1,]  1       1          6
 [2,]  2       2          7
 [3,]  3       3          8
 [4,]  4       4          9
 [5,]  5       5         10
 [6,]  6       1          1
 [7,]  7       2          2
 [8,]  8       3          3
 [9,]  9       4          4
[10,] 10       5          5
</code></pre>
<p>Thanks!</p>
","<r><dplyr><tidyverse><data-management>","2020-11-19 11:32:54","174","0","2","64911091","<p>You can reverse the <code>id</code> values for each <code>pair_id</code> :</p>
<pre><code>library(dplyr)
df %&gt;% group_by(pair_id) %&gt;% mutate(partner_id = rev(id))

#      id pair_id partner_id
#   &lt;int&gt;   &lt;int&gt;      &lt;int&gt;
# 1     1       1          6
# 2     2       2          7
# 3     3       3          8
# 4     4       4          9
# 5     5       5         10
# 6     6       1          1
# 7     7       2          2
# 8     8       3          3
# 9     9       4          4
#10    10       5          5
</code></pre>
<p>The equivalent in base R :</p>
<pre><code>df$partner_id &lt;- with(df, ave(id, pair_id, FUN = rev))
</code></pre>
<p>and in <code>data.table</code> is</p>
<pre><code>library(data.table)
setDT(df)[, partner_id := rev(id), pair_id]
</code></pre>
<p><strong>data</strong></p>
<pre><code>df &lt;- data.frame(id, pair_id)
</code></pre>
"
"64910831","If columns match, take value from another column and assign it to a new variable","<p>I have a data set that contains a group an id variable and a pair id variable. Each observation is paired with another observation. I want to create a new variable that takes the id variable of its pair-partner and assign it as a new 'partner' variable.</p>
<p>Below I have an example data:</p>
<pre><code>id &lt;- 1:10
pair_id &lt;- rep(1:5,2)

df &lt;- cbind(id, pair_id)

&gt; df
      id pair_id
 [1,]  1       1
 [2,]  2       2
 [3,]  3       3
 [4,]  4       4
 [5,]  5       5
 [6,]  6       1
 [7,]  7       2
 [8,]  8       3
 [9,]  9       4
[10,] 10       5
</code></pre>
<p>As I said above, I want to add a variable indicating on each <code>id</code> partner's <code>id</code>, where the partnership is identified with the <code>pair_id</code>. As an example, observations with <code>id == 1</code> has <code>pair_id == 1</code>, which makes it partner to be the observation with <code>id == 6</code>, since they share <code> pair_id</code>.</p>
<p>So the end result should look like this:</p>
<pre><code>     id pair_id partner_id
 [1,]  1       1          6
 [2,]  2       2          7
 [3,]  3       3          8
 [4,]  4       4          9
 [5,]  5       5         10
 [6,]  6       1          1
 [7,]  7       2          2
 [8,]  8       3          3
 [9,]  9       4          4
[10,] 10       5          5
</code></pre>
<p>Thanks!</p>
","<r><dplyr><tidyverse><data-management>","2020-11-19 11:32:54","174","0","2","64921254","<p>We can use</p>
<pre><code> library(dplyr)
df %&gt;%
      group_by(pair_id) %&gt;%
      mutate(partner_id = id[n():1])
</code></pre>
"
"64844955","Handling Contracts extension and licensing/Subscriptions addition/removal in dimensional model","<p>Background: I am trying to design a star schema for a data warehouse. We have the following business model where we have few products that our customers can buy and then use. The customers are companies and then they have people in their organization who can be mapped to the licenses they have brought for products.</p>
<p>I have the following dimensions.</p>
<p>Account_dim: The dimension contains all the list of companies that have are our current/prospective with our company. It could have companies who still don't have a contract with us and are still in a discussion phase. so some rows might not have a contract.</p>
<p>User_dim: This is the list of users the company has nominated point of contacts for their company. So a user will belong to one particular Account in the Account_dim. One account can have many users.</p>
<p>Product_Dim: This dimension contains all the information regarding all the products we sell. The cost of a license and how many users are allowed on a license.So if for example he brought product A a max of two users can use it.</p>
<p>Now I have three tables that have data regarding the contract.</p>
<p>Contract: It contains information regarding a contract we have which will include the contract start date and end date and the account which this contract is assigned to.</p>
<p>products_bought: This table contains the product brought under a contract. A contract can hold multiple products bought.Each product row will have the product start date/end date and the price of the asset the client has paid.</p>
<p>allocated users:Each product bought can have users mapped to it who are allowed to use the product which is the user in user_dim for that account. Basically attaching a license to a user.</p>
<p>I am trying to model the contract, product bought and allocated user so I can generate the following data.</p>
<ol>
<li>The amount of money a account has spend on products.</li>
<li>THe utilization of licenses by an account. for example an account has a product that allows 3 users but has only one user mapped to it will show the product is under utilized.</li>
</ol>
<p>I tried denormalizing all three tables into one fact table but the I am running into problem where the contract end date can be changed if it is extended. As well as new assets can be mapped to it. Also last be not least, the company can remove a user and then map another user to the product or remove users because they left the company or add more users.</p>
<p>How can this be best modeled. Because they contract and asset users can change they should be SCDs rather than fact table or how should I implement a fact to handle these changes as well which must be captured as well to maintain history of usage over time.</p>
","<data-warehouse><dimensional-modeling><star-schema><star-schema-datawarehouse>","2020-11-15 13:28:06","73","-3","1","64858246","<p>your best bet is to read a book on how to go about designing a data warehouse: <a href=""https://www.amazon.co.uk/Data-Warehouse-Lifecycle-Toolkit-2nd/dp/0470149779/ref=sr_1_4?dchild=1&amp;keywords=ralph%20kimball&amp;qid=1605529380&amp;sr=8-4"" rel=""nofollow noreferrer"">The Data Warehouse Lifecycle Toolkit</a> as this will give you all the information you need to be able to answer questions like this.</p>
<p>However, to specifically address your question, the best way to approach this is as follows:</p>
<ol>
<li>Define your measures: what are the values that you wish to be able to aggregate in your reports</li>
<li>Define the grain of each measure: what are the dimensions that uniquely identify each measure. For example, a transaction amount might be defined by Store, Customer and Date/Time; if you dropped any of these then the transaction amount would change; if you added another dimension, such as rainfall, it would not change the transaction amount (n.b. having defined the grain of a measure you should never add dimensions that would change the grain e.g. Product Dimension, in this example)</li>
</ol>
<p>Once you have defined your measures and their grains you can add all the other dimensions to them (that won't affect their grain) andn then decide whether to hold them in separate fact tables or combine them into one fact table:</p>
<ol>
<li>Rule: if two measures don't have the same grain you must not put them in the same fact table</li>
<li>Guidance: for measures that meet the above rule, if there is also significant overlap in the other dimensions you want to use for each measure then consider combining them into a single fact table. My rule of thumb is that if you have 2-3 dimensions that don't apply to all measures then that's OK; if you hit 5 or more then you probably need to be thinking of splitting the measures into separate facts</li>
</ol>
"
"64827627","How to take data from 2 database and move it to a data warehouse using SSIS?","<p>I am currently studying databases, and I have a question.
The professor told us to create 2 different databases, and then move all the data to a star schema data model.</p>
<p>Here is the diagram for the first database, I have already filled the tables with data.
<img src=""https://i.stack.imgur.com/y0fO1.png"" alt=""Here is the diagram for the first database, i have already filled the tables with data."" /></p>
<p>This is the diagram for the second database, also with data</p>
<p><img src=""https://i.stack.imgur.com/cVvSX.png"" alt=""This is the diagram for the second database, also with data."" /></p>
<p>This is my star schema model
<img src=""https://i.stack.imgur.com/qeibo.png"" alt=""And finally the star schema data model"" /></p>
<p>The problem I am facing is i that i do not know how to start doing the mapping when adding my origin OLE DB and destination OLE DB.</p>
<p>I have already searched through the web, but I only find examples where they have to move just one database to the star schema data model.</p>
","<ssis><etl><data-warehouse>","2020-11-13 20:27:57","302","1","2","64828475","<p>the standard approach would be to do the following:</p>
<ol>
<li>Copy your source data into staging tables in your target database</li>
<li>Write the queries, necessary to populate the star schema, against the staging tables</li>
</ol>
<p>You populate all your dimension tables first and then all your fact tables.</p>
"
"64827627","How to take data from 2 database and move it to a data warehouse using SSIS?","<p>I am currently studying databases, and I have a question.
The professor told us to create 2 different databases, and then move all the data to a star schema data model.</p>
<p>Here is the diagram for the first database, I have already filled the tables with data.
<img src=""https://i.stack.imgur.com/y0fO1.png"" alt=""Here is the diagram for the first database, i have already filled the tables with data."" /></p>
<p>This is the diagram for the second database, also with data</p>
<p><img src=""https://i.stack.imgur.com/cVvSX.png"" alt=""This is the diagram for the second database, also with data."" /></p>
<p>This is my star schema model
<img src=""https://i.stack.imgur.com/qeibo.png"" alt=""And finally the star schema data model"" /></p>
<p>The problem I am facing is i that i do not know how to start doing the mapping when adding my origin OLE DB and destination OLE DB.</p>
<p>I have already searched through the web, but I only find examples where they have to move just one database to the star schema data model.</p>
","<ssis><etl><data-warehouse>","2020-11-13 20:27:57","302","1","2","64828650","<p>The task you have is to consolidate two transactional/OLTP systems into a data warehouse. The reason you find only examples of moving/mapping one system into a data warehouse is that you simply repeat the process for each additional source that feeds into the DW.</p>
<p>In your example, you are integrating two sales systems to produce a unified sales business process report.</p>
<p>My approach would be to copy all the tables as is into your DW and put them into a staging schema (stage_d1, stage_d2). This way, you have all the data locally and it's likely consistent with the start of your data extract i.e. as of 9:30 this morning.</p>
<p>Now that you have the data locally, you need to transform and enrich the data to populate your dimensions and then your fact table.</p>
<p>Let's analyze <code>Dim_Customer</code>. This table is probably a little light in terms of providing value but the methodology is what you should focus on. System 1 supplies a first and last name and a city, state and zipcode. System 2 gives us a Company name, contact name, city, state, postal code and phone.</p>
<p>Given the usage of postal code and zip code, that would have me wondering whether we're dealing with international addresses versus a US centric (zip code) data. I'd also notice that we don't have an actual address line for this data so the point is, analyse your data so you know that you're modeling something that solves the problem (report on sales across all systems).</p>
<p>The next question I'd wonder about is how we populate the Customer dimension. If a Mario Almaguer had a purchase in both system 1 and system 2, are they the &quot;same&quot; person? Does it matter for this business process? If we sold to a person in TX and that name also exists in ME, does it matter if the name is in there twice?</p>
<p>I'll assume we only care about unique customer names. If it's a bad assumption, we go back and model it differently.</p>
<p>In my source, I'll write a query.</p>
<pre><code>SELECT DISTINCT CONCAT(C.FirstName, ' ', C.LastName) AS CustomerName FROM stage_d1.Customer AS C;
</code></pre>
<p>Run that, see that it returns the data I want. I'll then use an <code>OLE DB Source</code> in an SSIS data flow and use the third drop down option of Source query.</p>
<p>If I run the package, we'll get all the unique customer names from the source system. But we only want the names we don't have so that means we need to use something to check our reference table for existing matches. That's the <code>Lookup Component</code></p>
<p>The source for the Lookup will be the DW's <code>Dim_Customer</code> table. You'll match based on CustomerName. The lookup component will tell us whether an incoming row matched and we can get two output streams: Match and no-match. We're only interested in the no-match path because that's new data. Andy Leonard has an excellent <a href=""https://www.sqlservercentral.com/stairways/stairway-to-integration-services"" rel=""nofollow noreferrer"">Stairway to Integration Services</a> and in particular, we're talking about an <a href=""https://www.sqlservercentral.com/steps/adding-rows-in-incremental-loads-level-3-of-the-stairway-to-integration-services"" rel=""nofollow noreferrer"">Incremental Load</a>.</p>
<p>From the Lookup, we'll drag the no-match branch to an OLE DB Destination where we point at Dim_Customer table.</p>
<p>You run that and Dim_Customer is populated. Run it again, and no new rows should be added as we're looking for new data only.</p>
<p>Now we need to solve getting the second staged customer data integrated. Fortunately, it's the same steps except this time our query is easier.</p>
<pre><code>SELECT DISTINCT C.ContactName AS CustomerName FROM stage_d2.Customers AS C;
</code></pre>
<p>Lather, rinse repeat for all of your other dimensions.</p>
<p>You could also skipped the data flows and simply executed a query to do the same.</p>
<pre><code>INSERT INTO dbo.Dim_Customer(CustomerName) 
SELECT DISTINCT CONCAT(C.FirstName, ' ', C.LastName) AS CustomerName 
FROM stage_d1.Customer AS C 
WHERE NOT EXISTS (SELECT * FROM dbo.Dim_Customer AS DC WHERE DC.CustomerName = CONCAT(C.FirstName, ' ', C.LastName));
</code></pre>
<p>Lather, rinse, repeat for the remaining dimensions.</p>
<p>Loading the fact is similar except we will use the Lookup components to find matches (as we need to translate our data to their dimension ids). Here I'll show how we'd populate a simplified version of your fact table</p>
<pre><code>SELECT O.Price AS UnitPrice, BO.OrderDate AS [Date], 1 AS Quantity, 0 AS Discount, CONCAT(C.FirstName, ' ', C.LastName) AS CustomerName
FROM stage_d1.Ordering as O 
INNER JOIN stage_d1.Book_Order AS BO 
    ON BO.OrderID = O.OrderID 
INNER JOIN stage_d1.Customer AS C 
    ON C.CustomerID = BO.Cus_CustomerID;
</code></pre>
<p>That's my source query. The customer lookup will continue to match on Dim_Customer's CustomerName but this time we'll retrieve the CustomerID from the lookup component.</p>
<p>The destination then uses the UnitPrice, Date (depends on how you do it), Quantity and Discount directly from our source. The rest of the dimension keys, we populate through our lookup.</p>
"
"64825258","Assigning group ID to different number of rows each time in R","<p>I have some data where a different number of rows corresponds to one ID (e.g. person). Here is what that looks like:</p>
<pre><code>label      | response
-----------+------------
'consent'  |  'yes'
'age'      |  '34'
'gender'   |  'female'
'language' |  'english'
'education'|  'college'
'consent'  |  'yes'
'age '     |  '37'
'gender '  |  'male'
'language' |  'english'
'education'|  'high school'
'race'     |  'white'
</code></pre>
<p>These responses correspond to two people, one who included a response on race while the other didn't. Since everyone has an answer to <code>consent</code>, I was wondering if there was a way to assign person ID conditional on the consent label. E.g. if label=consent, assign same ID for every row until next consent. For example, I would want the data to look like this:</p>
<pre><code>label      | response   |  ID
-----------+------------+------
'consent'  |  'yes'     |  1 
'age'      |  '34'      |  1
'gender'   |  'female'  |  1
'language' |  'english' |  1
'education'|  'college' |  1
'consent'  |  'yes'     |  2
'age '     |  '37'      |  2
'gender '  |  'male'    |  2
'language' |  'english' |  2
'education'|  'HS'      |  2
'race'     |  'white'   |  2
</code></pre>
<p>I've tried a number of for loops and if statements, but haven't figured out a way to do it. Hopefully it's possible to do.</p>
<p>Thanks!</p>
","<r><data-management>","2020-11-13 17:20:42","24","0","1","64825431","<p>Does this work:</p>
<pre><code>library(dplyr)
df %&gt;% mutate(ID = cumsum(label == 'consent'))
# A tibble: 11 x 3
   label     response       ID
   &lt;chr&gt;     &lt;chr&gt;       &lt;int&gt;
 1 consent   yes             1
 2 age       34              1
 3 gender    female          1
 4 language  english         1
 5 education college         1
 6 consent   yes             2
 7 age       37              2
 8 gender    male            2
 9 language  english         2
10 education high school     2
11 race      white           2
</code></pre>
"
"64809797","Duplicate Rows in Fact Table from SCD2 changes in Dimension","<p>Okay so here are simplified tables to help explain my situation</p>
<p>FactListingCreated:</p>
<pre><code>ListingCreatedSk
CreatedDateSk
ListingSk
StateSk
</code></pre>
<p>DimListing</p>
<pre><code>ListingSk
ListingBk
StateCode
ListingPrice (SCD2)
ListingStatus (SCD2)
RowEffectiveDate
RowExpirationDate
RowCurrentIndicator
</code></pre>
<p>The problem I am having is that when I merge updates from my dimension to my basic transactional fact table I end up with duplicate entries (same ListingBK) in my Fact as a result of SCD2 changes in the dimension adding a new row. What is the best way to handle these situations, with the key constraint we have is we want each row in the fact to point at the original Sk in the dimension table.</p>
<p>Current Procedure:</p>
<pre><code>MERGE INTO dbo.FactListingCreated AS target
    USING 
        (
            SELECT       dlm.CreatedDateSk,
                         dl.ListingSk,
                         CASE
                             WHEN db.BrokerageSk IS NULL THEN -1
                             ELSE db.BrokerageSk 
                         END as BrokerageSk,
                         ds.StateSk

            FROM         stage.DimListingMerge as dlm 

            LEFT JOIN    dbo.DimDate as dd
            ON           dd.DateSk = dlm.CreatedDateSk

            LEFT JOIN    dbo.DimListing as dl
            ON           dl.ListingBk = dlm.ListingBk
            AND          dl.RowCurrentIndicator = 1

            LEFT JOIN    dbo.DimBrokerage as db
            ON           db.BrokerageBk = dlm.BrokerageBk

            LEFT JOIN    dbo.DimState as ds
            ON           ds.StateCode = dlm.StateCode
        ) source  
    ON (target.ListingSk = source.ListingSk)
    THEN UPDATE SET
        target.CreatedDateSk = source.CreatedDateSk,
        target.BrokerageSk = source.BrokerageSk,
        target.StateSk = source.StateSk
    WHEN NOT MATCHED THEN
      INSERT VALUES
      (
        source.CreatedDateSk,
        source.ListingSk,
        source.StateSk
      );
</code></pre>
<p>So I think this procedure would work on updates (where just the previous day's data is pulled), however, would the best method just be to make a separate initial run (pulls all data from dim) where it pulls the initial row for each record? Or am I missing something very obvious that would make this possible with a single stored procedure?</p>
","<sql><data-warehouse><scd2>","2020-11-12 18:35:19","616","0","1","64824047","<p>When you load a fact table that references an SCD2 dimension you need to select the dimension record, from the many with the same BK, that was applicable at the time of the fact &quot;event&quot; date - what that event date is for your facts is determined by your business logic, so it might be a created date, an effective date or something else. Let's assume it is a column called EventDate...</p>
<p>Your SQL JOIN needs to look something like this:</p>
<pre><code>LEFT JOIN    dbo.DimListing as dl
ON           dl.ListingBk = dlm.ListingBk
AND          dlm.EventDate BETWEEN dl.RowEffectiveDate AND dl.RowExpirationDate
</code></pre>
<p>At the moment, your SQL is just picking up the current row from the dimension for all your fact records hence, I suspect, the reason you are getting duplicates</p>
"
"64733941","Q. In SSAS - Referenced Relationship Type","<p>In SSAS, Multidimensional Model has shown in the below image.  <img src=""https://i.stack.imgur.com/1jNsj.png"" alt=""fact-dimension model"" /></p>
<p>Users need to be able to analyze sales by color.
You need to create a dimension that contains all of the colors for products sold by the company.
Which relationship type should you use between the InternetSales table and the new dimension?</p>
<p>A. no relationship<br />
B. regular<br />
C. fact<br />
<strong>D. referenced</strong><br />
E. many-to-many<br />
F. data mining</p>
<p>Is it referenced Relationship?
The fact table is linked to the Colors dimension via another dimension named Products. It should be a referenced relationship. is it?</p>
","<sql-server><database><ssas><data-warehouse>","2020-11-08 01:07:09","73","0","1","64736651","<p>If say referenced. Regular dimensions are joined direct to fact table. The others aren't applicable in this scenario.</p>
"
"64728612","what are the steps I need to perform to clean the data if data into the dimension/fact table improperly loaded","<p>Suppose there is a scenario where there is a data loading process into the fact table\dimensional table, and after analysis found that 100 millions records are being improperly
loaded, what are the steps I need to perform to clean the data properly.</p>
","<etl><data-warehouse><sql-data-warehouse>","2020-11-07 14:18:29","94","0","1","64733340","<p>Here are two practices which help in that scenario:</p>
<ol>
<li><p>Take a backup or snapshot before each batch. In the case of a major error like this you can roll back to the snapshot, reload and process the correct data.</p>
</li>
<li><p>Maintain an insert-only persistent staging area in the DW, such as a data vault, with each row stamped with a batch ID and timestamp. Remove the rows in error, and rebuild your facts and dimensions.</p>
</li>
</ol>
<p>If this represents a real situation your only chance is #1.</p>
<p>If you don't have a reliable backup, and you have updated and/or deleted rows during the ETL/ELT process, you don't have any record of the pre-fail state and it may be impossible to go back.</p>
"
"64691767","How to get the correct table as a Fact Table with relevant keys?(Star Schema)","<p>I have a problem to select the suitable table for the fact table.
I have problem with following two tables</p>
<p>OrderData Table :</p>
<ol>
<li>OrderID</li>
<li>CustomerID</li>
<li>OrderStatus</li>
<li>OrderPurchaseAt</li>
<li>OrderAprovedAt</li>
<li>OrderDeliveredCarrier</li>
<li>OrderDeliveredCustomer</li>
<li>OrderestimatedDelivered</li>
</ol>
<p>OrderItems Table :</p>
<ol>
<li>OrderID</li>
<li>OrderItemID</li>
<li>ProductID</li>
<li>SellerID</li>
<li>ShippingLimitDate</li>
<li>Price</li>
<li>Frieghtweight</li>
</ol>
<p>What is the suitable table for fact Table? my data source is <a href=""https://www.kaggle.com/olistbr/brazilian-ecommerce"" rel=""nofollow noreferrer"">https://www.kaggle.com/olistbr/brazilian-ecommerce</a></p>
<p>Please give me a support.</p>
","<data-warehouse><dimensional-modeling><fact-table><surrogate-key><star-schema-datawarehouse>","2020-11-05 05:39:36","90","1","1","66428946","<p>Let's imagine that you have CustomerID=200 who ordered 2 products ProductID=15 and ProductID=18  with different Sellers, Prices and FreightWeight and same  ShippingLimitDates.</p>
<p>Your will have two fact tables one for purchasing  :</p>
<pre><code>OrderID CustomerSK OrderItemID ProductSK SellerSK ShippingLimitDate Price Frieghtweight
100     200        1           15        1001     15/02/2020        100   12.20
100     200        2           18        1001     15/02/2020        100   12.20
</code></pre>
<p>and one for shipping :</p>
<pre><code>OrderID CustomerSK OrderStatus OrderPurchaseAt OrderAprovedAt OrderDeliveredCarrier OrderDeliveredCustomer OrderestimatedDelivered
100     100        Delivered   14/02/2020      14/02/2020     15/02/2020            16/02/2020             16/02/2020
</code></pre>
<p>You model will be like below :</p>
<p><a href=""https://i.stack.imgur.com/OXhJU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OXhJU.png"" alt=""enter image description here"" /></a></p>
"
"64664194","Fact table reconciliation or verify","<p>In a data warehouse project how do I verify that my fact table loaded in a data warehouse DB through SSIS ETL load is correct with my staging table so that later I don't have incorrect reporting?</p>
","<ssis><etl><data-warehouse><fact-table><datamart>","2020-11-03 13:54:33","165","2","1","64674516","<p>Good question, people creates different systems for this. So you understand this is one of most complex check/reconciliation process that developers built. I tried to give you three ways to do this. I would recommend first one because its easier and most efficient.</p>
<p>You can -</p>
<ol>
<li>Post Load reports: create reports which will reconcile data after load. Write SQL to compare source data and target data - compare count, compare amount, compare null values, compare daily data etc. If the comparison generates flag/alert  - this means some issue in load.</li>
<li>Check as you go : You can create some reusable function or mapping which will compare incoming source data and target data - compare count, compare amount, compare null values, compare daily data etc. and store in a table. A script will keep on checking those values and if there is any issue, script will notify support team.</li>
<li>Pre process check : Before starting any ETL, you can check source data - like count, null values, daily count etc. to verify how is the data, if there is any file missing etc.</li>
</ol>
"
"64654981","Enable users to ""hotfix"" source data while waiting for upstream source data to change","<p>For a few SaaS tools our company uses, a 3rd party administrates the tools and provides us with daily feeds, which we load into our data warehouse.</p>
<p>Occasionally, a record in one of the feeds will have an error that needs to be fixed ASAP for downstream reporting. However, the SLA for the 3rd party to correct the record(s) in the source SaaS system can take up to two weeks. The 'error' doesn't break anything it is just that a record is closed when it should have stayed open, or a field has the wrong value.</p>
<p>The process is as follows:</p>
<ol>
<li>BI team A, downstream of us in the data warehouse team, notices the discrepancy.</li>
<li>BI team A corrects the record in their database, which other teams consume from</li>
<li>BI team B, which receives data from the data warehouse and BI team A, raises an alarm because they see a discrepancy between our output and that which they receive from team A.</li>
<li>We (data warehouse team) have to correct the source data</li>
<li>The upstream 3rd party eventually corrects the records</li>
</ol>
<p>Does anyone have a best practice for this scenario? What is an approach that would:</p>
<p>A. enable the BI team A to correct records ASAP without impacting the data warehouse team, and
B. be rollback-able once the upstream 3rd party corrects the source data?</p>
<p>One idea I had was to use a source-controlled csv file (like a <a href=""https://docs.getdbt.com/docs/building-a-dbt-project/seeds"" rel=""nofollow noreferrer""><code>dbt</code> <code>seed</code></a> table) were it not that records usually contain PII and therefore can't version controlled.</p>
","<sql-server><data-warehouse><data-management><azure-synapse><dbt>","2020-11-03 00:01:26","121","0","1","64680063","<p>how I would approach this:</p>
<ol>
<li>Ensure that you have controls on your DW to catch any errors. Having a consumer of your data (BI Team A) telling you that your data is wrong is not a good place to be in!</li>
<li>Have 1 team responsible for fixing the data and in 1 place - this ensures you have control, consistency and auditing. As the data starts in the DW and then moves downstream to other systems, the DW is the place to fix it.</li>
<li>Build a standard process for fixing data that involves as little manual intervention as possible and which has been developed and tested in advance. When you encounter an error, and are under pressure from your customers to fix it, the last thing you want is to be trying to work out how to resolve the error and then developing/running untested code</li>
<li>At a high-level, your standard process should be a copy of the Production process e.g. a copy of the staging table (where you can insert the corrected versions of the incorrect records) and a copy of the loading process but pointed at this copied staging table . Depending on your Production logic you may need to amend the copy to delete/insert or update the incorrect records in your DW. Depending on your toolset, you might be able to achieve this with a separate config file rather than copying tables/logic.</li>
<li>Auditing. You should always be able to trace the fact that records have been amended, which records have been affected and what the changes were</li>
</ol>
<p>Obviously you need to ensure that the changes you make to the DW cascade down to any consuming systems - either in the regular update process (if your consumers can wait until then) or as a one-off process. Similarly, you need to ensure that when the amended record is finally received from the 3rd Party that it updates your DW correctly and that you've audited the fact that an error has been corrected - presumably you'd want to be able to report on any errors not fixed by the 3rd party within their SLA?</p>
"
"64606496","Sales Person (Regional Manager, Zonal manager, Country Head) linking to Fact table","<p>I have a small data warehouse for Sales. Here i have Fact Table for Sales Invoices and Dimensions like customer, Date Time, Sales Geo, Product Code.</p>
<p>Fact Table: <code>Sales</code>   - &gt;</p>
<pre><code>Invoice Date, Customer code, Product Code, Sales Geo Code, Billing qty, Amount, Tax, Total Amount
</code></pre>
<p>For <code>Sales Geo dimension</code> - &gt;</p>
<pre><code>Sales Geo Code, City Name, Regional Code, Zone Code, Regional Manager Code, Zonal Manager code  
</code></pre>
<p>I have confusion in how to link my sales persons like Regional Manager and Zonal Managers etc.</p>
<p>Regional Manager is leading one region of multiple cities,
zonal manager is leading multiple region.</p>
<p>Sometimes we change the regional area or zonal area, they get promoted, they left etc.</p>
<p>How to create dimension and link sales team with Sales Fact to get correct Sales report.</p>
<p>regards</p>
","<analytics><data-warehouse><msbi>","2020-10-30 10:20:23","34","1","2","64607391","<p>there are a few options I can think of:</p>
<ol>
<li>Denormalise the Regional and Zonal Manager information into your Sales Geo Dimension</li>
<li>Create a hierarchical Manager dimension keyed on Regional Manager and including their zonal manager details</li>
<li>Create a Person Dim and associate it twice to the Fact - once in the role of Regional Manager and once in the Role of Zonal Manager</li>
</ol>
<p>If you will never want to link Manager information to a fact except in the context of the Sales Geo then option 1 probably makes more sense - as you have fewer potential joins in queries using this fact table.</p>
<p>Option 2 is more flexible as you can associate manager information to a fact without also using the Sales Geo</p>
<p>Option 3 is the most flexible but also likely to give the worst query performance (for any query that needs both types of manager) and also the only link between Regional and Zonal managers is via the Fact; there is no hierarchical information held in the Dimensions. Therefore option 3 is the one I would be least likely to choose</p>
"
"64606496","Sales Person (Regional Manager, Zonal manager, Country Head) linking to Fact table","<p>I have a small data warehouse for Sales. Here i have Fact Table for Sales Invoices and Dimensions like customer, Date Time, Sales Geo, Product Code.</p>
<p>Fact Table: <code>Sales</code>   - &gt;</p>
<pre><code>Invoice Date, Customer code, Product Code, Sales Geo Code, Billing qty, Amount, Tax, Total Amount
</code></pre>
<p>For <code>Sales Geo dimension</code> - &gt;</p>
<pre><code>Sales Geo Code, City Name, Regional Code, Zone Code, Regional Manager Code, Zonal Manager code  
</code></pre>
<p>I have confusion in how to link my sales persons like Regional Manager and Zonal Managers etc.</p>
<p>Regional Manager is leading one region of multiple cities,
zonal manager is leading multiple region.</p>
<p>Sometimes we change the regional area or zonal area, they get promoted, they left etc.</p>
<p>How to create dimension and link sales team with Sales Fact to get correct Sales report.</p>
<p>regards</p>
","<analytics><data-warehouse><msbi>","2020-10-30 10:20:23","34","1","2","64609194","<p><strong>Denormalize</strong> the <em>Regional Manager Code</em> and <em>Zonal Manager code</em> in your <em>fact table</em>.</p>
<p>So basically you will store in each fact row along with the <em>Sales Geo Code</em> also the current assignment of the two manager roles at the time of the sales (more precise at the time of the loading the record).</p>
<p>This model allows both types of the reports using</p>
<ul>
<li><p>managers assigned at the transaction time (direct from the fact table)  and</p>
</li>
<li><p>current managers (join from the fact table to <em>Sales Geo dimension</em> to get the code of the both managers)</p>
</li>
</ul>
<p>Now your setup allows only the second type of the reporting, which could be suboptimal in case that the managers are frequently re-assigned.</p>
<p>If you prefer <strong>not to  denormalize</strong> the fact table you can always switch the <em>Sales Geo dimension</em> to  SCD type 2 which will introduce a historical view on the dimension and the assignment of the managers.</p>
<p>You'll have to join not only with the <em>Sales Geo Code</em> from the fact table to the <em>Sales Geo dimension</em> but also considering the transaction date...</p>
<pre><code>Invoice_Date between sales_geo.validfrom_date and sales_geo.validto_date 
</code></pre>
<p>... to get the managers assigned at the time of the transaction.</p>
<p>The decision has a <strong>typical tradeoff</strong> between storage plus maintaining the consistency and more complex joins plus maintaing the dimension history on the other side.</p>
"
"64587765","Using Surrogate Keys in Data Warehouse Pros and Cons","<p>A surrogate key is a mechanism that exists in our books for years and I hate for bringing into discussion again. Everyone is talking about the benefits of using a surrogate key instead of a business key. Even Microsoft Analysis Services Tabular and Microsoft PowerBI Tabular Models are working with the surrogate key. Both platforms mentioned give you the ability to connect a dimension and a fact using one column, and therefore is a surrogate key, as is very difficult to have one single business key in real life.</p>
<p>Working as BI Architect in the latest years I worked with Analysis Services Multidimensional and Tabular, I had projects in Multidimensional, which were managed up to 500GB in the DataWarehouse each night. I faced facts contracted from 5-6 unions and 8-10 joins among tables with millions of records.</p>
<p>Here comes the question, using Surrogate Key, in order the fact to be able to know the dimensions Key we need to make an extra Join. As a result, if we want to be able to &quot;Relate&quot; N dimensions (which are not already connected with a fact in construction expression) with a single Fact we need N additional Joins in the DataWarehouse.</p>
<p>Let's take the previous example, so for this particular fact, we need 5-6 unions + (8-10 + N) joins which increases the complexity, image of what will happen once we have the requirement to relate this fact with 10-15 dimensions to get the surrogate key.</p>
<p>All these years I was trying to read my facts expressions using my early coffee like reading a newspaper and remove unused columns, unions, joins, and make everything to reduce the complexity ta save ETL process time.</p>
<p>Its fully understand that we will save time for querying DataWarehouse and Semantic Layer, but what about ETL, I am missing something?</p>
","<sql><performance><etl><data-warehouse><surrogate-key>","2020-10-29 08:55:03","172","0","1","64589155","<p>a couple of thoughts about your question...</p>
<ol>
<li>If you didn't use SKs then how would you handle SCD2 dimensions where the natural/business keys from the source system (even if they were a single column) wouldn't be unique?</li>
<li>The purpose of a DW is to make it easier and quicker to query your data. If you consider that any problem takes a certain amount of effort to resolve then you have a choice where you apply that effort in the chain of activities required to produce the solution. If you want to reduce the effort of querying then you need to increase the effort in data preparation i.e. your ETL</li>
</ol>
"
"64538637","One or multiple fact tables for different businesses in one company?","<p>I'm new to database design.
I'm trying to design a data model for car sharing company.
They have more than one business model like car sharing, scooter sharing, delivery.</p>
<p>Here is my dilemma, which choice should I choose:</p>
<ol>
<li><p>(<strong>database warehouse bus architecture</strong>)
Should I create more than one fact table for different businesses?
different fact table represents one business model and share some conformed dimensions?</p>
</li>
<li><p>(<strong>one star schema</strong>)
Or should I store the different business names(car sharing, scooter, delivery) in one dimension table call it product?</p>
</li>
</ol>
<p>Thank you!!!!</p>
","<database-design><data-warehouse><dimensional-modeling><star-schema><fact-table>","2020-10-26 14:00:40","800","-1","1","64553529","<p>the way I would approach this is as follows:</p>
<ol>
<li>List all your facts and their associated dimensions</li>
<li>Determine the level of commonality of dimensions across all facts</li>
</ol>
<p>If your facts all have roughly the same dimensions then you <em><strong>could</strong></em> put them all in one fact table. There is no hard and fast rule for for this but I would say that if a fact has more than about 2 dimensions that are not used by other facts (or are used by other facts but not by this fact) then it needs to be in its own fact table.</p>
<p>If you decide that you could put more than one fact in a single fact table then it becomes a judgement call. Things to consider when making the decision include (obviously not a definitive list):</p>
<ul>
<li>Would you ever want to query different facts at the same time, in the same query? If so, then having them in 1 fact table makes this easier e.g. count by fact type by date</li>
<li>Are your data volumes so large (or likely to be in the future) that having a single fact table is going to cause issues? If so, then having separate fact tables at the start, rather than having to split them in the future, makes sense</li>
<li>Security: do you need to restrict access to different facts to different groups of people. If so then having separate fact tables would probably makes this easier</li>
</ul>
<p>My personal preference, assuming querying different facts at the same time wasn't a major requirement (first bullet point above), would be to use separate fact tables. While there is more ELT involved it shouldn't be significant - once you've built the logic for the 1st fact table then all the others should be &quot;copy-and-paste&quot; plus a small amount of editing. Having different fact tables is more flexible and is probably going to cause you fewer issues in the future</p>
"
"64507623","Precalculate OLAP cube inside Azure Synapse","<p>We have dimensinal model with fact tables of 100-300 GBs in parquet each. We build PBI reports on top of Azure Synapse (DirectQuery) and experience performance issues on slicing/dicing and especially on calculating multiple KPIs. In the same time data volume is pretty expensive to be kept in Azure Analysis Services. Because of number of dimensions, the fact table can't be aggregated significantly, so PBI import mode or composite model isn't an option as well.</p>
<p>Azure Synapse Analytics <a href=""https://learn.microsoft.com/en-us/sql/t-sql/queries/select-group-by-transact-sql?view=sql-server-ver15"" rel=""nofollow noreferrer"">faciliates OLAP operations</a>, like GROUP BY ROLLUP/CUBE/GROUPING SETS.</p>
<ol>
<li><em>How can I benefit from Synapse's OLAP operations support?</em></li>
<li><em>Is that possible to pre-calculate OLAP cubes inside Synapse in order to boost PBI reports performance? How?</em></li>
<li><em>If the answer is yes, is that recomended to pre-calculate KPIs? Means moving KPIs definition to DWH OLAP cube level - is it an anti-pattern?</em></li>
</ol>
<p>P.S. using separate aggreagations for each PBI visualisation is not an option, it's more an exception from the rule. Synapse is clever enough to take the benefit from materialized view aggregation even on querying a base table, but this way you can't implement RLS and managing that number of materialized views also looks cumbersome.</p>
<h2>Upd for @NickW</h2>
<p>Could you please answer the following sub-questions:</p>
<ol>
<li>Have I got it right - OLAP operations support is mainly for downstream cube providers, not for Warehouse performance?</li>
<li>Is spawning Warehouse with materialized views in order to boost performance is considered a common practice or an anti-pattern? I've found (see <a href=""https://www.jamesserra.com/archive/2020/09/microsoft-ignite-announcements-2020/"" rel=""nofollow noreferrer"">the link</a>) Power BI can create materialized views automatically based on query patterns. Still I'm afraid it won't be able to provide a stable testable solution, and RLS support again.</li>
<li>Is KPIs pre-calculation at Warehouse side considered as a common way or an anti-pattern? As I understand this is usually done no cube provider side, but if I haven't got one?</li>
<li>Do you see any other options to boost the performance? I can think only about reducing query parallelism by using PBI composite model and importing all dimensions to PBI. Not sure if it'd help.</li>
</ol>
","<azure><data-warehouse><olap><olap-cube><azure-synapse>","2020-10-23 21:39:33","1832","1","2","64553196","<p>to hopefully answer some of your questions...</p>
<ol>
<li>You can't pre-calculate OLAP cubes in Synapse; the closest you could get is creating aggregate tables and you've stated that this is not a viable solution</li>
<li>OLAP operations can be used in queries but don't &quot;pre-build&quot; anything that can be used by other queries (ignoring CTEs, sub-queries, etc.). So if you have existing queries that don't use these functions then re-writing them to use these functions might improve performance - but only for each specific query</li>
</ol>
<p>I realise that your question was about OLAP but the underlying issue is obviously performance. Given that OLAP is unlikely to be a solution to your performance issues, I'd be happy to talk about performance tuning if you want?</p>
<h2>Update 1 - Answers to additional numbered questions</h2>
<ol>
<li>I'm not entirely sure I understand the question so this may not be an answer: the OLAP functions are there so that it is possible to write queries that use them. There can be an infinite number of reasons why people might need to to write queries that use these functions</li>
<li>Performance is the main (only?) reason for creating materialised views. They are very effective for creating datasets that will be used frequently i.e. when base data is at day level but lots of reports are aggregated at week/month level. As stated by another user in the comments, Synapse can manage this process automatically but whether it can actually create aggregates that are useful for a significant proportion of your queries is obviously entirely dependent on your particular circumstances.</li>
<li>KPI pre-calculation. In a DW any measures that can be calculated in advance should be (by your ETL/ELT process). For example, if you have reports that use Net Sales Amount (Gross Sales - Tax) and your source system is only providing Gross Sales and Tax amounts then your should be calculating Net Sales as a measure when loading your fact table. Obviously there are KPIs that can't be calculated in advance (i.e. probably anything involving averages) and these need to be defined in your BI tool</li>
<li>Boosting Performance: I'll cover this in the next section as it is a longer topic</li>
</ol>
<h2>Boosting Performance</h2>
<p>Performance tuning is a massive subject - some areas are generic and some will be specific to your infrastructure; this is not going to be a comprehensive review but will highlight a few areas you might need to consider.</p>
<p>Bear in mind a couple of things:</p>
<ol>
<li>There is always an absolute limit on performance - based on your infrastructure - so even in a perfectly tuned system there is always going to be a limit that may not be what you hoped to achieve. However, with modern cloud infrastructure the chances of you hitting this limit are very low</li>
<li>Performance costs money. If all you can afford is a Mini then regardless of how well you tune it, it is never going to be as fast as a Ferrari</li>
</ol>
<p>Given these caveats, a few things you can look at:</p>
<ol>
<li>Query plan. Have a look at how your queries are executing and whether there are any obvious bottlenecks you can then focus on. This link give some further information <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor"" rel=""nofollow noreferrer"">Monitor SQL Workloads</a></li>
<li>Scale up your Synapse SQL pool. If you throw more resources at your queries they will run quicker. Obviously this is a bit of a &quot;blunt instrument&quot; approach but worth trying once other tuning activities have been tried. If this does turn out to give you acceptable performance you'd need to decide if it is worth the additional cost. <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/quickstart-scale-compute-portal"" rel=""nofollow noreferrer"">Scale Compute</a></li>
<li>Ensure your statistics are up to date</li>
<li>Check if the distribution mechanism (Round Robin, Hash) you've used for each table is still appropriate and, on a related topic, check the skew on each table</li>
<li>Indexing. Adding appropriate indexes will speed up your queries though they also have a storage implication and will slow down data loads. This article is a reasonable starting point when looking at your indexing: <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-index"" rel=""nofollow noreferrer"">Synapse Table Indexing</a></li>
<li>Materialised Views. Covered previously but worth investigating. I think the automatic management of MVs may not be out yet (or is only in public preview) but may be something to consider down the line</li>
<li>Data Model. If you have some fairly generic facts and dimensions that support a lot of queries then you might need to look at creating additional facts/dimensions just to support specific reports. I would always (if possible) derive them from existing facts/dimensions but you can create new tables by dropping unused SKs from facts, reducing data volumes, sub-setting the columns in tables, combining tables, etc.</li>
</ol>
<p>Hopefully this gives you at least a starting point for investigating your performance issues.</p>
"
"64507623","Precalculate OLAP cube inside Azure Synapse","<p>We have dimensinal model with fact tables of 100-300 GBs in parquet each. We build PBI reports on top of Azure Synapse (DirectQuery) and experience performance issues on slicing/dicing and especially on calculating multiple KPIs. In the same time data volume is pretty expensive to be kept in Azure Analysis Services. Because of number of dimensions, the fact table can't be aggregated significantly, so PBI import mode or composite model isn't an option as well.</p>
<p>Azure Synapse Analytics <a href=""https://learn.microsoft.com/en-us/sql/t-sql/queries/select-group-by-transact-sql?view=sql-server-ver15"" rel=""nofollow noreferrer"">faciliates OLAP operations</a>, like GROUP BY ROLLUP/CUBE/GROUPING SETS.</p>
<ol>
<li><em>How can I benefit from Synapse's OLAP operations support?</em></li>
<li><em>Is that possible to pre-calculate OLAP cubes inside Synapse in order to boost PBI reports performance? How?</em></li>
<li><em>If the answer is yes, is that recomended to pre-calculate KPIs? Means moving KPIs definition to DWH OLAP cube level - is it an anti-pattern?</em></li>
</ol>
<p>P.S. using separate aggreagations for each PBI visualisation is not an option, it's more an exception from the rule. Synapse is clever enough to take the benefit from materialized view aggregation even on querying a base table, but this way you can't implement RLS and managing that number of materialized views also looks cumbersome.</p>
<h2>Upd for @NickW</h2>
<p>Could you please answer the following sub-questions:</p>
<ol>
<li>Have I got it right - OLAP operations support is mainly for downstream cube providers, not for Warehouse performance?</li>
<li>Is spawning Warehouse with materialized views in order to boost performance is considered a common practice or an anti-pattern? I've found (see <a href=""https://www.jamesserra.com/archive/2020/09/microsoft-ignite-announcements-2020/"" rel=""nofollow noreferrer"">the link</a>) Power BI can create materialized views automatically based on query patterns. Still I'm afraid it won't be able to provide a stable testable solution, and RLS support again.</li>
<li>Is KPIs pre-calculation at Warehouse side considered as a common way or an anti-pattern? As I understand this is usually done no cube provider side, but if I haven't got one?</li>
<li>Do you see any other options to boost the performance? I can think only about reducing query parallelism by using PBI composite model and importing all dimensions to PBI. Not sure if it'd help.</li>
</ol>
","<azure><data-warehouse><olap><olap-cube><azure-synapse>","2020-10-23 21:39:33","1832","1","2","64559187","<p>Synapse <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/performance-tuning-result-set-caching#:%7E:text=When%20result%20set%20caching%20is%20enabled%2C%20Synapse%20SQL,improves%20query%20performance%20and%20reduces%20compute%20resource%20usage."" rel=""nofollow noreferrer"">Result Set Caching</a> and <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-materialized-view-performance-tuning#:%7E:text=Materialized%20views%20vs.%20standard%20views%20%20%20,%20%20Fast%20%202%20more%20rows%20"" rel=""nofollow noreferrer"">Materialized Views</a> can both help.</p>
<p>In the future the creation and maintence of Materialized Views will be automated.</p>
<blockquote>
<p>Azure Synapse will automatically create and manage materialized views
for larger Power BI Premium datasets in DirectQuery mode. The
materialized views will be based on usage and query patterns. They
will be automatically maintained as a self-learning, self-optimizing
system. Power BI queries to Azure Synapse in DirectQuery mode will
automatically use the materialized views. This feature will provide
enhanced performance and user concurrency.</p>
</blockquote>
<p><a href=""https://learn.microsoft.com/en-us/power-platform-release-plan/2020wave2/power-bi/synapse-integration"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/power-platform-release-plan/2020wave2/power-bi/synapse-integration</a></p>
<p><a href=""https://learn.microsoft.com/en-us/power-bi/transform-model/desktop-aggregations"" rel=""nofollow noreferrer"">Power BI Aggregations</a> can also help.  If there are a lot of dimensions, select the most commonly used to create aggregations.</p>
"
"64477448","In Snowflake how to make NEW data available once all related objects have been loaded?","<p>Lets say we have dimensional data model in our Snowflake data warehouse that is loaded once a day. The requirement is that the data loaded today should be available to end users only when ALL objects have been loaded successfully.</p>
<p>So if dimension d1 has been loaded successfully today, but fact f1 load fails for what ever reason, then the end users should stay with the data loaded yesterday (also for the dimension d1 then). The flip over to the new data should take place only when all dimensional model objects have loaded successfully.</p>
<p>We have currently then done it the way that we stage the incremental set / new data to staging tables and once all successfully there then there is a huge SQL script doing DELETE/INSERT one dimensional model table at a time and then the single commit in the end. So the enda data load is not done in parallel.</p>
<p>Is there a better way to achieve the requirement? We were thinking about materialized views (load all the underlying dimensional model tables and the refresh the &quot;select * from table&quot; mat. views ) but they are refreshed automatically in Snowflake. Also then DROP target table / CLONE from source to target table does not suite us as there is the chance that the target table is queried at the time when it does not exist.</p>
","<snowflake-cloud-data-platform><data-warehouse><dimensional-modeling>","2020-10-22 07:39:37","147","1","2","64514066","<p>I do not understand why you eliminate the CLONE option. You do not need to clone each object, you can clone the entire database or schema with one command, make it available to your users. Next day, when all data is loaded, you can re-clone the object. It seems like an ideal solution for what you want to achieve.</p>
<p><a href=""https://community.snowflake.com/s/article/cloning-in-snowflake"" rel=""nofollow noreferrer"">https://community.snowflake.com/s/article/cloning-in-snowflake</a></p>
<p>The CLONE operation does not copy data, it is a metadata operation. Therefore it will not occupy extra space unless the cloned data is removed from the original/source tables, or you add new data to the cloned objects:</p>
<p><a href=""https://docs.snowflake.com/en/user-guide/tables-storage-considerations.html#cloning-tables-schemas-and-databases"" rel=""nofollow noreferrer"">https://docs.snowflake.com/en/user-guide/tables-storage-considerations.html#cloning-tables-schemas-and-databases</a></p>
"
"64477448","In Snowflake how to make NEW data available once all related objects have been loaded?","<p>Lets say we have dimensional data model in our Snowflake data warehouse that is loaded once a day. The requirement is that the data loaded today should be available to end users only when ALL objects have been loaded successfully.</p>
<p>So if dimension d1 has been loaded successfully today, but fact f1 load fails for what ever reason, then the end users should stay with the data loaded yesterday (also for the dimension d1 then). The flip over to the new data should take place only when all dimensional model objects have loaded successfully.</p>
<p>We have currently then done it the way that we stage the incremental set / new data to staging tables and once all successfully there then there is a huge SQL script doing DELETE/INSERT one dimensional model table at a time and then the single commit in the end. So the enda data load is not done in parallel.</p>
<p>Is there a better way to achieve the requirement? We were thinking about materialized views (load all the underlying dimensional model tables and the refresh the &quot;select * from table&quot; mat. views ) but they are refreshed automatically in Snowflake. Also then DROP target table / CLONE from source to target table does not suite us as there is the chance that the target table is queried at the time when it does not exist.</p>
","<snowflake-cloud-data-platform><data-warehouse><dimensional-modeling>","2020-10-22 07:39:37","147","1","2","64529836","<p>Ok, I think I may have understood your problem statement. But let me rephrase it so that I am sure I have really understood it. You have a number of dimension tables and a fact table. For example</p>
<p>You have D1, D2,D3 as dimension table and then you have a fact table F1. Your daily ETL process loads F1, D1, D2 and D3. But you want to load D1,D2,D3 with the new data only when the F1 load is successful and during the load you do not want users to get affected.</p>
<p>If this is correct one approach will be as below(this approach has a disadvantage that it will have additional storage cost associated for the duration of the ETL)</p>
<p>When your ETL starts, clone new tables F1', D1',D2',D3' from the respective F1, D1,D2,D3 tables. Load the incremental data first to F1',D1',D2' and D3'. If the load is successful for all these tables do an ALTER SWAP and swap F1' to become F1, D1' to become D1, D2' to become D2 and D3' to become D3. Post that you can truncate F1',D1',D2'D3' and repeat this process everyday.</p>
"
"64462673","How can I make ids for new table and insert genres_id and genres (2038 records) in a new table in data warehouse using PLSQL","<p>I have 1 table <code>movies</code>from csv file. I need to make data warehouse with additional tables <code>genres</code>, <code>production_companies</code>, <code>directors</code>.</p>
<p>For start I want to make new table <code>genres</code> that will have 2083 records.
If I use <code>sequence_name.nextval()</code> than I will have duplicates as I have 10856 movies in <code>movies</code> table.</p>
<p>I will also need to have only <code>genres_id</code> in the table <code>dw_movies</code>, not the <code>genres_name</code></p>
<p>Please, give me your advise.
Here is <code>movies</code> table:</p>
<pre><code>CREATE TABLE &quot;AS6400U&quot;.&quot;MOVIES&quot; (   
        &quot;ID&quot; NUMBER(38,0), 
        &quot;IMDB_ID&quot; VARCHAR2(26 BYTE), 
        &quot;POPULARITY&quot; NUMBER(38,6), 
        &quot;BUDGET&quot; NUMBER(38,0), 
        &quot;REVENUE&quot; NUMBER(38,0), 
        &quot;ORIGINAL_TITLE&quot; VARCHAR2(500 BYTE), 
        &quot;CAST&quot; VARCHAR2(500 BYTE), 
        &quot;HOMEPAGE&quot; VARCHAR2(300 BYTE), 
        &quot;DIRECTOR&quot; VARCHAR2(600 BYTE), 
        &quot;TAGLINE&quot; VARCHAR2(400 BYTE), 
        &quot;KEYWORDS&quot; VARCHAR2(500 BYTE), 
        &quot;OVERVIEW&quot; VARCHAR2(2000 BYTE), 
        &quot;RUNTIME&quot; NUMBER(38,0), 
        &quot;GENRES&quot; VARCHAR2(400 BYTE), 
        &quot;PRODUCTION_COMPANIES&quot; VARCHAR2(500 BYTE), 
        &quot;RELEASE_DATE&quot; DATE, 
        &quot;VOTE_COUNT&quot; NUMBER(38,0), 
        &quot;VOTE_AVERAGE&quot; NUMBER(38,1), 
        &quot;RELEASE_YEAR&quot; NUMBER(38,0), 
        &quot;BUDGET_ADJ&quot; NUMBER(38,3), 
        &quot;REVENUE_ADJ&quot; NUMBER(38,5)
       )
</code></pre>
<p>Kind regards,
Anna</p>
","<oracle><database-design><data-modeling><data-warehouse>","2020-10-21 11:34:56","48","-1","2","64463420","<p>This is about database modelling. Tables usually depend on each other, and that relationship is maintained through foreign key constraints. Furthermore, tables should be normalized (up to 3rd normal form) so that you wouldn't store the same data twice (or more times).</p>
<p>However, data warehouses <em>act</em> differently because they deal with a lot of data so fetching rows from several huge tables takes time, so DW tables aren't always normalized.</p>
<p>Saying that your tables have 10K rows - that's close-to-nothing for Oracle, so I believe that you should do what I mentioned in the first paragraph: normalize tables first, set relationships (primary keys referenced by foreign keys). Write all those <code>CREATE TABLE</code> statements, one-by-one to make sure you're doing it right.</p>
<p>As of the sequence next value: I'd say that you got it wrong. Usually, its (sequences) values are in ascending order. It doesn't matter that current table has 10K rows and you'd want to create a new table with 2K rows; so what? <code>sequence.nextval</code> will probably be 10K-something (if it was used for movies table as well). You shouldn't (and probably won't) get any duplicates.</p>
"
"64462673","How can I make ids for new table and insert genres_id and genres (2038 records) in a new table in data warehouse using PLSQL","<p>I have 1 table <code>movies</code>from csv file. I need to make data warehouse with additional tables <code>genres</code>, <code>production_companies</code>, <code>directors</code>.</p>
<p>For start I want to make new table <code>genres</code> that will have 2083 records.
If I use <code>sequence_name.nextval()</code> than I will have duplicates as I have 10856 movies in <code>movies</code> table.</p>
<p>I will also need to have only <code>genres_id</code> in the table <code>dw_movies</code>, not the <code>genres_name</code></p>
<p>Please, give me your advise.
Here is <code>movies</code> table:</p>
<pre><code>CREATE TABLE &quot;AS6400U&quot;.&quot;MOVIES&quot; (   
        &quot;ID&quot; NUMBER(38,0), 
        &quot;IMDB_ID&quot; VARCHAR2(26 BYTE), 
        &quot;POPULARITY&quot; NUMBER(38,6), 
        &quot;BUDGET&quot; NUMBER(38,0), 
        &quot;REVENUE&quot; NUMBER(38,0), 
        &quot;ORIGINAL_TITLE&quot; VARCHAR2(500 BYTE), 
        &quot;CAST&quot; VARCHAR2(500 BYTE), 
        &quot;HOMEPAGE&quot; VARCHAR2(300 BYTE), 
        &quot;DIRECTOR&quot; VARCHAR2(600 BYTE), 
        &quot;TAGLINE&quot; VARCHAR2(400 BYTE), 
        &quot;KEYWORDS&quot; VARCHAR2(500 BYTE), 
        &quot;OVERVIEW&quot; VARCHAR2(2000 BYTE), 
        &quot;RUNTIME&quot; NUMBER(38,0), 
        &quot;GENRES&quot; VARCHAR2(400 BYTE), 
        &quot;PRODUCTION_COMPANIES&quot; VARCHAR2(500 BYTE), 
        &quot;RELEASE_DATE&quot; DATE, 
        &quot;VOTE_COUNT&quot; NUMBER(38,0), 
        &quot;VOTE_AVERAGE&quot; NUMBER(38,1), 
        &quot;RELEASE_YEAR&quot; NUMBER(38,0), 
        &quot;BUDGET_ADJ&quot; NUMBER(38,3), 
        &quot;REVENUE_ADJ&quot; NUMBER(38,5)
       )
</code></pre>
<p>Kind regards,
Anna</p>
","<oracle><database-design><data-modeling><data-warehouse>","2020-10-21 11:34:56","48","-1","2","64467303","<p>I'd echo @Littlefoot, with additional observations about your data model.</p>
<p>RELEASE_YEAR a 38-digit number?  Really?  It should be a DATE, with default values for month,day, and time components.  Even if you want to store it as a NUMBER (bad idea), why do you need 38 digits for it?</p>
<p>GENRES shouldn't be there at all. A movie can have multiple genres, so that should be in a 'link' table -  a table with one column as an FK back to the PK of MOVIES, and a second as a FK back to the PK of the GENRES table.</p>
<p>Some of the other columns look like they could use the same treatment. Especially CAST and KEYWORD and PRODUCTION_COMPANIES.  (The fact that PRODUCTION_COMPANIES is plural is a giveaway).</p>
<p>All of those 38-digit numbers!  38 digits to hold revenue?  A producer could only dream of a movie grossing more than the gross domestic product of the entire planet.</p>
<p>You may think of your database as a 'data warehouse', and that may be true for it's usage patterns (or it may not - the term often gets used very loosely to describe just about any database), but as a practical matter its size is barely a blip.  You should design all tables to Third Normal Form, and only resort to 'demoralizing' for performance when you discover an actual performance issue that is positively confirmed needing such treatment.</p>
"
"64462484","DynamoDB GSI data modelling for an articles app","<p>I want to create an articles application using serverless (AWS Lambda + DynamoDB + S3 for hosting the FE).
I have some questions regarding the &quot;1 table approach&quot;.
The actions I want to follow:</p>
<ol>
<li>Get latest (6) articles sorted by date</li>
<li>Get an article by id</li>
<li>Get the prev/next article relative to the article opened (based on creation date)</li>
<li>Get related articles by tags</li>
<li>Get comments by article</li>
</ol>
<p>I have created an initial spreadsheet for the information:
<a href=""https://i.stack.imgur.com/zmkNG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zmkNG.png"" alt=""enter image description here"" /></a></p>
<p>The first problem I have is that for action nr. 1, I cannot get all the articles based on date, I've added the SK for articles as a date, but because the PK has separate articles, each with its id: article-1, article-2.. and so on, I don't know how to fetch all the articles only by SK.</p>
<p>I then tried creating a LSI , but then I noticed that the LSI needs to have the PK the same as the table, so I can select based on LSI type = 'ARTICLE', but I still cannot selected them ordered by date (entities_sort value)</p>
<p>I know AWS says its good for PK to be unique, but then how do you group the data in this case?</p>
<p>I've created a GSI
<a href=""https://i.stack.imgur.com/NaL9c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NaL9c.png"" alt=""enter image description here"" /></a></p>
<p>This helps me get articles by type(GSI2PK)='ARTICLE' sorted by entities_sort (GSI2SK), but isn't there a better way of achieving this? Having your articles as a PK in a table, but somehow still being able to get them sorted by date?</p>
<p>Having GSI1PK, GSI1SK this way - I can get all the comments for an article using reverse lookup, so thats good.</p>
<p>But I still also don't know how to implement number 3. Get the prev/next article relative to the article opened (based on creation date): getting an article by id, check its creation date(entities_sort), then somehow get the next article before and after based on that creation date (entities_sort), is there a function in DynamoDB that can do this for me?</p>
<p>In my approach I try to query/process as few items as possible so I don't want to use filter functions, rather partition my information.</p>
<p>My question is, how should I achieve 1 and 3? And isn't creating 2 GSI's for such few actions overkill?</p>
<p>What is the pattern to have articles on a PK, unique with ids, but still being able to get them sorted by creation date?</p>
<p>Thank you</p>
","<amazon-web-services><amazon-dynamodb><serverless><serverless-architecture>","2020-10-21 11:22:43","166","0","1","64502720","<p>So what I've ended up doing is:</p>
<p>My access patterns in detail are:</p>
<ol>
<li>Get any Article by Id (for edit/delete)</li>
<li>Get any Comment by Id (for edit/delete)</li>
<li>Get any Tag by Id (for edit/delete)</li>
<li>Get all Articles ordered by date</li>
<li>Get all the Tags of an Article</li>
<li>Get all comments for an article, sorted by date</li>
<li>Get all Articles that have a specific tag, ordered by date (because I want to show only the last 3 ones)</li>
</ol>
<p><a href=""https://i.stack.imgur.com/xEhoY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xEhoY.png"" alt=""enter image description here"" /></a></p>
<p>This is the way I've implemented my model, and I can get all the informations needed.</p>
<p>Also, all my data is partitioned and the queries are really efficient, I always get exactly what I need and the ScannedDocuments value is always the number or returned objects.</p>
<p>The Global Secondary Index helps me query by Article Id and I get, all the comments and tags of that Article.</p>
<p>I've solved the many-to-many between Tags and Articles by a new record in the end:
tag_id, article_date, arct_id, tag_id</p>
<p>So, if I want all articles that have a specific tag sorted by date I can query the PK of the table and sort by SK. If I want to get a single Tag (for edit/delete) I can use the GSI by: article_id, tag_id .. and I get the relation between them.</p>
<p>For getting all Articles sorted by date, i query PK: ARTICLE  and an option condition if I want to get only the ones after a date or not I can condition the SK.</p>
<p>For all the comments and tags of an Article I can use the GSI with : article_link_pk: article_id and I get all comments and tags. If I want only comments I can say article_link_pk: article_id  and article_link_sk: begins_with(article_link_sk, '2020') in this way I get only comments, without tags.</p>
<p>The data model in NoSQL Developer looks like this:
<a href=""https://i.stack.imgur.com/mlVYO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mlVYO.png"" alt=""enter image description here"" /></a></p>
<p>The GSI reverse lookup looks like this:
<a href=""https://i.stack.imgur.com/rSYg0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rSYg0.png"" alt=""enter image description here"" /></a></p>
<p>It's been a journey, but I feel like I finally got a grasp on how to do data modelling in DynamoDB</p>
"
"64444234","Swap parts of columns in Pandas dataframe","<p>I have to manage a dataset that looks like
<a href=""https://i.stack.imgur.com/cLOZM.png"" rel=""nofollow noreferrer"">this</a>
when I plot it. As seen from the graph, around x=17, the values of the &quot;orange&quot; column are taking the place of the data where the &quot;green&quot; column should take. Respectively, the &quot;blue&quot; data are taking the place of &quot;orange&quot; ones, and the &quot;green&quot; ones the place of &quot;blue&quot; ones. This swap occurs around x=17. Later on the graph (around x=24) the swap is different. My question is how can I get the data in the right place (columns). The swap point is not always constant so I can't just swap parts of the columns iteratively. My thought on that is that I have to check for the difference between two points. When the difference is bigger than some value, this is probably a swap point. Although this is not always the case since most of the graphs have a non-linear behavior.
A typical dataset contains many more rows so I am looking for a solution to this as much as parameterized  as possible.
This is the dataset for the above graph:</p>
<pre><code>col1 = [8724.96757035, 8720.86855769, 8713.03560178, 8711.77188717,
       8723.40967556, 8717.95864342, 8719.46206709, 8716.15746255,
       8715.83456161, 8722.05038594, 8721.822529  , 8714.29076839,
       8721.68118216, 8714.94677413, 8706.33839393, 8719.94888389,
       8715.71175774, 8480.37544428, 9151.63757245, 9138.71268152,
       9127.43234993, 9146.51437639, 9148.00997757, 9130.06677617,
       9151.43128313, 8481.34668127, 8482.40548913, 8481.96440291,
       8481.39530663, 8482.7611363 , 8481.26267875, 8480.71911933,
       8481.02279341]

col2 = [8718.4606092 , 9150.29254687, 9130.86473512, 9140.34929925,
       9142.43843709, 9158.33993226, 9148.70914607, 9164.89441174,
       9145.08470894, 9147.82723909, 9132.61236281, 9200.58503831,
       9129.96054189, 9135.65207477, 9165.43826932, 9145.35463759,
       9134.02400092, 8481.58635709, 8480.90717793, 8479.96295137,
       8483.73891949, 8481.93224816, 8482.40478411, 8481.96627135,
       8481.34086757, 8722.99646005, 8736.61137791, 8724.85719973,
       8721.86321039, 8723.91810368, 8720.82987529, 8720.19864748,
       8720.00514769]

col3 = [9157.20772734, 8481.17028812, 8479.95897581, 8481.66854465,
       8481.12688288, 8481.30670312, 8480.84656953, 8483.54011535,
       8481.81742774, 8479.23373517, 8480.44659188, 8480.90515565,
       8481.35596211, 8479.94614036, 8480.12735803, 8482.70698043,
       8481.50464731, 8725.55716505, 8712.41651697, 8737.46352274,
       8719.20402175, 8710.77791026, 8721.07604204, 8718.88881952,
       8720.0611123 , 9158.13239686, 9158.70309418, 9185.89920375,
       9189.72527817, 9153.04424809, 9152.17774172, 9148.59275477,
       9133.33557359]

df = pd.DataFrame({&quot;A&quot;:col1, &quot;B&quot;:col2, &quot;C&quot;:col3})
</code></pre>
<p>Any suggestions on that would be much appreciated. Thanks in advance.</p>
","<python><pandas><data-management>","2020-10-20 11:30:35","100","0","2","64444592","<p>Try</p>
<pre><code>df.plot(y=[&quot;A&quot;, &quot;B&quot;,&quot;C&quot;])
</code></pre>
"
"64444234","Swap parts of columns in Pandas dataframe","<p>I have to manage a dataset that looks like
<a href=""https://i.stack.imgur.com/cLOZM.png"" rel=""nofollow noreferrer"">this</a>
when I plot it. As seen from the graph, around x=17, the values of the &quot;orange&quot; column are taking the place of the data where the &quot;green&quot; column should take. Respectively, the &quot;blue&quot; data are taking the place of &quot;orange&quot; ones, and the &quot;green&quot; ones the place of &quot;blue&quot; ones. This swap occurs around x=17. Later on the graph (around x=24) the swap is different. My question is how can I get the data in the right place (columns). The swap point is not always constant so I can't just swap parts of the columns iteratively. My thought on that is that I have to check for the difference between two points. When the difference is bigger than some value, this is probably a swap point. Although this is not always the case since most of the graphs have a non-linear behavior.
A typical dataset contains many more rows so I am looking for a solution to this as much as parameterized  as possible.
This is the dataset for the above graph:</p>
<pre><code>col1 = [8724.96757035, 8720.86855769, 8713.03560178, 8711.77188717,
       8723.40967556, 8717.95864342, 8719.46206709, 8716.15746255,
       8715.83456161, 8722.05038594, 8721.822529  , 8714.29076839,
       8721.68118216, 8714.94677413, 8706.33839393, 8719.94888389,
       8715.71175774, 8480.37544428, 9151.63757245, 9138.71268152,
       9127.43234993, 9146.51437639, 9148.00997757, 9130.06677617,
       9151.43128313, 8481.34668127, 8482.40548913, 8481.96440291,
       8481.39530663, 8482.7611363 , 8481.26267875, 8480.71911933,
       8481.02279341]

col2 = [8718.4606092 , 9150.29254687, 9130.86473512, 9140.34929925,
       9142.43843709, 9158.33993226, 9148.70914607, 9164.89441174,
       9145.08470894, 9147.82723909, 9132.61236281, 9200.58503831,
       9129.96054189, 9135.65207477, 9165.43826932, 9145.35463759,
       9134.02400092, 8481.58635709, 8480.90717793, 8479.96295137,
       8483.73891949, 8481.93224816, 8482.40478411, 8481.96627135,
       8481.34086757, 8722.99646005, 8736.61137791, 8724.85719973,
       8721.86321039, 8723.91810368, 8720.82987529, 8720.19864748,
       8720.00514769]

col3 = [9157.20772734, 8481.17028812, 8479.95897581, 8481.66854465,
       8481.12688288, 8481.30670312, 8480.84656953, 8483.54011535,
       8481.81742774, 8479.23373517, 8480.44659188, 8480.90515565,
       8481.35596211, 8479.94614036, 8480.12735803, 8482.70698043,
       8481.50464731, 8725.55716505, 8712.41651697, 8737.46352274,
       8719.20402175, 8710.77791026, 8721.07604204, 8718.88881952,
       8720.0611123 , 9158.13239686, 9158.70309418, 9185.89920375,
       9189.72527817, 9153.04424809, 9152.17774172, 9148.59275477,
       9133.33557359]

df = pd.DataFrame({&quot;A&quot;:col1, &quot;B&quot;:col2, &quot;C&quot;:col3})
</code></pre>
<p>Any suggestions on that would be much appreciated. Thanks in advance.</p>
","<python><pandas><data-management>","2020-10-20 11:30:35","100","0","2","64444755","<p>One simple solution would be to sort the data, i.e. the smallest value is always in column &quot;A&quot; and the largest value is always in column &quot;C&quot;.</p>
<pre><code>df2 = pd.DataFrame(df.apply(sorted, axis=1).to_list()).rename(columns={0:'A', 1:'B', 2:'C'})
df2.plot()
</code></pre>
<p>The resulting plot would look like this:</p>
<p><a href=""https://i.stack.imgur.com/DdyyW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DdyyW.png"" alt=""enter image description here"" /></a></p>
"
"64430170","Data warehouse modelling issue","<p>I have a typical fact table in the data warehouse - the table has some surrogate keys and measures. In data, the warehouse is also lookup tables - small dimensions inside which history is not maintained. Just surrogate key, business key, and one or two attributes. During fact table load, surrogate keys are taken from lookup tables (joins are based on business keys). So basically on some stage of fact loading, we have business keys inside fact which are used to grab surrogate keys from lookup tables, after that operation, business keys are gone, and later e.g. in data mart (for reporting purposes) we can join lookups with fact using surrogate keys only for some attributes. Until now the process was rather straightforward because we used only one business key to set up attribute value.</p>
<p>But now there are cases where we should use 3 or even more.</p>
<p>E.g. these are conditions:</p>
<pre><code>COLUMN_1 = 'ABC' 
AND COLUMN_2 &lt;&gt; 'Z'
AND COLUMN_3 IN ('1', '2')

COLUMN_1 = 'ABC' 
AND COLUMN_2 &lt;&gt; 'Z'
AND  COLUMN_3 IN ('3', '4', '5')

COLUMN_1 &lt;&gt; 'ABC' OR COLUMN_2 = 'Z'
</code></pre>
<p>COLUMN_1, COLUMN_2, COLUMN_3 are business keys that are presented in the fact table.
For sure above logic will be applied in the look-up load so let's say that we will have 3 surrogate keys: 1, 2, and 3.</p>
<p>But the main questions is - which approach will be better for the fact table:</p>
<ul>
<li>To duplicate logic from look up load to fact load? Better performance but this is worse for maintenance (if this needs to be changed in the future, the change will need to be applied in two places), and also surrogate keys will need to be hardcoded.</li>
<li>Put above conditions in join condition? Obviously will be better for maintenance, however much worse for performance (fact table has around 10 000 000 rows inserted daily).</li>
<li>Or maybe there is another solution? Combining the above conditions in the source system also is not an option.</li>
</ul>
<p>All suggestions are welcome.</p>
","<sql><database><oracle><data-modeling><data-warehouse>","2020-10-19 15:11:45","111","0","1","64433320","<p>I have modelled the surrogate keys &amp; fact loads many a times in the past and in my experience (15 years) what works the best as a good balance is the following design:</p>
<p>Surrogate key table design (dim_sgk)
Dim_ID
BR1_ID1
BR1_ID2
BR1_ID3
BRn_IDx...
Record_Start_dttm</p>
<p>Assuming you have a stage table (stg_tbl) where you load your source data / file with business keys src1.col1, src1.col2 &amp; src1.col3</p>
<p>Now, when you load a surrogate key table from a stage table such that you</p>
<pre><code>Select *
from 
stg_tbl left outer join dim_sgk 
on stg_tbl.src1.col1 = dim_sgk.br1_id1
and stg_tbl.src1.col2 = dim_sgk.br1_id2
and stg_tbl.src1.col3 = dim_sgk.br1_id3
where dim sgk.dim_id is null
</code></pre>
<p>and generate (or auto based on rdbms technology and pros/cons for it) surrogate keys for all unique combinations of the 3 business keys.</p>
<p>Once, surrogate key table has been refreshed; you can begin loading your fact by joining your source transaction table with surrogate key table &amp; picking up the surrogate keys along the way (left outer join)</p>
<p>I would already keep the business keys in the fact table as a non pk attribute only for reporting purposes. There is no need to join surrogate keys with facts to only pick business keys later on. You use surrogate keys to optimize joins and distribution of data on disks. But while keeping business keys as non identifying attributes in fact you get best of both worlds.</p>
<p>There are multiple principles of surrogate keys you should keep in mind (error handling, orphan handling etc). Depending upon your rdbms technology it might make sense to partition your table based on a certain index which helps you retrieve errors / -1s and reprocess them into fact table without any performance hit. If you need to know more on this technique feel free to reach out to me. Happy to help.</p>
<p>There is a very detailed guidline I put together for another question on SGKs and you can use it for reference <a href=""https://stackoverflow.com/questions/47948372/managing-surrogate-keys-in-a-data-warehouse/48230818#48230818"">Managing surrogate keys in a data warehouse</a></p>
<p>Kind regards,
Babar</p>
"
"64419276","Weighted mean using aggregate across groups in r","<p>I want to take the weighted mean of a group using the aggregate function in r.</p>
<p>Here is what my data looks like:</p>
<pre><code>set.seed(1980)
Group_1 &lt;- sample(letters[1:4], 50, TRUE)
Group_2 &lt;- sample(letters[8:13], 50, TRUE)
Weight &lt;- sample(seq(1,50), 50, TRUE)
Value &lt;- sample(seq(1,50), 50, TRUE)

DF &lt;- data.frame(Group_1, Group_2, Weight, Value)

head(DF)
</code></pre>
<p>I want to take the weighted mean of the <code>Value</code> column, using the <code>Weight</code> column, for each pairwise group.</p>
<p>Such that, the aggregate function would look like:</p>
<pre><code>aggregate(Value ~ Group_1 + Group_2, data = df, mean)
</code></pre>
<p>How would I take the weighted mean using the aggregate function?</p>
","<r><aggregate><data-management>","2020-10-18 23:21:21","113","3","1","64419289","<p>Instead of <code>mean</code>, use <code>weighted.mean</code>.  However, <code>aggregate</code>, may not be an option here because <code>aggregate</code> loop over only the 'Value' column and it doesn't have access to the 'Weight' for each group</p>
<pre><code>library(dplyr)
DF %&gt;%
     group_by(Group_1, Group_2) %&gt;%
     summarise(wt_mean = weighted.mean(Value, Weight), .groups = 'drop')
</code></pre>
<p>-output</p>
<pre><code># A tibble: 21 x 3
# Groups:   Group_1 [4]
#   Group_1 Group_2 wt_mean
#   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;
# 1 a       h         24.7 
# 2 a       i         15   
# 3 a       j         21.1 
# 4 a       k         23.6 
# 5 a       m         14.1 
# 6 b       i         40   
# 7 b       j         12.7 
# 8 b       k          6.88
# 9 b       l         30.6 
10 b       m          5   
# … with 11 more rows
</code></pre>
<hr />
<p>If we want to use <code>base R</code>, then <code>by</code> should work</p>
<pre><code>by(DF, DF[c('Group_1', 'Group_2')], function(x) weighted.mean(x$Value, x$Weight))
</code></pre>
"
"64413808","Best approach to build factless fact in star schema DWH","<p>I have a case where I'm building factless fact table for my DWH. There are 2 dimension that I want to ask for this case: location and store. I have 2 approach.</p>
<ol>
<li><p>Building dim_store and fact_account. Then put all the location data into the fact_account table</p>
</li>
<li><p>Building dim_store, dim_location, and fact_account. Then put the store_id and location_id on the fact_account</p>
</li>
</ol>
<p>Here is the visualization for these 2 approaches:</p>
<p>1.
<a href=""https://i.stack.imgur.com/QlhB8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QlhB8.png"" alt=""FIRST APPROACH"" /></a></p>
<p>2.
<a href=""https://i.stack.imgur.com/boTw7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/boTw7.png"" alt=""SECOND APPROACH"" /></a></p>
<p>Which is the best approach and why?</p>
<p>Thank you in advance.</p>
","<data-warehouse><dimensional-modeling><fact><star-schema-datawarehouse>","2020-10-18 13:18:50","436","0","1","64424153","<p>Option 1 is definitely wrong, what is described there is not a dimensional model.</p>
<p>Option 2 is a correctly designed dimensional model. Whether it is the best way to dimensionally model your data depends on your reporting requirements.</p>
"
"64369954","Handle Changes in multiple tables used in creation of dimension","<p>I am working on data warehousing project, Need help with below</p>
<p>OLAP Table:</p>
<p><strong>Product Dimension Table:
Product_id, category_id, category_name,brand_id, brand_name ,manufacturer_id, manufacturer_name</strong></p>
<p>OLTP Tables:
Each table contains create_ts and update_ts for tracking creation &amp; update in tables.</p>
<p>**Product_info, id, product_name,category_id,brand_id,manufacturer,create_ts, update_ts</p>
<p>Product_category_mapping: id,product_id,category_id,create_ts, update_ts</p>
<p>brand: id, name,create_ts, update_ts</p>
<p>manufacturer:id, name,create_ts, update_ts**</p>
<p>Looking to track all the changes in any of the tables, should reflect in the dimension table.</p>
<p>For Example:</p>
<p>Current OLAP Snapshot</p>
<p>Product_id, category_id, category_name,brand_id, brand_name ,manufacturer_id, manufacturer_name
1,33,Noodles,45, Nestle,455,nestele_pvt_ltd</p>
<p>Suppose brand name changes from nestle to nestle-us, How will we track this as we are capturing changes based on only product_info update_ts??</p>
<p>Should we consider all the 4 table changes??</p>
<p>Please suggest.</p>
","<amazon-redshift><data-warehouse><dimensions><dimensional-modeling>","2020-10-15 10:50:45","27","0","1","64372531","<p>if data changes in any table that is a source for your DW then you need to include it in your extract logic.</p>
<p>For reference data like this where you can have a number of tables that contribute to a single &quot;target&quot; table, an approach I often take is to create a View across these tables in your source DB, include all the columns you need to take across to the DW but only have a single update_ts column that is calculated using the SQL GREATEST function where you pass in the update_ts columns from all the tables in the View. Then you only need to compare this single column to your &quot;last extracted date&quot; to determine if there are any changes that you may need to process</p>
"
"64360419","Source-to-target mapping document","<p>When we say source-to-target mapping document, does it typically include all the mappings between the different layers?</p>
<p>For example, given the following lineage:</p>
<p>source systems -&gt; staging tables -&gt; EDW -&gt; data marts</p>
<p>Would there be 3 separate mapping documents?
(i.e., 1. source systems to staging tables 2. staging tables to EDW and 3. EDW to data marts)</p>
","<mapping><data-modeling><datamodel><data-management><master-data-management>","2020-10-14 19:38:48","283","0","1","65454072","<p>It depends on how you manage your documentation but general practice is 2 different source 2 target document:</p>
<ol>
<li>Source System --&gt; EDW</li>
<li>EDW --&gt; Data Mart</li>
</ol>
"
"64325869","Can you extract a dimension table from a fact table?","<p>Here's the situation, in the source database we have more than 600K active rows for a dimension but in reality the business only uses 100 of them.</p>
<p>Unfortunately the list of values that they might use is not known and we can't manually filter on those values to populate the dimension table.</p>
<p>I was thinking, what if I include the dimension columns for that table in the fact table and then when we send that to staging area, just seperate it from the fact and send it to it's own table.</p>
<p>This way, I will only capture the values that are actually used.</p>
<p>P.S. They have a search function in the application that help users navigate through 600K values. it's not like a drop-down field !</p>
<p>Do you have a better recommendation?</p>
","<data-warehouse><snowflake-schema>","2020-10-12 21:59:04","314","0","1","64334596","<p>Yes - you could build the Dimension from the fact staging table. A couple of things to consider:</p>
<ol>
<li>If the only attribute for the Dimension is the field in the fact staging table then you can keep this as a degenerate dimension in the fact table; no need to build a dimension table for it - unless you have other requirements that require a standalone dimension table, such as your BI tool needs it.</li>
<li>If there are other attributes you need to include in the dimension then you are still going to need to bring in the source dimension table - but you can filter it using the the values in the fact staging table and only load the used values into your dimension</li>
</ol>
"
"64308440","How to Build an ODS layer?","<p>sorry for my noob question
I'm working on a project where I have to take data from an excel source and build a small DW from it. so I was thinking of these steps
Source &gt; staging&gt; OLETP &gt; ODS &gt; DW
in the requirements, I have to create an ODS layer so I'm not sure whats my process will be and what exactly needs to be inside the ODS, can you please let me know what steps should I take?
Thanks</p>
","<ssis><etl><data-warehouse>","2020-10-11 19:57:56","166","0","1","64316053","<p>As mater of fact ODS Operational Data Store(source) is a database that you need to create every relation or check constraint or any checks in DB for your business to check all data that you want use in data warehouse in this layer you must design database like Normal DB but not normalize and you can get any redundancy that you want.pay attention this design is not base on fact or dimension and it just wants to cleans data for data warehouse</p>
"
"64286481","collecting annual aggregated data for later quick access","<p>I have a number of sql queries which take <code>year</code> as a parameter and generate various annual reports for the given year.</p>
<p>Those queries are quite cumbersome and take a considerable amount of time to execute (20 min - 40 min).</p>
<p>In order to give my users the ability to view annual report whenever they need to, I am considering to pre-execute these queries and store the results for later use.</p>
<p>One solution would be to schedule execution of these queries and insert the results in some temp tables.</p>
<p>But I am looking for more clever approach one that would not involve writing dozens of execute immediate  statements or custom inserts for all these queries.</p>
<p>Any idea would be appreciated. Also I don't know if materialized views can be used to that end.</p>
<p>expected result would be a table or a view with a year column so that a user could execute quick search for any year.</p>
<p>e.g.</p>
<pre><code>product_id |annual_sales|max_price|min_price|year
124|1200,56|80|50|2019
124|1400,00|85|55|2020
</code></pre>
","<sql><oracle><scheduled-tasks><data-warehouse><materialized-views>","2020-10-09 19:54:47","42","0","1","64286931","<p>A materialized view would be a great option for what you are looking to do. This way you can write the query once for the view, then have the data in the materialized view refresh as often as you'd like. You can have a job that refreshes the data once per night, on the weekends, or whatever frequency you choose.</p>
<p>After the materialized view is created, you can also add indexes on top of the materialized view to assist with query performance if you so choose.</p>
<p>A quick example on how to create a materialized view can be seen below.</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE sale
(
    product_id     NUMBER,
    sale_date      DATE,
    sale_amount    NUMBER
);

INSERT INTO sale (product_id, sale_date, sale_amount)
     VALUES (124, DATE '2019-02-01', 40.25);

INSERT INTO sale (product_id, sale_date, sale_amount)
     VALUES (124, DATE '2019-02-01', 80.99);

INSERT INTO sale (product_id, sale_date, sale_amount)
     VALUES (124, DATE '2020-02-01', 30.50);

INSERT INTO sale (product_id, sale_date, sale_amount)
     VALUES (124, DATE '2020-02-01', 46.75);

CREATE MATERIALIZED VIEW sales_summary
BUILD IMMEDIATE
REFRESH FORCE ON DEMAND
AS
      SELECT product_id,
             SUM (sale_amount)                 AS annual_sales,
             MAX (sale_amount)                 AS max_price,
             MIN (sale_amount)                 AS min_price,
             EXTRACT (YEAR FROM sale_date)     AS year
        FROM sale
    GROUP BY product_id, EXTRACT (YEAR FROM sale_date);
</code></pre>
<p>Result</p>
<pre class=""lang-sql prettyprint-override""><code>select * from sales_summary;

   PRODUCT_ID    ANNUAL_SALES    MAX_PRICE    MIN_PRICE    YEAR
_____________ _______________ ____________ ____________ _______
          124          121.24        80.99        40.25    2019
          124           77.25        46.75         30.5    2020
</code></pre>
"
"64275722","How are fact tables formed in relation to the dimension tables?","<p>I am trying to understand how fact tables are form in relation to the dimension tables.</p>
<p>E.g. Sale Fact Table
For there is a query for Sale of product by year/month/week/day, do I create a dimension for each type of period: Dim_Year, Dim_Month, Dim_Week and Dim_Day, each with their own respective keys?
Or is it possible to just use one dimension for all periods: Dim_Date and only have one date key?</p>
<p>Another area I am confused about is that why do some fact tables not contain their own ID? E.g. Sale fact table does not have SaleID included in the fact table.</p>
<p><a href=""https://i.stack.imgur.com/TVnYV.png"" rel=""nofollow noreferrer"">Sale Fact Table Textbook Example</a></p>
","<database><data-warehouse><star-schema><fact-table>","2020-10-09 07:30:51","207","0","1","64279521","<p><strong>DATES</strong></p>
<p>Your date dimension needs to correspond to the grain of your fact table. So if you had daily sales you would have a Dim_Day, weekly sales you would have a Dim_Week, etc.</p>
<p>You would normally have multiple date dimensions (at different grains) in your data warehouse as you would have facts at different date grains.</p>
<p>Each date dimension would hold hold attributes applicable to levels higher up in the date hierarchy. So a Dim_Day might hold day, week, month, year attributes; Dim_Month might hold month, quarter and year attributes, etc.</p>
<p><strong>PRIMARY KEYS</strong></p>
<p>Primary keys are rarely (never?) a technical requirement when creating tables in a database i.e. you can create a table without defining a PK. So you need to consider why we normally (at least in OLTP DBs) include PKs. Common reasons include:</p>
<ul>
<li>To easily identify an individual record</li>
<li>To ensure that duplicate records (those with the same PK value) are
not created</li>
</ul>
<p>So there are good reasons for creating PKs, however there are cost overheads e.g. the PK needs to be checked every time a new record is inserted into the table.</p>
<p>In a dimensional model where you are performing bulk inserts/updates, having PKs would cause a significant performance hit. Additionally, the insert logic/checks should always be implemented in your ETL processes so there is no need to include these types of checks/constraints in the DB itself.</p>
<p>Fact tables do have a primary key but it is often implicit rather than explicit - so a group of the FKs in the fact table uniquely identify each record. This compound PK may be documented but is is never enabled/implemented.</p>
<p>Occasionally a fact table will have an explicit, single column, PK. This is normally used when the fact table needs to be updated and its implicit PK involves a large number of columns. There is normally logic required to identify the record to be updated using its FKs but this returns the PK; then the update statement just has a clause like this:</p>
<pre><code>WHERE table_pk = 12345678
</code></pre>
<p>rather than having to include all the columns in the implicit PK:</p>
<pre><code>WHERE table_sk1 = 1234
AND table_sk2 = 5678
AND table_sk3 = 9876
....
</code></pre>
<p>Hope this helps?</p>
"
"64264434","Temporal Tables and Time Dimension in SQL Data Warehouse and Tableau","<p>I am in the beginning stages of redesigning our data warehouse and found temporal tables to be a pretty awesome way to do SCD type 4. This iteration of the data warehouse will feature a date dimension so that we can go to points in time comparisons using business logic (comparing specific days in fiscal quarters for example). The date dimension would have all sorts of cool business time marks like fiscal year/quarter/month and day in month/quarter/year plus all sorts of other start and end date flags signaling different business processes and 'seasons'.</p>
<p>The only problem I see is that I don't see how you can leverage the date dimension in a query without always using separate date fields and executing the query as 'for system_time all'. This is because you can't really do any manipulation with the date after the 'for system_time' statement.</p>
<p>Now I understand you could see the date to be a variable and you could do your manipulations when setting the variable. The problem I have is that Tableau doesn't allow you to execute sql scripts (like power BI does), just individual statements. So, I can't have a scenario where we set the date as a variable based on a query result. Because of this, I am not sure how I'd run a temporal query to say let's get the values on this particular day of a particular fiscal quarter. Hope that makes sense.</p>
<p>If I have to manually create the history tables using triggers I will, but I assume there is a better solution that my Googling has yet to find. Any ideas or articles you can point me to?</p>
<hr />
<p>edit adding example:</p>
<p>Let's for example say we have a date dimension (dateTable) that has a few fields:</p>
<pre><code>date
fiscalYear
fiscalDayOfYear
</code></pre>
<p>Now let's say I have a table (statusTable) with a few fields:</p>
<pre><code>userid
status
date (FK to date dimension)
</code></pre>
<p>Now let's say I want to compare a specific user status' on the 20th day of the fiscal year from 2019 and 2020. Without using temporal tables it would look like:</p>
<pre><code>select st.userid, st.status from statusTable st inner join dateTable dt on st.date = dt.date where dt.fiscalDayOfYear = '20' AND (dt.fiscalYear = '2020' or dt.fiscalYear = '2019')
</code></pre>
<p>Now if statusTable were a temporal table, it would look like this instead:</p>
<pre><code>userid
status
systemtimeFrom (generated by temporal table)
systemtimeTo (generated by temporal table)
</code></pre>
<p>How would I do what I do in the query above with the temporal table? I've seen examples of people putting in field 'date' into the statusTable in addition to the system time fields. They have that value be automatically generated. So, in theory with that, I could join just like I did in the query above. The problem I see though is that if I query the temporal table without temporal constraints, it will only look at the latest status (since that's all that there is in the table). If I want to include the history AND the current table, I think I need to do a select that contains 'FOR SYSTEM_TIME ALL' which I feel removes the benefit of the temporal tables (since you can't do the cool time slicing temporal queries).</p>
<p>What I am looking to find out if there is a way to do the above query with a temporal table using some kind of magic I am not aware of. I'd love to be able to do something like (and I know this isn't valid SQL since you can only supply a date or variable in the as of clause):</p>
<pre><code>select st.userid, st.status from statusTablest st for system_time as of (select date from datetable where dt.fiscalDayOfYear = '20' AND (dt.fiscalYear = '2020' or dt.fiscalYear = '2019'))
</code></pre>
<p>Sorry I know isn't a good example. I don't have much in the way of examples since I am designing this from scratch. I hope I get what I am trying to do across. Let me know if not and I can try to say it in a different way.</p>
","<tsql><tableau-api><data-warehouse><temporal-tables>","2020-10-08 14:14:58","430","0","1","64279142","<p>thanks for the example and additional information. I realise that trying to come up with a real-world example for these sorts of questions is never easy but I need to point out that your example is not compatible with a dimensional model as you would never join dimensions together like that and use one dimension to filter another - dimensions are joined to, and are used to filter, fact tables.</p>
<p>However, with that caveat stated, to look at your example...</p>
<p>The non-temporal table you've shown is not going to support the type of query you want to run as it would require a record for every single date/user/status combination you might want to query for - which is obviously unsupportable. Instead you would need effective start and end dates to show the date period each combination of userid and status was applicable for. If you had these dates then your query would probably look something like this (an untested example as I don't have data to run it against):</p>
<pre><code>select st.userid, st.status, st.ValidFrom, st.ValidTo 
from statusTable st 
inner join dateTable dt1 on st.ValidFrom = dt1.date 
inner join dateTable dt2 on st.ValidTo = dt2.date 
where 
(dt1.fiscalDayOfYear &lt;= '20' AND dt2.fiscalDayOfYear &gt;= '20') -- your fiscal day is within the coverage of the statusTable record
AND
(dt1.fiscalYear = '2020' or dt1.fiscalYear = '2019') -- limit to the 2 years in question
AND
(dt1.fiscalYear = dt2.fiscalYear); -- Attempt to filter to a single record.Probably incorrect if the ValidFrom and ValidTo dates for a record are in different years; required logic more complex than this 
</code></pre>
<p>So you basically have to have an SCD2-type table in order to be able to run these types of query.</p>
<p>A temporal table also has ValidFrom and ValidTo columns - so it is fundamentally the same as a traditional SCD2 table; the difference being that an SCD2 table has to be maintained using ETL logic whereas a temporal table is maintained automatically by the DB engine.</p>
<p>The logic for querying an SCD2 table and an equivalent temporal table is going to be almost identical. The main difference would be the use of the</p>
<pre><code>FOR SYSTEM_TIME ...
</code></pre>
<p>clause - but all this is going to do is subset the data from the statusTable being used in the query and to do that you would need to be able to construct, and pass into the query, suitable start and end date parameters. If you can't do this (or the time period is so wide that producing the subset is unlikely to benefit the query) then you'd just use:</p>
<pre><code>FOR SYSTEM_TIME ALL
</code></pre>
<p>So to hopefully clarify/answer some of the points you raised:</p>
<ul>
<li>Temporal tables are a way of automatically storing history rather than having to manually maintain SCD2 tables - but the end result is effectively the same</li>
<li>Your statement &quot;<em>I think I need to do a select that contains 'FOR SYSTEM_TIME ALL' which I feel removes the benefit of the temporal tables (since you can't do the cool time slicing temporal queries).</em>&quot; indicates a possible misunderstanding regarding the point of temporal tables. Their benefit is that they hold history automatically and allow you to query it. The &quot;<em>the cool time slicing temporal queries</em>&quot; are enabled by your date dimension, not by your temporal tables, and therefore you still need to join your temporal tables to the date dimension in order to get this to work</li>
</ul>
"
"64212097","Snowflake → Zapier Integration","<p>I'm using Zapier with Redshift to fetch data from custom queries and trigger a wide array of actions when new rows are detected from either a table or custom query, including sending emails through Gmail or Mailchimp, exporting data to Google Sheets, and more. Zapier's UI enables our non-technical product stakeholders to take over these workflows and customize them as needed. Zapier has several integrations built for Postgres, and since Redshift supports the Postgres protocol, these custom workflows can be easily built in Zapier.</p>
<p>I'm switching our data warehouse from Redshift to Snowflake and the final obstacle is moving these Zapier Integrations. Snowflake doesn't support the Postgres protocol so it cannot be used as a drop in replacement for these workflows. No other data source has all the information that we need for these workflows so connecting to an upstream datasource of Snowflake is not an option. Would appreciate guidance on alternatives I could pursue, including the following:</p>
<ol>
<li>Moving these workflows into application code</li>
<li>Using a foreign data wrapper in Postgres for Snowflake to continue using the existing workflows from a dummy Postgres instance</li>
<li>Using custom-code blocks in Zapier instead of the Postgres integration</li>
</ol>
","<snowflake-cloud-data-platform><data-warehouse><zapier>","2020-10-05 15:55:41","558","1","2","64614647","<p>I'm not sure if Snowflake has an API that will allow you to do what you want, but you can create a private Zapier Integration that will have all the same features and permissions as a public integration, but you can customize it for your team.</p>
<p>There's info about that process here: <a href=""https://platform.zapier.com/"" rel=""nofollow noreferrer"">https://platform.zapier.com/</a></p>
"
"64212097","Snowflake → Zapier Integration","<p>I'm using Zapier with Redshift to fetch data from custom queries and trigger a wide array of actions when new rows are detected from either a table or custom query, including sending emails through Gmail or Mailchimp, exporting data to Google Sheets, and more. Zapier's UI enables our non-technical product stakeholders to take over these workflows and customize them as needed. Zapier has several integrations built for Postgres, and since Redshift supports the Postgres protocol, these custom workflows can be easily built in Zapier.</p>
<p>I'm switching our data warehouse from Redshift to Snowflake and the final obstacle is moving these Zapier Integrations. Snowflake doesn't support the Postgres protocol so it cannot be used as a drop in replacement for these workflows. No other data source has all the information that we need for these workflows so connecting to an upstream datasource of Snowflake is not an option. Would appreciate guidance on alternatives I could pursue, including the following:</p>
<ol>
<li>Moving these workflows into application code</li>
<li>Using a foreign data wrapper in Postgres for Snowflake to continue using the existing workflows from a dummy Postgres instance</li>
<li>Using custom-code blocks in Zapier instead of the Postgres integration</li>
</ol>
","<snowflake-cloud-data-platform><data-warehouse><zapier>","2020-10-05 15:55:41","558","1","2","73242781","<p>You might find it easier to use a vendor solution like Census to forward rows as events to Zapier. Their free plan is pretty sizeable for getting started. More info here <a href=""https://www.getcensus.com/integrations/zapier"" rel=""nofollow noreferrer"">https://www.getcensus.com/integrations/zapier</a></p>
"
"64207587","JDBC connection string for SQL DW","<p>how can i use access token in JDBC connection string to connect to Azure SQL DW ?</p>
","<sql><azure><jdbc><access-token><data-warehouse>","2020-10-05 11:18:32","368","0","1","64218677","<p>Please reference this Azure tutorial: <a href=""https://learn.microsoft.com/en-us/sql/connect/jdbc/connecting-using-azure-active-directory-authentication?view=sql-server-ver15#connecting-using-access-token"" rel=""nofollow noreferrer"">Connecting using access token</a>:</p>
<ul>
<li>Applications/services can retrieve an access token from the Azure
Active Directory and use that to connect to Azure SQL Database/Data
Warehouse.</li>
</ul>
<p>The example below contains a simple Java application that connects to Azure SQL Database/Data Warehouse using access token-based authentication.</p>
<p>Code example:</p>
<pre><code>import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.Statement;
import java.util.concurrent.Executors;
import java.util.concurrent.Future;
import com.microsoft.sqlserver.jdbc.SQLServerDataSource;

// The azure-activedirectory-library-for-java is needed to retrieve the access token from the AD.
import com.microsoft.aad.adal4j.AuthenticationContext;
import com.microsoft.aad.adal4j.AuthenticationResult;
import com.microsoft.aad.adal4j.ClientCredential;

public class AADTokenBased {

    public static void main(String[] args) throws Exception {

        // Retrieve the access token from the AD.
        String spn = &quot;https://database.windows.net/&quot;;
        String stsurl = &quot;https://login.microsoftonline.com/...&quot;; // Replace with your STS URL.
        String clientId = &quot;1846943b-ad04-4808-aa13-4702d908b5c1&quot;; // Replace with your client ID.
        String clientSecret = &quot;...&quot;; // Replace with your client secret.

        AuthenticationContext context = new AuthenticationContext(stsurl, false, Executors.newFixedThreadPool(1));
        ClientCredential cred = new ClientCredential(clientId, clientSecret);

        Future&lt;AuthenticationResult&gt; future = context.acquireToken(spn, cred, null);
        String accessToken = future.get().getAccessToken();

        System.out.println(&quot;Access Token: &quot; + accessToken);

        // Connect with the access token.
        SQLServerDataSource ds = new SQLServerDataSource();

        ds.setServerName(&quot;aad-managed-demo.database.windows.net&quot;); // Replace with your server name.
        ds.setDatabaseName(&quot;demo&quot;); // Replace with your database name.
        ds.setAccessToken(accessToken);

        try (Connection connection = ds.getConnection(); 
                Statement stmt = connection.createStatement();
                ResultSet rs = stmt.executeQuery(&quot;SELECT SUSER_SNAME()&quot;)) {
            if (rs.next()) {
                System.out.println(&quot;You have successfully logged on as: &quot; + rs.getString(1));
            }
        }
    }
}
</code></pre>
<p>HTH.</p>
"
"64200295","Firestore database time-series data modelling","<p>I would like to store time-series data in firestore database in a way that would allow to retrieve and update it as efficiently as possible by utilizing firebase filtering capabilities and best practices.</p>
<p>In terms of actual data that will be stored is daily data points that will have a corresponding single numeric value, e.g. 1580651062 - 45.</p>
<p>The following is an example of the initial firestore structure that needs to be followed:</p>
<p><em><strong>/exampleData/userId/accountId/</strong></em> &lt;-- and from here on I would like to store time-series data.</p>
<p>Example json structure:</p>
<pre><code>&quot;exampleData&quot;: {
    &quot;123&quot;: { // accountId
        &quot;1546300800&quot;: { // 'yearKey' - 2019/01/01/00:00:000
            &quot;1548979200&quot;: [ // 'monthKey' - 2019/02/01/00:00:000, that contains daily data of 2019/02/x/xx:xx:xxx
                {
                    &quot;timestamp&quot;: 1549070411,
                    &quot;value&quot;: 20
                },
                {
                    &quot;timestamp&quot;: 1549077675,
                    &quot;value&quot;: 10
                }
            ],
            &quot;1551398400&quot;: [ // 'monthKey' - 2019/03/01/00:00:000, that contains daily data of 2019/03/x/xx:xx:xxx
                {
                    &quot;timestamp&quot;: 1551669675,
                    &quot;value&quot;: 22
                }
            ]
        },
        &quot;1577836800&quot;: { // 'yearKey' - 2020/01/01/00:00:000
            &quot;1577836800&quot;: [ // 'monthKey' - 2019/02/01/00:00:000, that contains daily data of 2019/02/x/xx:xx:xxx
                {
                    &quot;timestamp&quot;: 1580651062,
                    &quot;value&quot;: 33
                }
            ]
        }
    }
}
</code></pre>
<p>Based on the above I have the following as a potential solution:</p>
<pre><code>/exampleData
    /userId(Document) 
        /797499614(Collection)
            /randomDocId
                /yearlyData(Collection)
                    /randomDocId
                        /monthlyData(Collection)
                            /randomDocId
                                /dailyData(Collection)
                                    /randomDocId
</code></pre>
<p>where yearlyData collection contain yearKey's and corresponding monthlyData collections which then contain monthKey's and corresponding dailyData collection that contains separate document for each data point within which dayKey and a value. In theory, having the latter structure should allow to perform granular filtering and allow to minimize the amount of data is downloaded to the client before processing, e.g.</p>
<pre><code>db.collection(&quot;exampleData&quot;)
  .doc(&quot;userId&quot;)
  .collection(&quot;accountId&quot;)
  .collection(&quot;yearlyData&quot;)
  .where(&quot;yearKey&quot;, &quot;&gt;=&quot;, 30)
  .where(&quot;yearKey&quot;, &quot;&lt;=&quot;, 40)
  .getMonthlyDataCollections()
  .where(&quot;monthKey&quot;, &quot;&gt;=&quot;, 20)
  .where(&quot;monthKey&quot;, &quot;&lt;=&quot;, 30)
  .getDailyDataCollections()
  .where(&quot;dayKey&quot;, &quot;&gt;=&quot;, 5)
  .where(&quot;monthKey&quot;, &quot;&lt;=&quot;, 10)
  .get()
  .then(data =&gt; {
       process(data);
  });
</code></pre>
<p>Is there a better way to achieve the requirement than the above solution?</p>
<p>If I was to go with the above solution, what would be the example implementation of the &quot;.getMonthlyDataCollections()&quot; and &quot;.getDailyDataCollections()&quot; from the above firestore query? Meaning, does firestore api provide a kind of &quot;join&quot; like option to actually achieve the above without having to pull all data to the client?</p>
<p>Thanks.</p>
","<javascript><firebase><google-cloud-firestore>","2020-10-04 21:57:55","273","0","1","64200455","<blockquote>
<p>does firestore api provide a kind of &quot;join&quot; like option to actually achieve the above without having to pull all data to the client?</p>
</blockquote>
<p>Firestore queries do not provide any sort of SQL-like joins.  Every query can consider only a single collection at a time.  The only exception to this is collection group queries, where you can query among all collections with the same name.</p>
<p>As such, it's common in Firestore (and all NoSQL type databases) to duplicate data among multiple collections in order to satisfy for easier querying.  This is called &quot;denormalization&quot;.</p>
"
"64166055","How does Snowflake do instantaneous resizes?","<p>I was using Snowflake and I was surprised how it is able to do instantaneous resizes. Here is a very <a href=""https://gyazo.com/4af7d9f1c9415521efd300e7261aa80d"" rel=""nofollow noreferrer"">10s video</a> of how it instantly does a resize, and the query is still 'warm' the next time it is run (Note I have a <code>CURRENT_TIMESTAMP</code> in the query so it never returns from cache):</p>
<p><a href=""https://i.stack.imgur.com/soqsf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/soqsf.png"" alt=""enter image description here"" /></a></p>
<p>How is Snowflake able to do instantaneous resizes (completely different than something like Redshift)? Does this mean that it just has a fleet of servers that are always on, and a resize is just a virtual allocation of memory/cpu to run that task? Is the underlying data stored on a shared disk or in memory?</p>
","<snowflake-cloud-data-platform><data-warehouse>","2020-10-02 04:11:16","209","2","2","64167580","<p>To answer your question about resizing in short: Yes, you are absolutely right.</p>
<p>As far as I know Snowflake manages a pool of running servers in the background. All customers can be assigned something from here.
Consequence: A resize for you from S to XS is a reallocation of a server from that pool.</p>
<p>Most probably the Virtual Private Snowflake-Edition behaves differently as those accounts don't share resources (e.g. Virtual Warehouses) with other accounts (outside that VPS). More infos: <a href=""https://docs.snowflake.com/en/user-guide/intro-editions.html#virtual-private-snowflake-vps"" rel=""noreferrer"">https://docs.snowflake.com/en/user-guide/intro-editions.html#virtual-private-snowflake-vps</a></p>
<p>Regarding your storage-question:
Snowflake's storage layer is basically a storage service, e.g. Amazon S3. In here Snowflake saves the data in columnar format, to be more presice in micro-partitions. More information regarding micro-partitions can be found here: <a href=""https://docs.snowflake.com/en/user-guide/tables-clustering-micropartitions.html"" rel=""noreferrer"">https://docs.snowflake.com/en/user-guide/tables-clustering-micropartitions.html</a></p>
<p>Your virtual warehouse accesses this storage layer (remote disk) or - if the query was run before - a cache. There are a local disc cache (this is your virtual warehouse using SSD-storage) and a result cache (available across virtual warehouses for queries within the last 24 hours): <a href=""https://community.snowflake.com/s/article/Caching-in-Snowflake-Data-Warehouse"" rel=""noreferrer"">https://community.snowflake.com/s/article/Caching-in-Snowflake-Data-Warehouse</a></p>
"
"64166055","How does Snowflake do instantaneous resizes?","<p>I was using Snowflake and I was surprised how it is able to do instantaneous resizes. Here is a very <a href=""https://gyazo.com/4af7d9f1c9415521efd300e7261aa80d"" rel=""nofollow noreferrer"">10s video</a> of how it instantly does a resize, and the query is still 'warm' the next time it is run (Note I have a <code>CURRENT_TIMESTAMP</code> in the query so it never returns from cache):</p>
<p><a href=""https://i.stack.imgur.com/soqsf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/soqsf.png"" alt=""enter image description here"" /></a></p>
<p>How is Snowflake able to do instantaneous resizes (completely different than something like Redshift)? Does this mean that it just has a fleet of servers that are always on, and a resize is just a virtual allocation of memory/cpu to run that task? Is the underlying data stored on a shared disk or in memory?</p>
","<snowflake-cloud-data-platform><data-warehouse>","2020-10-02 04:11:16","209","2","2","67339273","<p>To extend the existing answer, <code>ALTER WAREHOUSE</code> in standard setup is non-blocking statement, which means it returns control as soon as it is submitted.</p>
<blockquote>
<p><a href=""https://docs.snowflake.com/en/sql-reference/sql/alter-warehouse.html"" rel=""nofollow noreferrer"">ALTER WAREHOUSE</a></p>
<pre><code>WAIT_FOR_COMPLETION = FALSE | TRUE
</code></pre>
<p><strong>When resizing a warehouse, you can use this parameter to block the return of the ALTER WAREHOUSE command until the resize has finished provisioning all its servers.</strong> Blocking the return of the command when resizing to a larger warehouse serves to notify you that your servers have been fully provisioned and the warehouse is now ready to execute queries using all the new resources.</p>
<p>Valid values</p>
<ul>
<li><p>FALSE: The ALTER WAREHOUSE command returns immediately, before the warehouse resize completes.</p>
</li>
<li><p><strong>TRUE: The ALTER WAREHOUSE command will block until the warehouse resize completes.</strong></p>
</li>
</ul>
<p>Default:    FALSE</p>
</blockquote>
<p>For instance:</p>
<pre><code>ALTER WAREHOUSE &lt;warehouse_name&gt; SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE;
</code></pre>
<hr />
<p><strong>EDIT:</strong></p>
<p><a href=""http://info.snowflake.net/rs/252-RFO-227/images/Snowflake_SIGMOD.pdf"" rel=""nofollow noreferrer"">The Snowflake Elastic Data Warehouse</a></p>
<blockquote>
<p>3.2.1  Elasticity and Isolation VWs are <strong>pure compute resources.</strong></p>
<p>They can be created,destroyed, or resized at any point, on demand.  Creating or destroying a VW has no effect on the state of the database. It is perfectly legal (and encouraged) that users shut down all their  VWs  when  they  have  no  queries.   This  elasticity allows users to dynamically match their compute resources to usage demands, independent of the data volume.</p>
</blockquote>
"
"64138980","Data modeling in columnar database vs multi-dimensional for reporting","<p>In my way of learning Redshift (my first columnar database), I am struggling to figure out the approach for designing the model. Columnar database does promote flat table design, yet admits that star schema or snowflake could be a better choice for some cases.</p>
<p>Here is a simple example of where I am struggling</p>
<p><a href=""https://i.stack.imgur.com/XxrV0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XxrV0.png"" alt=""enter image description here"" /></a></p>
<p>As you can see multi-dimensional approach have few dimensions and 1 fact table. I could have made it snowflake design but I kept it simple for star schema.</p>
<p>Approach 1: Used common columns from tables (in this scenario demographics). This could reduce the table size for Customer &amp; Store but will include the extra dimension.</p>
<p>Approach 2: Flat table design with all the columns</p>
<p>My Questions:</p>
<ol>
<li>Which approach data modeler use to design data model in columnar databases like Redshift? Or they use different approach?</li>
<li>Considering this example, what is the best way to design a data model for data warehousing.</li>
<li>Which approach is good for reporting (considering that client PC\Laptop would have limited memory). Or even cloud reporting may become costly when heavy data set is used.
Approach 3 will produce a massive amount of data set for reporting. This could be a costly affair if doing reporting (using Power BI or Tableau or any other Self reporting tool)
Multidimenion approach is best for self reporting (cost &amp; performance) but then it defeats the purpose of columnar database.
Approach 1 is also good for reporting but with more joins &amp; complexity.</li>
</ol>
","<amazon-redshift><data-modeling><data-warehouse><dimensions>","2020-09-30 13:47:51","1955","2","1","64223187","<p>Sorry, late to the party.</p>
<p>I will post is as answer, because it is too long for a comment.</p>
<p>I saw in chat that test results show that star schema is better. But it was tested on regular (MSSQL), not columnar database (just as vertica, redshift, snowflake, bigquery..).</p>
<p>There is some experience from project implementation where I tested both approaches - OBT and star schema while implementing dwh for reporting. Ths was already more than 2 years ago, so don't expect much details.
Database: Redshift 2 nodes of dc2.8xlarge. Might be a bit overkill, but other option was to have a bunch of lower level nodes, which wouldn't be more cost efficient. This example will be just for one data area.</p>
<p>Data: ~ 6 tables which could be joined as somewhat similar to star schemas. Containing of 3 fact tables and based on denormalization level 5-8 dimensions.</p>
<p>With various approaches and different optimization paths, using star schema it would be common to reach SQL times to about 30 seconds. Which is not bad, but also not too responsive from user perspective.
SQLs on flat denormalized fact tables rarely exceed 5 seconds. Some tables contain more than 100 columns, row counts are between 50M and 100M. To not overcomplicate, we use zstd compression for all columns.
In columnar databases data compresses very well as many similar or same values are used in single column.</p>
<p>We took OBT table approach and there are some pros and cons:</p>
<p>pros:</p>
<ol>
<li>Responsive reports in reporting tool (most important one)</li>
<li>Fewer objects for ETL developers to handle.</li>
<li>Analysts which query database directly can create simpler queries using less tables.</li>
<li>Don't need to worry about data inconsistencies if some dimensions are outdated,  which could happen in star schema.</li>
<li>Easier approach for reporting tool cache clearing.</li>
<li>Easier reporting performance tuning.</li>
<li>Easier modeling in reporting tool, do not need to define table join strategies.</li>
</ol>
<p>cons:</p>
<ol>
<li>Might take more space. Didn't really tested this closely as storage space is not an issue for us.</li>
<li>Filters in reporting tools might take a bit longer to provide list of values (select distinct one_column from table)</li>
<li>Table refresh might take a bit longer for one big table compared to multiple smaller tables.</li>
</ol>
<p>Hopefully this helps.</p>
"
"64076132","Javascript data mapping","<p>Can someone explain to me how can I implement in a clean way a solution to get a status (string) from combining two other ones?
I need to declare a function which takes two params (two strings) and needs to return another one based on the combination of those two strings.
For ex:</p>
<pre><code>carStatus(status, secondaryStatus) =&gt; string
</code></pre>
<p>where secondaryStatus can have multiple options.
I'm thinking using an if/else if statement which returns a third status which I need.
For ex when status is 'OPEN' and secondaryStatus is 'payment1' or 'payment2' or 'payment3', the function must return a new string (status) like 'CONFIRMED'.
So, an example of how I'm thinking to implement at this moment would be something like this:</p>
<pre><code>carStatus = (status, secondaryStatus) =&gt; {
  if(status === 'OPEN' &amp;&amp; (secondaryStatus === 'payment1' || 'payment2' || 'payment3')){
    return 'CONFIRMED';
  } else if(status === 'CANCELLED' &amp;&amp; (secondaryStatus === 'payment4' || 'payment5' || 'payment6')){
    return 'REMOVED';
  } else if(status === 'REVIEW' &amp;&amp; (secondaryStatus === 'payment2' || 'payment5' || 'payment5')){
    return 'CHECKED';
  }
}

&lt;div&gt;carStatus('OPEN', 'payment1')&lt;/div&gt;
</code></pre>
<p>In div must be rendered 'CONFIRMED'.</p>
<p>In my implementation, I'll have to write I think other 5 else if statements.. so maybe there is a cleaner way to implement this.</p>
<p>My project is written in React, but I'm thinking to put this function in utils folder. Perhaps a solution written in React could be more clean? I don't know.</p>
<p>Any help will be appreciated.</p>
","<javascript><reactjs>","2020-09-26 09:44:26","51","-1","1","64076333","<p>You can probably use a lookup object for the various options. Since criteria given is very vague following is a very basic example</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>const carStatus = (carState, carSecondaryState) =&gt; {
  const secondaries = {
    ORDERED: 'CLIENT'
  }
  return `${secondaries[carSecondaryState]}_${carState}`
}

console.log(carStatus('NEW', 'ORDERED'))</code></pre>
</div>
</div>
</p>
"
"64055943","SCD2 WITH FACT TABLE IMPLEMENTATION","<p>I am asked to build a client dimension and a bed dimension .</p>
<p>and bring them together in the sense of clientID-SK,bedID_SK,Bed_begin_date,bed_end-date.Both tables contains SCD1, and SC2 fields.How do I implement this if the dates the clients was and out off bed and out  has nothing to do with what defines as a client or bed(types).</p>
<p>I have been able to combine them but my challenge is that when I load them into a fact table the
table only has the begin_date .How will I update the fact table end_date which is suppose to = the begin_date of the next bed assignment.</p>
<p>e.g
clientID,bedID,Start_Date,End_Date
10      ,ROO1, ,01-19-2020, 3000-01-01 00:00:00.000</p>
<p>Dimension
10      ,ROO1, ,01-19-2020, 10-19-2020
10      ,ROO2, ,10-19-2020, 3000-01-01 00:00:00.000</p>
<p>We have a table called current bed that keeps track of our current client and I was able to build a slowly changing dimension off that table.</p>
<p>But we are concerned to follow standard practice we have to have a star schema in place .</p>
<p>Any suggestion</p>
","<sql><data-warehouse>","2020-09-24 23:58:43","2139","-2","1","64062961","<p>So you have, at least, the following tables:</p>
<ol>
<li>Client Dimension holding all the client attributes</li>
<li>Bed Dimension holding all the Bed attributes</li>
<li>A Date Dimension</li>
<li>A Bed Occupancy Fact with FKs to Client Dim, Bed Dim and 2 FKs to Date Dim (one for Bed occupied and one for bed vacated)</li>
</ol>
<p>When a bed is first occupied by a client you create a new fact record and populate the Client, Bed and Date Occupied FKs. You populate the Bed Vacated with 0 (or whatever key value you have used in the Date Dim to indicate the 'unknown' record).</p>
<p>When a bed is next occupied, you create a new fact record for the new client and update the Bed Vacated FK on the previous record with the relevant Date key.</p>
<p>A few things to think about:</p>
<ol>
<li>Are you only working at the Date level of granularity or at Time level i.e. are you interested in what time of day (or morning/afternoon, etc.) when a bed was occupied/vacated?</li>
<li>I would ensure that the Date Vacated of the previous occupancy and the Date Occupied of the current one are not the same value otherwise you can get double counting on that overlapping date unless you start implementing logic to prevent it. For example, if a bed is occupied on the 25th Sept then set the Vacated date of the previous record to 24th Sept</li>
<li>Can you have periods when a bed is unoccupied? If you can, then I would create a fact record for this in exactly the same way as you would for an occupied bed but set the client ID FK to 0 (or whatever value you use in the client Dim to indicate a &quot;not applicable&quot; client)</li>
</ol>
<p>Hope this helps?</p>
<p><strong>Update 1 following response</strong></p>
<p>If you need to include Time then you need a time dimension and 2 additional keys in the fact for occupied and vacated time.</p>
<p>I'm not sure I understand your question about how you update the fact table. You have the information required to identify the fact record (bed id and vacated date key = 0) and the value needed to update the fact record. What am I missing?</p>
<p><strong>UPDATE 2</strong></p>
<p>I think you need to take a step back and think clearly about what it is you are trying to achieve - then the answers to your questions should become more obvious.</p>
<p>The first question you need to ask is what are you trying to measure: once you have clearly defined that then the grain of the fact table is established and it becomes clearer what changes in attributes you need to handle. For example:</p>
<ol>
<li>If you just want to know the status of a bed every time the occupant changes, and only the status of the occupant when they first use the bed (or last use the bed), then you only need to add a fact record when the bed occupancy changes and there is no need to record any updates during that patient's occupancy</li>
<li>If you want to know the state of of the bed at any point in time then first you need to define what you mean by &quot;any point in time&quot;: every day, hour, minute, etc? Then you need to decide what you want to record if there are multiple changes in that time period i.e. the position at the start of the hour or the end of the hour. Based on these decisions, you then need to work out if there have been any changes during that time period and, if there have been, insert/update the relevant records</li>
<li>If you want to treat each patient's occupancy of a bed as a single fact then your fact record obviously has start and end dates but you also need to make the decision about which single state you are going to record for any attributes that can change over that period - you can record the patient's status at the start or end of the occupancy but not throughout the occupancy as that would affect the grain of the fact table</li>
</ol>
<p>So to try and answer your questions...</p>
<p>If there is a change in dimension attributes and it affects your fact table then you'll need to handle this e.g. by inserting or updating a fact record:</p>
<ul>
<li>If you are only interested in the state of the patient at the start or end of the occupancy then any change to the patient's attributes during the occupancy can be ignored</li>
<li>If you are interested in the state of the patient at any point in the occupancy then you'll need to make changes to the fact table whenever one of the patient's attributes changes</li>
</ul>
<p>Records in your fact table should never overlap each other - so at any point in time there is only one active fact record per bed and per patient. Each time you insert a new fact record you would expire the previous applicable fact record.</p>
<p>So when you ask <em>&quot;The update to the end_date when the client moves to a new bed will be on all 3 added surrogate key rows?&quot;</em>, the answer is no - you would have set the end date on the first 2 records when you created the next record each time i.e. set the end date of record 1 when you create record 2, set the end date of record 2 when you create record 3, etc.; so you will only be updating the last record when the client moves.</p>
<p>Adding a PK to a fact table is only required when there is a requirement to update the fact table - as is the case here. Whether you do so is a choice - but I would look at how complicated the compound key is i.e. how many SKs do you need to use to identify the correct fact record to be updated. In you case you only need the Bed SK and the end_date = null (or 31/12/3000 or however you have chosen to set it) so there is probably no benefit in defining a single PK field on the fact table. If you needed more than about 5 SKs to identify a fact record then there is probably a case for using a single PK field.</p>
<h1><strong>UPDATE 3 - following comment added on 17/11/2020</strong></h1>
<p>Mini-dimensions: just seem to be more, unnecessary complication but I can't really comment unless you can clearly articulate what the issue is that you think mini-dimensions will solve and why you think mini-dimensions are a solution to the issue</p>
<h2>Dates</h2>
<p>You seem to be confused about the effective dates on an SDC2 dimension and foreign keys on a Fact table referencing the Date dimension - as they are very different things.</p>
<p>Date FKs on a Fact are attributes that you have chosen to record for that fact. In your example, for each bed occupancy fact (i.e. <em><strong>a single record</strong></em> in your fact table) you might have &quot;Date Occupied&quot; and &quot;Date Vacated&quot; attributes/FKs that reference the Date Dimension. When a fact record is created you would populate the &quot;Date Occupied&quot; field with the appropriate date and the &quot;Date Vacated&quot; with &quot;0&quot; (or whatever value points to the &quot;Unknown&quot; record in your Date Dimension). When the bed becomes unoccupied you update the fact record and set the &quot;Date Vacated&quot; field to the appropriate date.</p>
<p>Because you need to record 2 different dates against the fact, you need to have two FKs referencing the Date dimension; you couldn't record the Date Occupied and the Date Vacated using a single reference to the Date Dimension.</p>
<p>The same type of thinking applies when you want to have an FK on a fact table that references an SCD2 dimension; you need to decide what the point-in-time context of that reference is and then link to the correct version of the record in the SCD2 dimension. So if you want to record the state of the patient at the point they occupy the bed then you pick their record in the dimension where Fact.DateOccupied between Dim.EffStartDate and Dim.EffEndDate. If you want to <em>also</em> record the date of the patient at a different (but specific) time, such as when the bed was vacated, then you would need to add a separate FK to the fact table to hold this additional reference to the Patient Dim.</p>
<p>Having populated your fact table, if you want to know the state of the patient at a specific point in time you don't need to do anything to the fact table; instead you need to join the Patient Dim to itself. e.g.</p>
<ol>
<li>The fact table holds an FK that references a record in the Patient Dim</li>
<li>From this Patient Dim record you can get the patient's BK</li>
<li>Join from this BK back to the Patient Dim and filter on the date that you want to get the patient's details for</li>
</ol>
<p>Pseudo-code SQL for this would look something like (assuming you wanted to know the state of the patient on '2020-11-17'):</p>
<pre><code>SELECT
    P2.*
FROM
    FACT_TABLE F
    INNER JOIN PATIENT_DIM P1
        ON F.PATIENT_SK = P1.PATIENT_SK
    INNER JOIN PATIENT_DIM P2
        ON  P1.PATIENT_BK         = P2.PATIENT_BK
            AND P2.EFFSTART_DATE &lt;= '2020-11-17'
            AND P2.EFF_END_DATE  &gt;= '2020-11-17'
</code></pre>
<p>Hope this helps?</p>
"
"64032276","How to establish Oracle Anonymous data warehouse ODBC connection with Microstrategy","<p>Need to confirm host name, port ,Service, TNS etc to connect Microstrategy host on AWS Cloud.</p>
","<oracle><odbc><cloud><microstrategy><oracle-cloud-infrastructure>","2020-09-23 16:23:37","189","-2","1","64486379","<p>Not a Microstrategy expert, but from the research it appears that they're using ODBC drivers for connection to Oracle databases.</p>
<p>You can reuse this documentation that's showing how to connect to ADW with Progress odbc:
<a href=""https://www.oracle.com/a/ocom/docs/database/datadirect-connection-adw.pdf"" rel=""nofollow noreferrer"">Connecting to Autonomous Data Warehouse with Progress DataDirect ODBC Driver</a></p>
"
"64022285","Is it possible to query Azure data warehouse within log analytics","<p>I have a scenario where I would like to query Azure Data warehouse tables within the Log Analytics workspace and using those records I need to create a result set and prepare a chart.</p>
<p>I do see some objects in log analytics workspace like a database, table but not sure what is the purpose and are these objects specific to a resource or generic and how to use them I couldn't get documentation for these objects can somebody guide me on this.</p>
","<azure><azure-log-analytics><azure-synapse>","2020-09-23 06:35:01","362","0","1","64059172","<p>Unfortunately, you cannot use Azure Log Analytics to query Azure SQL Data Warehouse.</p>
<p>Use <a href=""https://learn.microsoft.com/en-us/sql/azure-data-studio/quickstart-sql-dw?view=sql-server-ver15"" rel=""nofollow noreferrer"">Azure Data Studio</a> to connect and query data in Azure SQL data warehouse.</p>
<p><a href=""https://learn.microsoft.com/en-us/sql/tools/overview-sql-tools?view=sql-server-ver15"" rel=""nofollow noreferrer"">Recommended tools</a> for querying data in Azure SQL Data Warehouse.</p>
<blockquote>
<p>Azure Log Analytics is used to write, execute, and manage Azure Monitor log queries in the Azure portal. You can use Log Analytics queries to search for terms, identify trends, analyze patterns, and provide many other insights from your data.</p>
</blockquote>
<p>For more information about log queries, see <a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/log-query/log-query-overview"" rel=""nofollow noreferrer"">Overview of log queries in Azure Monito</a>r.</p>
<p>For a detailed tutorial on writing log queries, see <a href=""https://learn.microsoft.com/en-us/azure/azure-monitor/log-query/get-started-queries"" rel=""nofollow noreferrer"">Get started with log queries in Azure Monitor</a>.</p>
"
"64017826","Using Factless Fact table used as source for Fact Table","<p>I have a design for a Property / Occupancy factless fact so I can capture the data for when customers move in and out of properties.</p>
<p>However I am not sure if it is best practice to use this as a source for maybe a occupied flag for my SCD property?</p>
<p>Or a fact table that needs to capture the status of the occupancy?</p>
<p>Please advise :)</p>
","<data-warehouse><fact>","2020-09-22 21:18:43","39","0","1","64043130","<p>I would never (or only in exceptional cases) build a fact table just to provide information to update other tables in your DW. Your source system presumably provides the occupancy information so you can use this to update dimensional attributes, where necessary, as part of your ETL processes</p>
"
"63989878","Modelling a bitemporal data set in a data warehouse","<p>I have a bitemporal EAV type model.</p>
<p>There are multiple attribute tables(one for each value data type(int_attributes, float_attributes, etc. )) looking like:</p>
<pre><code>|id|field_id|value|as_of_date_start|as_of_date_end|system_date_start|system_date_end|
</code></pre>
<p>And a reference table for attributes:</p>
<pre><code>|field_id|name|data_type|target_attribute_table|
</code></pre>
<p>It is possible to construct an entity and get all its attributes at any point in time by querying a union of all attribute value tables.</p>
<p><strong>Question</strong></p>
<p>I want to move this data to a data warehouse(Snowflake). I wasn't able to find a whole lot of info on moving bitemporal data to a warehouse and how to model it. I don't understand where would I need a facts and dimensions related modelling in this scenario. Also would it make any sense to pivot this table to get attributes as columns when loading the data in the system?</p>
<p>I'm thinking of moving this data as is to the warehouse.</p>
<p>The main use cases of this data is to get the current snapshot of data, get entire history of some entities and get histories of selected attributes on selected entities and do some analysis on them.</p>
<p>I'm very new to data modelling in a warehouse.</p>
","<database><database-design><snowflake-cloud-data-platform><data-warehouse>","2020-09-21 09:49:47","544","-2","1","64024693","<p>I think you would find it helpful to read a book on Dimensional Modelling - the &quot;bible&quot; is <a href=""https://www.amazon.co.uk/Data-Warehouse-Toolkit-Definitive-Dimensional/dp/1118530802/ref=sr_1_1?crid=Y2ML0GJU51O0&amp;dchild=1&amp;keywords=ralph%20kimball&amp;qid=1600851134&amp;sprefix=ralph%20kimba%2Caps%2C144&amp;sr=8-1"" rel=""nofollow noreferrer"">The Data Warehouse Toolkit</a></p>
<p>To simplify things a lot, you need to do the following:</p>
<ol>
<li>Define your entities (dimensions) e.g. Customer, Product, Country, Employee, etc.</li>
<li>Add the appropriate attributes to each entity</li>
<li>Add effective start and end dates attributes to each entity (to enable SCD2 logic)</li>
</ol>
<p>For each record for each entity there will presumably be time periods when none of the attributes changed so you need to insert a record into the entity for each of these static periods. For example, if Customer A started off with a Value attribute of 1, then on 23rd Jan 2020 this was changed to 2 and then on 3rd August 2020 this was changed to 3 (and assuming no other attribute values changed during this time period for this Customer), you would end up with these records in your Customer Dimension:</p>
<p><a href=""https://i.stack.imgur.com/QpKt9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QpKt9.png"" alt=""enter image description here"" /></a></p>
<p>Hope this helps?</p>
"
"63984691","Domain specific language to perform data extraction and transformation in ETL pipeline","<p>Does anyone of domain specific languages (DSL) that facilitate data extraction and transformation as part of an Extract-Transform-Load (ETL) pipeline?</p>
<p>I'd like to extract data from a 3rd party SQL database and transform the data to an already defined JSON format to store it into my application. There are many different possible database schemata to extract data from, so I was wondering whether there is already a way to configure this through the help of a (commonly used) extraction language (ideally that language is also agnostic to other data sources such as web services, etc).</p>
<p>I had a look around, but other than a few research papers I couldn't find much in terms of agreed standards for ETL (minus the 'L' which I've got covered) and I don't want to reinvent the wheel.</p>
<p>I'd appreciate any pointers in the right direction.</p>
","<sql><etl><data-warehouse><dsl>","2020-09-20 23:50:56","998","1","1","64043477","<p>Creating a good, all-encompassing DSL for ETL is I believe not just hard, it's a bit of a fool's errand. To handle the many real-world ETL complexities, you end up re-creating a general-purpose language.</p>
<p>And ETL &quot;without programming skill&quot; as this <a href=""http://www.wcse.org/WCSE_2017/041.pdf"" rel=""nofollow noreferrer"">research paper</a> attempts will struggle with the messiness of cleaning and conforming disparate source systems.</p>
<p>Using a general-purpose language by itself is of course possible but very time consuming due to the low abstraction level, and all the infrastructure code you'd have to implement.</p>
<p>Graphical ETL tools and some ETL DSLs address this by adding scripts or calling out to external programs. While useful and essential, this does have the disadvantage of employing multiple different programming models, with associated mental and technical friction when moving between them.</p>
<p>A different and I believe a better approach is to instead add ETL capabilities to a general-purpose language. Done well, you combine the benefits of ETL specific functionality and a high abstraction level with the power of a general-purpose language and its large eco-system, all delivered via a single programming model.</p>
<p>As one example of this latter approach, my company provides <a href=""https://envobi.com"" rel=""nofollow noreferrer"">actionETL</a>, a cross-platform .NET ETL library that combines an ETL mindset with the advantages of modern application development. For example, it provides familiar <a href=""https://docs.envobi.com/articles/workers.html#hierarchy"" rel=""nofollow noreferrer"">control flow</a> and <a href=""https://docs.envobi.com/articles/dataflow/dataflow.html"" rel=""nofollow noreferrer"">dataflow</a> ETL capabilities, and uses <a href=""https://martinfowler.com/bliki/DomainSpecificLanguage.html"" rel=""nofollow noreferrer"">internal DSLs</a> in several places to simplify configuration. Do <a href=""https://envobi.com/trial"" rel=""nofollow noreferrer"">try it out</a> if it sounds like a good fit.</p>
<p>actionETL now also has a free <a href=""https://envobi.com/community-edition"" rel=""nofollow noreferrer"">Community edition</a>.</p>
<p>Cheers,
Kristian</p>
"
"63978857","Cassandra user table data modelling","<p>I am learning cassandra. I have to do basic user table modeling. Here waht I have did.</p>
<pre><code>CREATE KEYSPACE IF NOT EXISTS users 
WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3};

CREATE TYPE IF NOT EXISTS users.date_type(
    month SMALLINT,
    day SMALLINT,
    year INT
);

CREATE TYPE IF NOT EXISTS users.phone_type(
    code INT,
    phone TEXT,
);

-- create user
CREATE TABLE IF NOT EXISTS users.user(
    userid TIMEUUID,
    name TEXT,
    dob FROZEN&lt;date_type&gt;,
    username TEXT,
    email TEXT,
    phone FROZEN&lt;phone_type&gt;,
    email_verified BOOLEAN,
    phone_verified BOOLEAN,
    created_on TIMESTAMP,
    PRIMARY KEY (userid)
);

-- get user by username
CREATE TABLE IF NOT EXISTS users.user_by_username(
    userid TIMEUUID,
    username TEXT,
    password TEXT,
    PRIMARY KEY (username)
);

-- get user by user phone number
-- can have same number for more than one account
CREATE TABLE IF NOT EXISTS users.user_by_phone(
    userid TIMEUUID,
    phone FROZEN&lt;phone_type&gt;,
    password TEXT,
    PRIMARY KEY (phone, userid)
);

-- get user by email
-- can have same email for more than one account
CREATE TABLE IF NOT EXISTS users.user_by_email(
    userid TIMEUUID,
    email TEXT,
    password TEXT,
    PRIMARY KEY (email, userid)
);

-- user history
CREATE TABLE IF NOT EXISTS users.user_history(
    userid TIMEUUID,
    name TEXT,
    dob FROZEN&lt;date_type&gt;,
    username TEXT,
    phone FROZEN&lt;phone_type&gt;,
    email TEXT,
    phone_verified BOOLEAN,
    email_verified BOOLEAN,
    action TEXT,
    date TIMESTAMP,
    PRIMARY KEY (userid, date, action)
);
</code></pre>
<ol>
<li>Is above table design valid as per cassandra data modeling?</li>
<li>Why because if I want to create an user, I have to insert in 5 tables</li>
<li>And if I want to update email, phone or password, first I have to delete entry in table then insert with updated values</li>
</ol>
","<cassandra><data-modeling><cassandra-3.0><datamodel>","2020-09-20 12:14:17","198","1","1","63985914","<blockquote>
<p>Is above table design valid as per cassandra data modeling?</p>
</blockquote>
<p>Yes it is valid table design as per Cassandra data modelling.</p>
<blockquote>
<p>Why because if I want to create an user, I have to insert in 5 tables</p>
</blockquote>
<p>This is how you do in Cassandra, first you have queries which you want to cater and then design your tables according to them.</p>
<blockquote>
<p>And if I want to update email, phone or password, first I have to
delete entry in table then insert with updated values</p>
</blockquote>
<p>Just update the values, you do not need to delete them first. But make sure you update corresponding entries in each table.</p>
"
"64063226","Modelling a bitemporal data set in a data warehouse","<p>I have a bitemporal EAV type model.</p>
<p>There are multiple attribute tables(one for each value data type(int_attributes, float_attributes, etc. )) looking like:</p>
<pre><code>|id|field_id|value|as_of_date_start|as_of_date_end|system_date_start|system_date_end|
</code></pre>
<p>And a reference table for attributes:</p>
<pre><code>|field_id|name|data_type|target_attribute_table|
</code></pre>
<p>It is possible to construct an entity and get all its attributes at any point in time by querying a union of all attribute value tables.</p>
<p>I want to move this data to a data warehouse(Snowflake). I wasn't able to find a whole lot of info on moving bitemporal data to a warehouse and how to model it. I don't understand where would I need a facts and dimensions related modelling in this scenario. Also would it make any sense to pivot this table to get attributes as columns when loading the data in the system?</p>
<p>The main use cases of this data is to get the current snapshot of data, and get entire history of some entities.</p>
<p>I'm very new to data modelling in a warehouse, any pointers/books/resources to understand this are also appreciated.</p>
","<database-design><data-warehouse><entity-attribute-value><snowflake-cloud-data-platform>","2020-09-19 12:08:49","57","-1","1","64063227","<p>Snowflake <a href=""https://docs.snowflake.com/en/user-guide/querying-semistructured.html"" rel=""nofollow noreferrer"">claims</a> to be efficient at querying semi-structured data. Dump your data in this format and import it to Snowflake. Keep specific columns for the keys and dates. Everything else - why you needed EAV - goes in the JSON.</p>
"
"63962769","How to Model Date Dimensions with Fact Tables of Different Grains","<p>We have some use cases for our DW where we have fact tables at different grains - e.g., sales by store by day (fact 1) and sales budget targets by month (fact 2). They both involve Date as a grain, but in one case the grain is day and the other the grain is period.</p>
<p>Assuming we can't in the near term change the grain, what's the right way to model this?</p>
<p>A Date and a Month dimension, which will have conformed attributes?
1 Date dimension, with nulls or flags or something when it's representing a higher value (e.g., month)
Something else?</p>
","<data-warehouse>","2020-09-18 20:59:20","525","2","2","63966959","<p>Two different dimensions, one for Date and one for Month</p>
"
"63962769","How to Model Date Dimensions with Fact Tables of Different Grains","<p>We have some use cases for our DW where we have fact tables at different grains - e.g., sales by store by day (fact 1) and sales budget targets by month (fact 2). They both involve Date as a grain, but in one case the grain is day and the other the grain is period.</p>
<p>Assuming we can't in the near term change the grain, what's the right way to model this?</p>
<p>A Date and a Month dimension, which will have conformed attributes?
1 Date dimension, with nulls or flags or something when it's representing a higher value (e.g., month)
Something else?</p>
","<data-warehouse>","2020-09-18 20:59:20","525","2","2","63967655","<p>You only need one date dimension with one row per day.  Just link to the last day of your period.
E.g. for a monthly aggregated fact just link to the last day of the month in your date dimension.</p>
"
"63941175","Calculate the descriptive statistics in a loop across several columns","<p>I'm generating random numbers for one variable and repeating the process several times. I want to calculate the mean of  <code>value </code> for each group (<code>group1</code>, <code>group2</code>, <code>group3</code>) in each iteration of the loop. I want to store the result so I afterward can estimate the mean share for each group across all iterations of the loop.</p>
<pre><code>require(tidyverse)

set.seed(21)
group1 &lt;- sample(c(&quot;A1&quot;, &quot;A2&quot;, &quot;B1&quot;, &quot;B2&quot;, &quot;C1&quot;, &quot;C2&quot;), 1000, TRUE)
group2 &lt;- sample(c(&quot;G1&quot;, &quot;G2&quot;, &quot;G4&quot;), 1000, TRUE)
group3 &lt;- sample(c(&quot;D1&quot;, &quot;D2&quot;), 1000, TRUE)
prob &lt;- runif(1000, 0, 1)
df &lt;- as.data.frame(cbind(group1, group2, group3, prob))

df$prob &lt;- as.numeric(df$prob)

for (i in 1:15) {
  
  df &lt;- df %&gt;%
    mutate(value = rbinom(nrow(df), 1, prob = prob))

  # [INSERT CALCULATION OF MEAN FOR EACH GROUP VARIABLE AND STORE IT]
}

# [INSERT CALCULATION OF MEAN ACROSS ALL ITERATIONS]
</code></pre>
<p>My main issue is how to estimate the mean of <code>value </code> across several variables in an efficient way and store the result in a smooth way.</p>
<p>Thanks in advance.</p>
<p><em><strong>To clarify:</strong></em></p>
<p>I want the end result to look something like this:</p>
<pre><code>col &quot;group1_A1&quot; &quot;Group1_A2&quot; &quot;group1_B1&quot; &quot;group1_B2&quot; &quot;group1_C1&quot; &quot;group1_C2&quot; &quot;group2_G1&quot; &quot;group2_G2&quot; &quot;group2_G4&quot; &quot;group3_D1&quot; &quot;group3_D2&quot;
x1  &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;    
x2  &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;    
x3  &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;    
x4  &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;    
x4  &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;    
x5  &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;    
x6  &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;    
x7  &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;    
x8  &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;    
x9  &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;    
x10 &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;
</code></pre>
<p>Where the three groups subgroups mean is replacing <code>&quot;x_bar</code> and each row is one iteration calculated means. An easy solution would be to use <code>dplyr</code>'s <code>group_by</code> but I want to find a solution so I go through all the three grouping variables.</p>
<p>To put this in a context: imagine that the variable <code>prob</code> is the probability of dying. <code>group1</code> is a variable indicating 6 age groups, <code>group2</code> is indicating on socioeconomic status, and <code>group3</code> is gender. I then want to see who is most likely to die. To do so I randomly generate a Bernoulli variable that is dependent on the probability of <code>prob</code>. To remove some stochastic I repeat this process 15 times and then want to see how big a share of each sociodemographic group that died (received a value of <code>1</code> on the variable <code>value</code>. For each iteration, I want to calculate the group belonging of those who died (so how many males died, how many old people died). I'm sorry for not coming up with a more joyful example.</p>
","<r><dplyr><data-management>","2020-09-17 15:16:46","295","0","2","63941412","<pre class=""lang-r prettyprint-override""><code>library(dplyr)
df &lt;- df %&gt;%
    mutate(value = rbinom(nrow(df), 1, prob = prob)) %&gt;%
    summarize_if(is.numeric,mean,na.rm = TRUE) #summarize_all also works if all columns are numeric anyway
</code></pre>
"
"63941175","Calculate the descriptive statistics in a loop across several columns","<p>I'm generating random numbers for one variable and repeating the process several times. I want to calculate the mean of  <code>value </code> for each group (<code>group1</code>, <code>group2</code>, <code>group3</code>) in each iteration of the loop. I want to store the result so I afterward can estimate the mean share for each group across all iterations of the loop.</p>
<pre><code>require(tidyverse)

set.seed(21)
group1 &lt;- sample(c(&quot;A1&quot;, &quot;A2&quot;, &quot;B1&quot;, &quot;B2&quot;, &quot;C1&quot;, &quot;C2&quot;), 1000, TRUE)
group2 &lt;- sample(c(&quot;G1&quot;, &quot;G2&quot;, &quot;G4&quot;), 1000, TRUE)
group3 &lt;- sample(c(&quot;D1&quot;, &quot;D2&quot;), 1000, TRUE)
prob &lt;- runif(1000, 0, 1)
df &lt;- as.data.frame(cbind(group1, group2, group3, prob))

df$prob &lt;- as.numeric(df$prob)

for (i in 1:15) {
  
  df &lt;- df %&gt;%
    mutate(value = rbinom(nrow(df), 1, prob = prob))

  # [INSERT CALCULATION OF MEAN FOR EACH GROUP VARIABLE AND STORE IT]
}

# [INSERT CALCULATION OF MEAN ACROSS ALL ITERATIONS]
</code></pre>
<p>My main issue is how to estimate the mean of <code>value </code> across several variables in an efficient way and store the result in a smooth way.</p>
<p>Thanks in advance.</p>
<p><em><strong>To clarify:</strong></em></p>
<p>I want the end result to look something like this:</p>
<pre><code>col &quot;group1_A1&quot; &quot;Group1_A2&quot; &quot;group1_B1&quot; &quot;group1_B2&quot; &quot;group1_C1&quot; &quot;group1_C2&quot; &quot;group2_G1&quot; &quot;group2_G2&quot; &quot;group2_G4&quot; &quot;group3_D1&quot; &quot;group3_D2&quot;
x1  &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;    
x2  &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;    
x3  &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;    
x4  &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;    
x4  &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;    
x5  &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;    
x6  &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;    
x7  &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;    
x8  &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;    
x9  &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;    
x10 &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;     &quot;x_bar&quot;
</code></pre>
<p>Where the three groups subgroups mean is replacing <code>&quot;x_bar</code> and each row is one iteration calculated means. An easy solution would be to use <code>dplyr</code>'s <code>group_by</code> but I want to find a solution so I go through all the three grouping variables.</p>
<p>To put this in a context: imagine that the variable <code>prob</code> is the probability of dying. <code>group1</code> is a variable indicating 6 age groups, <code>group2</code> is indicating on socioeconomic status, and <code>group3</code> is gender. I then want to see who is most likely to die. To do so I randomly generate a Bernoulli variable that is dependent on the probability of <code>prob</code>. To remove some stochastic I repeat this process 15 times and then want to see how big a share of each sociodemographic group that died (received a value of <code>1</code> on the variable <code>value</code>. For each iteration, I want to calculate the group belonging of those who died (so how many males died, how many old people died). I'm sorry for not coming up with a more joyful example.</p>
","<r><dplyr><data-management>","2020-09-17 15:16:46","295","0","2","63942190","<p>Here is an approach using some <code>tidyverse</code> functions.</p>
<pre><code>library(dplyr)
library(tidyr)
df2 &lt;- df %&gt;% 
  pivot_longer(starts_with(&quot;group&quot;) ) %&gt;%
  mutate(group = paste0(name, &quot;_&quot;, value)) %&gt;%
  select(group)

for (i in 1:15) {
  
  df2 &lt;- df %&gt;%
    mutate(value = rbinom(nrow(df), 1, prob = prob)) %&gt;%
    pivot_longer(starts_with(&quot;group&quot;), values_to = &quot;val&quot; ) %&gt;%
    mutate(group = paste0(name, &quot;_&quot;, val)) %&gt;%
    group_by(group) %&gt;%
    summarise(mean = mean(value, na.rm = TRUE)) %&gt;%
    rename_with(.cols = mean, .fn = ~ paste0(&quot;mean&quot;, i)) %&gt;%
    inner_join(df2, by = c(&quot;group&quot; = &quot;group&quot;))
}
df2 %&gt;%
  pivot_longer(starts_with(&quot;mean&quot;), names_to = &quot;trial&quot;, names_prefix = &quot;mean&quot;) %&gt;%
  distinct() %&gt;%
  pivot_wider(id_cols = mean, names_from = &quot;group&quot;, values_from = &quot;value&quot;)
# A tibble: 15 x 12
   trial group1_A1 group1_A2 group1_B1 group1_B2 group1_C1 group1_C2 group2_G1 group2_G2 group2_G4 group3_D1 group3_D2
   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
 1 15        0.519     0.514     0.516     0.519     0.551     0.533     0.529     0.542     0.507     0.518     0.533
 2 14        0.481     0.486     0.503     0.536     0.487     0.550     0.526     0.493     0.507     0.495     0.520
 3 13        0.506     0.541     0.477     0.470     0.572     0.556     0.575     0.499     0.496     0.486     0.552
 4 12        0.519     0.534     0.497     0.557     0.604     0.509     0.549     0.522     0.548     0.548     0.531
 5 11        0.5       0.568     0.458     0.481     0.497     0.562     0.542     0.496     0.496     0.467     0.548
 6 10        0.525     0.466     0.503     0.503     0.535     0.580     0.581     0.490     0.496     0.488     0.548
 7 9         0.494     0.547     0.490     0.448     0.610     0.598     0.578     0.504     0.519     0.501     0.560
 8 8         0.538     0.554     0.471     0.530     0.599     0.538     0.545     0.516     0.559     0.565     0.518
 9 7         0.525     0.588     0.548     0.475     0.535     0.568     0.601     0.499     0.522     0.507     0.565
10 6         0.513     0.527     0.529     0.546     0.561     0.503     0.562     0.513     0.522     0.510     0.550
11 5         0.462     0.493     0.503     0.508     0.513     0.568     0.513     0.493     0.522     0.507     0.510
12 4         0.506     0.5       0.452     0.481     0.599     0.556     0.545     0.516     0.496     0.520     0.516
13 3         0.525     0.466     0.503     0.525     0.567     0.556     0.529     0.550     0.499     0.497     0.552
14 2         0.462     0.554     0.471     0.514     0.519     0.574     0.536     0.516     0.499     0.520     0.512
15 1         0.506     0.541     0.497     0.470     0.519     0.544     0.510     0.519     0.507     0.510     0.514
</code></pre>
<p>This gets you your first part - a data.frame where each row is a trial with means per group.</p>
<p>Your second part is as follows:</p>
<pre><code>df2 %&gt;%
  pivot_longer(starts_with(&quot;mean&quot;), names_to = &quot;trial&quot;, names_prefix = &quot;mean&quot;) %&gt;%
  distinct() %&gt;%
  group_by(group) %&gt;%
  summarize(mean = mean(value))
# A tibble: 11 x 2
   group      mean
   &lt;chr&gt;     &lt;dbl&gt;
 1 group1_A1 0.505
 2 group1_A2 0.525
 3 group1_B1 0.495
 4 group1_B2 0.504
 5 group1_C1 0.551
 6 group1_C2 0.553
 7 group2_G1 0.548
 8 group2_G2 0.511
 9 group2_G4 0.513
10 group3_D1 0.509
11 group3_D2 0.535
</code></pre>
"
"63929693","ad-hoc slowly-changing dimensions materialization from external table of timestamped csvs in a data lake","<h1>Question</h1>
<h3>main question</h3>
<blockquote>
<p>How can I ephemerally materialize slowly changing dimension type 2 from from a folder of daily extracts, where each csv is one full extract of a table from from a source system?</p>
</blockquote>
<h3>rationale</h3>
<p>We're designing ephemeral data warehouses as data marts for end users that can be spun up and burned down without consequence. This requires we have all data in a lake/blob/bucket.</p>
<p>We're ripping daily full extracts because:</p>
<ol>
<li>we couldn't reliably extract just the changeset (for reasons out of our control), and</li>
<li>we'd like to maintain a data lake with the &quot;rawest&quot; possible data.</li>
</ol>
<h3>challenge question</h3>
<blockquote>
<p>Is there a solution that could give me the state as of a specific date and not just the &quot;newest&quot; state?</p>
</blockquote>
<h3>existential question</h3>
<blockquote>
<p>Am I thinking about this completely backwards and there's a much easier way to do this?</p>
</blockquote>
<h1>Possible Approaches</h1>
<h3>custom <code>dbt</code> materialization</h3>
<p>There's a <a href=""https://github.com/fishtown-analytics/dbt-utils#insert_by_period-source"" rel=""nofollow noreferrer""><code>insert_by_period</code></a> dbt materialization in the <code>dbt.utils</code> package, that I think might be exactly what I'm looking for? But I'm confused as it's <code>dbt snapshot</code>, but:</p>
<ol>
<li>run <code>dbt snapshot</code> for each file incrementally, all at once; and,</li>
<li>built directly off of an external table?</li>
</ol>
<h3>Delta Lake</h3>
<p>I  don't know much about Databricks's Delta Lake, but it seems like it should be possible with <a href=""https://docs.databricks.com/delta/delta-update.html#id2"" rel=""nofollow noreferrer"">Delta Tables</a>?</p>
<h3>Fix the extraction job</h3>
<p>Is our oroblem is solved if we can make our extracts contain only what has changed since the previous extract?</p>
<h1>Example</h1>
<p>Suppose the following three files are in a folder of a data lake. (<a href=""https://gist.github.com/swanderz/950212913cd2366528abf2a788fe54ad"" rel=""nofollow noreferrer"">Gist with the 3 csvs and desired table outcome as csv</a>).
I added the Extracted column in case parsing the timestamp from the filename is too tricky.</p>
<h3><code>2020-09-14_CRM_extract.csv</code></h3>
<pre><code>| OppId | CustId | Stage       | Won | LastModified | Extracted |
|-------|--------|-------------|-----|--------------|-----------|
| 1     | A      | 2 - Qualify |     | 9/1          | 9/14      |
| 2     | B      | 3 - Propose |     | 9/12         | 9/14      |
</code></pre>
<h3><code>2020-09-15_CRM_extract.csv</code></h3>
<pre><code>| OppId | CustId | Stage       | Won | LastModified | Extracted |
|-------|--------|-------------|-----|--------------|-----------|
| 1     | A      | 2 - Qualify |     | 9/1          | 9/15      |
| 2     | B      | 4 - Closed  | Y   | 9/14         | 9/15      |
| 3     | C      | 1 - Lead    |     | 9/14         | 9/15      |
</code></pre>
<h3><code>2020-09-16_CRM_extract.csv</code></h3>
<pre><code>| OppId | CustId | Stage       | Won | LastModified | Extracted |
|-------|--------|-------------|-----|--------------|-----------|
| 1     | A      | 2 - Qualify |     | 9/1          | 9/16      |
| 2     | B      | 4 - Closed  | Y   | 9/14         | 9/16      |
| 3     | C      | 2 - Qualify |     | 9/15         | 9/16      |
</code></pre>
<h3>End Result</h3>
<p>Below is SCD-II for the three files as of 9/16. SCD-II as of 9/15 would be the same but <code>OppId=3</code> has only one from <code>valid_from=9/15</code> and <code>valid_to=null</code></p>
<pre><code>| OppId | CustId | Stage       | Won | LastModified | valid_from | valid_to |
|-------|--------|-------------|-----|--------------|------------|----------|
| 1     | A      | 2 - Qualify |     | 9/1          | 9/14       | null     |
| 2     | B      | 3 - Propose |     | 9/12         | 9/14       | 9/15     |
| 2     | B      | 4 - Closed  | Y   | 9/14         | 9/15       | null     |
| 3     | C      | 1 - Lead    |     | 9/14         | 9/15       | 9/16     |
| 3     | C      | 2 - Qualify |     | 9/15         | 9/16       | null     |
</code></pre>
","<snowflake-cloud-data-platform><data-warehouse><external-tables><delta-lake><dbt>","2020-09-17 00:38:36","746","0","2","63930605","<p>I don't know whether this is the best or not, but I've seen it done.  When you build your initial SCD-II table, add a column that is a stored <code>HASH()</code> value of all of the values of the record (you can exclude the primary key).  Then, you can create an External Table over your incoming full data set each day, which includes the same <code>HASH()</code> function.  Now, you can execute a <code>MERGE</code> or <code>INSERT/UPDATE</code> against your SCD-II based on primary key and whether the HASH value has changed.</p>
<p>Your main advantage doing things this way is you avoid loading all of the data into Snowflake each day to do the comparison, but it will be slower to execute this way.  You could also load to a temp table with the <code>HASH()</code> function included in your <code>COPY INTO</code> statement and then update your SCD-II and then drop the temp table, which could actually be faster.</p>
"
"63929693","ad-hoc slowly-changing dimensions materialization from external table of timestamped csvs in a data lake","<h1>Question</h1>
<h3>main question</h3>
<blockquote>
<p>How can I ephemerally materialize slowly changing dimension type 2 from from a folder of daily extracts, where each csv is one full extract of a table from from a source system?</p>
</blockquote>
<h3>rationale</h3>
<p>We're designing ephemeral data warehouses as data marts for end users that can be spun up and burned down without consequence. This requires we have all data in a lake/blob/bucket.</p>
<p>We're ripping daily full extracts because:</p>
<ol>
<li>we couldn't reliably extract just the changeset (for reasons out of our control), and</li>
<li>we'd like to maintain a data lake with the &quot;rawest&quot; possible data.</li>
</ol>
<h3>challenge question</h3>
<blockquote>
<p>Is there a solution that could give me the state as of a specific date and not just the &quot;newest&quot; state?</p>
</blockquote>
<h3>existential question</h3>
<blockquote>
<p>Am I thinking about this completely backwards and there's a much easier way to do this?</p>
</blockquote>
<h1>Possible Approaches</h1>
<h3>custom <code>dbt</code> materialization</h3>
<p>There's a <a href=""https://github.com/fishtown-analytics/dbt-utils#insert_by_period-source"" rel=""nofollow noreferrer""><code>insert_by_period</code></a> dbt materialization in the <code>dbt.utils</code> package, that I think might be exactly what I'm looking for? But I'm confused as it's <code>dbt snapshot</code>, but:</p>
<ol>
<li>run <code>dbt snapshot</code> for each file incrementally, all at once; and,</li>
<li>built directly off of an external table?</li>
</ol>
<h3>Delta Lake</h3>
<p>I  don't know much about Databricks's Delta Lake, but it seems like it should be possible with <a href=""https://docs.databricks.com/delta/delta-update.html#id2"" rel=""nofollow noreferrer"">Delta Tables</a>?</p>
<h3>Fix the extraction job</h3>
<p>Is our oroblem is solved if we can make our extracts contain only what has changed since the previous extract?</p>
<h1>Example</h1>
<p>Suppose the following three files are in a folder of a data lake. (<a href=""https://gist.github.com/swanderz/950212913cd2366528abf2a788fe54ad"" rel=""nofollow noreferrer"">Gist with the 3 csvs and desired table outcome as csv</a>).
I added the Extracted column in case parsing the timestamp from the filename is too tricky.</p>
<h3><code>2020-09-14_CRM_extract.csv</code></h3>
<pre><code>| OppId | CustId | Stage       | Won | LastModified | Extracted |
|-------|--------|-------------|-----|--------------|-----------|
| 1     | A      | 2 - Qualify |     | 9/1          | 9/14      |
| 2     | B      | 3 - Propose |     | 9/12         | 9/14      |
</code></pre>
<h3><code>2020-09-15_CRM_extract.csv</code></h3>
<pre><code>| OppId | CustId | Stage       | Won | LastModified | Extracted |
|-------|--------|-------------|-----|--------------|-----------|
| 1     | A      | 2 - Qualify |     | 9/1          | 9/15      |
| 2     | B      | 4 - Closed  | Y   | 9/14         | 9/15      |
| 3     | C      | 1 - Lead    |     | 9/14         | 9/15      |
</code></pre>
<h3><code>2020-09-16_CRM_extract.csv</code></h3>
<pre><code>| OppId | CustId | Stage       | Won | LastModified | Extracted |
|-------|--------|-------------|-----|--------------|-----------|
| 1     | A      | 2 - Qualify |     | 9/1          | 9/16      |
| 2     | B      | 4 - Closed  | Y   | 9/14         | 9/16      |
| 3     | C      | 2 - Qualify |     | 9/15         | 9/16      |
</code></pre>
<h3>End Result</h3>
<p>Below is SCD-II for the three files as of 9/16. SCD-II as of 9/15 would be the same but <code>OppId=3</code> has only one from <code>valid_from=9/15</code> and <code>valid_to=null</code></p>
<pre><code>| OppId | CustId | Stage       | Won | LastModified | valid_from | valid_to |
|-------|--------|-------------|-----|--------------|------------|----------|
| 1     | A      | 2 - Qualify |     | 9/1          | 9/14       | null     |
| 2     | B      | 3 - Propose |     | 9/12         | 9/14       | 9/15     |
| 2     | B      | 4 - Closed  | Y   | 9/14         | 9/15       | null     |
| 3     | C      | 1 - Lead    |     | 9/14         | 9/15       | 9/16     |
| 3     | C      | 2 - Qualify |     | 9/15         | 9/16       | null     |
</code></pre>
","<snowflake-cloud-data-platform><data-warehouse><external-tables><delta-lake><dbt>","2020-09-17 00:38:36","746","0","2","63953553","<p>Interesting concept and of course it would a longer conversation than is possible in this forum to fully understand your business, stakeholders, data, etc. I can see that it might work if you had a relatively small volume of data, your source systems rarely changed, your reporting requirements (and hence, datamarts) also rarely changed and you only needed to spin up these datamarts very infrequently.</p>
<p>My concerns would be:</p>
<ol>
<li>If your source or target requirements change how are you going to handle this? You will need to spin up your datamart, do full regression testing on it, apply your changes and then test them. If you do this as/when the changes are known then it's a lot of effort for a Datamart that's not being used - especially if you need to do this multiple times between uses; if you do this when the datamart is needed then you're not meeting your objective of having the datamart available for &quot;instant&quot; use.</li>
</ol>
<p>Your statement &quot;we have a DW as code that can be deleted, updated, and recreated without the complexity that goes along with traditional DW change management&quot; I'm not sure is true. How are you going to test updates to your code without spinning up the datamart(s) and going through a standard test cycle with data - and then how is this different from traditional DW change management?</p>
<ol start=""2"">
<li>What happens if there is corrupt/unexpected data in your source systems? In a &quot;normal&quot; DW where you are loading data daily this would normally be noticed and fixed on the day. In your solution the dodgy data might have occurred days/weeks ago and, assuming it loaded into your datamart rather than erroring on load, you would need processes in place to spot it and then potentially have to unravel days of SCD records to fix the problem</li>
<li>(Only relevant if you have a significant volume of data) Given the low cost of storage, I'm not sure I see the benefit of spinning up a datamart when needed as opposed to just holding the data so it's ready for use. Loading large volumes of data everytime you spin up a datamart is going to be time-consuming and expensive. Possible hybrid approach might be to only run incremental loads when the datamart is needed rather than running them every day - so you have the data from when the datamart was last used ready to go at all times and you just add the records created/updated since the last load</li>
</ol>
"
"63918499","dbt table coverage metric","<p>Given that I have a data warehouse with various tables being created from various sources many of them by dbt, I want to measure a concept like 'dbt table coverage' which I define as:</p>
<pre><code>dtc = count(tables and views that exist) / count(non ephemeral models and sources)
</code></pre>
<p>This would be really useful in order to maintain a sense of quality/completeness, especially during transition projects. Is there a dbt command like:</p>
<pre><code>dbt report table-coverage --schemas=['reporting','example']
&gt;&gt;&gt; 96% coverage, 48/50 tables in the schemas provided are captured in dbt. 
</code></pre>
<p>If not, how can we add this to the project?!</p>
<p>What alternate approaches could I take to solving the problem</p>
","<data-warehouse><dbt>","2020-09-16 10:54:12","558","0","3","63945244","<p>To do this I would probably create a model (view) that queried the information_schema and made some assumptions about a 1-to-1 mapping of <code>{sourceTableName}</code> to <code>stg_{sourceTableName}</code> (Assuming this means coverage for you).</p>
<p>Additionally I would look into using the <code>graph.sources.values()</code> JINJA function in order to iterate through all of the documented sources in your project, and then compare that with each of the models in <code>{target.schema}</code></p>
<p><a href=""https://docs.getdbt.com/reference/dbt-jinja-functions/graph#accessing-sources"" rel=""nofollow noreferrer"">https://docs.getdbt.com/reference/dbt-jinja-functions/graph#accessing-sources</a></p>
<p>If you're comparing the existence of <code>source.schema.yml</code> with the <code>source.information_schema</code>. I would alter the approach to consider mapping each of the items in the graph against the total count of items in the information_schema on the source database.</p>
"
"63918499","dbt table coverage metric","<p>Given that I have a data warehouse with various tables being created from various sources many of them by dbt, I want to measure a concept like 'dbt table coverage' which I define as:</p>
<pre><code>dtc = count(tables and views that exist) / count(non ephemeral models and sources)
</code></pre>
<p>This would be really useful in order to maintain a sense of quality/completeness, especially during transition projects. Is there a dbt command like:</p>
<pre><code>dbt report table-coverage --schemas=['reporting','example']
&gt;&gt;&gt; 96% coverage, 48/50 tables in the schemas provided are captured in dbt. 
</code></pre>
<p>If not, how can we add this to the project?!</p>
<p>What alternate approaches could I take to solving the problem</p>
","<data-warehouse><dbt>","2020-09-16 10:54:12","558","0","3","63958949","<p>A couple thoughts here since this is pretty intriguing to my current case as well:</p>
<ol>
<li><p>dbt doesn't give outputs of queries or return a result to the command line. (That I know of!) So that if 1 inherently unsupported feature at this time. i.e. <code>dbt report</code> or <code>dbt query</code> doesn't exist yet. If that's desired, I'm recommend build out a feature request here:
<a href=""https://github.com/fishtown-analytics/dbt/issues"" rel=""nofollow noreferrer"">https://github.com/fishtown-analytics/dbt/issues</a></p>
</li>
<li><p>If you're ok with making a model in dbt and then just executing that via your client of choice, let's give that a shot. (I'm using postgres so convert accordingly)</p>
<pre><code>WITH schema_map as
   (select schemaname as schema,
    tablename as name,
    'Table' as Type,
    CASE WHEN schemaname like '%dbt%' THEN 1
     ELSE 0 END as dbt_created
    from pg_tables
WHERE NOT schemaname = ANY('{information_schema,pg_catalog}')
UNION
select schemaname as schema,
    viewname as name,
    'View' as Type,
    CASE WHEN schemaname like '%dbt%' THEN 1
         ELSE 0 END as dbt_created
    from pg_views
 WHERE NOT schemaname = ANY('{information_schema,pg_catalog}') )
 SELECT count(name) as total_tables_and_views,
    sum(dbt_created) as dbt_created,
    to_char((sum(dbt_created)::dec/count(name)::dec)*100,'999D99%') as dbt_coverage
 FROM schema_map
</code></pre>
</li>
</ol>
<p>Gives the result:</p>
<pre><code>total_tables_and_views | dbt_created | dbt_coverage
391                    |292          |  74.68%
</code></pre>
"
"63918499","dbt table coverage metric","<p>Given that I have a data warehouse with various tables being created from various sources many of them by dbt, I want to measure a concept like 'dbt table coverage' which I define as:</p>
<pre><code>dtc = count(tables and views that exist) / count(non ephemeral models and sources)
</code></pre>
<p>This would be really useful in order to maintain a sense of quality/completeness, especially during transition projects. Is there a dbt command like:</p>
<pre><code>dbt report table-coverage --schemas=['reporting','example']
&gt;&gt;&gt; 96% coverage, 48/50 tables in the schemas provided are captured in dbt. 
</code></pre>
<p>If not, how can we add this to the project?!</p>
<p>What alternate approaches could I take to solving the problem</p>
","<data-warehouse><dbt>","2020-09-16 10:54:12","558","0","3","64538592","<p>Just to feed back to the community and thanks to Jordan and Gscott for the inspiration. The solution I executed for SQL Server/ Synapse was:</p>
<ol>
<li>A Daily execution of Counts of models in INFORMATION_SCHEMA.TABLES and in the dbt graph as one table.</li>
<li>An incremental table built on 1 that selects schemas of interest and aggregates. In my case below I filter out staging and testing.</li>
</ol>
<p>DbtModelCounts:</p>
<pre><code>
{% set models = [] -%}

{% if execute %}
  {% for node in graph.nodes.values()
    | selectattr(&quot;resource_type&quot;, &quot;equalto&quot;, &quot;model&quot;)
    %}
        {%- do models.append(node.name) -%}

  {% endfor %}
{% endif %}

with tables AS
(
SELECT table_catalog [db], table_schema [schema_name], table_name [name], table_type [type]
FROM INFORMATION_SCHEMA.TABLES
),
dbt_tables AS
(
SELECT *
FROM tables
WHERE name in (
    {%- for model in models %}
    ('{{ model}}') 
    {% if not loop.last %},
    {% endif %}
    {% endfor %}
    )
)
SELECT
    tables.db, 
    tables.schema_name,
    tables.type,
    COUNT(tables.name) ModelCount,
    COUNT(dbt_tables.name) DbtModelCount
FROM tables
LEFT JOIN dbt_tables ON
    tables.name=dbt_tables.name AND
    tables.schema_name = dbt_tables.schema_name AND
    tables.db = dbt_tables.db AND 
    tables.type = dbt_tables.type
GROUP BY
    tables.db,
    tables.schema_name,
    tables.type
</code></pre>
<p>Dbt Coverage:</p>
<pre><code>{{
  config(
    materialized='incremental',
    unique_key='DateCreated'
  )
}}
SELECT 
    CAST(GETDATE() AS DATE) AS DateCreated,
    GETDATE() AS DateTimeCreatedUTC,
    SUM(DbtModelCount) AS DbtModelCount, 
    SUM(ModelCount) AS TotalModels,
    SUM(DbtModelCount)*100.0/SUM(ModelCount) as DbtCoveragePercentage
FROM {{ref('DbtModelCounts')}}
WHERE schema_name NOT LIKE 'testing%' AND schema_name NOT LIKE 'staging%'
</code></pre>
<p>To do, add logic for defined sources to also compute percentage of sources that map to my staging or raw schema tables .</p>
"
"63914339","How to build a data catalog in Glue for Snowflake?","<p>I am new to snowflake. I am wondering how to build a data catalog for snowflake's data?</p>
<p>I cannot find documentation on that. I can find this <a href=""https://community.snowflake.com/s/article/How-To-Use-AWS-Glue-With-Snowflake"" rel=""nofollow noreferrer"">post</a>. But it seems that it does not use the catalog function.</p>
","<snowflake-cloud-data-platform><aws-glue-data-catalog><snowflake-pipe>","2020-09-16 06:30:18","1685","1","1","64153780","<p>I find useful information <a href=""https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html"" rel=""nofollow noreferrer"">here</a> where you need to add jdbc connection then define a <a href=""https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html"" rel=""nofollow noreferrer"">crawler</a> but seems not supporting Snowflake database as the latter link says</p>
<p>Crawlers can crawl the following data stores through a JDBC connection:</p>
<blockquote>
<p>Amazon Redshift</p>
<p>Amazon Relational Database Service (Amazon RDS)</p>
<p>Amazon Aurora</p>
<p>Microsoft SQL Server</p>
<p>MySQL</p>
<p>Oracle</p>
<p>PostgreSQL</p>
</blockquote>
<p>Publicly accessible databases</p>
<blockquote>
<p>Aurora</p>
<p>Microsoft SQL Server</p>
<p>MySQL</p>
<p>Oracle</p>
<p>PostgreSQL</p>
</blockquote>
<p>Also, again I find in defining connection link <a href=""https://docs.aws.amazon.com/glue/latest/dg/connection-defining.html"" rel=""nofollow noreferrer"">here</a> those are the supported databases only</p>
<blockquote>
<p>AWS Glue can connect to the following data stores through a JDBC
connection:</p>
<p>Amazon Redshift</p>
<p>Amazon Aurora</p>
<p>Microsoft SQL Server</p>
<p>MySQL</p>
<p>Oracle</p>
<p>PostgreSQL</p>
</blockquote>
"
"63903022","How to check in SQL if multi columnar set is in the table (without string concatenation)","<p>Let's assume I've 3 columns in a table with values like this:</p>
<pre><code>table_1:
   A   |   B   |   C
-----------------------
  'xx' |  ''   |  'y'
  'x'  |  'y'  |  'x'
  'x'  |  'x'  |  'y'
  'x'  |  'yy' |  ''
  'x'  |  ''   |  'yy'
  'x'  |  'y'  |  'y'
</code></pre>
<p>I've a result set (result of an SQL SELECT statement) which I want to identify in the above table if it exists there:</p>
<pre><code>[
    ('x', 'x', 'y')
    ('x', 'y', 'y')
]
</code></pre>
<p>This result set would match for 5 (of 6) rows in instead of the 2 from the table above if I've compared the results of simple string concatenation, e.g. I would simply compare the results of this: <code>SELECT concat(A, B, C) FROM table_1</code></p>
<p>I could solve this problem with comparing the results of more complex string concatenation functions like this: <code>SELECT concat('A=', A, '_', 'B=', B, '_', 'C=', C )</code></p>
<p><strong>BUT:</strong></p>
<ul>
<li>I don't want to use any hardcoded special separator in a string concatenation like <code>_</code> or <code>=</code>
<ul>
<li>because any character might be in the data
<ul>
<li>e.g.: somewhere in column B there might be this value: <code>xx_C=yy</code></li>
</ul>
</li>
<li>it's not a clean solution</li>
</ul>
</li>
<li>I don't want to use string concatenation at all, because it's an ugly solution
<ul>
<li>it makes the &quot;distance&quot; between the attributes disappear</li>
<li>not general enough</li>
</ul>
</li>
<li>maybe I've columns with different datatypes I don't want to convert to a STRING based column</li>
</ul>
<p><strong>Question:</strong></p>
<p>Is it possible to solve somehow this problem without using string concatenation?
Is there a simple solution for this multi column value checking problem?</p>
<p>I want to solve this in BiqQuery, but I'm interested in a general solution for every relational databse/datawarehouse.</p>
<p>Thank you!</p>
<hr />
<pre><code>CREATE TABLE test.table_1 (
  A STRING,
  B STRING,
  C STRING
) AS
SELECT * FROM (
  SELECT 'xx', '', 'y'
  UNION ALL
  SELECT 'x', 'y', 'x'
  UNION ALL
  SELECT 'x', 'x', 'y'
  UNION ALL
  SELECT 'x', 'yy', ''
  UNION ALL
  SELECT 'x', '', 'yy'
  UNION ALL
  SELECT 'x', 'y', 'y'
)

SELECT A, B, C
FROM test.table_1
WHERE (A, B, C) IN (    -&gt; I need this functionality
  SELECT 'x', 'x', 'y'
  UNION ALL
  SELECT 'x', 'y', 'y'
);
</code></pre>
","<sql><database><google-bigquery><relational-database><data-warehouse>","2020-09-15 13:33:41","20","0","2","63903053","<p>Use <code>join</code>:</p>
<pre><code>SELECT t1.*
FROM test.table_1 t1 JOIN
     (SELECT 'x' as a, 'x' as b, 'y' as c
      UNION ALL
      SELECT 'x', 'y', 'y'
     ) t2
     USING (a, b, c);
</code></pre>
"
"63903022","How to check in SQL if multi columnar set is in the table (without string concatenation)","<p>Let's assume I've 3 columns in a table with values like this:</p>
<pre><code>table_1:
   A   |   B   |   C
-----------------------
  'xx' |  ''   |  'y'
  'x'  |  'y'  |  'x'
  'x'  |  'x'  |  'y'
  'x'  |  'yy' |  ''
  'x'  |  ''   |  'yy'
  'x'  |  'y'  |  'y'
</code></pre>
<p>I've a result set (result of an SQL SELECT statement) which I want to identify in the above table if it exists there:</p>
<pre><code>[
    ('x', 'x', 'y')
    ('x', 'y', 'y')
]
</code></pre>
<p>This result set would match for 5 (of 6) rows in instead of the 2 from the table above if I've compared the results of simple string concatenation, e.g. I would simply compare the results of this: <code>SELECT concat(A, B, C) FROM table_1</code></p>
<p>I could solve this problem with comparing the results of more complex string concatenation functions like this: <code>SELECT concat('A=', A, '_', 'B=', B, '_', 'C=', C )</code></p>
<p><strong>BUT:</strong></p>
<ul>
<li>I don't want to use any hardcoded special separator in a string concatenation like <code>_</code> or <code>=</code>
<ul>
<li>because any character might be in the data
<ul>
<li>e.g.: somewhere in column B there might be this value: <code>xx_C=yy</code></li>
</ul>
</li>
<li>it's not a clean solution</li>
</ul>
</li>
<li>I don't want to use string concatenation at all, because it's an ugly solution
<ul>
<li>it makes the &quot;distance&quot; between the attributes disappear</li>
<li>not general enough</li>
</ul>
</li>
<li>maybe I've columns with different datatypes I don't want to convert to a STRING based column</li>
</ul>
<p><strong>Question:</strong></p>
<p>Is it possible to solve somehow this problem without using string concatenation?
Is there a simple solution for this multi column value checking problem?</p>
<p>I want to solve this in BiqQuery, but I'm interested in a general solution for every relational databse/datawarehouse.</p>
<p>Thank you!</p>
<hr />
<pre><code>CREATE TABLE test.table_1 (
  A STRING,
  B STRING,
  C STRING
) AS
SELECT * FROM (
  SELECT 'xx', '', 'y'
  UNION ALL
  SELECT 'x', 'y', 'x'
  UNION ALL
  SELECT 'x', 'x', 'y'
  UNION ALL
  SELECT 'x', 'yy', ''
  UNION ALL
  SELECT 'x', '', 'yy'
  UNION ALL
  SELECT 'x', 'y', 'y'
)

SELECT A, B, C
FROM test.table_1
WHERE (A, B, C) IN (    -&gt; I need this functionality
  SELECT 'x', 'x', 'y'
  UNION ALL
  SELECT 'x', 'y', 'y'
);
</code></pre>
","<sql><database><google-bigquery><relational-database><data-warehouse>","2020-09-15 13:33:41","20","0","2","63906520","<p>Below is the most generic way I can think of (BigQuery Standard SQL):</p>
<pre><code>#standardSQL
SELECT *
FROM `project.test.table1` t
WHERE t IN (
  SELECT t
  FROM `project.test.table2` t
)
</code></pre>
<p>You can test, play with above using sample data from your question as in below example</p>
<pre><code>#standardSQL
WITH `project.test.table1` AS (
  SELECT 'xx' a, '' b, 'y' c UNION ALL
  SELECT 'x', 'y', 'x' UNION ALL
  SELECT 'x', 'x', 'y' UNION ALL
  SELECT 'x', 'yy', '' UNION ALL
  SELECT 'x', '', 'yy' UNION ALL
  SELECT 'x', 'y', 'y'
), `project.test.table2` AS (
  SELECT 'x' a, 'x' b, 'y' c UNION ALL
  SELECT 'x', 'y', 'y'
)
SELECT *
FROM `project.test.table1` t
WHERE t IN (
  SELECT t
  FROM `project.test.table2` t
)
</code></pre>
<p>with output</p>
<pre><code>Row a   b   c    
1   x   x   y    
2   x   y   y    
</code></pre>
"
"63900050","Specify datetime2 format in Azure SQL data warehouse (synapse)","<p>What is the correct way to specify the format of a datetime2 field when creating a table in Azure SQL data warehouse? I don't seem to be able to find an example in the documentation.</p>
<p>The data looks like this:</p>
<p>&quot;2020-09-14T20:50:48.000Z&quot;</p>
<pre><code>CREATE TABLE [Foo].[Bar](
    ...
    MyDateTime datetime2(['YYYY-MM-DDThh:mm:ss[.fractional seconds]')
)
</code></pre>
","<azure><data-warehouse><azure-synapse>","2020-09-15 10:35:15","2122","0","1","63913176","<p>As Panagiotis notes, the underlying representation is an int/long for the actual date value. This is how RDBMS engines can quickly compute the delta between two dates (days between Monday and Friday is a simple subtraction problem). To answer your question, you simply would format your create table as:</p>
<pre><code>CREATE TABLE [Foo].[Bar](
    ...
    MyDateTime datetime2
)
</code></pre>
<p>If you're interested in formatting the result in a query, you can look to the <a href=""https://learn.microsoft.com/sql/t-sql/functions/cast-and-convert-transact-sql"" rel=""nofollow noreferrer"">CONVERT</a> or <a href=""https://learn.microsoft.com/sql/t-sql/functions/format-transact-sql"" rel=""nofollow noreferrer"">FORMAT</a> functions. For example, if you wanted the format <em>dd-mm-yyyy</em> (Italian date), you could use either of the following:</p>
<pre><code>SELECT
    CONVERT(VARCHAR, CURRENT_TIMESTAMP, 105)
    , FORMAT(CURRENT_TIMESTAMP, 'dd-MM-yyyy')
</code></pre>
<p>Note: <em>CONVERT</em> is generally faster than <em>FORMAT</em> and is the recommended approach if you have a date format that is supported. This is because the <em>FORMAT</em> function relies on the CLR which will include a context/process jump.</p>
"
"63897611","Data Lake with Kimball's Star Schema and Data Mart","<h2>Objective</h2>
<p>I little bit confused by terminology: I've built Data Lake (not DW) based on Kimball's data modeling approaches and now not sure if I can use Data Mart definition to name my MPP database layer.</p>
<p>I came from the assumption that you still need Dimensional Modeling and Star Schema for mid+ size organization reports, same reasoning as in <a href=""https://www.advancinganalytics.co.uk/blog/2019/6/17/is-kimball-still-relevant-in-the-modern-data-warehouse"" rel=""nofollow noreferrer"">this article</a>.</p>
<h2>Questions</h2>
<ol>
<li><em>Is it right to call Synapse a Data Mart at <a href=""https://learn.microsoft.com/en-gb/azure/architecture/solution-ideas/articles/advanced-analytics-on-big-data"" rel=""nofollow noreferrer"">the following architecture</a> (see picture below)?</em></li>
<li><em>Can I say that I don't have DW (even if I have Star Schema), but instead I have Data Lake + Data Mart(s)?</em></li>
<li><em>Shall I split Synapse into multiple schemas based on business/reports sub-domains (multiple Data Marts)?</em></li>
</ol>
<h2>Architecture details</h2>
<p><a href=""https://i.stack.imgur.com/UBEqLl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UBEqLl.png"" alt=""enter image description here"" /></a></p>
<p>To be more specific, in my case:</p>
<p>2-3) ADLS + Databricks form Data Lake. All ETL and Star Schema build happens at Data Lake layer. All logic seats here. Still it has structured and unstructured data at raw layer, use cheap ADLS storage, lack Governance, has ML and will have streaming in the future. In other hand, we have schema-on-write in all DL zones except raw, we have tables modeled upfront (with a lot of requirements changes during the process). <em><strong>Am I right to call it Data Lake?</strong></em></p>
<p>4.) Synapse serves as a tiny projection/model of ETL/Lake results in order to speed up reports response time. Almost zero logic here, few aggregations. Only final model is loaded to Synapse.
Data are not splitted by business sub-domains, we just load everythin in a single DATAMART schema. <em>Is that a good approach?</em></p>
","<database-design><architecture><data-warehouse><databricks><data-lake>","2020-09-15 08:03:48","2077","1","1","63898961","<p>Firstly, I wouldn't get too bogged down in definitions as there are loads of (slightly) different definitions of these terms. However, given that, I would give a high-level definition of these terms as follows:</p>
<ol>
<li><p>Data Lake: this is your source data loaded into data store where you can start to analyse it. It is normally structured in the same way as it is in the source systems (i.e. it is the &quot;raw&quot; data) plus, optionally, some auditing columns to show where the data came from, when it was loaded, etc.
Some data lakes have multiple layers e.g. the raw data layer and then a governed data layer where the data has been cleansed, standardised, etc. - but is still in basically the same structure as in the raw data layer</p>
</li>
<li><p>Data Warehouse: this is your Kimball model of all your fact and dimension tables (plus other tables such as bridges). It will be built from the data that exists in your data lake</p>
</li>
<li><p>Data Mart: this is a subject area sourced from your data warehouse. This might be a logical definition (e.g. the Sales mart is the Sales fact table and related dimensions) or it might be physicalised e.g. a single wide table generated from a fact and its dimensions. How you define your datamarts is normally dependent on who/what is consuming them and what their requirements are. For example, you could have multiple Sales Data marts, all based off the same Sales Star, because you have multiple tools that prefer to consume data structured in particular ways</p>
</li>
</ol>
<p>Hope this helps?</p>
"
"63854572","Handling data quality issues on medium csv report. Best Practices","<h3>Need help with a <strong>better pratices</strong> question</h3>
<p>I have an <strong>azure function</strong> that brings data form differents APIs and match them toguether to create a final csv report. I have a poblation of 60k-100k and 30 columns</p>
<p>For the sake of the explanation, I'm going to use a small School example.</p>
<pre class=""lang-cs prettyprint-override""><code>public Student {
    string Grade {get; set;}
    Name   LegName {get; set;}
    string FatherName {get; set;}
    string TeacherId {get; set;}
    string SchoolId {get; set;}
}

public Name {
    string FirstName {get; set;}
    string LastName {get; set;}
}
</code></pre>
<p>Before constructing the report, I create two <code>Dictionary</code> with &lt;Id, Name&gt; from two APIs that expose Schools and Teachers information. And of course, a list of <code>Student</code> that comes from the Student APIs. I have no control of this trhee APIs, design, data quality, nothing.</p>
<p>Now, when I have all the data, I start to create the report.</p>
<pre class=""lang-cs prettyprint-override""><code>string GenerateTXT(Dictionary&lt;string, string&gt; schools, Dictionary&lt;string,  string&gt; teachers, Student students){
    StringBuilder content = new StringBuilder();

    foreach(var student in students){
        content.Append($&quot;{student.Grade}\t&quot;);
        content.Append($&quot;{student.LegName.FirstName}\t&quot;);
        content.Append($&quot;{student.LegName.LastName}\t&quot;);
        content.Append($&quot;{schools.TryGetValue(student.TeacherId)}\t&quot;);
        content.Append($&quot;{teachers.TryGetValue(student.SchoolId)}t&quot;;        
        content.Append($&quot;{student.FatherNme}\t&quot;);
        content.AppendLine();
    }

    return content.ToString();    
}
</code></pre>
<p>Now here comes the problem. I started noticing data quality issues so the function started throwing exceptions. For example, students who do not have a valid school or teacher, or a student who does not have a name. I tried to solve expected scenarios and exception handling.</p>
<pre class=""lang-cs prettyprint-override""><code>string GenerateTXT(Dictionary&lt;string, string&gt; schools, Dictionary&lt;string,  string&gt; teachers, Student students){
    StringBuilder content = new StringBuilder();
    var value = string.Empty;
    foreach(var student in students){
        try {
            content.Append($&quot;{student.Grade}\t&quot;);
            content.Append($&quot;{student.LegName.FirstName}\t&quot;);
            content.Append($&quot;{student.LegName.LastName}\t&quot;);            
            if(teachers.TryGetValue(student.TeacherId))
                content.Append($&quot;{teachers[student.TeacherId]}\t&quot;);
            else
                content.Append($&quot;\t&quot;);
            if(schools.TryGetValue(student.SchoolId))
                content.Append($&quot;{schools[student.SchoolId]}\t&quot;);
            else
                content.Append($&quot;\t&quot;);            
            content.Append($&quot;{student.FatherNme}\t&quot;);
            content.AppendLine();
        }
        catch(Exception ex) {
            log.Error($&quot;Error reading worker {student.FirstName}&quot;);
        }
        
    }
    return content.ToString();
}
</code></pre>
<p>The problem with this is that when an unexpected error happens, I <strong>stop</strong> reading the next columns of data that maybe I have and instead jump to the next worker. Therefore, if a student for some random reason does not have a name, that row in the report will only have the grade, and nothing else, but I actually had the rest of the values. So here comes the question. I could put a <code>try catch</code> on each column, but remember that my real scenario has like 30 columns and could be more... so I think it's a really bad solution. Is there a pattern to solve this in a better way?</p>
<h2>Thanks in advance!</h2>
","<c#><design-patterns><azure-functions>","2020-09-11 21:33:42","61","1","1","63856670","<p>So the first bit of advice I am going to give you is to use <a href=""https://www.nuget.org/packages/CsvHelper/"" rel=""nofollow noreferrer"">CsvHelper</a>. This is a tried and true library as it handles all those edge cases you will never think of. So, saying that, give this a shot:</p>
<pre><code>public class Student
{
    public string Grade { get; set; }
    public Name LegName { get; set; }
    public string FatherName { get; set; }
    public string TeacherId { get; set; }
    public string SchoolId { get; set; }
}

public class Name
{
    public string FirstName { get; set; }
    public string LastName { get; set; }
}

public class NormalizedData
{
    public string Grade { get; set; }
    public string FirstName { get; set; }
    public string LastName { get; set; }
    public string School { get; set; }
    public string Teacher { get; set; }
    public string FatherName { get; set; }
}

static void GenerateCSVData(CsvHelper.CsvWriter csv, Dictionary&lt;string, string&gt; schools,
    Dictionary&lt;string, string&gt; teachers, Student[] students)
{
    var normalizedData = students.Select(x =&gt; new NormalizedData
    {
        Grade = x.Grade,
        FatherName = x.FatherName,
        FirstName = x.LegName?.FirstName, // sanity check incase LegName is null
        LastName = x.LegName?.LastName, // ...
        School = schools.ContainsKey(x.SchoolId ?? string.Empty) ? schools[x.SchoolId] : null,
        Teacher = teachers.ContainsKey(x.TeacherId ?? string.Empty) ? teachers[x.TeacherId] : null
    });
    csv.WriteRecords(normalizedData);
}

private static string GenerateStringCSVData(Dictionary&lt;string, string&gt; schools,
    Dictionary&lt;string, string&gt; teachers, Student[] students)
{
    using(var ms = new MemoryStream())
    {
        using(var sr = new StreamWriter(ms, leaveOpen: true))
        using (var csv = new CsvHelper.CsvWriter(sr,
            new CsvConfiguration(CultureInfo.InvariantCulture)
        {
            Delimiter = &quot;,&quot;, // change this to &quot;\t&quot; if you want to use tabs
            Encoding = Encoding.UTF8
        }))
        {
            GenerateCSVData(csv, schools, teachers, students);
        }
        ms.Position = 0;
        return Encoding.UTF8.GetString(ms.GetBuffer(), 0, (int)ms.Length);
    }
}

private static int Main(string[] args)
{
    var teachers = new Dictionary&lt;string, string&gt;
    {
        { &quot;j123&quot;, &quot;Jimmy Carter&quot; },
        { &quot;r334&quot;, &quot;Ronald Reagan&quot; },
        { &quot;g477&quot;, &quot;George Bush&quot; }
    };
    var schools = new Dictionary&lt;string, string&gt;
    {
        { &quot;s123&quot;, &quot;Jimmy Carter University&quot; },
        { &quot;s334&quot;, &quot;Ronald Reagan University&quot; },
        { &quot;s477&quot;, &quot;George Bush University&quot; }
    };

    var students = new Student[]
    {
        new Student
        {
            FatherName = &quot;Bob Jimmy&quot;,
            SchoolId = &quot;s477&quot;,
            Grade = &quot;5&quot;,
            LegName = new Name{ FirstName = &quot;Apple&quot;, LastName = &quot;Jimmy&quot; },
            TeacherId = &quot;r334&quot;
        },
        new Student
        {
            FatherName = &quot;Jim Bobby&quot;,
            SchoolId = null, // intentional
            Grade = &quot;&quot;, // intentional
            LegName = null, // intentional
            TeacherId = &quot;invalid id&quot; // intentional
        },
        new Student
        {
            FatherName = &quot;Mike Michael&quot;,
            SchoolId = &quot;s123&quot;,
            Grade = &quot;12&quot;,
            LegName = new Name{ FirstName = &quot;Peach&quot;, LastName = &quot;Michael&quot; },
            TeacherId = &quot;g477&quot;
        },
    };

    var stringData = GenerateStringCSVData(schools, teachers, students);

    return 0;
}
</code></pre>
<p>This outputs:</p>
<pre><code>Grade,FirstName,LastName,School,Teacher,FatherName
5,Apple,Jimmy,George Bush University,Ronald Reagan,Bob Jimmy
,,,,,Jim Bobby
12,Peach,Michael,Jimmy Carter University,George Bush,Mike Michael
</code></pre>
<p>So, you can see, one of the students has invalid data in it, but it recovers just fine by placing blank data instead of crashing or throwing exceptions.</p>
<p>Now I haven't seen your original data, so there may be more tweaks you have to make to this to cover all edge cases, but it will be a lot easier to tweak this when using CsvHelper as your writer.</p>
"
"63794680","Dynamic SSIS package to load N tables from Oracle to SQL","<p>We have N tables on Oracle server and we wanted to load all those tables from Oracle to SQL server. We are creating dynamic SSIS packages for same which will take the Oracle ServerName, DB name, schema name, tables list etc. and will load all these tables to SQL server. We have added Link Server on SQL Server (SSMS) for Oracle.</p>
<p>But we are not getting the efficient way to do the same. How we can achieve this in a single SSIS package. How we can handle metadata of Oracle tables and creating the same on SQL server ? This SSIS package should create tables dynamically on SQL server as well , for this we tried Temp table in SSIS package.</p>
","<sql-server><oracle><ssis><data-warehouse><sql-data-warehouse>","2020-09-08 13:11:43","202","0","1","63798057","<p>Since you have to do it with a large number of tables, I'd write a pl/sql procedure something, built around something like this:</p>
<p>declare
v_sql varchar2(1024);</p>
<pre><code>begin
for x in (select owner, table_name from dba_tables where .....)
  v_sql := 'created table '||
           table_name ||
           '@mssql a select * from '||
           x.owner || '.' || x.table_name || ';';
  exec immediate v_sql;
end loop;
end;
/
</code></pre>
<p>or, if you want to look it over before launching, use sql to write sql.  In sqlplus:</p>
<pre><code>set echo off feedback off verify off trimsp on pages 0
spool doit.sql
select 'create table '||
        table_name ||
        '@mssql as select * from '||
        owner || '.' || table_name || ';'
from dba_tables
where .....
;
spool off
</code></pre>
<p>then check the spooled sql file for any issues before running.</p>
<p>All code above is off the top of my head. There may be minor syntax issues.</p>
"
"63790683","Choosing data catalog in pyathena?","<p>I'm trying to use pyathena (which looks simpler than the native boto3) to perform some queries .
However, I wasn't able to find how can I define which data catalog to use.
For example the query execution using boto3:</p>
<pre><code>    athena_client = boto3.client('athena')

    start_execution_response = athena_client.start_query_execution(
        QueryString='SELECT * FROM test_table',
        QueryExecutionContext={
            'Database': 'default',
            'Catalog': 'AwsDataCatalog'
        },
        ResultConfiguration={
            'OutputLocation': 's3://some_bucket/query_results/'
        }
    )
</code></pre>
<p>And using pyathena I can do something like that (and it worked):</p>
<pre><code>    cursor = connect(s3_staging_dir='s3://some_bucket/query_results/',
                     schema_name=&quot;some_db&quot;,
                     ).cursor()

    cursor.execute(&quot;SELECT * FROM table1&quot;)
</code></pre>
<p>But how can it work when I have multiple catalogs? Am I missing something?</p>
<p>Thanks,
Nir.</p>
","<amazon-web-services><amazon-athena><pyathena>","2020-09-08 09:07:46","790","1","2","63793888","<p>I would assume you can also just write a query with fully qualified table name</p>
<pre><code>SELECT * FROM AwsDataCatalog.default.test_table
</code></pre>
<p>And then reference to different catalogs.</p>
"
"63790683","Choosing data catalog in pyathena?","<p>I'm trying to use pyathena (which looks simpler than the native boto3) to perform some queries .
However, I wasn't able to find how can I define which data catalog to use.
For example the query execution using boto3:</p>
<pre><code>    athena_client = boto3.client('athena')

    start_execution_response = athena_client.start_query_execution(
        QueryString='SELECT * FROM test_table',
        QueryExecutionContext={
            'Database': 'default',
            'Catalog': 'AwsDataCatalog'
        },
        ResultConfiguration={
            'OutputLocation': 's3://some_bucket/query_results/'
        }
    )
</code></pre>
<p>And using pyathena I can do something like that (and it worked):</p>
<pre><code>    cursor = connect(s3_staging_dir='s3://some_bucket/query_results/',
                     schema_name=&quot;some_db&quot;,
                     ).cursor()

    cursor.execute(&quot;SELECT * FROM table1&quot;)
</code></pre>
<p>But how can it work when I have multiple catalogs? Am I missing something?</p>
<p>Thanks,
Nir.</p>
","<amazon-web-services><amazon-athena><pyathena>","2020-09-08 09:07:46","790","1","2","69221427","<p>You can specify catalog_name while creating a connection as below</p>
<pre><code> cursor = connect(s3_staging_dir='s3://some_bucket/query_results/',
                 catalog_name=&quot;catalog&quot;,
                 schema_name=&quot;some_db&quot;,
                 ).cursor()
 cursor.execute(&quot;SELECT * FROM table1&quot;)
</code></pre>
<p>You can refer to this source code, which mentions all the parameters it supports.
<a href=""https://github.com/laughingman7743/PyAthena/blob/master/pyathena/connection.py#227"" rel=""nofollow noreferrer"">https://github.com/laughingman7743/PyAthena/blob/master/pyathena/connection.py#227</a></p>
"
"63771746","Handle empty cell in Db using SSIS","<p>I have a database(Source) which has a column named &quot;country name&quot; and few cells in it are empty &amp; when I am transferring its data to another database (destination) it is also empty</p>
<p>I have tried to use derived column in which I had used REPLACE() but is didn't work I thought it would have been any &quot;&quot;, &quot; &quot; or &quot;\t&quot; but it was neither of these then I thought may be it is NULL &amp; I used ISNULL() but this also failed.</p>
","<ssis><etl><data-warehouse><business-intelligence>","2020-09-07 05:16:47","76","0","2","63783853","<p>I assume you are trying to work out what the &quot;empty&quot; data in the source column actually is? If thats the case then just convert the value to its ascii value and look that value up in an ascii table to find out what it is.</p>
<p>If the empty data is actually more than one character you may need to extract each character individually using, for example, the MID function.</p>
"
"63771746","Handle empty cell in Db using SSIS","<p>I have a database(Source) which has a column named &quot;country name&quot; and few cells in it are empty &amp; when I am transferring its data to another database (destination) it is also empty</p>
<p>I have tried to use derived column in which I had used REPLACE() but is didn't work I thought it would have been any &quot;&quot;, &quot; &quot; or &quot;\t&quot; but it was neither of these then I thought may be it is NULL &amp; I used ISNULL() but this also failed.</p>
","<ssis><etl><data-warehouse><business-intelligence>","2020-09-07 05:16:47","76","0","2","63794997","<p>You did not specify which DB is in the source and which is in the target, and it has meaning.<BR>
Anyway, add this code to the Derived Column component:<BR><BR></p>
<pre><code>(DT_WSTR,100)country_name == &quot;&quot; || ISNULL(country_name) ==  TRUE  ? &quot;new_value&quot; : country_name
</code></pre>
<p><strong>Results:</strong>
<BR><BR>
<a href=""https://i.stack.imgur.com/DV4JS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DV4JS.png"" alt=""enter image description here"" /></a></p>
"
"63769010","R Code for removing values within a group","<p>I am working with data similar to the data below:</p>
<pre><code>ID &lt;- seq(1,10,1)
Letter &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;,&quot;C&quot;, &quot;C&quot;, &quot;D&quot;, &quot;D&quot;, &quot;E&quot;,&quot;F&quot;, &quot;F&quot;)
df&lt;- data.frame(ID, Letter)
</code></pre>
<pre><code>   ID Letter
1   1      A
2   2      B
3   3      C
4   4      C
5   5      C
6   6      D
7   7      D
8   8      E
9   9      F
10 10      F
</code></pre>
<p>Looking specifically at the <code>Letter</code> column, I want to subset the data such that the <code>Letter</code> column only includes the <code>ID</code> value 4. But I want to keep all other values as well. So the data looks like:</p>
<pre><code>   ID Letter
1   1      A
2   2      B
4   4      C
6   6      D
7   7      D
8   8      E
9   9      F
10 10      F
</code></pre>
<p>Any help you can offer would be greatly appreciated!</p>
","<r><dataframe><data-management>","2020-09-06 21:21:27","48","1","3","63769031","<p>We could do a group by operation i.e. grouping by 'Letter', <code>if</code> there is <code>any</code> 'ID' having 4, just get the logical vector based on that comparison or <code>else</code> return everything (<code>TRUE</code>)</p>
<pre><code>library(dplyr)
df %&gt;%
     group_by(Letter) %&gt;%
     filter(if(any(ID == 4)) ID == 4 else TRUE)
# A tibble: 8 x 2
# Groups:   Letter [6]
#     ID Letter
#  &lt;dbl&gt; &lt;chr&gt; 
#1     1 A     
#2     2 B     
#3     4 C     
#4     6 D     
#5     7 D     
#6     8 E     
#7     9 F     
#8    10 F     
</code></pre>
<hr />
<p>Or another option without using a grouping would be</p>
<pre><code>df %&gt;%
   filter(Letter %in% setdiff(Letter, unique(Letter[ID == 4]))|ID == 4)
#  ID Letter
#1  1      A
#2  2      B
#3  4      C
#4  6      D
#5  7      D
#6  8      E
#7  9      F
#8 10      F
</code></pre>
<p>and its corresponding implementation in <code>base R</code></p>
<pre><code>subset(df, Letter %in% setdiff(Letter, unique(Letter[ID == 4]))|ID == 4)
</code></pre>
"
"63769010","R Code for removing values within a group","<p>I am working with data similar to the data below:</p>
<pre><code>ID &lt;- seq(1,10,1)
Letter &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;,&quot;C&quot;, &quot;C&quot;, &quot;D&quot;, &quot;D&quot;, &quot;E&quot;,&quot;F&quot;, &quot;F&quot;)
df&lt;- data.frame(ID, Letter)
</code></pre>
<pre><code>   ID Letter
1   1      A
2   2      B
3   3      C
4   4      C
5   5      C
6   6      D
7   7      D
8   8      E
9   9      F
10 10      F
</code></pre>
<p>Looking specifically at the <code>Letter</code> column, I want to subset the data such that the <code>Letter</code> column only includes the <code>ID</code> value 4. But I want to keep all other values as well. So the data looks like:</p>
<pre><code>   ID Letter
1   1      A
2   2      B
4   4      C
6   6      D
7   7      D
8   8      E
9   9      F
10 10      F
</code></pre>
<p>Any help you can offer would be greatly appreciated!</p>
","<r><dataframe><data-management>","2020-09-06 21:21:27","48","1","3","63769082","<p>Here is a base R option</p>
<pre><code>subset(df,ave(ID==4,Letter,FUN = function(x) Negate(any)(x)|x))
</code></pre>
<p>giving</p>
<pre><code>   ID Letter
1   1      A
2   2      B
4   4      C
6   6      D
7   7      D
8   8      E
9   9      F
10 10      F
</code></pre>
"
"63769010","R Code for removing values within a group","<p>I am working with data similar to the data below:</p>
<pre><code>ID &lt;- seq(1,10,1)
Letter &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;,&quot;C&quot;, &quot;C&quot;, &quot;D&quot;, &quot;D&quot;, &quot;E&quot;,&quot;F&quot;, &quot;F&quot;)
df&lt;- data.frame(ID, Letter)
</code></pre>
<pre><code>   ID Letter
1   1      A
2   2      B
3   3      C
4   4      C
5   5      C
6   6      D
7   7      D
8   8      E
9   9      F
10 10      F
</code></pre>
<p>Looking specifically at the <code>Letter</code> column, I want to subset the data such that the <code>Letter</code> column only includes the <code>ID</code> value 4. But I want to keep all other values as well. So the data looks like:</p>
<pre><code>   ID Letter
1   1      A
2   2      B
4   4      C
6   6      D
7   7      D
8   8      E
9   9      F
10 10      F
</code></pre>
<p>Any help you can offer would be greatly appreciated!</p>
","<r><dataframe><data-management>","2020-09-06 21:21:27","48","1","3","63770060","<p>We can <code>subset</code> <code>ID = 4</code> or all other values where <code>Letter</code> is different than the values in <code>ID = 4</code>.</p>
<pre><code>subset(df, ID == 4 | !Letter %in% Letter[ID == 4])

#   ID Letter
#1   1      A
#2   2      B
#4   4      C
#6   6      D
#7   7      D
#8   8      E
#9   9      F
#10 10      F
</code></pre>
<p>This can also be written in <code>dplyr</code> as :</p>
<pre><code>library(dplyr)
df %&gt;% filter(ID == 4 | !Letter %in% Letter[ID == 4])
</code></pre>
"
"63751351","ETL for CSV to OLE db using SSIS","<p>I have data like</p>
<pre><code>ID,Hospital name,address,zipcode
1,SHELBY medical center,1000 FIRST STREET NORTH,ALABASTER,11205
2,CALLAHAN EYE HOSPITAL,1720 UNIVERSITY BLVD,BIRMINGHAM,25844
</code></pre>
<p>but the problem I am facing here is when I use comma as a delimiter it is shifting the area name to zipcode column
for example in second row it is shifting 'BIRMINGHAM' to zipcode column</p>
","<csv><ssis><etl><data-warehouse><business-intelligence>","2020-09-05 07:05:02","41","0","1","63752430","<p>As you've found out, you can't define a delimiter as one character and then generate data that doesn't use that character, uniquely, as a delimiter.</p>
<p>You will need to put quotes round the strings e.g.</p>
<p>1,&quot;SHELBY medical center&quot;,&quot;1000 FIRST STREET NORTH,ALABASTER&quot;,11205
2,&quot;CALLAHAN EYE HOSPITAL&quot;,&quot;1720 UNIVERSITY BLVD,BIRMINGHAM&quot;,25844</p>
<p>Alternatively, you would need to generate the data with a delimiter that can never appear in your strings, possibly a pipe character:</p>
<p>1|SHELBY medical center|1000 FIRST STREET NORTH,ALABASTER|11205
2|CALLAHAN EYE HOSPITAL|1720 UNIVERSITY BLVD,BIRMINGHAM|25844</p>
"
"63742064","How to find unused rows in a dimension table","<p>I have a dimension table in my database that has grown too large. With that I mean that is has too many records - over a million - because it grew at the same pace as the linked facts. This is mostly due to a bad design, and I'm trying to clean it up.</p>
<p>One of the things I try to do is to remove dimension records which are no longer used. The fact tables are regularly maintained and old snapshots are removed. Because the dimensions were not maintained like that, there are many rows in the table whose primary key value no longer appears in any of the linked fact tables anymore.
All the fact tables have foreign key constraints.</p>
<p>Is there a way to locate table rows whose primary key value no longer appears in any of the tables which are linked with a foreign key constraint?</p>
<p>I tried writing a script to track this. Basically this:</p>
<pre><code>select key from dimension 
where not exists (select 1 from fact1 where fk = pk) 
and not exists (select 1 from fact2 where fk = pk) 
and not exists (select 1 from fact3 where fk = pk)
</code></pre>
<p>But with a lot of linked tables this query dies after some time - at least, my management studio crashed. So I'm not sure if there are any other options.</p>
","<sql-server><tsql><data-warehouse><sql-data-warehouse>","2020-09-04 13:22:21","418","2","2","63744518","<p>You may want to split that into different queries. Check unused rows in fact1, then on fact2, etc, individually. Then intersect all those results to get to the rows that are unused in all fact tables.</p>
<p>I would also suggest a left outer join instead of nested queries, counting rows in the fact table for each pk, and filter out from the resultset those that have a non zero count.</p>
<p>Your query will struggle as it’ll scan every fact table at the same time.</p>
"
"63742064","How to find unused rows in a dimension table","<p>I have a dimension table in my database that has grown too large. With that I mean that is has too many records - over a million - because it grew at the same pace as the linked facts. This is mostly due to a bad design, and I'm trying to clean it up.</p>
<p>One of the things I try to do is to remove dimension records which are no longer used. The fact tables are regularly maintained and old snapshots are removed. Because the dimensions were not maintained like that, there are many rows in the table whose primary key value no longer appears in any of the linked fact tables anymore.
All the fact tables have foreign key constraints.</p>
<p>Is there a way to locate table rows whose primary key value no longer appears in any of the tables which are linked with a foreign key constraint?</p>
<p>I tried writing a script to track this. Basically this:</p>
<pre><code>select key from dimension 
where not exists (select 1 from fact1 where fk = pk) 
and not exists (select 1 from fact2 where fk = pk) 
and not exists (select 1 from fact3 where fk = pk)
</code></pre>
<p>But with a lot of linked tables this query dies after some time - at least, my management studio crashed. So I'm not sure if there are any other options.</p>
","<sql-server><tsql><data-warehouse><sql-data-warehouse>","2020-09-04 13:22:21","418","2","2","63755605","<p>we had to do something similar to this at one of my clients. The query, like yours with &quot;not exists.... and not exists.... and not exists....&quot; was taking ~22 hours to run before we change our strategy to handle this in ~20 minutes.</p>
<p>As Nsousa suggest, you have to split the query so SQL Server doesn't have to handle all data in one shot, having to unnecessarily use tempdb and all other things.</p>
<p>First, create new table with all keys in it. The reason to create this table is to not have to read the full table scan for every query, having more keys on a 8k page and to deal with a smaller and smaller set of keys after each delete.</p>
<pre><code>create table DimensionkeysToDelete (Dimkey char(32) primary key nonclustered);
insert into DimensionkeysToDelete 
select key from dimension order by key; 
</code></pre>
<p>Then, instead of deleting unused key, delete the keys that exists in facts table, beginning with the fact table that has the least numbers of rows.
Make sure facts table have proper indexing for performance.</p>
<pre><code>delete from DimensionkeysToDelete 
from DimensionkeysToDelete d 
inner join fact1 on f.fk = d.Dimkey;

delete from DimensionkeysToDelete 
from DimensionkeysToDelete d 
inner join fact2 on f.fk = d.Dimkey;

delete from DimensionkeysToDelete 
from DimensionkeysToDelete d 
inner join fact3 on f.fk = d.Dimkey;
</code></pre>
<p>Once all facts tables done, only unused keys remains in DimensionkeysToDelete.  To answers your question, just perform a select on this table to get all unused key for that particular dimension, or join it with the dimension to get data.</p>
<p>But, from what I understand of your needs for cleaning up you warehouse, use this table to delete from the orignal dimension table. At this step, you might also want take some action for auditing purposes (ie: insert in an audit table 'Key ' + key + ' deleted on + convert(datetime, getdate(),121) + ' by script X'.... )</p>
<p>I think this can be optimize, take a look at the execution plan, but my client was happy with it so we didn't have to put much effort in it.</p>
"
"63718165","How to handle graph relationship in data warehouse?","<p>Imagine we have many companies and they are related to each other by relationships like &quot;A is a buyer of B&quot;, &quot;B is a seller of A&quot;, &quot;C is a subsidiary of B&quot;, etc.</p>
<p>We may use a graph database to handle those relationships. But in a typical data warehouse environment (table format), how can we represent those relationships effectively?</p>
<p>Currently, I am thinking of using a relationship table to store relationships between companies and a separate table for storing company attributes.</p>
","<database><data-warehouse>","2020-09-03 06:41:12","27","0","1","63728706","<p>(I'm assuming we are talking about generic relationships rather than hierarchies - which are a very specific type of relationship)</p>
<p>In a Data Warehouse, Company data would be held in a Dimension table and (apart from some special cases) you don't relate Dimensions to each other directly: they are related to each other via a fact table.</p>
<p>Where a fact has more than one relationship to the same dimension then you just have multiple FKs in the Fact pointing to the Dimension (Role-playing dimensions). So for a transactional fact you might have a Buyer FK, a Seller FK, etc. all referencing records in the Company dimension.</p>
<p>If your relationships have any real level of complexity then, as the previous comment states, you would be much better off using a graph DB if you need to report on those relationships.</p>
<p>If you have a limited number of relationship types and each relationship type has a fixed cardinality then you might be able to model it in a factless fact table i.e. a fact table that just contains FKs to the Company dimension. In order to get this to work in any useful way, as a starting point you'd probably need to be able to define a single core role and then show the relationships to that role. For example, you might start with a Subsidiary for each fact record and then show the related Parent, Buyer, etc. to that Subsidiary</p>
"
"63680824","2 Fact tables and business wants to know percent of total of one fact to another fact","<p>We have 2 Fact table in our warehouse.<br />
CaseFacts and IncidentFacts.  Both have dimension joins to date dimension and other various items</p>
<p>In our OLTP these 2 tables are related as each Incident of a Case.  So a Case can have multiple Incidents.</p>
<p>OLTP table structure</p>
<ul>
<li>Cases Table CaseId (Primary key)</li>
<li>Address</li>
<li>etc..</li>
</ul>
<p>Incident Table</p>
<ul>
<li>IncidentId (Primary)</li>
<li>CaseId (Foreign key to Cases table)</li>
<li>IncidentType</li>
<li>etc..</li>
</ul>
<p>Case is 1:n to Incidents</p>
<p>The question is this:
The business wants to know the Percent of total cases that have an IncidentType of &quot;X&quot;</p>
<p>In our warehouse, if we don't join one fact table to another, I'm not sure what the best way to handle this.</p>
<p>Basic data of what I need:</p>
<ol>
<li>Need distinct CaseId and IncidentType for a date time range</li>
<li>Then need incident type counts (numerator)</li>
<li>then need number of Cases in that same time range (denominator)</li>
<li>then could be grouped by anything else like State, Region, Department..</li>
</ol>
<p>Not sure what to do for the warehouse structure if needing 2 data points from different datamarts essentially.</p>
","<reporting><data-warehouse>","2020-09-01 03:15:05","103","1","1","63687342","<p>This is a standard pattern in data warehousing: the correct approach (in most circumstances) is to retrieve the data in 2 separate queries, join the 2 result sets together (using the common, conformed, dimensions) and then query this combined result set to get the final answer.</p>
<p>For your specific example I would suggest:</p>
<p><strong>Query 1</strong>: count distinct CaseId from incident table grouped by any required dimension values</p>
<p><strong>Query 2</strong>: count IncidentId from incident table where IncidentType = &quot;X&quot; grouped by any required dimension values (may need additional logic if a case can have multiple incidents of the same type and how you want to handle this in your calculation). These grouping values would need to match those in Query 1.</p>
<p><strong>Query 3</strong>: join Query 1 results to Query 2 results by the common &quot;group by&quot; fields and divide one count by the other to get the percentage</p>
<p>Exactly how you would write this in SQL would depend on your DBMS but using CTEs is probably the easiest solution in most cases</p>
"
"63676131","Different SCD types for different columns","<p><em>Does it make sense to have different SCD types for different columns at single dimension? Or it's always better to split a dimension table into two tables?</em></p>
<p>For example, the operational systems sends me the following data:</p>
<pre><code>ID | CHANNEL_CODE | NAME | TYPE
1  |      A       |  X   |  0
2  |      B       |  Y   |  1
</code></pre>
<p>Where <code>CHANNEL_CODE</code> = natural key, <code>TYPE</code> should be SCD type 2, while <code>NAME</code> could be SCD type 1. Let say <code>NAME</code> is frequently changed.</p>
","<sql><data-warehouse><dimensional-modeling><star-schema><scd>","2020-08-31 18:19:01","554","0","3","63678981","<p>I always keep Type 0, 1 and 2 in one table.</p>
<p>I also treat Type 0-s and Type 1-s the same, and Type 2-s the other way.</p>
<ul>
<li>Type 1/0 changes lead to updating all versions in the SCD table for the same business/natural ID to reflect the new info.</li>
<li>Type 2 changes lead to updating the newest row in the SCD table to contain the new change date as the valid-to date, and the current-indicator to contain FALSE, and a new row with the new data,valid-from as the change date, valid-to to '9999-12-31'.</li>
</ul>
<p>With this type of staging data:</p>
<pre><code>CREATE TABLE stg_customer (
     cust_id            INT NOT NULL -- natural identifier
    ,cust_chg_dt        DATE         -- change date
    ,cust_first_name    VARCHAR(30)  -- type 1
    ,cust_last_name     VARCHAR(30)  -- type 1
    ,cust_phoneno       VARCHAR(30)  -- type 1
    ,cust_loyalty_lvl   INT          -- type 2
    ,cust_org_id        INT          -- type 2
) ;
</code></pre>
<p>... my SCD table looks like so:</p>
<pre><code>CREATE TABLE dim_customer_scd (
  cust_key         BIGINT 
      DEFAULT dim_customer_scd_seq.NEXTVAL NOT NULL -- surrogate key, primary key
, cust_id          BIGINT       NOT NULL  -- natural identifier
, cust_from_dt     DATE         NOT NULL  -- effective begin date
, cust_to_dt       DATE         NOT NULL  -- effective end date
, cust_is_current  BOOLEAN 
    DEFAULT(cust_to_dt='9999-12-31') NOT NULL -- current indicator
, cust_cre_ts      TIMESTAMP(0) NOT NULL  -- created timestamp
, cust_udt_ts      TIMESTAMP(0) NOT NULL  -- updated timestamp
, cust_first_name  VARCHAR(30)            -- Type 1 column
, cust_last_name   VARCHAR(30)            -- Type 1 column
, cust_phoneno     VARCHAR(30)            -- Type 1 column
, cust_loyalty_lvl INT                    -- Type 2 column
, cust_org_id      INT                    -- Type 2 column
)
;
</code></pre>
"
"63676131","Different SCD types for different columns","<p><em>Does it make sense to have different SCD types for different columns at single dimension? Or it's always better to split a dimension table into two tables?</em></p>
<p>For example, the operational systems sends me the following data:</p>
<pre><code>ID | CHANNEL_CODE | NAME | TYPE
1  |      A       |  X   |  0
2  |      B       |  Y   |  1
</code></pre>
<p>Where <code>CHANNEL_CODE</code> = natural key, <code>TYPE</code> should be SCD type 2, while <code>NAME</code> could be SCD type 1. Let say <code>NAME</code> is frequently changed.</p>
","<sql><data-warehouse><dimensional-modeling><star-schema><scd>","2020-08-31 18:19:01","554","0","3","63687035","<p>The answer, IMO, is it depends...</p>
<p>If you need the current and historic values of fields in a Dimension, and the volume of data + frequency of change makes it practicable, then implement <strong>SCD Type 7</strong>.</p>
<p>If the data for a small number of fields in the dimension is volatile, and the number of records in the dimension is large, then implement <strong>SCD Type 4</strong> or <strong>SCD Type 5</strong></p>
"
"63676131","Different SCD types for different columns","<p><em>Does it make sense to have different SCD types for different columns at single dimension? Or it's always better to split a dimension table into two tables?</em></p>
<p>For example, the operational systems sends me the following data:</p>
<pre><code>ID | CHANNEL_CODE | NAME | TYPE
1  |      A       |  X   |  0
2  |      B       |  Y   |  1
</code></pre>
<p>Where <code>CHANNEL_CODE</code> = natural key, <code>TYPE</code> should be SCD type 2, while <code>NAME</code> could be SCD type 1. Let say <code>NAME</code> is frequently changed.</p>
","<sql><data-warehouse><dimensional-modeling><star-schema><scd>","2020-08-31 18:19:01","554","0","3","63996779","<p>As is typically the case, it depends.</p>
<p>First, make sure you understand the distinction between the OLAP model and the RDBMS storage.  The RDBMS storage strategy does not have to reflect the OLAP model.</p>
<p>You can easily split your SCD types into separate tables and than join them in a view to present a single source to the OLAP model.  This allows the underlying tables to have completely different ETL strategies.</p>
<p>If most of your columns are type 2, separate tables doesn't make much sense as there will be little gain.  However, if most of your columns are not type 2, you may see some performance gains in the ETL and processing tasks.  Skinner tables are quicker to read and update in OLAP processing.</p>
<p>On a separate note, if you use a view, I recommend investigating using left joins to combine the tables.  In many OLAP load processing models, the load processing queries for a single attribute at a time.  With multiple tables as left joins, the query processor can discard the table references that are not needed to provide the results, rather than filtering the data through inner joins.  This can improve overall OLAP processing speed.</p>
"
"63676015","Am I implementing SCD type 1 & 7 correctly","<h2>SCD type 1</h2>
<p>Suppose I've built SCD type 1 over the following data that comes from operational system:</p>
<pre><code>ID | CHANNEL_CODE | NAME | TYPE
1  |      A       |  X   |  0
2  |      B       |  Y   |  1
</code></pre>
<p>Because, <a href=""https://stackoverflow.com/questions/60736651/scd-1-dimension-without-surrogate-key"">Surrogate Keys are preferable even for SCD type 1</a>, we're throwing away <code>ID</code> column and generating <code>SRK</code> from natural key (<code>CHANNEL_CODE</code>):</p>
<pre><code>SRK | CHANNEL_CODE | NAME | TYPE
11  |      A       |  X   |  0
12  |      B       |  Y   |  1
</code></pre>
<p>Means <code>CHANNEL_CODE</code> is expected to never change, in case of <code>NAME</code> or <code>TYPE</code> update - overwrite happens.</p>
<p><em>Is this the right standard implementation of SCD type 1?</em></p>
<h2>SCD 1 + durable key</h2>
<p>Natural keys may be changed because of sim or credit card change, duplicates, integration of source systems, business reasons, etc. From <a href=""https://www.kimballgroup.com/2012/07/design-tip-147-durable-super-natural-keys/"" rel=""nofollow noreferrer"">Kimball's Design Tip #147</a>, I know the issue is solved with <em>durable</em> srk.</p>
<p>Means, operational system have to send me an event like: &quot;From now CHANNEL_CODE=A is CHANNEL_CODE=C&quot;. So I should have the following data (fact table contains both srks):</p>
<pre><code>DURABLE_SRK | SRK | CHANNEL_CODE | NAME | TYPE
    11      | 11  |      A       |  X   |  0
    12      | 12  |      B       |  Y   |  1
    11      | 13  |      C       |  X   |  0
</code></pre>
<p>Still change to <code>NAME</code> or <code>TYPE</code> columns will result in simple overwrite (no new rows).</p>
<p><em>Shall <code>NAME</code> be overwritten here by <code>SRK</code> or <code>DURABLE_SRK</code>? Is it still an SCD 1?</em></p>
<h2>SCD type 7</h2>
<p>In my understanding, from <a href=""https://www.kimballgroup.com/2013/02/design-tip-152-slowly-changing-dimension-types-0-4-5-6-7/"" rel=""nofollow noreferrer"">Kimball's Design Tip #152</a>, <code>SCD 7 = SCD 1 + durable key + SCD 2 (history for not natural key columns)</code>. So SCD type 7 should generate a new row on every column update. For example, on <code>NAME update from X to Z where CHANNEL_CODE=C</code>:</p>
<pre><code>DURABLE_SRK | SRK | CHANNEL_CODE | NAME | TYPE | EFFECTIVE_START_DATE | EXPIDATION_DATE | IS_CURRENT_IND
    11      | 11  |      A       |  X   |  0   |      2020-05-02      |    2020-06-12   | False
    12      | 12  |      B       |  Y   |  1   |      2020-01-12      |    2100-01-01   | True
    11      | 13  |      C       |  X   |  0   |      2020-06-12      |    2020-08-15   | False
    11      | 13  |      C       |  Z   |  0   |      2020-08-15      |    2100-01-01   | True
</code></pre>
<p><em>Is this correct implementation of SCD type 7?</em></p>
","<sql><data-warehouse><dimensional-modeling><star-schema><scd>","2020-08-31 18:09:46","614","1","1","63686865","<p><strong>SCD TYPE1</strong></p>
<p>Yes, that's correct though there is no need to throw away ID and I would probably keep it as it may help in your ETL and for debugging purposes as it allows the corresponding record in the source system to be easily identified (see next paragraph for an example).</p>
<p><strong>SCD 1 + Durable Key</strong></p>
<p>If this is meant to be SCD1 then your example is incorrect. If the channel code on the same source record has changed then it would overwrite the record in the  Dimension table, not insert a new record. This is a good example of why you should retain the ID as it makes it obvious how the records in your dimension relate to your source. For an SCD1, a SK and a Durable SK are, almost by definition, the same thing.</p>
<p>I realise that your examples may be simplified compared to a real-world scenario but I would suggest that Channel Code is a genuine natural key and therefore would never change: a different Channel Code would imply a different record. A natural key only really changes when there is no genuine unique business identifier in the source record e.g. a person might have a genuine unique identifier such as social security number (which never changes) but if that wasn't available they might be identified by first name, last name and email address - any of which might change and therefore aren't genuine natural keys - and this would be a good case for including a Durable SK.</p>
<p><strong>SCD Type 7</strong></p>
<p>For this type, the Dimension table is entirely SCD Type 2 and includes a Durable SK. The SCD1 aspect can be thought of as virtual, as it is implemented as a View over the Dimension where the Current Flag = True. Any Fact table joining to this table has two FKs - one that holds the Dimension SK for the row applicable at the time of the event (standard SCD2 logic) and one that holds the Durable SK and references the View (to get the SCD1-like record)</p>
"
"63667686","How to combine columns based on another column","<p>I have a data frame similar to this one. Where each 'id' has values connected to three groups. Right now there are 15 rows and 3 columns.</p>
<pre><code>id &lt;- c(1,1,1,2,2,2,3,3,3,4,4,4,5,5,5)
group &lt;- c('Group1','Group2', 'Group3','Group1','Group2', 'Group3','Group1','Group2', 'Group3','Group1','Group2', 'Group3','Group1','Group2', 'Group3')
value &lt;- c(49, 76, 14, 97, 78, 51, 48, 44, 81, 74, 85, 11, 82, 63, 91)

df &lt;- cbind(id,group,value)
</code></pre>
<p>This gives the following data frame.</p>
<pre><code>      id  group    value
 [1,] &quot;1&quot; &quot;Group1&quot; &quot;49&quot; 
 [2,] &quot;1&quot; &quot;Group2&quot; &quot;76&quot; 
 [3,] &quot;1&quot; &quot;Group3&quot; &quot;14&quot; 
 [4,] &quot;2&quot; &quot;Group1&quot; &quot;97&quot; 
 [5,] &quot;2&quot; &quot;Group2&quot; &quot;78&quot; 
 [6,] &quot;2&quot; &quot;Group3&quot; &quot;51&quot; 
 [7,] &quot;3&quot; &quot;Group1&quot; &quot;48&quot; 
 [8,] &quot;3&quot; &quot;Group2&quot; &quot;44&quot; 
 [9,] &quot;3&quot; &quot;Group3&quot; &quot;81&quot; 
[10,] &quot;4&quot; &quot;Group1&quot; &quot;74&quot; 
[11,] &quot;4&quot; &quot;Group2&quot; &quot;85&quot; 
[12,] &quot;4&quot; &quot;Group3&quot; &quot;11&quot; 
[13,] &quot;5&quot; &quot;Group1&quot; &quot;82&quot; 
[14,] &quot;5&quot; &quot;Group2&quot; &quot;63&quot; 
[15,] &quot;5&quot; &quot;Group3&quot; &quot;91&quot;
</code></pre>
<p>I want to reshape the data set so it has 5 rows and 4 columns. One row for each unique 'id' and one column for each group's value. The desired outcome looks like this:</p>
<pre><code>     id Group1_value Group2_value Group3_value
[1,]  1           49           76           14
[2,]  2           97           78           51
[3,]  3           48           44           81
[4,]  4           74           85           11
[5,]  5           82           63           91
</code></pre>
<p>Thanks in advance!</p>
","<r><data-management>","2020-08-31 08:52:33","230","0","2","63667775","<p>If you do not mind using some packages from tidyverse, you can do:</p>
<pre><code>library(dplyr)
library(tidyr)
df = as_tibble(df) # you can also use as.data.frame(), this is necessary bcs matrix do not work with pivot_wider
df %&gt;% mutate(group = paste0(group, &quot;_value&quot;)) %&gt;% pivot_wider(names_from = group, values_from = value) # the mutate part is necessary only if you really want &quot;_value&quot; in the final column names
</code></pre>
<p>And here is the output:</p>
<pre><code>&gt; df %&gt;% mutate(group = paste0(group, &quot;_value&quot;)) %&gt;% pivot_wider(names_from = group, values_from = value)
# A tibble: 5 x 4
  id    Group1_value Group2_value Group3_value
  &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;       
1 1     49           76           14          
2 2     97           78           51          
3 3     48           44           81          
4 4     74           85           11          
5 5     82           63           91 
</code></pre>
"
"63667686","How to combine columns based on another column","<p>I have a data frame similar to this one. Where each 'id' has values connected to three groups. Right now there are 15 rows and 3 columns.</p>
<pre><code>id &lt;- c(1,1,1,2,2,2,3,3,3,4,4,4,5,5,5)
group &lt;- c('Group1','Group2', 'Group3','Group1','Group2', 'Group3','Group1','Group2', 'Group3','Group1','Group2', 'Group3','Group1','Group2', 'Group3')
value &lt;- c(49, 76, 14, 97, 78, 51, 48, 44, 81, 74, 85, 11, 82, 63, 91)

df &lt;- cbind(id,group,value)
</code></pre>
<p>This gives the following data frame.</p>
<pre><code>      id  group    value
 [1,] &quot;1&quot; &quot;Group1&quot; &quot;49&quot; 
 [2,] &quot;1&quot; &quot;Group2&quot; &quot;76&quot; 
 [3,] &quot;1&quot; &quot;Group3&quot; &quot;14&quot; 
 [4,] &quot;2&quot; &quot;Group1&quot; &quot;97&quot; 
 [5,] &quot;2&quot; &quot;Group2&quot; &quot;78&quot; 
 [6,] &quot;2&quot; &quot;Group3&quot; &quot;51&quot; 
 [7,] &quot;3&quot; &quot;Group1&quot; &quot;48&quot; 
 [8,] &quot;3&quot; &quot;Group2&quot; &quot;44&quot; 
 [9,] &quot;3&quot; &quot;Group3&quot; &quot;81&quot; 
[10,] &quot;4&quot; &quot;Group1&quot; &quot;74&quot; 
[11,] &quot;4&quot; &quot;Group2&quot; &quot;85&quot; 
[12,] &quot;4&quot; &quot;Group3&quot; &quot;11&quot; 
[13,] &quot;5&quot; &quot;Group1&quot; &quot;82&quot; 
[14,] &quot;5&quot; &quot;Group2&quot; &quot;63&quot; 
[15,] &quot;5&quot; &quot;Group3&quot; &quot;91&quot;
</code></pre>
<p>I want to reshape the data set so it has 5 rows and 4 columns. One row for each unique 'id' and one column for each group's value. The desired outcome looks like this:</p>
<pre><code>     id Group1_value Group2_value Group3_value
[1,]  1           49           76           14
[2,]  2           97           78           51
[3,]  3           48           44           81
[4,]  4           74           85           11
[5,]  5           82           63           91
</code></pre>
<p>Thanks in advance!</p>
","<r><data-management>","2020-08-31 08:52:33","230","0","2","63667785","<p>Here is a base R solution:</p>
<pre><code>reshape(as.data.frame(df), v.names = &quot;value&quot;, idvar = &quot;id&quot;, 
        timevar = &quot;group&quot;, direction = &quot;wide&quot;)

#&gt;    id value.Group1 value.Group2 value.Group3
#&gt; 1   1           49           76           14
#&gt; 4   2           97           78           51
#&gt; 7   3           48           44           81
#&gt; 10  4           74           85           11
#&gt; 13  5           82           63           91
</code></pre>
"
"63660018","Loading different types of files into a database","<p>At my work, we receive thousands of emails per day with attached files (xlsx, csv, xml, html, pdf, etc). Those emails get processed by a program and the files get downloaded and filtered into different folders depending on the sender.</p>
<p>The data in those files is then loaded into our MS SQL Server db by a proprietary software for which we are charged a hefty sum of money each year.</p>
<p>Now, I'm sure this is a super common process for enterprises, so there are probably some open source tools that we could use to replace this software.</p>
<p>How do most people do this? With individual scripts? What would be the correct way to do it?</p>
<p>Thank you very much!</p>
<p>EDIT: all the email attachments get converted to xlsx before being moved to their respective folders.</p>
","<database><etl><data-warehouse><data-processing>","2020-08-30 17:10:10","64","0","1","64045177","<p>The 'correct' way would be to not use Excel as the interchange format - its fuzzy notion of data types (e.g. numbers vs. text) creates many issues.</p>
<p>Still, Excel feeds are a fact of life, so it's crucial to have a comprehensive and reliable way of dealing with multiple feeds that sometimes fail. I like <a href=""https://docs.envobi.com/articles/process-incoming-files-example.html"" rel=""nofollow noreferrer"">this approach</a>; the example uses my company's commercial <a href=""https://envobi.com/"" rel=""nofollow noreferrer"">cross-platform .NET ETL library</a>, but I've also used the same approach with several other ETL tools previously.</p>
<p>Cheers,
Kristian</p>
"
"63611767","Sql Server Why call it a Time Dimension?","<p>I have concerning question as to why when preparing Cubes in sql server they have have a time DIMENSION.
From my studies on time I have always learnt that
time in math is a function.
t squared is speed
t cubed is velocity.
ok
a time dimension sounds like something out of sci fi film.
so why did they call it this?</p>
<p>Any ideas.</p>
","<multidimensional-array><data-warehouse><dimensions><star-schema>","2020-08-27 08:09:20","50","0","2","63611802","<p>Dimensional modeling (DM) is part of the Business Dimensional Lifecycle methodology developed by Ralph Kimball.</p>
<p>In DM you have fact tables and dimentsion tables.</p>
<p>A fact table consists of the measurements, metrics or facts of a business process.</p>
<p>A dimension table is one of the set of companion tables to a fact table. They contain descriptive attributes that generally provide the context for  (how,where,when) the fact occured.</p>
<p>So you if you have a table dimension that contains time, you would call it a <code>time dimension</code>. Most of the time however, it is called a date dimension.</p>
"
"63611767","Sql Server Why call it a Time Dimension?","<p>I have concerning question as to why when preparing Cubes in sql server they have have a time DIMENSION.
From my studies on time I have always learnt that
time in math is a function.
t squared is speed
t cubed is velocity.
ok
a time dimension sounds like something out of sci fi film.
so why did they call it this?</p>
<p>Any ideas.</p>
","<multidimensional-array><data-warehouse><dimensions><star-schema>","2020-08-27 08:09:20","50","0","2","63997345","<p>Date and Time dimensions serve very different purposes.  You must remember that the purpose of a dimension is to describe the event(Fact) that occurred.  Whereas a date dimension can be used to answer questions like events on a holiday or events per quarter, the Time dimension usually breaks the grain down to the second (but can go lower) to assign additional meaning beyond just the ticks of the clock.</p>
<p>For example your business may designate business hours as 9am-5pm.  You can associate a flag attribute &quot;Business Hours&quot; to each second to allow you to easily identify facts occurring after business hours.  If you have multiple overlapping work shifts, you can create a column for each shift to indicate if that time corresponds to that shift.</p>
<p>A time dimension relationship should tell you more than just when the event occurred by adding additional analytical layers to the When.</p>
<p>For example, relating events to multiple time dimensions (e.g. local vs UTC) can also allow interesting analysis across time zones (How much arson occurs between 1 - 2am) or (How much arson occurred after the flying spaghetti monster attacked at 1:23am UTC).</p>
<p>FYI: I would expressly recommend against combining time and date into a single DateTime dimension.  Clock seconds are finite (86400 per day); days are infinite.  DateTime dimension at the second grain can become unmanageably large very quickly.  However, some analysis needs this grain, specifically when looking at events dependent on sunrise/sunset or other attributes that vary based on the Date AND Time.</p>
"
"63578813","In CRM Design, should it be mandatory to update a phone number whenever an address change is requested?","<p>should it be mandatory to update a phone number whenever an address change is requested.</p>
<p>Thank you</p>
","<dynamics-crm><crm><application-design><data-quality>","2020-08-25 12:25:11","52","-2","1","63579000","<p>Depends. On a lot of factors.</p>
<p>For eg, in India, if the phone type is a landline and the address changed to a different city, the area code of the phone number will be different. Hence, allowing the user to update the phone number also while updating the address makes a lot of sense.</p>
<p>With mobile phones, there is number portability (in most jurisdictions). However, a user could choose to not port their mobile number (rare, but can happen for a number of reasons such as provider coverage in the new area being poor or unavailable). In this case also, it makes sense to update the phone number while updating the address.</p>
<p>So, there is a business case for simplifying the user experience with one single update screen instead of having to update address and phone separately upon change of address, IMHO.</p>
"
"63560360","ERD design to keep a record of changes made by users","<p>I am trying to design the ERD Schema for a small project. It is a loan business that tracks loans (Amounts, interest, to whom, repayment date, etc etc.) This is what i have so far, if you have any suggestions that would be great. But my main question is:</p>
<p>How do i track changes made to loans? For instance, a Lender changes the date that the repayment needs to be made by, I would normally just go into the DB and update the date value. However, now i want to be able to have a record of what the original value(date) was. Or someone with an existing loan takes out another loan, do i then make a new record in the Loan table with the combined values? and how can i store what the original value loaned was, and what the new amount is?</p>
<p>I think i maybe need some kind of DW? let me know what you think. Cheers</p>
<p><a href=""https://i.stack.imgur.com/o34wr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o34wr.png"" alt=""Current ERD diagram"" /></a></p>
","<sql><database-design><relational-database><data-warehouse><erd>","2020-08-24 11:43:14","447","-2","2","63560724","<blockquote>
<p>How do I track changes made to loans?</p>
</blockquote>
<p>Create a Loan Adjustment table.  You would put both the interest rate and repayment date in the adjustment row, whether one or both columns were adjusted.</p>
<p>You'll have to check if one or more adjustment rows exist whenever you SELECT the Loan row.  If you just want the most recent adjustment, add a WHERE MAX(Modified Timestamp).</p>
<pre><code>Loan Adjustment
---------------
Loan Adjustment ID
Loan ID
Borrower ID
Lender ID
Modified Interest Rate
Modified Repayment Date
Modified Timestamp
</code></pre>
<blockquote>
<p>Or someone with an existing loan takes out another loan, do I then
make a new record in the Loan table with the combined values?</p>
</blockquote>
<p>No.  You create a new loan row in the Loan table with a new Loan ID.  You can create an index on the Borrower ID in the Loan table and see if the borrower has more than one loan.</p>
<p>No, you don't need a data warehouse, unless you want to have a place to keep loans that have either been repaid or defaulted.  Active loans belong in an operational relational database.</p>
"
"63560360","ERD design to keep a record of changes made by users","<p>I am trying to design the ERD Schema for a small project. It is a loan business that tracks loans (Amounts, interest, to whom, repayment date, etc etc.) This is what i have so far, if you have any suggestions that would be great. But my main question is:</p>
<p>How do i track changes made to loans? For instance, a Lender changes the date that the repayment needs to be made by, I would normally just go into the DB and update the date value. However, now i want to be able to have a record of what the original value(date) was. Or someone with an existing loan takes out another loan, do i then make a new record in the Loan table with the combined values? and how can i store what the original value loaned was, and what the new amount is?</p>
<p>I think i maybe need some kind of DW? let me know what you think. Cheers</p>
<p><a href=""https://i.stack.imgur.com/o34wr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o34wr.png"" alt=""Current ERD diagram"" /></a></p>
","<sql><database-design><relational-database><data-warehouse><erd>","2020-08-24 11:43:14","447","-2","2","63567075","<p>Similar to the above answer, but I would create a Loan Header table and a Loan Details table. The Header table would hold the unchanging elements and the Detail would hold the changeable elements + FK to the Header, an effective date field and, possibly, a Current Flag</p>
"
"63510962","SQL Server - Aggregate data by minute over multiple days","<h2>Context</h2>
<p>I'm using Microsoft SQL Server 2016.</p>
<p>There is a database table &quot;Raw_data&quot;, that contains the status of a machine, together with it's starting time. There are several machines and each one writes it's status to the database multiple times per minute.</p>
<p>To reduce the data volume I'm trying to aggregate the data into 1-Minute chunks to save it for further analysis. Due to a capacity constraint, I want to execute this transition-logic every few minutes (e.g. scheduled SQL Server Agent Job), delete the raw data and just keep the aggregated data.</p>
<p>To simplify the example, let's assume &quot;Raw_data&quot; looks something like this:</p>
<pre><code>╔════╦════════════╦════════╦═════════════════════╗
║ id ║ fk_machine ║ status ║     created_at      ║
╠════╬════════════╬════════╬═════════════════════╣
║  1 ║       2222 ║      0 ║ 2020-08-19 22:15:00 ║
║  2 ║       2222 ║      3 ║ 2020-08-19 22:15:30 ║
║  3 ║       2222 ║      5 ║ 2020-08-19 23:07:00 ║
║  4 ║       2222 ║      1 ║ 2020-08-20 00:20:00 ║
║  5 ║       2222 ║      0 ║ 2020-08-20 00:45:00 ║
║  6 ║       2222 ║      5 ║ 2020-08-20 02:20:00 ║
╚════╩════════════╩════════╩═════════════════════╝
</code></pre>
<p>Also there are database tables &quot;Dim_date&quot; and &quot;Dim_time&quot;, that look something like that:</p>
<pre><code>╔══════════╦══════════════╗
║ datekey  ║ date_iso8601 ║
╠══════════╬══════════════╣
║ 20200101 ║ 2020-01-01   ║
║ 20200102 ║ 2020-01-02   ║
║ ...      ║ ...          ║
║ 20351231 ║ 2035-12-31   ║
╚══════════╩══════════════╝

╔═════════╦══════════╦═════════════════╗
║ timekey ║ time_iso ║ min_lower_bound ║
╠═════════╬══════════╬═════════════════╣
║ 1       ║ 00:00:01 ║ 00:00:00        ║
║ 2       ║ 00:00:02 ║ 00:00:00        ║
║ ...     ║ ...      ║ ...             ║
║ 80345   ║ 08:03:45 ║ 08:03:00        ║
║ ...     ║ ...      ║ ...             ║
║ 134504  ║ 13:45:04 ║ 13:45:00        ║
║ 134505  ║ 14:45:05 ║ 13:45:00        ║
║ ...     ║ ...      ║ ...             ║
║ 235959  ║ 23:59:59 ║ 23:59:59        ║
╚═════════╩══════════╩═════════════════╝
</code></pre>
<p>The result should look like this:</p>
<pre><code>╔══════════════╦═════════════════╦════════════╦════════╦═══════════════╗
║ date_iso8601 ║ min_lower_bound ║ fk_machine ║ status ║ total_seconds ║
╠══════════════╬═════════════════╬════════════╬════════╬═══════════════╣
║ 2020-08-19   ║ 22:15:00        ║ 2222       ║ 0      ║ 30            ║
║ 2020-08-19   ║ 20:15:00        ║ 2222       ║ 3      ║ 30            ║
║ 2020-08-19   ║ 20:16:00        ║ 2222       ║ 3      ║ 60            ║
║ 2020-08-19   ║ 20:17:00        ║ 2222       ║ 3      ║ 60            ║
║ ...          ║ ...             ║ ...        ║ ...    ║ ...           ║
║ 2020-08-19   ║ 23:06:00        ║ 2222       ║ 3      ║ 60            ║
║ 2020-08-19   ║ 23:07:00        ║ 2222       ║ 5      ║ 60            ║
║ 2020-08-19   ║ 23:08:00        ║ 2222       ║ 5      ║ 60            ║
║ ...          ║ ...             ║ ...        ║ ...    ║ ...           ║
║ 2020-08-20   ║ 00:19:00        ║ 2222       ║ 5      ║ 60            ║
║ 2020-08-20   ║ 00:20:00        ║ 2222       ║ 1      ║ 60            ║
║ 2020-08-20   ║ 00:21:00        ║ 2222       ║ 1      ║ 60            ║
║ ...          ║ ...             ║ ...        ║ ...    ║ ...           ║
║ 2020-08-20   ║ 00:44:00        ║ 2222       ║ 1      ║ 60            ║
║ 2020-08-20   ║ 00:45:00        ║ 2222       ║ 0      ║ 60            ║
╚══════════════╩═════════════════╩════════════╩════════╩═══════════════╝
</code></pre>
<hr />
<h2>Attempt</h2>
<p>To calculate the duration of each status per minute I used an <a href=""https://learn.microsoft.com/en-us/sql/t-sql/queries/with-common-table-expression-transact-sql?view=sql-server-2016"" rel=""nofollow noreferrer"">CTE</a> and <a href=""https://learn.microsoft.com/en-us/sql/t-sql/functions/lead-transact-sql?view=sql-server-2016"" rel=""nofollow noreferrer"">LEAD</a> to fetch the starting date and time from the next status in the database table, then joined with the dimension tables and aggregated the result.</p>
<pre class=""lang-sql prettyprint-override""><code>WITH CTE_MACHINE_STATES(START_DATEKEY, 
                        START_TIMEKEY, 
                        FK_MACHINE, 
                        END_DATEKEY, 
                        END_TIMEKEY)
     AS (SELECT CAST(CONVERT(CHAR(8), CREATED_AT, 112) AS INT), -- ISO: yyyymmdd
                CONVERT(INT, REPLACE(CONVERT(CHAR(8), READING_TIME, 108), ':', '')), 
                FK_MACHINE, 
                STATUS, 
                CAST(CONVERT(CHAR(8), LEAD(CREATED_AT, 1) OVER(PARTITION BY FK_MACHINE
                ORDER BY CREATED_AT), 112) AS INT),
                CONVERT(INT, REPLACE(CONVERT(CHAR(8), LEAD(CREATED_AT, 1) OVER(PARTITION BY FK_MACHINE
                ORDER BY CREATED_AT), 108), ':', ''))
         FROM RAW_DATA)
     SELECT DATE_ISO8601, 
            MIN_LOWER_BOUND, 
            FK_MACHINE, 
            STATUS, 
            SUM(1) AS TOTAL_SECONDS -- Duration
     FROM CTE_MACHINE_STATES
     CROSS JOIN DIM_DATE
     CROSS JOIN DIM_TIME
     WHERE TIMEKEY &gt;= START_TIMEKEY AND 
           TIMEKEY &lt; END_TIMEKEY AND 
           END_TIMEKEY IS NOT NULL AND -- last entry per machine and status
           DATEKEY BETWEEN START_DATEKEY AND END_DATEKEY
     GROUP BY FK_MACHINE, 
              STATUS, 
              DATE_ISO8610, 
              MIN_LOWER_BOUND
     ORDER BY DATE_ISO8610, 
              MIN_LOWER_BOUND;

</code></pre>
<h2>The Problem</h2>
<p>If the status lasts past midnight it won't be aggregated correctly. For example the status at id = 3 in &quot;Raw_data&quot; starts at 23:07 and ends on 00:20 the next day. Here, timekey is greater than end_timekey, so the status get's excluded from the resulting table by the filter <code>TIMEKEY &lt; END_TIMEKEY</code>. I haven't come up with a solution on how to change the join-condition to include such long-lasting states, but get the expected result.</p>
<p>PS: I already wrote, that normally status-updates are happening every several seconds. Thus, the problem only occurs in edge cases, e.g. if a machine get's turned off.</p>
<hr />
<h2>Solution</h2>
<p>Unfortunately I did not receive an answer on how to get the expected result using the date- and time dimension tables. But dnoeth's approach using a recursive CTE is good, so I went with it:</p>
<pre class=""lang-sql prettyprint-override""><code>WITH cte_outer AS (
    SELECT fk_machine,
           status,
           created_at,
           DATEADD(minute, DATEDIFF(minute, '2000', created_at), '2000') AS min_lower_bound, --truncates seconds from start time
           LEAD(created_at) OVER(PARTITION BY fk_machine ORDER BY created_at) AS end_time
    FROM raw_data
),
    cte_recursive AS (
        SELECT fk_machine,
               status,
               min_lower_bound,
               end_time,
               CASE
                 WHEN end_time &gt; DATEADD(minute, 1, min_lower_bound)
                 THEN DATEDIFF(s, created_at, DATEADD(minute, 1, min_lower_bound))
                 ELSE DATEDIFF(s, created_at, end_time)
               END AS total_seconds
        FROM cte_outer

        UNION ALL

        SELECT fk_machine,
               status,
               DATEADD(minute, 1, min_lower_bound), -- next time segment (minute)
               end_time,
               CASE
                 WHEN end_time &gt;= DATEADD(minute, 2, min_lower_bound)
                 THEN 60
                 ELSE DATEDIFF(s, DATEADD(minute, 1, min_lower_bound), end_time)
               END
        FROM cte_recursive
        WHERE end_time &gt; DATEADD(minute, 1, min_lower_bound)
)
SELECT min_lower_bound,
       fk_machine,
       status,
       total_seconds
FROM cte_recursive
ORDER BY  fk_machine, 
          min_lower_bound
</code></pre>
","<sql><sql-server><tsql><time><data-warehouse>","2020-08-20 18:11:15","340","3","2","63512483","<p>For something like this, concatenating the keys to a single datetime isn’t as costly as it might seem. Then you can call DATEDIFF() to check for positive, negative, absolute, values for the comparison. I’ve run something similar translating instantaneous data to minute aggregates across multiple decades, and datediff really makes the difference. However, this would do much better if you simply pull the raw data and perform the calculations in a language with a good datetime library. SQL is always the answer until it isn’t.</p>
<p>What’s likely causing one of the problems here is the following statement:</p>
<pre><code>WHERE TIMEKEY &gt;= START_TIMEKEY AND 
              TIMEKEY &lt; END_TIMEKEY AND 
              END_TIMEKEY IS NOT NULL AND 
              DATEKEY BETWEEN START_DATEKEY AND END_DATEKEY
</code></pre>
<p>If the date and time aren’t separated, you can say:</p>
<pre><code>WHERE DateTimeKey &gt;= START_DateTimeKey AND 
              DateTimeKey &lt; END_DateTimeKey AND 
              END_TIME-KEY IS NOT NULL
</code></pre>
<p>If you are trying to aggregate by a time value, it would be helpful to eliminate any timekey table, that may be another source of problems.  It may be a good idea to replace the timekey table with a recursion and a period duration. You will also need to account for these conditions:</p>
<p>End time of the event must always be after the start time of the aggregate period start time:</p>
<pre><code>DateDiff(second, Period_Start_Time, Event_End) &gt; 0
</code></pre>
<p>Start time of the event must always be before the end of the aggregate period end time:</p>
<pre><code>DateDiff(second, Period_Start_Time, Event_Start) &lt;= @Period_Duration
</code></pre>
<p>There are several ways to distribute the event data across the periods, but datediff helps with linear distribution as well.</p>
"
"63510962","SQL Server - Aggregate data by minute over multiple days","<h2>Context</h2>
<p>I'm using Microsoft SQL Server 2016.</p>
<p>There is a database table &quot;Raw_data&quot;, that contains the status of a machine, together with it's starting time. There are several machines and each one writes it's status to the database multiple times per minute.</p>
<p>To reduce the data volume I'm trying to aggregate the data into 1-Minute chunks to save it for further analysis. Due to a capacity constraint, I want to execute this transition-logic every few minutes (e.g. scheduled SQL Server Agent Job), delete the raw data and just keep the aggregated data.</p>
<p>To simplify the example, let's assume &quot;Raw_data&quot; looks something like this:</p>
<pre><code>╔════╦════════════╦════════╦═════════════════════╗
║ id ║ fk_machine ║ status ║     created_at      ║
╠════╬════════════╬════════╬═════════════════════╣
║  1 ║       2222 ║      0 ║ 2020-08-19 22:15:00 ║
║  2 ║       2222 ║      3 ║ 2020-08-19 22:15:30 ║
║  3 ║       2222 ║      5 ║ 2020-08-19 23:07:00 ║
║  4 ║       2222 ║      1 ║ 2020-08-20 00:20:00 ║
║  5 ║       2222 ║      0 ║ 2020-08-20 00:45:00 ║
║  6 ║       2222 ║      5 ║ 2020-08-20 02:20:00 ║
╚════╩════════════╩════════╩═════════════════════╝
</code></pre>
<p>Also there are database tables &quot;Dim_date&quot; and &quot;Dim_time&quot;, that look something like that:</p>
<pre><code>╔══════════╦══════════════╗
║ datekey  ║ date_iso8601 ║
╠══════════╬══════════════╣
║ 20200101 ║ 2020-01-01   ║
║ 20200102 ║ 2020-01-02   ║
║ ...      ║ ...          ║
║ 20351231 ║ 2035-12-31   ║
╚══════════╩══════════════╝

╔═════════╦══════════╦═════════════════╗
║ timekey ║ time_iso ║ min_lower_bound ║
╠═════════╬══════════╬═════════════════╣
║ 1       ║ 00:00:01 ║ 00:00:00        ║
║ 2       ║ 00:00:02 ║ 00:00:00        ║
║ ...     ║ ...      ║ ...             ║
║ 80345   ║ 08:03:45 ║ 08:03:00        ║
║ ...     ║ ...      ║ ...             ║
║ 134504  ║ 13:45:04 ║ 13:45:00        ║
║ 134505  ║ 14:45:05 ║ 13:45:00        ║
║ ...     ║ ...      ║ ...             ║
║ 235959  ║ 23:59:59 ║ 23:59:59        ║
╚═════════╩══════════╩═════════════════╝
</code></pre>
<p>The result should look like this:</p>
<pre><code>╔══════════════╦═════════════════╦════════════╦════════╦═══════════════╗
║ date_iso8601 ║ min_lower_bound ║ fk_machine ║ status ║ total_seconds ║
╠══════════════╬═════════════════╬════════════╬════════╬═══════════════╣
║ 2020-08-19   ║ 22:15:00        ║ 2222       ║ 0      ║ 30            ║
║ 2020-08-19   ║ 20:15:00        ║ 2222       ║ 3      ║ 30            ║
║ 2020-08-19   ║ 20:16:00        ║ 2222       ║ 3      ║ 60            ║
║ 2020-08-19   ║ 20:17:00        ║ 2222       ║ 3      ║ 60            ║
║ ...          ║ ...             ║ ...        ║ ...    ║ ...           ║
║ 2020-08-19   ║ 23:06:00        ║ 2222       ║ 3      ║ 60            ║
║ 2020-08-19   ║ 23:07:00        ║ 2222       ║ 5      ║ 60            ║
║ 2020-08-19   ║ 23:08:00        ║ 2222       ║ 5      ║ 60            ║
║ ...          ║ ...             ║ ...        ║ ...    ║ ...           ║
║ 2020-08-20   ║ 00:19:00        ║ 2222       ║ 5      ║ 60            ║
║ 2020-08-20   ║ 00:20:00        ║ 2222       ║ 1      ║ 60            ║
║ 2020-08-20   ║ 00:21:00        ║ 2222       ║ 1      ║ 60            ║
║ ...          ║ ...             ║ ...        ║ ...    ║ ...           ║
║ 2020-08-20   ║ 00:44:00        ║ 2222       ║ 1      ║ 60            ║
║ 2020-08-20   ║ 00:45:00        ║ 2222       ║ 0      ║ 60            ║
╚══════════════╩═════════════════╩════════════╩════════╩═══════════════╝
</code></pre>
<hr />
<h2>Attempt</h2>
<p>To calculate the duration of each status per minute I used an <a href=""https://learn.microsoft.com/en-us/sql/t-sql/queries/with-common-table-expression-transact-sql?view=sql-server-2016"" rel=""nofollow noreferrer"">CTE</a> and <a href=""https://learn.microsoft.com/en-us/sql/t-sql/functions/lead-transact-sql?view=sql-server-2016"" rel=""nofollow noreferrer"">LEAD</a> to fetch the starting date and time from the next status in the database table, then joined with the dimension tables and aggregated the result.</p>
<pre class=""lang-sql prettyprint-override""><code>WITH CTE_MACHINE_STATES(START_DATEKEY, 
                        START_TIMEKEY, 
                        FK_MACHINE, 
                        END_DATEKEY, 
                        END_TIMEKEY)
     AS (SELECT CAST(CONVERT(CHAR(8), CREATED_AT, 112) AS INT), -- ISO: yyyymmdd
                CONVERT(INT, REPLACE(CONVERT(CHAR(8), READING_TIME, 108), ':', '')), 
                FK_MACHINE, 
                STATUS, 
                CAST(CONVERT(CHAR(8), LEAD(CREATED_AT, 1) OVER(PARTITION BY FK_MACHINE
                ORDER BY CREATED_AT), 112) AS INT),
                CONVERT(INT, REPLACE(CONVERT(CHAR(8), LEAD(CREATED_AT, 1) OVER(PARTITION BY FK_MACHINE
                ORDER BY CREATED_AT), 108), ':', ''))
         FROM RAW_DATA)
     SELECT DATE_ISO8601, 
            MIN_LOWER_BOUND, 
            FK_MACHINE, 
            STATUS, 
            SUM(1) AS TOTAL_SECONDS -- Duration
     FROM CTE_MACHINE_STATES
     CROSS JOIN DIM_DATE
     CROSS JOIN DIM_TIME
     WHERE TIMEKEY &gt;= START_TIMEKEY AND 
           TIMEKEY &lt; END_TIMEKEY AND 
           END_TIMEKEY IS NOT NULL AND -- last entry per machine and status
           DATEKEY BETWEEN START_DATEKEY AND END_DATEKEY
     GROUP BY FK_MACHINE, 
              STATUS, 
              DATE_ISO8610, 
              MIN_LOWER_BOUND
     ORDER BY DATE_ISO8610, 
              MIN_LOWER_BOUND;

</code></pre>
<h2>The Problem</h2>
<p>If the status lasts past midnight it won't be aggregated correctly. For example the status at id = 3 in &quot;Raw_data&quot; starts at 23:07 and ends on 00:20 the next day. Here, timekey is greater than end_timekey, so the status get's excluded from the resulting table by the filter <code>TIMEKEY &lt; END_TIMEKEY</code>. I haven't come up with a solution on how to change the join-condition to include such long-lasting states, but get the expected result.</p>
<p>PS: I already wrote, that normally status-updates are happening every several seconds. Thus, the problem only occurs in edge cases, e.g. if a machine get's turned off.</p>
<hr />
<h2>Solution</h2>
<p>Unfortunately I did not receive an answer on how to get the expected result using the date- and time dimension tables. But dnoeth's approach using a recursive CTE is good, so I went with it:</p>
<pre class=""lang-sql prettyprint-override""><code>WITH cte_outer AS (
    SELECT fk_machine,
           status,
           created_at,
           DATEADD(minute, DATEDIFF(minute, '2000', created_at), '2000') AS min_lower_bound, --truncates seconds from start time
           LEAD(created_at) OVER(PARTITION BY fk_machine ORDER BY created_at) AS end_time
    FROM raw_data
),
    cte_recursive AS (
        SELECT fk_machine,
               status,
               min_lower_bound,
               end_time,
               CASE
                 WHEN end_time &gt; DATEADD(minute, 1, min_lower_bound)
                 THEN DATEDIFF(s, created_at, DATEADD(minute, 1, min_lower_bound))
                 ELSE DATEDIFF(s, created_at, end_time)
               END AS total_seconds
        FROM cte_outer

        UNION ALL

        SELECT fk_machine,
               status,
               DATEADD(minute, 1, min_lower_bound), -- next time segment (minute)
               end_time,
               CASE
                 WHEN end_time &gt;= DATEADD(minute, 2, min_lower_bound)
                 THEN 60
                 ELSE DATEDIFF(s, DATEADD(minute, 1, min_lower_bound), end_time)
               END
        FROM cte_recursive
        WHERE end_time &gt; DATEADD(minute, 1, min_lower_bound)
)
SELECT min_lower_bound,
       fk_machine,
       status,
       total_seconds
FROM cte_recursive
ORDER BY  fk_machine, 
          min_lower_bound
</code></pre>
","<sql><sql-server><tsql><time><data-warehouse>","2020-08-20 18:11:15","340","3","2","63524785","<p>This is a use-case for a recursive CTE, increasing <code>created_at</code> by one minute per recursion:</p>
<pre><code>with cte as 
 (
   select fk_machine
     ,status  
     ,start_minute
     ,end_time
     ,case
        when end_time &gt; dateadd(minute, 1,start_minute)
        then datediff(s, created_at, dateadd(minute, 1,start_minute)) 
        else datediff(s, created_at, end_time )
      end as seconds
   from
    (
      select fk_machine
        ,status
        ,created_at 
        ,dateadd(minute, datediff(minute, 0, created_at), 0) as start_minute
        ,lead(created_at)
         over (PARTITION BY fk_machine
               order by created_at) as end_time
      from tab
    ) as dt
 
   union all
 
   select fk_machine
     ,status
     ,dateadd(minute, 1,start_minute)
     ,end_time
     ,case
        when end_time &gt;= dateadd(minute, 2,start_minute)
        then 60
        else datediff(s, dateadd(minute, 1,start_minute), end_time)
      end
    from cte
    where end_time &gt; dateadd(minute, 1,start_minute)
 )
select * from cte
order by 1,3,4;
</code></pre>
<p>See <a href=""https://dbfiddle.uk/?rdbms=sqlserver_2019&amp;fiddle=d4a700eb5d4cce6537458a4822b67e64"" rel=""nofollow noreferrer"">fiddle</a></p>
"
"63509066","Design Pattern for Modeling Actuals that replace Estimates","<p>What if any is a good best practice / approach for a use case where a given business activity uses estimates that are then replaced by actual as they become available? In the same way that effective dates can be used to &quot;automatically&quot; (without user's having to know about it) retrieve historically accurate dimension rows, is there a similar way to have actual &quot;automatically&quot; replace the estimates without overwriting the data? I'd rather not have separate fact tables or columns and require that the users have to &quot;know&quot; about this and manually change it to get the latest actuals.</p>
","<data-warehouse>","2020-08-20 16:06:54","20","0","1","63538745","<p>Why not have 2 measures in your fact table, one for estimate and one for actual?
You could then have a View over the fact table with a single measure calculated as &quot;if actual = 0 then estimate else actual&quot;.</p>
<p>Users who just need the current position can use the View; users who need the full picture can access the underlying fact table</p>
"
"63496602","Dimension table, bridge table and fact table join","<p>I have four tables in total:
dimension tables- books and authors
bridge table- book/author
transactions fact</p>
<p>Since there are multiple authors for books and multiple books for an author, to resolve the many-to-many relationship a bridge table is used</p>
<p>Tables Structures are as follows:</p>
<p>Dim_Books:</p>
<pre><code>book_id (PK) 
ISBN 
Title 
</code></pre>
<p>Dim Author:</p>
<pre><code>Author_id (PK)
FName
LName
</code></pre>
<p>Book_auth_bridge:</p>
<pre><code>book_auth_id (PK)
book_id
auth_id
</code></pre>
<p>Fact_Sales:</p>
<pre><code>Sale_amount
quantity
Book_id
book_auth_id
</code></pre>
<p>I am trying to retrieve the total sales of authors from the fact table, since I am joining with the bridge table, I always get multiple rows from the inner join and the amounts are duplicated.</p>
<p>How do I write a SQL to get the total sales for an author?</p>
","<oracle><data-warehouse>","2020-08-20 00:11:31","402","0","1","63521868","<p>The most flexible solution is to include an &quot;allocation factor&quot; column in your bridge table and multiple that with your measure(s) to get the required proportions.</p>
<p>For example, if you wanted to allocate equally between 3 people then you'd have a factor of 0.333 in all 3 bridge table records. However, if you wanted to allocate 50% to a lead author and split the remainder equally with the other authors you would have factors of 0.5, 0.25 and 0.25</p>
"
"63478053","Histogram in Anomaly detection Deequ library","<p>Can we use histogram analyzer in anomaly detection?</p>
<p>Let's say, I want to check for the change in the ratio of variables in a specified column. For example
histogram analysis for a column with Male and Female as values is something like (Male - 0.6) and (Female - 0.4).
Now If the value changes from these to some other values it should be an anomaly in the data.
I have tried it but couldn't figure it out.</p>
<p>Can we achieve something like this currently?</p>
","<scala><data-quality><amazon-deequ>","2020-08-18 23:34:11","466","1","1","63774712","<p><a href=""https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/anomalydetection/AnomalyDetectionStrategy.scala#L30"" rel=""nofollow noreferrer"">AnomalyDetectionStrategy</a> needs a series of Doubles.</p>
<p>As long as you can cast your computation as a 1D anomaly detection problem you can use deequ for this. For example, you could use a Compliance analyzer (instead of histogram) to compute the frequency of Males/Females and feed this into the anomaly detector.</p>
"
"63472296","How to avoid multi joins","<p>I have the following code. In order to get the cost_center, I have to do 2 joins - first on the combination of billing_area_id and facility_id and if no match, I have to join on just billing_area_id to get it. I want to design a dimension for cost_center that should satisfy this scenario such that only one time join should be enough. Please let me know if you have any ideas</p>
<pre><code>select COALESCE(cs1.cost_center,cs2.cost_center) cost_center
from fact_invoice i
inner join dim_facility f on i.hospital_id = f.id 
left join lookup_billingarea_to_wd_costcenter cs1 on i.billing_area_id = cs1.billing_area_id and COALESCE (i.hospital_id,0) = cs1.facility_id
left join lookup_billingarea_to_wd_costcenter cs2 on i.billing_area_id = cs2.billing_area_id and (cs1.cost_center IS NULL and cs2.facility_id=0)
</code></pre>
<p>Added for clarification
I apologize for not providing the data. One billing_area_id can have multiple facility_id and a 0 record facility_id. But the challenge for me, there might be cases, where we dont get matching facility_id and in that case, we just need to take the record with 0 facility_id. For ex, please see below
Lets say we have</p>
<pre><code>Billing_area_id Facility_id cost_center
1   0   abc
1   3   acd
2   0   abd
2   1   ghf
2   2   hgf
</code></pre>
<p>If I have the input of Billing_area_id = 1 and Facility_id = 2 (which the combination do not exist in the lookup table), then, I need to get &quot;abc&quot; as the cost centre</p>
","<sql><data-warehouse><dimension>","2020-08-18 15:55:27","43","0","1","63472897","<p>You can create a view that does the join, then use the view in your multiple places:</p>
<pre><code>CREATE VIEW lookup_billingarea_to_wd_costcenter_with_default AS
SELECT 
  cs1.billing_area_id,
  cs1.facility_id,
  COALESCE(cs1.cost_center,cs2.cost_center) as cost_center
FROM
  lookup_billingarea_to_wd_costcenter cs1
  INNER JOIN lookup_billingarea_to_wd_costcenter cs2
  ON 
    cs1.billing_area_id = cs2.billing_area_id AND
    cs2.billing_area_id = 0
</code></pre>
<p>Note, you didn't post any example data for this so it's a bit of a guess that for every billing_area_id you have N facility_id that is non zero and also a row where facilityID is 0. If you don't have that, and sometimes there is no zero row, you'll need to left join cs2</p>
<p>You'd use it like:</p>
<pre><code>select COALESCE(cs1.cost_center,cs2.cost_center) cost_center
from fact_invoice i
inner join dim_facility f on i.hospital_id = f.id 
inner join lookup_billingarea_to_wd_costcenter_with_default cs 
ON i.billing_area_id = cs.billing_area_id AND COALESCE(i.hospital_id, 0) = cs.facility_id
</code></pre>
<p>If there is ever a case where there isn't a cost center for these things, you'll need to left join the view</p>
"
"63471561","Azure Synapse Analytics (formerly SQL SW) vs Azure Synapse Analytics (workspaces preview)","<p>What are the differences between the following Azure Services?</p>
<ul>
<li>Azure Synapse Analytics (formerly SQL DW)</li>
<li>Azure Synapse Analytics (private link hubs preview)</li>
<li>Azure Synapse Analytics (workspaces preview)</li>
</ul>
<p>Are these three different products? Or are the two preview services just new features that will eventually be added into Azure Synapse Analytics?</p>
<p>The documentation is a little confusing. This FAQ (<a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/overview-what-is"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/synapse-analytics/overview-what-is</a>) for the workspaces preview, for example, just looks like a FAQ for the overall Azure Synapse Analytics service.</p>
","<sql><azure><data-warehouse><azure-synapse>","2020-08-18 15:10:43","1310","3","1","63481716","<p>It would be useful to link to a document mentioning these terms so I could have some context. Without context, this is my understanding of these:</p>
<h1>Azure Synapse Analytics (formerly SQL DW)</h1>
<p>This is just the MPP relational platform piece of &quot;Azure Synapse Analytics&quot;</p>
<p>You can connect to it using Azure Data Studio, SQL Server Management Studio, or Synapse Workspace and run SQL queries on it. It's a relational database that stores data across 60 shared-nothing nodes</p>
<h1>Azure Synapse Analytics (private link hubs preview)</h1>
<p>private link is a new feature across many Azure resources (data lake etc.) that allows you to confine connectivity to internal Azure VNets, meaning that you can use the resource without requiring public access. This feature is not specific to Synapse, it's a network connectivity feature being rolled across multiple azure components</p>
<h1>Azure Synapse Analytics (workspaces preview)</h1>
<p>This is the actual front end that has tabs for various analytics components. One component is the MPP platform that used to be called SQL DW. Another component is MS spark engine. Other components are Power BI and Data Factory.</p>
<p>Do you have a use case or an objective here?</p>
"
"63457880","Kafka internal data management","<p>Can someone help me understand what would happen in the following scenario:</p>
<p>A Stream from Topic A has some various operations performed on it that causes multiple internal kafka topics to be generated such as :
KSTREAM-REDUCE-0000000014
KSTREAM-JOIN-0000000358
etc.</p>
<p>These show up in the topology as &quot;consumer-group-name-generated-name&quot;</p>
<p>Topic A joins Topic B ... B has to be rekey'd to join with A into  an internal topic &quot;group-Re-KeyB&quot;.</p>
<p>If my topology changes, then unless all these internal topics are named the same, I need to change my consumer group name or randomly  generated KSTREAM-REDUCE-0000000014 might contain a different kind of object.</p>
<p>If  I set the offset for the new consumer group to be latest committed from the previous consumer group, we won't be replaying Topic A or B from the beginning.</p>
<p>What  happens to those internal topics?  Would &quot;group-Re-KeyB&quot; for example have all the data to do a join to A or would it only know about new Topic B records ??</p>
","<apache-kafka><kafka-consumer-api><apache-kafka-streams>","2020-08-17 20:04:14","32","0","1","63471189","<p>If you change your topology and name changes, the old and new topology are most likely incompatible and it's recommended to reset your application and let the new topology reprocess all data from the beginning (to rebuild it's needed internal state): <a href=""https://docs.confluent.io/current/streams/developer-guide/app-reset-tool.html"" rel=""nofollow noreferrer"">https://docs.confluent.io/current/streams/developer-guide/app-reset-tool.html</a></p>
<p>As an alternative, you can specify explicit names for all operator (as of Kafka Stream 2.4), for example you can use <code>Materialzed.as(...)</code> to name a state store and the corresponding changelog topic. Explicit naming avoids that names of internal topics change and thus, even if you change the topology, you might be able to restart the new topology without using a new <code>application.id</code> and thus preserve the state from the old topology.</p>
"
"63453336","Dimension modeling in Redshift or flat design - Cost vs time","<p>I've started learning AWS Redshift &amp; I am coming across many things which I believe is not in favor of data warehouse star/snowflake schema.</p>
<p>Based on use responses, all recommended to have Redshift insert only method for the best performance because it is designed for read. But doesn't it increase the cost of storage? I am currently working on MSBI and my fact &amp; dimension have complex structure. Ex: One fact table is shared across various business (data mart), few dimensions are type 2 (where I have to track the history) &amp; few of them are not, few complex scenario need snowflake design.</p>
<p>Considering the cost of storage &amp; computation on cloud, I would like to keep minimilastic data on cloud (the same I do it in my on premise system, which contribute to 4TB storage).</p>
<p>Now, if I do the same approach which I've been doing it on premise then I would have to run my ETL, compare key columns with staging &amp; then perform CRUD, which makes it pointless to move my existing system to cloud.
If I do go with flat table structure then I will end up having 4-6 times more data in my table which will increase the cost of storage on cloud and computation on top of it may cost extra.</p>
<p><a href=""https://stackoverflow.com/questions/34297538/how-to-handle-slowly-changing-dimension-type-2-in-redshift"">How to handle Slowly Changing Dimension Type 2 in Redshift?</a>
<a href=""https://stackoverflow.com/questions/50282011/redshift-performance-of-flat-tables-vs-dimension-and-facts"">Redshift Performance of Flat Tables Vs Dimension and Facts</a></p>
<p>Answers to above question talks about how flat tables can be more relatable to Redshift</p>
<p><a href=""https://aws.amazon.com/blogs/big-data/optimizing-for-star-schemas-and-interleaved-sorting-on-amazon-redshift/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/big-data/optimizing-for-star-schemas-and-interleaved-sorting-on-amazon-redshift/</a></p>
<p>But above Redshift blog talk about how star schema can be optimized.</p>
<blockquote>
<p>Star and snowflake schemas run well on Amazon Redshift, and the
addition of interleaved sort keys further enhances performance by
reducing I/O for a wider range of filter predicates on a table when
needed.</p>
</blockquote>
<p>Now if I choose the approach for insert only (which compliments Redshift architecture) then I would be ending with paying more for storage.
&amp; if I choose to go for traditional data warehouse design then I would be ending up paying extra for computation cost.</p>
<p>Is there any real world example which you can state that can help me understand the approach you have followed in Redshift?</p>
","<amazon-web-services><amazon-redshift><data-warehouse>","2020-08-17 14:51:36","794","0","1","63455471","<p>In my experience Redshift handles flat tables well, and compression eliminates a lot of the storage overheads. For my use cases, the primary concern was keeping ETL as simple as possible, though.</p>
<p>Redshift will almost always suggest ZSTD compression, but for some dimensions you can get better compression with BYTEDICT when you know there will be few distinct values.</p>
<p>With a good sort key and a distribution key that supports your aggregation patterns you can make use of the full power of your cluster when querying a flat table and not be limited by bandwidth. The same goes for a star schema with distributed dimension tables, of course, but there's always that dimension which isn't small enough to be distributable and where the FK isn't a good fit as a distribution key.</p>
<hr />
<p>Before you dive too deep into Redshift, also consider if Athena could be a solution for you. Using S3 for storage is way cheaper than Redshift disks, and performance is comparable for many use cases. There's also the hybrid in Redshift Spectrum where you could offload old partitions to S3 and only keep recent partitions in a smaller cluster.</p>
"