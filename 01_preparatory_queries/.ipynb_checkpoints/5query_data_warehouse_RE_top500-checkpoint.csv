QuestionId,QuestionTitle,QuestionBody,QuestionTags,Date,ViewCount,Score,NumberOfAnswers,AnswerId,AnswerBody
"76049837","Using normalised table schema of RDS directly in Redshift","<p>Is it okay using normalised table schema of RDS directly in Redshift? Some articles says that one should use dimension modelling like star schema, etc in data warehouse?</p>
<p>I have 30-40 tables. My tables data is not in too large to be in billions. At most I think few tables (2-3) would have 1.5 million rows. The rest all relatively small.</p>
<p>Thanks in advance.</p>
","<amazon-web-services><amazon-redshift><data-warehouse>","2023-04-19 00:01:29","23","0","1","76059463","<p>You can use Redshift in any way you want.</p>
<p>It is excellent at <strong>querying</strong>, but rather poor if you want to use it for <strong>OLTP</strong> (eg inserts, deletes, updates).</p>
<p>People typically use a database like Postgres until it hits performance limits, then move to a Data Warehouse like <strong>Amazon Redshift</strong> when they want more performance for querying large quantities of data.</p>
<p>If <strong>Amazon RDS</strong> works for you, then keep using it. If it is causing problems due to data size and performance, then shift to Amazon Redshift. (It's more expensive, but has better performance.)</p>
<p>Another option is to use <strong>Amazon Athena</strong>, which has no 'running costs' -- you just pay per query. If the data is stored in an Amazon S3 bucket in <strong>Snappy-compressed Parquet format</strong> it will run very fast and cost very little. It is similar to &quot;Redshift Spectrum&quot;. Athena can also handle any type of table (wide, star) but doesn't really handle updates/inserts.</p>
"
"76047497","How to flatten array on joined snowflake data","<p>I currently have this query that filters for the latest record by it's <code>updated_at</code> column and looks for rows that have a specific type. I'm attempting to flatten the array so that I can extract that specific object data but I don't think I'm using <code>lateral flatten()</code> correctly.</p>
<p>The row has data that looks like so:</p>
<pre><code>[{ &quot;token&quot;: &quot;1234&quot;, type: &quot;STORE&quot; }, { &quot;token&quot;: &quot;4567&quot;, type: &quot;MERCHANT&quot; }]
</code></pre>
<p>Snowflake Query:</p>
<pre><code>with unique_merchants as (
  select 
    updated_at, 
    aliases, 
    merchant_token, 
    identity_token, 
    row_number() over (
      partition by merchant_token 
      order by 
        updated_at desc
    ) as row_number 
  from 
    table.PUBLIC.business_info
) 
select 
  identity_token, 
  merchant_token,
  aliases
from
  unique_merchants 
  inner join table.PUBLIC.stores on table.PUBLIC.stores.parent_token = merchant_token, 
  lateral flatten(aliases) as obj
where 
  value:type = 'STORE' 
  and row_number = 1;
</code></pre>
<p>Right now this query gives me <em>half</em> of what I need. The output returns all rows that have an object of type <code>STORE</code> in it's <code>ALIAS</code> column. I'd like to be able to also extract the token from objects of type <code>STORE</code> but that's where I'm having trouble. I've tried exploding the row in the CTE that I created and also in the second <code>FROM</code> clause but still no luck.</p>
","<sql><snowflake-cloud-data-platform><data-warehouse>","2023-04-18 17:31:08","28","0","2","76047925","<p>I was able to get what I needed by adding <code>value:token::string</code> in the select outside of the CTE.</p>
"
"76047497","How to flatten array on joined snowflake data","<p>I currently have this query that filters for the latest record by it's <code>updated_at</code> column and looks for rows that have a specific type. I'm attempting to flatten the array so that I can extract that specific object data but I don't think I'm using <code>lateral flatten()</code> correctly.</p>
<p>The row has data that looks like so:</p>
<pre><code>[{ &quot;token&quot;: &quot;1234&quot;, type: &quot;STORE&quot; }, { &quot;token&quot;: &quot;4567&quot;, type: &quot;MERCHANT&quot; }]
</code></pre>
<p>Snowflake Query:</p>
<pre><code>with unique_merchants as (
  select 
    updated_at, 
    aliases, 
    merchant_token, 
    identity_token, 
    row_number() over (
      partition by merchant_token 
      order by 
        updated_at desc
    ) as row_number 
  from 
    table.PUBLIC.business_info
) 
select 
  identity_token, 
  merchant_token,
  aliases
from
  unique_merchants 
  inner join table.PUBLIC.stores on table.PUBLIC.stores.parent_token = merchant_token, 
  lateral flatten(aliases) as obj
where 
  value:type = 'STORE' 
  and row_number = 1;
</code></pre>
<p>Right now this query gives me <em>half</em> of what I need. The output returns all rows that have an object of type <code>STORE</code> in it's <code>ALIAS</code> column. I'd like to be able to also extract the token from objects of type <code>STORE</code> but that's where I'm having trouble. I've tried exploding the row in the CTE that I created and also in the second <code>FROM</code> clause but still no luck.</p>
","<sql><snowflake-cloud-data-platform><data-warehouse>","2023-04-18 17:31:08","28","0","2","76048516","<p>To further simplify the query, it is possible to filter rows based on windowed function inside CTE:</p>
<pre><code>with unique_merchants as (
  select updated_at, 
         aliases, 
         merchant_token, 
         identity_token
  from table.PUBLIC.business_info
  qualify row_number() over (partition by merchant_token order by updated_at desc)=1
)
select 
  identity_token, 
  merchant_token,
  aliases,
  obj.value:token:TEXT
from unique_merchants 
join table.PUBLIC.stores 
  on table.PUBLIC.stores.parent_token = merchant_token 
,lateral flatten(unique_merchants.aliases) as obj
where obj.value:type::TEXT = 'STORE';
</code></pre>
"
"76046106","Replacing a Date Range BETWEEN clause using single Join column","<p>Database: Snowflake</p>
<p>I have a slowly changing Fact (yes, that's correct, fact not Dimension) that uses <code>EFFECTIVE_TS</code> and <code>EXPIRATION_TS</code> for the periodicity.</p>
<p>The Query uses <code>&lt;input_date&gt; BETWEEN EFFECTIVE_TS and EXPIRATION_TS</code> to get a Fact row. This has proved to be slow and before getting into clustering of table, I was wondering if I could have another generated column , call it <code>GENERATED_COL</code>,  generated out of <code>EFFECTIVE_TS</code> and <code>EXPIRATION_TS</code> with some magical logic such that instead of doing <code>&lt;input_date&gt; BETWEEN EFFECTIVE_TS and EXPIRATION_TS</code> I can do <code>Magical conversion(&lt;input_date&gt;) = GENERATED_COL</code>.</p>
<p>In other words, replacing a Date range with a single join column, if you have done this, please share , as we are facing challenges with between clause
(Due to the usecase, volume and slowly changing nature, the Fact has to be a
slowly changing timespan fact instead of a tranasaction or daily snapshot fact)</p>
<p>Thanks
Sunny</p>
","<snowflake-cloud-data-platform><data-modeling><data-warehouse><dimensional-modeling>","2023-04-18 14:55:50","31","0","1","76049955","<p>instead of &quot;replacing the BETWEEN&quot; you should do &quot;both&quot; aka, use the EQUI-JOIN to chunking/block/batch the data, which is faster, and then do the fine grain on the smaller set of results.</p>
<p>here is a <a href=""https://simeonpilgrim.com/blog/2016/08/02/snowflake-data-warehouse-date-range-based-joins-45x-speedup/"" rel=""nofollow noreferrer"">45x speed up article from 2016</a> and a more data driven modern version of it but now offering <a href=""https://select.dev/posts/snowflake-range-join-optimization"" rel=""nofollow noreferrer"">300x speed-up</a>. The speed-up themselves appear to have improved..</p>
"
"76029402","How can I set conditional child task to continue or pause based on status (success/failure/warning) of a parent task in SnowFlake data warehouse?","<p>Currently evaluating Snowflake database for our data warehouse, based on their documentation, Looks like Snowflake do have support for scheduling <a href=""https://docs.snowflake.com/en/user-guide/tasks-intro"" rel=""nofollow noreferrer"">Tasks</a> and create task tree hierarchy.</p>
<p>Snowflake does seem to support conditional child task but I can't figure out setting up a conditional task to continue or pause child task based on parent task success/warning or failure.</p>
<pre><code>## Sample Task tree creation in Snowflake

// Create a child task
CREATE OR REPLACE TASK DATA_IMPORT
WAREHOUSE = COMPUTE_WH
SCHEDULE = ‘60 MINUTE’

AS
INSERT INTO CUSTOMERS2 SELECT * FROM CUSTOMERS;

// Create a child task
CREATE OR REPLACE TASK DATA_IMPORT
WAREHOUSE = COMPUTE_WH
WHEN 1=2  // SAMPLE CONDITION
AFTER DATA_IMPORT
AS
CALL POST_PROCESS_STORED_PROCEDURE( );
</code></pre>
<p>Would appreciate any pointers if you have more knowledge on Snowflake Task structure.
Thank you</p>
","<snowflake-cloud-data-platform>","2023-04-16 17:46:47","37","1","1","76029783","<p><a href=""https://docs.snowflake.com/en/sql-reference/sql/create-task"" rel=""nofollow noreferrer"">CREATE TASK</a>:</p>
<blockquote>
<p>AFTER string [ , string , ... ]</p>
<p>Specifies one or more predecessor tasks for the current task. Use this option to create a DAG of tasks or add this task to an existing DAG. A DAG is a series of tasks that starts with a scheduled root task and is linked together by dependencies.</p>
<p><strong>A task runs after all of its predecessor tasks have finished their own runs successfully (after a brief lag).</strong></p>
<hr />
<p><a href=""https://docs.snowflake.com/en/sql-reference/sql/create-task#examples"" rel=""nofollow noreferrer"">CREATE TASK - Examples </a></p>
<p>...</p>
<p><strong>The child task runs only after all specified predecessor tasks have successfully completed their own runs.</strong></p>
</blockquote>
"
"75916632","Attaching tags to columns in data catalog using python","<p>I have a table in BQ and I am trying to attach tags to columns depending on the prefix. For ex., all the columns which start with ABC_ have to be tagged with the Private Info tag.</p>
<p>I have written below code -</p>
<pre><code>dataset_id = 'my_dataset'
for table in bigquery_client.list_tables(dataset_id):
    # Get the schema of the table
    table_ref = f'{project_id}.{dataset_id}.{table.table_id}'
    table = bigquery_client.get_table(table_ref)
    schema = table.schema
    table_id = table.table_id

    # Loop through the schema fields, and create tags for columns that match the criteria
    for field in schema:
        if field.name.startswith('RUR_'):
            tag = datacatalog.Tag()
            tag.template = f'projects/{project_id}/locations/us-central1/tagTemplates/{tag_template_id}'
            tag.fields['owner'].string_value = 'John Doe'
            tag = datacatalog_client.create_tag(parent=f'projects/{project_id}/locations/us-central1/entryGroups/{entry_group_id}/entries/{table_id}/fields/{field.name}', tag=tag)
            print(f'Tag created for column {field.name} in table {table_id}')
</code></pre>
<p>But I am getting the error Resource Name Invalid, saying that a column cannot be a resource.</p>
<p>Can someone suggest how this can be done in GCP.</p>
<p>Thanks in advance:)</p>
","<python><google-cloud-platform><google-data-catalog>","2023-04-03 06:41:54","44","0","1","75917225","<p>You have to rely on the API description <a href=""https://cloud.google.com/data-catalog/docs/reference/rest/v1/projects.locations.entryGroups.entries.tags/create"" rel=""nofollow noreferrer"">here</a>. As you can see, the parent parameter ends after the <code>entries/{table_id}</code>. You have to put the <code>column</code> in the <a href=""https://cloud.google.com/data-catalog/docs/reference/rest/v1/projects.locations.entryGroups.entries.tags#Tag"" rel=""nofollow noreferrer"">body of the request</a>, i.e. in the TAG object.</p>
<p><a href=""https://i.stack.imgur.com/a1gek.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/a1gek.png"" alt=""enter image description here"" /></a></p>
"
"75916054","Insufficient CPU quota when running data quality task in GCP dataplex","<p>I try to follow the below guide from GCP to create a data quality task. <a href=""https://cloud.google.com/dataplex/docs/check-data-quality?&amp;_ga=2.139058355.-143783534.1669209779#before_you_begin"" rel=""nofollow noreferrer"">https://cloud.google.com/dataplex/docs/check-data-quality?&amp;_ga=2.139058355.-143783534.1669209779#before_you_begin</a> when I try to run the task I get the error</p>
<p><a href=""https://i.stack.imgur.com/OquEI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OquEI.png"" alt=""Error"" /></a></p>
<ul>
<li>As per my understanding Dataplex is calling dataproc internally for running the spark jobs due to which we get the error message. But we dont have the option to adjust the number of CPUs that can be used.</li>
<li>I tried to increase the CPU quota but GCP does not allow me( My account is enabled for billing for a long time)
Any suggestion to correct this error will be appreciated.</li>
</ul>
","<google-cloud-platform><google-cloud-dataproc><google-dataplex>","2023-04-03 04:33:41","76","1","2","75919323","<p>It won't help you, but even if you have activated the billing, as personal user you have a very limited number of allowed CPU (here my CPU quotas for all region; and I'm a GDE! Check it in your project.)
<a href=""https://i.stack.imgur.com/gxtVe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gxtVe.png"" alt=""enter image description here"" /></a></p>
<p>And here the quotas with my corporate sandbox project
<a href=""https://i.stack.imgur.com/rj5O0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rj5O0.png"" alt=""enter image description here"" /></a></p>
<p>If you have the opportunity to test it in a corporate account, or if you have a Google Cloud contact, you could ask for a quota increase.</p>
"
"75916054","Insufficient CPU quota when running data quality task in GCP dataplex","<p>I try to follow the below guide from GCP to create a data quality task. <a href=""https://cloud.google.com/dataplex/docs/check-data-quality?&amp;_ga=2.139058355.-143783534.1669209779#before_you_begin"" rel=""nofollow noreferrer"">https://cloud.google.com/dataplex/docs/check-data-quality?&amp;_ga=2.139058355.-143783534.1669209779#before_you_begin</a> when I try to run the task I get the error</p>
<p><a href=""https://i.stack.imgur.com/OquEI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OquEI.png"" alt=""Error"" /></a></p>
<ul>
<li>As per my understanding Dataplex is calling dataproc internally for running the spark jobs due to which we get the error message. But we dont have the option to adjust the number of CPUs that can be used.</li>
<li>I tried to increase the CPU quota but GCP does not allow me( My account is enabled for billing for a long time)
Any suggestion to correct this error will be appreciated.</li>
</ul>
","<google-cloud-platform><google-cloud-dataproc><google-dataplex>","2023-04-03 04:33:41","76","1","2","75920854","<p>As mentioned by @guillaume, there is set a limit on the number of CPUs you can use for a region (and all regions) in GCP. It is different for individuals and organizations. You can <a href=""https://cloud.google.com/docs/quota#api_specific_quota"" rel=""nofollow noreferrer"">check</a> this under IAM -&gt; Quotas-&gt; Compute Engine API CPU quota.</p>
<p>Solutions to your error:</p>
<ul>
<li><p>You can request a <a href=""https://cloud.google.com/docs/quota#requesting_higher_quota"" rel=""nofollow noreferrer"">quota
increase</a>
on the number of CPUs for a region/all regions for your project.</p>
</li>
<li><p>Try changing the configuration of your Dataproc Cluster by decreasing
the number of secondary workers. <a href=""https://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms#using_secondary_workers"" rel=""nofollow noreferrer"">Reference</a></p>
</li>
</ul>
"
"75873537","Comparison between using DBT + (Athena vs Redshift or Snowflake) as a data warehouse - which path should I take?","<p>I'm currently using DBT and Athena as a data warehouse, and it's able to do transformations and write data back to S3. We don't do any inserts/updates/deletes.</p>
<p>I should say that the amount of data I currently have is quite small, but I imagine Athena can scale well considering it was developed for Facebook.</p>
<p>My question then, is what are the reasons for using Redshift or Snowflake considering how much more expensive they are? What am I missing?</p>
<p>If I had more data, in terabytes perhaps, would this affect the choice of warehouse?</p>
<p>I have not tried Snowflake or Redshift, so perhaps I am missing some context. Happy to learn from you folks!</p>
","<amazon-web-services><snowflake-cloud-data-platform><amazon-redshift><amazon-athena><data-warehouse>","2023-03-29 05:45:33","84","0","1","75873814","<p>Reporting systems need a means of <strong>running SQL against data</strong>.</p>
<p>Traditionally, this meant that a Database was required, and all databases (at the time) consisted of both <strong>Storage and Compute</strong>. There was no capability to separate these two components because the database stored its data in a proprietary format and the Compute component was required access that data.</p>
<p>As data volumes increased, traditional databases struggled to provide fast performance. This led to a new class of <strong>Data Warehouse systems</strong> that specialise in querying tables with billions of rows and Terabytes of data. These systems typically use parallel infrastructure and columnar storage split across multiple storage nodes to provide fast performance. Examples are: Amazon Redshift, Snowflake.</p>
<p>The next evolution came from <strong>Presto</strong> (and can be traced back to Hadoop), which was the idea of completely separating the Compute and Storage components of databases. <strong>Optimized for querying</strong>, Presto could query data stored in cloud services (eg Amazon S3) without having to load the data into the database (known as a 'query engine'). This was not only a mind-blowing concept, but depending upon the data format (eg Snappy-compressed Parquet) could actually rival the speed of Data Warehouses. Plus, the fact that they are cloud-native, it was easy to scale Compute as needed for short periods of time.</p>
<p>The main thing to understand about Query Engines is that data is not 'loaded' into them. Rather, when a query runs, they go to the storage service, <strong>look at data stored in whatever format</strong> and then calculate the answer to the query. Data can be added by simply adding another file in the storage location.</p>
<p>Examples of Query Engines are: Presto, Amazon Athena, Amazon Redshift Spectrum</p>
<p>The downside of using Query Engines is that they are not good at inserting/updating data. This has been addressed by the <a href=""https://delta.io/"" rel=""nofollow noreferrer"">Delta Lake file format</a> that uses a combination of Parquet files and logs files to allow data to be inserted, updated and deleted. This is the main focus of Databricks.</p>
<p>Of course, if your data needs are small, it is quite acceptable to use a traditional database (eg PostgreSQL) as a Data Warehouse.</p>
<p>The best approach is to <strong>start with something small</strong> until it no longer meets your needs. Then, move to something more powerful. If Amazon Athena is meeting your needs, then there is no need to move to anything else.</p>
<p>(My apologies for not including Google and other services as examples. My knowledge is mostly limited to AWS services.)</p>
"
"75873443","Does a data warehouse have one data model or many?","<p>How do you design a data warehouse? Assuming you have raw data in S3, you copy all the data into a data warehouse, and then start modelling the data (So ELT right?). Do you have a single data model that shows all the relationships, or do you create a data model per pipeline?</p>
<p>Eg. If I were to model all data related to customer purchases for an e-commerce store, would that be one data model, and then separately another data model for all the suppliers and related information?</p>
<p>I've been scouring the internet and have not found a clear answer. I'd really appreciate the help!</p>
<p>So far I have tried only a single data model - but I'm not sure how that lines up with ELT allowing for data being modelled as needed?</p>
","<data-modeling><data-warehouse>","2023-03-29 05:29:10","16","0","1","75873511","<p>Designing a data warehouse generally involves the following steps:</p>
<ol>
<li>Identify business requirements:</li>
</ol>
<p>identifying the data that is needed to support the business processes, the data sources, and the data quality requirements</p>
<ol start=""2"">
<li>Plan the architecture:</li>
</ol>
<p>decide on the architecture of the data warehouse(star or snowflake schema) + the hardware/software required + data integration strategy</p>
<ol start=""3"">
<li>Data modeling:</li>
</ol>
<p>defining the structure of the data warehouse(dimensions, facts + relationships between them)</p>
<ol start=""4"">
<li>Data integration:</li>
</ol>
<p>extracting data from the various data sources and loading it into the data warehouse(using ETL or ELT tool)</p>
<ol start=""5"">
<li>Testing and validation:</li>
</ol>
<p>ensure that the data is accurate, consistent, and reliable.</p>
<ol start=""6"">
<li>Deployment and maintenance:</li>
</ol>
<p>deploy and maintain(monitoring the performance of the data warehouse + making any necessary changes to the data model or data integration processes)</p>
<p>Regarding the part about data modeling:</p>
<ul>
<li><p>it is generally recommended to create a separate data model for each subject area or pipeline(allows for better organization of the data and easier maintenance of the data model)( exp: you could create a data model for customer purchases and another data model for supplier information)</p>
</li>
<li><p>it is important to ensure that the data models are consistent and can be integrated as needed</p>
</li>
<li><p>ELT allows for greater flexibility in data modeling, as the data can be transformed and modeled as needed in the data warehouse -- it is still important to have a clear understanding of the business requirements and a well-designed data model to ensure that the data warehouse meets the needs of the organization.</p>
</li>
</ul>
"
"75870971","value cannot be converted to a timestamp (Error in Nifi)","<p>I have a mysql destination table with column type datetime.</p>
<p>my nifi flow is like convertJsontoSQL -&gt; putsql (into this destination table). The action is delete for putsql.</p>
<p>After conversion of Jsontosql i am getting a value '2023-03-28 15:54:28' which is a valid timestamp.</p>
<p>but the flow is failing with below error:</p>
<pre><code>duetoThevalue of the sqlargs. 14.value is '2023-03-28 15:54:28',

which cannot be converted to a timestamp; routing to failure: java.sql.SQLDataException:

The value of the sql.args. 14.value is '2023-03-28 15:54:28',

which cannot be converted to a timestamp

- Caused by: java. text. ParseException: Unparseable date: &quot;2023-03-28 15:54:28&quot;
</code></pre>
<p>I can able to use the same value &quot;2023-03-28 15:54:28&quot; for manually inserting into a sample datetime column. and can also able to delete a record with where clause as this value.</p>
<p>I could not understand why Nifi is still not able to convert it to timestamp.</p>
<p>Any suggestions are appreciated.</p>
","<mysql><etl><apache-nifi><data-warehouse>","2023-03-28 20:52:20","45","0","1","75871188","<p><a href=""https://nifi.apache.org/docs/nifi-docs/components/org.apache.nifi/nifi-standard-nar/1.9.0/org.apache.nifi.processors.standard.PutSQL/"" rel=""nofollow noreferrer"">https://nifi.apache.org/docs/nifi-docs/components/org.apache.nifi/nifi-standard-nar/1.9.0/org.apache.nifi.processors.standard.PutSQL/</a></p>
<p>Section <strong>Reads Attributes / sql.args.N.format</strong></p>
<blockquote>
<p>Date, Time and Timestamp formats all support both custom formats or named format ('yyyy-MM-dd','ISO_OFFSET_DATE_TIME') as specified according to java.time.format.DateTimeFormatter. If not specified, a long value input is expected to be an unix epoch (milli seconds from 1970/1/1), or a string value in <strong>'yyyy-MM-dd'</strong> format for Date, <strong>'HH:mm:ss.SSS'</strong> for Time</p>
</blockquote>
<p>so, you need to set attribute <code>sql.args.14.format = yyyy-MM-dd HH:mm:ss</code></p>
<p>as alternative option - try to set <code>sql.args.14.type = 1</code> (CHAR) - in this case  conversion will be done on database level.</p>
"
"75789185","Dialogflow Interaction Logging in Bigquery data modelling","<p>I am exporting dialogflow interaction logging to bigquery in a table as created as per the structure in the link below</p>
<p><a href=""https://cloud.google.com/dialogflow/cx/docs/concept/export-bq"" rel=""nofollow noreferrer"">https://cloud.google.com/dialogflow/cx/docs/concept/export-bq</a></p>
<pre><code>CREATE TABLE &lt;your_dataset_name&gt;.dialogflow_bigquery_export_data(
  project_id STRING,
  agent_id STRING,
  conversation_name STRING,
  turn_position INTEGER,
  request_time TIMESTAMP,
  language_code STRING,
  request JSON,
  response JSON,
  partial_responses JSON,
  derived_data JSON,
  conversation_signals JSON
);
</code></pre>
<p>There are some fields like request,response, derived_data etc are in JSON format.</p>
<p>Wondering what's the best way of analyzing and querying this table? Do I need to flatten the table into another table or multiple tables for each JSON column?</p>
<p>How to achieve data modelling for the given table?</p>
<p>Looking for flattening the data if that's the right approach for analysing table.</p>
","<json><google-bigquery><dialogflow-cx>","2023-03-20 10:31:46","87","1","1","75813308","<p>JSON datatype was introduced as it is more performant and cost effective. When a column of datatype JSON is created, the data is stored in parsed form and hence you have benefits of a columnar database.
You don't have to flatten the table to query the json data. You can easily extract the values/fields of the JSON using dot operator.</p>
<p>For example:</p>
<pre><code>CREATE OR REPLACE TABLE mydataset.table1(id INT64, shopping_cart JSON);

INSERT INTO mydataset.table1 VALUES
(1, JSON &quot;&quot;&quot;{
        &quot;name&quot;: &quot;Alice&quot;,
        &quot;items&quot;: [
            {&quot;product&quot;: &quot;flowers&quot;, &quot;price&quot;: 10},
            {&quot;product&quot;: &quot;book&quot;, &quot;price&quot;: 50}
        ]
    }&quot;&quot;&quot;);
</code></pre>
<p>The following example returns the name field of the shopping_cart column.</p>
<pre><code>SELECT shopping_cart.name
FROM mydataset.table1;
</code></pre>
"
"75763984","Apache Doris：Short files in the library directory","<p>I am using build.sh to build Doris but failed. It says:
lib/doris_be': No such file or directory</p>
","<database><bigdata><open-source><data-warehouse><olap>","2023-03-17 05:17:17","17","0","1","75919324","<p>The <a href=""https://github.com/apache/doris/blob/master/build.sh"" rel=""nofollow noreferrer"">build.sh</a> script seems to search for the be library from Apache doris in the
<code>${ROOT}/output/be/lib/doris_be</code>
directory, where ${ROOT} is taken from the <a href=""https://stackoverflow.com/a/35006505/151759"">BASH_SOURCE variable</a>. My assumption is that you started the script not from the root directory of the doris source code, and therefore the build script looks in the wrong directory for the be library. Can you check?</p>
"
"75746315","Snowflake insert overwrite behaviour","<p>If I execute <strong>INSERT OVERWRITE TABLEA</strong> statement and it takes 5 minutes to run. During its execution if I run <strong>SELECT * FROM TABLEA</strong> is there any possibility that I will not see any data? because of <strong>INSERT OVERWRITE</strong> first truncate the data and then load it.</p>
<p>I experienced that the data was not there in TABLEA but after the pipeline run it i saw the data</p>
","<sql><snowflake-cloud-data-platform><data-warehouse>","2023-03-15 14:45:18","66","2","1","75746702","<p>The default isolation level in Snowflake is Read Committed; which means you can't read uncommitted data. The Transaction you are running is INSERT OVERWRITE which works similarly to Truncate and load; the data is seen only when the transaction is complete.
The command to view the default isolation level in Snowflake is</p>
<pre><code>SHOW PARAMETERS LIKE 'TRANSACTION_DEFAULT_ISOLATION_LEVEL' IN ACCOUNT;
</code></pre>
"
"75736474","Apache Doris: Tablet writer failed to write","<p>tablet_id=27306172, txn_id=28573520, err=-235</p>
<p>This error occurred when I was doing the data import and I believe there is something wrong with my version compaction.</p>
","<database><open-source><data-warehouse><olap><data-ingestion>","2023-03-14 17:25:30","30","-1","1","75742285","<p>This is because the import speed is too fast and the compaction cannot keep up. The solution is as follows:</p>
<ol>
<li>Reduce the import speed, such as increasing the checkpoint of flink.</li>
<li>Optimize compaction parameters, such as increase the parameter of be: max_tablet_version_num</li>
</ol>
<p>If this doesn't solve your problem, welcome to the doris slack community, we will try to help you</p>
"
"75695419","Using great expectations with databricks autolaoder","<p>I have implemented a data pipeline using autoloader bronze --&gt; silver --&gt; gold.</p>
<p>now while I do this I want to perform some data quality checks, and for that I'm using great expectations library.</p>
<p>However I'm stuck with below error when trying to validate the data</p>
<pre><code>validator.expect_column_values_to_not_be_null(column=&quot;col1&quot;)​
validator.expect_column_values_to_be_in_set(
   column=&quot;col2&quot;,
   value_set=[1,6]
)
</code></pre>
<blockquote>
<p>MetricResolutionError: Queries with streaming sources must be executed with writeStream.start();</p>
</blockquote>
<p>looks like great expectations can work with only static/batch data.</p>
<p>How can I get it working for streaming data?</p>
<p>I followed below in my databricks notebook to get started with great_expectations</p>
<p><a href=""https://docs.greatexpectations.io/docs/deployment_patterns/how_to_use_great_expectations_in_databricks/"" rel=""nofollow noreferrer"">https://docs.greatexpectations.io/docs/deployment_patterns/how_to_use_great_expectations_in_databricks/</a></p>
<pre><code>from pyspark.sql.functions import col, to_date, date_format
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType
import time
 
# autoloader table and checkpoint paths
basepath = &quot;/mnt/autoloaderdemodl/datagenerator/&quot;
bronzeTable = basepath + &quot;bronze/&quot;
bronzeCheckpoint = basepath + &quot;checkpoint/bronze/&quot;
bronzeSchema = basepath + &quot;schema/bronze/&quot;
silverTable = basepath + &quot;silver/&quot;
silverCheckpoint = basepath + &quot;checkpoint/silver/&quot;
landingZoneLocation = &quot;/mnt/autoloaderdemodl/datageneratorraw/customerdata_csv&quot;
 
# Load data from the CSV file using Auto Loader to bronze table using rescue as schema evolution option
raw_df = spark.readStream.format(&quot;cloudFiles&quot;) \
            .option(&quot;cloudFiles.format&quot;, &quot;csv&quot;) \
            .option(&quot;cloudFiles.schemaEvolutionMode&quot;, &quot;rescue&quot;) \
            .option(&quot;Header&quot;, True) \
            .option(&quot;cloudFiles.schemaLocation&quot;, bronzeSchema) \
            .option(&quot;cloudFiles.inferSchema&quot;, &quot;true&quot;) \
            .option(&quot;cloudFiles.inferColumnTypes&quot;, True) \
        .load(landingZoneLocation)
 
        # Write raw data to the bronze layer
bronze_df = raw_df.writeStream.format(&quot;delta&quot;) \
            .trigger(once=True) \
            .queryName(&quot;bronzeLoader&quot;) \
            .option(&quot;checkpointLocation&quot;, bronzeCheckpoint) \
            .option(&quot;mergeSchema&quot;, &quot;true&quot;) \
            .outputMode(&quot;append&quot;) \
            .start(bronzeTable)
# Wait for the bronze stream to finish
bronze_df.awaitTermination()
bronze = spark.read.format(&quot;delta&quot;).load(bronzeTable)
bronze_count = bronze.count()
display(bronze)
print(&quot;Number of rows in bronze table: {}&quot;.format(bronze_count))
 
 
bronze_df = spark.readStream.format(&quot;delta&quot;).load(bronzeTable)
 
# Apply date format transformations to the DataFrame
# Transform the date columns
silver_df = bronze_df.withColumn(&quot;date1&quot;, to_date(col(&quot;date1&quot;), &quot;yyyyDDD&quot;))\
                     .withColumn(&quot;date2&quot;, to_date(col(&quot;date2&quot;), &quot;yyyyDDD&quot;))\
                     .withColumn(&quot;date3&quot;, to_date(col(&quot;date3&quot;), &quot;MMddyy&quot;))
 
# Write the transformed DataFrame to the Silver layer
silver_stream  = silver_df.writeStream \
    .format(&quot;delta&quot;) \
    .outputMode(&quot;append&quot;) \
    .option(&quot;mergeSchema&quot;, &quot;true&quot;) \
    .option(&quot;checkpointLocation&quot;, silverCheckpoint) \
    .trigger(once=True) \
    .start(silverTable)
 

# Wait for the write stream to complete
silver_stream.awaitTermination()
# Count the number of rows in the Silver table
silver = spark.read.format(&quot;delta&quot;).load(silverTable)
display(silver)
silver_count = silver.count()
print(&quot;Number of rows in silver table: {}&quot;.format(silver_count))
</code></pre>
<p>PS - customer doesn't want to use DLT yet.</p>
<p>code to include expectation validation</p>
<pre><code>import great_expectations as ge
from great_expectations.datasource.types import BatchKwargs

bronze_df = spark.readStream.format(&quot;delta&quot;).load(bronzeTable)

# Apply date format transformations to the DataFrame
# Transform the date columns
silver_df = bronze_df.withColumn(&quot;date1&quot;, to_date(col(&quot;date1&quot;), &quot;yyyyDDD&quot;))\
                     .withColumn(&quot;date2&quot;, to_date(col(&quot;date2&quot;), &quot;yyyyDDD&quot;))\
                     .withColumn(&quot;date3&quot;, to_date(col(&quot;date3&quot;), &quot;MMddyy&quot;))
def validate_micro_batch(batch_df, epoch):
    print(&quot;inside function&quot;)
    # Use Great Expectations to validate the batch DataFrame
    clean_df = batch_df
    clean_df.expect_column_values_to_not_be_null(column=&quot;col1&quot;)
    clean_df.expect_column_values_to_be_between(
        column=&quot;col2&quot;, min_value=0, max_value=1000
    )
    clean_df.write.format(&quot;delta&quot;).option(&quot;mergeSchema&quot;, &quot;true&quot;).mode(&quot;append&quot;).saveAsTable(silverTable)
    # Print the validation results for the batch
    validation_results = clean_df.validate()
    print(&quot;Validation results for batch {}:&quot;.format(batch_id))
    print(validation_results)
    
# Write the transformed DataFrame to the Silver layer if it passes all expectations
silver_stream = silver_df.writeStream \
    .format(&quot;delta&quot;) \
    .outputMode(&quot;append&quot;) \
    .foreachBatch(validate_micro_batch) \
    .option(&quot;checkpointLocation&quot;, silverCheckpoint) \
    .trigger(once=True) \
    .start()
# Wait for the write stream to complete
silver_stream.awaitTermination()
# Count the number of rows in the Silver table
silver = spark.read.format(&quot;delta&quot;).load(silverTable)
display(silver)
silver_count = silver.count()
print(&quot;Number of rows in silver table: {}&quot;.format(silver_count))
</code></pre>
","<azure-databricks><spark-structured-streaming><great-expectations><data-quality><databricks-autoloader>","2023-03-10 11:12:13","136","1","1","75713053","<p>Great Expectations is designed to work with batches of the data, so if you want to use it with Spark structured streaming then you will need to implement your checks inside a function that will be passed to <code>foreachBatch</code> argument of <code>writeStream</code> (<a href=""https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#foreachbatch"" rel=""nofollow noreferrer"">doc</a>).</p>
<p>It will look something like this:</p>
<pre class=""lang-py prettyprint-override""><code>def foreach_batch_func(df, epoch):
  # apply GE expectations to df and get clean dataframe
  clean_df = df....
  clean_df.write.format(&quot;delta&quot;).option(&quot;mergeSchema&quot;, &quot;true&quot;) \
    .mode(&quot;append&quot;).saveAsTable(silverTable)

silver_stream  = silver_df.writeStream \
    .format(&quot;delta&quot;) \
    .outputMode(&quot;append&quot;) \
    .foreachBatch(foreach_batch_func) \
    .option(&quot;checkpointLocation&quot;, silverCheckpoint) \
    .trigger(once=True) \
    .start()
</code></pre>
<p>But really, for this kind of the checks, Great Expectations would be overkill. And really, you need to discuss about adoption of Delta Live Tables for this.</p>
<p>P.S. You may need to <a href=""https://docs.databricks.com/structured-streaming/delta-lake.html#idempotent-table-writes-in-foreachbatch"" rel=""nofollow noreferrer"">add options for idempotent writes to Delta</a>.</p>
"
"75665949","How to select all rows that have the same value in each column using tidyverse?","<p>I'm working on data quality analysis for a questionnaire where respondents were asked to check mark every bit of food that they ate. Some respondents left the form blank so I'm trying to figure out a way to select or count all rows where each column is blank.</p>
<p>In this particular dataset, the value of &quot;No&quot; indicates that the box was not checked on the form. A value of &quot;Yes&quot; means that the box was checked. They are currently character variables in the dataset. It's easy to count the number of yes and no responses in a particular column, but I'm interested in counting rows where every single response is &quot;NO.&quot;</p>
<p>To make things simple, let's say that there are 5 columns representing different foods on the chart: Apple, Banana, Sandwich, Yogurt, Strawberry (the actual dataset has over 40 different food items). How would I select every row where &quot;No&quot; is the response for each food item?</p>
<p>This is what I've tried so far and it isn't working:</p>
<pre><code>food_history &lt;- food %&gt;%
     filter(Apple:Strawberry==&quot;No&quot;)
</code></pre>
","<r><filter><tidyverse><data-quality>","2023-03-07 18:36:28","42","1","1","75665986","<p>We can use <code>if_all</code></p>
<pre><code>library(dplyr)
food %&gt;%
   filter(if_all(Apple:Strawberry, ~ .x == &quot;No&quot;))
</code></pre>
"
"75657673","inserting 8k + character strings into azure sql data warehouse using pyodbc","<p>As the title states, im trying to insert an 8k+ string into azure sql data warehouse using pyodbc in a python notebook. I've seen a few answers related to changing the driver, and ive updated to latest driver which supposedly supports this, still no luck.</p>
<p>Heres the code i'm using</p>
<pre><code>import pyodbc
connectionString = &quot;Driver={ODBC Driver 18 for SQL Server};Server=myServerInfo;Database=myDB;Uid=user;Pwd={pw};Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;LongAsMax=1;&quot;
cnxn = pyodbc.connect(connectionString)

cnxn.setdecoding(pyodbc.SQL_WCHAR, encoding='utf-8')
cnxn.setencoding(encoding='utf-8')

cursor = cnxn.cursor()

sql = &quot;insert into myTable (id, veryLongText) values (?, ?)&quot; 
params = (1, 'a' * 8001) 

cursor.execute(sql, params) 

</code></pre>
<p>this gives the error</p>
<pre><code>ProgrammingError: ('42000', &quot;[42000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]104220;Cannot find data type 'text'. (100000) (SQLExecDirectW)&quot;)
</code></pre>
<p>if i truncate the string to 4k characters, i dont have any issues, but unfort i need all the data as is for the project.</p>
<p>I've tried casting the long string to varchar(max) to see if i can force the driver to use varchar(max) instead of text</p>
<pre><code>sql = &quot;insert into myTable (id, veryLongText) values (?, cast(? as varchar(max))&quot; 
</code></pre>
<p>which gives the error:</p>
<pre><code>ProgrammingError: ('42000', '[42000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Insert values statement can contain only constant literal values or variable references. (104334) (SQLExecDirectW)')
</code></pre>
<p>I've also tried using <code>pyodbc.Binary</code> to cast the string into a varchar(max) datatype (chatgpts reommendation lol) but i get this error:</p>
<pre><code>ProgrammingError: ('42000', '[42000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Unsupported data type error. Statement references a data type that is unsupported in Parallel Data Warehouse, or there is an expression that yields an unsupported data type. Modify the statement and re-execute it. (104051) (SQLExecDirectW)')
</code></pre>
<p>can anyone shed some light on how I can fix this?</p>
<p>Updates:</p>
<p>using SiddheshDesai's answer, I've tried inserting using a stored procedure instead, but I'm getting the same error</p>
<pre><code>CREATE PROCEDURE InsertLongText
    @id int,
    @veryLongText varchar(max)
AS
BEGIN
    INSERT INTO myTable (id, veryLongText) VALUES (@id, @veryLongText)
END
</code></pre>
<pre><code>sql = &quot;EXEC InsertLongText ?, ?&quot;

params = (1, 'a' * 8001)

  

cursor.execute(sql, params)
</code></pre>
<p>error:</p>
<pre><code>ProgrammingError: ('42000', &quot;[42000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]104220;Cannot find data type 'text'. (100000) (SQLExecDirectW)&quot;)
</code></pre>
<p>It seems the issue is with the driver and SQL Pools (formally known as SQL data warehouse). Specifically, I am using Azure Synapse Analytics</p>
","<python><sql><azure><pyodbc><azure-synapse>","2023-03-07 03:01:52","113","0","1","75659349","<p><em><strong>I created a myTable in Azure SQL  and Inserted the string successfully, Refer below:-</strong></em></p>
<p>Created <strong>myTable</strong> with stored procedure in the SQL DB :-</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE myTable (
    id int,
    veryLongText varchar(max)
)

GO

CREATE PROCEDURE InsertLongText
    @id int,
    @veryLongText varchar(max)
AS
BEGIN
    INSERT INTO myTable (id, veryLongText) VALUES (@id, @veryLongText)
END

Select * from myTable
GO
</code></pre>
<p><em><strong>Installed Pyodbc MS SQL Driver 17 from the link below:-</strong></em></p>
<p><a href=""https://learn.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server?view=sql-server-ver16#version-17"" rel=""nofollow noreferrer"">Download ODBC Driver for SQL Server - ODBC Driver for SQL Server | Microsoft Learn</a></p>
<p>Used the below Python code to Insert the string :-</p>
<pre class=""lang-py prettyprint-override""><code>import  pyodbc

  

connectionString = &quot;Driver={ODBC Driver 17 for SQL Server};Server=azure-sql-server.database.windows.net;Database=azuresqldb;Uid=siliconuser;Pwd=&lt;password&gt;;Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;LongAsMax=1;&quot;

cnxn = pyodbc.connect(connectionString)

  

cnxn.setdecoding(pyodbc.SQL_WCHAR, encoding='utf-8')

cnxn.setencoding(encoding='utf-8')

  

cursor = cnxn.cursor()

  

sql = &quot;EXEC InsertLongText ?, ?&quot;

params = (1, 'a' * 8001)

  

cursor.execute(sql, params)

cursor.execute('Select * From myTable' )

for  i  in  cursor:

print(i)

cnxn.commit()

cursor.close()

cnxn.close()
</code></pre>
<p>Output:-</p>
<p><img src=""https://i.imgur.com/aJ5Uuni.png"" alt=""enter image description here"" /></p>
"
"75657343","Check the data quality in Google Sheets (asking for suggestions)","<p>I'm trying to create a sheet to check the data quality from a survey in Google Sheets the document have this format:</p>
<p><a href=""https://i.stack.imgur.com/iajHg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iajHg.png"" alt=""enter image description here"" /></a></p>
<p>So basically I was using this formula <code>=COUNTIF(B2:F2,&quot;Don't know&quot;)</code> to count <code>Don't know</code>, empty spaces, 0's and numbers &gt; than 9, if the percentage from the counts is bigger than 0.31 or 31% the data quality is not good, as an example I'll take <code>row 2</code>, it has responses from Column B-F (5 cells) and this is the total number of responses, the count of bad data is 2, so in percentage will be represented the quality as <code>2/5 = 40%</code>, but I want to do the same for the all survey questions in a survey with 1600 questions, <a href=""https://docs.google.com/spreadsheets/d/1SylcKsNjuA9WJ6aCiD1YuORWHfJidlAl3umthylGwuE/edit?usp=sharing"" rel=""nofollow noreferrer"">this document</a>,the document attached contains a small piece of data from the original survey. So I would like to ask for a better solution that counting all the columns and the rows, I'm asking for a recomendation or how I should check the data quality, basically in the document I have all the formulas that I need but I would like to have all the formulas in just one. Also in the document in the desirable output column there is the final result that I would like to have. Hoping my explanaiton was good about the desired output.</p>
","<google-sheets><google-sheets-formula><data-quality>","2023-03-07 01:44:44","124","0","2","75680753","<p>First of all clear everything in ranges <code>G2:J21</code> and <code>L2:M21</code></p>
<p>You can now try the following 2 formulas in cells <code>G2</code> and <code>L2</code> respectively.<br />
<em>As for column <code>K2</code>, it becomes obsolete since it is a fixed number (number 5) that is easily incorporated within the 2nd formula</em></p>
<p>in cell <code>G2</code> try</p>
<pre><code>=INDEX(IF(A2:A21=&quot;&quot;,,
          {BYROW(B2:F21,LAMBDA(zz,{COUNTIF(zz,&quot;Don't know&quot;),COUNTBLANK(zz)})),
           BYROW(F2:F21,LAMBDA(xx,{COUNTIF(xx, 0),COUNTIF(xx, &quot;&gt;9&quot;)}))}))
</code></pre>
<p>in cell <code>L2</code> try</p>
<pre><code>={INDEX(IF(A2:A21=&quot;&quot;,,
            LET(vv,BYROW(G2:J21,LAMBDA(ww,SUM(ww)/5)),
               {vv,INDEX(IF((vv&lt;= 0.3),&quot;Good Quality&quot;,IF(vv&gt;0.31,&quot;Bad Quality&quot;,)))})))}
</code></pre>
<p><a href=""https://i.stack.imgur.com/lHyyX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lHyyX.png"" alt=""enter image description here"" /></a></p>
<p><em>(<strong>Do</strong> adjust the formulae according to your ranges and locale)</em></p>
"
"75657343","Check the data quality in Google Sheets (asking for suggestions)","<p>I'm trying to create a sheet to check the data quality from a survey in Google Sheets the document have this format:</p>
<p><a href=""https://i.stack.imgur.com/iajHg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iajHg.png"" alt=""enter image description here"" /></a></p>
<p>So basically I was using this formula <code>=COUNTIF(B2:F2,&quot;Don't know&quot;)</code> to count <code>Don't know</code>, empty spaces, 0's and numbers &gt; than 9, if the percentage from the counts is bigger than 0.31 or 31% the data quality is not good, as an example I'll take <code>row 2</code>, it has responses from Column B-F (5 cells) and this is the total number of responses, the count of bad data is 2, so in percentage will be represented the quality as <code>2/5 = 40%</code>, but I want to do the same for the all survey questions in a survey with 1600 questions, <a href=""https://docs.google.com/spreadsheets/d/1SylcKsNjuA9WJ6aCiD1YuORWHfJidlAl3umthylGwuE/edit?usp=sharing"" rel=""nofollow noreferrer"">this document</a>,the document attached contains a small piece of data from the original survey. So I would like to ask for a better solution that counting all the columns and the rows, I'm asking for a recomendation or how I should check the data quality, basically in the document I have all the formulas that I need but I would like to have all the formulas in just one. Also in the document in the desirable output column there is the final result that I would like to have. Hoping my explanaiton was good about the desired output.</p>
","<google-sheets><google-sheets-formula><data-quality>","2023-03-07 01:44:44","124","0","2","75684319","<p>You may try this:</p>
<pre><code>=byrow(B2:F,lambda(z,if(offset(index(z,,1),0,-1)=&quot;&quot;,,
      let(x,(countif(z,&quot;Don't Know&quot;)+countblank(z)+--(index(z,,5)=0)+--(index(z,,5)&gt;9))/columns(z),
      if(x&gt;0.3,&quot;Bad&quot;,&quot;Good&quot;)&amp;&quot; Quality&quot;))))
</code></pre>
<p><a href=""https://i.stack.imgur.com/vjZ37.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vjZ37.png"" alt=""enter image description here"" /></a></p>
<p>if you wish to see the score alongside the quality status then use:</p>
<pre><code>=byrow(B2:F,lambda(z,if(offset(index(z,,1),0,-1)=&quot;&quot;,,let(x,(countif(z,&quot;Don't Know&quot;)+countblank(z)+--(index(z,,5)=0)+--(index(z,,5)&gt;9))/columns(z),{to_percent(x),if(x&gt;0.3,&quot;Bad&quot;,&quot;Good&quot;)&amp;&quot; Quality&quot;}))))
</code></pre>
"
"75635786","R - adding a new column based on binary data across many columns","<p>I cannot get my data frame to add an additional column. I have reviewed so many stack overflows, but here is a subset (<a href=""https://stackoverflow.com/questions/32074399/adding-a-new-column-in-a-matrix-in-r"">Adding a new column in a matrix in R</a>, <a href=""https://stackoverflow.com/questions/22710470/adding-new-column-to-data-frame-in-r"">adding new column to data frame in R</a>, <a href=""https://stackoverflow.com/questions/39610801/new-column-not-added-to-dataframe-in-r"">new column not added to dataframe in R</a>,<a href=""https://stackoverflow.com/questions/69732688/r-complete-a-dataset-with-a-new-column-added"">R: complete a dataset with a new column added</a>, <a href=""https://stackoverflow.com/questions/45983471/r-add-a-new-column-to-dataframes-from-a-function"">R: add a new column to dataframes from a function</a>)</p>
<p>I need a single column that tells us if there is a positive or &quot;1&quot; in any of the viral rows I have.</p>
<p>I am trying to determine probability and from what I see, I will need this column to do further calculations, so please help if able!</p>
<p>Sample data</p>
<pre><code>Filovirus (MOD) PCR   :    Phlebo (Sanchez-Seco) PCR
0                          0         
0                          1            
0                          0            
0                          0        
0                          0         
0                          0        
0                          0       
0                          0         
0                          0        
0                          0   


species code  forest site
&lt;fctr&gt;  &lt;dbl&gt; &lt;fctr&gt;
SM      1     UMNP-mangabey
SM      1     UMNP-mangabey
RC      9     UMNP-hondohondoc
BWC     9     UMNP-hondohondod
BWC     9     UMNP-hondohondod
BWC     9     UMNP-hondohondod
BWC     9     UMNP-hondohondod
BWC     9     UMNP-hondohondod
BWC     9     UMNP-hondohondod
BWC     9     UMNP-hondohondod

</code></pre>
<p>The closest I have gotten is getting base R to call which rows have the positive value</p>
<p>I followed the solution <a href=""https://stackoverflow.com/questions/70076693/creating-a-new-column-based-on-other-columns-in-r"">here</a> but have yet to get it to work for me.</p>
<pre><code>tmp=which(data==1,arr.ind=T)    
tmp=tmp[order(tmp[,&quot;row&quot;]),]
c(&quot;positive&quot;,&quot;negative&quot;)[tmp[,&quot;col&quot;]] -&gt; data$new
</code></pre>
<p>Any advice is greatly appreciated.</p>
<p>Dput</p>
<pre><code>structure(list(`Filovirus (MOD) PCR` = c(&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Filovirus (A) PCR` = c(&quot;0&quot;, &quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Filovirus (B) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Filo C PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Filovirus (D) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Coronavirus   (Quan) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Coronavirus (Watanabe) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Paramyxo  (Tong)  PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Flavivirus Moureau PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Flavivirus  Sanchez-seco PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Arena Lozano 1 PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Retrovirus Courgnard PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Simian Foamy Goldberg (Pol) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Simian Foamy Goldberg (LTR Region) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Influenza (Anthony) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Influenza (Liang) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Rhabdo (CII) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Enterovirus CII I PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Enterovirus CII-II PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Alphav   (Sanchez-Seco) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Lyssavirus (Vasquez-Moron) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Seadornavirus (CII) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Hantavirus (Raboni) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Hantavirus (Klempa) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Nipah (Wacharapleusadee) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Henipa (Feldman) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Bunya S (Briese) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Bunya L (Briese) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Phlebo (Sanchez-Seco) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), species = structure(c(3L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L), .Label = c(&quot;SM&quot;, &quot;SY&quot;, &quot;BWC&quot;, 
&quot;YB&quot;, &quot;RC&quot;), class = &quot;factor&quot;), code = c(2, 5, 5, 5, 5, 5, 5, 
5, 5, 5), forestsite = structure(c(3L, 14L, 14L, 14L, 14L, 14L, 
14L, 14L, 14L, 14L), .Label = c(&quot;Magombera1&quot;, &quot;Magombera2&quot;, &quot;NDUFR&quot;, 
&quot;Ndundulu1&quot;, &quot;Ndundulu2&quot;, &quot;Ndundulu3&quot;, &quot;Nyumbanitu&quot;, &quot;UMNP-campsite3&quot;, 
&quot;UMNP-hondohondoa&quot;, &quot;UMNP-hondohondob&quot;, &quot;UMNP-hondohondoc&quot;, &quot;UMNP-hondohondod&quot;, 
&quot;UMNP-hondohondoe&quot;, &quot;UMNP-HQ&quot;, &quot;MamaGoti&quot;, &quot;UMNP-mangabey&quot;, &quot;UMNP-njokamoni&quot;, 
&quot;UMNP-Sanje1&quot;, &quot;UMNP-Sanje2&quot;, &quot;UMNP-Sanje3&quot;, &quot;Sonjo&quot;, &quot;SonjoRoad&quot;
), class = &quot;factor&quot;)), row.names = c(NA, -10L), class = c(&quot;tbl_df&quot;, 
&quot;tbl&quot;, &quot;data.frame&quot;))
</code></pre>
","<r><statistics><r-markdown><data-manipulation><data-management>","2023-03-04 12:52:30","69","1","3","75635903","<p>Updated, given character columns, and new 32 column example</p>
<pre><code>df[&quot;new&quot;] = apply(df[, -c(29:32)], 1,\(x) ifelse(sum(as.numeric(x))&gt;0, &quot;positive&quot;, &quot;negative&quot;))
</code></pre>
<h1>Original answer (assuming numeric columns):</h1>
<p>You can simply do this:</p>
<pre><code>df[&quot;new&quot;] =ifelse(rowSums(df[,-(1:3)])&gt;0, &quot;positive&quot;, &quot;negative&quot;)
</code></pre>
<p>Output:</p>
<pre><code>   species code      forest_site Filovirus (MOD) PCR Phlebo (Sanchez-Seco) PCR      new
1       SM    1    UMNP-mangabey                   0                         0 negative
2       SM    1    UMNP-mangabey                   0                         1 positive
3       RC    9 UMNP-hondohondoc                   0                         0 negative
4      BWC    9 UMNP-hondohondod                   0                         0 negative
5      BWC    9 UMNP-hondohondod                   0                         0 negative
6      BWC    9 UMNP-hondohondod                   0                         0 negative
7      BWC    9 UMNP-hondohondod                   0                         0 negative
8      BWC    9 UMNP-hondohondod                   0                         0 negative
9      BWC    9 UMNP-hondohondod                   0                         0 negative
10     BWC    9 UMNP-hondohondod                   0                         0 negative
</code></pre>
<p>Input:</p>
<pre><code>structure(list(species = c(&quot;SM&quot;, &quot;SM&quot;, &quot;RC&quot;, &quot;BWC&quot;, &quot;BWC&quot;, &quot;BWC&quot;, 
&quot;BWC&quot;, &quot;BWC&quot;, &quot;BWC&quot;, &quot;BWC&quot;), code = c(1L, 1L, 9L, 9L, 9L, 9L, 
9L, 9L, 9L, 9L), forest_site = c(&quot;UMNP-mangabey&quot;, &quot;UMNP-mangabey&quot;, 
&quot;UMNP-hondohondoc&quot;, &quot;UMNP-hondohondod&quot;, &quot;UMNP-hondohondod&quot;, &quot;UMNP-hondohondod&quot;, 
&quot;UMNP-hondohondod&quot;, &quot;UMNP-hondohondod&quot;, &quot;UMNP-hondohondod&quot;, &quot;UMNP-hondohondod&quot;
), `Filovirus (MOD) PCR` = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0), `Phlebo (Sanchez-Seco) PCR` = c(0, 
1, 0, 0, 0, 0, 0, 0, 0, 0)), class = &quot;data.frame&quot;, row.names = c(NA, 
-10L))
</code></pre>
"
"75635786","R - adding a new column based on binary data across many columns","<p>I cannot get my data frame to add an additional column. I have reviewed so many stack overflows, but here is a subset (<a href=""https://stackoverflow.com/questions/32074399/adding-a-new-column-in-a-matrix-in-r"">Adding a new column in a matrix in R</a>, <a href=""https://stackoverflow.com/questions/22710470/adding-new-column-to-data-frame-in-r"">adding new column to data frame in R</a>, <a href=""https://stackoverflow.com/questions/39610801/new-column-not-added-to-dataframe-in-r"">new column not added to dataframe in R</a>,<a href=""https://stackoverflow.com/questions/69732688/r-complete-a-dataset-with-a-new-column-added"">R: complete a dataset with a new column added</a>, <a href=""https://stackoverflow.com/questions/45983471/r-add-a-new-column-to-dataframes-from-a-function"">R: add a new column to dataframes from a function</a>)</p>
<p>I need a single column that tells us if there is a positive or &quot;1&quot; in any of the viral rows I have.</p>
<p>I am trying to determine probability and from what I see, I will need this column to do further calculations, so please help if able!</p>
<p>Sample data</p>
<pre><code>Filovirus (MOD) PCR   :    Phlebo (Sanchez-Seco) PCR
0                          0         
0                          1            
0                          0            
0                          0        
0                          0         
0                          0        
0                          0       
0                          0         
0                          0        
0                          0   


species code  forest site
&lt;fctr&gt;  &lt;dbl&gt; &lt;fctr&gt;
SM      1     UMNP-mangabey
SM      1     UMNP-mangabey
RC      9     UMNP-hondohondoc
BWC     9     UMNP-hondohondod
BWC     9     UMNP-hondohondod
BWC     9     UMNP-hondohondod
BWC     9     UMNP-hondohondod
BWC     9     UMNP-hondohondod
BWC     9     UMNP-hondohondod
BWC     9     UMNP-hondohondod

</code></pre>
<p>The closest I have gotten is getting base R to call which rows have the positive value</p>
<p>I followed the solution <a href=""https://stackoverflow.com/questions/70076693/creating-a-new-column-based-on-other-columns-in-r"">here</a> but have yet to get it to work for me.</p>
<pre><code>tmp=which(data==1,arr.ind=T)    
tmp=tmp[order(tmp[,&quot;row&quot;]),]
c(&quot;positive&quot;,&quot;negative&quot;)[tmp[,&quot;col&quot;]] -&gt; data$new
</code></pre>
<p>Any advice is greatly appreciated.</p>
<p>Dput</p>
<pre><code>structure(list(`Filovirus (MOD) PCR` = c(&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Filovirus (A) PCR` = c(&quot;0&quot;, &quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Filovirus (B) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Filo C PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Filovirus (D) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Coronavirus   (Quan) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Coronavirus (Watanabe) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Paramyxo  (Tong)  PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Flavivirus Moureau PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Flavivirus  Sanchez-seco PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Arena Lozano 1 PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Retrovirus Courgnard PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Simian Foamy Goldberg (Pol) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Simian Foamy Goldberg (LTR Region) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Influenza (Anthony) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Influenza (Liang) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Rhabdo (CII) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Enterovirus CII I PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Enterovirus CII-II PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Alphav   (Sanchez-Seco) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Lyssavirus (Vasquez-Moron) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Seadornavirus (CII) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Hantavirus (Raboni) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Hantavirus (Klempa) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Nipah (Wacharapleusadee) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Henipa (Feldman) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Bunya S (Briese) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Bunya L (Briese) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Phlebo (Sanchez-Seco) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), species = structure(c(3L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L), .Label = c(&quot;SM&quot;, &quot;SY&quot;, &quot;BWC&quot;, 
&quot;YB&quot;, &quot;RC&quot;), class = &quot;factor&quot;), code = c(2, 5, 5, 5, 5, 5, 5, 
5, 5, 5), forestsite = structure(c(3L, 14L, 14L, 14L, 14L, 14L, 
14L, 14L, 14L, 14L), .Label = c(&quot;Magombera1&quot;, &quot;Magombera2&quot;, &quot;NDUFR&quot;, 
&quot;Ndundulu1&quot;, &quot;Ndundulu2&quot;, &quot;Ndundulu3&quot;, &quot;Nyumbanitu&quot;, &quot;UMNP-campsite3&quot;, 
&quot;UMNP-hondohondoa&quot;, &quot;UMNP-hondohondob&quot;, &quot;UMNP-hondohondoc&quot;, &quot;UMNP-hondohondod&quot;, 
&quot;UMNP-hondohondoe&quot;, &quot;UMNP-HQ&quot;, &quot;MamaGoti&quot;, &quot;UMNP-mangabey&quot;, &quot;UMNP-njokamoni&quot;, 
&quot;UMNP-Sanje1&quot;, &quot;UMNP-Sanje2&quot;, &quot;UMNP-Sanje3&quot;, &quot;Sonjo&quot;, &quot;SonjoRoad&quot;
), class = &quot;factor&quot;)), row.names = c(NA, -10L), class = c(&quot;tbl_df&quot;, 
&quot;tbl&quot;, &quot;data.frame&quot;))
</code></pre>
","<r><statistics><r-markdown><data-manipulation><data-management>","2023-03-04 12:52:30","69","1","3","75636021","<p><strong>Update:</strong>
Your <code>0</code> and <code>1</code> are character type. Transforming to number with <code>type.convert(as.is = TRUE)</code> will make the code work:</p>
<pre><code>library(dplyr)

df %&gt;%
  type.convert(as.is=TRUE) %&gt;% 
  mutate(new_column = if_else(rowSums(select(., contains(&quot;PCR&quot;))) &gt; 0, &quot;positive&quot;, &quot;negative&quot;))
</code></pre>
<pre><code>   Filovirus (…¹ Filov…² Filov…³ Filo …⁴ Filov…⁵ Coron…⁶ Coron…⁷ Param…⁸ Flavi…⁹ Flavi…˟ Arena…˟ Retro…˟ Simia…˟ Simia…˟ Influ…˟
           &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;
 1             0       0       0       0       0       0       0       0       0       0       0       0       0       0       0
 2             0       0       0       0       0       0       0       0       0       0       0       0       0       0       0
 3             0       0       0       0       0       0       0       0       0       0       0       0       0       0       0
 4             0       0       0       0       0       0       0       0       0       0       0       0       0       0       0
 5             0       0       0       0       0       0       0       0       0       0       0       0       0       0       0
 6             0       0       0       0       0       0       0       0       0       0       0       0       0       0       0
 7             0       0       0       0       0       0       0       0       0       0       0       0       0       0       0
 8             0       0       0       0       0       0       0       0       0       0       0       0       0       0       0
 9             0       0       0       0       0       0       0       0       0       0       0       0       0       0       0
10             0       0       0       0       0       0       0       0       0       0       0       0       0       0       0
# … with 18 more variables: `Influenza (Liang) PCR` &lt;int&gt;, `Rhabdo (CII) PCR` &lt;int&gt;, `Enterovirus CII I PCR` &lt;int&gt;,
#   `Enterovirus CII-II PCR` &lt;int&gt;, `Alphav   (Sanchez-Seco) PCR` &lt;int&gt;, `Lyssavirus (Vasquez-Moron) PCR` &lt;int&gt;,
#   `Seadornavirus (CII) PCR` &lt;int&gt;, `Hantavirus (Raboni) PCR` &lt;int&gt;, `Hantavirus (Klempa) PCR` &lt;int&gt;,
#   `Nipah (Wacharapleusadee) PCR` &lt;int&gt;, `Henipa (Feldman) PCR` &lt;int&gt;, `Bunya S (Briese) PCR` &lt;int&gt;,
#   `Bunya L (Briese) PCR` &lt;int&gt;, `Phlebo (Sanchez-Seco) PCR` &lt;int&gt;, species &lt;chr&gt;, code &lt;int&gt;, forestsite &lt;chr&gt;,
#   new_column &lt;chr&gt;, and abbreviated variable names ¹​`Filovirus (MOD) PCR`, ²​`Filovirus (A) PCR`, ³​`Filovirus (B) PCR`,
#   ⁴​`Filo C PCR`, ⁵​`Filovirus (D) PCR`, ⁶​`Coronavirus   (Quan) PCR`, ⁷​`Coronavirus (Watanabe) PCR`, …
# ℹ Use `colnames()` to see all variable names
</code></pre>
<p><strong>First answer:</strong>
The <code>dplyr</code> pendant would be:
Data taken from @langtang(many thanks):</p>
<pre><code>library(dplyr)

df %&gt;%
  mutate(new_column = if_else(rowSums(select(., contains(&quot;PCR&quot;))) &gt; 0, &quot;positive&quot;, &quot;negative&quot;))

</code></pre>
<pre><code>   species code      forest_site Filovirus (MOD) PCR Phlebo (Sanchez-Seco) PCR
1       SM    1    UMNP-mangabey            negative                  negative
2       SM    1    UMNP-mangabey            negative                  positive
3       RC    9 UMNP-hondohondoc            negative                  negative
4      BWC    9 UMNP-hondohondod            negative                  negative
5      BWC    9 UMNP-hondohondod            negative                  negative
6      BWC    9 UMNP-hondohondod            negative                  negative
7      BWC    9 UMNP-hondohondod            negative                  negative
8      BWC    9 UMNP-hondohondod            negative                  negative
9      BWC    9 UMNP-hondohondod            negative                  negative
10     BWC    9 UMNP-hondohondod            negative                  negative
</code></pre>
"
"75635786","R - adding a new column based on binary data across many columns","<p>I cannot get my data frame to add an additional column. I have reviewed so many stack overflows, but here is a subset (<a href=""https://stackoverflow.com/questions/32074399/adding-a-new-column-in-a-matrix-in-r"">Adding a new column in a matrix in R</a>, <a href=""https://stackoverflow.com/questions/22710470/adding-new-column-to-data-frame-in-r"">adding new column to data frame in R</a>, <a href=""https://stackoverflow.com/questions/39610801/new-column-not-added-to-dataframe-in-r"">new column not added to dataframe in R</a>,<a href=""https://stackoverflow.com/questions/69732688/r-complete-a-dataset-with-a-new-column-added"">R: complete a dataset with a new column added</a>, <a href=""https://stackoverflow.com/questions/45983471/r-add-a-new-column-to-dataframes-from-a-function"">R: add a new column to dataframes from a function</a>)</p>
<p>I need a single column that tells us if there is a positive or &quot;1&quot; in any of the viral rows I have.</p>
<p>I am trying to determine probability and from what I see, I will need this column to do further calculations, so please help if able!</p>
<p>Sample data</p>
<pre><code>Filovirus (MOD) PCR   :    Phlebo (Sanchez-Seco) PCR
0                          0         
0                          1            
0                          0            
0                          0        
0                          0         
0                          0        
0                          0       
0                          0         
0                          0        
0                          0   


species code  forest site
&lt;fctr&gt;  &lt;dbl&gt; &lt;fctr&gt;
SM      1     UMNP-mangabey
SM      1     UMNP-mangabey
RC      9     UMNP-hondohondoc
BWC     9     UMNP-hondohondod
BWC     9     UMNP-hondohondod
BWC     9     UMNP-hondohondod
BWC     9     UMNP-hondohondod
BWC     9     UMNP-hondohondod
BWC     9     UMNP-hondohondod
BWC     9     UMNP-hondohondod

</code></pre>
<p>The closest I have gotten is getting base R to call which rows have the positive value</p>
<p>I followed the solution <a href=""https://stackoverflow.com/questions/70076693/creating-a-new-column-based-on-other-columns-in-r"">here</a> but have yet to get it to work for me.</p>
<pre><code>tmp=which(data==1,arr.ind=T)    
tmp=tmp[order(tmp[,&quot;row&quot;]),]
c(&quot;positive&quot;,&quot;negative&quot;)[tmp[,&quot;col&quot;]] -&gt; data$new
</code></pre>
<p>Any advice is greatly appreciated.</p>
<p>Dput</p>
<pre><code>structure(list(`Filovirus (MOD) PCR` = c(&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Filovirus (A) PCR` = c(&quot;0&quot;, &quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Filovirus (B) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Filo C PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Filovirus (D) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Coronavirus   (Quan) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Coronavirus (Watanabe) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Paramyxo  (Tong)  PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Flavivirus Moureau PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Flavivirus  Sanchez-seco PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Arena Lozano 1 PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Retrovirus Courgnard PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Simian Foamy Goldberg (Pol) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Simian Foamy Goldberg (LTR Region) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Influenza (Anthony) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Influenza (Liang) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Rhabdo (CII) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Enterovirus CII I PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Enterovirus CII-II PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Alphav   (Sanchez-Seco) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Lyssavirus (Vasquez-Moron) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Seadornavirus (CII) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Hantavirus (Raboni) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Hantavirus (Klempa) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Nipah (Wacharapleusadee) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Henipa (Feldman) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Bunya S (Briese) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Bunya L (Briese) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), `Phlebo (Sanchez-Seco) PCR` = c(&quot;0&quot;, 
&quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;), species = structure(c(3L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L), .Label = c(&quot;SM&quot;, &quot;SY&quot;, &quot;BWC&quot;, 
&quot;YB&quot;, &quot;RC&quot;), class = &quot;factor&quot;), code = c(2, 5, 5, 5, 5, 5, 5, 
5, 5, 5), forestsite = structure(c(3L, 14L, 14L, 14L, 14L, 14L, 
14L, 14L, 14L, 14L), .Label = c(&quot;Magombera1&quot;, &quot;Magombera2&quot;, &quot;NDUFR&quot;, 
&quot;Ndundulu1&quot;, &quot;Ndundulu2&quot;, &quot;Ndundulu3&quot;, &quot;Nyumbanitu&quot;, &quot;UMNP-campsite3&quot;, 
&quot;UMNP-hondohondoa&quot;, &quot;UMNP-hondohondob&quot;, &quot;UMNP-hondohondoc&quot;, &quot;UMNP-hondohondod&quot;, 
&quot;UMNP-hondohondoe&quot;, &quot;UMNP-HQ&quot;, &quot;MamaGoti&quot;, &quot;UMNP-mangabey&quot;, &quot;UMNP-njokamoni&quot;, 
&quot;UMNP-Sanje1&quot;, &quot;UMNP-Sanje2&quot;, &quot;UMNP-Sanje3&quot;, &quot;Sonjo&quot;, &quot;SonjoRoad&quot;
), class = &quot;factor&quot;)), row.names = c(NA, -10L), class = c(&quot;tbl_df&quot;, 
&quot;tbl&quot;, &quot;data.frame&quot;))
</code></pre>
","<r><statistics><r-markdown><data-manipulation><data-management>","2023-03-04 12:52:30","69","1","3","75636627","<p>Another option is <code>if_any</code></p>
<pre><code>library(dplyr)
df1 %&gt;%
 type.convert(as.is = TRUE) %&gt;%
 mutate(new_column = c(&quot;negative&quot;, &quot;positive&quot;)[if_any(contains(&quot;PCR&quot;)) + 1])
</code></pre>
<p>-output</p>
<pre><code>  species code      forest_site Filovirus (MOD) PCR Phlebo (Sanchez-Seco) PCR new_column
1       SM    1    UMNP-mangabey                   0                         0   negative
2       SM    1    UMNP-mangabey                   0                         1   positive
3       RC    9 UMNP-hondohondoc                   0                         0   negative
4      BWC    9 UMNP-hondohondod                   0                         0   negative
5      BWC    9 UMNP-hondohondod                   0                         0   negative
6      BWC    9 UMNP-hondohondod                   0                         0   negative
7      BWC    9 UMNP-hondohondod                   0                         0   negative
8      BWC    9 UMNP-hondohondod                   0                         0   negative
9      BWC    9 UMNP-hondohondod                   0                         0   negative
10     BWC    9 UMNP-hondohondod                   0                         0   negative
</code></pre>
"
"75616772","React Trypescript Axios response data mapping","<p>I'm having a problem with using the response that I get with axios from a simple weather API - specifically, I'm having trouble mapping the response.</p>
<p>The <code>.map()</code> function does not work since my response is an object, I get that. My WeatherApi component returns the response - an object.</p>
<p>I then want to import the WeatherApi component into my App component and then access the data inside it like: <code>weather.location.name</code> and either map everything in advance to variables (like in the interface) or just do it on the fly when rendering the HTML like <code>&lt;h1&gt;{weather.location.name}&lt;/h1&gt;</code>.</p>
<p>If I do this, I either get errors in the app that sometimes weather is undefined or build / type errors that <code>.location</code> does not exist on type <code>WeatherData[]</code> which it does (check below).</p>
<p>How can I map everything nice and tidy to use in my App component?</p>
<p><strong>WeatherApi.ts</strong></p>
<pre><code>const WeatherApi = () =&gt; {
  const [weather, setWeather] = useState&lt;WeatherData[] | null&gt;([])
  
  useEffect(() =&gt; {
    const url:string = 'https://api.weatherapi.com/v1/current.json?key=d7db22f333fc4e6aaf3110311222601&amp;q=London&amp;aqi=no';

    axios
      .get&lt;WeatherData[]&gt;(url)
      .then(res =&gt; {
       setWeather(res.data)
      })
      .catch(err =&gt; {
        console.log(err)
      })
  },[])

  return weather ? weather : null
}

export default WeatherApi
</code></pre>
<p><strong>WeatherData.ts</strong></p>
<pre><code>export interface WeatherData {
    location: Location
    current: Current
  }
  
  export interface Location {
    name: string
    country: string
    localtime_epoch: number
    localtime: string
  }
  
  export interface Current {
    temp_c: number
    is_day: number
    condition: Condition
    feelslike_c: number
  }
  
  export interface Condition {
    text: string
    icon: string
    code: number
  }
</code></pre>
<p><strong>App.tsx</strong></p>
<pre><code>import WeatherApi from '../api/WeatherApi'

const Weather = () =&gt; {
    const weatherData = WeatherApi()

    // map weatherData here like in interface
    // const name = weatherData.location.name
    // const temp = weatherData.current.temp_c
    // etc.

  return (
    &lt;div&gt;
      // or add the data directly here as {weatherData.location.name} etc.
    &lt;/div&gt;
  )
}

export default Weather
</code></pre>
","<javascript><reactjs><typescript>","2023-03-02 14:24:29","28","0","1","75616915","<p>I think since you use a hook into <code>WeatherApi()</code> (the <code>useEffect</code>) you actually need to name it something like <code>useWeatherApi()</code> in order for react to understand it is actually a hook.</p>
<p>And use it like so</p>
<pre><code>const Weather = () =&gt; {
    const weatherData = useWeatherApi()

    // map weatherData here like in interface
    // const name = weatherData.location.name
    // const temp = weatherData.current.temp_c
    // etc.

  return (
    &lt;div&gt;
      // or add the data directly here as {weatherData.location.name} etc.
    &lt;/div&gt;
  )
}
</code></pre>
"
"75615540","Not able to upgrade hive metastore In postgres","<p>I am upgrading Hive from 2.3.2 to 3.1.3
And during startup of hive metastore service I got version related error. That metastore is in 2.3.2 version and it requires 3.1.3</p>
<p>So I try to upgrade it using schemaTool but it failed with the error that table already present. As metastore was present with version 2.3.2 i think that table will be present only.</p>
<p>Still on two to three try when i was not able to run it successfully i tried to manually delete that table but when i tried it after deleting it again failed with another table already present.</p>
<p>If i will drop table one by one like this then metadata will go away for hive tables.
I can recreate table but partition recreation is difficult for me so is there any way to deal with this problem.</p>
<p>Hive metastore may be in inconsistent state as i have done many things which may lead to this, how to repair hive metastore.
How to Take backup and reinitialize backed metastore??</p>
<pre><code>schematool -validate -dbType postgres
Output&gt;&gt;
Starting metastore validation

Validating schema version
Metastore schema version is not compatible. Hive Version: 3.1.0, Database Schema Version: 2.3.0
Failed in schema version validation.
[FAIL]

Validating sequence number for SEQUENCE_TABLE
Succeeded in sequence number validation for SEQUENCE_TABLE.
[SUCCESS]

Validating metastore schema tables
Succeeded in schema table validation.
[SUCCESS]

Validating DFS locations
Succeeded in DFS location validation.
[SUCCESS]

Validating columns for incorrect NULL values.
Succeeded in column validation for incorrect NULL values.
[SUCCESS]

Done with metastore validation: [FAIL]
</code></pre>
<pre><code>schematool -upgradeSchema -dbType postgres -verbose
Output&gt;&gt;
Metastore connection URL:        jdbc:postgresql://PostgreSQL-V1.hadoop.com/hive?useSSL=
Metastore Connection Driver :    org.postgresql.Driver
Metastore connection User:       hivesrv
Starting upgrade metastore schema from version 2.3.0 to 3.1.0
Upgrade script upgrade-2.3.0-to-3.0.0.postgres.sql
Connecting to jdbc:postgresql://PostgreSQL-V1.hadoop.com/hive?useSSL=
Connected to: PostgreSQL (version 13.9)
Driver: PostgreSQL Native Driver (version PostgreSQL 9.4.1208.jre7)
Transaction isolation: TRANSACTION_READ_COMMITTED
0: jdbc:postgresql://PostgreSQL-V1&gt; !autocommit on
Autocommit status: true
0: jdbc:postgresql://PostgreSQL-V1&gt; SELECT 'Upgrading MetaStore schema from 2.3.0 to 3.0.0'
+-------------------------------------------------+
|                    ?column?                     |
+-------------------------------------------------+
| Upgrading MetaStore schema from 2.3.0 to 3.0.0  |
+-------------------------------------------------+
1 row selected (0.042 seconds)
0: jdbc:postgresql://PostgreSQL-V1&gt; CREATE TABLE &quot;METASTORE_DB_PROPERTIES&quot; ( &quot;PROPERTY_KEY&quot; VARCHAR(255) NOT NULL, &quot;PROPERTY_VALUE&quot; VARCHAR(1000) NOT NULL, &quot;DESCRIPTION&quot; VARCHAR(1000) )
Error: ERROR: relation &quot;METASTORE_DB_PROPERTIES&quot; already exists (state=42P07,code=0)
Closing: 0: jdbc:postgresql://PostgreSQL-V1.hadoop.com/hive?useSSL=
org.apache.hadoop.hive.metastore.HiveMetaException: Upgrade FAILED! Metastore state would be inconsistent !!
Underlying cause: java.io.IOException : Schema script failed, errorcode 2
org.apache.hadoop.hive.metastore.HiveMetaException: Upgrade FAILED! Metastore state would be inconsistent !!
        at org.apache.hive.beeline.HiveSchemaTool.doUpgrade(HiveSchemaTool.java:553)
        at org.apache.hive.beeline.HiveSchemaTool.doUpgrade(HiveSchemaTool.java:517)
        at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:1512)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:236)
Caused by: java.io.IOException: Schema script failed, errorcode 2
        at org.apache.hive.beeline.HiveSchemaTool.runBeeLine(HiveSchemaTool.java:1226)
        at org.apache.hive.beeline.HiveSchemaTool.runBeeLine(HiveSchemaTool.java:1204)
        at org.apache.hive.beeline.HiveSchemaTool.doUpgrade(HiveSchemaTool.java:548)
        ... 8 more
*** schemaTool failed ***
</code></pre>
","<postgresql><hadoop><hive><data-warehouse><hive-metastore>","2023-03-02 12:39:39","110","0","1","75778280","<p>Hi Replying to my Own question for future readers. So after spending 2 to 3 days i didn't get any known or standard solution for the same. The solutions which i applied needs to be verified by some expert from community, plz... provide your view as it will be helpfull to me also.</p>
<p>So i went to <code>/hive/scripts/metastore/upgrade/postgres</code> path and opened the <code>upgrade-2.3.0-to-3.0.0.postgres.sql</code> file and ran queries below <code>CREATE TABLE &quot;METASTORE_DB_PROPERTIES&quot; ( &quot;PROPERTY_KEY&quot; VARCHAR(255) NOT NULL, &quot;PROPERTY_VALUE&quot; VARCHAR(1000) NOT NULL, &quot;DESCRIPTION&quot; VARCHAR(1000) )</code> (which was giving error in my case) directly inside Postgres DB. It again fail 2 to 3 times with another query, similarly I skipped that query also and ran queries below them.</p>
<p>After this i ran <code>upgrade-3.0.0-to-3.1.0.postgres.sql</code>, similar to above script i skip the queries which were failing.</p>
<p>Point to be noted was, every single query which was failing was DROP queries and it was failing because it was trying to drop either table or constraint which doesn't exists.(So theoretically skipping the drop queries for tables and constraints which never exists actually should not affect metastore schema and data inside it).</p>
<p>Either it the issue where &quot;IF EXIST&quot; needs to be added in those DROP statements or It might be due to patch upgrades which added those tables and constraint which i never upgraded in my case.</p>
<p>In the end i checked in hive, partitions data and all were present at least for my case. So again this is not standard way to do this, so handle it carefully.</p>
"
"75609959","Aggregate functions for daily totals using records with start and end dates","<p>I am working with a list of &quot;sale event&quot; records in a BigQuery data warehouse. Each represents a time period when a particular item was available to purchase at a specific price. I need to calculate the combined dollar value of all items for sale on each day - assuming quantity = 1 here for simplicity.</p>
<p>So for the following input:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Sale_ID</th>
<th>SKU</th>
<th>start_date</th>
<th>end_date</th>
<th>sale_price</th>
</tr>
</thead>
<tbody>
<tr>
<td>ABC</td>
<td>123</td>
<td>2023-01-01</td>
<td>2023-01-04</td>
<td>3000.00</td>
</tr>
<tr>
<td>DEF</td>
<td>123</td>
<td>2023-01-05</td>
<td>2023-01-10</td>
<td>2500.00</td>
</tr>
<tr>
<td>GHI</td>
<td>456</td>
<td>2023-01-03</td>
<td>2023-01-08</td>
<td>1200.00</td>
</tr>
<tr>
<td>JKL</td>
<td>789</td>
<td>2023-01-02</td>
<td>2023-01-10</td>
<td>2400.00</td>
</tr>
</tbody>
</table>
</div>
<p>Output would be:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>selling_date</th>
<th>total_value_for_sale</th>
<th>items_for_sale*</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023-01-01</td>
<td>3000.00</td>
<td>123</td>
</tr>
<tr>
<td>2023-01-02</td>
<td>5400.00</td>
<td>123, 789</td>
</tr>
<tr>
<td>2023-01-03</td>
<td>6600.00</td>
<td>123, 456, 789</td>
</tr>
<tr>
<td>2023-01-04</td>
<td>6600.00</td>
<td>123, 456, 789</td>
</tr>
<tr>
<td>2023-01-05</td>
<td>6100.00</td>
<td>123, 456, 789</td>
</tr>
<tr>
<td>2023-01-06</td>
<td>6100.00</td>
<td>123, 456, 789</td>
</tr>
<tr>
<td>2023-01-07</td>
<td>6100.00</td>
<td>123, 456, 789</td>
</tr>
<tr>
<td>2023-01-08</td>
<td>6100.00</td>
<td>123, 456, 789</td>
</tr>
<tr>
<td>2023-01-09</td>
<td>3900.00</td>
<td>123, 789</td>
</tr>
<tr>
<td>2023-01-10</td>
<td>3900.00</td>
<td>123, 789</td>
</tr>
</tbody>
</table>
</div>
<p>*items_for_sale is not required output, just showing it to make the example clearer</p>
<p>I am using the following very simple but computationally intensive solution, which is not ideal for the large volume of data we have. I'm curious if anyone has a method that doesn't duplicate each sale record for each day it was active.</p>
<pre><code>with date_series as (
select dd
from unnest(generate_date_array(date('2023-01-01'), date('2023-01-10'), INTERVAL 1 DAY)) as dd)

select
d.dd as selling_date,
sum(price) as total_value_for_sale
from date_series d
left join sales_records s on s.start_date &lt;= d.dd and s.end_date &gt;= d.dd
group by selling_date
order by selling_date
</code></pre>
","<sql><google-bigquery><data-warehouse>","2023-03-01 23:18:18","56","0","1","75610442","<p>You can try below</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT selling_date, SUM(sale_price) total_value_for_sale, STRING_AGG(SKU, ', ') items_for_sale
  FROM sale_event,
UNNEST(GENERATE_DATE_ARRAY(start_date, end_date)) selling_date
 GROUP BY 1;
</code></pre>
<p><strong>Query results</strong></p>
<p><a href=""https://i.stack.imgur.com/EXJnrm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EXJnrm.png"" alt=""enter image description here"" /></a></p>
"
"75519682","Pub/Sub: How to retrieve messages sent to a dead-letter-topic and store them into a BigQuery table?","<p><strong>For context</strong></p>
<p>We currently have a data mining script which collects a massive throughput of event data and publishes it to a Pub/Sub topic with a schema enforcement (with all fields being optional). A Pub/Sub-to-BigQuery subscription then sends the data to a destination table.</p>
<p><strong>The problem</strong></p>
<p>At present the script fetching the data sometimes also picks up events that do not conform to the schema. In order to avoid downtimes, the script has been configured to ingest erroneous messages and send them to the destination table in the form of a single field with a JSON string containing message contents while the rest of the fields for that row are nulls. This obviously poses a data quality issue, since end-users shouldn't have to filter through so many null values. However, since the schema failure could have resulted on a variety of fields, I need to somehow retain the raw JSON string and monitor which events are causing trouble.</p>
<p><strong>Potential Solution</strong></p>
<p>To move toward a cleaner approach, I have been exploring the use of a dead-letter topic to which these erroneous messages are sent. The idea is to have two separate BigQuery tables:</p>
<ul>
<li>One corresponding a stricter policy that filters out errors before they reach destination table.</li>
<li>One which retains only the messages from the dead-letter topic (storing the unacknowledged messages in a GCS bucket would also be an option).</li>
</ul>
<p>This would avoid the data-quality issue, while also retain information regarding errors in a more cost-efficient way.</p>
<p><strong>Question</strong></p>
<p>Is what I am trying to achieve possible? I have explored Google's documentation on dead-letter topics, but there is no details on how to retrieve unacknowledged messages.</p>
","<google-bigquery><google-cloud-pubsub><data-warehouse><dead-letter>","2023-02-21 11:12:26","101","0","1","75520061","<p>A dead-letter topic is just like any other topic. Once messages are published to it, you can use a subscription that is attached to the topic to fetch the messages and do whatever you want with them. If all you want to do is write the messages to a BigQuery table, consider attaching a <a href=""https://cloud.google.com/pubsub/docs/bigquery"" rel=""nofollow noreferrer"">BigQuery subscription</a> to your dead-letter topic.</p>
"
"75519541","Data Insertion takes too long Pentaho (PDI)","<p>I'm trying to insert 500k+ rows into a table but it takes too long. The speed at the beginning is around 1000 r/s but it goes down to 100 r/s quickly after that.</p>
<p>Here is the code I'm executing in my transformation</p>
<pre><code>SELECT 
    accident_index,
    longitude,
    latitude,
    accident_severity as accident_severity_fk,
    number_of_vehicles,
    number_of_casualties,
    EXTRACT('year' from formatted_date)*10000 + EXTRACT('month' from formatted_date)*100+EXTRACT('day' from formatted_date) as date_fk,
    r.road_information_pk as road_information_fk,
    speed_limit,
    p.pedestrian_crossing_information_pk as pedestrian_crossing_information_fk,
    l.light_conditions_pk as light_conditions_fk,
    w.weather_conditions_pk as weather_conditions_fk,
    rs.road_surface_conditions_pk as road_surface_conditions_fk,
    s.special_conditions_at_site_pk as special_conditions_at_site_fk,
    c.carriageway_hazards_pk as carriageway_hazards_fk,
    urban_or_rural_area as urban_or_rural_area_fk,
    time
FROM staging.accidents ac
left join core.road_information r on r.road_type = ac.road_type and r.junction_control = coalesce(ac.junction_control,'Unknown')
left join core.pedestrian_crossing_information p on p.pedestrian_crossing_human_control = coalesce(ac.pedestrian_crossing_human_control,'Unknown')
    and p.pedestrian_crossing_physical_facilities = coalesce(ac.pedestrian_crossing_physical_facilities,'Unknown')
left join core.light_conditions as l on l.light_conditions = coalesce(ac.light_conditions,'Unknown')
left join core.weather_conditions as w on w.weather_conditions = coalesce(ac.weather_conditions,'Unknown')
left join core.road_surface_conditions as rs on rs.road_surface_conditions = coalesce(ac.road_surface_conditions,'Unknown')
left join core.special_conditions_at_site as s on s.special_conditions_at_site = coalesce(ac.special_conditions_at_site,'Unknown')
left join core.carriageway_hazards as c on c.carriageway_hazards = coalesce(ac.carriageway_hazards,'Unknown')

</code></pre>
<p>I'm using a <code>Insert/Update</code> tranformation. For testing purposes, I tried inserting only a couple of rows and avoid the joins but it goes even a bit slower. Any idea on how I can improve the speed?</p>
","<sql><database><pentaho><data-warehouse><pdi>","2023-02-21 10:58:08","80","0","1","75520938","<p>Your speed problem is more likely to be on write than on read. Best way to check this is disabling the hop coming into the insert/update step and running the transformation. It should run at several thousand or even tens of thousand of rows per second. If that’s the case, optimising your read query is pointless, the bottleneck is on the output.</p>
<p>Insert/Update is notoriously slow, as it needs round trips to lookup each row and decide whether to insert or update. If the target table is initially empty then there’s no need to insert/update. Just sort the rows on the lookup keys, pick up the values you want for each attribute and use a table output step.</p>
<p>If you need to update the table, then there are different possible strategies to speed that up, depending on how much of that dataset is new vs existing:</p>
<ul>
<li>if only a few rows need updating and most are inserted then you can use a table output step to insert them all, rely on a db constraint to reject duplicates and have an Update step after the table output step and set it as “error handling”. PDI will try to insert all rows, a few of them fail, are sent down the error handling hop and those are updated. Advantage is that only actual updates use the update step, so the number of round trips goes to almost zero. In this scenario the “skip lookup” checkbox should be selected. No point in doing the lookup, you already know there’s an existing row, just update.</li>
<li>If the ratio of updates v inserts is higher then you can read from the target table, do a merge rows (diff) and then a switch/case to send new rows to a table output step, deleted rows to a delete step, changed rows to an update step (with skip lookup box checked) and identical rows ignored.</li>
</ul>
"
"75509434","How bucketing works for Hive Acid Tables?","<p>In Hive, I understand how bucketing works for External Tables and Non Acid Managed tables.Based on the column that is specified inside clustered-by clause in the corresponding DDL statement, bucket is identified for corresponding row and that data is inserted into that relevant directory on the HDFS.</p>
<p>For Hive ACID Tables, I checked the directory structure of tables and noticed data is directed towards specific buckets inside delta directory though no bucketing is configured in corresponding DDL statement while creating that table. Following is example</p>
<blockquote>
<p>hdfs dfs -ls /warehouse/tablespace/managed/hive/part.db/employee/delta_0000001_0000001_0000<br />
Found 3 items
/warehouse/tablespace/managed/hive/part.db/employee/delta_0000001_0000001_0000/bucket_00000_0
/warehouse/tablespace/managed/hive/part.db/employee/delta_0000001_0000001_0000/bucket_00001_0
/warehouse/tablespace/managed/hive/part.db/employee/delta_0000001_0000001_0000/bucket_00002_0</p>
</blockquote>
<p>Can someone please help here in understanding the above directory structure of Hive ACID tables as there are total 3 buckets are present inside delta directory for the employee table?</p>
","<hadoop><hive><bigdata><data-warehouse><acid>","2023-02-20 12:44:40","33","1","1","75552313","<p>If you are curious about the code then follow below link.</p>
<p><a href=""https://github.com/apache/hive/blob/36f5d91acb0fac00a5d46049bd45b744fe9aaab6/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java#L490"" rel=""nofollow noreferrer"">https://github.com/apache/hive/blob/36f5d91acb0fac00a5d46049bd45b744fe9aaab6/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java#L490</a></p>
<p>Basically this is done for delete operation in hive.</p>
"
"75403669","creating new columns for unique events and then counting events in R by group","<p>A subset of my data looks like this but with many more groupings (IDs):</p>
<pre><code> ID                          time                class    
   &lt;chr&gt;                       &lt;dttm&gt;              &lt;fct&gt;    
 1 BBR-b172021-M_fall_winter_4 2022-11-01 19:03:31 migrating
 2 BBR-b172021-M_fall_winter_4 2022-11-04 22:03:33 migrating 
 3 BBR-b172021-M_fall_winter_4 2022-11-07 18:03:34 migrating 
 4 BBR-b172021-M_fall_winter_4 2022-11-08 21:03:34 stopover 
 5 BBR-b172021-M_fall_winter_4 2022-11-10 21:03:39 stopover 
 6 BBR-b172021-M_fall_winter_4 2022-11-14 18:03:37 migrating 
 7 BBR-b172021-M_fall_winter_4 2022-11-17 06:04:08 migrating 
 8 BBR-b172021-M_fall_winter_4 2022-11-18 06:04:08 stopover 
 9 BBR-b172021-M_fall_winter_4 2022-11-19 00:03:41 winter 
10 BBR-b172021-M_fall_winter_4 2022-11-27 00:03:51 winter 
11 LINWR-b1282020-M_fall_winter_3 2022-01-14 11:00:08 migrating
12 LINWR-b1282020-M_fall_winter_3 2022-01-15 13:59:45 stopover
13 LINWR-b1282020-M_fall_winter_3 2022-01-20 02:59:54 stopover
14 LINWR-b1282020-M_fall_winter_3 2022-01-21 03:00:14 migrating
15 LINWR-b1282020-M_fall_winter_3 2022-01-21 16:59:47 stopover
16 LINWR-b1282020-M_fall_winter_3 2022-01-22 16:59:45 winter
</code></pre>
<p>I am trying to create unique columns either through <code>mapping</code> or <code>group_by</code> and <code>mutate</code> but I don't know where to begin. I would like several new columns describing unique sequential events, their sum, and their duration. New columns added to the dataframe I would suspect would look something like this:</p>
<pre><code>
newcols &lt;- data.frame(unique_class = c(&quot;migrating1&quot;, &quot;migrating1&quot;, &quot;migrating1&quot;, &quot;stopover1&quot;, 
                                       &quot;stopover1&quot;, &quot;migrating2&quot;, &quot;migrating2&quot;, &quot;stopover2&quot;, 
                                       &quot;winter1&quot;, &quot;winter1&quot;, &quot;migrating1&quot;, &quot;stopover1&quot;, 
                                       &quot;stopover1&quot;, &quot;migrating2&quot;, &quot;stopover2&quot;, &quot;winter1&quot;),
                      migrate_sum = c(2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2),
                      stopover_sum = c(2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2),
                      winter_sum = c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1),
                      event_duration = c(6,6,6,2,2,3,3,0,8,8,0,5,5,0,0,0))
</code></pre>
<p>...where event_duration column would equate to time in days or hours. I know I need to <code>group_by(ID)</code> but and <code>mutate()</code> but not sure how to get the unique classes or lagged duration of times for each class. Any help appreciated.</p>
<p>NOT SURE WHERE TO PUT THIS SO EDITING MY QUESTION: I TRIED @AKRUN SOLUTION BUT IT DIDN'T QUITE WORK. IT PRODUCED THE UNIQUE_CLASS WELL BUT SUMMARIES ARE NOT INCORRECT. HERE'S AN EXAMPLE OF A DATAFRAME PRODUCED USING SOLUTION BELOW AND SUBSET BY UNIQUE ID: <code>fall_mig2 %&gt;% filter(BirdsID_season == &quot;BBR-b432021-M_fall_winter_4&quot;) %&gt;% select(BirdsID_season, x, y, time, unique_class, class, stopover_sum) slice_head &lt;-  fall_mig2 %&gt;% filter(BirdsID_season == &quot;BBR-b432021-M_fall_winter_4&quot;) %&gt;% slice_head(n = 10) slice_tail &lt;- fall_mig2 %&gt;% filter(BirdsID_season == &quot;BBR-b432021-M_fall_winter_4&quot;) %&gt;% slice_tail(n = 10) bind_rows(slice_head, slice_tail) %&gt;% select(BirdsID_season, x, y, time, stopover_sum)</code> and the result:</p>
<pre><code> BirdsID_season                  x     y time                unique_class class     stopover_sum
   &lt;chr&gt;                       &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;        &lt;chr&gt;            &lt;int&gt;
 1 BBR-b432021-M_fall_winter_4 -99.2  48.1 2022-11-09 19:09:01 migrating1   migrating            3
 2 BBR-b432021-M_fall_winter_4 -99.2  48.1 2022-11-09 21:08:36 migrating1   migrating            3
 3 BBR-b432021-M_fall_winter_4 -99.2  48.1 2022-11-09 23:08:55 migrating1   migrating            3
 4 BBR-b432021-M_fall_winter_4 -99.2  48.1 2022-11-10 01:09:11 migrating1   migrating            3
 5 BBR-b432021-M_fall_winter_4 -99.2  48.1 2022-11-10 03:08:50 migrating1   migrating            3
 6 BBR-b432021-M_fall_winter_4 -99.2  48.1 2022-11-10 05:09:06 migrating1   migrating            3
 7 BBR-b432021-M_fall_winter_4 -99.2  48.1 2022-11-10 07:08:43 migrating1   migrating            3
 8 BBR-b432021-M_fall_winter_4 -99.2  48.1 2022-11-10 09:08:54 migrating1   migrating            3
 9 BBR-b432021-M_fall_winter_4 -99.2  48.1 2022-11-10 11:09:07 migrating1   migrating            3
10 BBR-b432021-M_fall_winter_4 -99.2  48.1 2022-11-10 13:08:39 migrating1   migrating            3
11 BBR-b432021-M_fall_winter_4 -89.3  36.7 2022-12-13 23:08:30 winter1      winter               1
12 BBR-b432021-M_fall_winter_4 -89.3  36.7 2022-12-14 01:08:45 winter1      winter               1
13 BBR-b432021-M_fall_winter_4 -89.3  36.7 2022-12-14 03:08:45 winter1      winter               1
14 BBR-b432021-M_fall_winter_4 -89.3  36.7 2022-12-14 05:08:26 winter1      winter               1
15 BBR-b432021-M_fall_winter_4 -89.3  36.7 2022-12-14 07:08:22 winter1      winter               1
16 BBR-b432021-M_fall_winter_4 -89.3  36.7 2022-12-14 09:08:45 winter1      winter               1
17 BBR-b432021-M_fall_winter_4 -89.3  36.7 2022-12-14 11:08:54 winter1      winter               1
18 BBR-b432021-M_fall_winter_4 -89.3  36.7 2022-12-14 13:08:19 winter1      winter               1
19 BBR-b432021-M_fall_winter_4 -89.3  36.7 2022-12-14 15:08:47 winter1      winter               1
20 BBR-b432021-M_fall_winter_4 -89.4  36.7 2022-12-14 17:08:19 winter1      winter               1
</code></pre>
<p><code>stopover_sum</code> should be 1 (which is in the middle of the subsetted df)
. I'm not sure where the 3 is coming from. Trying to dissect the solution now.</p>
","<r><time-series><uniqueidentifier><data-management>","2023-02-09 19:42:13","51","2","2","75404371","<p>We may create a run-length-id column grouped by 'Class', convert the 'time' to <code>Date</code> class, then grouped by 'ID', 'class', get the number of distinct (<code>n_distinct</code>) elements in 'grp', as well as the <code>unique_class</code> is created by <code>paste</code>ing the 'class' with the <code>unique</code> 'grp' indexes. Do a second grouping by 'ID', 'unique_class' to calculate the 'event_duration' ie. the number of days between the <code>max/min</code> 'date' values, select the columns of interest, reshape to 'wide' with <code>pivot_wider</code> and <code>fill</code> the values in the <code>_sum</code> to previous non-NA values</p>
<pre><code>library(dplyr)
library(lubridate)
library(tidyr)
library(stringr)
library(data.table)
df1 %&gt;% 
   mutate(grp = rleid(class), date = as.Date(ymd_hms(time))) %&gt;% 
   group_by(ID, class) %&gt;% 
   mutate(Count = n_distinct(grp), 
     unique_class = str_c(class, match(grp, unique(grp)))) %&gt;% 
   group_by(ID, unique_class) %&gt;% 
   mutate(event_duration = as.integer(max(date) - min(date))) %&gt;% 
   ungroup %&gt;% 
   transmute(rn = row_number(), class = str_c(class, '_sum'),
    Count, unique_class, event_duration) %&gt;% 
   pivot_wider(names_from = class, values_from = Count) %&gt;% 
   fill(ends_with(&quot;_sum&quot;), .direction = &quot;downup&quot;) %&gt;% 
   select(-rn) %&gt;%
   relocate(event_duration, .after = last_col())
</code></pre>
<p>-output</p>
<pre><code># A tibble: 16 × 5
   unique_class migrating_sum stopover_sum winter_sum event_duration
   &lt;chr&gt;                &lt;int&gt;        &lt;int&gt;      &lt;int&gt;          &lt;int&gt;
 1 migrating1               2            2          1              6
 2 migrating1               2            2          1              6
 3 migrating1               2            2          1              6
 4 stopover1                2            2          1              2
 5 stopover1                2            2          1              2
 6 migrating2               2            2          1              3
 7 migrating2               2            2          1              3
 8 stopover2                2            2          1              0
 9 winter1                  2            2          1              8
10 winter1                  2            2          1              8
11 migrating1               2            2          1              0
12 stopover1                2            2          1              5
13 stopover1                2            2          1              5
14 migrating2               2            2          1              0
15 stopover2                2            2          1              0
16 winter1                  2            2          1              0
</code></pre>
<h3>data</h3>
<pre><code>df1 &lt;- structure(list(ID = c(&quot;BBR-b172021-M_fall_winter_4&quot;,
 &quot;BBR-b172021-M_fall_winter_4&quot;, 
&quot;BBR-b172021-M_fall_winter_4&quot;, &quot;BBR-b172021-M_fall_winter_4&quot;, 
&quot;BBR-b172021-M_fall_winter_4&quot;, &quot;BBR-b172021-M_fall_winter_4&quot;, 
&quot;BBR-b172021-M_fall_winter_4&quot;, &quot;BBR-b172021-M_fall_winter_4&quot;, 
&quot;BBR-b172021-M_fall_winter_4&quot;, &quot;BBR-b172021-M_fall_winter_4&quot;, 
&quot;LINWR-b1282020-M_fall_winter_3&quot;, &quot;LINWR-b1282020-M_fall_winter_3&quot;, 
&quot;LINWR-b1282020-M_fall_winter_3&quot;, &quot;LINWR-b1282020-M_fall_winter_3&quot;, 
&quot;LINWR-b1282020-M_fall_winter_3&quot;, &quot;LINWR-b1282020-M_fall_winter_3&quot;
), time = c(&quot;2022-11-01 19:03:31&quot;, &quot;2022-11-04 22:03:33&quot;, &quot;2022-11-07 18:03:34&quot;, 
&quot;2022-11-08 21:03:34&quot;, &quot;2022-11-10 21:03:39&quot;, &quot;2022-11-14 18:03:37&quot;, 
&quot;2022-11-17 06:04:08&quot;, &quot;2022-11-18 06:04:08&quot;, &quot;2022-11-19 00:03:41&quot;, 
&quot;2022-11-27 00:03:51&quot;, &quot;2022-01-14 11:00:08&quot;, &quot;2022-01-15 13:59:45&quot;, 
&quot;2022-01-20 02:59:54&quot;, &quot;2022-01-21 03:00:14&quot;, &quot;2022-01-21 16:59:47&quot;, 
&quot;2022-01-22 16:59:45&quot;), class = c(&quot;migrating&quot;, &quot;migrating&quot;, &quot;migrating&quot;, 
&quot;stopover&quot;, &quot;stopover&quot;, &quot;migrating&quot;, &quot;migrating&quot;, &quot;stopover&quot;, 
&quot;winter&quot;, &quot;winter&quot;, &quot;migrating&quot;, &quot;stopover&quot;, &quot;stopover&quot;, &quot;migrating&quot;, 
&quot;stopover&quot;, &quot;winter&quot;)), class = &quot;data.frame&quot;, row.names = c(&quot;1&quot;, 
&quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;, &quot;13&quot;, 
&quot;14&quot;, &quot;15&quot;, &quot;16&quot;))
</code></pre>
"
"75403669","creating new columns for unique events and then counting events in R by group","<p>A subset of my data looks like this but with many more groupings (IDs):</p>
<pre><code> ID                          time                class    
   &lt;chr&gt;                       &lt;dttm&gt;              &lt;fct&gt;    
 1 BBR-b172021-M_fall_winter_4 2022-11-01 19:03:31 migrating
 2 BBR-b172021-M_fall_winter_4 2022-11-04 22:03:33 migrating 
 3 BBR-b172021-M_fall_winter_4 2022-11-07 18:03:34 migrating 
 4 BBR-b172021-M_fall_winter_4 2022-11-08 21:03:34 stopover 
 5 BBR-b172021-M_fall_winter_4 2022-11-10 21:03:39 stopover 
 6 BBR-b172021-M_fall_winter_4 2022-11-14 18:03:37 migrating 
 7 BBR-b172021-M_fall_winter_4 2022-11-17 06:04:08 migrating 
 8 BBR-b172021-M_fall_winter_4 2022-11-18 06:04:08 stopover 
 9 BBR-b172021-M_fall_winter_4 2022-11-19 00:03:41 winter 
10 BBR-b172021-M_fall_winter_4 2022-11-27 00:03:51 winter 
11 LINWR-b1282020-M_fall_winter_3 2022-01-14 11:00:08 migrating
12 LINWR-b1282020-M_fall_winter_3 2022-01-15 13:59:45 stopover
13 LINWR-b1282020-M_fall_winter_3 2022-01-20 02:59:54 stopover
14 LINWR-b1282020-M_fall_winter_3 2022-01-21 03:00:14 migrating
15 LINWR-b1282020-M_fall_winter_3 2022-01-21 16:59:47 stopover
16 LINWR-b1282020-M_fall_winter_3 2022-01-22 16:59:45 winter
</code></pre>
<p>I am trying to create unique columns either through <code>mapping</code> or <code>group_by</code> and <code>mutate</code> but I don't know where to begin. I would like several new columns describing unique sequential events, their sum, and their duration. New columns added to the dataframe I would suspect would look something like this:</p>
<pre><code>
newcols &lt;- data.frame(unique_class = c(&quot;migrating1&quot;, &quot;migrating1&quot;, &quot;migrating1&quot;, &quot;stopover1&quot;, 
                                       &quot;stopover1&quot;, &quot;migrating2&quot;, &quot;migrating2&quot;, &quot;stopover2&quot;, 
                                       &quot;winter1&quot;, &quot;winter1&quot;, &quot;migrating1&quot;, &quot;stopover1&quot;, 
                                       &quot;stopover1&quot;, &quot;migrating2&quot;, &quot;stopover2&quot;, &quot;winter1&quot;),
                      migrate_sum = c(2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2),
                      stopover_sum = c(2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2),
                      winter_sum = c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1),
                      event_duration = c(6,6,6,2,2,3,3,0,8,8,0,5,5,0,0,0))
</code></pre>
<p>...where event_duration column would equate to time in days or hours. I know I need to <code>group_by(ID)</code> but and <code>mutate()</code> but not sure how to get the unique classes or lagged duration of times for each class. Any help appreciated.</p>
<p>NOT SURE WHERE TO PUT THIS SO EDITING MY QUESTION: I TRIED @AKRUN SOLUTION BUT IT DIDN'T QUITE WORK. IT PRODUCED THE UNIQUE_CLASS WELL BUT SUMMARIES ARE NOT INCORRECT. HERE'S AN EXAMPLE OF A DATAFRAME PRODUCED USING SOLUTION BELOW AND SUBSET BY UNIQUE ID: <code>fall_mig2 %&gt;% filter(BirdsID_season == &quot;BBR-b432021-M_fall_winter_4&quot;) %&gt;% select(BirdsID_season, x, y, time, unique_class, class, stopover_sum) slice_head &lt;-  fall_mig2 %&gt;% filter(BirdsID_season == &quot;BBR-b432021-M_fall_winter_4&quot;) %&gt;% slice_head(n = 10) slice_tail &lt;- fall_mig2 %&gt;% filter(BirdsID_season == &quot;BBR-b432021-M_fall_winter_4&quot;) %&gt;% slice_tail(n = 10) bind_rows(slice_head, slice_tail) %&gt;% select(BirdsID_season, x, y, time, stopover_sum)</code> and the result:</p>
<pre><code> BirdsID_season                  x     y time                unique_class class     stopover_sum
   &lt;chr&gt;                       &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;        &lt;chr&gt;            &lt;int&gt;
 1 BBR-b432021-M_fall_winter_4 -99.2  48.1 2022-11-09 19:09:01 migrating1   migrating            3
 2 BBR-b432021-M_fall_winter_4 -99.2  48.1 2022-11-09 21:08:36 migrating1   migrating            3
 3 BBR-b432021-M_fall_winter_4 -99.2  48.1 2022-11-09 23:08:55 migrating1   migrating            3
 4 BBR-b432021-M_fall_winter_4 -99.2  48.1 2022-11-10 01:09:11 migrating1   migrating            3
 5 BBR-b432021-M_fall_winter_4 -99.2  48.1 2022-11-10 03:08:50 migrating1   migrating            3
 6 BBR-b432021-M_fall_winter_4 -99.2  48.1 2022-11-10 05:09:06 migrating1   migrating            3
 7 BBR-b432021-M_fall_winter_4 -99.2  48.1 2022-11-10 07:08:43 migrating1   migrating            3
 8 BBR-b432021-M_fall_winter_4 -99.2  48.1 2022-11-10 09:08:54 migrating1   migrating            3
 9 BBR-b432021-M_fall_winter_4 -99.2  48.1 2022-11-10 11:09:07 migrating1   migrating            3
10 BBR-b432021-M_fall_winter_4 -99.2  48.1 2022-11-10 13:08:39 migrating1   migrating            3
11 BBR-b432021-M_fall_winter_4 -89.3  36.7 2022-12-13 23:08:30 winter1      winter               1
12 BBR-b432021-M_fall_winter_4 -89.3  36.7 2022-12-14 01:08:45 winter1      winter               1
13 BBR-b432021-M_fall_winter_4 -89.3  36.7 2022-12-14 03:08:45 winter1      winter               1
14 BBR-b432021-M_fall_winter_4 -89.3  36.7 2022-12-14 05:08:26 winter1      winter               1
15 BBR-b432021-M_fall_winter_4 -89.3  36.7 2022-12-14 07:08:22 winter1      winter               1
16 BBR-b432021-M_fall_winter_4 -89.3  36.7 2022-12-14 09:08:45 winter1      winter               1
17 BBR-b432021-M_fall_winter_4 -89.3  36.7 2022-12-14 11:08:54 winter1      winter               1
18 BBR-b432021-M_fall_winter_4 -89.3  36.7 2022-12-14 13:08:19 winter1      winter               1
19 BBR-b432021-M_fall_winter_4 -89.3  36.7 2022-12-14 15:08:47 winter1      winter               1
20 BBR-b432021-M_fall_winter_4 -89.4  36.7 2022-12-14 17:08:19 winter1      winter               1
</code></pre>
<p><code>stopover_sum</code> should be 1 (which is in the middle of the subsetted df)
. I'm not sure where the 3 is coming from. Trying to dissect the solution now.</p>
","<r><time-series><uniqueidentifier><data-management>","2023-02-09 19:42:13","51","2","2","75416860","<p>Again thank you @akrun. My question/intended output was a poorly worded; you're solution did exactly what I asked. I should have specified that I wanted to retain the entire dataset. To do so, I adjusted your solution with <code>mutate</code> instead of <code>transmute</code> and added duration columns, using the same <code>pivot_wider</code> procedure. I duplicated <code>class</code> and <code>event_duration</code> columns so I could retain them. Admittedly, quite clumsy but effective nonetheless. Thanks again. Below should be the final solution:</p>
<pre><code>df &lt;- df %&gt;%
  mutate(grp = data.table::rleid(class), 
         date = as.Date(ymd_hms(time))) %&gt;% 
  group_by(ID, class) %&gt;% 
  mutate(count = n_distinct(grp), 
         unique_class = str_c(class, match(grp, unique(grp)))) %&gt;% 
  group_by(ID, unique_class) %&gt;% 
  mutate(event_duration = difftime(max(time), min(time), units = &quot;days&quot;)) %&gt;% 
  ungroup() %&gt;% 
  mutate(class = str_c(class, '_sum')) %&gt;% 
  pivot_wider(names_from = class, values_from = count) %&gt;% 
  relocate(event_duration, .after = last_col()) %&gt;%
  mutate(class = str_sub(unique_class, start = 1, end = -2),
         class2 = class,
         event_duration2 = event_duration) %&gt;%
  pivot_wider(names_from = class2, values_from = event_duration2) %&gt;%
  mutate(mig_dur = migrating,
         stop_dur = stopover,
         winter_dur = winter) %&gt;%
  dplyr::select(-migrating, -stopover, -winter)

</code></pre>
"
"75402278","Standard definitions/schemas across developers for entities","<p>Is there a standard or a de-facto standard recognized among developers for definitions/schemas in data systems?</p>
<p>Example: &quot;username&quot;, often inconsistently referred to as &quot;login&quot;, &quot;nickname&quot;, &quot;user&quot;, category/metadata &quot;access data, personal data&quot;; other ex: &quot;retina scan&quot;, often inconsistently referred to as &quot;retina scan&quot;, &quot;iris scan&quot;, category/metadata &quot;biometric data, access data&quot;.</p>
<p>Is there a standard that provides standard definitions/schemas for such terms?</p>
<p>Looking forward very much to your answers.</p>
<p>I have looked at ISO-11179 so far. However, this standard only provides guidance on how to define the terms, but no definitions themselves.</p>
<p>ISO-8000 is similar and for master data only.</p>
<p>Microsoft's Common Data Model provides definitions to a certain extent, but the CDM only covers a certain part of all possible definitions. Moreover, it is questionable how widespread it actually is. The Microsoft Open Data Initiative does not seem to have worked properly.</p>
<p>It is similar with schema.org.</p>
<p>I wonder if there is such a list of common definitions or each company ultimately creates their own definitions in their own systems (CRM / ERP / Whatever).</p>
","<standards><crm><data-modeling><data-warehouse><erp>","2023-02-09 17:26:32","43","-1","1","75402790","<p>There is no global standard definition. There may be standards for specific industries, countries, ecosystems, etc but nothing that you could assume would always be accepted</p>
"
"75361222","Is there a way I can fast load data with SSIS?","<p>I'm moving data from ODBC to OLE Destination, records get inserted everyday on the ODBC in different tables. The packages gets slower and slower it take about a day for million records sometimes more. The tables can have new data inserted or new updated data and the loading and looking up of new data slows the processs. Is the anyway i can fast track the ETL process or is there any open source platform i can use to load the data faster</p>
<p>Tried to count the number of rows in the OLE Destination to check and only insert new records that are greater than the ones in the ODBC Source, but to my surprise the ROW_NUMBER() function isn't supported in Openedge ODBC</p>
","<sql-server><ssis><data-warehouse><openedge><progress-db>","2023-02-06 12:18:08","102","-3","2","75365533","<p>Based on the limited information in your question, I'd design your packages like the following</p>
<h1>SEQC PG to SQL</h1>
<p>The point of these operations is to transfer data from our source system verbatim to the target. The target table should be brand new and the SQL Server equivalent of the PG table from a data type perspective. Clustered Key if one exists, otherwise, see how a heap performs. I am going to reference this as a staging table.</p>
<p><a href=""https://i.stack.imgur.com/X3aaU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X3aaU.png"" alt=""enter image description here"" /></a></p>
<p>The Data Flow itself is going to be bang simple</p>
<p><a href=""https://i.stack.imgur.com/IZ75x.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IZ75x.png"" alt=""enter image description here"" /></a></p>
<p>By default, the destination will perform a fast load and lock the table.</p>
<p>Run the package and observe times.</p>
<p>Edit the OLE DB Destination and change the Maximum Commit Size to something less than 2147483647. Try 100000 - is it better, worse? Move up/down an order of magnitude until you have an idea of what it looks like will be the fastest the package can move data.</p>
<p>There are a ton of variables at this stage of the game - how busy is the source PG database, what are the data types involved, how far does the data need to travel from the Source, to your computer, to the Destination but this can at least help you understand &quot;can I pull <em>(insert large number here)</em> rows from the source system within the expected tolerance&quot;  If you can get the data moved from PG to SQL within the expected SLA and you still have processing time left, then move on to the next section.</p>
<p>Otherwise, you have to rethink your strategy for what data gets brought over. Maybe there's reliable (system generated) insert/update times associated to the rows. Maybe it's a financial-like system where rows aren't updated, just new versions of the row are insert and the net values are all that matters. Too many possibilities here but you'll likely need to find a Subject Matter Expert on the system - someone who knows the logical business process the database models as well as how the data is stored in the database. Buy that person some tasty snacks because they are worth their weight in gold.</p>
<h1>Now what?</h1>
<p>At this point, we have transferred the data from PG to SQL Server and we need to figure out what to do with it. 4 possibilities exist</p>
<ul>
<li>The data is brand new. We need to add the row into the target table</li>
<li>The data is unchanged. Do nothing</li>
<li>The data exists but is different. We need to change the existing row in the target table</li>
<li>There is data in the target table that isn't in the staging table. We're not going to do anything about this case either.</li>
</ul>
<p>Adding data, <code>insert</code>s, are easy and can be fast - it depends on table design.</p>
<p>Changing data, <code>update</code>s, are less easy in SSIS and are slower than adding new rows. Slower because behind the scenes, the database will delete and add the row back in.</p>
<p>Non-Clustered indexes are also potential bottlenecks here, but they can also be  beneficial. Welcome to the world of &quot;it depends&quot;</p>
<p>Option 1 is to just write the SQL statements to handle the insert and update. Yes, you have a lovely GUI tool for creating data flows but you need speed and this is how you get it (especially since we've already moved all the data from the external system to a central repository)</p>
<p>Option 2 is to use a Data Flow and potentially an Execute SQL Task to move the data. The idea being, the Data Flow will segment your data into New which will use an OLE DB Destination to write the inserts. The updates - it depends on volume what makes the most sense from an efficiency perspective. If it's tens, hundreds, thousands of rows to update, eh take the performance penalty and use an OLE DB Command to update the row. Maybe it's hundreds of thousands and the package runs good enough, then keep it.</p>
<p>Otherwise, route your changed rows to yet another staging table and then do a mass update from the staged updates to the target table. But at this point, you just wrote half the query you needed for the first option so just write the Insert and be done (and speed up performance because now everything is just SQL Engine &quot;stuff&quot;)</p>
"
"75361222","Is there a way I can fast load data with SSIS?","<p>I'm moving data from ODBC to OLE Destination, records get inserted everyday on the ODBC in different tables. The packages gets slower and slower it take about a day for million records sometimes more. The tables can have new data inserted or new updated data and the loading and looking up of new data slows the processs. Is the anyway i can fast track the ETL process or is there any open source platform i can use to load the data faster</p>
<p>Tried to count the number of rows in the OLE Destination to check and only insert new records that are greater than the ones in the ODBC Source, but to my surprise the ROW_NUMBER() function isn't supported in Openedge ODBC</p>
","<sql-server><ssis><data-warehouse><openedge><progress-db>","2023-02-06 12:18:08","102","-3","2","75377126","<p>You might want to investigate Progress' Change Data Capture feature. If you have a modern release of OpenEdge (11.7 or better) and the proper licenses you can enable CDC policies to track changes. Your ETL process could then use that information to target its efforts.</p>
<p>Warning: it's complicated. There is a lot more to actually doing it than marketing would have you believe. But if your use-case is straight-forward it might not be too terrible.</p>
<p>Or you could implement Progress &quot;Pro2&quot; product to do all the dirty work for you. (That's an extra cost option.)</p>
"
"75333301","Should I create new table in data mart or use Power Query for each new dashboard?","<p>I am working on a PowerBI report that consists of multiple dashboards. The data needed is from a single table with 100K rows of data in DWH . The table stores all the variables and values for different stores, as shown in the picture below.</p>
<p>Currently, we are creating new table in data mart for each separate dashboard, such as total profit in each country, total number of staff in each country etc. However, I realize I can do the same using Power Query without adding new tables for my data mart. So I am curious which approach is better?</p>
<p>And this leads to another question I always have, when we need a tranformed table for dashboard, shoud we create new tables in data mart, or should we do it in the BI tool such as PBI or Tableau? I think performance is a factor to be considered, but not sure about the other factors.</p>
<p>Appreciate if anyone can share your opinion.</p>
<p><img src=""https://i.stack.imgur.com/Lpxsh.png"" alt=""1"" /></p>
<p>Appreciate if anyone can share your opinion.</p>
","<database><powerbi><data-warehouse>","2023-02-03 08:40:43","45","0","1","75337633","<p>Given the amount of transformation that needs to occur, it would be worth doing this in the DWH.  Power BI does well with a star schema, so it would be good to break out dimensions like country, store and date into their own tables.</p>
<p>You might also work the measures into a single fact table - or maybe two if some of the facts are transactional and others are semi-additive snapshot facts.  i.e. profit vs. number of staff.  Designed right, the model could support all of the dashboards, so you would not need a report table for each.</p>
"
"75333015","How to detach or remove a data catalog tag from a BigQuery table using gcloud command","<p>Could anyone please share ETL tag template in GCP data catalog?
I'd like to refresh a tag value with its ETL status every time a BigQuery table is updated. I'm trying to use gcloud commands to create a tag template. I need to remove the tag from the template using the gcloud command and add another tag to that template, so that I can maintain the ETL status through automation.</p>
<p>I am able to remove the tag through UI manually. I need corresponding gcloud command for the same.</p>
","<google-bigquery><tags><google-data-catalog>","2023-02-03 08:10:11","73","0","1","75449659","<p>The procedure is explained in the <a href=""https://cloud.google.com/data-catalog/docs/tag-bigquery-dataset#create_a_public_tag_template_and_attach_a_tag_for_your_entry"" rel=""nofollow noreferrer"">Data Catalog documentation</a>:</p>
<ol>
<li><p>Suppose that you have the following tag template created:</p>
<pre><code>gcloud data-catalog tag-templates create demo_template \
 --location=us-central1 \
 --display-name=&quot;Demo Tag Template&quot; \
 --field=id=source,display-name=&quot;Source of data asset&quot;,type=string,required=TRUE \
 --field=id=num_rows,display-name=&quot;Number of rows in the data asset&quot;,type=double \
 --field=id=has_pii,display-name=&quot;Has PII&quot;,type=bool \
 --field=id=pii_type,display-name=&quot;PII type&quot;,type='enum(EMAIL_ADDRESS|US_SOCIAL_SECURITY_NUMBER|NONE)'
</code></pre>
</li>
<li><p>You need to lookup for Data Catalog entry created for your BigQuery table:</p>
<pre><code>ENTRY_NAME=$(gcloud data-catalog entries lookup '//bigquery.googleapis.com/projects/PROJECT_ID/datasets/DATASET/tables/TABLE' --format=&quot;value(name)&quot;)
</code></pre>
</li>
<li><p>Once you have entry name you can:</p>
<ul>
<li><p><a href=""https://cloud.google.com/sdk/gcloud/reference/data-catalog/tags/create"" rel=""nofollow noreferrer"">Create</a> a tag if it wasn't created on the entry or if it was deleted earlier:</p>
<pre><code>cat &gt; tag_file.json &lt;&lt; EOF
{
  &quot;source&quot;: &quot;BigQuery&quot;,
  &quot;num_rows&quot;: 1000,
  &quot;has_pii&quot;: true,
  &quot;pii_type&quot;: &quot;EMAIL_ADDRESS&quot;
}
EOF

gcloud data-catalog tags create --entry=${ENTRY_NAME} --tag-template=demo_template --tag-template-location=us-central1 --tag-file=tag_file.json
</code></pre>
<p>The command returns (among others) tag name that can be used with update or delete.</p>
</li>
<li><p><a href=""https://cloud.google.com/sdk/gcloud/reference/data-catalog/tags/delete"" rel=""nofollow noreferrer"">Delete</a> a tag from entry:</p>
<pre><code>gcloud data-catalog tags delete TAG_NAME
</code></pre>
</li>
<li><p><a href=""https://cloud.google.com/sdk/gcloud/reference/data-catalog/tags/update"" rel=""nofollow noreferrer"">Update</a> a tag, so that you don't have to delete it and recreate:</p>
<pre><code>gcloud data-catalog tags update --entry=${ENTRY_NAME} --tag-template=demo_template --tag-template-location=us-central1 --tag-file=tag_file.json
</code></pre>
</li>
<li><p>If you lost your tag name use <a href=""https://cloud.google.com/sdk/gcloud/reference/data-catalog/tags/list"" rel=""nofollow noreferrer"">list command</a>:</p>
<pre><code>gcloud data-catalog tags list --entry=${ENTRY_NAME}
</code></pre>
</li>
</ul>
</li>
</ol>
"
"75331098","How do you access DVC remote storage to view the file content?","<p>I am very new to DVC and I encounter a few problems with remote storage. I stored my data into dvc remote storage here (<code>.dvc/config</code> file):</p>
<pre class=""lang-ini prettyprint-override""><code>[core]
    remote = dvc-remote
['remote &quot;dvc-remote&quot;']
    url = /tmp/dvc-storage
</code></pre>
<p>Questions:</p>
<ol>
<li><p>Where can I access it in my file explorer? Or is there any way to check for the content inside without <code>dvc pull</code>?</p>
</li>
<li><p>I first store a data name <code>dataset</code> into this remote storage, after I retrieve and deleted some pictures in the <code>dataset</code> and push it back into the storage, is my original <code>dataset</code> overwrite or <code>dataset</code> files are kept?</p>
</li>
<li><p>I only <code>dvc add</code> the <code>dataset</code> file into dvc remote, why is it that on Iterative Studio the path of my other files changed to <code>/tmp/dvc-storage/d5/df97ac43b0</code> as well?</p>
</li>
</ol>
","<data-management><mlops><dvc>","2023-02-03 03:05:23","181","1","1","75332435","<p>To recap, your DVC project's default remote found in a local directory (<code>/tmp/dvc-storage</code>). OK</p>
<ol>
<li><p>All your data files are in <code>/tmp/dvc-storage</code> so that's where you could point your file explorer to, but this type* of DVC remote (local directory) is not meant for direct human handling. They're been renamed and reorganized in the <a href=""https://dvc.org/doc/user-guide/project-structure/internal-files#structure-of-the-cache-directory"" rel=""nofollow noreferrer"">same way as the project cache</a>.<br />
Basically, the directory structure (let's call it space dimension) AND data versions (time dimension) are flattened into a content-addressable data store. This is why you see all those 2 letter directories containing long hex file names (similar to <a href=""https://git-scm.com/book/en/v2/Git-Internals-Git-Objects"" rel=""nofollow noreferrer"">Git Object Storage</a>).</p>
</li>
<li><p>By default nothing is deleted from the cache (or remote storage) during regular <code>dvc</code> operations. The data store is append-only for the most part. This way you can <code>git checkout</code> and <code>dvc checkout</code> (or <code>dvc pull</code>) the data for a previous project version (past Git repo commit).<br />
You'd have to specifically garbage collect certain data from cache or storage locations using <code>dvc gc</code>, and even then it's designed to try preserving stuff you might need in the future.</p>
</li>
<li><p><em>Note that <code>dvc add</code> does not affect remote storage, it only works with the local cache. You need to <code>dvc push</code> and <code>dvc pull</code> to sync the data cache with a DVC remote.</em><br />
Wrt the Studio UI, I'm not sure where you see that path but its correct (as its hopefully clearer now). You'd get the same from <code>dvc get --show-url</code>, so maybe reading <a href=""https://dvc.org/doc/command-reference/get"" rel=""nofollow noreferrer"">that reference</a> helps.</p>
</li>
</ol>
<blockquote>
<p>* Note that DVC remotes can integrate with <a href=""https://dvc.org/doc/user-guide/data-management/cloud-versioning"" rel=""nofollow noreferrer"">cloud versioning</a> on Amazon S3, Azure Blob Storage, and Google Cloud Storage (probably more in the future). This means that if you use those types and enable this feature, you'll see the same directory structure as in your project folder (not the obfuscated cache structure). Cloud-versioned remotes are easier to handle directly (although it may also not be ideal).</p>
</blockquote>
"
"75326449","Am I modelling my warehouse tables the right way?","<p>I'm designing a website where users answer surveys. I need to design a data warehouse to aggregate their responses. So far in my model I have:</p>
<ol>
<li>A dim table for Users.</li>
<li>A dim table for Questions.</li>
<li>A fact table for UserResponses. &lt;= This is where I'm having the problem.</li>
</ol>
<p>So the problem I have is that additional comments can be added to their responses. For example, somebody may come in and make 2 comments against a single response. How should I model this in the database?</p>
<p>I was thinking of creating another fact table for &quot;Comments&quot;, and linking it to a record in UserResponses. Is this the right thing to do? This additional table would have something like the below columns:</p>
<ol>
<li>CommentText</li>
<li>Foreign key relationship to fact.UserResponses.</li>
</ol>
","<database><data-modeling><data-warehouse>","2023-02-02 16:52:54","39","1","2","75326507","<p>Yes, your idea to create another table is correct. I would typically call it a &quot;child&quot; table rather than calling it another fact table.</p>
<p>The key thing that you didn't mention is that the table <code>comments</code> still needs an ID field. A table without an ID would be bad design (although it is indeed <em>possible</em> to create the table with no ID) since you would have no simple way to refer to individual comments.</p>
"
"75326449","Am I modelling my warehouse tables the right way?","<p>I'm designing a website where users answer surveys. I need to design a data warehouse to aggregate their responses. So far in my model I have:</p>
<ol>
<li>A dim table for Users.</li>
<li>A dim table for Questions.</li>
<li>A fact table for UserResponses. &lt;= This is where I'm having the problem.</li>
</ol>
<p>So the problem I have is that additional comments can be added to their responses. For example, somebody may come in and make 2 comments against a single response. How should I model this in the database?</p>
<p>I was thinking of creating another fact table for &quot;Comments&quot;, and linking it to a record in UserResponses. Is this the right thing to do? This additional table would have something like the below columns:</p>
<ol>
<li>CommentText</li>
<li>Foreign key relationship to fact.UserResponses.</li>
</ol>
","<database><data-modeling><data-warehouse>","2023-02-02 16:52:54","39","1","2","75326862","<p>In a dimension model, fact tables are never linked to each other, as the grain of the data will be compromised.</p>
<p>The back-end database of a client application is not usually a data warehouse schema, but more of an online transactional processing (OLTP) schema. This is because transactional systems work better with <a href=""https://www.1keydata.com/database-normalization/third-normal-form-3nf.php"" rel=""nofollow noreferrer"">third normal form</a>. Analytical systems work better with dimensional models because the data can be aggregated (i.e., &quot;sliced and diced&quot;) more easily.</p>
<p>I would recommend switching back to an OLTP database. It can still be aggregated when needed, but maintains third normal form for easier transactional processing.</p>
<p>Here is a good comparison between a dimensional model (OLAP) and a transactional system (OLTP):</p>
<p><a href=""https://www.guru99.com/oltp-vs-olap.html"" rel=""nofollow noreferrer"">https://www.guru99.com/oltp-vs-olap.html</a></p>
"
"75281472","How to join a fact table to a dimension table which has a duplicate key value while avoiding duplications in fact table?","<p>How do I join a fact table to a dimension table with a duplicate key value, while at the same time avoiding duplication in the fact table that would result from the join?</p>
<p>Dimension table: <a href=""https://i.stack.imgur.com/j7Y8Z.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>fact table: <a href=""https://i.stack.imgur.com/sS7gb.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>product look-up table (another dimension table): <a href=""https://i.stack.imgur.com/fGtmG.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I thought of using the activation date as the next unique value, but they share a month in common.</p>
<p>I thought of creating a snowflake schema which connects dimension table in question (marketing campaigns) to product dimension which in turn connects to the fact table with no issues.</p>
<p>edit:</p>
<p>I am designing a datawarehouse which should answer how effective marketing campaigns based on purchase data.</p>
<p>Purchase data which will be the core of my fact table looks like this:</p>
<p>product_id  timestamp sales_price user_id
1           5/9/2015   120          124
2           6/9/2015   150          129</p>
<p>the product lookup table looks like this:</p>
<p>id  product_name  model   production_cost
6    ring         2019     300
5    headband     2018     200</p>
<p>the marketing campaigns look up table looks like this:</p>
<p>startdate  enddate    type         amount_spent    currency product_id
1/1/2019   7/1/2019   print         100,000         USD      6
6/1/2019   1/1/2020   socialmedia   10,000,000      USD      6
6/1/2019   1/1/2020   socialmedia   10,000,000      USD      3</p>
<p>The issue is that the marketing table has duplicate product id value of 6. So, when I use it as my natural key to create a surrogate primary key for that dimension table and pull that surrogate key to the fact table as a foreign key it's going to cause duplications for anything with product_id of 6 (as it's not unique). How do I connect marketing campaigns data to fact table, whilst keeping the data integrity intact -- that is no duplications?</p>
<p>I thought about combining start/end date with product_id to create a composite primary key, but they share/overlap a month (6/1/2019 to 7/1/2019)</p>
<p>I also thought about connecting the purchases (fact table) to product lookup and then product to marketing campaigns (a snowflake schema) to avoid the duplication.</p>
","<sql><database-design><data-warehouse>","2023-01-30 07:49:47","98","-1","1","75282402","<p>I suggest you take the time to read the details of dimensional database design.</p>
<p>If you mean dimensional design, there is no such thing as a lookup table there; there is either a Slowly Changing Dimension (SCD), or just a Dimension. Your product lookup table could be a product lookup dimension. Your Dimension table looks imperfect, too: It does contain the element of time , but not correctly. You need - usually in this order:</p>
<ul>
<li>a completely arbitrary integer as a surrogate, primary, key - often populated by a sequence or defined as IDENTITY</li>
<li>a business identifier - that could be the SKU for a product, first part of a business unique identifier</li>
<li>the valid-from-date, second part of a business unique identifier</li>
<li>the valid-to-date, '9999-12-31' for the current row, or equal to the valid-from-date of its successor</li>
<li>Type 1 attributes, those that don't change over time</li>
<li>Type 2 attributes, those that change over time and need a new row every time they change</li>
</ul>
<p>There can be more columns: the Boolean current-indicator, and an inserted timestamp and an updated timestamp.</p>
<p>The fact table is populated from the source transactions, after the dimension table. For each transaction row, you join with the SCD table with the business identifier (SKU in our case), that must be equal and the transaction's timestamp, that must be greater or equal to the valid-from-date and less than the valid-to-date. You pick the surrogate key of the row found in the SCD to populate the fact table's foreign key.</p>
<p>This is an exemplary, minimal, customer SCD table, without the insert/change timestamps and without the current-indicator:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;"">c_key</th>
<th style=""text-align: right;"">c_id</th>
<th>c_from_dt</th>
<th>c_to_dt</th>
<th>c_fname</th>
<th>c_lname</th>
<th style=""text-align: right;"">c_loy_lvl</th>
<th style=""text-align: right;"">c_org_id</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">66459</td>
<td style=""text-align: right;"">1</td>
<td>2022-01-25</td>
<td>9999-12-31</td>
<td>Arthur</td>
<td>Dent</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">1</td>
</tr>
<tr>
<td style=""text-align: right;"">34168</td>
<td style=""text-align: right;"">2</td>
<td>2022-01-25</td>
<td>9999-12-31</td>
<td>Ford</td>
<td>Prefect</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">2</td>
</tr>
<tr>
<td style=""text-align: right;"">2284</td>
<td style=""text-align: right;"">3</td>
<td>2021-12-25</td>
<td>9999-12-31</td>
<td>Zaphod</td>
<td>Beeblebrox</td>
<td style=""text-align: right;"">3</td>
<td style=""text-align: right;"">3</td>
</tr>
<tr>
<td style=""text-align: right;"">84768</td>
<td style=""text-align: right;"">4</td>
<td>2021-12-25</td>
<td>9999-12-31</td>
<td>Tricia</td>
<td>McMillan</td>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;"">4</td>
</tr>
<tr>
<td style=""text-align: right;"">80080</td>
<td style=""text-align: right;"">5</td>
<td>2022-01-25</td>
<td>9999-12-31</td>
<td>Gag</td>
<td>Halfrunt</td>
<td style=""text-align: right;"">5</td>
<td style=""text-align: right;"">5</td>
</tr>
<tr>
<td style=""text-align: right;"">57458</td>
<td style=""text-align: right;"">6</td>
<td>2022-01-25</td>
<td>9999-12-31</td>
<td>Prostetnic Vogon</td>
<td>Jeltz</td>
<td style=""text-align: right;"">6</td>
<td style=""text-align: right;"">6</td>
</tr>
<tr>
<td style=""text-align: right;"">1076</td>
<td style=""text-align: right;"">7</td>
<td>2022-01-25</td>
<td>9999-12-31</td>
<td>Lionel</td>
<td>Prosser</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">0</td>
</tr>
<tr>
<td style=""text-align: right;"">9782</td>
<td style=""text-align: right;"">8</td>
<td>2021-12-25</td>
<td>9999-12-31</td>
<td>Benji</td>
<td>Mouse</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">1</td>
</tr>
<tr>
<td style=""text-align: right;"">42655</td>
<td style=""text-align: right;"">9</td>
<td>2021-12-25</td>
<td>9999-12-31</td>
<td>Frankie</td>
<td>Mouse</td>
<td style=""text-align: right;"">3</td>
<td style=""text-align: right;"">2</td>
</tr>
<tr>
<td style=""text-align: right;"">57348</td>
<td style=""text-align: right;"">10</td>
<td>2021-09-25</td>
<td>2021-10-25</td>
<td>Wonko</td>
<td>The Sane</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">3</td>
</tr>
<tr>
<td style=""text-align: right;"">22279</td>
<td style=""text-align: right;"">10</td>
<td>2021-10-25</td>
<td>2021-11-25</td>
<td>Wonko</td>
<td>The Sane</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">3</td>
</tr>
<tr>
<td style=""text-align: right;"">3675</td>
<td style=""text-align: right;"">10</td>
<td>2021-11-25</td>
<td>2021-12-25</td>
<td>Wonko</td>
<td>The Sane</td>
<td style=""text-align: right;"">3</td>
<td style=""text-align: right;"">3</td>
</tr>
<tr>
<td style=""text-align: right;"">95534</td>
<td style=""text-align: right;"">10</td>
<td>2021-12-25</td>
<td>2022-01-25</td>
<td>Wonko</td>
<td>The Sane</td>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;"">3</td>
</tr>
<tr>
<td style=""text-align: right;"">69529</td>
<td style=""text-align: right;"">10</td>
<td>2022-01-25</td>
<td>9999-12-31</td>
<td>Wonko</td>
<td>The Sane</td>
<td style=""text-align: right;"">5</td>
<td style=""text-align: right;"">3</td>
</tr>
<tr>
<td style=""text-align: right;"">34845</td>
<td style=""text-align: right;"">11</td>
<td>2022-01-25</td>
<td>9999-12-31</td>
<td>Eccentrica</td>
<td>Gallumbitis</td>
<td style=""text-align: right;"">6</td>
<td style=""text-align: right;"">4</td>
</tr>
</tbody>
</table>
</div>"
"75280714","Combine columns of data in excel into one column without changing sequence","<p>I know this question has been asked before but I couldn't follow the instructions from the previous post to make it work. I am trying to combine a few columns of data that look like <a href=""https://i.stack.imgur.com/pjDCQ.png"" rel=""nofollow noreferrer"">Table on the left is what I have trying to get to table on the right</a> the block of data on the left which I want to combine into one, unchanged row (the sequence is very important as it follows a time series). I am compiling data for all 50 states in a separate table where I want to paste this column into. The data runs from columns (A:AY). Help would be greatly appreciated cause I really don't know what I am doing(complete novice here, I have no experience with VBA). A proper breakdown of the process would be greatly appreciated.</p>
<p>I tried using the CONCATENATE funcion but keep running into an error, I found a few torubleshooting methods on google but they seem to interrupt the sequence of data. I tried using the tutorial from <a href=""https://stackoverflow.com/questions/66941583/transpose-multiple-rows-of-column-of-data-into-one-column"">This</a> but couldn't get it to work.</p>
","<excel><csv><data-management>","2023-01-30 05:58:56","65","0","1","75280879","<p>In case you don't have tocol, you can just as well use INDEX, like so.</p>
<p><strong>Table named &quot;data&quot;:</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>col1</th>
<th>col2</th>
<th>col3</th>
</tr>
</thead>
<tbody>
<tr>
<td>a</td>
<td>g</td>
<td>m</td>
</tr>
<tr>
<td>b</td>
<td>h</td>
<td>n</td>
</tr>
<tr>
<td>c</td>
<td>i</td>
<td>o</td>
</tr>
<tr>
<td>d</td>
<td>j</td>
<td>p</td>
</tr>
<tr>
<td>e</td>
<td>k</td>
<td>q</td>
</tr>
<tr>
<td>f</td>
<td>l</td>
<td>r</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Then this formula will put the data into a single column. Just put it anywhere in the first row, change &quot;data&quot; to your input range and expand down.</strong></p>
<pre><code>=INDEX(data,MOD(ROW()-1,ROWS(data))+1,CEILING.MATH(ROW()/ROWS(data)))
</code></pre>
<p>Or, if your data is very large and you don't want to expand manually, you can try this VBA approach.The only thing you need to change is the &quot;data&quot; and &quot;targetCell&quot; variables at the top.</p>
<p>&quot;data&quot; is your original table so in your example, it would be Range(&quot;B2:F12&quot;). &quot;targetCell&quot; is where the column will start. In your Example it's Range(&quot;BC2&quot;)</p>
<pre class=""lang-vba prettyprint-override""><code>
Sub singleColumn()
    Dim data As Range, targetCell As Range, targetRange As Range
    Set data = Range(&quot;data&quot;)
    Set targetCell = Range(&quot;G20&quot;)
    
    Dim resultArr() As Variant, arr As Variant
    ReDim resultArr(1 To data.Rows.Count * data.Columns.Count, 1 To 1)
    arr = data

    For i = 1 To UBound(resultArr)
        resultArr(i, 1) = arr(((i - 1) Mod UBound(arr)) + 1, WorksheetFunction.Ceiling_Math(i / UBound(arr)))
    Next i
    
    Set targetRange = Range(targetCell, Cells(targetCell.Row + UBound(resultArr) - 1, targetCell.Column))
    targetRange = resultArr
End Sub

</code></pre>
"
"75229394","Can we use Microsoft Purview and Unity Catalog together","<p><code>Unity Catalog</code> is the Azure Databricks data governance solution for the Lakehouse. Whereas, <code>Microsoft Purview</code> provides a unified data governance solution to help manage and govern your on-premises, multicloud, and software as a service (SaaS) data.</p>
<p><strong>Question</strong>: In our <code>same</code> Azure Cloud project, can we use <code>Unity Catalog</code> for the Azure Databricks Lakehouse, and use Microsoft Purview for the rest of our Azure project?</p>
<p><strong>Update</strong>: In our current Azure subscription, we have divided workload as follows:</p>
<ol>
<li><strong>SQL related workload</strong>: we are doing all our SQL database work using Databricks <code>only</code> (no Azure SQL databases are involved). That is, we are using Databricks Lakehouse, Delta Lake, Deatricks SQL etc. to perform <code>ETL</code> and all <code>Data Analytics work</code>.</li>
<li><strong>All Non-SQL workload</strong>: All other assets (Excel files, csv files, pdf, media files etc.) are stored in various Azure storage accounts.</li>
</ol>
<p>MS Purview is doing a good job in scanning assets in scenario 2 above, and it easily creates a holistic, up-to-date map of our data landscape with automated data discovery, sensitive data classification, and end-to-end data lineage. It also enables our data consumers to access valuable, trustworthy data management.</p>
<p>However, our almost 50% of the work (SQL, ETL, Data Analytics etc.) is done in Azure Databricks where we have significant challenges with Purview. We were wondering if it's possible to keep Purview and Unity Catalog separate as follows: Purview does its Data Governance work for scenario 1 only and Unity Catalog does its Data Governance work for scenario 2 only.</p>
<p>This recently released update may resolve our issue of making Purview work better with Azure Databricks but we have not tried it yet: <a href=""https://learn.microsoft.com/en-us/azure/purview/register-scan-azure-databricks"" rel=""nofollow noreferrer"">Connect to and manage Azure Databricks in Microsoft Purview (Preview)</a></p>
","<azure><azure-databricks><azure-purview><databricks-unity-catalog><data-governance>","2023-01-25 03:16:02","1692","2","3","75231419","<p>As of right now there is no official integration between Unity Catalog and Purview yet, but it may come in the future. You may join <a href=""https://www.databricks.com/p/webinar/productroadmapwebinar"" rel=""nofollow noreferrer"">Azure Databricks roadmap webinar</a> that will be tomorrow to get more information.</p>
<p>Regarding the actual question - imho, nothing prevents you from using UC &amp; Purview in the same Azure project.</p>
<p>P.S. You can get metadata &amp; lineage information into Purview by loading data from <a href=""https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/sql-ref-information-schema"" rel=""nofollow noreferrer"">information schema tables</a> and using Purview APIs to store it in Purview.</p>
"
"75229394","Can we use Microsoft Purview and Unity Catalog together","<p><code>Unity Catalog</code> is the Azure Databricks data governance solution for the Lakehouse. Whereas, <code>Microsoft Purview</code> provides a unified data governance solution to help manage and govern your on-premises, multicloud, and software as a service (SaaS) data.</p>
<p><strong>Question</strong>: In our <code>same</code> Azure Cloud project, can we use <code>Unity Catalog</code> for the Azure Databricks Lakehouse, and use Microsoft Purview for the rest of our Azure project?</p>
<p><strong>Update</strong>: In our current Azure subscription, we have divided workload as follows:</p>
<ol>
<li><strong>SQL related workload</strong>: we are doing all our SQL database work using Databricks <code>only</code> (no Azure SQL databases are involved). That is, we are using Databricks Lakehouse, Delta Lake, Deatricks SQL etc. to perform <code>ETL</code> and all <code>Data Analytics work</code>.</li>
<li><strong>All Non-SQL workload</strong>: All other assets (Excel files, csv files, pdf, media files etc.) are stored in various Azure storage accounts.</li>
</ol>
<p>MS Purview is doing a good job in scanning assets in scenario 2 above, and it easily creates a holistic, up-to-date map of our data landscape with automated data discovery, sensitive data classification, and end-to-end data lineage. It also enables our data consumers to access valuable, trustworthy data management.</p>
<p>However, our almost 50% of the work (SQL, ETL, Data Analytics etc.) is done in Azure Databricks where we have significant challenges with Purview. We were wondering if it's possible to keep Purview and Unity Catalog separate as follows: Purview does its Data Governance work for scenario 1 only and Unity Catalog does its Data Governance work for scenario 2 only.</p>
<p>This recently released update may resolve our issue of making Purview work better with Azure Databricks but we have not tried it yet: <a href=""https://learn.microsoft.com/en-us/azure/purview/register-scan-azure-databricks"" rel=""nofollow noreferrer"">Connect to and manage Azure Databricks in Microsoft Purview (Preview)</a></p>
","<azure><azure-databricks><azure-purview><databricks-unity-catalog><data-governance>","2023-01-25 03:16:02","1692","2","3","75536926","<p>The integration between Purview and UC is in private preview.</p>
"
"75229394","Can we use Microsoft Purview and Unity Catalog together","<p><code>Unity Catalog</code> is the Azure Databricks data governance solution for the Lakehouse. Whereas, <code>Microsoft Purview</code> provides a unified data governance solution to help manage and govern your on-premises, multicloud, and software as a service (SaaS) data.</p>
<p><strong>Question</strong>: In our <code>same</code> Azure Cloud project, can we use <code>Unity Catalog</code> for the Azure Databricks Lakehouse, and use Microsoft Purview for the rest of our Azure project?</p>
<p><strong>Update</strong>: In our current Azure subscription, we have divided workload as follows:</p>
<ol>
<li><strong>SQL related workload</strong>: we are doing all our SQL database work using Databricks <code>only</code> (no Azure SQL databases are involved). That is, we are using Databricks Lakehouse, Delta Lake, Deatricks SQL etc. to perform <code>ETL</code> and all <code>Data Analytics work</code>.</li>
<li><strong>All Non-SQL workload</strong>: All other assets (Excel files, csv files, pdf, media files etc.) are stored in various Azure storage accounts.</li>
</ol>
<p>MS Purview is doing a good job in scanning assets in scenario 2 above, and it easily creates a holistic, up-to-date map of our data landscape with automated data discovery, sensitive data classification, and end-to-end data lineage. It also enables our data consumers to access valuable, trustworthy data management.</p>
<p>However, our almost 50% of the work (SQL, ETL, Data Analytics etc.) is done in Azure Databricks where we have significant challenges with Purview. We were wondering if it's possible to keep Purview and Unity Catalog separate as follows: Purview does its Data Governance work for scenario 1 only and Unity Catalog does its Data Governance work for scenario 2 only.</p>
<p>This recently released update may resolve our issue of making Purview work better with Azure Databricks but we have not tried it yet: <a href=""https://learn.microsoft.com/en-us/azure/purview/register-scan-azure-databricks"" rel=""nofollow noreferrer"">Connect to and manage Azure Databricks in Microsoft Purview (Preview)</a></p>
","<azure><azure-databricks><azure-purview><databricks-unity-catalog><data-governance>","2023-01-25 03:16:02","1692","2","3","75899684","<p>Purview currently doesn't support scanning catalogs with a metastore attached.
I tried to set this up too, but I only get tables from the standard hive_metastore directory.</p>
<p>There is an Azure Databricks to Purview Lineage Connector.
You can check it out <a href=""https://github.com/microsoft/Purview-ADB-Lineage-Solution-Accelerator"" rel=""nofollow noreferrer"">here</a>.</p>
"
"75212789","SQL SCD2 weekly statistic grouped by shipping date","<p>I've been having one of those weeks...</p>
<p>I am creating a simple POC for the Director, who is looking for weekly stats of order values shipping.</p>
<p>The scenario they asked is &quot;At the end of each week (Sunday), I want to see the orders total ($), for the next 52 Weeks, based on the shipping date&quot;.</p>
<p>Standard SCD2 schemas, expanded below. Simple Date Dimension.</p>
<p>This is simple to answer in its basic form, DIMDATE.DATEKEY_SK = ORDERS.ORDERDATE_SK WHERE ORDERS.CURRENTRECORD ='Y' GROUP BY DIMDATE.WEEKEND. Which gives me all required DIMDATE attributes e.g. WeekStart, WeekEnd, MonthStart, MonthEnd etc etc.</p>
<p>However, the Director has now said he would like to see the previous 12 weeks as well, (-12/+52) in order to compare week on week trends.</p>
<p>Now i could insert all data from the &quot;simple&quot; query into a snapshot table each week which would solve the problem BUT i feel this is the cheats way out and there must be a way to do the same through T-SQL. This is where i am stuck because i have [EffectiveDate] and [EndDate] in my ORDERS table, along with the [OrderShipDate_SK].</p>
<p>Example Schemas Below:</p>
<p>DIMDATE</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>DATEKEY_SK</th>
<th>WeekStart</th>
<th>WeekEnd</th>
</tr>
</thead>
<tbody>
<tr>
<td>20230101</td>
<td>2023/01/01</td>
<td>2023/01/07</td>
</tr>
<tr>
<td>20250131</td>
<td>2023/01/29</td>
<td>2023/02/04</td>
</tr>
</tbody>
</table>
</div>
<p>ORDERS</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ORDERKEY_SK</th>
<th>EffectiveDate</th>
<th>EndDate</th>
<th>OrderId</th>
<th>OrderAmount</th>
<th>OrderShipDate_SK</th>
<th>CurrentRecord</th>
</tr>
</thead>
<tbody>
<tr>
<td>1234</td>
<td>2023/01/01</td>
<td>2023/01/03</td>
<td>IF76876</td>
<td>35000</td>
<td>20230107</td>
<td>N</td>
</tr>
<tr>
<td>1234</td>
<td>2023/01/04</td>
<td>2023/01/08</td>
<td>IF76876</td>
<td>37000</td>
<td>20230107</td>
<td>N</td>
</tr>
<tr>
<td>3345</td>
<td>2023/01/09</td>
<td>2999/12/31</td>
<td>IF76876</td>
<td>40000</td>
<td>20230114</td>
<td>Y</td>
</tr>
<tr>
<td>1432</td>
<td>2023/01/01</td>
<td>2023/01/12</td>
<td>IF89996</td>
<td>10000</td>
<td>20230107</td>
<td>N</td>
</tr>
<tr>
<td>5456</td>
<td>2023/01/13</td>
<td>2999/12/31</td>
<td>IF89996</td>
<td>12000</td>
<td>20230114</td>
<td>Y</td>
</tr>
<tr>
<td>1932</td>
<td>2023/01/01</td>
<td>2023/01/22</td>
<td>IF62336</td>
<td>10000</td>
<td>20230131</td>
<td>N</td>
</tr>
<tr>
<td>7456</td>
<td>2023/01/23</td>
<td>2999/12/31</td>
<td>IF62336</td>
<td>12000</td>
<td>20230218</td>
<td>Y</td>
</tr>
</tbody>
</table>
</div>
<p>What I'm after is like the below:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>WeekStart</th>
<th>WeekEnd</th>
<th>OrderTotal</th>
<th>ShippingWeekEnd (OrderShipDate)</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023/01/01</td>
<td>2023/01/07</td>
<td>47000</td>
<td>2023/01/07</td>
</tr>
<tr>
<td>2023/01/01</td>
<td>2023/01/07</td>
<td>0</td>
<td>2023/01/14</td>
</tr>
<tr>
<td>2023/01/01</td>
<td>2023/01/07</td>
<td>0</td>
<td>2023/01/21</td>
</tr>
<tr>
<td>2023/01/01</td>
<td>2023/01/07</td>
<td>0</td>
<td>2023/01/28</td>
</tr>
<tr>
<td>2023/01/01</td>
<td>2023/01/07</td>
<td>10000</td>
<td>2023/02/04</td>
</tr>
<tr>
<td>2023/01/08</td>
<td>2023/01/14</td>
<td>52000</td>
<td>2023/01/14</td>
</tr>
<tr>
<td>2023/01/08</td>
<td>2023/01/14</td>
<td>0</td>
<td>2023/01/21</td>
</tr>
<tr>
<td>2023/01/08</td>
<td>2023/01/14</td>
<td>0</td>
<td>2023/01/28</td>
</tr>
<tr>
<td>2023/01/08</td>
<td>2023/01/14</td>
<td>10000</td>
<td>2023/02/04</td>
</tr>
<tr>
<td>2023/01/15</td>
<td>2023/01/21</td>
<td>0</td>
<td>2023/01/21</td>
</tr>
<tr>
<td>2023/01/15</td>
<td>2023/01/21</td>
<td>0</td>
<td>2023/01/28</td>
</tr>
<tr>
<td>2023/01/15</td>
<td>2023/01/21</td>
<td>10000</td>
<td>2023/02/04</td>
</tr>
<tr>
<td>2023/01/15</td>
<td>2023/01/21</td>
<td>0</td>
<td>2023/02/11</td>
</tr>
<tr>
<td>2023/01/22</td>
<td>2023/01/28</td>
<td>12000</td>
<td>2023/01/28</td>
</tr>
<tr>
<td>2023/01/22</td>
<td>2023/01/28</td>
<td>0</td>
<td>2023/02/04</td>
</tr>
<tr>
<td>2023/01/22</td>
<td>2023/01/28</td>
<td>0</td>
<td>2023/02/11</td>
</tr>
<tr>
<td>2023/01/22</td>
<td>2023/01/28</td>
<td>12000</td>
<td>2023/02/18</td>
</tr>
</tbody>
</table>
</div>
<p>I want to show (64) rows for each week start/end with the sum of orders, grouped by the shipping week end (always Saturdays) E.g. If the order changes mid week only the latest record during that week is factored in at the end of the week.</p>
<p>I just cant figure out how i need to construct the query to do this. Existing Query Below</p>
<pre><code>SELECT SUM(OrderTotal),DIMDATE.WeekEnd
FROM ORDERS
LEFT JOIN DIMDATE ON DIMDATE.DATEKEY_SK = ORDERS.OrderShipDate_SK
WHERE ORDERS.CurrentRecord = 'Y'
GROUP BY DIMDATE.WeekEnd DESC
</code></pre>
","<sql><tsql><data-warehouse><dimensional-modeling><scd>","2023-01-23 17:07:36","74","0","1","75222977","<p>I think i have cracked it after sleeping on it.</p>
<p>First use the date dimensions to list all possible week start combinations, then do some extra calculations to generate some other dates.</p>
<pre><code>WITH
DATES AS (
SELECT distinct
DDATE.FirstOfWeek as 'WeekStart',
CAST(REPLACE(DDATE.FirstOfWeek,'-','')as int) AS 'skWeekstart', -- Want this as int for joins
DDATE.LastOfWeek,
CAST(REPLACE(DDATE.LastOfWeek,'-','')as int) AS 'skWeekend', -- Want this as int for joins
CAST(DATEADD(MONTH, DATEDIFF(MONTH, 0, FirstOfWeek)-1, 0) as date) AS LastMonth,
CAST(REPLACE(CAST(DATEADD(month, DATEDIFF(month, 0, FirstOfWeek), 0) as date),'-','') as int) AS MonthStart,
CAST(REPLACE(EOMONTH(FirstOfWeek),'-','') as int) as MonthEnd
FROM vw_dim_date DDATE
WHERE EOMONTH(FirstOfWeek) &gt; '2021-01-01' AND LastOfWeek &lt;= DATEADD(wk, DATEDIFF(wk, 6, CURRENT_TIMESTAMP), 6 + 7)  --- No point looking into the future as it hasnt happened yet.
),
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>WeekStart</th>
<th>skWeekstart</th>
<th>LastOfWeek</th>
<th>skWeekend</th>
<th>LastMonth</th>
<th>MonthStart</th>
<th>MonthEnd</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021-01-03</td>
<td>20210103</td>
<td>2021-01-09</td>
<td>20210109</td>
<td>2020-12-01</td>
<td>20210101</td>
<td>20210131</td>
</tr>
<tr>
<td>2021-01-10</td>
<td>20210110</td>
<td>2021-01-16</td>
<td>20210116</td>
<td>2020-12-01</td>
<td>20210101</td>
<td>20210131</td>
</tr>
<tr>
<td>2021-01-17</td>
<td>20210117</td>
<td>2021-01-23</td>
<td>20210123</td>
<td>2020-12-01</td>
<td>20210101</td>
<td>20210131</td>
</tr>
<tr>
<td>2021-01-24</td>
<td>20210124</td>
<td>2021-01-30</td>
<td>20210130</td>
<td>2020-12-01</td>
<td>20210101</td>
<td>20210131</td>
</tr>
<tr>
<td>2021-01-31</td>
<td>20210131</td>
<td>2021-02-06</td>
<td>20210206</td>
<td>2020-12-01</td>
<td>20210101</td>
<td>20210131</td>
</tr>
</tbody>
</table>
</div>
<p>Now get all the surrogate keys for the ORDERS and the effective dates of those records.</p>
<pre><code>
--- Get all the surrogate keys for ORDERS and the effective dates. Only looking for non deleted records and those with a shipping date 
ORDERS AS (
SELECT 
skOrder,orderid,
CAST(REPLACE(EffectiveDate,'-','') as int) as skeffdate,
CAST(REPLACE(EndDate,'-','') as int) as skenddate
FROM vw_dim_Orders
WHERE IsDeleted =0 AND skShippingDate &lt;&gt;-1
),
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>skOrder</th>
<th>OrderId</th>
<th>skeffdate</th>
<th>skenddate</th>
</tr>
</thead>
<tbody>
<tr>
<td>12005892</td>
<td>IG67787</td>
<td>20220325</td>
<td>20220710</td>
</tr>
<tr>
<td>12017996</td>
<td>IG577872</td>
<td>20220711</td>
<td>99991231</td>
</tr>
<tr>
<td>12000030</td>
<td>IH66798</td>
<td>20211110</td>
<td>20220324</td>
</tr>
</tbody>
</table>
</div>
<p>Join back to the DATES CTE and group by Date attributes. Join is using skeffectivedate &lt; skweekend which works because im selecting the latest surrogate key for that record for that week.</p>
<pre><code>--- Now join back into the weeks to get a list of ORDERS for each week.
--- 1) Join on ORDER effective date &lt; dates current weekend
--- 2) Get the MAX surrogate Key for that ORDER for that week. This ensures the latest record for that week is taken. E.g. 2 + changes, only the last change for that week is taken.
ORDERSKS AS (
SELECT DATES.*,
MAX(skOrders) AS skOrders
FROM DATES
LEFT JOIN ORDERS ON (skeffdate &lt; skWeekend)
GROUP BY DATES.WeekStart,DATES.skWeekstart,DATES.LastOfWeek,DATES.LastMonth,DATES.MonthStart,DATES.MonthEnd,skWeekend,orderId
)
</code></pre>
<p>This provides a list of ORDER surrogate keys which were valid on that week. Natuarlly there are millions of rows returned, but the results are narrow so it runs in seconds.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>WeekStart</th>
<th>skWeekstart</th>
<th>LastOfWeek</th>
<th>skWeekend</th>
<th>LastMonth</th>
<th>MonthStart</th>
<th>MonthEnd</th>
<th>skOrder</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023-01-22</td>
<td>20230122</td>
<td>2023-01-28</td>
<td>20230128</td>
<td>2022-12-01</td>
<td>20230101</td>
<td>20230131</td>
<td>12011381</td>
</tr>
<tr>
<td>2023-01-22</td>
<td>20230122</td>
<td>2023-01-28</td>
<td>20230128</td>
<td>2022-12-01</td>
<td>20230101</td>
<td>20230131</td>
<td>12028584</td>
</tr>
<tr>
<td>2023-01-22</td>
<td>20230122</td>
<td>2023-01-28</td>
<td>20230128</td>
<td>2022-12-01</td>
<td>20230101</td>
<td>20230131</td>
<td>12014886</td>
</tr>
<tr>
<td>2023-01-22</td>
<td>20230122</td>
<td>2023-01-28</td>
<td>20230128</td>
<td>2022-12-01</td>
<td>20230101</td>
<td>20230131</td>
<td>12011382</td>
</tr>
</tbody>
</table>
</div>
<p>With this i can join back to the dimension on the SK to get the shipping dates and order totals and tally from there.</p>
<p>It may not be the prettiest but it gives me every single order record for each week, factoring in if the record was updated mid week. This is really only for history to be able to look back 12 weeks, but i can look back as far as i have data now.</p>
"
"75161670","DWH primary key conflict between staging tables and DWH tables","<p>I am building a DWH based on data I am collecting from an ERP API.
currently, I am fetching the data from the API based on an incremental mechanism I built using python: The python script fetches all invoices whose last modified date is in the last 24 hours and inserts the data into a &quot;staging table&quot; (no changes are required during this step).</p>
<p>The next step is to insert all data from the staging area into the &quot;final tables&quot;. The final tables include primary keys according to the ERP (for example invoice number).
There are no primary keys defined at the staging tables.
For now, I am putting aside the data manipulation and transformation.</p>
<p>In some cases, it's possible that a specific invoice is already in the &quot;final tables&quot;, but then the user updates the invoice at the ERP system which causes the python script to fetch the data again from the API into the staging tables. In the case when I try to insert the invoice into the &quot;final table&quot; I will get a conflict due to the primary key restriction at the &quot;final tables&quot;.</p>
<p>Any idea of how to solve this?
I am thinking to add a field that details the date and timestamp at which the record land at the staging table (&quot;insert date&quot;) and then upsert the records if
insert date at the staging table &gt; insert date at the final tables</p>
<p>Is this best practice?
Any other suggestions? maybe use a specific tool/data solution?
I prefer using python scripts since it is part of a wider project.</p>
<p>Thank you!</p>
","<python><etl><data-warehouse>","2023-01-18 15:36:36","19","0","1","75196474","<p>Instead of a straight INSERT use an UPSERT pattern.  Either the MERGE statement if your database has it, or UPDATE the existing rows, followed by INSERTing the new ones.</p>
"
"75145342","Does kedro data catalog accept .arrow files?","<p>While using <a href=""https://kedro.readthedocs.io/en/stable/index.html"" rel=""nofollow noreferrer"">Kedro</a> I want to load some data and work with it. To do that, one has to register the data in a <em>conf/base/catalog.yml</em> file.
The <a href=""https://www.stackoverflow.com/"">Kedro Documentation of the Data Catalog</a> explains how one can register data for Kedro to load. However, there is little to no information on how to load a <a href=""https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/"" rel=""nofollow noreferrer"">.arrow</a> file.</p>
<p>In the <em>conf/base/catalog.yml</em> I tried to register my data thus:</p>
<pre><code>dataframe:
  type: arrow.ArrowDataSet
  filepath: &quot;home/place/data.arrow&quot;
  layer : primary
</code></pre>
<p>And ofcourse tried on different combinations from the data catalog documentation mentioned above. <br>
The error code I get is the following :<br>
<code>DataSetError: An exception occurred when parsing config for DataSet 'dataframe': Class 'arrow.ArrowDataSet' not found or one of its dependencies has not been installed.</code></p>
<p>I have ofcourse installed the arrow package in my environment.</p>
<p>Does the Kedro Data Catalog simply not accept .arrow files or is there a way to register such a format in the catalog.yml file?</p>
<p>Thanks in advance,</p>
<p>Jamal</p>
","<yaml><pyarrow><apache-arrow><kedro><arrow-python>","2023-01-17 10:55:29","50","0","1","75422597","<p>Like said @0x26res, you can use the parquet dataset or others that <a href=""https://kedro.readthedocs.io/en/stable/kedro.extras.datasets.html"" rel=""nofollow noreferrer"">kedro supports</a>. Parquet could be handled in kedro by pyarrow engine because under the hood is <a href=""https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html"" rel=""nofollow noreferrer"">pandas read_parquet</a> with 2 engines and pyarrow by default.</p>
<p>It may be necessary to <a href=""https://kedro.readthedocs.io/en/stable/kedro_project_setup/dependencies.html#install-dependencies-at-a-type-level"" rel=""nofollow noreferrer"">install dependencies to use other dataset types</a>:</p>
<pre><code>pip install kedro[pandas.ParquetDataSet]
</code></pre>
"
"75143359","Invoice header and invoice line fact table extraction question","<p>I have a question regarding the best practice regarding the loading of invoice header and  line data inside InvoiceLineFact table. I am following the Kimball recommendation where I bring all the dimensionality of the header down to the line items like shown on the picture :</p>
<p><a href=""https://i.stack.imgur.com/8VPQt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8VPQt.png"" alt=""HeaderLineFact table example"" /></a></p>
<p>For data extraction part we use store procedures created on our DWH SQL Server which are automatically executed via job in sequential order. What we do in every procedure is that we target the transaction tables on remote/linked server (DynamicsNAV db sql server):</p>
<p>Example:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT *
FROM LinkedServer.NAVDB.dbo.InvoiceHeader
INNER JOIN LinkedServer.NAVDB.dbo.InvoiceLine
</code></pre>
<p>The problem is that the our ETL is started to slow down from day to day.</p>
<p>I wanted to ask if this is the efficient thing to do or there is a better way of doing this, like extracting the raw data in separate table and then join it on the DWH server, or maybe placing it inside SSIS package as it is.</p>
","<sql-server><data-warehouse><sql-data-warehouse>","2023-01-17 07:56:32","51","0","1","75151760","<blockquote>
<p>if this is the efficient thing to do</p>
</blockquote>
<p>No.  Joining distributed tables and sending complex queries to operational systems are both problematic.</p>
<blockquote>
<p>or there is a better way of doing this,</p>
</blockquote>
<p>Yes</p>
<blockquote>
<p>like extracting the raw data in separate table and then join it on the DWH server</p>
</blockquote>
<p>Yes.  This is standard best-practice for building data warehouses.</p>
"
"75137470","Can I attach migrations to a corresponding data warehouse model using SQLAlchemy and Alembic?","<p>Let's say I want to build a data warehouse using these two tools. I was thinking of something like</p>
<pre><code>root
- database_schema
-- table1.py (SQLAlchemy model)
-- table2.py
...
- database_schema2
-- table1.py
...
- alembic
</code></pre>
<p>However, alembic is creating all migrations in one folder (versions). Is it possible to &quot;attach&quot; migrations to the corresponding model? So e.g. I would have something like this:</p>
<pre><code>root
- database_schema
-- table1
--- migrations
---- migration1.py (Alembic migration)
--- table1.py (SQLAlchemy model)
-- table2
--- migrations
--- table2.py
...
- database_schema2
-- table1.py
...
- alembic
</code></pre>
","<python><sqlalchemy><data-warehouse><alembic>","2023-01-16 17:08:32","50","0","1","75139137","<p>It seems there is support for something like this but it looks meant more for large submodules:
<a href=""https://alembic.sqlalchemy.org/en/latest/branches.html#working-with-multiple-bases"" rel=""nofollow noreferrer"">https://alembic.sqlalchemy.org/en/latest/branches.html#working-with-multiple-bases</a></p>
<p>Also I wouldn't mix the migrations in with your app code like that.  You might want to run different linters/formatters or have other scanning tools that are going to keep getting hung up on the migration files.  I would keep them under version control but in a directory parallel to your source code.  Also sometimes migrations involve more than 1 table and you'd have this weird ambiguity if you are trying to make them 1-to-1.</p>
<p>You could also apply some crude labeling to the suffix of the revisions, ie. <code>versions/{revision_id}_table1.py</code>.  Just do <code>alembic revision -m table1 --autogenerate</code>.  I think you can make that suffix whatever you want, and still set the message if you wanted to.</p>
"
"75131262","great expectation with delta table","<p>I am trying to run a great expectation suite on a delta table in Databricks. But I would want to run this on part of the table with a query. Though the validation is running fine, it's running on full table data.</p>
<p>I know that I can load a Dataframe and pass it to Batch Request but I would like to load the data directly with query.</p>
<pre><code>batch_request = RuntimeBatchRequest(
    datasource_name=&quot;datasource&quot;,
    data_connector_name=&quot;data_quality_run&quot;,
    data_asset_name=&quot;Input Data&quot;,
    runtime_parameters={&quot;path&quot;: &quot;/delta table path&quot;},
    batch_identifiers={&quot;data_quality_check&quot;: f&quot;data_quality_check_{datetime.date.today().strftime('%Y%m%d')}&quot;},
    batch_spec_passthrough={&quot;reader_method&quot;: &quot;delta&quot;, &quot;reader_options&quot;: {&quot;header&quot;: True}, &quot;query&quot; : {&quot;name&quot;:&quot;John&quot;}},
)

</code></pre>
<p>Above batch request loading the data ignoring the query option. Is there any way to pass the query for delta table in the batch request</p>
","<databricks><delta-lake><great-expectations><data-quality>","2023-01-16 07:27:39","149","0","1","75259725","<p>You can try to put <code>query</code> inside of <code>runtime_parameters</code>.</p>
<p>This works for me when I am querying data in SQL Server:</p>
<pre class=""lang-py prettyprint-override""><code>batch_request = RuntimeBatchRequest(
    datasource_name=&quot;my_mssql_datasource&quot;,
    data_connector_name=&quot;default_runtime_data_connector_name&quot;,
    data_asset_name=&quot;default_name&quot;,
    runtime_parameters={
        &quot;query&quot;: &quot;SELECT * from dbo.MyTable WHERE Created = GETDATE()&quot;
    },
    batch_identifiers={&quot;default_identifier_name&quot;: &quot;default_identifier&quot;},
)
</code></pre>
"
"75103052","How to stop rendering react component every data file while mapping","<p>So I have a js file where I store all of my data, and I will map out that js file to render the data I want to render. To stop myself from rendering all the data, I have made it so that it only conditionally renders every time title = element.id, therefore it should only render out the data that has the id specific id of the title. However, although this works, it ends up rendering a bunch of empty divs, and so this pushes my actual data down.
<a href=""https://i.stack.imgur.com/7u4Nf.jpg"" rel=""nofollow noreferrer"">Image of my data being pushed down</a>
As you can see the empty divs have pushed my actual data down a lot inside of my modal.</p>
<p>Does anybody know how to stop this from happening? (How to get rid of the space above by stopping the rendering of the blanks divs maybe?)</p>
<p>My modal file looks like this</p>
<pre><code>const Modal = ({ handleClose, id, title, subtitle, description, techStack, image, github, devpost }) =&gt; {
    let isId = false;
    return (
        &lt;Backdrop onClick={handleClose}&gt;
            &lt;motion.div 
                onClick={(e) =&gt; e.stopPropagation()}
                className='modal-ics'
                variants={dropIn}
                initial='hidden'
                animate='visible'
                exit='exit'
                
            &gt;
                &lt;div className='w-full flex flex-col items-center'&gt;
                    &lt;ModalButton onClick={handleClose} label='Close'&gt;&lt;/ModalButton&gt;
                    &lt;div className='flex flex-col w-full h-full justify-between items-start'&gt;
                        hello
       
                        {ICSData.map((element) =&gt; {
                            if (id == element.id) {
                                isId = true;
                            }

                            else {
                                isId = false;
                            }

                            return (
                                &lt;div&gt;
           
                                    { isId &amp;&amp;
                                        &lt;div&gt;
                                            &lt;div className='modal-text-separator'&gt;
                                                &lt;h1 className='modal-title'&gt;{element.title}&lt;/h1&gt;
                                                &lt;h3 className='modal-date'&gt;{element.subtitle1}&lt;/h3&gt;
                                                &lt;p className='modal-description mt-2'&gt;{element.description1}&lt;/p&gt;
                                            &lt;/div&gt;         
                                            { element.subtitle2 &amp;&amp; 
                                                &lt;div className='modal-text-separator'&gt;
                                                    &lt;h3 className='modal-date'&gt;{element.subtitle2}&lt;/h3&gt;
                                                    &lt;p className='modal-description mt-2'&gt;{element.description2}&lt;/p&gt;
                                                &lt;/div&gt;        
                                            }
                                            { element.subtitle3 &amp;&amp; 
                                                &lt;div className='modal-text-separator'&gt;
                                                    &lt;h3 className='modal-date'&gt;{element.subtitle3}&lt;/h3&gt;
                                                    &lt;p className='modal-description mt-2'&gt;{element.description3}&lt;/p&gt;
                                                &lt;/div&gt;        
                                            }
                                            { element.subtitle4 &amp;&amp; 
                                                &lt;div className='modal-text-separator'&gt;
                                                    &lt;h3 className='modal-date'&gt;{element.subtitle4}&lt;/h3&gt;
                                                    &lt;p className='modal-description mt-2'&gt;{element.description4}&lt;/p&gt;
                                                &lt;/div&gt;        
                                            }
                                            { element.subtitle5 &amp;&amp; 
                                                &lt;div className='modal-text-separator'&gt;
                                                    &lt;h3 className='modal-date'&gt;{element.subtitle5}&lt;/h3&gt;
                                                    &lt;p className='modal-description mt-2'&gt;{element.description5}&lt;/p&gt;
                                                &lt;/div&gt;        
                                            }
                                        &lt;/div&gt;
                                      
                                    }
                                &lt;/div&gt;                           
                            );
                        })}             
                    &lt;/div&gt;                  
                &lt;/div&gt;
            &lt;/motion.div&gt;
        &lt;/Backdrop&gt;
    )
}
export default Modal;
</code></pre>
<p>My data file looks like this</p>
<pre><code>import React from 'react'

const ICSData = [
  {
    id: &quot;classes&quot;,
    title: &quot;Classes&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  },
  {
    id: &quot;documentation-ds&quot;,
    title: &quot;Documentation&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  },
  {
    id: &quot;file-reading-and-writing&quot;,
    title: &quot;File Reading and Writing&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  },
  {
    id: &quot;object-concepts&quot;,
    title: &quot;Object Concepts&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  },
  {
    id: &quot;uml-diagrams&quot;,
    title: &quot;UML Diagrams&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  },
  {
    id: &quot;documentation-algorithms&quot;,
    title: &quot;Documentation&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  }
]
export default ICSData;
</code></pre>
","<javascript><reactjs><mapping><data-mapping>","2023-01-12 22:41:59","35","2","2","75103107","<p>You could just limit it by only calling the map function up until a certain ID</p>
<pre><code>{ICSData.filter((item, idx) =&gt; idx &lt; 5).map((element) =&gt; {...})}
</code></pre>
<p>And then you can also replace the 5 with a variable that controls that number if you want to make a load more button.</p>
"
"75103052","How to stop rendering react component every data file while mapping","<p>So I have a js file where I store all of my data, and I will map out that js file to render the data I want to render. To stop myself from rendering all the data, I have made it so that it only conditionally renders every time title = element.id, therefore it should only render out the data that has the id specific id of the title. However, although this works, it ends up rendering a bunch of empty divs, and so this pushes my actual data down.
<a href=""https://i.stack.imgur.com/7u4Nf.jpg"" rel=""nofollow noreferrer"">Image of my data being pushed down</a>
As you can see the empty divs have pushed my actual data down a lot inside of my modal.</p>
<p>Does anybody know how to stop this from happening? (How to get rid of the space above by stopping the rendering of the blanks divs maybe?)</p>
<p>My modal file looks like this</p>
<pre><code>const Modal = ({ handleClose, id, title, subtitle, description, techStack, image, github, devpost }) =&gt; {
    let isId = false;
    return (
        &lt;Backdrop onClick={handleClose}&gt;
            &lt;motion.div 
                onClick={(e) =&gt; e.stopPropagation()}
                className='modal-ics'
                variants={dropIn}
                initial='hidden'
                animate='visible'
                exit='exit'
                
            &gt;
                &lt;div className='w-full flex flex-col items-center'&gt;
                    &lt;ModalButton onClick={handleClose} label='Close'&gt;&lt;/ModalButton&gt;
                    &lt;div className='flex flex-col w-full h-full justify-between items-start'&gt;
                        hello
       
                        {ICSData.map((element) =&gt; {
                            if (id == element.id) {
                                isId = true;
                            }

                            else {
                                isId = false;
                            }

                            return (
                                &lt;div&gt;
           
                                    { isId &amp;&amp;
                                        &lt;div&gt;
                                            &lt;div className='modal-text-separator'&gt;
                                                &lt;h1 className='modal-title'&gt;{element.title}&lt;/h1&gt;
                                                &lt;h3 className='modal-date'&gt;{element.subtitle1}&lt;/h3&gt;
                                                &lt;p className='modal-description mt-2'&gt;{element.description1}&lt;/p&gt;
                                            &lt;/div&gt;         
                                            { element.subtitle2 &amp;&amp; 
                                                &lt;div className='modal-text-separator'&gt;
                                                    &lt;h3 className='modal-date'&gt;{element.subtitle2}&lt;/h3&gt;
                                                    &lt;p className='modal-description mt-2'&gt;{element.description2}&lt;/p&gt;
                                                &lt;/div&gt;        
                                            }
                                            { element.subtitle3 &amp;&amp; 
                                                &lt;div className='modal-text-separator'&gt;
                                                    &lt;h3 className='modal-date'&gt;{element.subtitle3}&lt;/h3&gt;
                                                    &lt;p className='modal-description mt-2'&gt;{element.description3}&lt;/p&gt;
                                                &lt;/div&gt;        
                                            }
                                            { element.subtitle4 &amp;&amp; 
                                                &lt;div className='modal-text-separator'&gt;
                                                    &lt;h3 className='modal-date'&gt;{element.subtitle4}&lt;/h3&gt;
                                                    &lt;p className='modal-description mt-2'&gt;{element.description4}&lt;/p&gt;
                                                &lt;/div&gt;        
                                            }
                                            { element.subtitle5 &amp;&amp; 
                                                &lt;div className='modal-text-separator'&gt;
                                                    &lt;h3 className='modal-date'&gt;{element.subtitle5}&lt;/h3&gt;
                                                    &lt;p className='modal-description mt-2'&gt;{element.description5}&lt;/p&gt;
                                                &lt;/div&gt;        
                                            }
                                        &lt;/div&gt;
                                      
                                    }
                                &lt;/div&gt;                           
                            );
                        })}             
                    &lt;/div&gt;                  
                &lt;/div&gt;
            &lt;/motion.div&gt;
        &lt;/Backdrop&gt;
    )
}
export default Modal;
</code></pre>
<p>My data file looks like this</p>
<pre><code>import React from 'react'

const ICSData = [
  {
    id: &quot;classes&quot;,
    title: &quot;Classes&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  },
  {
    id: &quot;documentation-ds&quot;,
    title: &quot;Documentation&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  },
  {
    id: &quot;file-reading-and-writing&quot;,
    title: &quot;File Reading and Writing&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  },
  {
    id: &quot;object-concepts&quot;,
    title: &quot;Object Concepts&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  },
  {
    id: &quot;uml-diagrams&quot;,
    title: &quot;UML Diagrams&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  },
  {
    id: &quot;documentation-algorithms&quot;,
    title: &quot;Documentation&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  }
]
export default ICSData;
</code></pre>
","<javascript><reactjs><mapping><data-mapping>","2023-01-12 22:41:59","35","2","2","75103367","<p>The reason why you end up having multiple empty divs where there shouldn't be lies in your return statement of the mapping:</p>
<pre class=""lang-js prettyprint-override""><code>                 {ICSData.map((element) =&gt; {
                            if (id == element.id) {
                                isId = true;
                            }

                            else {
                                isId = false;
                            }

                            return (
                                &lt;div&gt;
                                    { isId &amp;&amp;
                                        &lt;div&gt;
                                            &lt;div className='modal-text-separator'&gt;
                                     ...
                                     }
                                &lt;/div&gt;
                            )
...
</code></pre>
<p>Here, you're still creating a div whether there should be or not. The easiest fix would be to return an empty element if your id doesn't match, like this:</p>
<pre class=""lang-js prettyprint-override""><code>{ICSData.map(element =&gt; {
    if (id == element.id) {
        return (
          &lt;YOUR_CODE /&gt;
        )
    } else {
        return &lt;&gt;&lt;/&gt;
    }
}
</code></pre>
<p>However, looping through all your data to show one element probably isn't the best idea. A few alternatives:</p>
<ul>
<li>change your data so that instead of being an array, it's an object of objects, with the key being an entry's ID and the values being the rest of your data (assuming all your IDs are unique)</li>
<li>use the find function (once again assuming your IDs are unique)</li>
</ul>
<p>I'll just go over the second solution since it doesn't involve changing your data and is just more straightforward. However, it is slower than the first solution as the <code>.find</code> function would be O(n), compared to the first solution would be O(1). This likely doesn't matter if you have little data, but is something to keep in mind if the number of items in your <code>ICSData</code> array grows.</p>
<p>You can implement the second solution using one line:</p>
<pre><code>const data = ICSData.find(element =&gt; id === element.id)
// Do whatever you want with this var in your component
</code></pre>
"
"75073537","Allow User to Extract Data Dumps From DW","<p>We use synapse in azure as our warehouse and create reports in power bi for our users on top of this. We currently have a request to move all of the data dumps from our production system onto our warehouse DB as some of them are causing performance issue in production when run. We've been looking to re-do these into reports in power bi, however in some instances we still need to provide the &quot;raw&quot; data in csv/excel format. This has thrown an issue as some of these extracts are above 150k rows and therefore we can't use power bi to provide the extract as it has a limit on the rows it can export. Our solution would be to build a process to runs against the db and spits out a file into sharepoint for the user to consume, which we can do however we're unsure of how we could provide a method of the user triggering the extract. One of the ways I was thinking of doing it would be using power apps, however I'm wondering if there is an easier way someone on here might be able to suggest? I just need to provide pages with various buttons that trigger extracts to sharepoint from azure when clicked, which can be controlled by security in some way. Any advice would be appreciated.</p>
","<data-warehouse><azure-synapse>","2023-01-10 17:11:28","43","0","1","75113322","<p>Paginated Report Export doesn't have that row limit.</p>
<p>See, eg
<a href=""https://Export%20Power%20BI%20paginated%20reports%20with%20Power%20Automate"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/power-bi/collaborate-share/service-automate-paginated-integration</a></p>
<p>Or you can use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-overview"" rel=""nofollow noreferrer"">ADF Copy Activity</a> to create .csv extracts.</p>
"
"75041924","Create an IAM Role that makes Redshift able to access S3 bucket (ReadOnly)","<p>I'm trying to create a new IAM role and attach the S3 Read only Access policy but when I'm running the below code. I'm getting the following error: <strong>An error occurred (InvalidClientTokenId) when calling the CreateRole operation: The security token included in the request is invalid.</strong></p>
<p>I have set up the correct aws access key and security key in the configuration file but still I'm not able to get through this error.</p>
<p>Code for creating the IAM role.</p>
<pre><code>try:
    print('1.1 Creating a new IAM Role')
    dwhRole = iam.create_role(
        Path='/',
        RoleName=DWH_IAM_ROLE_NAME,
        Description='Allows Redshift clusters to call AWS services on your behalf.',
        AssumeRolePolicyDocument=json.dumps({
              &quot;Version&quot;: &quot;2012-10-17&quot;,
              &quot;Statement&quot;: [
                {
                  &quot;Effect&quot;: &quot;Allow&quot;,
                  &quot;Principal&quot;: {
                    &quot;Service&quot;: &quot;redshift.amazonaws.com&quot;
                  },
                  &quot;Action&quot;: &quot;sts:AssumeRole&quot;
                }
              ]
            }),
    )

except Exception as e:
    print(e)

# TODO: Attach Policy
print('1.2 Attaching Policy')
iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,
                       PolicyArn=&quot;arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess&quot;
                      )['ResponseMetadata']['HTTPStatusCode']

# TODO: Get and print the IAM role ARN
print('1.3 Get the IAM role ARN')
roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']

print(roleArn)

</code></pre>
<p>DWH_IAM_ROLE_NAME is a variable which is defined the configuration file as well.</p>
","<amazon-web-services><amazon-s3><amazon-redshift><data-warehouse>","2023-01-07 16:24:36","115","0","1","75050350","<p>First, unset the below env variables:-</p>
<pre><code>unset AWS_ACCESS_KEY_ID
unset AWS_SECRET_ACCESS_KEY
unset AWS_SESSION_TOKEN
</code></pre>
<p>Get a new session token and save it or copy it to your clipboard:-</p>
<pre><code>aws sts get-session-token
</code></pre>
<p>Run the below command and it is ask for SECRET KEY, ID, TOKEN</p>
<pre><code>aws configure
</code></pre>
<p>Finally, re-run your script.</p>
"
"74984622","SQL query that calculates, on a daily basis, the absolute number of active users, and the percentage of active users among all users","<p>I have to build a SQL query, ideally SQLlite syntax, that calculates
on a daily basis, the absolute number of active users, and the percentage of active users among all users. An active user is defined as a user to be active one some day X, if the user listened to at least one song in the time interval [X-6 days,X] . The result should adhere to the following schema꞉
1 row per day
3 columns꞉ (date, number_active_users, percentage_active_users)</p>
<p>We have the following Schema:</p>
<pre><code>-- Tables
-- tracks dimension
CREATE TABLE IF NOT EXISTS tracks(
    recording_msid        TEXT   NOT NULL,
    track_name            TEXT   NOT NULL,
    release_msid          TEXT   NOT NULL,
    release_name          TEXT   NOT NULL,
    release_mbid          TEXT   DEFAULT NULL,
    recording_mbid        TEXT   DEFAULT NULL,
    isrc                  TEXT   DEFAULT NULL,
    track_number          TEXT   DEFAULT NULL,
    track_mbid            TEXT   DEFAULT NULL,
    work_mbids            TEXT   DEFAULT NULL,
    dedup_tag             TEXT   DEFAULT NULL,
    tags                  TEXT   DEFAULT NULL,
    release_group_mbid    TEXT   DEFAULT NULL,
    artist_names          TEXT   DEFAULT NULL,
    discnumber            INT    DEFAULT NULL,
    release_artist_name   TEXT   DEFAULT NULL,
    release_artist_names  TEXT   DEFAULT NULL,
    rating                INT    DEFAULT NULL,
    source                TEXT   DEFAULT NULL,
    track_length          INT    DEFAULT NULL,
    albumartist           TEXT   DEFAULT NULL,
    totaldiscs            INT    DEFAULT NULL,
    totaltracks           INT    DEFAULT NULL,
    choosen_by_user       TEXT   DEFAULT NULL,
    PRIMARY KEY           (recording_msid,track_name)
);

-- users dimension
CREATE TABLE IF NOT EXISTS users(
    user_name                TEXT      PRIMARY KEY,
    spotify_id               TEXT      DEFAULT NULL,
    spotify_album_artist_ids TEXT      DEFAULT NULL,
    spotify_album_id         INTEGER   DEFAULT NULL,
    spotify_artist_ids       TEXT      DEFAULT NULL
);

-- artists dimension
CREATE TABLE IF NOT EXISTS artists(
    artist_msid     TEXT  PRIMARY KEY,
    artist_name     TEXT  NOT NULL,
    artist_mbids    TEXT  DEFAULT NULL
);

-- fact table user history
CREATE TABLE IF NOT EXISTS user_history(
    listened_at         NUMERIC NOT NULL,
    track_name          TEXT NOT NULL,
    recording_msid      TEXT NOT NULL,
    user_name           TEXT NOT NULL,
    artist_msid         TEXT NOT NULL,
    duration_ms         INT DEFAULT NULL,
    duration            INT DEFAULT NULL,
    listening_from      NUMERIC DEFAULT NULL,
    &quot;date&quot;              NUMERIC DEFAULT NULL,
    FOREIGN KEY(track_name) REFERENCES tracks(track_name),
    FOREIGN KEY(recording_msid) REFERENCES tracks(recording_msid),
    FOREIGN KEY(user_name) REFERENCES users(user_name),
    FOREIGN KEY(artist_msid) REFERENCES artists(artist_msid)
);
</code></pre>
<p>Any help or simple pointing in the right direction will do. Thanks! Some alterations to the schema are also possible</p>
<p>This is what I tried but it gives only one date group</p>
<pre><code>with user_groups as(
select date(listened_at, 'unixepoch','localtime') as dt, count(user_name) as cnt
                ROW_NUMBER() OVER (ORDER BY date(listened_at, 'unixepoch','localtime'))
                as row_no
                FROM user_history GROUP by date(listened_at, 'unixepoch','localtime')
           )
select dt as date, cnt as active_users, cnt/SUM(cnt) as percentage
from user_groups
where row_no between 1 and 6;
</code></pre>
","<sql><database><sqlite><data-warehouse>","2023-01-02 16:09:05","34","-2","1","74984657","<p>try self joins. I cannot write an SQL but know that this can be easily solved by creating a 6 way self join. Self join works the same way in SQLlite as in other databases</p>
"
"74922070","Working with Datawarehouse for Multi-tenancy","<p>We have Multi-tenancy application where we have separate database for each client in MongoDB, PostgreSQL &amp; separate Index in ElasticSearch.</p>
<p>Now we are planning for Datawarehouse to be created in Amazon-RedShift Serverless &amp; ingest all data from above databases into it as a separate database for each client.</p>
<p>We are planning to use AWS-Glue for the same. To ingest data from multiple data sources and multiple databases within these data sources.</p>
<p>Would like to know the approach(es) we should go for the this use case.</p>
<p>We are thinking the ETL process like this</p>
<ul>
<li>File with all data source and database in S3 bucket</li>
<li>Credentials in AWS Secret Manager</li>
<li>Loop through the source file</li>
<li>Create database in Redshift</li>
<li>Ingest raw data from multiple data sources for single client.</li>
<li>As it is RDBMS, we can maintain data normalization</li>
</ul>
<p>Is this the right approach?</p>
","<aws-glue><multi-tenant><data-warehouse><amazon-redshift-serverless>","2022-12-26 16:12:40","248","3","1","75018866","<p>I have couple of questions -</p>
<ol>
<li>Are these live databases?</li>
<li>Are you already on AWS?</li>
</ol>
<p>If moving from on premise, it might be better to leverage amazon direct connect (pricing here <a href=""https://aws.amazon.com/directconnect/pricing/"" rel=""nofollow noreferrer"">https://aws.amazon.com/directconnect/pricing/</a>) and use Glue to connect directly to on-premise DBs and ingest data to Datawarehouse, instead of creating another staging in S3.</p>
<p>All the best.</p>
"
"74908959","How can I allow Rust to preform operations on numbers past 128bits?","<p>I wrote some simple code that generates the fibonacci sequence starting at just 32 bit variables in a for loop moving to 64 then, 128 bit. I cannot get past 182 generations with 128 bits. Is there a way I can rewrite this code to get past this limit?</p>
<p>This code can only generate 182 iterations</p>
<pre><code>fn main() {

  let mut in1: i128 = 1;
  let mut in2: i128 = 1;
  let mut op1: i128;

  for iterations in 0..183 {
  //add the given numbers
    op1 = in1 + in2;
  //print the result
    println!(&quot;{}: {}&quot;, iterations + 1, op1);
  //prepare the next iterations
    in1 = in2;
    in2 = op1;
    //repeat
  }

}

</code></pre>
<p>This code will give the following error if more than 182 generations are given</p>
<p>'main' panicked at 'attempt to add with overflow'&quot;</p>
","<rust><data-management>","2022-12-24 16:32:10","62","1","1","74909161","<p>There is no default data type beyond <code>i128</code>. There are big integer libraries though, that allow for an arbitrary number of digits.</p>
<p>For example, <a href=""https://docs.rs/num/latest/num/struct.BigInt.html"" rel=""nofollow noreferrer""><code>num_bigint::BigInt</code></a>, a sub-crate of <a href=""https://docs.rs/num"" rel=""nofollow noreferrer""><code>num</code></a>, and at the time of writing the most established big integer library (to my knowledge):</p>
<pre class=""lang-rust prettyprint-override""><code>use num_bigint::BigInt;

fn main() {
    let mut in1: BigInt = 1.into();
    let mut in2: BigInt = 1.into();
    let mut op1: BigInt;

    for iterations in 0..183 {
        //add the given numbers
        op1 = in1 + &amp;in2;
        //print the result
        println!(&quot;{}: {}&quot;, iterations + 1, op1);
        //prepare the next iterations
        in1 = in2;
        in2 = op1;
        //repeat
    }
}
</code></pre>
<pre><code>...
179: 30010821454963453907530667147829489881
180: 48558529144435440119720805669229197641
181: 78569350599398894027251472817058687522
182: 127127879743834334146972278486287885163
183: 205697230343233228174223751303346572685
</code></pre>
<hr />
<p><strong>Notice:</strong></p>
<p>While <code>num_bigint::BigInt</code> is the most established, it is by far not the fastest library.</p>
<p>More info here: <a href=""https://crates.io/crates/bigint-benchmark"" rel=""nofollow noreferrer"">https://crates.io/crates/bigint-benchmark</a></p>
<p>However, please note that the fastest libraries are <strong>not</strong> MIT/Apache-2.0 licensed.</p>
"
"74894405","Google Cloud Storage JSONs to Pandas Dataframe to Warehouse","<p>I am a newbie in ETL. I just managed to extract a lot of information in form of JSONs to GCS. Each JSON file includes identical key-value pairs and now I would like to transform them into dataframes on the basis of certain key values.</p>
<p>The next step would be loading this into a data warehouse like Clickhouse, I guess? I was not able to find any tutorials on this process.
TLDR 1) Is there a way to transform JSON data on GCS in Python without downloading the whole data?
TLDR 2) How can I set this up to run periodically or in real time?
TLDR 3) How can I go about loading the data into a warehouse?</p>
<p>If these are too much, I would love it if you can point me to resources around this. Appreciate the help</p>
","<python><pandas><google-cloud-platform><etl><data-warehouse>","2022-12-22 22:18:42","38","0","1","74894507","<p>There are some ways to do this.</p>
<ol>
<li><p>You can add files to storage, then a <code>Cloud Functions</code> is activated every time a new file is added (<a href=""https://cloud.google.com/functions/docs/calling/storage"" rel=""nofollow noreferrer"">https://cloud.google.com/functions/docs/calling/storage</a>) and will call an endpoint in <code>Cloud Run</code> (container service - <a href=""https://cloud.google.com/run/docs/building/containers"" rel=""nofollow noreferrer"">https://cloud.google.com/run/docs/building/containers</a>) running a <code>Python</code> application to transform these <code>JSONs</code> in a <code>dataframe</code>. Note that the container image will be stored in <code>Container Registry</code>. Then the Python notebook running on Cloud Run will save the rows incrementally to <code>BigQuery</code> (warehouse). After that you can have analytics with <code>Looker Studio</code>.</p>
</li>
<li><p>If you need to scale the solution to millions/billions of rows, you can add files to storage, <code>Cloud Functions</code> is activated and calls <code>Dataproc</code>, a service where you can run Python, <code>Anaconda</code>, etc. (<a href=""https://stackoverflow.com/questions/50259301/how-to-call-google-dataproc-job-from-google-cloud-function"">How to call google dataproc job from google cloud function</a>). Then this <code>Dataproc</code> cluster will structurate the JSONs as a dataframe and save to the warehouse (BigQuery).</p>
</li>
</ol>
"
"74860723","OLAP Data Warehouse - composite primary key as multiple or single fields","<p>I'm building a data warehouse, and the data is of a quality where 8 fields may be required to uniquely identify a record, and this applies to three tables, each of which will have a few million rows of data per year. It's all 0NF.</p>
<p>Obviously every situation is unique, but considering that the purpose of the data warehouse is for OLAP, am I right in thinking that I would be better to create a single column to use as the primary key rather than a composite primary key of 8 separate fields? It's straightforward to concatenate the fields into an extra column as part of the ETL pipeline.</p>
<p>I appreciate the redundancy increases the storage requirement, and we are talking millions of rows a year, but I'm guessing it'll significantly improve query performance? And reduce memory requirements if the data is modelled in a BI tool?</p>
<p>Can anybody give me any general thoughts or advice on this please?</p>
<p>Below is some entirely made-up simulated data. I need to like the order table to the shipment table to get where the order was shipped from, for example, or maybe the order table to the shipment table to sum the quantity shipped.</p>
<p>I don't think normalising the tables is the way to go, as all four of the columns I'm using here would be subject to change, and only combined they form a reliable key for a unique shipment.</p>
<p>Because of this the data is bulk deleted/inserted based on shift date.</p>
<p><a href=""https://i.stack.imgur.com/jvaJW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jvaJW.png"" alt=""![enter image description here"" /></a></p>
<p>Thanks!</p>
<p>Phil.</p>
","<sql><database><data-warehouse><olap><composite-primary-key>","2022-12-20 09:09:50","82","0","1","74893810","<p>Those look like fact tables.  In a dimensional model only dimension tables need single-column keys.  Fact tables typically have compound keys made up of the dimension foreign keys that define the fact table grain.</p>
"
"74860270","Temporal tables VS SCD 2","<p>The temporal table can use it to replace the SCD type2 in a data warehouse ?.
I use temporal table in azure sql database.</p>
","<sql-server><etl><data-warehouse><scd2><historical-db>","2022-12-20 08:26:50","186","0","1","74893840","<p>Typically no.  Temporal tables are a good fit for staging tables, and can be used as a source to create slowly-changing dimensions if needed in a dimensional model.</p>
<p>The whole point of a dimensional model is to make writing queries easy.  In SCD the fact table still has a simple single-column foreign key to the dimension table.  So you get a historically accurate dimension values for each fact rows without complicating the queries.</p>
<p>To get the same result from a temporal table you'd have to join both the main table and the archive table, and filter them on both the business key of the dimension and the validfrom and validto dates.</p>
<p>Also temporal tables only support system versioning, which means that the validfrom and validto are always refer to the clock of the database server.  In a data warehouse you might want to use some other temporal reference in your data to model your SCD.</p>
"
"74827559","Check which indexes are needed in a data warehouse","<p>I'm working on a data warehouse that is updated twice daily. Over the course of mart development and query optimization, a high number of indexes were defined. But also the reports have been changed and updated. Is there a way to find out which indexes are still required and which one could been dropped?</p>
","<mariadb><data-warehouse>","2022-12-16 16:58:21","32","0","1","74828140","<p>First enable statistics with:</p>
<pre><code>SET GLOBAL userstat = 1;
</code></pre>
<p>And run all the queries that are currently in use.</p>
<p>Then read the statistics with</p>
<pre><code>SHOW INDEX_STATISTICS;
</code></pre>
<p>which will show the used indexes.</p>
<p>It might be needed to <code>FLUSH INDEX_STATISTICS</code> first to make sure to only see the indexes used.</p>
<p>See <a href=""https://mariadb.com/kb/en/user-statistics/"" rel=""nofollow noreferrer"">the documentation</a> for more information.</p>
"
"74821626","Importing CSV file(s) into SQL server with ADF","<p>I've a CSV file stored in a blob storage and want to upsert the records in a on-premise SQL server.
Currently I'm getting errors on inserting date fields. In the CSV file those date fields are typed as a date field. In my SQL table they are marked as DateTime2 fields. In the CSV file I have about 1000 records, 500 of them are seen as correct and 500 as incorrect.</p>
<p>When I check the file I really don't see a difference in the date fields. There is no time hidden or whatever which could actually cause my pipeline to fail.</p>
<p>What would be a good way to handle those fields? In the current situation, as a work around so that the business could continue, I've inserted all the fields as nvarchar(500) fields. So the imports works, but it's not ideal...</p>
<p>-- update</p>
<p>I've uploaded an image of the file which I'm importing, removed the fields where I'm not getting errors on. As you can see in the csv file the cells where there is some data are marked as Date fields.</p>
<p>Excel example
<img src=""https://i.stack.imgur.com/P45h6.png"" alt=""Excel example"" /></p>
<p>In ADF all fields are identified as a 'string' field, so maybe that's the problem. Never the less I can't change the schema in the import settings.</p>
<p>ADF identifying them as string
<img src=""https://i.stack.imgur.com/sJ8Nd.png"" alt=""ADF identifying them as string "" /></p>
<p>In SQL they are set to DateTime2 (setting it to dateTime or date gave the same error).</p>
<p>Sink field schema
<img src=""https://i.stack.imgur.com/Am0YG.png"" alt=""Sink field schema"" /></p>
<p>Here the mapping itself:</p>
<p>Source and Sink mapping
<img src=""https://i.stack.imgur.com/ZQCxS.png"" alt=""Source and Sink mapping"" /></p>
<p>Eventually the error that I'm getting, is saying that it can't insert a string value into a datetime field.</p>
<pre><code>{
&quot;errorCode&quot;: &quot;2200&quot;,
&quot;message&quot;: &quot;ErrorCode=TypeConversionFailure,Er is een uitzondering opgetreden bij het converteren van de waarde 29-8-2017 voor de kolomnaam TT_STARTDATE van het type String (Precision: , Scale: ) naar type DateTime (Precision: 255, Scale: 7). Aanvullende informatie: String was not recognized as a valid DateTime.&quot;,
&quot;failureType&quot;: &quot;UserError&quot;,
&quot;target&quot;: &quot;EMP&quot;,
&quot;details&quot;: [
    {
        &quot;errorCode&quot;: 0,
        &quot;message&quot;: &quot;'Type=System.FormatException,Message=String was not recognized as a valid DateTime.,Source=mscorlib,'&quot;,
        &quot;details&quot;: []
    }
]}
</code></pre>
<p>Thanks in advance!</p>
","<sql><csv><azure-data-factory><data-warehouse><azure-synapse>","2022-12-16 08:01:20","167","1","1","74999295","<p>Well i think it is the fomat of the date which is different in excel and what SQL is expecting is creating the error . I think in Excel you are providing dd-MM-YYYY , may be your SQL is expecting MM-DD-YYYY. I suggest please check this on the SQL side by inserting a record from TSQL using SSMS or other IDE . Once you are sure you can change the format in EXCEL accordingly .</p>
"
"74808208","How to filter a dataframe based on a list threshold values","<p>Dataframe A-</p>
<pre><code>target  query.   score
SDOIII  a          92.8
SDOII   a          72.8
SoxH    a          66
SDOIII  b          67
LbpA1   b          18
SoxH    b          12
SoxH    a          7
..............
</code></pre>
<p>Dataframe B (thresholds)</p>
<pre><code>target        threshold

SDOIII           4
SDOII            5
SoxH             6
LbpA1            7
.................
</code></pre>
<p>dataframe B has many target variables with their thresholds.</p>
<p>I wish to filter dataframe A based on scores &gt;= the threshold scores based on DataframeB.
Can anyone guide me with a code in pyhton/R ?</p>
<p>I tried filtering multiple conditions but not feasible for so many conditions or scores</p>
","<python><dataframe><data-manipulation><data-management>","2022-12-15 07:39:02","61","-1","1","74808913","<p>You can use something like this:</p>
<pre><code>filter_dict=dict(zip(df2.target,df2.threshold))
#{'SDOIII': 4, 'SDOII': 5, 'SoxH': 6, 'LbpA1': 7}

def check(target,score):
    if target in list(filter_dict.keys()):
        if score &gt;= filter_dict[target]:
            return True
        else:
            return False
    else:
        return None

df['check']=df[['target','score']].apply(lambda x: check(x['target'], x['score']),axis=1)
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>    target  query   score   check
0   SDOIII  a       92.8    True
1   SDOII   a       72.8    True
2   SoxH    a       66.0    True
3   SDOIII  b       67.0    True
4   LbpA1   b       18.0    True
5   SoxH    b       12.0    True
6   SoxH    a       7.0     True

#now you can select only True values
#df[df['check']==True]
</code></pre>
"
"74799758","Best way to manage multiple Google Sheets in BQ and then to Power BI","<p>I am trying to warehouse data from Google Sheets in BQ. Basically, we have about 20 different spreadsheets that all have the same tab 'RMLData' that cleans and organizes data. The 'RMLData' tab is identical across all spreadsheets.</p>
<p><a href=""https://i.stack.imgur.com/rWZkB.png"" rel=""nofollow noreferrer"">RMLData tab preview</a></p>
<p>In BQ, I want to add each 'RMLData' tab from each individual spreadsheet as a data source and have BQ combine the data from all 'RMLData' tabs into one big table that refreshes every hour.</p>
<p>Then, I need PowerBI to be able to receive the data from BQ and have that be auto-refreshed as well.</p>
<p>Question: what would be the best approach here to combine data from about 20 different Google Sheets in BQ that auto-refreshes at least once an hour?</p>
<p>Note: I have successfully appended queries of all the 'RMLData' tabs in Power BI but I'm <strong>not</strong> sure that's a <strong>scalable option</strong> as the company continues to grow bigger.</p>
<p>I think my best bet, in order to setup a framework that can handle even larger amounts of data, is to gather and organize the data from all 'RMLData' tabs in BQ and have that be able to be sourced to Power BI and/or some other BI software (Tableau, Data Studio, etc.)</p>
","<google-sheets><google-bigquery><powerbi><data-warehouse><powerbi-datasource>","2022-12-14 14:27:18","65","0","1","74809595","<p>A simple way to solve this if you want to retain the google sheet files is to take advantage of the external tables feature in BigQuery.</p>
<p><a href=""https://cloud.google.com/bigquery/docs/external-data-drive"" rel=""nofollow noreferrer"">https://cloud.google.com/bigquery/docs/external-data-drive</a></p>
<p>By creating external tables, you are preserving the source data as it is (google sheets) and leveraging BigQuery as the query engine, without having to ingest the data into BigQuery and therefore have a system in place to refresh it.</p>
<p>The SQL queries on top of those external tables will always pull fresh data, so PowerBI or others can also benefit from that.</p>
<p>Please, be aware of the limitations of this solution, especially in terms of performance.</p>
<p>If performance is an issue, then you must consider an alternative solution and ingest the data into BigQuery.</p>
"
"74782703","How exactly DBT models continuous works?","<p>I learned to create models with different levels (raw, source, staging, mart) using DBT. I also created a few basic models with real data cases using DBT.</p>
<p>I have a snowflake as a data warehouse. I'm confused about how to auto-execute my DBT models when data is constantly flowing from the source.</p>
<p>i.e. I want to calculate production data through DBT models and on the other hand we have real-time PowerBI reports on top of those transformed data.</p>
<p>To be very specific,</p>
<ul>
<li>I have all the necessary models created in DBT.</li>
<li>I'm doing some data transformation on the staging dbt layer.</li>
<li>I have set up a data pipeline using ADF to snowflake.</li>
</ul>
<p>How do I auto-execute my dbt models logic when data is continuously flowing to snowflake from ADF?</p>
","<snowflake-cloud-data-platform><data-warehouse><dbt>","2022-12-13 09:38:35","136","-1","1","74792302","<p>If you want your DBT models to reflect the <em>current state</em> of your data, you have two options:</p>
<p><strong>OPTION ONE</strong></p>
<p>Materialise all your models as views, not tables. A view is ultimately just a saved query masquerading as a table.</p>
<p>The downside is that your 'real time' reports are going to be slower, because they need to execute a complex SQL query instead of just pulling data from a materialised table. But that's a trade off everyone has to wrestle with when determining materialisation strategy.</p>
<p><strong>OPTION TWO</strong></p>
<p>Accept that 'current' is a lie; there will always be some lag, and it's up to you to establish what is acceptable.</p>
<p>If hour old data is good enough, schedule DBT to run every hour. If day old data is good enough, schedule it daily.</p>
"
"74777636","Data Modelisation : Bank Churn dataset","<p>To integrate the data on Talend I must first model the data warehouse including the dimension and the fact tables, it is something that I cannot do for the dataset attached .There is also the Business Requirement Document for this dataset.</p>
<p><a href=""https://drive.google.com/drive/folders/1e94lj4c3N6cTmyYaPkHj-6ogpX7WZ-L4"" rel=""nofollow noreferrer"">https://drive.google.com/drive/folders/1e94lj4c3N6cTmyYaPkHj-6ogpX7WZ-L4</a></p>
<p>For my dimensions tables should I have
oActiveCustomer
oBank_Churn
oCreditCard
oCustomerInfo
oExitCustomer
oGender
oGeography</p>
<p>The fact table will be the custumer churn? How can this table be stuctured ?</p>
<p>My dataset contains .xslx files and if I want to load the
metadata on talend I cannot manage to do so because they are not .csv files.</p>
","<dataframe><etl><data-modeling><talend><data-warehouse>","2022-12-12 21:40:17","40","0","1","75024854","<p>Based on what you provided, I can say that you need a transaction fact table : you want to capture the occurrence of that thing and so you record a transaction in your data warehouse</p>
<p>Assuming that the customer will be accumulating points to get the credit score.</p>
<p>In a typical Kimball-style star schema, the fact table that is at the centre of the schema would consist of order transaction data where you can have numeric measures.</p>
<p><a href=""https://i.stack.imgur.com/uLWm4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uLWm4.png"" alt=""enter image description here"" /></a></p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;div class=""mxgraph"" style=""max-width:100%;border:1px solid transparent;"" data-mxgraph=""{&amp;quot;highlight&amp;quot;:&amp;quot;#0000ff&amp;quot;,&amp;quot;nav&amp;quot;:true,&amp;quot;resize&amp;quot;:true,&amp;quot;toolbar&amp;quot;:&amp;quot;zoom layers tags lightbox&amp;quot;,&amp;quot;edit&amp;quot;:&amp;quot;_blank&amp;quot;,&amp;quot;xml&amp;quot;:&amp;quot;&amp;lt;mxfile host=\&amp;quot;app.diagrams.net\&amp;quot; modified=\&amp;quot;2023-01-05T22:05:04.539Z\&amp;quot; agent=\&amp;quot;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36 Edg/108.0.1462.54\&amp;quot; etag=\&amp;quot;ziXNxJ3eew2bmR6wmLdf\&amp;quot; version=\&amp;quot;20.7.4\&amp;quot;&amp;gt;&amp;lt;diagram id=\&amp;quot;9iJC2scZwhvFUXkt-TRG\&amp;quot; name=\&amp;quot;Page-1\&amp;quot;&amp;gt;7ZxRc9o4EMc/DY/XwTYG8pgY0vaaXDJHOr0+ZRRb2LrIEiPLAfrpb2VkDFFoyFVUeVCGmaC1ZFn6ac1/rYVelJSrjwItimueYdoL+9mqF016YRj0+wP4pyzrjSUeRRtDLkimK3WGGfmB25baWpMMV3sVJedUksW+MeWM4VTu2ZAQfLlfbc7pfq8LlGPDMEsRNa3fSCaLjXUcjjr7J0zyou05GJ5tjpSoraxHUhUo48sdUzTtRYngXG7elasEUzV57bxs2l0eOLq9MIGZPKbB012J7+nia4zG4e2cTbP69usf+ixPiNZ6wBNS3id1JXmJhb5wuW5no1qSkiIGpYs5Z3Kmj/ShnBaEZldozWt1NZVE6WNbuii4ID+gPqJwKAADHBZSww6H6myE0oRTLsDAeNNB12imTqa7EbiCZrftqINnpmu02qt4hSrZXiCnFC0q8tBcsmpYIpETdsElDFZX0tOBhcSrg/McbOnBsscwUVKsoYpuMNa82wU/1OVlt3yCgbYVO0snisZ62eolm29P3VGFNxrsGyCHBuQW8OyLgRgGLhtCgj/iZ0heoIQoyRkUKZ6rZmrmCHjPuTZLvlAnW6CUsPyqqTMZdJa/9fCViUPbOW08pCBZhpmCyCWS6GG7yBacMNlMT3wBL5jFpP8h7sVw4QmUg64ML1VdyIQzGAsiDTwMy2GJ1ZI4jvRhnzHxa96wnI/C3dazTjs6SPvzxNO2TDsOHdMeGLRntWCoxB61ZdSjsWPUgenZHzHLsEiQxDnXPXjk9pAHrXp1x9z073MQrKq/KOgEi2duj3l0pF47HfPYYP4JVQkSmYdtG3bsWq0FQwP25+o8leQJX+Py4YUYzDP/ReZj15otGBnM/6rLm/mt4FmdwvR55JZjsr5z7TY2kE8rSUpQbtkMUeTFm33okXPxdmZAv8OsFj42s//QxbVoC00HTwTOiFS67W698MytMx+71m5tpLDD/AKxx8nNnz5AOx33KHCt38z4TG2awJXmag/M/CT3uyZb08Fdk2j4bNskOPKOvo3XrWM2I7MtYr9v8j/cOn7lI9yxV5tBWcJrJuDUft/EOu2j901OtSdqyjW4X0rsUVtHPXYuzU2Z9gH+PGjLoIP+sbfwwYlIm/F2k82CKGYZMp+kel32ui4bDvZ12TAOjtRl8ckeq5j+PPtyP4G7t3fpN7v02fsWZcELCWoetH3QzvNY2qB+B/Q13C4LT9oyaedpLKHp0pdEVPJm7oGfArj7JJbQ9G0vwE9B2nnqSmimqF2iVN4nRS2Y198WnouOjpVk2zQH+5DNnLQ2w/jSPxd9u1NvnOb9avDQ3O/YPgj3wO0Dd67FQ3PnQwVdnrV91u7V+AvbHk3ewizlPlHFPnD3ajx64SsjSkBtnqv4vIXToXcuzyPzk3zKMg/+1OCdZ5lHgUEVZzluIy6YlILnnCE67aww1TXLcKYnuqtzxRXCJub5F0u51pEXqiVXgZYs27gMJk2s/9Htm8J3VQAcujhZ7R6crLel7Fx9dbtbU2C5JGrIO2GVGsBBTO14eS1S/JOJab9rATfAHP8U8YG9L4EpUsn6e93a5xe65dch+75HzDm/dm/3dX4HXPQ38Yuc8gver/+Fjv0Pit1PMDTHdn7IIpr+Bw==&amp;lt;/diagram&amp;gt;&amp;lt;/mxfile&amp;gt;&amp;quot;}""&gt;&lt;/div&gt;
&lt;script type=""text/javascript"" src=""https://viewer.diagrams.net/js/viewer-static.min.js""&gt;&lt;/script&gt;</code></pre>
</div>
</div>
</p>
"
"74694421","data cleansing - 2 columns change the data in one column if criteria is met","<p>I have columns with vehicle data, for vehicles greater than 1 year old with mileage less than 100 I want to replace mileage less than 100 with 1000.</p>
<p>my attempts -</p>
<pre><code>mileage_corr = vehicle_data_all.loc[(vehicle_data_all[&quot;mileage&quot;] &lt; 100) &amp; (vehicle_data_all[&quot;year&quot;] &lt; 2020)], 1000

Error - AttributeError: 'tuple' object has no attribute
</code></pre>
<p>and</p>
<pre><code>mileage_corr = vehicle_data_all.loc[(vehicle_data_all[&quot;mileage&quot;] &lt; 100) &amp; (vehicle_data_all[&quot;year&quot;] &lt; 2020)]
mileage_corr['mileage'].where(mileage_corr['mileage'] &lt;= 100, 1000, inplace=True)

error -
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  return self._where(
</code></pre>
","<python><tuples><where-clause>","2022-12-05 21:10:48","17","0","1","74694968","<p>Without complete information, assuming your <code>vehicle_data_all</code> DataFrame looks something like this,</p>
<pre><code>    years   mileage
0   2019    192
1   2014    78
2   2010    38
3   2018    119
4   2019    4
5   2012    122
6   2005    50
7   2015    69
8   2004    56
9   2003    194
</code></pre>
<p>Pandas has a way of assigning based on a filter result. This is referred to as <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer"">setting values</a>.</p>
<pre class=""lang-py prettyprint-override""><code>df.loc[condition, &quot;field_to_change&quot;] = desired_change
</code></pre>
<p>Applied to your dataframe would look something like this,</p>
<pre class=""lang-py prettyprint-override""><code>vehicle_data_all.loc[((vehicle_data_all[&quot;mileage&quot;] &lt; 100) &amp; (vehicle_data_all[&quot;year&quot;] &lt; 2020)), &quot;mileage&quot;] = 1000
</code></pre>
<p>This was my result,</p>
<pre><code>    years   mileage
0   2019    192
1   2014    1000
2   2010    1000
3   2018    119
4   2019    1000
5   2012    122
6   2005    1000
7   2015    1000
8   2004    1000
9   2003    194
</code></pre>
"
"74691848","time series data in a relational database?","<p>I have time series data in a relational database (postgres). Data  import to the database every 5 minutes, but imput get overwritten during the day, meaning at the end of the day there is only 1 record for that day for specific id (id and date-&gt;  composite PKs).</p>
<p>current process  is like this -&gt;Data comes in and is evaluated the same way 1:1.  (data comes in every table as they are in source, there is many redundancy.</p>
<p>3 problems:</p>
<ul>
<li>currently performance of getting data out of database(reading) is fast (good performance)</li>
</ul>
<p>frontend get query from this database and show data. result of the query is very fast. if I do normalization then getting the query become slower, but writing and updating become easier.
how can I optimize this database?</p>
<ul>
<li>missing data (<em>ignore this problem</em> )</li>
</ul>
<p>if we are able to store more records daily (history of one ID in different points of time everyday) then we can show comparison of two points in time in a day. does database support huge amoount of data every day?</p>
<ul>
<li>DWH</li>
</ul>
<p>source is just one, all data come from one source. can we have DWH for it or since source is only one, there is no need for it?</p>
<p>Edit:</p>
<p>How can I optimise this database?</p>
<p>currently there is only one Schema in a database. Data comes in and is evaluated the same way 1:1. writng is hard since we have redundany.</p>
<p>my solution:</p>
<p>I want to create 3 schemas for this database.</p>
<p>1 schema, for inserting data into tables, tables structure is base on data source. ( I assume data remains here temporary, and will be transfer in second schema)</p>
<p>2 schema, incoming data stored, and  data is  structured in 3NF.</p>
<p>3 Schema, denormlising data again because we need to get fast query (fast reading is required).</p>
","<database><optimization><database-design><time-series><data-warehouse>","2022-12-05 17:06:01","98","-1","1","74838737","<p>Your three schema model is exactly how this has been done for many years.</p>
<p><strong>Schema 1:</strong></p>
<p>Names: Staging/Landing/Ingestion</p>
<p>Schema matches the source system but it is cleared and reloaded for every load batch. Typically has a &quot;looser&quot; schema definition to allow for import and capture of bad data</p>
<p><strong>Schema 2:</strong></p>
<p>Names: Replica/ODS/Persisted data store</p>
<p>Schema 2 is never cleared, it's permanent. Following a data load, this layer should look exactly like your source systems. Data in schema 1 is &quot;merged&quot; into schema 2 each time. For example on a daily load cycle, Schema 1 just contains that days data but schema 2 contains the entire history of data loaded. Reference data is merged on a known primary key. Transactional data might be merged on a key or it might be merged on a &quot;windowing&quot; basis - i.e. delete the last days data from schema 2 and load schema 1 in</p>
<p>Some people like to have a &quot;point in time view&quot; where they can recreate what the source system looks like a historical point in time. I've never seen anyone use that though.</p>
<p><strong>Schema 3:</strong></p>
<p>Names: Business Layer/Star Schema/Reporting Layer/Datamart/Sematic Layer</p>
<p>Layer 2, which is usually a replica of an OLTP data model (OLTP is optimised for entering data). This is transformed into a data model that is optimised for reporting.</p>
<p>The tried and tested data model here is a star schema. It's been around for decades. If you research any reporting tool (i.e. Power BI), thay all say that the preferred data model to report from is a star schema. Yes a star schema is denormalised and has other benefits beyonf perforamnce, for example it is more easily understood by a business user, supports slowly changing dimensions etc.</p>
<p>All these concepts are explained further online but of you have any specific questions happy to expand further</p>
"
"74657435","Bigquery : Frequent Updates to a record","<p>We are planning to use bigquery for analytical purpose for our inventory system. Since this is inventory, a record of storeid-productid combination is going to change very often. In terms of volume, the total store-product records are somewhere between 200M - 400M. In total 500K mutations per day are expected. The mutations are coming in kafka topics.</p>
<p>From cost standpoint, what's the optimum solution. Options are</p>
<ol>
<li><p>A kafka listener issues a DML statement. <code>UPDATE inventory SET quantity=? WHERE productid=? AND storeid=?</code>. =&gt; My assessment on this option is, This is simplest of all, but may incur higher cost because bigquery doesn't have a notion of primary key. Will <a href=""https://cloud.google.com/bigquery/docs/search-index"" rel=""nofollow noreferrer"">search index</a>/ <a href=""https://hoffa.medium.com/bigquery-optimized-cluster-your-tables-65e2f684594b"" rel=""nofollow noreferrer"">clustering</a> etc help?</p>
</li>
<li><p>Have a staging table where we store every mutation, then periodically, using <code>MERGE</code> update the main\reporting table</p>
</li>
</ol>
<p>Something like this <a href=""https://cloud.google.com/blog/products/bigquery/performing-large-scale-mutations-in-bigquery"" rel=""nofollow noreferrer"">https://cloud.google.com/blog/products/bigquery/performing-large-scale-mutations-in-bigquery</a> (However this is a 2018 article, things might have changed a lot - for example, I think the 3 hour lag mentioned here is now 30 minutes)</p>
<pre><code>MERGE dataset.Inventory T
USING dataset.inventory_staging S
ON T.ProductID = S.ProductID and T.storeid = S.storeid
WHEN MATCHED THEN
  UPDATE SET quantity = s.quantity
WHEN NOT MATCHED THEN
  INSERT (ProductID, quantity) VALUES (ProductID, quantity)
</code></pre>
<p>Now the <strong>second question</strong>, if we are to take the second approach,</p>
<p>What's the cost effective way to sink a kafka topic to big query.
Does Kafka -&gt; GCS -&gt; BQ give any advantage over streaming solutions (like a boilerplate kafkalistener that does <a href=""https://cloud.google.com/bigquery/docs/write-api#write-api-overview"" rel=""nofollow noreferrer"">https://cloud.google.com/bigquery/docs/write-api#write-api-overview</a>)</p>
","<google-cloud-platform><apache-kafka><google-bigquery><data-warehouse>","2022-12-02 14:38:20","287","2","2","74660721","<p>Running one UPDATE statement per item would be crazy expensive, you need to have the stage table and run periodical MERGEs.</p>
<p>Kafka -&gt; GCS -&gt; BQ is the most cost effective way.</p>
<p>As additional suggestion you may explore creating a topic in Pub/Sub that replaces kafka. Also Pub / Sub has direct <a href=""https://cloud.google.com/dataflow/docs/tutorials/dataflow-stream-to-bigquery"" rel=""nofollow noreferrer"">ingestion</a> to bigquery.</p>
"
"74657435","Bigquery : Frequent Updates to a record","<p>We are planning to use bigquery for analytical purpose for our inventory system. Since this is inventory, a record of storeid-productid combination is going to change very often. In terms of volume, the total store-product records are somewhere between 200M - 400M. In total 500K mutations per day are expected. The mutations are coming in kafka topics.</p>
<p>From cost standpoint, what's the optimum solution. Options are</p>
<ol>
<li><p>A kafka listener issues a DML statement. <code>UPDATE inventory SET quantity=? WHERE productid=? AND storeid=?</code>. =&gt; My assessment on this option is, This is simplest of all, but may incur higher cost because bigquery doesn't have a notion of primary key. Will <a href=""https://cloud.google.com/bigquery/docs/search-index"" rel=""nofollow noreferrer"">search index</a>/ <a href=""https://hoffa.medium.com/bigquery-optimized-cluster-your-tables-65e2f684594b"" rel=""nofollow noreferrer"">clustering</a> etc help?</p>
</li>
<li><p>Have a staging table where we store every mutation, then periodically, using <code>MERGE</code> update the main\reporting table</p>
</li>
</ol>
<p>Something like this <a href=""https://cloud.google.com/blog/products/bigquery/performing-large-scale-mutations-in-bigquery"" rel=""nofollow noreferrer"">https://cloud.google.com/blog/products/bigquery/performing-large-scale-mutations-in-bigquery</a> (However this is a 2018 article, things might have changed a lot - for example, I think the 3 hour lag mentioned here is now 30 minutes)</p>
<pre><code>MERGE dataset.Inventory T
USING dataset.inventory_staging S
ON T.ProductID = S.ProductID and T.storeid = S.storeid
WHEN MATCHED THEN
  UPDATE SET quantity = s.quantity
WHEN NOT MATCHED THEN
  INSERT (ProductID, quantity) VALUES (ProductID, quantity)
</code></pre>
<p>Now the <strong>second question</strong>, if we are to take the second approach,</p>
<p>What's the cost effective way to sink a kafka topic to big query.
Does Kafka -&gt; GCS -&gt; BQ give any advantage over streaming solutions (like a boilerplate kafkalistener that does <a href=""https://cloud.google.com/bigquery/docs/write-api#write-api-overview"" rel=""nofollow noreferrer"">https://cloud.google.com/bigquery/docs/write-api#write-api-overview</a>)</p>
","<google-cloud-platform><apache-kafka><google-bigquery><data-warehouse>","2022-12-02 14:38:20","287","2","2","74716674","<p>For the first question, single UPDATE statements are a no-go, scheduled MERGES have some trade-offs in data availability, so another option is to store all the mutations and version the data so that you can always recover the last value.</p>
<p>Then, you can schedule a query to delete or archive old versions by removing them from the source table. This will improve data scanning.</p>
<p>Regarding your second question, the fastest way to solve this would be:</p>
<p>Kafka &gt; PubSub sink &gt; Bigquery.</p>
<p>Just be aware of the limitations and specific transformations that will happen at each step.</p>
<p><a href=""https://cloud.google.com/pubsub/docs/connect_kafka#convert-to-pubsub"" rel=""nofollow noreferrer"">https://cloud.google.com/pubsub/docs/connect_kafka#convert-to-pubsub</a></p>
<p><a href=""https://cloud.google.com/pubsub/docs/bigquery"" rel=""nofollow noreferrer"">https://cloud.google.com/pubsub/docs/bigquery</a></p>
"
"74646474","Working with multiple data warehouses in dbt","<p>I'm building an application where each of our clients needs their own data warehouse (for security, compliance, and maintainability reasons). For each client we pull in data from multiple third party integrations and then merge them into a unified view, which we use to perform analytics and report metrics for the data across those integrations. These transformations and all relevant schemas are the same for all clients. We would need this to scale to 1000s of clients.</p>
<p>From what I gather dbt is designed so each project corresponds with one warehouse. I see two options:</p>
<ol>
<li>Use one project and create a separate environment target for each client (and maybe a single dev environment). Given that environments aren't designed for this, are there any catches to this? Will scheduling, orchestrating, or querying the outputs be painful or unscalable for some reason?</li>
</ol>
<p>profiles.yml:</p>
<pre><code>example_project:
  target: dev
  outputs:
    dev:
      type: redshift
      ...
    client_1:
      type: redshift
      ...
    client_2:
      type: redshift
      ...
    ...
</code></pre>
<ol start=""2"">
<li>Create multiple projects, and create a shared dbt package containing most of the logic. This seems very unwieldy needing to maintain a separate repo for each client and less developer friendly.</li>
</ol>
<p>profiles.yml:</p>
<pre><code>client_1_project:
  target: dev
  outputs:
    client_1:
      type: redshift
      ...
client_2_project:
  target: dev
  outputs:
    client_2:
      type: redshift
      ...

</code></pre>
<p>Thoughts?</p>
","<amazon-redshift><data-warehouse><dbt>","2022-12-01 18:18:10","286","0","1","74647219","<p>I think you captured both options.</p>
<p>If you have a single database connection, and your client data is logically separated in that connection, I would definitely pick #2 (one package, many client projects) over #1. Some reasons:</p>
<ol>
<li>Selecting data from a different source (within a single connection), depending on the target, is a bit <a href=""https://stackoverflow.com/questions/73563223/dbt-conditionally-set-schema-config/73589026#73589026"">hacky</a>, and wouldn't scale well for 1000's of clients.</li>
<li>The developer experience for packages isn't so bad. You will want a developer data source, but depending on your business you could maybe get away with using one client's data (or an anonymized version of that). It will be good to keep this developer environment logically separate from any individual client's implementation, and packages allow you to do that.</li>
<li>I would consider generating the client projects programmatically, probably using a Python CLI to set up, <code>dbt run</code>, and tear down the required files for each client project (I'm assuming you're not going to use dbt Cloud and have another orchestrator or compute environment that you control). It's easy to write YAML from Python with <code>pyyaml</code> (each file is just a dict), and your individual projects probably only need separate <code>profiles.yml</code>, <code>sources.yml</code>, and (maybe) <code>dbt_project.yml</code> files. I wouldn't check these generated files for each client into source control -- just check in the script and generate the files you need with each invocation of dbt.</li>
</ol>
<p>On the other hand, if your clients each have their own physical database with separate connections and credentials, and those databases are absolutely identical, you could get away with #1 (one project, many profiles). The &quot;hardest&quot; parts of that approach would likely be managing secrets and generating/maintaining a list of targets that you could iterate over (ideally in a parallel fashion).</p>
"
"74632398","Improving insert query for SCD2 solution","<p>I have two insert statements. The first query is to inserta new row if the id doesn't exist in the target table. The second query inserts to the target table only if the joined id hash value is different (indicates that the row has been updated in the source table) and the id in the source table is not null. These solutions are meant to be used for my SCD2 solution, which will be used for inserts of hundreds thousands of rows. I'm trying not to use the MERGE statement for practices.</p>
<p>The columns &quot;Current&quot; value 1 indicates that the row is new and 0 indicates that the row has expired. I use this information later to expire my rows in the target table with my update queries.</p>
<p>Besides indexing is there a more competent and effective way to improve my insert queries in a way that resembles the like of the SCD2 merge statement for inserting new/updated rows?</p>
<p>Query:</p>
<pre><code>Query 1:
INSERT INTO TARGET
SELECT Name,Middlename,Age, 1 as current,Row_HashValue,id
from Source s
Where s.id not in (select id from TARGET) and s.id is not null

Query 2:
INSERT INTO TARGET
SELECT Name,Middlename,Age,1 as current ,Row_HashValue,id
FROM SOURCE s 
LEFT JOIN TARGET t ON s.id = t.id
AND s.Row_HashValue = t.Row_HashValue
WHERE t.Row_HashValue IS NULL and s.ID IS NOT NULL

</code></pre>
","<sql><sql-server><tsql><etl><data-warehouse>","2022-11-30 18:13:09","39","-1","1","74632624","<p>You can use <code>WHERE NOT EXISTS</code>, and have just one <code>INSERT</code> statement:</p>
<pre><code>INSERT INTO TARGET
SELECT Name,Middlename,Age,1 as current ,Row_HashValue,id
FROM SOURCE s 
WHERE NOT EXISTS (
    SELECT 1
    FROM TARGET t 
    WHERE s.id = t.id
    AND s.Row_HashValue = t.Row_HashValue)
AND s.ID IS NOT NULL;
</code></pre>
"
"74615600","Azure storage account permissions for data management but not delete create metadata like new table or container or delete a table","<p>I am looking for a way to share a connectionstring with all developers so that they can create data/delete data from all services in storage account like Table/Blob/file etc but cant create a new table or delete a table.
How can I generate such a key for them?
Access keys give all permissions and while generating SAS token I dont see an option to remove delete permissions for tables/containers etc.</p>
","<azure><azure-storage>","2022-11-29 14:26:34","87","0","1","74641670","<p><em><strong>I tried to reproduce the same in my environment and got the results like below:</strong></em></p>
<p>Generating <strong>SAS Token or Connection string</strong> by giving permissions like Delete and Create will allow the users not only to create/delete data from all services but also allows users to create/delete Table/Blob/file etc.</p>
<p><img src=""https://i.imgur.com/Ts6wDgN.png"" alt=""enter image description here"" /></p>
<p>To restrict the access, try adding the permission at Blob Level so that, the users create new data.
Not Assigning the Permissions at container level will not allow users to create any new blob/table.</p>
<p>I Generated the SAS Token in <strong>Blob Level</strong> like below:</p>
<p><img src=""https://i.imgur.com/Pn2GP5e.png"" alt=""enter image description here"" /></p>
<p>You can also make use of below <strong>CLI command to Generate SAS Token</strong>:</p>
<pre class=""lang-bash prettyprint-override""><code>New-AzStorageBlobSASToken -Context $ctx 
    -Container &lt;container&gt; 
    -Blob &lt;blob&gt; 
    -Permission rcd 
    -ExpiryTime &lt;date-time&gt;
    -FullUri
</code></pre>
<p><img src=""https://i.imgur.com/Bck0FPi.png"" alt=""enter image description here"" /></p>
<p>These are the operations you can perform to Generate SAS Token to restrict user access.
Based on your requirement it is not possible to generate SAS Token or Connection String for those operations as SAS Token can only be applied for Container/File/Table/ level or Blobs/Entity/Queue and Connection String is generated at Storage Account Level.</p>
<p><strong>Note</strong> that: If you generate the SAS Token with Create permission on Container/Blob, the user will be able to create or access Container and Blob and same with Files, Tables, Queues.</p>
"
"74600144","Azure Synapse Data Flows - parquet file names not working","<p>I have created a data flow within Azure synapse to:</p>
<ol>
<li>take data from a dedicated SQL pool</li>
<li>perform some transformations</li>
<li>send the resulting output to parquet files</li>
</ol>
<p>I am then creating a View based on the resulting parquet file using OPENROWSET to allow PowerBI to use the data via the built-in serverless SQL pool</p>
<p>My issue is that whatever the file name I enter on the integration record, the parquet files always look like part-00000-2a6168ba-6442-46d2-99e4-1f92bdbd7d86-c000.snappy.parquet - or similar</p>
<p>Is there a way to have a fixed filename which is updated each time the pipeline is run, or alternatively is there a way to update the parquet file to which the View refers each time the pipeline is run, in an automated way.</p>
<p>Fairly new to this kind of integration, so if there is a better way to acheive this whole thing then please let me know</p>
<p><a href=""https://i.stack.imgur.com/vjiN3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vjiN3.png"" alt=""enter image description here"" /></a></p>
","<sql-server><azure><parquet><data-warehouse><azure-synapse>","2022-11-28 11:48:28","112","0","1","74600957","<blockquote>
<p>Azure Synapse Data Flows - parquet file names not working</p>
</blockquote>
<ul>
<li>I repro'd the same and got the file name as in below image.
<img src=""https://i.imgur.com/ZbKm3i9.png"" alt=""enter image description here"" /></li>
</ul>
<p>In order to have the fixed name for sink file name,</p>
<ul>
<li>Set Sink settings as follows</li>
</ul>
<pre><code>File name Option: Output to single file
Output to single file: tgtfile (give the file name)
</code></pre>
<p><img src=""https://i.imgur.com/HYgMUlE.png"" alt=""enter image description here"" /></p>
<ul>
<li>In optimize, Select single partition.</li>
</ul>
<p><img src=""https://i.imgur.com/9vgzwTD.png"" alt=""enter image description here"" /></p>
<p><strong>Filename is as per the settings</strong></p>
<p><img src=""https://i.imgur.com/rPbJHTJ.png"" alt=""enter image description here"" /></p>
"
"74599499","Why does my insert not filter out null values?","<p>I'm trying to implement SCD2 by using <code>INSERT</code> and <code>UPDATE</code> instead of using MERGE. I need to insert new rows from my source table into my target table if the id doesn't exist and at the same time make sure that the column row <code>compositekey</code> is not null without crashing the insertion. At the moment I get this result when I run the select query:</p>
<p>Output:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>namn</th>
<th>hash</th>
<th>compositekey</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>demo</td>
<td>222</td>
<td>null</td>
</tr>
<tr>
<td>2</td>
<td>demo2</td>
<td>22220</td>
<td>212</td>
</tr>
<tr>
<td>3</td>
<td>demo3</td>
<td>22220</td>
<td>null</td>
</tr>
<tr>
<td>4</td>
<td>demo2</td>
<td>22220</td>
<td>434</td>
</tr>
</tbody>
</table>
</div>
<p>Expected output:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>namn</th>
<th>hash</th>
<th>compositekey</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>demo2</td>
<td>22220</td>
<td>212</td>
</tr>
<tr>
<td>4</td>
<td>demo2</td>
<td>22220</td>
<td>434</td>
</tr>
</tbody>
</table>
</div>
<p>Query:</p>
<pre class=""lang-sql prettyprint-override""><code>Insert into target
SELECT s.ID, s.namn, s.hash, s.compositekey
FROM source s
WHERE NOT EXISTS
(select id, compositekey  from target  where id = s.id and compositekey is null and s.compositekey is null)
</code></pre>
<p>I'm expecting the query to filter out and only give me rows that do not exist in my target table based on the id and composite key that is not null from the source table. I suspect that I'm querying wrong in the &quot;is null&quot; part.</p>
","<sql><tsql><etl><data-warehouse>","2022-11-28 10:52:23","38","0","1","74599669","<p>It's depends on TARGET table contents.
if &quot;compositekey is null&quot; is not True, so the INSERT will insert the compositekey with NULL values.</p>
<p>Try this way (The Where should be with UNIQUE columns on TARGET tables):</p>
<p>insert into target
SELECT s.ID, s.namn, s.hash, s.compositekey
FROM source s
where (s.ID, s.compositekey) NOT IN
(select id, compositekey  from target )
;</p>
"
"74593434","Why does my insert not detect existing rows in my target table?","<p>I'm trying to implement SCD2 by using insert and update instead of using <code>MERGE</code>. I need to insert a new row into my target table if the matching ids have different hash values. The table contains id, name, hash value, and 1 as enabled, which entails that the rows is the most current version.</p>
<p>As of the moment, I'm not getting the expected output. For example, if I have the id “1” in both the target and source table but the hash value differs, it inserts the value if I run the query again into my target table, leaving me with the id “1” with many duplicate hash values.</p>
<p>Query:</p>
<pre><code>INSERT INTO target
    SELECT s.ID, s.namn, s.hashh, 1 AS enablee
    FROM source s
    JOIN target t ON s.id = t.id 
    WHERE s.hashh &lt;&gt; t.hashh
</code></pre>
<p>Output:</p>
<pre><code>1   demo    222 0
1   demo    22220
1   demo    222 1
2   demo2   666 1
2   demo2   666 1
2   demo2   888  1
</code></pre>
<p>Expected output:</p>
<pre><code>1   demo    222 1
1   demo    22220
2   demo2   666 1
2   demo2   888 0
</code></pre>
<p>Ideally, I would like the insertion not to work and give me the output: (0 rows affected) if the hash value already exists in the targeted table.</p>
","<sql><tsql><etl><data-warehouse><scd2>","2022-11-27 20:07:08","49","0","1","74593764","<p>To understand the behavior, consider than WHERE applies to SELECT statement, not INSERT.</p>
<p>You can just run</p>
<pre><code>SELECT s.ID, s.namn, s.hashh, 1 AS enablee
FROM source s
JOIN target t ON s.id = t.id 
WHERE s.hashh &lt;&gt; t.hashh
</code></pre>
<p>to see what is inserted. The join finds all rows with same id, and mismatched hash. If all hashes match, it produces no result. But if there are some rows with mismatched hash, you get the results even if there is a matching row too.</p>
<p>What you need is the opposite, join on matched hash only and check if you found a match. Something like</p>
<pre><code>SELECT s.ID, s.namn, s.hashh, 1 AS enablee
FROM source s
LEFT JOIN target t ON s.id = t.id 
    AND s.hashh = t.hashh
WHERE t.hashh IS NULL
</code></pre>
"
"74588596","ETL / ELT pipelines - Metainformation about the pipeline","<p>how do you add metainformation about the used ETL / ELT code (and version of this ELT code) to the produced sink files / tables?</p>
<p>Do u consider it as required to have information like &quot;PipelineID&quot; or &quot;DataProductionTime&quot; in the targetfolder?</p>
","<azure><azure-data-factory><etl><data-warehouse><data-lakehouse>","2022-11-27 08:35:58","68","0","1","74597157","<blockquote>
<p>how do you add metainformation about the used ETL / ELT code (and version of this ELT code) to the produced sink files / tables?</p>
</blockquote>
<p>You can do it using the <strong>pipeline dynamic content</strong> and <strong>additional column in copy activity</strong>.</p>
<p><strong>This is my source file:</strong></p>
<p><img src=""https://i.imgur.com/y3rKdpp.png"" alt=""enter image description here"" /></p>
<p>In source of copy activity use additional column.</p>
<p><img src=""https://i.imgur.com/enC7LBY.png"" alt=""enter image description here"" /></p>
<p>In ADF you can find meta information about the pipeline in <strong>System variables</strong> of dynamic content.</p>
<p><img src=""https://i.imgur.com/1GZsH11.png"" alt=""enter image description here"" /></p>
<p><strong>Target file in sink folder:</strong></p>
<p><img src=""https://i.imgur.com/stac0Pe.png"" alt=""enter image description here"" /></p>
<blockquote>
<p>Do u consider it as required to have information like &quot;PipelineID&quot; or &quot;DataProductionTime&quot; in the targetfolder?</p>
</blockquote>
<p>All this depends on your requirement like if you want to know by which pipeline you are getting data you can use this.</p>
"
"74553430","User data management and authentication combined with Firebase?","<p><em>I know this isn't a coding question so I'm not sure which SO Community this belongs under so I'll ask here and see if anyone suggests another Community.</em></p>
<p>I'm using Firebase Auth to handle user authentication and Firestore as a database for data for each user.</p>
<p>When a user authenticates,</p>
<ol>
<li>the Auth ID for that user is returned from Firebase Auth</li>
<li>then I take the ID and call the Firestore API to get the user data for that user.</li>
</ol>
<p>Is there a way in Firebase to authenticate and have the data for the user in the same call?  Essentially combining the two steps of auth and user data in the above description.</p>
<p>Firebase Auth provides user data but only what is associated with their log in method: <a href=""https://firebase.google.com/docs/auth/web/manage-users#update_a_users_profile"" rel=""nofollow noreferrer"">https://firebase.google.com/docs/auth/web/manage-users#update_a_users_profile</a></p>
<p>Most solutions I found either do one the other but not both.</p>
","<firebase><firebase-authentication>","2022-11-23 21:59:34","76","0","1","74553629","<p>There is no single API call in Firebase to sign in the user and get their their data from Firestore in one go.</p>
<p>You will either have to make at least two API calls to Firebase, or implement your own server-side functionality to combine the operations (for example in Cloud Functions or Cloud Run) and then call <em>that</em> from your application.</p>
"
"74533985","Can't access snowflake cloud panel","<p>Please note that I'm totally new user of snowflake. The below question may seems silly to many.</p>
<p>I just set up a trial account with snowflake. The problem is that it is always logging me in at app.snowflake.com (instead of my cloud instance). Below one,</p>
<p><a href=""https://i.stack.imgur.com/sKueJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sKueJ.png"" alt=""enter image description here"" /></a></p>
<p>How can I access the standard interface? Below one,</p>
<p><a href=""https://i.stack.imgur.com/CvNKG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CvNKG.png"" alt=""enter image description here"" /></a></p>
<p>Am I missing something on the configuration end?</p>
","<snowflake-cloud-data-platform><data-warehouse>","2022-11-22 14:09:57","36","0","1","74534082","<p>There is a link to switch to Classical UI but it's not visible anymore for trial accounts.</p>
<p>So you should go to the &quot;profile page&quot; and change &quot;Default Experience&quot; to &quot;Classical UI&quot;. Logout and login again.</p>
<p><a href=""https://i.stack.imgur.com/UNWwW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UNWwW.png"" alt=""enter image description here"" /></a></p>
"
"74509478","Change source of data in Power BI","<p>I have been using IBM Netezza as a source of data in Microsoft Power BI, but recently we moved our Data Warehouse on Cloud (Azure). The tables remain the same (only the prefixes are different). I have already made a couple of reports in Power BI when we used IBM netezza, how can I use the same reports, but connect them with Azure??</p>
","<azure><powerbi><data-warehouse><netezza>","2022-11-20 15:34:02","73","0","1","74510176","<blockquote>
<p>I have already made a couple of reports in Power BI when we used IBM netezza, how can I use the same reports, but connect them with Azure??</p>
</blockquote>
<p>Sure. Open the report.  Create a new Power Query query that connects to your new Data Warehouse.  In the Power Query advanced editor compare the query to your Netezza queries, and update your existing queries to connect to the new Data Warehouse.</p>
<p>It should just be a change to the first line or two of the Power Query.</p>
"
"74493186","Using Pydequu on Jupyter Notebook and having this ""An error occurred while calling o70.run.'","<p>I'm trying to use Pydequu on Jupyter Notebook when i try to use <code>ConstraintSuggestionRunner</code> and show this error:</p>
<pre><code>Py4JJavaError: An error occurred while calling o70.run.
: java.lang.NoSuchMethodError: 'org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression org.apache.spark.sql.catalyst.expressions.aggregate.AggregateFunction.toAggregateExpression(boolean)'
</code></pre>
<p>I'm using this setup for the test:</p>
<ul>
<li>SDKMAN</li>
<li>sdk install <code>java 8.0.292.hs-adpt</code></li>
<li>SPARK 3.0.0</li>
</ul>
<p><em>I got this configs from <a href=""https://github.com/awslabs/python-deequ"" rel=""nofollow noreferrer"">awslabs/python-dequu</a> on README.md file.</em></p>
<pre class=""lang-py prettyprint-override""><code>import os
from pyspark.sql import SparkSession, Row
import pydeequ

os.environ[&quot;SPARK_VERSION&quot;] = &quot;3.0.0&quot;
</code></pre>
<p>The <code>error</code> it's from below code:</p>
<pre class=""lang-py prettyprint-override""><code>spark = (SparkSession
    .builder
    .config(&quot;spark.jars.packages&quot;, pydeequ.deequ_maven_coord)
    .config(&quot;spark.jars.excludes&quot;, pydeequ.f2j_maven_coord)
    .config(&quot;org.apache.spark.sql.catalyst&quot;, &quot;spark-catalyst_2.12-3.1.2-amzn-0.jar&quot;)
    .getOrCreate())

df = spark.sparkContext.parallelize([
            Row(a=&quot;foo&quot;, b=1, c=5),
            Row(a=&quot;bar&quot;, b=2, c=6),
            Row(a=&quot;baz&quot;, b=3, c=None)]).toDF()
</code></pre>
<p>Complete <code>error</code>:</p>
<pre><code>---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
Input In [3], in &lt;cell line: 3&gt;()
      1 from pydeequ.suggestions import *
      3 suggestionResult = ConstraintSuggestionRunner(spark) \
      4              .onData(df) \
      5              .addConstraintRule(DEFAULT()) \
----&gt; 6              .run()
      8 # Constraint Suggestions in JSON format
      9 print(suggestionResult)

File /opt/conda/lib/python3.10/site-packages/pydeequ/suggestions.py:81, in ConstraintSuggestionRunBuilder.run(self)
     74 def run(self):
     75     &quot;&quot;&quot;
     76     A method that runs the desired ConstraintSuggestionRunBuilder functions on the data to obtain a constraint
     77             suggestion result. The result is then translated to python.
     78 
     79     :return: A constraint suggestion result
     80     &quot;&quot;&quot;
---&gt; 81     result = self._ConstraintSuggestionRunBuilder.run()
     83     jvmSuggestionResult = self._jvm.com.amazon.deequ.suggestions.ConstraintSuggestionResult
     84     result_json = json.loads(jvmSuggestionResult.getConstraintSuggestionsAsJson(result))

File /usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321, in JavaMember.__call__(self, *args)
   1315 command = proto.CALL_COMMAND_NAME +\
   1316     self.command_header +\
   1317     args_command +\
   1318     proto.END_COMMAND_PART
   1320 answer = self.gateway_client.send_command(command)
-&gt; 1321 return_value = get_return_value(
   1322     answer, self.gateway_client, self.target_id, self.name)
   1324 for temp_arg in temp_args:
   1325     temp_arg._detach()

File /usr/local/spark/python/pyspark/sql/utils.py:190, in capture_sql_exception.&lt;locals&gt;.deco(*a, **kw)
    188 def deco(*a: Any, **kw: Any) -&gt; Any:
    189     try:
--&gt; 190         return f(*a, **kw)
    191     except Py4JJavaError as e:
    192         converted = convert_exception(e.java_exception)

File /usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)
    324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
    325 if answer[1] == REFERENCE_TYPE:
--&gt; 326     raise Py4JJavaError(
    327         &quot;An error occurred while calling {0}{1}{2}.\n&quot;.
    328         format(target_id, &quot;.&quot;, name), value)
    329 else:
    330     raise Py4JError(
    331         &quot;An error occurred while calling {0}{1}{2}. Trace:\n{3}\n&quot;.
    332         format(target_id, &quot;.&quot;, name, value))

Py4JJavaError: An error occurred while calling o70.run.
: java.lang.NoSuchMethodError: 'org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression org.apache.spark.sql.catalyst.expressions.aggregate.AggregateFunction.toAggregateExpression(boolean)'
    at org.apache.spark.sql.DeequFunctions$.withAggregateFunction(DeequFunctions.scala:31)
    at org.apache.spark.sql.DeequFunctions$.stateful_approx_count_distinct(DeequFunctions.scala:60)
    at com.amazon.deequ.analyzers.ApproxCountDistinct.aggregationFunctions(ApproxCountDistinct.scala:52)
    at com.amazon.deequ.analyzers.runners.AnalysisRunner$.$anonfun$runScanningAnalyzers$3(AnalysisRunner.scala:319)
    at scala.collection.immutable.List.flatMap(List.scala:366)
    at com.amazon.deequ.analyzers.runners.AnalysisRunner$.liftedTree1$1(AnalysisRunner.scala:319)
    at com.amazon.deequ.analyzers.runners.AnalysisRunner$.runScanningAnalyzers(AnalysisRunner.scala:318)
    at com.amazon.deequ.analyzers.runners.AnalysisRunner$.doAnalysisRun(AnalysisRunner.scala:167)
    at com.amazon.deequ.analyzers.runners.AnalysisRunBuilder.run(AnalysisRunBuilder.scala:110)
    at com.amazon.deequ.profiles.ColumnProfiler$.profile(ColumnProfiler.scala:141)
    at com.amazon.deequ.profiles.ColumnProfilerRunner.run(ColumnProfilerRunner.scala:72)
    at com.amazon.deequ.profiles.ColumnProfilerRunBuilder.run(ColumnProfilerRunBuilder.scala:185)
    at com.amazon.deequ.suggestions.ConstraintSuggestionRunner.profileAndSuggest(ConstraintSuggestionRunner.scala:203)
    at com.amazon.deequ.suggestions.ConstraintSuggestionRunner.run(ConstraintSuggestionRunner.scala:102)
    at com.amazon.deequ.suggestions.ConstraintSuggestionRunBuilder.run(ConstraintSuggestionRunBuilder.scala:226)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
    at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
    at java.base/java.lang.Thread.run(Thread.java:833)
</code></pre>
","<python><data-quality><data-profiling>","2022-11-18 17:12:13","139","2","1","75732362","<p>I had the same issue by using almost the same environment as you (meaning Jupyter notebook and Spark 3.1.1).
I solved the problem by following the steps:</p>
<ol>
<li><p>Download the deequ-2.0.0-spark-3.1.jar from maven repository <a href=""https://repo1.maven.org/maven2/com/amazon/deequ/deequ/2.0.0-spark-3.1/deequ-2.0.0-spark-3.1.jar"" rel=""nofollow noreferrer"">https://repo1.maven.org/maven2/com/amazon/deequ/deequ/2.0.0-spark-3.1/deequ-2.0.0-spark-3.1.jar</a></p>
</li>
<li><p>Upload deequ-2.0.0-spark-3.1.jar into the a Jupyter folder /home/jovyan/work/java-libs/</p>
</li>
<li><p>In the notebook add the following line:</p>
<p>os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars file:///home/jovyan/work/java-libs/deequ-2.0.0-spark-3.1.jar pyspark-shell'</p>
</li>
<li><p>Use the initialization code</p>
<pre><code>from pyspark.sql import SparkSession, Row
</code></pre>
<p>import pydeequ</p>
<p>spark=(SparkSession
.builder
.getOrCreate())</p>
</li>
</ol>
<p>By following the above steps I could get rid off the error that you've mentioned.
However, I bumped into a new one:</p>
<pre><code>  py4j.Py4JException: Constructor com.amazon.deequ.suggestions.rules.CategoricalRangeRule([]) does not exist
</code></pre>
<p>that I've solved with a code like this:</p>
<pre><code>suggestionResult = (
    ConstraintSuggestionRunner(spark)
    .onData(df)
    .addConstraintRule(CompleteIfCompleteRule())
    .addConstraintRule(NonNegativeNumbersRule())
    .addConstraintRule(RetainCompletenessRule())
    .addConstraintRule(RetainTypeRule())
    .addConstraintRule(UniqueIfApproximatelyUniqueRule())
    .run()
)

print(json.dumps(suggestionResult))
</code></pre>
"
"74454827","How to load only metadata in data catalog table using aws crawler","<p>I have created AWS crawler to read data from s3 (csv) and have given output config as catalog table. when I ran the crawler I can see both data and metadata populated in catalog table from the csv file.</p>
<p>Is there any way to populate only metadata info into catalog table ? please advise as I am new to AWS world</p>
","<aws-glue><glue-crawler>","2022-11-16 03:35:05","135","0","1","74458386","<p>AWS Glue Data Catalog <strong>does not</strong> hold any data in the catalog table. It is a <a href=""https://docs.aws.amazon.com/glue/latest/dg/start-data-catalog.html"" rel=""nofollow noreferrer"">persistent technical metadata store</a> that describes your data.</p>
<p>That means that when you crawl the data and the table is created in the Data Catalog, it describes the crawled data (showing column data types, location, data format, compression type (if any), partition keys, etc.)</p>
<p>The information you see in those tables <strong>is</strong> the metadata of the crawled table. To access the data that has been crawled, <a href=""https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html"" rel=""nofollow noreferrer"">you can use Athena</a> which uses Glue Data Catalog to store and retrieve table metadata for the Amazon S3 data.</p>
"
"74420549","Delta live tables data quality checks -Retain failed records","<p>There are 3 types of quality checks in Delta live tables:</p>
<ul>
<li><code>expect</code> (retain invalid records)</li>
<li><code>expect_or_drop</code> (drop invalid records)</li>
<li><code>expect_or_fail</code> (fail on invalid records)</li>
</ul>
<p>I want to retain invalid records, but I also want to keep track of them. So, by using <code>expect</code>, can I query the invalid records, or is it just for keeping stats like &quot;n records were invalid&quot;?</p>
","<databricks><azure-databricks><data-ingestion><delta-live-tables>","2022-11-13 11:34:13","273","1","1","74422470","<p><code>expect</code> just record that you had some problems so you have some statistics about you data quality in the pipeline.  But it's not very useful in practice.</p>
<p>Native quarantine functionality is still not available, that's why there is the <a href=""https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-cookbook.html#quarantine-invalid-data"" rel=""nofollow noreferrer"">recipe in the cookbook</a>.  Although it's not exactly what you need, you can still build on top of it, especially if you take into account the second part of the recipe that explicitly adds a <code>Quarantine</code> column - we can combine it with <code>expect</code> to get statistics into UI:</p>
<pre><code>import dlt
from pyspark.sql.functions import expr

rules = {}
quarantine_rules = {}

...
quarantine_rules = &quot;NOT({0})&quot;.format(&quot; AND &quot;.join(rules.values()))

@dlt.table(
  name=&quot;partitioned_farmers_market&quot;,
  partition_cols = [ 'Quarantine' ]
)
@dlt.expect_all(rules)
def get_partitioned_farmers_market():
  return (
    dlt.read(&quot;raw_farmers_market&quot;)
      .withColumn(&quot;Quarantine&quot;, expr(quarantine_rules))
      .select(&quot;MarketName&quot;, &quot;Website&quot;, &quot;Location&quot;, &quot;State&quot;,
              &quot;Facebook&quot;, &quot;Twitter&quot;, &quot;Youtube&quot;, &quot;Organic&quot;, &quot;updateTime&quot;,
              &quot;Quarantine&quot;)
  )
</code></pre>
<p>Another approach would be to use first part of the recipe (that uses <code>expect_all_or_drop</code>), and just union both tables (it's better to mark the valid/invalid tables with <code>temporary = True</code> marker)</p>
"
"74418824","How to delete a row on the master tab (destination) and also delete the same row value on the tab it was queried from automatically?","<p>This is a probably convoluted process in data collection. Any advice on how to best do this will be appreciated.</p>
<p>I am working on data collection and I am trying to clean it real time. The data is set up such hat each RA has their own tab on google sheet to put in observations. Then I have this master sheet that queries from all the RA tabs to create this master data. The master sheet can tell me which are duplicates and which of those duplicates have a better quality of information.</p>
<p>Doing a master data using query does not allow me to edit on it, for some reason. So, I copy and paste as value to another tab. I use the master tab to analyze any duplicate information. Ideally, when I clean the master tab and delete the lesser quality duplicate, it also deletes from the RA tab it originated from. How do I do this? Is this the best way of doing this? In a similar fashion, if I would like to edit in the master sheet and also have that row to be edited in the original tab.</p>
<p>I created a trial sheet to practice with.</p>
<p><a href=""https://www.stackoverflow.com/"">https://docs.google.com/spreadsheets/d/12XhUzH_v565C8fll_JwjloU0E12XmVarXesrUY83D-4/edit?usp=sharing</a></p>
<p>I have been doing this manually and wondering if there is a better way. I am open to using google apps script but I am still a new at that. I am thinking of using onEdit function but I do not know how to test my code or how to know what is in e.</p>
","<google-apps-script><google-sheets><data-cleaning><data-management><data-collection>","2022-11-13 06:01:21","19","0","1","74422512","<blockquote>
<p>The data is set up such hat each RA has their own tab on google sheet to put in observations.</p>
</blockquote>
<p>One easy solution would be to keep <em>all</em> the data in the master tab, have everyone edit that tab only, and use <a href=""https://support.google.com/docs/answer/3540681"" rel=""nofollow noreferrer"">filter views</a> to give every user their own private view so that everyone only sees &quot;their&quot; rows. This feature lets multiple simultaneous users sort, filter and edit the sheet without disturbing each other.</p>
<p>For additional ease of use, you can insert links in the frozen section of the sheet to easily switch between filter views, instead of having to go to <strong>Data &gt; Filter views</strong> to switch. See the <a href=""https://docs.google.com/spreadsheets/d/1ME5b4J6fQa8RRainxGtQkwTKFCg96N8_k6kCe8ABNyk/edit#gid=2048818001&amp;fvid=336145647"" rel=""nofollow noreferrer"">Filter views example</a> spreadsheet for an illustration.</p>
"
"74394976","Doing data mapping for each value in a df","<p>I have a data set like this</p>
<pre><code>df1&lt;-data.frame(ID=c(1,2,3,4),colA=c(101,102,103,104),colB=c(201,202,203,204))

df2&lt;-data.frame(var_id=c(101,102,103,104,201,202,203,204),var_value=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;,&quot;F&quot;,&quot;G&quot;,&quot;H&quot;))
</code></pre>
<p>I want to map any value in df1 that is in df2$var_id with the corresponding string in df2$var_value.</p>
<p>Desired output</p>
<pre><code>df1&lt;-data.frame(ID=c(1,2,3,4),colA=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;),colB=c(&quot;E&quot;,&quot;F&quot;,&quot;G&quot;,&quot;H&quot;))
</code></pre>
<p>I have tried write a function, and then do lapply, but it only display one var_value</p>
","<r><data-mapping>","2022-11-10 20:28:01","42","0","3","74395068","<pre><code>cols = c(&quot;colA&quot;, &quot;colB&quot;)
df1[cols] &lt;- lapply(df1[cols], \(x) df2$var_value[match(x, df2$var_id)])
df1
#   ID colA colB
# 1  1    A    E
# 2  2    B    F
# 3  3    C    G
# 4  4    D    H
</code></pre>
"
"74394976","Doing data mapping for each value in a df","<p>I have a data set like this</p>
<pre><code>df1&lt;-data.frame(ID=c(1,2,3,4),colA=c(101,102,103,104),colB=c(201,202,203,204))

df2&lt;-data.frame(var_id=c(101,102,103,104,201,202,203,204),var_value=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;,&quot;F&quot;,&quot;G&quot;,&quot;H&quot;))
</code></pre>
<p>I want to map any value in df1 that is in df2$var_id with the corresponding string in df2$var_value.</p>
<p>Desired output</p>
<pre><code>df1&lt;-data.frame(ID=c(1,2,3,4),colA=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;),colB=c(&quot;E&quot;,&quot;F&quot;,&quot;G&quot;,&quot;H&quot;))
</code></pre>
<p>I have tried write a function, and then do lapply, but it only display one var_value</p>
","<r><data-mapping>","2022-11-10 20:28:01","42","0","3","74395267","<p>You can join twice.</p>
<pre><code>library(dplyr)

df1 %&gt;%
  left_join(df2, by = c(&quot;colA&quot; = &quot;var_id&quot;)) %&gt;%
  left_join(df2, by = c(&quot;colB&quot; = &quot;var_id&quot;)) %&gt;%
  select(ID, colA = var_value.x, colB = var_value.y)

#   ID colA colB
# 1  1    A    E
# 2  2    B    F
# 3  3    C    G
# 4  4    D    H
</code></pre>
"
"74394976","Doing data mapping for each value in a df","<p>I have a data set like this</p>
<pre><code>df1&lt;-data.frame(ID=c(1,2,3,4),colA=c(101,102,103,104),colB=c(201,202,203,204))

df2&lt;-data.frame(var_id=c(101,102,103,104,201,202,203,204),var_value=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;,&quot;F&quot;,&quot;G&quot;,&quot;H&quot;))
</code></pre>
<p>I want to map any value in df1 that is in df2$var_id with the corresponding string in df2$var_value.</p>
<p>Desired output</p>
<pre><code>df1&lt;-data.frame(ID=c(1,2,3,4),colA=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;),colB=c(&quot;E&quot;,&quot;F&quot;,&quot;G&quot;,&quot;H&quot;))
</code></pre>
<p>I have tried write a function, and then do lapply, but it only display one var_value</p>
","<r><data-mapping>","2022-11-10 20:28:01","42","0","3","74396224","<p>with the <code>tidyverse</code> you can apply a function <code>across</code> several columns:</p>
<pre class=""lang-r prettyprint-override""><code>library(tidyverse)


df1 |&gt;
  mutate(across(colA:colB, \(x) map_chr(x, \(y) with(df2,var_value[y == var_id] ))))
#&gt;   ID colA colB
#&gt; 1  1    A    E
#&gt; 2  2    B    F
#&gt; 3  3    C    G
#&gt; 4  4    D    H

#or 

df1 |&gt;
  mutate(across(colA:colB, \(x) with(df2, var_value[match(x, var_id)])))
#&gt;   ID colA colB
#&gt; 1  1    A    E
#&gt; 2  2    B    F
#&gt; 3  3    C    G
#&gt; 4  4    D    H
</code></pre>
"
"74357203","How can I register kedro data catalog programmatically in Kedro 0.18?","<p>For various reasons (mainly ability to dynamically construct file paths) I like to define the data catalog programatically, and not use yaml file to define datasets e.g.</p>
<pre><code>DataCatalog(
    {&quot;products&quot;: ParquetDataSet(filepath=f{PREFIX}/products.parquet&quot;) 
...
})

</code></pre>
<p>In kedro 0.17 there <a href=""https://kedro.readthedocs.io/en/0.17.7/kedro.framework.hooks.specs.RegistrationSpecs.html#kedro.framework.hooks.specs.RegistrationSpecs.register_catalog"" rel=""nofollow noreferrer"">was</a> an easy way to register the catalog in order to use it's datasets in pipeline definition, via <code>register_catalog</code> hook.</p>
<p>However in 0.18 this hook is absent and there is no RegistrationSpecs section among hook <a href=""https://kedro.readthedocs.io/en/latest/kedro.framework.hooks.specs.html#module-kedro.framework.hooks.specs"" rel=""nofollow noreferrer"">specifications</a>.</p>
<p>What is there a way to replace <code>register_catalog</code> in kedro 0.18?</p>
<p>I searched the documentation and tried to implement catalog in yaml file, but that doesn't fit project's requirements. <code>TemplatedConfigLoader</code> is probably an option, but i does't allow to implement custom logic, constructing paths to datasets.</p>
","<python><kedro>","2022-11-08 07:45:05","312","0","1","74360644","<p>Not sure if this is the precise functionality that you're looking for, but I've been programmatically adding datasets by using a combination of <code>after_context_created</code> and <code>after_catalog_created</code> hooks.</p>
<p>Just create an &quot;add&quot; method for the dataset that you require and use the docs to see what args are needed. In the example below, I have my data in S3, so I created a method to grab my creds from <code>credentials.yml</code> and passed them to <code>PickleDataSet</code>.</p>
<pre class=""lang-py prettyprint-override""><code>import logging
from kedro.config import ConfigLoader
from kedro.framework.project import settings
from kedro.framework.hooks import hook_impl
from kedro.extras.datasets.pickle.pickle_dataset import PickleDataSet


class ProjectHooks:
    @property
    def _logger(self):
        return logging.getLogger(__name__)

    @hook_impl
    def after_context_created(self, context):
        self.project_path = context.project_path
        self._logger.info(f&quot;Project path: {self.project_path}&quot;)

    def _get_credentials(self, key):
        conf_path = f&quot;{self.project_path}/{settings.CONF_SOURCE}&quot;
        conf_loader = ConfigLoader(conf_source=conf_path, env=&quot;local&quot;)
        return conf_loader.get(&quot;credentials*&quot;)[key]

    def add_pickle_dataset(self, name, folder, layer=None):
        self.catalog.add(
            data_set_name=name,
            data_set=PickleDataSet(
                filepath=f&quot;s3://root/data/{folder}/{name}&quot;,
                credentials=self._get_credentials(&quot;dev_s3&quot;),
            ),
            replace=True,
        )
        if layer:
            self.catalog.layers[layer].add(name)
        self._logger.info(f&quot;Added dataset '{name}' to the data catalog.&quot;)

    @hook_impl
    def after_catalog_created(self, catalog):
        self.catalog = catalog
        datasets = self.catalog.load(&quot;params:datasets&quot;)
        for dataset in datasets:
            self.add_pickle_dataset(
                name=f&quot;{dataset}&quot;,
                folder=&quot;07_model_output&quot;,
                layer=&quot;Model Output&quot;,
            )
</code></pre>
"
"74341423","In R, how to return the value under one column based on a certain rule under another column?","<p>I have a dataframe in R that looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>stopping_file</th>
<th>delta</th>
</tr>
</thead>
<tbody>
<tr>
<td>14</td>
<td>6</td>
</tr>
<tr>
<td>20</td>
<td>1</td>
</tr>
<tr>
<td>21</td>
<td>1</td>
</tr>
<tr>
<td>22</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>How could I write a command to return the value &quot;22&quot; under the <em>stopping_file</em> column based on the rule that under the <em>delta</em> column in an ascending order (from top to bottom) <strong>count three 1s in consecutive</strong></p>
<p>Thanks for your suggestions in advance!</p>
<p>I tried to use the which command but no idea how to write this rule inside it.</p>
","<r><dataframe><data-wrangling><data-management>","2022-11-07 02:38:48","20","0","1","74341832","<p>With the inspiration from @zephryl, I managed to solve this problem proposed by myself.</p>
<p>The exact command to use is:</p>
<pre><code>data$stopping_file[data$delta ==1 &amp; lag(data$delta ==1) &amp; lag(lag(data$delta ==1))]
</code></pre>
<p>For an even more complicated situation: return the stopping_file # that corresponds to the 5th consecutive 1 under delta. The code will be like:</p>
<pre><code>data$stopping_file[data$delta ==1 &amp; lag(data$delta ==1) &amp; lag(lag(data$delta ==1)) &amp; lag(lag(lag(data$delta ==1))) &amp; lag(lag(lag(lag(data$delta ==1))))]
</code></pre>
"
"74337780","Populate Dim and Fact Table from Source Table which already has a Natural Key","<p>I have a <code>Source Table</code> which looks like as below (pls focus on the <code>show_id</code> column)
[<img src=""https://i.stack.imgur.com/g32Y4.png"" alt=""source table1"" /></p>
<p>And below is the <code>Dim and Fact Table Diagram</code> from my school's guide:
<a href=""https://i.stack.imgur.com/LzYQq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LzYQq.png"" alt=""dim and fact diagram"" /></a></p>
<p>They put the <code>show_id</code> from the <code>Source Table</code> directly inside the <code>Fact table</code>, which confuses me. I think that's not what I learned. Now I'm wondering whether should I put the <code>show_id</code> inside the <code>Dim_info</code> table instead or just don't include the <code>show_id</code> at all</p>
<p>Any advice?</p>
","<data-modeling><data-warehouse><dimensional-modeling><fact><fact-table>","2022-11-06 16:38:24","42","0","1","74377443","<p>There are two problems here.</p>
<ol>
<li><p>DIM_INFO is a vague table. If it's storing information about shows, then it should probably be called DIM_SHOW. My general rule of thumb here is that I should be able to ask ten knowledgeable people in the business what the &quot;info&quot; dimension is ... and if all ten can't give me a cogent answer, then a more descriptive table name is needed. Ergo, the suggestion about DIM_SHOW.</p>
</li>
<li><p>Show_ID appears to be a natural key from whatever the source system is for the show data. Natural keys should never be used to join facts to dimensions, as they are managed externally and are considered unreliable. DIM_SHOW should have its own surrogate key (ShowID?), which would be generated by the ETL process that loads the table. That surrogate key would then be used in the fact table.</p>
</li>
</ol>
"
"74336907","DataWarehouse for PowerBI Web","<p>People, I'm developing a DW for a set of internal information datamarts that i'm turning into a DWH.
In this moment, i'm using postgresql for this, but i'm open to try another tools/databases, like spark (STS), dremio, hive, etc.
What is my constraint here ?
The dataset and dashboard need to access the data stored in the database that is not in Azure and Azure for database, in this moment (for financial decisions), is not possible.</p>
<p>I've tried to access data with PowerBI Desktop on the PostgreSQL (hosted on another cloud provider), and that worked, but when i've published that, it required encryption that's available only in Azure PostgreSQL or i would need to use a gateway server to update the dataset in the PowerBI.
There is anyway to update the dataset without using a gateway on-premises server or Azure hosted services (SQL Database, Data Warehouse or Spark on Azure HDInsight) ?
The objective is to keep the actual cloud provider (as our applications and databases resides on that) and power the dashboards with good information.</p>
","<postgresql><powerbi><data-warehouse>","2022-11-06 14:48:35","35","-1","1","74339156","<blockquote>
<p>There is anyway to update the dataset without using a gateway on-premises server or Azure hosted services</p>
</blockquote>
<p>Not really.  That's what the <a href=""https://powerbi.microsoft.com/en-us/gateway/"" rel=""nofollow noreferrer"">Power BI Gateway</a> is for.</p>
<p>If you can't run a Gateway VM or a Database in Azure, you could push the data to Azure Storage and load into PowerBI from there.</p>
"
"74325667","How to go about data modelling?","<p>I've spent the last 2 hours or so figuring out how to apply it to my two variables. I am supposed to demonstrate/explain how I would handle the relationship of the two following variables in data modelling:</p>
<pre><code>   Pressure24h       DangerLevel24h     
     1000.2                45
     1014.8                90
     990.8                 14
     998.4                 95
     1002.1                46
     1006                  21
</code></pre>
<p>There is another 185,000 data to work with but that's just a very small sample of it. <strong>Pressure24h</strong> is measured in hectopascals and <strong>DangerLevel24h</strong> is measured in percentage. That's the only information I have to work with.
Is there any method that can be used to approach this?</p>
<p>I created a scatter plot to show the relationship but that was as far as I have gotten so far.</p>
<p><a href=""https://i.stack.imgur.com/Ty5Yn.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/Ty5Yn.png</a></p>
","<python><python-3.x><data-modeling>","2022-11-05 06:39:26","205","0","1","74326143","<p>Here's my code as discussed in the comments:</p>
<pre><code>def lobf(*cords):
    cords = cords[0]
    print(cords)
    x_mean, y_mean = 0, 0
    print(cords)
    for x, y in cords:
        x_mean += x # get x sum
        y_mean += y # get y sum
    x_mean /= len(cords) # get x mean
    y_mean /= len(cords) # get y mean
    # Step 2 from https://www.varsitytutors.com/hotmath/hotmath_help/topics/line-of-best-fit
    sigma_numerator, sigma_denominator = 0, 0
    for xi, yi in cords:
        sigma_numerator += (xi - x_mean)*(yi - y_mean) # get numerator
        sigma_denominator += (xi - x_mean)**2 # get denominator
    m = sigma_numerator/sigma_denominator # get slope
    c = y_mean - m*x_mean # get y-intercept
    return m ,c

data_values = [(2,2), (4,4)] # Sample data value you can put yours here
# Creating a for loop for every increment of 5 to avoid the blue blob you got.
# You can change the increment as per your choice
predicted_values = []
increment = 5
m ,c = lobf(data_values)
for i in range(data_values[-1][0]+increment, len(data_values)*100, increment): # You can consider dangerlevel24h as your x
    &quot;&quot;&quot;
    Starts with incrementing the last x value of your data
    &quot;&quot;&quot;
    predicted_values.append((i,i*m + c)) # appends x, y
print(predicted_values)
</code></pre>
<p>You can then plot every value from <code>predicted_values</code>. By iterating through every 5th or your desired iteration, you can avoid blue blobs to form. Also, this method will help you in predicting future values that aren't in your data. You can also try using Pearson's Theory of Correlation which is related to this method.
<a href=""https://i.stack.imgur.com/zwfqL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zwfqL.png"" alt=""enter image description here"" /></a></p>
"
"74279995","How to recode separate variables from a multiple response survey question into one variable","<p>I am trying to recode a variable that indicates total number of responses to a multiple response survey question. Question 4 has options 1, 2, 3, 4, 5, 6, and participants may choose one or more options when submitting a response. The data is currently coded as binary outputs for each option: var <code>Q4___1</code> = yes or no (1/0), var Q4___2 = yes or no (1/0), and so forth.</p>
<p>This is the <code>tabstat</code> of all yes (1) responses to the 6 Q4___* variables</p>
<pre><code>  Variable |       Sum
-------------+----------
      q4___1 |        63
      q4___2 |        33
      q4___3 |         7
      q4___4 |         2
      q4___5 |         3
      q4___6 |         7
------------------------
total = 115
</code></pre>
<p>I would like to create a new variable that encapsulates these values.</p>
<p>Can someone help me figure out how to create this variable, and if coding a variable in this manner for a multiple option survey question is valid?</p>
<p>When I used the <code>replace</code> command the total number of responses were not adding up, as shown below</p>
<pre><code>  gen q4=. 
    replace q4 =1 if q4___1 == 1
    replace q4 =2 if q4___2 == 1
    replace q4 =3 if q4___3 == 1 
    replace q4 =4 if q4___4 == 1
    replace q4 =5 if q4___5 == 1
    replace q4 =6 if q4___6 == 1
    label values q4 primarysource`

      q4 |      Freq.     Percent        Cum.
------------+-----------------------------------
          1 |         46       48.94       48.94
          2 |         31       32.98       81.91
          3 |          6        6.38       88.30
          4 |          1        1.06       89.36
          5 |          3        3.19       92.55
          6 |          7        7.45      100.00
------------+-----------------------------------
      Total |         94      100.00
</code></pre>
<p><strong>UPDATE</strong>
to specify I am trying to create a new variable that captures the column sum of each question, not the rowtotal across all questions. I know that 63 participants responded yes to question 4 a) and 33 to question 4 b) so I want my new variable to reflect that.</p>
<p>This is what I want my new variable's values to look like.</p>
<pre><code>q4
-------------+----------
      q4___1 |        63
      q4___2 |        33
      q4___3 |         7
      q4___4 |         2
      q4___5 |         3
      q4___6 |         7
------------------------
total = 115   
</code></pre>
","<stata><data-analysis><data-management><readstata13>","2022-11-01 18:00:05","103","0","1","74280416","<p>The fallacy here is ignoring the possibility of multiple 1s as answers to the various <code>Q4????</code> variables. For example if someone answers <code>1 1 1 1 1 1</code> to all questions, they appear in your final variable only in respect of their answer to the 6th question. Otherwise put, your code overwrites and so ignores all positive answers before the last positive answer.</p>
<p>What is likely to be more useful are</p>
<p>(1) the total across all 6 questions which is just</p>
<pre><code>egen Q4_total = rowtotal(Q4????)
</code></pre>
<p>where the 4 instances of <code>?</code> mean that by eye I count 3 underscores and 1 numeral.</p>
<p>(2) a concatenation of responses that is just</p>
<pre><code>egen Q4_concat = concat(Q4????)
</code></pre>
<p>(3) a variable that is a concatenation of questions with positive responses, so 246 if those questions were answered 1 and the others were answered 0.</p>
<pre><code>gen Q4_pos = &quot;&quot; 

forval j = 1/6 { 
    replace Q4_pos = Q4_pos + &quot;`j'&quot; if Q4____`j' == 1 
}
</code></pre>
<p>EDIT</p>
<p>Here is a test script giving concrete examples.</p>
<pre><code>clear 
set obs 6 
forval j = 1/6 {
    gen Q`j' = _n &lt;= `j'
}

list 

egen rowtotal = rowtotal(Q?)

su rowtotal, meanonly 

di r(sum)

* install from tab_chi on SSC
tabm Q? 
</code></pre>
<p>Results:</p>
<pre><code>. list 

     +-----------------------------+
     | Q1   Q2   Q3   Q4   Q5   Q6 |
     |-----------------------------|
  1. |  1    1    1    1    1    1 |
  2. |  0    1    1    1    1    1 |
  3. |  0    0    1    1    1    1 |
  4. |  0    0    0    1    1    1 |
  5. |  0    0    0    0    1    1 |
     |-----------------------------|
  6. |  0    0    0    0    0    1 |
     +-----------------------------+
 
. egen rowtotal = rowtotal(Q?)
. su rowtotal, meanonly 
. di r(sum)
21

. tabm Q? 

           |        values
  variable |         0          1 |     Total
-----------+----------------------+----------
        Q1 |         5          1 |         6 
        Q2 |         4          2 |         6 
        Q3 |         3          3 |         6 
        Q4 |         2          4 |         6 
        Q5 |         1          5 |         6 
        Q6 |         0          6 |         6 
-----------+----------------------+----------
     Total |        15         21 |        36 
</code></pre>
"
"74273153","Create a Data Warehouse with the database on SQL Developer","<p>I have a database in SQL Developer which pull data from an ERP tool and I would like to create a Data warehouse in order to connect it then to PowerBI.</p>
<p>It's my first time that I am doing all this process from the beginning so I am not so experienced.</p>
<p>So where are you suggesting to create the Data Warehouse (I was thinking on SSMS) and how can I connect it with PowerBI ?</p>
<p>My Data Warehouse will consist from some View of my tables and some Joins to get some data in the structure that I want since it is not possible to change anything in the DB.</p>
<p>Thanks in advance.</p>
","<database><oracle><data-warehouse>","2022-11-01 08:14:05","82","0","1","74284916","<p>A &quot;data warehouse&quot; is just a database. The distinction is really more about the commonly used schema design, in the sense that a warehouse is often built along the lines of a <a href=""https://en.wikipedia.org/wiki/Star_schema"" rel=""nofollow noreferrer"">star</a> or <a href=""https://en.wikipedia.org/wiki/Snowflake_schema"" rel=""nofollow noreferrer"">snowflake</a> design.</p>
<p>So if you already have a database that is extracting data from your ERP, there is nothing to stop you from pointing PowerBI directly at that and performing some analytics etc.  If your intention is to start with this database, and then clone/extract/load this data into a <em><strong>new</strong></em> database which is a star/snowflake schema, then that's a much bigger exercises.</p>
"
"74264295","Data Warehouse design(BigQuery), load into dimensional table independent of fact table","<p>I want to design a data warehouse (Data MART) with one fact table and 2 dimensional tables, where the data mart takes some Slowly Changing Dimensions into consideration, with surrogate key. I'm wondering how I can model this so that data insertion to the dimensional tables can be made independent (inserted before fact table row exist) of the fact table. The data will be streamed from PubSub to BigQuery via Dataflow, thus some of the dimensional data might arrive earlier, needing to be inserted into the dimensional table before the fact data.</p>
","<database><google-cloud-platform><data-warehouse>","2022-10-31 13:50:28","197","0","2","74265448","<p>I don't completely understand your question. Dimensions are always (or rather, <em>almost</em> always) populated before fact tables are, since fact table records refer to dimensions (and not the other way around).</p>
<p>If you're worried about being able to destroy and rebuild your dimension table without having to also rebuild your fact table, then you'll need to use some sort of surrogate key pipeline to maintain your surrogate key to natural key relationships. But again, I'm not sure that this is what you're asking.</p>
"
"74264295","Data Warehouse design(BigQuery), load into dimensional table independent of fact table","<p>I want to design a data warehouse (Data MART) with one fact table and 2 dimensional tables, where the data mart takes some Slowly Changing Dimensions into consideration, with surrogate key. I'm wondering how I can model this so that data insertion to the dimensional tables can be made independent (inserted before fact table row exist) of the fact table. The data will be streamed from PubSub to BigQuery via Dataflow, thus some of the dimensional data might arrive earlier, needing to be inserted into the dimensional table before the fact data.</p>
","<database><google-cloud-platform><data-warehouse>","2022-10-31 13:50:28","197","0","2","74265813","<p>BigQuery does not perform referential integrity check, which means it will not check whether parent row exists in dimension table while inserting child row into fact table and you don't need this in data analytics setup. You can keep appending records to both fact table and dimension tables independently in BigQuery.</p>
<p>Flatten / denormalise the table and keep dimensions in fact tables - repeated records are not going to be an issue in BigQuery - you can make use of <a href=""https://cloud.google.com/bigquery/docs/clustered-tables"" rel=""nofollow noreferrer"">Clustering and Partitioning</a></p>
<p><a href=""https://i.stack.imgur.com/Fgp55.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Fgp55.png"" alt=""Clustered and Partitioned Tables"" /></a></p>
<p>Other option is, if you have dimensions in RDBMS system, upload dimension tables as files to Cloud Storage / rows to Cloud SQL and join them in Dataflow, in this case you can skip multiple sinks - you can write to a flatten schema into a BigQuery table sink.</p>
<p>Inserting order does not matter in BigQuery, you can reference event records based on pubsub message publishing time / source event time, etc.</p>
"
"74262554","How should I model a business process and its subsequent related processes and actions?","<p>I feel it is similar to the age old Sales --&gt; Orders setup but I can't seem to finalize a solution.</p>
<p>I have a Business process of &quot;Complaints&quot; which are placed into a fact table which are in a traditional star schema linking to other dimensions such as customer and date.</p>
<p>There is also a process of Complaint Actions. Each complaint can have 1 or more actions which can result from it.</p>
<p><strong>I am wondering how to model this so the user can drill through from a Complaint to its actions in Power BI.</strong></p>
<p>The way it would ideally be displayed is a table visual with :</p>
<ul>
<li>Complaint Date</li>
<li>Complaint Number</li>
<li>Customer</li>
<li>Action Number</li>
<li>Action Date</li>
</ul>
<p>Currently the only common dimensions are Customer and Date.</p>
<p>My ideas are:</p>
<ol>
<li><p>Fact Complaints and Fact Complaints actions with conformed dimensions then place into a matrix visual so it is all visible on one page?</p>
</li>
<li><p>Create a Dim Complaints table which would hold the data for each complaint (being the same amount of rows as the Fact Complaints) and link that to the complaints action table.  Is this a bad approach in Dimensional modelling?</p>
</li>
<li><p>Combine the 2 Fact Tables</p>
</li>
<li><p>Create a &quot;bridging&quot; Dimension table which has each complaint ID and its associated Action IDs and use the ID here in the Power BI visuals I need.</p>
</li>
</ol>
<p>Example of tables below:</p>
<p>Fact Complaints</p>
<p>| Customer Key|
| Date Key|
| Measure 1 |
| measure 2  |</p>
<p>Fact Complaint Actions</p>
<p>| Customer Key|
| Date Key|
| Complaint Number (Degenerate Dimension) |<br />
| Measure 1 |
| measure 2  |</p>
","<powerbi><data-warehouse><dimensional-modeling><star-schema>","2022-10-31 11:25:23","51","0","1","74266928","<p>I think I'd go with option 2. DimComplaints would just be a conformed dimension, and there's nothing wrong with having two fact tables reference it (that's the whole gist of how a Kimball data warehouse works). Each action would just be another row in the FactComplaintAction table, so it'd be easy to aggregate metrics across complains and actions.</p>
"
"74251959","Subroute data management using go_router in Flutter","<p>I have a conceptual question about how Flutter goes about data/state management when using <code>go_router</code> navigation. My app has a homepage containing a calendar that shows a list of events from a database (using the <code>riverpod</code> package to listen to the API calls). I also have a separate event page that shows the data of an event given its ID. The route to the event is a subroute of the home route (see route definition below).</p>
<p>I want to allow users to navigate immediately to this event page by way of a notification on mobile or a URL to the page (e.g. <code>host/event/SomeEventID</code>). However, to minimize API calls, I want to know whether this type of navigation under the hood calls the HomePage builder in any way that may trigger the API call to fetch all the events and if so, how to prevent this from happening.</p>
<p>Current route definition:</p>
<pre class=""lang-dart prettyprint-override""><code>List&lt;GoRoute&gt; routes =&gt; &lt;GoRoute&gt;[
      GoRoute(
        name: 'home',
        path: '/',
        builder: (_, __) =&gt; const HomePage(),
        routes: &lt;GoRoute&gt;[
          GoRoute(
            name: 'event',
            path: 'event/:id',
            builder: (context, state) {
                // TODO: Fetch data
                // NOTE: Navigate to home if the id does not match an event
                // final String id = state.params['id']!;
            },
          ),
        ],
      ),
    ];
</code></pre>
","<flutter><flutter-go-router>","2022-10-30 09:41:57","322","0","1","74361825","<p>I have found a way to navigate directly to a subroute, and it turns out that indeed the HomePage gets build even though it is not navigated to.</p>
"
"74214041","How to maintain order of data frame when making pandas pivot table","<p>trying to make a heat map out of a pivot table but am having trouble keeping the order of how I sorted my original data frame. Below is a sample code of what my data looks exactly like and how I made my pivot table.</p>
<pre><code>simple_df = pd.DataFrame({'Skill 1': ['Python','Python','Python','Communication','Communication','Communication','Data Governance','Data Governance','Data Governance'], 'Skill 2': ['Python','Communication','Data Governance','Python','Communication','Data Governance','Python','Communication','Data Governance'],'Score':[1,0.9,0.4,0.9,1,0.4,0.4,0.4,1],'Skill 1 Type':['Programming','Programming','Programming','Written','Written','Written','Cyber','Cyber','Cyber']})
simple_df=simple_df.sort_values(by = ['Skill 1 Type'], ascending = [True], na_position = 'first')
test=simple_df.groupby(['Skill 1','Skill 2'], sort=True)['Score'].sum().unstack('Skill 2')
</code></pre>
<p>Since I sorted on &quot;Skill 1 Type&quot;, I would like to have the pivot table's y-axis labels keep the same order of how &quot;Skill 1&quot; appears in my sorted dataframe. So ideally, I would have (Data Governance, Python, Communication) rather than (Communication, Data Governance, Python) on my y axis. What are some ways I can do this? Thank you!</p>
","<python><pandas><sorting><pivot><data-governance>","2022-10-26 20:45:54","99","2","1","74214380","<p>You can save the original order and then use reindex to swap the columns and rows.</p>
<pre><code>original_order = simple_df[&quot;Skill 1&quot;].unique()
(simple_df.pivot(index=&quot;Skill 1&quot;, columns=&quot;Skill 2&quot;, values=&quot;Score&quot;)
 .reindex(index=original_order, columns=original_order))


Skill 2          Data Governance  Python  Communication
Skill 1                                                
Data Governance              1.0     0.4            0.4
Python                       0.4     1.0            0.9
Communication                0.4     0.9            1.0
</code></pre>
"
"74213734","Get all users and roles in BIM 360 data management API","<p>I am trying unsuccessfully to get all the users of a project and roles assigned to users. The errorCode AUTH-010 is not found on the Autodesk error  handling page.</p>
<p>Scope - data:read
Using 3-legged Auth to this endpoint <a href=""https://developer.api.autodesk.com/bim360/admin/v1/projects/:projectId/users"" rel=""nofollow noreferrer"">https://developer.api.autodesk.com/bim360/admin/v1/projects/:projectId/users</a>
with an access token that works for most other requests, get hubs, get project, get permissions by folder. The request return an error of the token does have privilege for the request. I am an account admin on the hub.</p>
<p>the error is</p>
<pre><code>{
    &quot;developerMessage&quot;: &quot;Token does not have the privilege for this request.&quot;,
    &quot;moreInfo&quot;: &quot;https://forge.autodesk.com/en/docs/oauth/v2/developers_guide/error_handling/&quot;,
    &quot;errorCode&quot;: &quot;AUTH-010&quot;
}
</code></pre>
<p>Is it possible to get users and user roles for a BIM 360 project?</p>
<p>Thanks</p>
","<autodesk-forge><autodesk-bim360>","2022-10-26 20:14:31","57","-1","1","74216629","<p>This endpoint requires the <code>account:read</code> scope, please ensure you have included it when getting the access tokens via the <a href=""https://forge.autodesk.com/en/docs/oauth/v2/developers_guide/overview/"" rel=""nofollow noreferrer"">Authentication (OAuth) API</a>.</p>
<p>ref: <a href=""https://forge.autodesk.com/en/docs/bim360/v1/reference/http/admin-v1-projects-projectId-users-GET/"" rel=""nofollow noreferrer"">https://forge.autodesk.com/en/docs/bim360/v1/reference/http/admin-v1-projects-projectId-users-GET/</a></p>
"
"74189809","Can I use Dimension table ‘startdate’ instead Fact table?","<p>I’m joining dim table &amp; fact table with start date. Can I use start date from dim table instead fact? If so why we need to use fact table start date? Below is the example:</p>
<pre><code>Select count(*)
    from dim_table d
    Inner join fact_table f
    On d.bizkeys = f.bizkeys
   Where currentind =‘1’ 
   And d.startdate = (select max(startdate)           from dim_table)
</code></pre>
<p>After giving startdate condition I’m getting 1.8 million records, if I give</p>
<pre><code>f.startdate =  (select max(startdate) from fact_table) 
</code></pre>
<p>I got 100 million records.
Can anyone Please clarify my doubt? Why I’m seeing this huge variation?</p>
","<sql-server><data-warehouse>","2022-10-25 06:21:47","44","1","1","74265840","<p>If you're just trying to get a list of all possible dates from the dimension (say, for a dropdown in your reporting tool that would let the user pick a date), then there's probably no reason that you would join to the fact table—unless you only want to include dates for which there's a corresponding fact record.</p>
<p>Without some sample data (or at least a little more information about the substance of the fact and dimension tables), I'm not sure I can give a better answer than that.</p>
"
"74136393","Data Warehouse Opensource Tool for ETL","<p>Can anyone suggest me an open source data warehouse for Data Engineering Tasks where i can store data through ETL ingestion and train the model and save results.</p>
","<open-source><data-warehouse>","2022-10-20 08:06:20","56","0","1","74140667","<p>you might want to check out caosdb (caosdb.org). it's a tool kit to put different data sources together and the ETL part should be covered well. the part regarding training the model, you might have to integrate another open source software into the tool.</p>
"
"74129563","How to create separate data mapping file with function names in Swift","<p>I have a macOS app that I'm creating in Swift and I have integrated an  external HID device that has a number of controls on it.
The HID part is done where I am receiving all of the hid commands from the device and I am trying to create a mapping file where I can maintain the HID key mappings in a separate swift file.
All I want in that file is the data and what I want to do is this;</p>
<ul>
<li>raw hid data is received from HID device (In ViewController)</li>
<li>Lookup the function name assigned to this hid data (In separate file)</li>
<li>Run the function that is mapped to that key. (Function located in the main ViewController)</li>
</ul>
<p>So far I have the external swift file setup with all of the mapping and that all works fine but my issue is when I try to call the looked up function in the ViewController, it says the function can't be found in the scope.</p>
<p>Initially I thought I would use a delegate but the external file isn't a viewcontroller, just a separate swift file so I don't know if I can do that?.</p>
<p>I've tried searching but everything I've found is calling a function from another ViewController which I'm not. It's very possible I'm not using the best approach and my goal is to just keep all of the mapping in a separate file as there is a lot and it woudl be easier to maintain.</p>
<p>Any suggestions are appreciated.</p>
","<swift>","2022-10-19 17:26:54","51","-1","2","74129590","<p>You can simply create a UIViewController as the external file and add it as a property to the main ViewController.</p>
<p>In the external file add this.</p>
<pre><code>@IBOutlet var uiViewController: UIViewController!
</code></pre>
<p>In the ViewController add this.</p>
<pre><code>var externalFileViewController: UIViewController!

override func viewDidLoad() {
    super.viewDidLoad()

    externalFileViewController = externalFileViewController?.loadView()

    // If we have an object then load it
    if let viewController = externalFileViewController {
        viewController.view.frame = view.frame
        viewController.view.autoresizingMask = [.flexibleHeight, .flexibleWidth]
        view.addSubview(viewController.view)
        uiViewController = viewController

    }
}
</code></pre>
<p>Now in the viewController look up the functions to be called from the external file and call them using the function name.</p>
<p>The functions are defined in the external file using @IBAction.</p>
<p>Let me know if you have any questions.</p>
"
"74129563","How to create separate data mapping file with function names in Swift","<p>I have a macOS app that I'm creating in Swift and I have integrated an  external HID device that has a number of controls on it.
The HID part is done where I am receiving all of the hid commands from the device and I am trying to create a mapping file where I can maintain the HID key mappings in a separate swift file.
All I want in that file is the data and what I want to do is this;</p>
<ul>
<li>raw hid data is received from HID device (In ViewController)</li>
<li>Lookup the function name assigned to this hid data (In separate file)</li>
<li>Run the function that is mapped to that key. (Function located in the main ViewController)</li>
</ul>
<p>So far I have the external swift file setup with all of the mapping and that all works fine but my issue is when I try to call the looked up function in the ViewController, it says the function can't be found in the scope.</p>
<p>Initially I thought I would use a delegate but the external file isn't a viewcontroller, just a separate swift file so I don't know if I can do that?.</p>
<p>I've tried searching but everything I've found is calling a function from another ViewController which I'm not. It's very possible I'm not using the best approach and my goal is to just keep all of the mapping in a separate file as there is a lot and it woudl be easier to maintain.</p>
<p>Any suggestions are appreciated.</p>
","<swift>","2022-10-19 17:26:54","51","-1","2","74141965","<p>This is one way to achieve this. It can get tedious. You can totally skip writing out a separate protocol for the delegate, but this is cleaner design.</p>
<pre><code>protocol HIDMessageDelegate: AnyObject {
  // example messages
  func message1()
  func message2()
  func message3()
}

class HIDMessageParser {
  static weak var delegate: HIDMessageDelegate?

  static func parseHIDMessage() {
    var condition = 0
    // this is where your switch statement will go and you'll parse things and call the relevant delegate method
    switch (condition) {
    default:
      delegate?.message1()
    }
  }
}

class MyViewController: UIViewController {
  override func viewDidLoad() {
    super.viewDidLoad()
    HIDMessageParser.delegate = self
  }
}

extension MyViewController: HIDMessageDelegate {
  func message1() {

  }

  func message2() {

  }

  func message3() {

  }
}
</code></pre>
"
"74128502","Dynamics budgeting data on week range, how to generate it on date level of granularity?","<p>I have a issue where I get budgeting data on week range like 1 2022 - 5 2022 meaning week 1 to week 5 - 2022.</p>
<p>What I don't I receive from Dynamics (source) is the dates between week 1 and 5 so only thing I receive is related to week 1 and also related to week 5.</p>
<p>Meaning week 2,3 and 4 it is assumed to be in the range but not any actual data for it.</p>
<p>Thats how it looks in the Budgeting table.</p>
<p>Then there is a Week table that have date ranges however it is depending on the budgeting table to show the budgeting data.</p>
<p>Budget table</p>
<p>Columns:</p>
<pre><code>StartWeekID (unique identifier)
EndWeekID (unique identifier)
DurationPerWeek (8 hours for instance)
</code></pre>
<p>Week table</p>
<pre><code>WeekID (unique identifier)
StartDate (2022-01-02)
EndDate  (2022-02-05)
WeekNumber (1 or 5 etc.)
</code></pre>
<pre><code>SELECT
    Sw.WeekNumber as 'Start Week'
    Ew.WeekNumber as 'End Week'
    Sw.StartDate,
    Ew.EndDate,
    b.DurationPerWeek
FROM Budget AS b
JOIN week AS Sw ON Sw.WeekID = b.StartWeekID
JOIN week AS Ew ON Ew.WeekID = b.EndWeekID
</code></pre>
<p>For above query result would look like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Start Week</th>
<th>End Week</th>
<th>StartDate</th>
<th>EndDate</th>
<th>DurationPerWeek</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2</td>
<td>2022-01-02</td>
<td>2022-01-16</td>
<td>60</td>
</tr>
<tr>
<td>1</td>
<td>9</td>
<td>2022-01-02</td>
<td>2022-03-06</td>
<td>60</td>
</tr>
<tr>
<td>1</td>
<td>40</td>
<td>2022-01-02</td>
<td>2022-10-09</td>
<td>30</td>
</tr>
<tr>
<td>1</td>
<td>52</td>
<td>2022-01-02</td>
<td>2023-01-01</td>
<td>2000</td>
</tr>
</tbody>
</table>
</div>
<p>Now the question is based on what I have and don't have, meaning I don't have StartDate nor EndDate in the Budget table i.e. its only on weekly level of granularity, is it still possible to make it on daily level of granularity, meaning so I can query budgets on day level? Please help with this, thanks!</p>
","<sql><sql-server><microsoft-dynamics><data-warehouse>","2022-10-19 16:04:28","42","0","1","74128665","<p>Just join Week like this:</p>
<pre><code>SELECT
    w.WeekNumber 
    w.StartDate,
    w.EndDate,
    b.DurationPerWeek
FROM Budget AS b
JOIN Week AS w
    ON w.WeekID &gt;= b.StartWeekID
    AND w.WeekID &lt;= b.EndWeek
</code></pre>
"
"74114687","Data warehouse modling - to use ID or text in a dimensional table","<p>I always doubted if I should include ID or the actual text in the dimensions. For example, see DimSalesPerson in the screenchot below - https%3A%2F%2Fwww.codeproject.com%2FArticles%2F652108%2FCreate-First-Data-WareHouse&amp;psig=AOvVaw2oGn_eyUgi8KV9m_u6PSIF&amp;ust=1666197880975000&amp;source=images&amp;cd=vfe&amp;ved=0CA0QjhxqFwoTCOjG9_6c6voCFQAAAAAdAAAAABBJ. Instead of using StoreName as text in DimSalesPerson, they used StoreID. What do you usually do in the cases like that when there is an attribute in a dimensions table A that has a stand-alone dimensions table B?</p>
<p><a href=""https://i.stack.imgur.com/EVVlI.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EVVlI.jpg"" alt=""enter image description here"" /></a></p>
","<database-design><data-modeling><data-warehouse><star-schema><star-schema-datawarehouse>","2022-10-18 16:54:12","65","0","1","74265898","<p>Dimensions should <em>never</em> link to other dimensions. The only exception that I normally make to this rule would be for dimensions with dates that would join to a date dimension. I'm sure there are other exceptions, but they're rare and I don't think they apply here.</p>
<p>So yes, the correct answer, based on what you provided, would be to recreate some of the DimStores dimension attributes inside of the DimSalesPersons dimension, based on the store that the salesperson is associated with. Depending on the type of dimension that DimStores is, your ETL processes will need to handle changes to an existing store record in both DimStores and DimSalesPersons. That's a pretty common design paradigm.</p>
"
"74107345","Can a Dimension Table contain Multiple Data Sets","<p>I am working with factual data that contains over 30+ attributes that can be used as dimension.</p>
<p>These attributes are basically one dimensional -&gt; ID, VALUE, SORTORDER. The source data from the source system also comes from a single table called Reference There are no hierarchies, no additional descriptive attributes. So creating and maintaining 30+ different Dimension table seems to overkill.</p>
<p>Can one for instance create a single Dimension table to contain all these records with a fields that specify the type of record. Then when the data is exposed through a tabular model a view is created for each type of record instead.</p>
<p><a href=""https://i.stack.imgur.com/TmBS0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TmBS0.png"" alt=""enter image description here"" /></a></p>
","<data-modeling><data-warehouse><kimball>","2022-10-18 07:38:12","102","0","1","74128269","<p>Obviously you can do whatever you want.  But if your goal is to transform the source data into a dimensional model, then the fact table should have separate columns for each attribute Status, Source, Type, ...</p>
<p>If there is no additional data associated with a, say Status, you wouldn't necessarily create a dimension for it.  It's probably a good practice, and may be required by your reporting tool to efficiently generate a list of available values.  But having categorical attribute modeled as a column on a fact table without a related dimension table isn't the end of the world.</p>
"
"74087493","How would you use AWS Data Pipeline, Elastic MapReduce, and Redshift to perform ETL and data warehousing?","<p>I'm very new to data warehousing and AWS.</p>
<p>For school, we have to make a presentation on how data warehousing can be performed using the following three technologies:</p>
<ul>
<li>Redshift</li>
<li>AWS Data Pipeline</li>
<li>Elastic MapReduce</li>
</ul>
<p>This is my understanding thus far:</p>
<ul>
<li>Redshift is the data warehouse platform where you would store your data to perform analysis and business intelligence activities.</li>
<li>AWS Data Pipeline can be used to schedule tasks and operations. Somehow it can also be used for data transformation</li>
<li>Elastic MapReduce can also be used for data transformation.</li>
</ul>
<p>I just don't understand how you would used these things together to perform data warehousing activities. Would you use the Data Pipeline to schedule ETL processes in map reduce and then transfer data to RedShift? If so, how can you do that?</p>
","<amazon-web-services><amazon-redshift><amazon-emr><data-warehouse><amazon-data-pipeline>","2022-10-16 13:27:02","45","0","1","74111271","<p><a href=""https://i.stack.imgur.com/W3h79.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W3h79.png"" alt=""data pipeline &lt;&gt; Redshift &lt;&gt; EMR job example"" /></a></p>
<p>Have explained the data flow here via diagram. We need to use EMR jobs when we need to find insights on large volume of data.</p>
<p>We can run SQL query on Redshift too in this case but assume some complex operation which can't be solved via SQL query.</p>
"
"74048095","Index creation time is high on Azure Managed Instance","<p>I am working with Azure Managed Instances for hosting a data warehouse.  For the large table loads the indexes are removed and rebuilt instead of inserting with the indexes in place.  The indexes are re-created using a stored procedure that builds them from a list kept in an admin table.  When moving from our on-prem solution to the managed instance, we have seen considerable decrease in performance when building the indexes.  The process takes roughly twice as long when running in Azure vs when running on-prem.</p>
<p>The specs for the server are higher in the Azure Managed Instance, more cores and more memory.  We have looked at IO time and tried increasing file size to increase IO but it has had a minimal impact.</p>
<p>Why would it take longer to build indexes on the same data using the same code in an Azure Managed Instance than it does on an on-pre SQL Server?</p>
<p>Is there a setting or configuration in Azure that could be changed to improve performance?</p>
","<sql><azure><indexing><data-warehouse><azure-managed-database>","2022-10-12 21:17:50","161","0","1","74051017","<p>Could you please check the transaction log file for the database. Monitor log space use by using sys.dm_db_log_space_usage. This DMV returns information about the amount of log space currently used and indicates when the transaction log needs truncation. Please see the referral link here <a href=""https://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/resource-limits?view=azuresql#file-io-characteristics-in-general-purpose-tier"" rel=""nofollow noreferrer"">sys.dm_db_log_space_usage (Transact-SQL) - SQL Server | Microsoft Docs</a></p>
<p>As creating the index will easily reach throughput limit either for data or log files, you might need to increase individual file sizes. <a href=""https://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/resource-limits?view=azuresql#file-io-characteristics-in-general-purpose-tier"" rel=""nofollow noreferrer"">Resource limits - Azure SQL Managed Instance | Microsoft Docs</a></p>
<p>You also can use this script <a href=""https://github.com/dimitri-furman/managed-instance/blob/master/MI-GP-storage-perf/MI-GP-storage-perf.sql"" rel=""nofollow noreferrer"">managed-instance/MI-GP-storage-perf.sql at master · dimitri-furman/managed-instance · GitHub</a> to determine if the IOPS/throughput seen against each database file.</p>
"
"74016176","Detecting similar columns across multiple files based on statistical profile","<p>I'm attempting to clean up a set of old files that contain sensor data measurements. Many of the files don't have headers, and the format (column ordering, etc.) is inconsistent.  I'm thinking the best that I can do in these cases is to match statistical profiles of the columns to data from files that do have good headers.  This seems like it should be simple using something like <a href=""https://github.com/ydataai/pandas-profiling"" rel=""nofollow noreferrer"">Pandas Profiling</a>, but I haven't found any examples. I'm looking for something that would calculate a score for the similarity between each column in the header-less file and each &quot;known&quot; column for which I already have headers.</p>
<p>Example Data with Headers:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Large Value Column</th>
<th>Small Value Column</th>
</tr>
</thead>
<tbody>
<tr>
<td>100</td>
<td>5</td>
</tr>
<tr>
<td>102</td>
<td>12</td>
</tr>
<tr>
<td>110</td>
<td>8</td>
</tr>
<tr>
<td>98</td>
<td>10</td>
</tr>
</tbody>
</table>
</div>
<p>Example Data with only column numbers:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>0</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr>
<td>6</td>
<td>99</td>
</tr>
<tr>
<td>9</td>
<td>105</td>
</tr>
<tr>
<td>11</td>
<td>101</td>
</tr>
<tr>
<td>14</td>
<td>100</td>
</tr>
</tbody>
</table>
</div>
<p>For the above example, I would like to automatically determine that column 1 should be added to &quot;Large Value Column&quot; and Column 0 to &quot;Small Value Column&quot;.</p>
","<python><pandas><pandas-profiling><data-quality><data-profiling>","2022-10-10 14:05:55","56","0","2","74016463","<p>That's an interesting question.</p>
<p>The solution that will work heavily depends on the data you're working with. But here's my thoughts, assuming that your data is similar to the little exampled above, just longer.</p>
<p>The first step would be to create a <strong>descriptor</strong> that takes as input a row and returns a low-dimension vector that describes this row. For instance: the mean, the standard deviation, the median, ... This should be designed according to what <em>actually</em> describes your data. Then, for each column of your dataset, you're going to have a <strong>descriptor vector</strong> and (eventually) a <strong>label</strong> (ie. the column's header), that may or not be known.</p>
<p>The next step can either be <strong>classification</strong> or <strong>clustering</strong>, depending on how much data you have and the quality of your descriptors. Look up <a href=""https://scikit-learn.org/stable/"" rel=""nofollow noreferrer"">Scikit</a> documentation about how to do so. Their API is very easy to use and comes with many examples. If you know the number of real headers, you could try to use <a href=""https://scikit-learn.org/stable/modules/clustering.html?highlight=k%20mean#k-means"" rel=""nofollow noreferrer""><strong>K-means</strong></a> and I think it would work really well. Maybe <a href=""https://scikit-learn.org/stable/modules/neighbors.html?highlight=knn"" rel=""nofollow noreferrer"">KNN</a> can also work. You probably want to look for a sorting algorithm quite 'simple' at first (it's easier to tune), and if nothing simple works then you could try more fancy approaches.</p>
<p>I hope this helps you.</p>
"
"74016176","Detecting similar columns across multiple files based on statistical profile","<p>I'm attempting to clean up a set of old files that contain sensor data measurements. Many of the files don't have headers, and the format (column ordering, etc.) is inconsistent.  I'm thinking the best that I can do in these cases is to match statistical profiles of the columns to data from files that do have good headers.  This seems like it should be simple using something like <a href=""https://github.com/ydataai/pandas-profiling"" rel=""nofollow noreferrer"">Pandas Profiling</a>, but I haven't found any examples. I'm looking for something that would calculate a score for the similarity between each column in the header-less file and each &quot;known&quot; column for which I already have headers.</p>
<p>Example Data with Headers:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Large Value Column</th>
<th>Small Value Column</th>
</tr>
</thead>
<tbody>
<tr>
<td>100</td>
<td>5</td>
</tr>
<tr>
<td>102</td>
<td>12</td>
</tr>
<tr>
<td>110</td>
<td>8</td>
</tr>
<tr>
<td>98</td>
<td>10</td>
</tr>
</tbody>
</table>
</div>
<p>Example Data with only column numbers:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>0</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr>
<td>6</td>
<td>99</td>
</tr>
<tr>
<td>9</td>
<td>105</td>
</tr>
<tr>
<td>11</td>
<td>101</td>
</tr>
<tr>
<td>14</td>
<td>100</td>
</tr>
</tbody>
</table>
</div>
<p>For the above example, I would like to automatically determine that column 1 should be added to &quot;Large Value Column&quot; and Column 0 to &quot;Small Value Column&quot;.</p>
","<python><pandas><pandas-profiling><data-quality><data-profiling>","2022-10-10 14:05:55","56","0","2","74017731","<p>As stated by <a href=""https://stackoverflow.com/users/13219173/arthur-bricq"">@Arthur Bricq</a>, the best solution depends a lot on the shape of your data. Unfortunately, here's no &quot;one solution fits all approach&quot;. Having said that, the following code was able to correctly rename the columns from your example, using the <a href=""https://pypi.org/project/fuzzywuzzy/"" rel=""nofollow noreferrer""><code>fuzzywuzzy</code></a> package:</p>
<blockquote>
<p>Note: You need to pip install <code>fuzzywuzzy</code> before running the code below. To do so, run the following command:</p>
<p><code>pip install fuzzywuzzy</code></p>
</blockquote>
<pre class=""lang-py prettyprint-override""><code>
from typing import Tuple, List
from fuzzywuzzy import fuzz
import pandas as pd


def find_similar(
    *dfs: pd.DataFrame, known_df: pd.DataFrame
) -&gt; Tuple[List[pd.DataFrame], pd.DataFrame]:
    &quot;&quot;&quot;
    Name columns in a list of dataframes based on the columns from known dataframe.

    Each of the dataframes in the list will be renamed based on the columns from the
    known dataframe. To avoid cases where two different unnamed columns match the
    same column from the known dataframe, the function matches each of the known columns
    to the unnamed column that contains the highest similarity score and then removes
    the matched columns from the list of possible columns. For example:

    &gt;&gt;&gt; x = pd.DataFrame(
    ...     {'Large Value Column' :[100, 102, 110, 98,]
    ...      'Small Value Column': [5, 12, 8, 10,]}
    ... )
    &gt;&gt;&gt; y = pd.DataFrame(
    ...     {0: [6, 9, 11, 14],
    ...      1: [20, 25, 55, 65]}
    ... )
    &gt;&gt;&gt; pd.DataFrame(
    ...     [
    ...         [
    ...             fuzz.ratio(
    ...                 list(map(str, y[col].values)), list(map(str, x[_col].values))
    ...             ) for _col in x.columns
    ...         ] for col in y.columns
    ...     ],
    ...     index=y.columns,
    ...     columns=x.columns,
    ... )
       Large Value Column  Small Value Column
    0                  73                  77
    1                  71                  74

    In the above example, &quot;Small Value Column&quot; has the highest similarity score for
    both columns &quot;0&quot; and &quot;1&quot;. However, since &quot;0&quot; has the highest similarity score
    then, &quot;1&quot; will be renamed to &quot;Large Value Column&quot;.

    Parameters
    ----------
    dfs : pd.DataFrame
        List of dataframes to rename.
    known_df : pd.DataFrame
        Pandas dataframe with the correct column names.

    Returns
    -------
    Tuple[List[pd.DataFrame], pd.DataFrame]
        List of dataframes with renamed columns, and the known dataframe.
    &quot;&quot;&quot;
    # Store the results in a list
    _dfs = []
    # Loop over the dataframes
    for df in dfs:
        # Create a dataframe that contains the similarities between the columns
        # in the known dataframe and the columns in the current dataframe.
        # The columns represent the known dataframe columns, and the index
        # represents the current dataframe columns.
        similarity_df = pd.DataFrame(
            [
                [
                    fuzz.ratio(
                        list(map(str, df[col].values)),
                        list(map(str, known_df[_col].values)),
                    )
                    for _col in known_df.columns
                ]
                for col in df.columns
            ],
            index=df.columns,
            columns=known_df.columns,
        )
        # Dictionary to map old and new column names
        rename_dict = {}

        # Keep track of the columns that have already been matched
        # to avoid naming two columns using the same name.
        used_names = []
        for col in similarity_df.max().sort_values(ascending=False).keys():
            old_name = similarity_df[~similarity_df.index.isin(used_names)][
                col
            ].idxmax()
            rename_dict[old_name] = col
            used_names.append(old_name)
        # Rename the columns in the current dataframe and append it to the list
        _dfs.append(df.rename(columns=rename_dict, errors=&quot;ignore&quot;))
    return _dfs, known_df

</code></pre>
<h2>Example</h2>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd


x = pd.DataFrame(
    {
        &quot;Large Value Column&quot;: [100, 102, 110, 98],
        &quot;Small Value Column&quot;: [5, 12, 8, 10],
    }
)
y = pd.DataFrame({0: [6, 9, 11, 14], 1: [20, 40, 55, 65]})
z = pd.DataFrame({0: [95, 80, 72, 100], 1: [0, 20, 14, 10]})

pd.concat(find_similar(y, z, known_df=x)[0])

# Returns:
#
#    Small Value Column  Large Value Column
# 0                   6                  20
# 1                   9                  40
# 2                  11                  55
# 3                  14                  65
# 0                   0                  95
# 1                  20                  80
# 2                  14                  72
# 3                  10                 100
</code></pre>
<p><a href=""https://i.stack.imgur.com/GyQGA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GyQGA.png"" alt=""enter image description here"" /></a></p>
"
"74015973","What's the difference between ETL and ELT?","<p>I know the fact that in ETL, we transform the data and Load in data warehouse.<br />
In ELT, we Load the data into data warehouse and then do the transformation.</p>
<ol>
<li><p>Apart from the above two lines, how would they exactly differ?</p>
</li>
<li><p>What's the advantage of one over the other?</p>
</li>
<li><p>In ELT, I see people telling, we leverage the power of data warehouse to do the transformation. Why transformation advantage does the warehouse provide?</p>
</li>
<li><p>In my use case, I have source data in source ADLS (source layer) and I ingest the data again into my ADLS (raw layer) as it is using azure Databricks, then take the raw layer data and transform the data using azure databricks and again store in ADLS (final layer). Now copy the final layer data from ADLS to azure sql server db as shown below.</p>
<p>Source ADLS --&gt; Raw layer ADLS [copy everything using ADB (source is mounted)]<br />
Raw layer ADLS --&gt; Final layer ADLS [Transform using ADB]<br />
Final layer ADLS --&gt; SQL Server db [copy using ADF]</p>
<p>Is it an ETL or ELT?</p>
</li>
</ol>
","<bigdata><azure-databricks><data-warehouse>","2022-10-10 13:50:33","69","3","2","74017623","<p>ETL was traditionally what most people used. Your ETL tool ran on its own infrastructure and did the transformations using its own engine before writing the data to the target database/file. This was because many databases didn't have the performance (at an acceptable cost) to be able to transform the data with the required throughput.</p>
<p>As the performance of RDBMSs improved (and the cost decreased), the RDBMS could take on the transformation workload and the ETL tool didn't need to transform the data external to the DB - hence the move to ELT</p>
"
"74015973","What's the difference between ETL and ELT?","<p>I know the fact that in ETL, we transform the data and Load in data warehouse.<br />
In ELT, we Load the data into data warehouse and then do the transformation.</p>
<ol>
<li><p>Apart from the above two lines, how would they exactly differ?</p>
</li>
<li><p>What's the advantage of one over the other?</p>
</li>
<li><p>In ELT, I see people telling, we leverage the power of data warehouse to do the transformation. Why transformation advantage does the warehouse provide?</p>
</li>
<li><p>In my use case, I have source data in source ADLS (source layer) and I ingest the data again into my ADLS (raw layer) as it is using azure Databricks, then take the raw layer data and transform the data using azure databricks and again store in ADLS (final layer). Now copy the final layer data from ADLS to azure sql server db as shown below.</p>
<p>Source ADLS --&gt; Raw layer ADLS [copy everything using ADB (source is mounted)]<br />
Raw layer ADLS --&gt; Final layer ADLS [Transform using ADB]<br />
Final layer ADLS --&gt; SQL Server db [copy using ADF]</p>
<p>Is it an ETL or ELT?</p>
</li>
</ol>
","<bigdata><azure-databricks><data-warehouse>","2022-10-10 13:50:33","69","3","2","74267759","<p>The answer from @NickW answer is a good summation. All I'd add is that much will depend on what you have the talent to support. If most of your on-deck talent is SQL based, then ELT (with transforms done in stored procedures, etc.) can make a lot more sense as your team won't have to learn to code transforms in an ETL tool. If you've got the ETL talent, then it might make more sense to do the transformations there, which will offload that processing from your data warehouse server.</p>
<p>This architectural question probably has more to do with the talent and skillsets that you have available to you.</p>
"
"74014445","Accidently delete derby.log and metastore_db folder in Spark, now tables won't save","<p>Currently, I'm working on an exercise with requires creating a Database and tables inside PySpark and working with them. The problem is I created a db before, so I decide to delete all the related things (<code>spark-warehouse folder</code>, <code>metastore_db folder</code>, <code>derby.log file</code>) before starting a new one.</p>
<p>Now when I first created a new database and new tables with the below code, it works fire.</p>
<pre><code>spark.sql(&quot;CREATE DATABASE IF NOT EXISTS asm1_db&quot;)
spark.sql(&quot;USE asm1_db&quot;)

dfQ1.write.bucketBy(2, &quot;Id&quot;).mode(&quot;overwrite&quot;).saveAsTable(&quot;asm1_db.tableQ1&quot;)
dfA1.write.bucketBy(2, &quot;ParentId&quot;).mode(&quot;overwrite&quot;).saveAsTable(&quot;asm1_db.tableA1&quot;)
</code></pre>
<p>But the table and the database won't save. For example the below code:</p>
<pre><code>spark.sql(&quot;USE asm1_db&quot;) =&gt; Can't find the database
</code></pre>
<p>I have to create the database again, even so, the tables won't save</p>
<pre><code>spark.sql(&quot;CREATE DATABASE IF NOT EXISTS asm1_db&quot;)
spark.sql(&quot;USE asm1_db&quot;)
spark.sql(&quot;select Id from tableq1&quot;).show() =&gt; Can't find the table tableq1
</code></pre>
<p>When I check the folder structure on Pycharm the <code>spark-warehouse folder</code> and all the db/tables are still there
<a href=""https://i.stack.imgur.com/ohEgl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ohEgl.png"" alt=""spark-warehouse"" /></a></p>
<p>I have to run the whole write data to the table again, but the data is very big so I don't want to run that every time. I know that <strong>saveAsTable</strong> method create a <strong>Permanent Table</strong>, not <strong>Temporary</strong></p>
<p>I think the problem is because I delete the <code>metastore_db folder</code>, <code>derby.log file</code> before. So is there anyway to restore them? I though it will recreate every time I create a new Database.</p>
","<database><apache-spark><pyspark><apache-spark-sql><data-warehouse>","2022-10-10 11:46:52","56","0","1","74112981","<p>Turn out in the SparkSession creation code. You have to add .enableHiveSupport(). For example</p>
<pre><code>spark = SparkSession \
    .builder \
    .master(&quot;local[*]&quot;) \
    .appName(&quot;SparkSQLTableDemo&quot;) \
    .enableHiveSupport() \
    .getOrCreate()
</code></pre>
"
"73994206","When to use Redshift Spectrum for your Redshift data warehouse","<p>I am still new to Redshift service and quite confused of when to use or what data to put into Spectrum.</p>
<p>Suppose I have star schema data warehouse on Redshift, should I put fact table or dim table into Spectrum(external tables from s3) for storage optimization?
Or typically data warehouse has different layers eg: landing, staging, or data vault. Should we put other layers of data into Spectrum only leave star schema data into Redshift.</p>
<p>And since data in S3 is appended only, do we need install apachi hudi or delta lake to work with Redshift Spectrum?</p>
<p>I found an aws article :<a href=""https://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/</a> states below, but still not clear.</p>
<blockquote>
<p>A few good use cases are the following:</p>
<ul>
<li>Huge volume but less frequently accessed data Heavy scan- and</li>
<li>aggregation-intensive queries Selective queries that can use</li>
<li>partition pruning and predicate pushdown, so the output is fairly    small</li>
</ul>
</blockquote>
<p>Anyone can provide real world example to explain? thanks</p>
","<amazon-web-services><amazon-redshift><delta-lake><spectrum><amazon-redshift-spectrum>","2022-10-08 03:41:56","502","1","2","73994425","<p>In general, put everything into 'normal' Amazon Redshift.</p>
<p>Redshift Spectrum is handy for accessing data stored in Amazon S3 without having to load it into the Redshift cluster, but it will not be as fast as accessing data stored in 'normal' Redshift.</p>
<p>Therefore, it is useful for rarely-accessed data or for one-off queries on a dataset without having to import the data into Redshift.</p>
<p>Do not use Spectrum as part of your normal ETL flow. One exception to this might be if you are receiving 'landing' data via Amazon S3 (eg Seed Files) -- rather than importing the tables into Redshift, they could be referenced via Spectrum. However, normal loading tools such as Fivetran can load the data directly into Redshift, which is preferable to using Spectrum.</p>
"
"73994206","When to use Redshift Spectrum for your Redshift data warehouse","<p>I am still new to Redshift service and quite confused of when to use or what data to put into Spectrum.</p>
<p>Suppose I have star schema data warehouse on Redshift, should I put fact table or dim table into Spectrum(external tables from s3) for storage optimization?
Or typically data warehouse has different layers eg: landing, staging, or data vault. Should we put other layers of data into Spectrum only leave star schema data into Redshift.</p>
<p>And since data in S3 is appended only, do we need install apachi hudi or delta lake to work with Redshift Spectrum?</p>
<p>I found an aws article :<a href=""https://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/</a> states below, but still not clear.</p>
<blockquote>
<p>A few good use cases are the following:</p>
<ul>
<li>Huge volume but less frequently accessed data Heavy scan- and</li>
<li>aggregation-intensive queries Selective queries that can use</li>
<li>partition pruning and predicate pushdown, so the output is fairly    small</li>
</ul>
</blockquote>
<p>Anyone can provide real world example to explain? thanks</p>
","<amazon-web-services><amazon-redshift><delta-lake><spectrum><amazon-redshift-spectrum>","2022-10-08 03:41:56","502","1","2","73994488","<p>This is a broad topic but I'll give a few thoughts.</p>
<p>First off Spectrum is a (often large) set of compute elements embedded in S3 that can do some aspect of the query plan.  These part centered around applying WHERE conditions and performing aggregation (GROUP BY).  There are also aspects of the query plan that cannot be perform in the S3 layer such as JOINs and advanced functions such as window functions.</p>
<p>The next thing to understand is that while these embedded compute elements are close to S3 in terms of access speed, the S3 service is far away from the Redshift cluster (network distance).  If the large amount of data stored in S3 can be pared down to a small set that is shipped to Redshift then Spectrum can be a huge performance improvement.  However, if the large amount of data stored in S3 needs to be moved to the Redshift cluster completely to perform the query then there can be a large hit to performance.</p>
<p>Spectrum can be a huge benefit; allowing for a very large amount of data to be filtered down quickly by a fleet of small compute elements.  This can result in a big win in performance and in the amount of data that can be addressed.</p>
<p>With these in mind you will want to have data in Spectrum that your query plan will want to get a subset transferred from S3 to redshift.  This in general will apply to your fact tables and not to your dim tables.  However, if your queries aren't going to apply a WHERE clause to the fact table or aggregate the data down then you won't see the advantages.  Also for this to work the WHERE clause needs to apply to a column in the fact table as JOINs cannot be done in S3 so filtering on dim columns won't help.  Similarly and GROUP BY needs to be applied only on the fact table columns or this won't reduce the data coming to Redshift from S3.</p>
<p>So fact tables.</p>
<p>Data generally gets into Redshift through S3 and this can be done with the COPY command.  You can also get data into Redshift from S3 using Spectrum.  This can be a useful tool if other tools are also using S3 for this shared data.  S3 can seem like a common data store for separate data systems. This can be useful for some data solutions.</p>
<p>You also bring up very large, infrequently used data.  Like older historical data that is usually needed but is sometimes needed.  This can be helpful in that older data can be offloaded from the Redshift cluster and the access time for this data isn't important as it is very infrequently used.  There is a potential issue - The Redshift cluster can only work on a certain size of data given it's disk space and memory.  So you can clog up your cluster if the amount of historical data is too large.  This may mean that looking at the full set of historical data in one query may not be possible.  Again if the data is aggregated or filtered in S3 this issue isn't a problem.</p>
<p>Bottom line - Spectrum is a great tool but isn't the right tool for every problem.</p>
"
"73984122","python great expectation compatible with pyspark","<p>I am implementing data quality checks using Great expectation library. does this library compatible with Pyspark does this run on multiple cores?</p>
","<python><data-quality><great-expectations>","2022-10-07 08:00:47","71","1","1","74014429","<p>Yes it is compatible with Pyspark. Here is the example.</p>
<p>datasource creation.</p>
<pre><code>datasources:
  spark_ds:
    class_name: Datasource
    execution_engine:
      module_name: great_expectations.execution_engine
      class_name: SparkDFExecutionEngine
      force_reuse_spark_context: true
    module_name: great_expectations.datasource
    data_connectors:
      spark_ds_connector:
        class_name: RuntimeDataConnector
        module_name: great_expectations.datasource.data_connector
        batch_identifiers:
          - batch_id
</code></pre>
<p>Create runtime batch request</p>
<pre><code>df=#Create your dataframe
request=RuntimeBatchRequest(
            datasource_name=&quot;spark_ds&quot;,
            data_connector_name=&quot;spark_ds_connector&quot;,
            data_asset_name=&quot;any_asset_name&quot;,  

            runtime_parameters={&quot;batch_data&quot;: df},  

            batch_identifiers={&quot;batch_id&quot;: &quot;batch_id&quot;},
        )
    
    
ge_context.run_checkpoint(checkpoint_name=&quot;checkpoint&quot;, validations=[{&quot;batch_request&quot;: request, &quot;expectation_suite_name&quot;: &quot;suite_name&quot;}])
</code></pre>
"
"73964350","Using EFFECTIVE_TS and EXPIRATION_TS on FACT tables","<p>I have a requirement to create a Fact table which stores granted_share_qty awarded to employees. There are surrounding Dimensions like SPS Grant_dim which stores info about each grant, SPS Plan Dim which stores info about the Plan, SPS Client Dim which stores info about the Employer and SPS Customer Dim which stores info about the customer. The DimKeys (Surrogate Key) and DurableKeys(Supernatural Keys) from each Dimension is added to the Fact.</p>
<p>Reporting need is &quot;as-of&quot; ie on any given date, one should be able to see the granted_share_qty as of that date (similar to account balance as of that date) along with point-in-time values of few attributes from the Grant,Plan, Client, Customer dimensions.</p>
<p>First, we thought of creating a daily snapshot table where the data is repeated everyday in the fact (unless source sends any changes). However since there could be more than 100 million grant records , repeating this everyday was almost impossible, moreover the granted_share_qty doesnt change that often so why copy this everyday?.</p>
<p>So instead of a daily snapshot we thought of adding an EFFECTIVE_DT and EXPIRATION_DT on the Fact table (like a TIMESPAN PERIODIC SNAPSHOT table if such a thing exists)
<a href=""https://i.stack.imgur.com/NjX5a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NjX5a.png"" alt=""enter image description here"" /></a></p>
<p>This reduces the volume and perfectly satisfies a reporting need like &quot;get me the granted_qty and grant details,client, plan, customer details as of 10/01/2022 &quot; will translate to  <em>&quot;select granted_qty from fact where 10/01/2022 between EFFECTIVE_DT and EXPIRATION_DT and Fact.DimKeys=Dim.DimKeys&quot;</em></p>
<p>The challenge however is to keep the Dim Keys of the Fact in sync with Dim Keys of the Dimensions. Even if the Fact doesn't change, any DimKey changes due to versioning in any of the Dimension need to be tracked and versioned in the Fact. This has become an implementation nightmare</p>
<p>(To worsen the things, the Dims could undergo multiple intraday changes , so these are to be tracked near-real-time :-( )</p>
<p>Any thoughts how to handle such situations will be highly appreciated (Database: Snowflake)
P:S: We could remove the DimKeys from the Fact and use DurableKeys+Date to join between the Facts and Type 2 Dims, but that proposal is not favored/approved as of now</p>
<p>Thanks
Sunil</p>
","<data-warehouse><dimensional-modeling>","2022-10-05 17:47:01","44","0","1","74000675","<blockquote>
<p>First, we thought of creating a daily snapshot table where the data is repeated everyday in the fact (unless source sends any changes). However</p>
</blockquote>
<p>Stop right there.  Whenever you know the right model but think it's un-workable for some reason, try harder.  At a minimum test your assumption that it would be &quot;too much data&quot;, and consider not materializing the snapshot but leaving it as a view and computing it at query time.</p>
<blockquote>
<p>... moreover the granted_share_qty doesnt change that often so why copy this everyday?.</p>
</blockquote>
<p>And there's your answer.  Use a monthly snapshot instead of a daily snapshot, and you've divided the data by 30.</p>
"
"73954156","How to make repetitive operations (loop) over a set of columns in a dataset in R","<p>I have to make a series of operations over a subset of columns. I have a set of columns which measures the same thing to different parties A, B, and C:</p>
<pre><code>id var1_A var1_B var1_C var2_A var2_B var2_C var3_A var3_B var3_C
</code></pre>
<p>So, in the example, var1_A var1_B var1_C refer to the same measurement for different parties. And var1_A, var2_A, var3_A refer to different variables for the same party A.</p>
<p>I would like to accomplish 2 things:</p>
<p>I need to create multiple data frames and merge the id with another dataframe, each one refers to one specific party. I wrote the code for each data frame individually, as the example below. The issue is that in the example it is simple. What complicates my life is that I have multiple datasets like df, and each of them contain information for multiple parties, and I end up with 50 lines of repetitive code. Is that a way to simplify?</p>
<pre><code>df_A &lt;- df %&gt;% select(id var1_A var2_A var3_A)
df_A &lt;- merge(df_A, df_merge, by=&quot;id&quot;)
df_B &lt;- df %&gt;% select(id var1_B var2_B var3_B)
df_B &lt;- merge(df_B, df_merge, by=&quot;id&quot;)
df_C &lt;- df %&gt;% select(id var1_C var2_C var3_C)
df_C &lt;- merge(df_C, df_merge, by=&quot;id&quot;)
</code></pre>
<p>The second thing I would like to accomplish is to change the variable name for df. I would like to change the variable name for all the columns that measure the same thing, but maintaining the party which it refers to. For example, say var1 refers to height, var2 refers to weight, and var3 refers to gender:</p>
<pre><code>id var1_A var1_B var1_C var2_A var2_B var2_C var3_A var3_B var3_C
</code></pre>
<p>I would like to get something like:</p>
<pre><code>id height_A height_B height_C weight_A weight_B weight_C gender_A gender_B gender_C
</code></pre>
<p>Is there a way to accomplish this with few lines of code? Or do I have to rename each of them individually (using rename command, for example)?</p>
","<r><loops><dplyr><tidyverse><data-management>","2022-10-04 22:20:59","190","1","2","73954714","<p>This is similar to @thelatemail's comment (answer) above, but with a couple of extra subsequent steps, i.e. rename the columns, pivot the data to 'long' format, split the df into groups (&quot;df_A&quot;, &quot;df_B&quot;, &quot;df_C&quot;), pivot the data back to wide, and save the dfs to your global environment:</p>
<pre class=""lang-r prettyprint-override""><code>library(tidyverse)
library(purrr)

df &lt;- data.frame(id = 1:10,
                 var1_A = runif(10),
                 var1_B = runif(10),
                 var1_C = runif(10),
                 var2_A = runif(10),
                 var2_B = runif(10),
                 var2_C = runif(10),
                 var3_A = runif(10),
                 var3_B = runif(10),
                 var3_C = runif(10))

list_of_dfs &lt;- df %&gt;%
  rename_with(.cols = starts_with(&quot;var1&quot;), ~gsub(&quot;var1&quot;, &quot;height&quot;, .x)) %&gt;%
  rename_with(.cols = starts_with(&quot;var2&quot;), ~gsub(&quot;var2&quot;, &quot;weight&quot;, .x)) %&gt;%
  rename_with(.cols = starts_with(&quot;var3&quot;), ~gsub(&quot;var3&quot;, &quot;gender&quot;, .x)) %&gt;%
  pivot_longer(-id) %&gt;%
  mutate(group = case_when(
    str_detect(name, &quot;_A&quot;) ~ &quot;df_A&quot;,
    str_detect(name, &quot;_B&quot;) ~ &quot;df_B&quot;,
    str_detect(name, &quot;_C&quot;) ~ &quot;df_C&quot;
    )) %&gt;%
  split(., .$group)

df_list &lt;- map(list_of_dfs, 
               \(x) pivot_wider(x, names_from = name,
                                values_from = value) %&gt;%
                 select(-group))

list2env(df_list, envir = .GlobalEnv)
#&gt; &lt;environment: R_GlobalEnv&gt;
ls()
#&gt; [1] &quot;df&quot;          &quot;df_A&quot;        &quot;df_B&quot;        &quot;df_C&quot;        &quot;df_list&quot;    
#&gt; [6] &quot;list_of_dfs&quot;
df_A
#&gt; # A tibble: 10 × 4
#&gt;       id height_A weight_A gender_A
#&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
#&gt;  1     1   0.417   0.693    0.320  
#&gt;  2     2   0.387   0.879    0.00590
#&gt;  3     3   0.882   0.805    0.861  
#&gt;  4     4   0.611   0.246    0.336  
#&gt;  5     5   0.795   0.185    0.680  
#&gt;  6     6   0.274   0.00675  0.568  
#&gt;  7     7   0.722   0.950    0.757  
#&gt;  8     8   0.776   0.757    0.0457 
#&gt;  9     9   0.613   0.352    0.853  
#&gt; 10    10   0.0603  0.438    0.421
</code></pre>
<p><sup>Created on 2022-10-05 by the <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex package</a> (v2.0.1)</sup></p>
<p>You can then merge/join the dfs as required. Hope this helps.</p>
"
"73954156","How to make repetitive operations (loop) over a set of columns in a dataset in R","<p>I have to make a series of operations over a subset of columns. I have a set of columns which measures the same thing to different parties A, B, and C:</p>
<pre><code>id var1_A var1_B var1_C var2_A var2_B var2_C var3_A var3_B var3_C
</code></pre>
<p>So, in the example, var1_A var1_B var1_C refer to the same measurement for different parties. And var1_A, var2_A, var3_A refer to different variables for the same party A.</p>
<p>I would like to accomplish 2 things:</p>
<p>I need to create multiple data frames and merge the id with another dataframe, each one refers to one specific party. I wrote the code for each data frame individually, as the example below. The issue is that in the example it is simple. What complicates my life is that I have multiple datasets like df, and each of them contain information for multiple parties, and I end up with 50 lines of repetitive code. Is that a way to simplify?</p>
<pre><code>df_A &lt;- df %&gt;% select(id var1_A var2_A var3_A)
df_A &lt;- merge(df_A, df_merge, by=&quot;id&quot;)
df_B &lt;- df %&gt;% select(id var1_B var2_B var3_B)
df_B &lt;- merge(df_B, df_merge, by=&quot;id&quot;)
df_C &lt;- df %&gt;% select(id var1_C var2_C var3_C)
df_C &lt;- merge(df_C, df_merge, by=&quot;id&quot;)
</code></pre>
<p>The second thing I would like to accomplish is to change the variable name for df. I would like to change the variable name for all the columns that measure the same thing, but maintaining the party which it refers to. For example, say var1 refers to height, var2 refers to weight, and var3 refers to gender:</p>
<pre><code>id var1_A var1_B var1_C var2_A var2_B var2_C var3_A var3_B var3_C
</code></pre>
<p>I would like to get something like:</p>
<pre><code>id height_A height_B height_C weight_A weight_B weight_C gender_A gender_B gender_C
</code></pre>
<p>Is there a way to accomplish this with few lines of code? Or do I have to rename each of them individually (using rename command, for example)?</p>
","<r><loops><dplyr><tidyverse><data-management>","2022-10-04 22:20:59","190","1","2","73956184","<p>A tidy way:</p>
<pre><code>require(tidyverse)

#CREATE DATA
df &lt;- data.frame(id = 1:10,
                 var1_A = runif(10),
                 var1_B = runif(10),
                 var1_C = runif(10),
                 var2_A = runif(10),
                 var2_B = runif(10),
                 var2_C = runif(10),
                 var3_A = runif(10),
                 var3_B = runif(10),
                 var3_C = runif(10))

df_merge&lt;-data.frame(id = 1:10,
                     value=11:20)
#grabs current names
nam&lt;-colnames(df)

#Create map of new names
new_names = c('var1'='height','var2'='weight','var3'='gender')

#replace the strings with new strings in map 
nam &lt;- str_replace_all(nam, new_names)

#reassign column names to dataframe
colnames(df)&lt;-nam

# loop through all letters in list assign to variable 
#pasted with &quot;df&quot; and the letter, selects columns ending with 
# letter, merges with df_ids and returns the new subset of data
#to the assigned variable name


for (letter in c('A', &quot;B&quot;, &quot;C&quot;)){

  assign(paste(&quot;df&quot;, letter, sep = '_'),
         df%&gt;%select(id, ends_with(letter))%&gt;%
           merge(df_merge, by='id'))
}
</code></pre>
"
"73910306","SQL Server Data warehousing Cube - query timeout expired error","<p>We've got about 10 data warehouses built, for some specific reason 4 of those for the last 3 days have been failing. There hasn't been any new data that has been added or major changes added to those 4.</p>
<p>The errors we are receiving look like this</p>
<blockquote>
<p>Errors in the OLAP storage engine: An error occurred while the 'MOS' attribute of the 'Lv Dim Seats Orders' dimension from the 'Seats' database was being processed.</p>
<p>OLE DB error: OLE DB or ODBC error: Query timeout expired; HYT00.
Errors in the OLAP storage engine: An error occurred while the 'Source Name' attribute of the 'Lv Dim Seats Orders' dimension from the 'Seats' database was being processed.</p>
<p>Server: The current operation was cancelled because another operation in the transaction failed.</p>
<p>Errors in the OLAP storage engine: An error occurred while the 'Channel' attribute of the 'Lv Dim Seats Orders' dimension from the 'Seats' database was being processed.</p>
<p>OLE DB error: OLE DB or ODBC error: Query timeout expired; HYT00.</p>
</blockquote>
<p>The errors all the same for all 4 of the warehouses that are failing. The sizes of the data in the fact tables behind these has not increased. Two of the 4 are small in size, 2 are larger - it isn't the cause of the issue. The timing when it runs doesn't seem to be be the direct cause either at least not one we can pin point.</p>
<p>I ran the following script to obtain timing and overlaps perhaps one was blocking the other but that doesn't seem to be the case either.</p>
<pre><code>select 
    j.name as 'JobName',
    --run_date,
    --run_time, 
    msdb.dbo.agent_datetime(run_date, run_time) as 'RunDateTime',
    h.step_id, 
    h.step_name, 
    h.run_duration
from 
    msdb.dbo.sysjobs j 
inner join
    msdb.dbo.sysjobhistory h on j.job_id = h.job_id 
where 
    j.enabled = 1  --Only Enabled Jobs
    and run_date = 20220929
order by 
    JobName, run_date, run_time desc
</code></pre>
<p>The output looks something like this</p>
<p><a href=""https://i.stack.imgur.com/ETlac.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ETlac.png"" alt=""enter image description here"" /></a></p>
<p>I am open to any suggestions - what to test and where to look.</p>
","<sql><sql-server><database><timeout><data-warehouse>","2022-09-30 14:30:47","603","0","1","73922572","<blockquote>
<p>OLE DB error: OLE DB or ODBC error: Query timeout expired; HYT00.</p>
</blockquote>
<p>Means that SSAS gave up on a long-running query.  So troubleshoot the long-running queries on the data source or increase the <a href=""https://learn.microsoft.com/en-us/analysis-services/server-properties/general-properties?view=asallproducts-allversions#externalcommandtimeout"" rel=""nofollow noreferrer"">ExternalCommandTimeout</a>.</p>
"
"73875704","React Application not rendering components when using the NavBar","<blockquote>
<p>EDIT: I am using React 18.2.0 and React router dom 5.3.3, and I recently replaced React.Strictmode with Fragment this evening in my index.jsx file.</p>
</blockquote>
<p>As title says, the application is working well except for the NavBar. When you click the sections it does not render the component onto the page, but the href changes in the browser itself. Here is my NavBar component code:</p>
<pre><code>import { Navbar, Nav, Container } from 'react-bootstrap';
import { withRouter } from 'react-router';
import { NavLink } from 'react-router-dom';
import styled, { ThemeContext } from 'styled-components';
import endpoints from '../constants/endpoints';
import ThemeToggler from './ThemeToggler';

const styles = {
  logoStyle: {
    width: 50,
    height: 40,
  },
};

const ExternalNavLink = styled.a`
  color: ${(props) =&gt; props.theme.navbarTheme.linkColor};
  &amp;:hover {
    color: ${(props) =&gt; props.theme.navbarTheme.linkHoverColor};
  }
  &amp;::after {
    background-color: ${(props) =&gt; props.theme.accentColor};
  }
`;

const InternalNavLink = styled(NavLink)`
  color: ${(props) =&gt; props.theme.navbarTheme.linkColor};
  &amp;:hover {
    color: ${(props) =&gt; props.theme.navbarTheme.linkHoverColor};
  }
  &amp;::after {
    background-color: ${(props) =&gt; props.theme.accentColor};
  }
  &amp;.navbar__link--active {
    color: ${(props) =&gt; props.theme.navbarTheme.linkActiveColor};
  }
`;

const NavBar = () =&gt; {
  const theme = useContext(ThemeContext);
  const [data, setData] = useState(null);
  const [expanded, setExpanded] = useState(false);

  useEffect(() =&gt; {
    fetch(endpoints.navbar, {
      method: 'GET',
    })
      .then((res) =&gt; res.json())
      .then((res) =&gt; setData(res))
      .catch((err) =&gt; err);
  }, []);

  return (
    &lt;Navbar
      fixed=&quot;top&quot;
      expand=&quot;md&quot;
      bg=&quot;dark&quot;
      variant=&quot;dark&quot;
      className=&quot;navbar-custom&quot;
      expanded={expanded}
    &gt;
      &lt;Container&gt;
        {data?.logo &amp;&amp; (
          &lt;Navbar.Brand href=&quot;/&quot;&gt;
            &lt;img
              src={data?.logo?.source}
              className=&quot;d-inline-block align-top&quot;
              alt=&quot;main logo&quot;
              style={
                data?.logo?.height &amp;&amp; data?.logo?.width
                  ? { height: data?.logo?.height, width: data?.logo?.width }
                  : styles.logoStyle
              }
            /&gt;
          &lt;/Navbar.Brand&gt;
        )}
        &lt;Navbar.Toggle
          aria-controls=&quot;responsive-navbar-nav&quot;
          onClick={() =&gt; setExpanded(!expanded)}
        /&gt;
        &lt;Navbar.Collapse id=&quot;responsive-navbar-nav&quot;&gt;
          &lt;Nav className=&quot;me-auto&quot; /&gt;
          &lt;Nav&gt;
            {data
              &amp;&amp; data.sections?.map((section, index) =&gt; (section?.type === 'link' ? (
                &lt;ExternalNavLink
                  key={section.title}
                  href={section.href}
                  target=&quot;_blank&quot;
                  rel=&quot;noopener noreferrer&quot;
                  onClick={() =&gt; setExpanded(false)}
                  className=&quot;navbar__link&quot;
                  theme={theme}
                &gt;
                  {section.title}
                &lt;/ExternalNavLink&gt;
              ) : (
                &lt;InternalNavLink
                  key={section.title}
                  onClick={() =&gt; setExpanded(false)}
                  exact={index === 0}
                  activeClassName=&quot;navbar__link--active&quot;
                  className=&quot;navbar__link&quot;
                  to={section.href}
                  theme={theme}
                &gt;
                  {section.title}
                &lt;/InternalNavLink&gt;
              )))}
          &lt;/Nav&gt;
          &lt;ThemeToggler
            onClick={() =&gt; setExpanded(false)}
          /&gt;
        &lt;/Navbar.Collapse&gt;
      &lt;/Container&gt;
    &lt;/Navbar&gt;
  );
};

const NavBarWithRouter = withRouter(NavBar);
export default NavBarWithRouter;
</code></pre>
<p>App.jsx:</p>
<pre><code>import React, { useEffect, useState, useContext } from 'react';
import { Navbar, Nav, Container } from 'react-bootstrap';
import { withRouter } from 'react-router';
import { NavLink } from 'react-router-dom';
import styled, { ThemeContext } from 'styled-components';
import endpoints from '../constants/endpoints';
import ThemeToggler from './ThemeToggler';

const styles = {
  logoStyle: {
    width: 50,
    height: 40,
  },
};

const ExternalNavLink = styled.a`
  color: ${(props) =&gt; props.theme.navbarTheme.linkColor};
  &amp;:hover {
    color: ${(props) =&gt; props.theme.navbarTheme.linkHoverColor};
  }
  &amp;::after {
    background-color: ${(props) =&gt; props.theme.accentColor};
  }
`;

const InternalNavLink = styled(NavLink)`
  color: ${(props) =&gt; props.theme.navbarTheme.linkColor};
  &amp;:hover {
    color: ${(props) =&gt; props.theme.navbarTheme.linkHoverColor};
  }
  &amp;::after {
    background-color: ${(props) =&gt; props.theme.accentColor};
  }
  &amp;.navbar__link--active {
    color: ${(props) =&gt; props.theme.navbarTheme.linkActiveColor};
  }
`;

const NavBar = () =&gt; {
  const theme = useContext(ThemeContext);
  const [data, setData] = useState(null);
  const [expanded, setExpanded] = useState(false);

  useEffect(() =&gt; {
    fetch(endpoints.navbar, {
      method: 'GET',
    })
      .then((res) =&gt; res.json())
      .then((res) =&gt; setData(res))
      .catch((err) =&gt; err);
  }, []);

  return (
    &lt;Navbar
      fixed=&quot;top&quot;
      expand=&quot;md&quot;
      bg=&quot;dark&quot;
      variant=&quot;dark&quot;
      className=&quot;navbar-custom&quot;
      expanded={expanded}
    &gt;
      &lt;Container&gt;
        {data?.logo &amp;&amp; (
          &lt;Navbar.Brand href=&quot;/&quot;&gt;
            &lt;img
              src={data?.logo?.source}
              className=&quot;d-inline-block align-top&quot;
              alt=&quot;main logo&quot;
              style={
                data?.logo?.height &amp;&amp; data?.logo?.width
                  ? { height: data?.logo?.height, width: data?.logo?.width }
                  : styles.logoStyle
              }
            /&gt;
          &lt;/Navbar.Brand&gt;
        )}
        &lt;Navbar.Toggle
          aria-controls=&quot;responsive-navbar-nav&quot;
          onClick={() =&gt; setExpanded(!expanded)}
        /&gt;
        &lt;Navbar.Collapse id=&quot;responsive-navbar-nav&quot;&gt;
          &lt;Nav className=&quot;me-auto&quot; /&gt;
          &lt;Nav&gt;
            {data
              &amp;&amp; data.sections?.map((section, index) =&gt; (section?.type === 'link' ? (
                &lt;ExternalNavLink
                  key={section.title}
                  href={section.href}
                  target=&quot;_blank&quot;
                  rel=&quot;noopener noreferrer&quot;
                  onClick={() =&gt; setExpanded(false)}
                  className=&quot;navbar__link&quot;
                  theme={theme}
                &gt;
                  {section.title}
                &lt;/ExternalNavLink&gt;
              ) : (
                &lt;InternalNavLink
                  key={section.title}
                  onClick={() =&gt; setExpanded(false)}
                  exact={index === 0}
                  activeClassName=&quot;navbar__link--active&quot;
                  className=&quot;navbar__link&quot;
                  to={section.href}
                  theme={theme}
                &gt;
                  {section.title}
                &lt;/InternalNavLink&gt;
              )))}
          &lt;/Nav&gt;
          &lt;ThemeToggler
            onClick={() =&gt; setExpanded(false)}
          /&gt;
        &lt;/Navbar.Collapse&gt;
      &lt;/Container&gt;
    &lt;/Navbar&gt;
  );
};

const NavBarWithRouter = withRouter
(NavBar);
export default NavBarWithRouter;
</code></pre>
<p>MainApp.jsx:</p>
<pre><code>import { BrowserRouter as Router, Route, Switch } from 'react-router-dom';
import FallbackSpinner from './components/FallbackSpinner';
import NavBarWithRouter from './components/NavBar';
import Home from './components/Home';
import endpoints from './constants/endpoints';

function MainApp() {
  const [data, setData] = useState(null);

  useEffect(() =&gt; {
    fetch(endpoints.routes, {
      method: 'GET',
    })
      .then((res) =&gt; res.json())
      .then((res) =&gt; setData(res))
      .catch((err) =&gt; err);
  }, []);

  return (
    &lt;div className=&quot;MainApp&quot;&gt;
      &lt;NavBarWithRouter /&gt;
      &lt;Router className=&quot;main&quot;&gt;
        &lt;Switch&gt;
          &lt;Suspense fallback={&lt;FallbackSpinner /&gt;}&gt;
            &lt;Route exact path=&quot;/&quot; component={Home} /&gt;
            {data
              &amp;&amp; data.sections.map((route) =&gt; {
                const SectionComponent = React.lazy(() =&gt; import('./components/' + route.component));
                return (
                  &lt;Route
                    key={route.headerTitle}
                    path={route.path}
                    component={() =&gt; (
                      &lt;SectionComponent header={route.headerTitle} /&gt;
                    )}
                  /&gt;
                );
              })}
          &lt;/Suspense&gt;
        &lt;/Switch&gt;
      &lt;/Router&gt;
    &lt;/div&gt;
  );
}

export default MainApp;
</code></pre>
<p>I appreciate any and all help on this, I've been pulling my hair out over it!</p>
","<reactjs><state><react-router-dom><data-mapping><react-bootstrap-nav>","2022-09-28 02:53:31","117","1","1","73875929","<p>Actually, besides recommending upgrading to <code>react-router-dom@5.3.3</code> or better, the other issue I see is the <code>NavBar</code> being rendered <em><strong>outside</strong></em> the <code>Router</code>. Move it <em>into</em> the <code>Router</code> and remove the <code>withRouter</code> HOC. The <code>withRouter</code> HOC only injects the route props of the routing context <em>above</em> it in the ReactTree and there is none.</p>
<p>NavBar</p>
<p>Instead of</p>
<pre><code>const NavBarWithRouter = withRouter(NavBar);
export default NavBarWithRouter;
</code></pre>
<p>use</p>
<pre><code>export default NavBar;
</code></pre>
<p>...</p>
<pre><code>import NavBar from './components/NavBar';
...

function MainApp() {
  ...

  return (
    &lt;div className=&quot;MainApp&quot;&gt;
      &lt;Router className=&quot;main&quot;&gt;
        &lt;NavBar /&gt; // &lt;-- inside router
        &lt;Switch&gt;
          &lt;Suspense fallback={&lt;FallbackSpinner /&gt;}&gt;
            &lt;Route exact path=&quot;/&quot; component={Home} /&gt;
            {data
              &amp;&amp; data.sections.map((route) =&gt; {
                const SectionComponent = React.lazy(() =&gt; import('./components/' + route.component));
                return (
                  &lt;Route
                    key={route.headerTitle}
                    path={route.path}
                    component={() =&gt; (
                      &lt;SectionComponent header={route.headerTitle} /&gt;
                    )}
                  /&gt;
                );
              })}
          &lt;/Suspense&gt;
        &lt;/Switch&gt;
      &lt;/Router&gt;
    &lt;/div&gt;
  );
}
</code></pre>
"
"73869461","How to filter rows that violates constraints deequ","<p>In order to do some unit test on my data I am using PyDeequ. Is there a way to filter out the rows which violate the defined constraints? I was not able to find anything online. Here is my code:</p>
<pre><code>df1 = (spark
       .read
       .format(&quot;csv&quot;)
       .option(&quot;header&quot;, &quot;true&quot;)
       .option(&quot;encoding&quot;, &quot;ISO-8859-1&quot;)
       .load(&quot;addresses.csv&quot;, sep = ','))

check = Check(spark, CheckLevel.Warning, &quot;Review Check&quot;)

checkResult = (VerificationSuite(spark)
    .onData(df1)
    .addCheck(
        check
        .isComplete(&quot;Nome&quot;)
        .isComplete(&quot;Citta&quot;)
        .isUnique(&quot;CAP&quot;)
        .isUnique(&quot;Number&quot;)
        .isContainedIn(&quot;Number&quot;, (&quot;11&quot;,&quot;12&quot;,&quot;13&quot;,&quot;14&quot;,&quot;15&quot;,&quot;16&quot;))
    )
    .run())

checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)
checkResult_df.show()
</code></pre>
","<python><data-quality><amazon-deequ>","2022-09-27 14:39:27","162","0","1","75538870","<p>Filtering for where <code>constraint_status</code> in <code>checkResult_df</code> is equal to <code>Failure</code> should be what you're looking for.</p>
<p>Building off the above example:</p>
<pre><code>from pydeequ.checks import Check, CheckLevel, ConstrainableDataTypes
from pydeequ.verification import VerificationResult, VerificationSuite
from pyspark.sql import functions as F

df1 = (spark
       .read
       .format(&quot;csv&quot;)
       .option(&quot;header&quot;, &quot;true&quot;)
       .option(&quot;encoding&quot;, &quot;ISO-8859-1&quot;)
       .load(&quot;addresses.csv&quot;, sep = ','))

check = Check(spark, CheckLevel.Warning, &quot;Review Check&quot;)

checkResult = (VerificationSuite(spark)
    .onData(df1)
    .addCheck(
        check
        .isComplete(&quot;Nome&quot;)
        .isComplete(&quot;Citta&quot;)
        .isUnique(&quot;CAP&quot;)
        .isUnique(&quot;Number&quot;)
        .isContainedIn(&quot;Number&quot;, (&quot;11&quot;,&quot;12&quot;,&quot;13&quot;,&quot;14&quot;,&quot;15&quot;,&quot;16&quot;))
    )
    .run())

checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)

# Added this snippet
# Filtering for any failed data quality constraints
df_checked_constraints_failures = \
    (checkResult_df
     .filter(F.col(&quot;constraint_status&quot;) == &quot;Failure&quot;))
</code></pre>
<p>It also might be helpful to alert or log these failures:</p>
<pre><code>import logging

logger = logging.getLogger(__name__)

# If any data quality check fails, log/raise exception/alert Slack
if df_checked_constraints_failures.count() &gt; 0:
    logger.info(
      df_checked_constraints_failures.show(n=df_checked_constraints_failures.count(),
                                           truncate=False)
    )
   # maybe raise exception here
   # maybe send POST message to Slack webhook for channel that monitors applications
</code></pre>
"
"73812846","how to create a azure data warehouse using parquet files incrementally","<p>I am working on a project involving incremental loading data
I need to implement an azure data warehouse in the following specifications:</p>
<p>example situation:
I have 2 parquet files having the same structure, one of them is in the data lake and the other is already loaded to a table in a dedicated SQL-pool.</p>
<p>what steps should i go through to end up with a table that merges the 2 files (updating existing columns using a specific id and inserting new column when not found)</p>
<p>I would prefer not using external tables as they are slower in performance</p>
","<tsql><parquet><data-warehouse><azure-synapse><incremental-build>","2022-09-22 10:07:36","457","0","2","73855682","<p>The target table has to be the same table in which the 2nd parquet file has already been loaded?</p>
<p>You could in any case define a simple Synapse pipeline in which you read both the parquet and the table from the dedicated sql pool, merge the two data flows, and the sink the result to the target sql table by means of an upsert</p>
<p>References:</p>
<ul>
<li>[https://learn.microsoft.com/en-us/azure/data-factory/data-flow-alter-row#merges-and-upserts-with-azure-sql-database-and-azure-synapse][1]</li>
<li>[https://www.taygan.co/blog/2018/04/20/upsert-to-azure-sql-db-with-azure-data-factory]</li>
</ul>
"
"73812846","how to create a azure data warehouse using parquet files incrementally","<p>I am working on a project involving incremental loading data
I need to implement an azure data warehouse in the following specifications:</p>
<p>example situation:
I have 2 parquet files having the same structure, one of them is in the data lake and the other is already loaded to a table in a dedicated SQL-pool.</p>
<p>what steps should i go through to end up with a table that merges the 2 files (updating existing columns using a specific id and inserting new column when not found)</p>
<p>I would prefer not using external tables as they are slower in performance</p>
","<tsql><parquet><data-warehouse><azure-synapse><incremental-build>","2022-09-22 10:07:36","457","0","2","73862933","<p>First of all, you can not update a parquet file without overwriting existing data which means you can not update rows or insert new records directly, but there is a way around.</p>
<p>You need to create layers of parquet files like,</p>
<ol>
<li>parquet files which contain your historical or old record.</li>
<li>parquet files which contain your incremental record.</li>
<li>parquet files created after merging the above two
Once you have these parquet files you overwrite historical parquet file with merged parquet file , and for every iteration you update incremental file.</li>
</ol>
<p>To merge these parquet files you can either use inbuilt tools of azure synapse copy activity or can convert parquet files in data frames, merge those data frames and create a parquet files from merged dataframes</p>
"
"73812304","Delta live tables data quality checks","<p>I'm using delta live tables from Databricks and I was trying to implement a complex data quality check (so-called expectations) by following <a href=""https://i.stack.imgur.com/zDR5e.png"" rel=""nofollow noreferrer"">this guide</a>. After I tested my implementation, I realized that even though the expectation is failing, the tables dependent downstream on the source table are still loaded.</p>
<p>To illustrate what I mean, here is an image describing the situation.</p>
<p><a href=""https://i.stack.imgur.com/zDR5e.png"" rel=""nofollow noreferrer"">Image of the pipeline lineage and the incorrect behaviour</a></p>
<p>I would assume that if the report_table fails due to the expectation not being met (in my case, it was validating for correct primary keys), then the Customer_s table would not be loaded. However, as can be seen in the photo, this is not quite what happened.</p>
<p>Do you have any idea on how to achieve the desired result? How can I define a complex validation with SQL that would cause the future nodes to not be loaded (or it would make the pipeline fail)?</p>
","<databricks><delta-live-tables>","2022-09-22 09:28:42","580","1","1","73923489","<p>The default behavior when expectation violation occurs in Delta Live Tables is to load the data but track the data quality metrics (retain invalid records). The other options are : ON VIOLATION DROP ROW and ON VIOLATION FAIL UPDATE. Choose &quot;ON VIOLATION DROP ROW&quot; if that is the behavior you want in your pipeline.
<a href=""https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-expectations.html#drop-invalid-records"" rel=""nofollow noreferrer"">https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-expectations.html#drop-invalid-records</a></p>
"
"73810212","Is Azure Synapse Link a good way of loading the data in a Data Warehouse?","<p>Azure Synapse Analytics is the datawarehouse solution from Azure.</p>
<p>There are 3 ways to load the data into the warehouse:</p>
<ol>
<li><p>COPY statement</p>
</li>
<li><p>PolyBase</p>
</li>
<li><p>Bulk insert</p>
</li>
</ol>
<p>The fastest and most scalable way to load data is through the COPY statement or the PolyBase.</p>
<p>However now it is also possible to load the data through <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/synapse-link"" rel=""nofollow noreferrer"">Synapse Links</a>. Which allows near-real time data.</p>
<p>But I do not see any documentation referring to Synapse Links being used in a traditional Data Warehouse for analytics.</p>
<p>The use cases in the <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/synapse-link-use-cases"" rel=""nofollow noreferrer"">documentation</a> are:</p>
<ul>
<li>Supply chain analytics, forecasting &amp; reporting</li>
<li>Real-time personalization</li>
<li>Predictive maintenance, anomaly detection in IOT scenarios</li>
</ul>
<p>Which are use cases that need real time data.</p>
<p>I do not need near real time data. Therefore I assume &quot;Synapse Link&quot; has some disadvantages for a traditional data warehouse solution.</p>
<p>Could someone please tell me their knowledge about using &quot;Synapse Link&quot; in a traditional analytics data warehouse ?</p>
<p>Thanks in advance</p>
","<azure><data-warehouse><azure-synapse>","2022-09-22 06:28:20","201","0","1","73877299","<p>With &quot;Traditional datawarehouse solution&quot; I assume you have ETL processes, that load/refresh your DWH say once a day.</p>
<p>The Synapse Link is a very convenient way to import Cosmos DB or Dataverse Data into a Data Lake connected to Synapse. The &quot;real time&quot; part of it shouldn't bother you, because you can always use batch jobs (dataflows) to load the data periodically from the lake into your datawarehouse.</p>
<p>With the Synapse Link you save time and development effort to bring the data properly from the Cosmos DB or Dataverse into your analytical environment. It works great for us.</p>
"
"73776932","In Data Warehousing, Should a Data Model have less+large Dimension Tables or split into more+small Dimension Tables?","<p>I've just started learning Data Modeling in DW. And now I'm a bit confused about choosing with field to build a Dimension Table and whether should I split into many DIM tables
For example, I have a Employee Table and it's columns as below:</p>
<pre><code>    EmployeeID
    FirstName
    LastName
    State
    Age
    Department
    Position
    Skin Color
    Hair Color
    Sale Amount (Fact)
</code></pre>
<p><strong>Emp_DIM table</strong> content every column above <strong>except Sale Amount</strong></p>
<p>and a <strong>Fact table</strong> that contains</p>
<pre><code>EmployeeID (Surrogate Key)
Sale Amount (Fact)
</code></pre>
<p>Ok, put the Date DIM aside. I want to ask about the Emp_DIM table</p>
<ol>
<li>Should I add <strong>Department_DIM</strong>(ID, Name, HeadPerson), <strong>State_DIM</strong> (ID, Name, StateCode), <strong>Position_DIM</strong> (ID, Name, Description), and <strong>Color_DIM</strong> (ID, Name) to represent <strong>Department</strong>, <strong>State</strong>, <strong>Position</strong>, <strong>Color</strong>? Then the <strong>Fact_Table</strong> will also have <strong>DepartmentID</strong>, <strong>StateID</strong>,... in it. Because sometimes I saw <strong>Department_DIM</strong>, <strong>Position_DIM</strong> and <strong>State_DIM</strong> table on the internet</li>
<li>How can I know which attribute should be a <strong>separate DIM_table</strong>? Because in the above example, I can still query everything like Total Sale Amount per Department, per State, per Position,... with SQL and join with 1 DIM table only.</li>
</ol>
<p>According to what I read on the internet, the DIM table should have as many fields as possible, and contain descriptive information. But I'm still confused and not sure.</p>
","<database><data-modeling><data-warehouse><dimensional-modeling><fact-table>","2022-09-19 17:04:52","136","0","2","73779312","<blockquote>
<p>Should I add Department_DIM</p>
</blockquote>
<p>If a Department has other attributes you need to report on, then you should consider splitting it out.  This &quot;normalization of dimensions&quot; is also called &quot;snowflaking&quot;, or moving from a star schema to a snowflake schema.  But it's totally optional and people do it both ways.</p>
<blockquote>
<p>Then the Fact_Table will also have DepartmentID</p>
</blockquote>
<p>This a <em>separate</em> decision.  This is denormalizing by putting non-key dimension attributes directly on your fact table, and it's a performance optimization.  You <em>might</em> do this for a few key attributes.</p>
"
"73776932","In Data Warehousing, Should a Data Model have less+large Dimension Tables or split into more+small Dimension Tables?","<p>I've just started learning Data Modeling in DW. And now I'm a bit confused about choosing with field to build a Dimension Table and whether should I split into many DIM tables
For example, I have a Employee Table and it's columns as below:</p>
<pre><code>    EmployeeID
    FirstName
    LastName
    State
    Age
    Department
    Position
    Skin Color
    Hair Color
    Sale Amount (Fact)
</code></pre>
<p><strong>Emp_DIM table</strong> content every column above <strong>except Sale Amount</strong></p>
<p>and a <strong>Fact table</strong> that contains</p>
<pre><code>EmployeeID (Surrogate Key)
Sale Amount (Fact)
</code></pre>
<p>Ok, put the Date DIM aside. I want to ask about the Emp_DIM table</p>
<ol>
<li>Should I add <strong>Department_DIM</strong>(ID, Name, HeadPerson), <strong>State_DIM</strong> (ID, Name, StateCode), <strong>Position_DIM</strong> (ID, Name, Description), and <strong>Color_DIM</strong> (ID, Name) to represent <strong>Department</strong>, <strong>State</strong>, <strong>Position</strong>, <strong>Color</strong>? Then the <strong>Fact_Table</strong> will also have <strong>DepartmentID</strong>, <strong>StateID</strong>,... in it. Because sometimes I saw <strong>Department_DIM</strong>, <strong>Position_DIM</strong> and <strong>State_DIM</strong> table on the internet</li>
<li>How can I know which attribute should be a <strong>separate DIM_table</strong>? Because in the above example, I can still query everything like Total Sale Amount per Department, per State, per Position,... with SQL and join with 1 DIM table only.</li>
</ol>
<p>According to what I read on the internet, the DIM table should have as many fields as possible, and contain descriptive information. But I'm still confused and not sure.</p>
","<database><data-modeling><data-warehouse><dimensional-modeling><fact-table>","2022-09-19 17:04:52","136","0","2","74267865","<p>The employee dimension and sales fact table that you showed here are intuitive and logical; that's a good start. On the surface I don't see any reason to split those up, as you're just going to create more surrogate keys to manage.</p>
<p>I would only split your dimension if there's a need for a fact to reference those entities separately. Here's an example of why this might be necessary.</p>
<p>For example: Jane Smith (an employee) sold product X at location B because she was covering for someone else that day, but she is normally assigned to location A. Ergo, her employee dimension record would show that she is normally associated with location A, but the fact record for that particular sale would show it as having occurred at location B.</p>
<p>That's a good example of why you'd have a separate location dimension ... but you'll note that I'm still including the employee's assigned location in the employee dimension.</p>
<p>But if there's not a good reason compelling you to split your dimension, then I wouldn't. It's just going to make your schema more complex, your ETL's (or ELT's) more complex, etc.</p>
"
"73762955","Should I add Foreign Key constraint when creating Fact Table in SQL?","<p>The question is a bit naive. But when I learned, it's said that you should add both Primary Key and Foreign Key to Fact Table as below:</p>
<p><a href=""https://i.stack.imgur.com/QGq7V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QGq7V.png"" alt=""LearnedCourse"" /></a></p>
<p>About the <strong>Primary Key</strong> in Fact Table, there're many posts on the internet about this and I've already got some clues. But about <strong>Foreign Key</strong>, when I did some research, I found out that when creating Fact Table on SQL, they never add Foreign Key constraint to any columns at all, which confuse me.</p>
<p>Here is what I found on <a href=""https://www.ibm.com/docs/en/informix-servers/12.10?topic=database-create-table-statement-dimension-fact-tables"" rel=""nofollow noreferrer"">IBM website</a> (other websites are nearly the same, no FK constraint at all when creating Fact Table)</p>
<pre><code>CREATE TABLE sales 
( 
customer_code  INTEGER,
district_code  SMALLINT,
time_code      INTEGER,
product_code   INTEGER,
units_sold     SMALLINT,
revenue        MONEY(8,2),
cost           MONEY(8,2),
net_profit     MONEY(8,2)
);
</code></pre>
<p>But I expect some FK constraints reference to Dimension Tables' Surrogate Key. I know that in SSIS we will look up the key anyways, but I'm still not sure should I add FK when creating Fact Table on SQL.</p>
","<sql><database><ssis><data-warehouse><fact-table>","2022-09-18 12:57:06","42","0","1","73764925","<p>If your database has not-enforced foreign keys like SQL Server does then you absolutely should have foreign keys on your fact table.  Then if you decide do turn of foreign key enforcement and manage data integrity in ETL you can make that tradeoff.</p>
<p>If your data warehouse is very large and the cost of enforcing foreign keys is prohibitive, then you might omit them.  But the general practice of omitting FKs in a data warehouse dates from the bad old days when a terabyte was a considered huge, and should be left behind.</p>
"
"73754069","Need approach on real time analytics","<p>My team wants to provide dashboards to users(store owners) with kpis specific to them, for e.g orders placed in their stores, total orders cancelled, total sales, deliveries completed. etc. This dashboard should be updated near real time (say delay of 15 mins). It will be integrated with the core app where orders are placed.</p>
<p>One route we are assessing is to to replicate all required tables to a warehouse, lets say a db for analytics, join tables, keep the grain at transaction level and save this massive table to the same warehouse. All these steps should be done in 15 mins. The dashboard will then calculate kpis over this massive table. I think this is not the most efficient way.</p>
<p>My inclination is to not warehouse the tables and directly let the dashboards query on live tables. as per the current session, the filter at user level will automatically eliminate majority of records. So it would be fast enough.</p>
<p>Streaming tools like Spark streaming + Kafka may not help as i have to listen to many tables for updates and then aggregate.</p>
<p>Any suggestions on what could be the best approach? Please help</p>
","<analytics><reporting><dashboard><data-warehouse><business-intelligence>","2022-09-17 10:29:18","32","-1","1","73756419","<p>Doing analytics on your production transactional system is generally a bad idea (and why we have separate warehouse systems) because</p>
<ol>
<li>It can affect the performance of the transactional processes</li>
<li>Data structures that effectively support OLTP tend to be very different to those that effectively support OLAP</li>
</ol>
<p>As you would only load the deltas into your warehouse and calculate your KPIs against them, and not your whole dataset, then you shouldn’t have performance issues. This is a pretty common pattern</p>
<p>But as David mentions, this is really just opinions as you don’t provide enough information and are asking a very broad question</p>
"
"73704772","Surrogate keys in star schema hierarchy dimension","<p>Is it necessary to have surrogate keys for each hierarchy level above the lowest level in a dimension table?</p>
<pre><code>Row  City_Key  City_Name    State
1     1234      Chicago    Illinois
2     3245      Dallas      Texas
3     4563      Huston      Texas
4     3457      Seattle    Washington
</code></pre>
<p>vs</p>
<pre><code>Row  City_Key  City_Name  State_Key  State
1     1234      Chicago      535    Illinois
2     3245      Dallas       659     Texas
3     4563      Huston       659     Texas
4     3457      Seattle      912   Washington
</code></pre>
<p>If so, how would I go about generating surrogate keys for levels in the hierarchy with SQL if it would not suffice to have an auto-incrementing key which would change per row like the lowest level key?</p>
<p>Would it be better to use a snowflake schema with normalized hierarchy dimensions or perhaps create/manage a denormalized hierarchy dimension table through joining a normalized hierarchy?</p>
","<data-warehouse><denormalization><dimensional-modeling><star-schema><surrogate-key>","2022-09-13 14:31:16","63","0","1","73708159","<blockquote>
<p>Is it necessary to have surrogate keys for each hierarchy level above the lowest level in a dimension table?</p>
</blockquote>
<p>No.  In a star schema there is no need, as the attribute hierarchies are modeled as non-key columns of a single dimension table.</p>
<p>In a snowflake design, where each level of the hierarchy is modeled as a separate table, such keys would of course be required.</p>
"
"73692007","Is it a good practice to separate roles with 'usage' permissions on data warehouse compute from other roles which provide access to databases?","<p>With the ability to allow secondary roles, is it a good practice to separate roles with 'usage' permissions on data warehouse compute from other roles which provide access to databases, schemas. tables? As example, I create a role called 'USE_DEV_DW&quot;. This role is granted usage to the DEV data warehouse compute. In parallel, I create a role that has usage on a specific set of databases and schemas. I then grant both roles to a user so that they can query the provided database tables using the separately provided compute (data warehouse). This way I can control what compute users have access to separate from what objects they can query. I don't see this being possible without the ability to provide for secondary roles? Looking to see if others have done something like this.</p>
","<snowflake-cloud-data-platform>","2022-09-12 15:55:00","75","0","1","73692359","<p>Even though there is no guidelines from Snowflake how to create the roles, the way you described makes sense.</p>
<p>When you have two layers of roles, let's call them <em>database</em> and <em>functional</em>,
it will be easier to create role hierarchies and assign them to users.</p>
<p>For illustration purposes assuming you have <strong>DEVELOPERS</strong> and <strong>ANALYSTS</strong> as your groups and you have <strong>SALES</strong> and <strong>PRODUCTS</strong> databases</p>
<p>You can create your roles and role hierarchies like this</p>
<p><em>Database Roles</em></p>
<pre><code>R_DB_SALES_READ_WRITE
R_DB_SALES_READ

R_DB_PRODUCTS_READ_WRITE
R_DB_PRODUCTS_READ
</code></pre>
<p><em>Functional Roles</em></p>
<pre><code>R_DEVELOPER
R_ANALYSTS
</code></pre>
<p><em>Role Hiercharchy</em></p>
<pre><code>R_DEVELOPER
  R_DB_SALES_READ_WRITE
  R_DB_PRODUCTS_READ_WRITE

R_ANALYSTS
  R_DB_SALES_READ
  R_DB_PRODUCTS_READ
</code></pre>
<p>With this you only need to assign functional roles to users</p>
"
"73687115","How to select different dates from one column in DATEDIFF sql","<p>I'm working on a data warehouse where I have one fact table 'Inventory' and three dimension tables: Product, Location and Time. Each dimension table is linked to the fact table with foreign key on their id.</p>
<p>In my fact table, I have two different dates, each references the primary key idTime in my Time table dimension.</p>
<p>My problem is the following: I want to find the average difference of days between two dates using DATEDIFF statement. Here is my query:</p>
<pre><code>select P.nameProduct, AVG (DATEDIFF(
        (select T.fulldate from Time as T, Inventory as I 
                        WHERE I.idTime_stockout = T.idTime),
        (select T.fulldate from Time as T, Inventory as I 
                        WHERE I.idTime_resupply = T.idTime)
    )) as Time_to_stock_out
FROM Inventory as I, Product as P, Time as T
WHERE I.idProduct = P.idProduct
GROUP BY P.nameProduct;
</code></pre>
<p>'fulldate' represent the date column in my Time dimension table.
'idTime_stockout' and 'idTime_stockout' represent two date column in my Fact table which references my Time table</p>
<p>In the average datediff i only want the dates that meet my criteria and not all the dates of the column. So for the first argument I select all the dates of the 'idTime_stockout' column and for the second argument all the dates of the 'idTime_resupply' column. And after I apply my conditions with a WHERE statement.</p>
<p>But I have a problem because the query returns the following error message:</p>
<pre><code>ERROR 1242 (21000): Subquery returns more than 1 row
</code></pre>
<p>Can you help me to resolve this problem please ?</p>
<p>Quentin</p>
","<mysql><sql><data-warehouse><datediff><star-schema>","2022-09-12 09:27:53","98","1","1","73687638","<p>maybe,you should using 'limit 1' after the both subquery in the DateDiff field?</p>
<pre class=""lang-sql prettyprint-override""><code>select P.nameProduct,
AVG(DATEDIFF((select T.fulldate from Time as T,
Inventory as I WHERE I.idTime_stockout = T.idTime limit 1),
(select T.fulldate from Time as T, 
Inventory as I WHERE I.idTime_resupply = T.idTime limit 1)))
 as Time_to_stock_out
FROM Inventory as I, Product as P WHERE I.idProduct = P.idProduct GROUP BY P.nameProduct; 
</code></pre>
"
"73686667","Approach or tools to transform raw data into usable formats - Data Consolidation, Data Preparation","<p>We are working on data preparation activity for a Datawarehousing project to build an enterprise level data warehouse. As an intermediate layer, we are bringing all the source database information into Hadoop datalake as hive tables.</p>
<p>In our use case, we have around 3000+ source tables. We would like to consolidate the  source data into a new schema in bigdata datalake. As part of this, we need to do the data profiling / data preparation for all the source database tables by analysing each and every columns to map/group into the respective meaningful destination table.</p>
<p>Requesting suggesting from the experts on any tools (open source or licensed) to easy the process.</p>
<p>Thanks,
Sakthivel</p>
","<data-warehouse><data-wrangling><data-preprocessing>","2022-09-12 08:47:54","25","0","1","73689030","<p>Here are a few questions to better understand the use case -</p>
<ol>
<li>Does the preparation requires joins?</li>
<li>SQL / NoSQL?</li>
<li>Cloud (Which one?) / On-prem?</li>
<li>Can the process happen in real-time?</li>
</ol>
"
"73676274","In Azure synapse SQL POOL getting Setting AnsiWarnings to 'OFF' is not supported how to turn it off then","<p>In Azure synapse SQL POOL I am getting Setting AnsiWarnings to 'OFF' is not supported warning message  how to turn it off then.
While executing my store procedure I am getting &quot;Null value is Eliminated by an Aggregate or Other SET Operation&quot; this error that is why I wanted to turn off so that it can execute for those null values as well.</p>
<p>Any solution Will help
Thank you</p>
","<azure><data-warehouse><azure-synapse>","2022-09-11 01:43:01","78","0","1","73682336","<p>This</p>
<blockquote>
<p>null value is Eliminated by an Aggregate or Other SET Operation</p>
</blockquote>
<p>Is not a error, it's only a warning.  It doesn't stop the results from being sent to the client.</p>
<p>If a client treats this as an error, that's a bug.  You can work around it as mentioned in the other comments.</p>
"
"73655107","How to find IP address to connect in AWS data warehouse","<p>I need to connect data warehouse on AWS but the administrator asked me this:</p>
<p>&quot;IP address that you're going to connect to the data warehouse from?&quot;</p>
<p>Which IP he is referring to, is it my router's IP? Or the internet company they generate?</p>
<p>How do I find out which IP he is talking about?</p>
<p>Thank you so much.</p>
","<windows><amazon-web-services><networking><ip><warehouse>","2022-09-08 21:14:52","21","0","1","73655142","<p>If you're on a private networks connecting to somewhere on the same private network then it would be the IP address of your computer.</p>
<p>If you're going over the internet from a private network you will most likely traverse a Network Address Translation (NAT) proxy. It will have a public ip address assigned from your ISP. You can find out that IP address by visiting one of several services or Google &quot;what's my ip&quot; and it will tell you your public facing NAT address.</p>
<p>Be careful that an external NAT address may change over time.</p>
"
"73646830","What inference can be made out of Baseline drift distance in data quality monitoring sagemaker?","<p><a href=""https://i.stack.imgur.com/GU4v0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GU4v0.png"" alt=""enter image description here"" /></a></p>
<p>How is the above distance calculated and can I set my own threshold?</p>
","<amazon-web-services><amazon-sagemaker>","2022-09-08 09:35:36","120","0","1","73666111","<p>You can find information on how the distributions are compared here (see <code>distribution_constraints</code> in the table):</p>
<p><a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-constraints.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-constraints.html</a></p>
<p>You can change the threshold in the constraint file to what you would like.</p>
<p>The baseline computes baseline schema constraints and statistics for each feature using Deequ. If youwould like more details you can take a look at the implementation here:</p>
<p><a href=""https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/analyzers/Distance.scala"" rel=""nofollow noreferrer"">https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/analyzers/Distance.scala</a></p>
"
"73632602","google play transfering data via link to app","<p>[context part]  I am currently on vacation in austria, and decided to buy some tickets for the zoo in insbruck. After typing in my data... a link to their app in the google play store. And after downloading it, google play didnt show the usual open button, to open the app, rather it showed a button saying &quot;continue&quot; and after clicking it, the app opened with my data already in place</p>
<p>so here is my question: how may they have done it, and how can i implement it in my own app / website</p>
<p>thanks a lot in advance, would be nice if there was a easy way of doing this</p>
<p>EDIT: idk, may they have shared the session-token somehow, used the ip of my mobile?</p>
","<web><google-play><data-management>","2022-09-07 08:51:38","31","1","1","73931981","<p>You can achieve this using <a href=""https://firebase.google.com/docs/dynamic-links"" rel=""nofollow noreferrer"">Firebase Dynamic Links</a></p>
<p><a href=""https://i.stack.imgur.com/C8l8I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/C8l8I.png"" alt=""enter image description here"" /></a></p>
<p>this is done by <a href=""https://firebase.google.com/docs/dynamic-links/android/receive#add-an-intent-filter-for-deep-links"" rel=""nofollow noreferrer"">Add an intent filter for deep links</a></p>
<blockquote>
<p>As with plain deep links, you must add a new intent filter to the activity that handles deep links for your app. The intent filter should catch deep links of your domain, since the Dynamic Link will redirect to your domain if your app is installed. This is required for your app to receive the Dynamic Link data after it is installed/updated from the Play Store and one taps on <strong>Continue button</strong>. In AndroidManifest.xml:</p>
</blockquote>
"
"73584002","Model scd by using role-playing?","<p>I have this model, but I am unable to find if its “correct”, and what is the technical name for it.</p>
<p>I am using Power BI, and for example, I have a <strong>Clients</strong> table:</p>
<p><a href=""https://i.stack.imgur.com/vjHHA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vjHHA.png"" alt=""enter image description here"" /></a></p>
<p>In my model (cube) I have it twice; I will have tables: <strong>Client</strong> and <strong>Client Current</strong></p>
<p>Data sources:
The source for <strong>Client</strong> is  (Select * from client)
The source for <strong>Client Current</strong> is  (Select * from client where Valid=1)</p>
<p>And of course in my fact I have:</p>
<p><a href=""https://i.stack.imgur.com/eoy8c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eoy8c.png"" alt=""enter image description here"" /></a></p>
<p>What is the name for this setup?  (one connects through client_surrogate key and the other from client_anotherkey)</p>
<p><a href=""https://i.stack.imgur.com/3Kb32.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3Kb32.png"" alt=""enter image description here"" /></a></p>
<p>I could be wrong, but I see SCD type 2 as a subset of ‘Role-Playing dimensions’?</p>
","<powerbi><data-modeling><data-warehouse><kimball>","2022-09-02 14:21:40","47","0","2","73584400","<p>SCD type 2 is a DWH technique that tracks the historical changes by adding a row to dimension table as changes are tracked and recorded.</p>
<p>Role Playing is a concept generally related to the date columns. For example:
there can be 2 foreign key date columns in fact tables; but you can't join these 2 keys to dimension tables; so you create a 2 different date tables with unique column names in order to have a relationship between them with original data table included in the relationship: See below:</p>
<p><a href=""https://i.stack.imgur.com/dmkL5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dmkL5.png"" alt=""Role Playing"" /></a></p>
<p>Your Model looks more like SCD Type 2.</p>
<p>client_surrogate key (FK : foreign key)
client_anotherkey (NK: Natural key )</p>
<p>I hope This info is enough for you.</p>
"
"73584002","Model scd by using role-playing?","<p>I have this model, but I am unable to find if its “correct”, and what is the technical name for it.</p>
<p>I am using Power BI, and for example, I have a <strong>Clients</strong> table:</p>
<p><a href=""https://i.stack.imgur.com/vjHHA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vjHHA.png"" alt=""enter image description here"" /></a></p>
<p>In my model (cube) I have it twice; I will have tables: <strong>Client</strong> and <strong>Client Current</strong></p>
<p>Data sources:
The source for <strong>Client</strong> is  (Select * from client)
The source for <strong>Client Current</strong> is  (Select * from client where Valid=1)</p>
<p>And of course in my fact I have:</p>
<p><a href=""https://i.stack.imgur.com/eoy8c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eoy8c.png"" alt=""enter image description here"" /></a></p>
<p>What is the name for this setup?  (one connects through client_surrogate key and the other from client_anotherkey)</p>
<p><a href=""https://i.stack.imgur.com/3Kb32.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3Kb32.png"" alt=""enter image description here"" /></a></p>
<p>I could be wrong, but I see SCD type 2 as a subset of ‘Role-Playing dimensions’?</p>
","<powerbi><data-modeling><data-warehouse><kimball>","2022-09-02 14:21:40","47","0","2","73584402","<p>This is a type-2 SCD.  Client_SurrogateKey is the dimension key.  And Client_AnotherKey is (typically) the key from the source system table.  Note that in a SCD Client_AnotherKey is not a key of the dimension table, as there are multiple rows with the same value.</p>
<p>It's unrelated to a Role-Playing Dimension.</p>
<p>The only thing atypical here is that the fact table has both keys.  Typically the fact table has only the dimension key, and would have to join the dimension to get the Client_AnotherKey.</p>
<p>But it's not unusual in dimensional models to copy (or denormalize) a dimension non-key attribute onto the fact table for convenience.</p>
"
"73580807","Data governance Snowflake unload/copy to export data","<p>I perform data unload form Snowflake to s3 or by using Snowql localy.
I'd like to know if there's any kind of data tracing (for data governance) to always record or tag and save somewhere in Snowflake that a data was unloaded.
Thanks</p>
","<snowflake-cloud-data-platform><data-governance>","2022-09-02 09:55:26","35","1","1","73583724","<p><code>COPY INTO &lt;location&gt;</code> used for data unloading will leave trace in
<a href=""https://docs.snowflake.com/en/sql-reference/account-usage/access_history.html"" rel=""nofollow noreferrer"">ACCESS_HISTORY</a>:</p>
<blockquote>
<p>This Account Usage view can be used to query the access history of Snowflake objects (e.g. table, view, column) within the last 365 days (1 year).</p>
<p>This view supports write operations of the following type:</p>
<p>Data unloading statements:</p>
<ul>
<li>COPY INTO internalStage FROM TABLE</li>
<li>COPY INTO externalStage FROM TABLE</li>
<li>COPY INTO externalLocation FROM TABLE</li>
</ul>
</blockquote>
<p>and <a href=""https://docs.snowflake.com/en/sql-reference/account-usage/query_history.html"" rel=""nofollow noreferrer"">QUERY_HISTORY</a></p>
<hr />
<p>For data metering perspective <a href=""https://docs.snowflake.com/en/sql-reference/account-usage/data_transfer_history.html"" rel=""nofollow noreferrer"">DATA_TRANSFER_HISTORY</a>:</p>
<blockquote>
<p>This Account Usage view can be used to query the history of data transferred from Snowflake tables into a different cloud storage provider’s network (i.e. from Snowflake on AWS, Google Cloud Platform, or Microsoft Azure into the other cloud provider’s network) and/or geographical region within the last 365 days (1 year).</p>
</blockquote>
"
"73570260","How Can I Attach Policy Tags to columns using Python API","<p>As a part of data governance, we have created Taxonomies, Policy Tags Using &quot;Python API&quot;. And I am trying to Assign Policy Tags to Columns [Name, Age] for a table <code>Project.Dataset.TMP_TBL</code>.
Looked across the GCP Documentation but couldn't find any code snippets of Python to do this.
Please Help me out with and Example code Snippet to do so.</p>
","<google-cloud-platform><google-bigquery><google-api-python-client><google-data-catalog><data-governance>","2022-09-01 13:33:46","64","1","1","73576768","<p>You can use <a href=""https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.client.Client.html#google.cloud.bigquery.client.Client.update_table"" rel=""nofollow noreferrer"">update_table()</a> to assign policy tags to columns. When updating a policy tag for a specific column see code below:</p>
<pre class=""lang-py prettyprint-override""><code>from google.cloud import bigquery
from google.cloud.bigquery.schema import SchemaField,PolicyTagList
client = bigquery.Client()

table_id = 'project_id.dataset_id.table_id'

policy_tags = PolicyTagList(names=[&quot;projects/&lt;my_project_id&gt;/locations/us/taxonomies/&lt;taxonomy_id&gt;/policyTags/&lt;policy_tag_id&gt;&quot;])
table = client.get_table(table_id)

table.schema = [
        SchemaField(
            name=&quot;name&quot;,
            field_type=&quot;STRING&quot;,
            policy_tags=policy_tags
            ),
        SchemaField(
            name=&quot;age&quot;,
            field_type=&quot;INT64&quot;
            )
        ]

table = client.update_table(table=table,fields=[&quot;schema&quot;])
print(table.schema)
</code></pre>
<p>Output:</p>
<p><a href=""https://i.stack.imgur.com/qvvpu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qvvpu.png"" alt=""enter image description here"" /></a></p>
<p>NOTE: For example you have a table with 2 columns (<code>name</code>, <code>age</code>). If you want to update the <code>policy_tag</code> for <code>name</code> only you can do so as shown with the code above. But you also need to redefine <code>age</code> so it won't error out.</p>
"
"73497701","Create A dateKey Column and save the result to the table","<p>I want to add a date key to my fact tables, so I wrote the statement but I couldn't do the insert step.
How Can I insert the result to my existing table?</p>
<pre><code>  SELECT  top (1000)
   [TransDate]
  ,[Store]
  ,[StoreId]
  ,[TillNo]
  ,[TransId]
  ,[LineNo]
  ,[Barcode]
  ,[Quantity]
  ,[NetAmount]
  ,[UnitPrice]
  ,[CostAmount]
  ,[TransType]
  ,[Discount]
  ,CAST (CONVERT(nvarchar(8),[TransDate] ,112) AS int) AS [TransDateKey]
  FROM [int].[FACT_SalesDetail] 
</code></pre>
<p>I created a column in the sales detail table named TransDateKey, I want to insert the cast result into it.</p>
<p>thanks in advance.</p>
","<sql><casting><data-warehouse>","2022-08-26 07:46:36","48","0","1","73497772","<p>If I understand you correctly, you can simply use an <code>UPDATE</code> statement for this:</p>
<pre><code>UPDATE [int].[FACT_SalesDetail]
SET [TransDateKey] = CAST(CONVERT(NVARCHAR(8), [TransDate], 112) AS INT)
</code></pre>
"
"73466418","How to export output from a loop into a dataframe?","<p>I have generated some predicted values using a custom linear regression function, <code>reg()</code>, and iterating it over multiple variables with a for-loop. They are saved in <code>forage.pred</code>, a vector of more than 5000 values.</p>
<p>I am now trying to convert this into a dataframe, but I don't know how to.</p>
<p>Several loop tutorials that I found on the Internet use dummy example with one vector and index based on a single vector, so they don't apply to my situation.</p>
<pre class=""lang-r prettyprint-override""><code>forage
# A tibble: 5,421 × 24
#    year standid trt    plot sedge legume woody forbs
#   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
# 1  1986       2 CCSP      1    30   14.4     0   30 
# 2  1986       7 CCSP      1     0    0      20    0 
# 3  1986      12 CCSP      1    18    0       0  158.
# 4  1987       2 CCSP      1     2    0       0   32 
# 5  1987       7 CCSP      1     0   40       0   80 
# 6  1987      12 CCSP      1     0    2       0   52 
# 7  1988       2 CCSP      1     0    2     424    2 
# 8  1988       7 CCSP      1   104    0       0    0 
# 9  1988      12 CCSP      1     4    4      64    2 
# 10  1989       2 CCSP      1     0    0       0    2 
# … with 5,411 more rows, and 16 more variables:
#   panicum &lt;dbl&gt;, grass &lt;dbl&gt;, litter &lt;dbl&gt;,
#   burned &lt;dbl&gt;, nonwoody &lt;dbl&gt;, pp_grow &lt;dbl&gt;,
#   pp_watyear &lt;dbl&gt;, pp_spring &lt;dbl&gt;, pp_summer &lt;dbl&gt;,
#   pp_winter &lt;dbl&gt;, pp_grow.s50 &lt;dbl&gt;,
#   pp_grow.s30 &lt;dbl&gt;, pp_grow.s10 &lt;dbl&gt;,
#   pp_grow.a10 &lt;dbl&gt;, pp_grow.a30 &lt;dbl&gt;, …
# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names

yvar &lt;- forage[, 5:10]
new.pp &lt;- forage[, 19:24]

# Fit linear model and create predicted values
reg &lt;- function(i, j) {
  lm.forage &lt;- lm(i ~ year + pp_grow,
                 data = forage)
  forage.pred &lt;- (predict(lm.forage,
                          newdata = list(pp_grow = j)))
  }

# Loop this over multiple variables
for (i in yvar) {
  for (j in new.pp) {
    reg(i, j)
  }
}
</code></pre>
","<r><dataframe><loops><data-science><data-management>","2022-08-24 01:08:59","83","-1","1","73473389","<p>I'm thinking that all the preliminary text about regressions for loops is obfuscating what is a simple question with a simple answer.
Question, how do you take a vector of numeric values, and convert to a dataframe ...
it would generally be as simple as</p>
<pre><code>(my_vector &lt;- 1:5)
(my_dataframe &lt;- data.frame(myv = my_vector))
</code></pre>
"
"73450137","Store DBT run execution output for auditing / analytics","<p>I'd like to know if I can store the outuput of a DBT run (successes or failures, model name, execution time, maybe some &quot;execution ID&quot; into a table, so that I can do analytics of the successes / errors.</p>
<pre><code>19:15:19  1 of 3 START table model dataset.table_1 ......... [RUN] 
19:15:19  1 of 3 ERROR table model dataset.table_1 ......... [ERROR in 999s]
19:15:20  2 of 3 SKIP relation dataset.table................ [SKIP]
19:15:20  3 of 3 start table dataset.table_3................ [RUN]
19:15:20  3 of 3 OK created dataset.table_3................. [CREATE TABLE (1M rows, 10TB processed)]
</code></pre>
<p>Is there a way I can get this info tabulated? Maybe is there some package that does it?</p>
<pre><code>model_name      | started_at | finished_at | status
-----------------------------------------------------
dataset.table_3 | 19:15:20   | 19:15:20    | success
-----------------------------------------------------
dataset.table_2 | 19:15:19   | NULL        | skip
-----------------------------------------------------
dataset.table_1 | 19:15:19   | 19:15:19    | error
</code></pre>
","<sql><data-warehouse><dbt>","2022-08-22 19:27:07","741","0","2","73450286","<p>I believe the best way to do this is to use the Brooklyn Data's excellent <a href=""https://hub.getdbt.com/brooklyn-data/dbt_artifacts/latest/"" rel=""nofollow noreferrer"">dbt-artifacts</a> package. You will specifically be interested in the <a href=""https://brooklyn-data.github.io/dbt_artifacts/#!/model/model.dbt_artifacts.fct_dbt__model_executions"" rel=""nofollow noreferrer""><code>fct_dbt__model_executions</code></a> table that it produces.</p>
<p>When dbt runs, it logs structured data to <code>run_results.json</code> and <code>manifest.json</code> files. This packages reads those files and then inserts data into your data warehouse based on their contents. This is much faster and better than adding model hooks that <code>insert</code> the data directly, at the end of each model execution.</p>
"
"73450137","Store DBT run execution output for auditing / analytics","<p>I'd like to know if I can store the outuput of a DBT run (successes or failures, model name, execution time, maybe some &quot;execution ID&quot; into a table, so that I can do analytics of the successes / errors.</p>
<pre><code>19:15:19  1 of 3 START table model dataset.table_1 ......... [RUN] 
19:15:19  1 of 3 ERROR table model dataset.table_1 ......... [ERROR in 999s]
19:15:20  2 of 3 SKIP relation dataset.table................ [SKIP]
19:15:20  3 of 3 start table dataset.table_3................ [RUN]
19:15:20  3 of 3 OK created dataset.table_3................. [CREATE TABLE (1M rows, 10TB processed)]
</code></pre>
<p>Is there a way I can get this info tabulated? Maybe is there some package that does it?</p>
<pre><code>model_name      | started_at | finished_at | status
-----------------------------------------------------
dataset.table_3 | 19:15:20   | 19:15:20    | success
-----------------------------------------------------
dataset.table_2 | 19:15:19   | NULL        | skip
-----------------------------------------------------
dataset.table_1 | 19:15:19   | 19:15:19    | error
</code></pre>
","<sql><data-warehouse><dbt>","2022-08-22 19:27:07","741","0","2","75080775","<p>Sounds like you're describing the elementary data reliability DBT package.
from the official docs: <a href=""https://hub.getdbt.com/elementary-data/elementary/latest/"" rel=""nofollow noreferrer"">https://hub.getdbt.com/elementary-data/elementary/latest/</a></p>
<p>The package automatically uploads the dbt artifacts and run results to tables in your data warehouse:</p>
<p>Run results tables:</p>
<ul>
<li>dbt_run_results</li>
<li>model_run_results</li>
<li>snapshot_run_results</li>
<li>elementary_test_results (all dbt test results)</li>
</ul>
<p>Metadata tables:</p>
<ul>
<li>dbt_models</li>
<li>dbt_tests</li>
<li>dbt_sources</li>
<li>dbt_exposures</li>
<li>dbt_metrics</li>
<li>dbt_snapshots</li>
</ul>
"
"73443328","How to UPIVOT all columns in a table and aggregate into Data Quality/ Validation Metrics? SQL SNOWFLAKE","<p>I have a table with 60+ columns in it that I would like to UNPIVOT so that each column becomes a row and then find the fill rate, min value and max value of each entry.</p>
<p>For Example</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>START_DATE</th>
<th>END_DATE</th>
<th>EVENT_ID</th>
<th>PROVIDER_CODE</th>
</tr>
</thead>
<tbody>
<tr>
<td>01</td>
<td>01/23/21</td>
<td>03/14/21</td>
<td>0023401</td>
<td>0012323</td>
</tr>
<tr>
<td>02</td>
<td>06/04/21</td>
<td>09/20/21</td>
<td>0025906</td>
<td>0023454</td>
</tr>
<tr>
<td>03</td>
<td>07/20/21</td>
<td>12/02/21</td>
<td>0027093</td>
<td>0034983</td>
</tr>
</tbody>
</table>
</div>
<p>And I want the output to look like</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Column_Name</th>
<th>Fill_Rate</th>
<th>Min</th>
<th>Max</th>
</tr>
</thead>
<tbody>
<tr>
<td>ID</td>
<td>0.7934</td>
<td>01</td>
<td>03</td>
</tr>
<tr>
<td>Start_Date</td>
<td>0.6990</td>
<td>01/23/21</td>
<td>07/20/21</td>
</tr>
<tr>
<td>End_Date</td>
<td>0.9089</td>
<td>03/14/21</td>
<td>12/02/21</td>
</tr>
<tr>
<td>Event_ID</td>
<td>1.0000</td>
<td>0023401</td>
<td>0027093</td>
</tr>
</tbody>
</table>
</div>
<p>Struggling to get the desired output, especially because of different data types in the different columns</p>
<p>i tried doing the following, but it doesn't allow taking the agg functions within the unpivot</p>
<pre><code>select *
from &quot;DSVC_MERCKPAN_PROD&quot;.&quot;COHORTS_LATEST&quot;.&quot;MEDICAL_HEADERS&quot;
UNPIVOT (
max(code) as max_value,
min(code) as min_value,
avg(code) as fill_rate,
code as column_name
)
</code></pre>
<p>For fill rate, I was trying to use this logic as ID is always populated so it has the total number of rows, however the other columns can be null</p>
<pre><code> (COUNT_IF(start_date is not null))/(COUNT_IF(ID is not null))) as FILL_RATE,
</code></pre>
","<sql><validation><snowflake-cloud-data-platform><unpivot><data-quality>","2022-08-22 10:00:53","175","0","1","73469976","<p>I have 2 ideas to implement the report.</p>
<p>The first way is casting all values to <code>VARCHAR</code> and then using <code>UNPIVOT</code>:</p>
<pre class=""lang-sql prettyprint-override""><code>-- Generate dummy data
create or replace table t1 (c1 int, c2 int, c3 int, c4 int, c5 int, c6 int, c7 int, c8 int, c9 int, c10 int) as
select
    iff(random()%2=0, random(), null), iff(random()%2=0, random(), null),
    iff(random()%2=0, random(), null), iff(random()%2=0, random(), null),
    iff(random()%2=0, random(), null), iff(random()%2=0, random(), null),
    iff(random()%2=0, random(), null), iff(random()%2=0, random(), null),
    iff(random()%2=0, random(), null), iff(random()%2=0, random(), null)
from table(generator(rowcount =&gt; 1000000000))
;

-- Query
with
cols as (
    select column_name, ordinal_position
    from information_schema.columns
    where table_catalog = current_database()
    and table_schema = current_schema()
    and table_name = 'T1'
),
stringified as (
    select
        c1::varchar c1, c2::varchar c2, c3::varchar c3, c4::varchar c4, c5::varchar c5,
        c6::varchar c6, c7::varchar c7, c8::varchar c8, c9::varchar c9, c10::varchar c10
    from t1
),
data as (
    select column_name, column_value
    from stringified
    unpivot(column_value for column_name in (c1, c2, c3, c4, c5, c6, c7, c8, c9, c10))
)
select
    c.column_name,
    count(d.column_value)/(select count(*) from t1) fill_rate,
    min(d.column_value) min,
    max(d.column_value) max
from cols c
left join data d using (column_name)
group by c.column_name, c.ordinal_position
order by c.ordinal_position
;
/*
COLUMN_NAME FILL_RATE   MIN MAX
C1  0.500000    -1000000069270747870    999999972962694409
C2  0.499980    -1000000027928146782    999999946877079818
C3  0.499996    -1000000012155323098    999999942281548701
C4  0.500017    -1000000056353213091    999999946421698482
C5  0.500015    -1000000015608859996    999999993977648967
C6  0.500003    -1000000007081089270    999999998851014730
C7  0.499987    -100000008605944993 999999968272328033
C8  0.499992    -1000000042470913027    999999977402822725
C9  0.500011    -1000000058928465662    999999969060696774
C10 0.500029    -1000000011306371004    99999996061390938
*/
</code></pre>
<p>It's a straightforward way, but it still needs to list up all column names twice and it's a bit tough in the case the number of columns is very massive (but I believe it's much better than a huge UNION ALL query).</p>
<hr />
<p>Another solution is a bit tricky, but you can unpivot a table by using <code>OBJECT_CONSTRUCT(*)</code> aggregation if the row length doesn't exceed a VARIANT value limit (16 MiB):</p>
<pre class=""lang-sql prettyprint-override""><code>with
cols as (
    select column_name, ordinal_position
    from information_schema.columns
    where table_catalog = current_database()
    and table_schema = current_schema()
    and table_name = 'T1'
),
data as (
    select f.key column_name, f.value::varchar column_value
    from (select object_construct(*) rec from t1) up,
    lateral flatten(up.rec) f
)
select
    c.column_name,
    count(d.column_value)/(select count(*) from t1) fill_rate,
    min(d.column_value) min,
    max(d.column_value) max
from cols c
left join data d using (column_name)
group by c.column_name, c.ordinal_position
order by c.ordinal_position
;

/*
COLUMN_NAME FILL_RATE   MIN MAX
C1  0.500000    -1000000069270747870    999999972962694409
C2  0.499980    -1000000027928146782    999999946877079818
C3  0.499996    -1000000012155323098    999999942281548701
C4  0.500017    -1000000056353213091    999999946421698482
C5  0.500015    -1000000015608859996    999999993977648967
C6  0.500003    -1000000007081089270    999999998851014730
C7  0.499987    -100000008605944993 999999968272328033
C8  0.499992    -1000000042470913027    999999977402822725
C9  0.500011    -1000000058928465662    999999969060696774
C10 0.500029    -1000000011306371004    99999996061390938
*/
</code></pre>
<p><code>OBJECT_CONSTRUCT(*)</code> aggregation is a special usage of the <code>OBJECT_CONSTRUCT</code> function that extracts column names as a key of each JSON object. As far as I know, this is the only way to extract column names from a table along with values in a programmatic way.</p>
<p>Since <code>OBJECT_CONSTRUCT</code> is relatively a heavy operation, it usually takes a longer time than the first solution, but you don't need to write all column names with this trick.</p>
"
"73429330","Problem placing attribute in dimensional layout","<p>I am doing a small exercise, I need to create a small dimensional design that deals with the tsunamis that have occurred in different countries over the years. I have created a &quot;Country&quot; dimension and a &quot;Location&quot; dimension. In each record of the provided table comes (or may not come) the longitude and latitude in which the place is located. My question is where should I put such attributes, whether in the fact table or in the location dimension. My understanding is that in the fact table it should only contain metrics and the foreign keys of the dimensions. However, I don't know how correct it can be to add the longitude and latitude to the location dimension, since by having the values a very wide range, many records are being created in the &quot;Location&quot; dimensional table. Would it be more appropriate to put those attributes in the fact table?
<a href=""https://i.stack.imgur.com/0xeol.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0xeol.png"" alt=""enter image description here"" /></a></p>
<p>Thanks.</p>
","<database><database-design><data-warehouse><dimensional-modeling><fact-table>","2022-08-20 18:52:42","43","-1","2","73429559","<p>On the fact table.  This is exactly like a fact that has a time attribute, with sub-second resolution.  It's not necessary to create a dimension table containing every possible point in time, or every possible lat/long.</p>
"
"73429330","Problem placing attribute in dimensional layout","<p>I am doing a small exercise, I need to create a small dimensional design that deals with the tsunamis that have occurred in different countries over the years. I have created a &quot;Country&quot; dimension and a &quot;Location&quot; dimension. In each record of the provided table comes (or may not come) the longitude and latitude in which the place is located. My question is where should I put such attributes, whether in the fact table or in the location dimension. My understanding is that in the fact table it should only contain metrics and the foreign keys of the dimensions. However, I don't know how correct it can be to add the longitude and latitude to the location dimension, since by having the values a very wide range, many records are being created in the &quot;Location&quot; dimensional table. Would it be more appropriate to put those attributes in the fact table?
<a href=""https://i.stack.imgur.com/0xeol.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0xeol.png"" alt=""enter image description here"" /></a></p>
<p>Thanks.</p>
","<database><database-design><data-warehouse><dimensional-modeling><fact-table>","2022-08-20 18:52:42","43","-1","2","73429663","<p>You should merge Location and Country into a single Location dimension (country is an attribute of location) and hold lat and long in the dimension</p>
"
"73404587","datawarehouse: what is the technical name for two different dimensions consuming from the same table?","<p>If I have only one Client table on my backend.</p>
<p>And then on my cube I create two different dimensiones: PayerClient and SellToClient. But they in fact come from the same source table dbo.Client.</p>
<p>Is there a technical name for these type of dimensions?</p>
","<sql><data-modeling><data-warehouse>","2022-08-18 14:13:28","15","0","1","73404741","<p>If the dimensions refer to the same physical table and are duplicates (apart from the name) then they are called role-playing dimensions</p>
"
"73382610","Using hash keys instead of integer keys on Redshift","<p>I'm building a star schema model on AWS Redshift which contains dimensions and facts.The dimension tables contain integer values as keys typically using an identity column.This is being used in lookup while loading the fact table and populating the fact table with the dimension keys as required for every record. I have been told that we can use hash keys instead of using integer keys which would simplify the loading process as we do not have to do a lookup and instead just hash the value of natural/business key that is being used. Has anyone tried this approach and what are the pros and cons of using hashed keys as dimension keys in the data model.The models will further be used in Power BI for reporting, will there be an performance impact of using hash keys there.</p>
","<database-design><amazon-redshift><data-warehouse><star-schema>","2022-08-17 03:37:22","140","-3","1","73405042","<blockquote>
<p>The models will further be used in Power BI for reporting, will there be an performance impact of using hash keys there.</p>
</blockquote>
<p>Assuming that the output of the hash is an integer, then there will very little performance difference between the two approaches.  You may get better compression on your fact table using incremental integers like 0x00000D3F0 instead of hash-generated ones like 0xE4AE353D, but I wouldn't expect it to be a big deal.</p>
"
"73377036","How to ETL my PostgreSQL data into a ClickHouse datawarehouse?","<p>I have data stored in postgreSQL as data source and I want to load dimensions and fact tables of the clickhouse datawarehouse , I am new to clickhouse and used to use traditional integration tools like Talend and Microsoft SSIS to perform ETL
(PS i'm using docker images for both clickhouse and postgreSQL)</p>
","<postgresql><etl><data-warehouse><business-intelligence><clickhouse>","2022-08-16 16:08:22","1190","1","2","73377557","<p>Have a look at the PostgreSQL engine integration <a href=""https://clickhouse.com/docs/en/engines/table-engines/integrations/postgresql/"" rel=""nofollow noreferrer"">here</a> where you can perform <code>SELECT</code> and <code>INSERT</code> queries on ClickHouse on data stored in remote PostgreSQL.</p>
<p>You can also make use of <a href=""https://clickhouse.com/docs/en/sql-reference/table-functions/postgresql"" rel=""nofollow noreferrer"">table function</a> as well.</p>
"
"73377036","How to ETL my PostgreSQL data into a ClickHouse datawarehouse?","<p>I have data stored in postgreSQL as data source and I want to load dimensions and fact tables of the clickhouse datawarehouse , I am new to clickhouse and used to use traditional integration tools like Talend and Microsoft SSIS to perform ETL
(PS i'm using docker images for both clickhouse and postgreSQL)</p>
","<postgresql><etl><data-warehouse><business-intelligence><clickhouse>","2022-08-16 16:08:22","1190","1","2","73467272","<p>You ingest data from Postgres into Clickhouse using:</p>
<ul>
<li><p>External ETL tools like <a href=""https://airbyte.com/"" rel=""nofollow noreferrer"">Airbyte</a> and make a connector from Postgres to Clikhouse</p>
</li>
<li><p><a href=""https://clickhouse.com/docs/en/engines/table-engines/integrations/"" rel=""nofollow noreferrer"">Clickhouse integrations table engine</a> to make a view from Clickhouse to Postgres data, after that use <code>insert into</code> query to insert data from that view into the real Clickhouse table</p>
</li>
</ul>
"
"73338184","Has any benchmarking been done on SAS data files (compared to a SQL database)?","<p>I work for a company that's currently using a collection of SAS data files (sas7bdat) as their data warehouse. I'm trying to make the case that moving from SAS to a SQL database would result in large performance gains. Based on how long SAS is currently taking to perform queries I have a gut feeling that a data warehouse in, say, PostgreSQL running on the same hardware would be much faster.</p>
<p>The problem is that it's really difficult to compare performance apples-to-apples (e.g. on the same hardware). I would love to fire up a VM on my home server and run the same set of operations on SAS and compare to a SQL db, but I'm not willing to pay for SAS's expensive licensing.</p>
<p>Has anyone done benchmarking on how long it takes to perform a query on a SAS dataset as compared to a SQL table?</p>
","<sql><sas><benchmarking><data-warehouse>","2022-08-12 18:05:13","88","0","1","73338402","<p>I have done that analysis before, as a consultant. I don't have the specifics in front of me but it is enormous (SQL Server is like 10-100x faster). Create the table using an index.</p>
<p>As a former SAS consultant (at SAS), we used to encourage clients to use an RDBMS vs SAS datasets. The sas7bdat is a proprietary, binary format designed a long time ago. It is nowhere near the speed or capability of an RDBMS.</p>
<p>Also, it is easy to convert from SAS datasets to a SQL table.</p>
<p>I am not sure how Postgres would perform but I would imagine the numbers would be comparable to SQL Server (probably not as fast but pretty close). I have used all of the major DBs but I tested on SQL Server.</p>
"
"73320123","Can observations in a fact table also be dimensions?","<p>My understanding is that a fact table uses keys, which are dimensions that ought to have their own dimension table, to identify observations and assign them values. Can these values themselves be dimensions? Or does that violate some principle of a star schema?</p>
<p>For example, is this a valid fact table design?</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Start Time</th>
<th>Stop Time</th>
<th>Employee ID</th>
<th>Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td>01</td>
<td>60</td>
<td>0100</td>
<td>Grade 3</td>
</tr>
<tr>
<td>01</td>
<td>20</td>
<td>0200</td>
<td>Grade 2</td>
</tr>
<tr>
<td>20</td>
<td>60</td>
<td>0200</td>
<td>Grade 3</td>
</tr>
</tbody>
</table>
</div>
<p>My dimensions that I use to identify facts are the first three columns, with the final column being an observation. However, if I have more information about what each Performance means, does that mean that there needs to be a Performance dimension table? Or, because Performance is an observation rather than a dimension, does this data need to be in  the fact table itself?</p>
","<data-warehouse><star-schema><fact>","2022-08-11 11:45:44","132","0","1","73320768","<p>In a fact table there are normally 3 types of column:</p>
<ul>
<li>measures: anything that can be aggregated</li>
<li>dimension keys: key to a record in a dimension table</li>
<li>degenerate dimensions: attributes that do not naturally sit in a dimension (often because they would be the only attribute in the dimension)</li>
</ul>
<p>It is also possible for an attribute to be both a measure in a fact table and  an attribute in a dimension. For example, the price of a product could be both a measure in a fact table and an attribute in the product dimension</p>
<p>Does this help?</p>
<h1>Update</h1>
<p>Say you wanted to know the average price of the products you have sold: in this case product price is a measure and lives in a sales fact table; that fact table would almost certainly have an FK to your Product Dimension - so you could filter on product attributes e.g. average price of product for products whose category = &quot;Food&quot;.</p>
<p>You might also want to filter a query based on product price: in this case product price is an attribute in your product dimension (which would probably be an SCD2 dimension to cater for price changes). For example, you might want to query your stock-level fact table (which doesn't hold product prices as measures but does have an FK to the product dimension) for all products whose price is between £10 and £20</p>
"
"73311734","Is there way to copy data from the GCP data catalog from a project to another?","<p>I'm doing a test to use GCP Data Catalog (DataPlex) to document tables from Big Query.
I'm doing in a dev project, in the future, I'd like transfer data that I documented (description of columns, overview of table) from dev project to prod project?
Is it possible?</p>
","<google-data-catalog><google-dataplex>","2022-08-10 19:27:12","144","0","1","74653085","<p><a href=""https://github.com/mesmacosta/datacatalog-util"" rel=""nofollow noreferrer"">https://github.com/mesmacosta/datacatalog-util</a> is an export/import utility that lets you achieve what I believe you are looking for, provided that you use tags to store these descriptions?</p>
"
"73280033","Provide aws credentials to Airflow GreatExpectationsOperator","<p>I would like to use GreatExpectationsOperator to perform data quality validations.</p>
<p>The validation results data should be stored in S3.
I don't see an option to send an airflow connection name to the GE operator, and the AWS credentials in my organization are stored in an airflow connection.</p>
<p>How can great expectations retrieve s3 credentials from airflow connection? and not from the default aws credentials in .aws dir?</p>
<p>Thanks!</p>
","<airflow><data-quality><great-expectations>","2022-08-08 15:03:11","178","0","1","73410609","<p>We ended up creating a new oprator that inherit from GE operator and the operator get the connection as part of its ecxeute method.</p>
"
"73235266","Managing Schema/Data In Static/Fixed-Content Dimensions with Lakehouse","<p>In the absence of DML (not leveraging Delta Lake as of yet), I'm looking for ways to manage Static/Fixed-Content Dimensions in a Data Lakehouse (i.e. Gender, OrderType, Country).</p>
<p>Ideally the schema and data within these dimensions would be managed by non-technical staff, but at this point, I'm just looking for development patterns to support the concept technically without being able DML. Preferably with history on source (who added 10 rows to this dimension?)</p>
<p>Thank you in advance for any assistance.</p>
","<data-warehouse><azure-data-lake><data-lakehouse>","2022-08-04 11:36:51","33","0","1","73271252","<p>The Lake/Lakehouse/Warehouse should not be the system-of-record for any of your data.  So you have another system where that data is &quot;mastered&quot; and then you copy it.  Just like anything else.</p>
<p>The &quot;system-of-record&quot; can be a SharePoint List, an Excel workbook on OneDrive, a Dataverse table, or whatever you find convenient and is accessible to the people managing the data.</p>
"
"73218896","Dimensions inside FACT table?","<p>I am in the process of building a warehouse and I am unsure how to handle a certain element. We have a query that returns a table of &quot;Quotes&quot; a quote is basically when we have provided a quote to a customer for a service. Some of these quotes will lead to a sale and they will eventually become a customer but not all quotes will have a corresponding customer account.</p>
<p>The quote contains a lot of numeric data like the quote amount, various fees added and other numeric data we would want to aggregate later. So it seems like a typical fact table; a row for each quote. However it also has data relating to do it that it &quot;Dimensional&quot; in nature, although there's not many of them.</p>
<p>For example we have a label field called quote type that is a label, quote purpose which contains a label and a recommendation flag with is simply Y/N. Both &quot;Label&quot; fields do have an ID elsewhere in the database and the labels are fixed labels we use across the board.</p>
<p>So based on this, I feel the &quot;Quote&quot; query should really be a fact table, but I'm not sure what to do with these additional 3 fields that are labels. Should they have a dimension each? as that seems like a bit overkill. Or is it reasonable to have these 3 dimensions inside the fact table? Or should I have a DIM.Quotes and a FACT.Quotes and store these both separately?</p>
<p>Any help is appreciated</p>
","<data-warehouse><sql-data-warehouse>","2022-08-03 08:53:58","41","0","1","74312110","<p>What you're asking is a Degenerated Dimension (see definition here : <a href=""https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/degenerate-dimension/"" rel=""nofollow noreferrer"">https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/degenerate-dimension/</a> ).</p>
<p>Yes you can but it's not recommended. you wan to keep a fact table row size as small as possible.</p>
<p>Maybe you want to take a look at the Junk dimension ( <a href=""https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/junk-dimension/"" rel=""nofollow noreferrer"">https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/junk-dimension/</a> ) which is a dimension with field that are not necessary related. And since you mentioned that all labels are fixed, you can also pre-build all combinaison.</p>
"
"73207861","How to change the way Talend formulates SQL queries in a JDBC connection?","<p>In Talend Data Quality, I have configured a JDBC connection to an OpenEdge database and it's working fine.</p>
<p>I can pull the list of tables and select columns to analyse, but when executing analysis, I get this :</p>
<p><strong>Table &quot;DBGSS.SGSSGSS&quot; cannot be found.</strong></p>
<p>This is because it does not specify a schema, only the database name - <strong>DBGSS</strong>.</p>
<p>How can I make it specify database, <strong>schema</strong> and then the table name ? Or just the table name, its would work too.</p>
<p>Thanks !</p>
","<java><jdbc><talend><data-quality>","2022-08-02 12:50:17","51","0","1","73218274","<p>You can use a <code>tDBConnection </code> component that give you the right to specify a schéma</p>
<p>Then , use it with the option of <code>Use Existing connection</code></p>
<p>See below documentation , <a href=""https://help.talend.com/r/en-US/7.3/db-generic/tdbconnection"" rel=""nofollow noreferrer"">https://help.talend.com/r/en-US/7.3/db-generic/tdbconnection</a></p>
"
"73195619","Mapping column in Pandas DataFrame","<p><a href=""https://i.stack.imgur.com/fkV8q.png"" rel=""nofollow noreferrer"">1</a>I have created 3 data frames from a JSON file.
I'm trying to create a column on my working data frame by mapping the key column to the other 2 data frames, the method I use works, but it throws up a warning &quot;<em>A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead</em>.
How do I go around this?</p>
<p>Error image</p>
<p><img src=""https://i.stack.imgur.com/fkV8q.png"" alt=""1"" /></p>
","<python-3.x><pandas><dataframe><data-mapping>","2022-08-01 14:53:58","48","0","1","73195739","<p>Alternatively, you can use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer"">pandas merge</a> function to join the dataframes.</p>
<p>if you are just bothered by warning messages and if you don't like to see them, you can suppress them by the following code</p>
<pre><code>import warnings
warnings.filterwarnings('ignore')
</code></pre>
<p>Good luck!</p>
"
"73176157","Using Riverpod in Flutter to implement session user data management","<p>I am using Riverpod in Flutter to handle Firebase authentication, and it seems to be working fine. Additionally, I'd like to use Riverpod to handle core user data (i.e. &quot;session&quot; data). The problem is that if a user logs out and another logs in, the user session data is not fetched/refreshed for that new user.</p>
<p>Authentication (and the semi-working session data handling) is handled the following way:</p>
<pre><code>return authState.when(
    data: (authData) {
      if (authData != null) {

        // Get user data here:
        final userState = ref.watch(appUserProvider);

        return userState.when(
            data: (appUser) {
              if (appUser.isFirstRun) {
                return const OnboardingPage();
              } else {
                return const AppRoot();
              }
            },
            loading: () =&gt; const LoadingScreen(),
            error: (e, stackTrace) =&gt; ErrorDisplay(e, stackTrace));
      }
      return const LoginPage();
    },
    loading: () =&gt; const LoadingScreen(),
    error: (e, stackTrace) =&gt; ErrorScreen(e, stackTrace));
</code></pre>
<p>As seen above, I'd like the appUserProvider to be provided once the auth state has been provided, but am having trouble getting this nested approach to work properly. When one user logs out and another logs in, the data is not automatically refreshed without an app restart. Trying to refresh the appUserProvider explicitly (using ref.read()) does not work either.</p>
<p>The appUserProvider looks like this:</p>
<pre><code>   final appUserProvider = StateNotifierProvider&lt;AppUserNotifier, AsyncValue&lt;AppUser&gt;&gt;((ref) {
      return AppUserNotifier(ref);
    });
        
    class AppUserNotifier extends StateNotifier&lt;AsyncValue&lt;AppUser&gt;&gt; {
      final StateNotifierProviderRef ref;
        
      late final UserService service;
        
      AppUserNotifier(this.ref) : super(AsyncValue.data(AppUser())) {
        service = ref.watch(userService);
        get();
    } 
// get method omitted
</code></pre>
<p>How can I get this to work properly? Is it even a good approach?</p>
<p>Thankful for any input here!</p>
","<flutter><firebase><riverpod>","2022-07-30 13:50:51","636","0","1","73184401","<p>You can use this for your StateNotifierProvider:</p>
<pre class=""lang-dart prettyprint-override""><code>final appUserProvider = StateNotifierProvider.autoDispose&lt;AppUserNotifier, AsyncValue&lt;AppUser&gt;&gt;((ref) {
      return AppUserNotifier(ref);
    });
</code></pre>
<p>Then the provider will be disposed of as soon as it is no longer used. When a new user logs in, the provider is initialized again with the correct data.</p>
"
"73175735","Advice on basic star-schema design (Date dimension)","<p>I have the following dataset ready to load to a data warehouse:</p>
<p><strong>CUSTOMER_KEY | PRODUCT_KEY | PRODUCT_DESCRIPTION | DATE | VOLUME | AMOUNT</strong></p>
<p>I am confused with the Date dimension. The date format in the dataset is 'YYYY-MM-DD'.
I want to create a dimension for date with year, month and day instead of having just the date in the FACT_SALES table.</p>
<p>For example, would this star schema work?</p>
<ul>
<li><p>FACT_SALES: CUSTOMER_KEY (PK), PRODUCT_KEY (FK), DATE (FK), VOLUME, AMOUNT</p>
</li>
<li><p>DIM_PRODUCT: PRODUCT_KEY (PK), PRODUCT_DESCRIPTION</p>
</li>
<li><p>DIM_DATE: DATE (PK), YEAR, MONTH, DAY</p>
</li>
</ul>
<p>When I load my dataset to the data warehouse (where the Date column is in 'YYYY-MM-DD' format), will the YEAR, MONTH and DAY be automatically populated based on the star-schema design I created?</p>
<p>Or do I need to create YEAR, MONTH and DAY columns in the dataset before loading it to the data warehouse?</p>
","<data-warehouse><star-schema><star-schema-datawarehouse>","2022-07-30 12:45:58","134","0","2","73177573","<p>You would normally create and populate your date dimension as a one-off activity when you first set up your DWH</p>
"
"73175735","Advice on basic star-schema design (Date dimension)","<p>I have the following dataset ready to load to a data warehouse:</p>
<p><strong>CUSTOMER_KEY | PRODUCT_KEY | PRODUCT_DESCRIPTION | DATE | VOLUME | AMOUNT</strong></p>
<p>I am confused with the Date dimension. The date format in the dataset is 'YYYY-MM-DD'.
I want to create a dimension for date with year, month and day instead of having just the date in the FACT_SALES table.</p>
<p>For example, would this star schema work?</p>
<ul>
<li><p>FACT_SALES: CUSTOMER_KEY (PK), PRODUCT_KEY (FK), DATE (FK), VOLUME, AMOUNT</p>
</li>
<li><p>DIM_PRODUCT: PRODUCT_KEY (PK), PRODUCT_DESCRIPTION</p>
</li>
<li><p>DIM_DATE: DATE (PK), YEAR, MONTH, DAY</p>
</li>
</ul>
<p>When I load my dataset to the data warehouse (where the Date column is in 'YYYY-MM-DD' format), will the YEAR, MONTH and DAY be automatically populated based on the star-schema design I created?</p>
<p>Or do I need to create YEAR, MONTH and DAY columns in the dataset before loading it to the data warehouse?</p>
","<data-warehouse><star-schema><star-schema-datawarehouse>","2022-07-30 12:45:58","134","0","2","73961670","<p>When you are first creating your data warehouse you generate the date dimesion as a table. Here's an example of the SQL I used for Snowflake to generate the data.</p>
<pre><code>WITH CTE_MY_DATE AS (
  SELECT DATEADD(DAY, SEQ4(), '2000-01-01') AS MY_DATE
    FROM TABLE(GENERATOR(ROWCOUNT=&gt;10000))  -- Number of days after reference date in previous line
)
SELECT MY_DATE::DATE                AS date,
       YEAR(MY_DATE)::NUMBER        AS year,
       MONTH(MY_DATE)::NUMBER       AS month,
       MONTHNAME(MY_DATE)::VARCHAR  AS month_name,
       DAY(MY_DATE)::NUMBER         AS day_of_month,
       DAYOFWEEK(MY_DATE)::VARCHAR  AS day_of_week,
       WEEKOFYEAR(MY_DATE)::NUMBER  AS week_of_year,
       DAYOFYEAR(MY_DATE)::NUMBER   AS day_of_year
  FROM CTE_MY_DATE
</code></pre>
<p>If you search data warehouse date spline you'll probably find some examples for whichever db you're using.</p>
"
"73135434","Apache Nifi and cassandra4.0","<p>Our company use Cassandra 4.0 as our DWH and we are trying to change our ETL tool to nifi.</p>
<p>But nifi only supports Cassandra 3.0 which Cassandra supports only python 2.7.</p>
<p>Is there any way to use nifi with Cassandra 4.0?
what open source ETL tool would you recommend?</p>
","<cassandra><etl><apache-nifi><data-warehouse>","2022-07-27 09:28:52","47","0","1","73155076","<p>I will provide some information that may be helpful. It looks like there is a request for using Cassandra 4.0 in a future version of NiFi, as a ticket was just submitted. If it were to get enough noise, then I think they might add it in. <a href=""https://issues.apache.org/jira/projects/NIFI/issues/NIFI-10285"" rel=""nofollow noreferrer"">https://issues.apache.org/jira/projects/NIFI/issues/NIFI-10285</a></p>
"
"73117895","Azure synapse With Databricks Framework for modern data warehouse","<p>I am working On Databricks. I have curated data in form of fact and dims. Theses data  consume for power bi reporting by synapse. I am not sure what is the use of synapse, If data is already cook in databricks layer. why we are using synapse in this framework.</p>
","<azure-databricks><azure-synapse>","2022-07-26 04:54:06","107","1","1","73208798","<blockquote>
<p>why we are using synapse in this framework</p>
</blockquote>
<ul>
<li>An analytics service for data warehouses and large data is called Azure Synapse. We can combine Azure services like Power BI, Machine Learning, and others using Azure Synapse.</li>
<li>It offers a number of connectors that make it easier to transfer a sizable volume of data between Azure Databricks and Azure Synapse. It also offers a mechanism for Azure Databricks users to connect to Azure Synapse.</li>
<li>Additionally, Azure Synapse offers SQL pools for the computing environment and data warehousing.</li>
</ul>
"
"73105278","How to join dimension tables while populating a fact table if business key of one dimension table is not in the other joining table?","<p>I have an OLTP database having some tables. I want to build a DW star schema from this database. I extracted three tables--Customer, Store, SalesPerson--from source to staging area and put a surrogate key as primary key in all three tables. These three tables also have respective primary keys (business keys) from OLTP but don't have relationship with each other through foreign keys. After that I populated dimension tables from staging area. Now i want to populate fact table from these dimension tables. The strategy is that I have to join dimension tables based on their business keys and put surrogate key of each dimension to fact table along with some fact values. But the problem is that the dimensions don't have business keys on which I could join them. And if I join dimensions on surrogate keys then how to keep track to the OLTP in case of SCD type 2?
<a href=""https://i.stack.imgur.com/RjMLP.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>The image is attached. This is the source OLTP. I want Customers, Products, Offices and Employees as dimension tables and &quot;Price each&quot;,&quot;quantity ordered&quot; as facts. Can you please how can I build a data warehouse from these mentioned tables and fact. Thank you</p>
","<primary-key><data-warehouse><dimensional-modeling><fact><star-schema-datawarehouse>","2022-07-25 07:15:02","543","-1","1","73111434","<p>Say your source OLTP system has a sale record that looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>customer_id</th>
<th>product_id</th>
<th>sale_date</th>
<th>amount</th>
</tr>
</thead>
<tbody>
<tr>
<td>ABCD</td>
<td>X1234</td>
<td>2022-07-26</td>
<td>13.67</td>
</tr>
</tbody>
</table>
</div>
<p>In your data warehouse you have 3 dimension tables:</p>
<p>Customer_dim:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>customer_sk</th>
<th>customer_bk</th>
</tr>
</thead>
<tbody>
<tr>
<td>9876</td>
<td>ABCD</td>
</tr>
</tbody>
</table>
</div>
<p>Product_dim:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>product_sk</th>
<th>product_bk</th>
</tr>
</thead>
<tbody>
<tr>
<td>5432</td>
<td>X1234</td>
</tr>
</tbody>
</table>
</div>
<p>Date_dim:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>date_sk</th>
<th>date_bk</th>
</tr>
</thead>
<tbody>
<tr>
<td>20220726</td>
<td>2022-07-26</td>
</tr>
</tbody>
</table>
</div>
<p>From your source record, you take each relevant column, join it to the BK in the relevant dimension table to look up the corresponding SK and then write the record to your fact table. So your fact table would look like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>customer_sk</th>
<th>product_sk</th>
<th>date_sk</th>
<th>amount</th>
</tr>
</thead>
<tbody>
<tr>
<td>9876</td>
<td>5432</td>
<td>20220726</td>
<td>13.67</td>
</tr>
</tbody>
</table>
</div>"
"73103088","Can trusted data layer in data lake replace data warehouse?","<p>in modern data architecture, data lakes are divided into multiple layers, data is store in raw form in landing zone then curated and stored with summarizations in trusted/application layers. And I believe we can even do reporting out of trusted data in data lakes. IF that's the case I am unable to think of a logic which could justify existence of data warehouse ? Or is it that trusted data itself is Datawarehouse ? Or trusted data is it more like just aggregating the data and not doing proper modelling, hence still we need data warehouse.?</p>
<p>Can someone please help me. Thanks in Advance.</p>
","<google-cloud-platform><snowflake-cloud-data-platform><data-warehouse><azure-data-lake><data-lake>","2022-07-25 00:48:13","83","0","1","73111718","<p>Data lakes originally referred only to the RAW layer. What you are describing is the standard data warehouse layered architecture. I think it is just semantics. If you have the data you need to support the typical data warehouse data use cases such as reporting in the curated and stored summarizations then you don't need another layer. If that layer isn't supporting some data use cases you can either add to that layer or create another layer and call whichever one you want a data warehouse.</p>
"
"73087265","If I shard my microservice data by enduser/tenant and each server has a subset of the total data, how do I query for data across all servers?","<p>Imagine a highly scalable architecture where each tenant is sharded and distributed by region and availability zone and each server holds a subset of total data. There is also redundancy, there are physical shards that host the same logical shard.</p>
<p>This works great, clients can use a map/reduce style to retrieve all data when handling requests that return extreme amounts of data if they know all the logical shards that a user is assigned to. This solves the problem if the amount of data for a user is larger than the capacity of any individual server's storage or memory or compute.</p>
<p>My question then is, if the data for a noun microservice is isolated and sharded across multiple servers and every server hosts a different subset of users or tenants, how do I create a view of all the objects in the system? I've effectively denormalised for performance but that means there is extreme read amplification to see the total number of objects in the system.</p>
<p>If I wanted a GUI that would show me all the noun objects of each microservice, and there is N physical shards and M noun services, I would need to create N×M requests to fetch all the data, sort it for presentation. It would be incredibly inefficient.</p>
<p>I'm thinking for more of an administration GUI perspective. Nobody wants to log into X microservice or microservice frontends to manage all the data in the system.</p>
<p>Is this a usecase for data warehousing or data lake?</p>
","<microservices><data-warehouse><scalability>","2022-07-23 00:39:56","45","0","1","73092065","<blockquote>
<p>Is this a usecase for data warehousing or data lake?</p>
</blockquote>
<p>Yes.  Replicating data into a central repository (Operational Data Store, Data Lake, or Data Warehouse) is common pattern in microservice and multi-tenant application architectures.</p>
"
"73018563","Can we get INSERT UPDATE Count from Slowly Changing Dimension Type-2 stored procedure?","<p>How to get</p>
<pre><code>Insert Count = ? Update Count = ?
</code></pre>
<p>in slowly changing dimension type 2 ?</p>
<p>Here is my procedure</p>
<pre><code>CREATE PROCEDURE dbo.MergeDimCustomer
AS
BEGIN
    DECLARE @CurrentDateTime datetime
    DECLARE @MinDateTime datetime
    DECLARE @MaxDateTime datetime
     
    SELECT
        @CurrentDateTime = cast(getdate() as datetime),
        @MinDateTime = cast('1900-01-01' as datetime),
        @MaxDateTime = cast('9999-12-31' as datetime)
    
    -- SCD1
    
    MERGE [dim].[CustomerPhone] as [target]
    USING
    (
        SELECT
            [Address],
            [Id],
            [Name],
            [Telephone]
        FROM [stg].[CustomerPhone]
    ) as [source]
    ON
    (
        [source].[Id] = [target].[Id]
    )
     
    WHEN MATCHED AND
    (
        ([target].[EndDate] = @MaxDateTime OR ([target].[EndDate] IS NULL AND @MaxDateTime IS NULL))
    )
    AND
    (
        ([source].[Name] &lt;&gt; [target].[Name] OR ([source].[Name] IS NULL AND [target].[Name] IS NOT NULL) OR ([source].[Name] IS NOT NULL AND [target].[Name] IS NULL)) OR
        ([source].[Telephone] &lt;&gt; [target].[Telephone] OR ([source].[Telephone] IS NULL AND [target].[Telephone] IS NOT NULL) OR ([source].[Telephone] IS NOT NULL AND [target].[Telephone] IS NULL))
    )
    AND
    (
        ([source].[Address] = [target].[Address] OR ([source].[Address] IS NULL AND [target].[Address] IS NULL))
    )
    THEN UPDATE
    SET
        [target].[Name] = [source].[Name],
        [target].[ScdVersion] = [target].[ScdVersion] + 1,
        [target].[Telephone] = [source].[Telephone]
    ;
    
    -- SCD2
    
    INSERT INTO [dim].[CustomerPhone]
    (
        [Address],
        [EndDate],
        [Id],
        [Name],
        [ScdVersion],
        [StartDate],
        [Telephone]
    )
    SELECT
        [Address],
        [EndDate],
        [Id],
        [Name],
        [ScdVersion],
        [StartDate],
        [Telephone]
    FROM
    (
        MERGE [dim].[CustomerPhone] as [target]
        USING
        (
            SELECT
                [Address],
                [Id],
                [Name],
                [Telephone]
            FROM [stg].[CustomerPhone]
        ) as [source]
        ON
        (
            [source].[Id] = [target].[Id]
        )
        WHEN NOT MATCHED BY TARGET
        THEN INSERT
        (
            [Address],
            [EndDate],
            [Id],
            [Name],
            [ScdVersion],
            [StartDate],
            [Telephone]
        )
        VALUES
        (
            [Address],
            @MaxDateTime,
            [Id],
            [Name],
            1,
            @MinDateTime,
            [Telephone]
        )
    WHEN MATCHED AND
    (
        ([EndDate] = @MaxDateTime OR ([EndDate] IS NULL AND @MaxDateTime IS NULL))
    )
    AND
    (
        ([target].[Address] &lt;&gt; [source].[Address] OR ([target].[Address] IS NULL AND [source].[Address] IS NOT NULL) OR ([target].[Address] IS NOT NULL AND [source].[Address] IS NULL))
    )
        THEN UPDATE
        SET
            [EndDate] = @CurrentDateTime
        OUTPUT
            $Action as [MERGE_ACTION_91025df2-1344-4e4e-84f7-e790d1f77d7c],
            [source].[Address] AS [Address],
            @MaxDateTime AS [EndDate],
            [source].[Id] AS [Id],
            [source].[Name] AS [Name],
            INSERTED.[ScdVersion] + 1 AS [ScdVersion],
            @CurrentDateTime AS [StartDate],
            [source].[Telephone] AS [Telephone]
     
    ) MERGE_OUTPUT
    WHERE MERGE_OUTPUT.[MERGE_ACTION_91025df2-1344-4e4e-84f7-e790d1f77d7c] = 'UPDATE' 
        AND MERGE_OUTPUT.[Id] IS NOT NULL
    ;
    END
    GO
 
</code></pre>
","<sql-server><stored-procedures><data-warehouse><scd><scd2>","2022-07-18 07:20:27","139","-2","1","73019367","<p>You can do it this way:</p>
<pre><code>.....

    THEN UPDATE
            SET
                [target].[Name] = [source].[Name],
                [target].[ScdVersion] = [target].[ScdVersion] + 1,
                [target].[Telephone] = [source].[Telephone]
    
    OUTPUT
       inserted.*,
       deleted.*;
</code></pre>
<p>Upgrade your code based on this post: <a href=""https://www.sqlservercentral.com/articles/the-output-clause-for-the-merge-statements"" rel=""nofollow noreferrer"">the output clause for the merge statements</a></p>
"
"72967564","Timestamp '""2013-08-19 09:50:37.000""' is not recognized","<p>In snowflake I am trying to do the following command:</p>
<pre><code>copy into trips from @citibike_trips
file_format=CSV;
</code></pre>
<p>Before this command, I have already created a table:</p>
<pre><code>CREATE TABLE &quot;CITIBIKE&quot;.&quot;PUBLIC&quot;.&quot;TRIPS&quot; 
    (&quot;TRIPDURATION&quot; INTEGER
      , &quot;STARTTIME&quot; TIMESTAMP
      , &quot;STOPTIME&quot; TIMESTAMP
      , &quot;START_STATION_ID&quot; INTEGER
      , &quot;START_STATION_NAME&quot; STRING
      , &quot;START_STATION_LATITUDE&quot; FLOAT
      , &quot;START_STATION_LONGITUDE&quot; FLOAT
      , &quot;END_STATION_ID&quot; INTEGER
      , &quot;END_STATION_NAME&quot; STRING
      , &quot;END_STATION_LATTITUDE&quot; FLOAT
      , &quot;END_STATION_LONGITUDE&quot; FLOAT
      , &quot;BIKEID&quot; INTEGER
      , &quot;MEMBERSHIP_TYPE&quot; STRING
      , &quot;USERTYPE&quot; STRING
      , &quot;BIRTH_YEAR&quot; INTEGER
      , &quot;GENDER&quot; INTEGER);
</code></pre>
<p>Now by typing the command copy into trips from @citibike_trips
file_format=CSV;</p>
<p>I am receiving the error: Timestamp '&quot;2013-08-19 09:50:37.000&quot;' is not recognized</p>
<p>I have already tried the solution, which I found on the Internet, which would be:</p>
<pre><code>ALTER citibike SET  TIMESTAMP_INPUT_FORMAT  = 'yyyy/mm/dd HH24:MI:SS';
</code></pre>
<p>But this is not helping. It is also not helping, if I try to use 'yyyy/mm/dd' or 'AUTO'</p>
<p>Does anyone have any idea how to solve this?</p>
","<sql><snowflake-cloud-data-platform><data-warehouse>","2022-07-13 13:57:36","118","0","1","72969362","<p>Look at the error message very closely, especially the highlighted parts:</p>
<blockquote>
<p>Timestamp '<code>&quot;</code>2013-08-19 09:50:37.000<code>&quot;</code>' is not recognized</p>
</blockquote>
<p>Your timestamp format <code>YYYY-MM-DD HH24:MI:SS.FF3</code> is correct, but the string includes double quotes. You can specify <code>FIELD_OPTIONALLY_ENCLOSED_BY = '\042'</code> in your file format to correct this.</p>
"
"72955422","What syntax should be used for reading the final row in an Array on the Mapping tab on the Copy Data activity in Azure Data Factory / Synapse?","<p>I'm using the copy data activity in Azure Data Factory to copy data from an API to our data lake for alerting &amp; reporting purposes. The API response is comprised of multiple complex nested JSON arrays with key-value pairs. The API is updated on a quarter-hourly basis and data is only held for 2 days before falling off the stack. The API adopts an oldest-to-newest record structure and so the newest addition to the array would be the final item in the array as opposed to the first.</p>
<p>My requirement is to copy only the most recent record from the API as opposed to the collection - so the 192th reading or item 191 of the array (with the array starting at 0.)
Due to the nature of the solution, there are times when the API isn't being updated as the sensors that collect and send over the data to the server may not be reachable.</p>
<p>The current solution is triggered every 15 minutes and tries a copy data activity of item 191, then 190, then 189 and so on. After 6 attempts it fails and so the record is missed.</p>
<p><a href=""https://i.stack.imgur.com/Kzder.png"" rel=""nofollow noreferrer"">current pipeline structure</a></p>
<p>I have used the mapping tab to specify the items in the array as follows (copy attempt 1 example):</p>
<p><code>$['meta']['params']['sensors'][*]['name']</code>  <br/>
<code>$['meta']['sensorReadings'][*]['readings'][191]['dateTime']</code>  <br/>
<code>$['meta']['sensorReadings'][*]['readings'][191]['value']</code>  <br/></p>
<p>Instead of explicitly referencing the array number, I was wondering if it is possible to reference the last item of the array in the above code?
I understand we can use 0 for the first record however I don't understand how to reference the final item. I've tried the following using the 'last' function but am unsure of how to place it:</p>
<p><code>$['meta']['sensorReadings'][*]['readings'][last]['dateTime']</code>  <br/>
<code>$['meta']['sensorReadings'][*]['readings']['last']['dateTime']</code>  <br/>
<code>last['meta']['sensorReadings'][*]['readings']['dateTime']</code>  <br/>
<code>$['meta']['sensorReadings'][*]['readings']last['dateTime']</code>  <br/></p>
<p><a href=""https://i.stack.imgur.com/LQdgE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LQdgE.png"" alt=""Example of my API response"" /></a></p>
<p>Any help or advice on a better way to proceed would be greatly appreciated.</p>
","<arrays><azure><azure-data-factory><pipeline><data-mapping>","2022-07-12 16:19:05","124","1","1","72969650","<p>Can you call your API with a Web activity?  If so, this pulls the API result into the data pipeline and then apply ADF functions like <code>last</code> to it.</p>
<p>A simple example calling the <a href=""https://www.api.gov.uk/gds/bank-holidays/#bank-holidays"" rel=""nofollow noreferrer"">UK Gov Bank Holidays API</a>:</p>
<p><a href=""https://i.stack.imgur.com/gwes6l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gwes6l.png"" alt=""enter image description here"" /></a></p>
<p>This returns a resultset that looks like this:</p>
<pre><code>{
    &quot;england-and-wales&quot;: {
        &quot;division&quot;: &quot;england-and-wales&quot;,
        &quot;events&quot;: [
            {
                &quot;title&quot;: &quot;New Year’s Day&quot;,
                &quot;date&quot;: &quot;2017-01-02&quot;,
                &quot;notes&quot;: &quot;Substitute day&quot;,
                &quot;bunting&quot;: true
            },
            {
                &quot;title&quot;: &quot;Good Friday&quot;,
                &quot;date&quot;: &quot;2017-04-14&quot;,
                &quot;notes&quot;: &quot;&quot;,
                &quot;bunting&quot;: false
            },
            {
                &quot;title&quot;: &quot;Easter Monday&quot;,
                &quot;date&quot;: &quot;2017-04-17&quot;,
                &quot;notes&quot;: &quot;&quot;,
                &quot;bunting&quot;: true
            },
            ... etc
</code></pre>
<p>You can now apply the last function to is, e.g. using a <code>Set Variable</code> activity:</p>
<pre><code>@string(last(activity('Web1').output['england-and-wales'].events))
</code></pre>
<p>Which yields the last bank holiday of 2023:</p>
<pre><code>{
    &quot;name&quot;: &quot;varWorking&quot;,
    &quot;value&quot;: &quot;{\&quot;title\&quot;:\&quot;Boxing Day\&quot;,\&quot;date\&quot;:\&quot;2023-12-26\&quot;,\&quot;notes\&quot;:\&quot;\&quot;,\&quot;bunting\&quot;:true}&quot;
}
</code></pre>
<p>Or</p>
<pre><code>@string(last(activity('Web1').output['england-and-wales'].events).date)
</code></pre>
"
"72953212","Is it possible to use data mapping with a custom field?","<p>I was wondering if I can link a custom field to a cms block/slot (Layout)? If not, can I write a custom plugin that allows this functionality?</p>
","<shopware><shopware6>","2022-07-12 13:35:10","263","0","1","72953615","<p>Yes. I had similar functionality. You have to override administration component to add your field to the list and your logic to load the data.</p>
"
"72913371","Catalogs in Databricks","<p>I have started reading about the Unity Catalog that Databricks has introduced. I understand the basic issue that it is trying to solve, but I do not understand what exactly a Catalog is.</p>
<p>This was available in the Databricks documentation,</p>
<blockquote>
<p>A catalog contains schemas (databases), and a schema contains tables and views.</p>
</blockquote>
<p><a href=""https://docs.databricks.com/data-governance/unity-catalog/create-catalogs.html"" rel=""nofollow noreferrer"">https://docs.databricks.com/data-governance/unity-catalog/create-catalogs.html</a></p>
<p>How does this added layer (on top of schemas) help? I am guessing it has something to do with governance?</p>
<p>I would really appreciate an example, if possible.</p>
","<databricks><catalog><databricks-unity-catalog><data-governance>","2022-07-08 15:02:39","1035","4","1","72915616","<p>Really, Catalog is an another data management layer inside the bigger objects - Unity Catalog Metastore.  Closest analogy of the Catalog is a single Hive Metastore - it's also contains databases (schemas) that contain tables and views. Catalogs could be used to isolate objects of some entity (business unit/project/environments (dev,stagin,prod)/...) from objects of other entities.  You can give manage permissions of the catalogs to respective admins of the business units, projects, ..., and they can then assign permissions on individual schemas and tables/views.</p>
"
"72831408","Caching reporting data in same transaction database VS using a data warehouse","<p>We have a SaaS solution, in which each  Tenant has his own MySQL database. Now I'm designing the dashboards of this SaaS system and it requires some analytical charts. To get the data needed for charts we could query the transactional data of each tenant from its database in real time. and get updated charts with no bad performance since so far the data volume not that big. However, because the data volume will be growing we decided to separate the analytical and transactional data of each company, we will get the analytical  data for the charts in the background, save/caching them and do periodical updates. My question is:</p>
<ul>
<li>What good questions or factors we should considering before deciding whether or not we need to include a data warehouse and data modeling from the beginning or simply Caching the analytical data of the charts resulting from our API in JSON columns in a new table for charts in each tenant's MYSQL database.</li>
</ul>
","<mysql><data-analysis><data-warehouse>","2022-07-01 15:20:16","54","2","1","72833236","<p>Instead of reaching into the &quot;Fact&quot; table for millions of rows, build and maintain a Summary table, then fetch from that.  It may run 10 times as fast.</p>
<p>This does require code changes because of the extra table, but it may be well worth it.</p>
<p><a href=""http://mysql.rjweb.org/doc.php/summarytables"" rel=""nofollow noreferrer""><em>Summary Tables</em></a></p>
<p>In other words, if the dataset will become bigger than X, Summary tables is the best solution.  Caching will not help.  Hardware won't be sufficient.  JSON only gets in the way.</p>
<p>Building a year-long graph from a year's worth data points (one per second) is slow and wasteful.  Building a year-long graph from daily subtotals is much more reasonable.</p>
"
"72752043","Load existing data catalog programmatically","<ol>
<li><p>I want to write pytest unit test in <strong>Kedro 0.17.5</strong>. They need to perform integrity checks on dataframes created by the pipeline.
These dataframes are specified in the <code>catalog.yml</code> and already persisted successfully using <code>kedro run</code>. The <code>catalog.yml</code> is in <code>conf/base</code>.</p>
</li>
<li><p>I have a test module <code>test_my_dataframe.py</code> in <code>src/tests/pipelines/my_pipeline/</code>.</p>
</li>
</ol>
<p>How can I load the data catalog based on my <code>catalog.yml</code> programmatically from within <code>test_my_dataframe.py</code> in order to properly access my specified dataframes?</p>
<p>Or, for that matter, how can I programmatically load the whole project context (including the data catalog) in order to also execute nodes etc.?</p>
","<python><unit-testing><pytest><kedro>","2022-06-25 07:30:41","392","2","1","72752148","<ol>
<li><p>For unit testing, we test just the function which we are testing, and everything external to the function we should mock/patch. Check if you really need kedro project context while writing the unit test.</p>
</li>
<li><p>If you really need project context in test, you can do something like following</p>
</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>from kedro.framework.project import configure_project
from kedro.framework.session import KedroSession

with KedroSession.create(package_name=&quot;demo&quot;, project_path=Path.cwd()) as session:
    context = session.load_context()
    catalog = context.catalog
</code></pre>
<p>or you can also create pytest fixture to use it again and again with scope of your choice.</p>
<pre class=""lang-py prettyprint-override""><code>@pytest.fixture
def get_project_context():
    session = KedroSession.create(
        package_name=&quot;demo&quot;,
        project_path=Path.cwd()
    )
    _activate_session(session, force=True)
    context = session.load_context()
    return context
</code></pre>
<p>Different args supported by KedroSession create you can check it here <a href=""https://kedro.readthedocs.io/en/0.17.5/kedro.framework.session.session.KedroSession.html#kedro.framework.session.session.KedroSession.create"" rel=""nofollow noreferrer"">https://kedro.readthedocs.io/en/0.17.5/kedro.framework.session.session.KedroSession.html#kedro.framework.session.session.KedroSession.create</a></p>
<p>To read more about pytest fixture you can refer to <a href=""https://docs.pytest.org/en/6.2.x/fixture.html#scope-sharing-fixtures-across-classes-modules-packages-or-session"" rel=""nofollow noreferrer"">https://docs.pytest.org/en/6.2.x/fixture.html#scope-sharing-fixtures-across-classes-modules-packages-or-session</a></p>
"
"72745222","How to lookup a surrogate key using two columns as business key in SSIS","<p>I am working on a Data warehouse project; I have a design to follow a part of it is shown in the screenshot below:</p>
<p><a href=""https://i.stack.imgur.com/DgbGY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DgbGY.png"" alt=""enter image description here"" /></a></p>
<p>The source table is as follows:</p>
<p><a href=""https://i.stack.imgur.com/tfRBd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tfRBd.png"" alt=""enter image description here"" /></a></p>
<p>As the source table shows, the entity column is not unique in its own but unique if combined with the committee title.</p>
<p>Problem: I need to have a business key to lookup the surrogate key &quot;COMMITTEE_SRGT&quot;, from the DIM_COMMITTE to the FACT_COMMITTEE using lookup component &quot;if it's the best practice&quot;,</p>
<p>My solution: I concatenate the two columns in the staging ETL as shown in the structure script of both the tables in the staging:</p>
<p><strong>FACT table in the staging ETL:</strong></p>
<pre><code>CREATE TABLE [dbo].[STG_STR_COMMITTEES_MEETINGS](
[COMMITTE_BKEY] [nvarchar](520) NULL,
[ENTITY] [nvarchar](255) NULL,
[COMMITTEE_TITLE] [nvarchar](255) NULL,
[NUMBER_MEETING_CONDUCTED] [int] NULL,
[NUMBRER_MEETING_ATTENDED_BY_MEMBER] [int] NULL
)
</code></pre>
<p><strong>DIM table in the staging ETL:</strong></p>
<pre><code>CREATE TABLE [dbo].[STG_STR_COMMITTEES](
[COMMITTE_BKEY] [nvarchar](520) NULL,
[ENTITY] [nvarchar](255) NULL,
[COMMITTEE_TITLE] [nvarchar](255) NULL,
[MEMBERSHIP_STATUS] [nvarchar](20) NULL,
[START_DATE] [date] NULL,
[END_DATE] [date] NULL,
[MEMBERS_NAMES] [nvarchar](255) NULL
)
</code></pre>
<p>With this solution the lookup was easy, but it was not accepted as it changed the DIM table design.</p>
<p>If there is any better solution &quot;methodology&quot;, where I can combine two columns to generate a composite key to lookup the surrogate key from the DIM table to the FACT without changing the datawarehouse design.</p>
<p><strong>EDIT:</strong></p>
<p>Query:</p>
<pre><code>SELECT ISNULL(COMMITTEE_SRGT, 0) AS COMMITTEE_SRGT,
cm.ENTITY, NUMBER_MEETING_CONDUCTED, 
NUMBRER_MEETING_ATTENDED_BY_SAAC_MEMBER
FROM [dbo].[STG_STR_COMMITTEES_MEETINGS] cm
 LEFT JOIN [dbo].[STG_STR_COMMITTEES] c 
      ON cm.ENTITY = c.ENTITY
      AND cm.COMMITTEE_TITLE = c.COMMITTEE_TITLE
</code></pre>
","<ssis><lookup><data-warehouse><surrogate-key>","2022-06-24 14:00:54","124","0","1","72745652","<p>Using the lookup component is not a best practice.  I'll outline why that is below.  However, you are able to use multiple columns for a look up as shown here:</p>
<p><a href=""https://i.stack.imgur.com/ZPFC7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZPFC7.png"" alt=""enter image description here"" /></a></p>
<p><strong>Why the Lookup Component is problematic</strong></p>
<ul>
<li>It's case sensitive, so you have to be careful when joining on string columns</li>
<li>The component will cache the whole table in memory by default, which could be undesirable on large dimensions.  It is best to use a query as the source instead of the table, so you only select the columns you need.  On large tables, its better to use cache files, which is a lot of extra work for something that's easy to solve in SQL</li>
<li>Duplicate values in the lookup will throw a warning, but the component will just select the first value it encounters, kind of randomly</li>
</ul>
<p><strong>A way better solution</strong></p>
<p>Just do the join in staging.  Use a left join to refer to an unknown run if the dimension row does not exist or insert inferred members in advance of the join:</p>
<pre><code>SELECT ISNULL(Committee_SRGT, 0) AS Committee_SRGT,
 ...
FROM [dbo].[STG_STR_COMMITTEES_MEETINGS] cm
     LEFT JOIN [dbo].[STG_STR_COMMITTEES] c 
          ON cm.Entity = c.Entity 
          AND cm.committee_title = c.committee_title
</code></pre>
"
"72721373","How to model many to many relationships in data warehouse dimmensions","<p>I'm starting to learn about data warehouses and have run across a scenario I can't wrap my head around. I was attempting to sketch out a data warehouse that would track monthly sales for different stores. If the source OLTP database for stores has a many-to-many relationship between store's attributes and store, how would I represent the store dimension in the data warehouse?</p>
<p>Dimension's Source OLTP Database:</p>
<p><a href=""https://i.stack.imgur.com/k6WrP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/k6WrP.png"" alt=""enter image description here"" /></a></p>
<p>Data Warehouse ERD - How would the many-to-many attribute relationhip be represented:</p>
<p><a href=""https://i.stack.imgur.com/K54mF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K54mF.png"" alt=""enter image description here"" /></a></p>
","<many-to-many><data-warehouse><erd><warehouse>","2022-06-22 20:03:10","72","1","1","72764693","<p>If your database uses a <a href=""https://en.wikipedia.org/wiki/Star_schema"" rel=""nofollow noreferrer"">star-schema</a> build around the fact table, it could be tempting as a first guess to bring the attributes into the fact table:</p>
<ul>
<li>This would make sense if the attributes would be unique in relation to the elementary metrics, i.e. if each unit sold could be unambiguously assigned to one value of that attribute. This would make sense if there would be one attribute for each store, or for each product, since the summation of metrics in the fact table would stay consistent.</li>
<li>This does not make sense if each unit sold could be assigned to multiple values of the same attribute at the same time.  In fact the attributes here remain derived from the store and there is no way to bring this in the facts without either losing some relationship or duplicating units in the metrics.  So you'd end-up keeping them related to the store dimension.  As a consequence, you cannot break down the metrics according to these attributes,  you could only filter according to these attributes.</li>
</ul>
<p>The latter approach does not allow to really handle the attributes in the same way as dimensions. A slightly different approach could be to go for a  dimension <code>attribute_combination</code>:</p>
<ul>
<li>You would then create a unique id for each unique combination of attributes that you encounter in a store.  In this case,  you could derive from the store dimension this attribute_combination and use it as a dimension.</li>
</ul>
"
"72714773","If a payment contains 1-1 mapping with transfer and each transfer contains 1-many mapping with attempts, how should I model my data vault","<p>Trying to model a process on process in DV 2.0.</p>
<p>If a payment contains 1-1 mapping with transfer and each transfer contains 1-many mapping with attempts, how should I model my data vault</p>
<p>Unable to understand how to model these scenarios of one process initiating another processes in Data Vault 2.0</p>
<p><a href=""https://i.stack.imgur.com/84dm2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/84dm2.png"" alt=""Current DV Model"" /></a></p>
","<data-modeling><data-warehouse><datamodel><data-vault>","2022-06-22 11:35:40","113","2","1","73411354","<p>First of all, link are designed to be a many-to-many relationship only. this cover all relation that could exists in past, present and future (if business logics change, you won't have to redesign your DV).</p>
<p>Second, do not create a link-to-link relationship, that is bad design. A link must represent relationship between business entities, not between business entities and relationship.</p>
<p>To answer your question, take a look at the transactional link (aka non-historized link), it should suit your need. without having your model, here's what the transaction link could look like :</p>
<pre><code>__hashKeyLink            : hashkey for the link
__hashKeyHubPayement     : hashkey for the payment
__hashKeyHubTransfert    : hashkey for the transfert
__attempt                : int,  assuming attempt is a sequence number unique per transfert
__loadDate, source ...   : others system fields
</code></pre>
"
"72651285","subset the data (based on categorical variable) in r, using %in%","<p>I made an minimum reproducible example</p>
<pre><code>modelcoef &lt;- c( 'model1_1_ef','model1_1_ev1','model1_1_ev2','model2_1_ef','model2_1_ev1','model2_1_ev2')
id &lt;- 1:6
value &lt;- c(3,1,4,6,4,6)

data&lt;-data.frame(modelcoef,id,value)
</code></pre>
<pre><code>subset1&lt;- data %&gt;%
  subset(modelcoef %in% c('ev1','ev2'))

# observation 0, so failed. 
</code></pre>
<p>I try to subset my data based on the categorical variable &quot;modelcoef&quot;.
However, that above code does not seem to be work.
I want to subset the data-- I want to take a dataset that  modelcoef columns contains &quot;ev1&quot; or &quot;ev2&quot;.</p>
<p>I can do it manually with this example, but my real data is really huge, I cannot do that manually</p>
","<r><string><dplyr><subset><data-management>","2022-06-16 19:58:34","62","0","2","72663430","<pre class=""lang-r prettyprint-override""><code>library(dplyr)
library(stringr)
library(tidyr)

modelcoef &lt;- c( 'model1_1_ef','model1_1_ev1','model1_1_ev2','model2_1_ef','model2_1_ev1','model2_1_ev2')
id &lt;- 1:6
value &lt;- c(3,1,4,6,4,6)

data &lt;- data.frame(modelcoef,id,value)



# Regular expression solution 
# You can add levels with the | separator (or)
# it can be more dangerous if there is a possibility to have ev1 in other occurences of ev1 ev2 in the initial modelcoef variable
subset1 &lt;- data %&gt;% 
  filter(stringr::str_detect(modelcoef, &quot;ev1|ev2&quot;))

# tidyr better solution

data %&gt;%
  tidyr::separate(modelcoef, into = c(&quot;model_id&quot;, &quot;unknown_thing&quot;, &quot;modelcoef&quot;), sep = &quot;_&quot;) %&gt;%  
  filter(modelcoef %in% c(&quot;ev1&quot;, &quot;ev2&quot;))
#&gt;   model_id unknown_thing modelcoef id value
#&gt; 1   model1             1       ev1  2     1
#&gt; 2   model1             1       ev2  3     4
#&gt; 3   model2             1       ev1  5     4
#&gt; 4   model2             1       ev2  6     6
</code></pre>
<p><sup>Created on 2022-06-17 by the <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex package</a> (v2.0.1)</sup></p>
<p>The <code>tidyr::separate()</code> function takes a column as input and separates it as multiple column. Your initial modelcoef column seemed to follow the pattern
<code>(model_id)_(number)_(modelcoef)</code>, so if it is true for all your data, the solution should work. It just separates your data into 3 separate columns.</p>
<p>Then you can use your newly created variable &quot;modelcoef&quot; to filter for ev1, ev2</p>
"
"72651285","subset the data (based on categorical variable) in r, using %in%","<p>I made an minimum reproducible example</p>
<pre><code>modelcoef &lt;- c( 'model1_1_ef','model1_1_ev1','model1_1_ev2','model2_1_ef','model2_1_ev1','model2_1_ev2')
id &lt;- 1:6
value &lt;- c(3,1,4,6,4,6)

data&lt;-data.frame(modelcoef,id,value)
</code></pre>
<pre><code>subset1&lt;- data %&gt;%
  subset(modelcoef %in% c('ev1','ev2'))

# observation 0, so failed. 
</code></pre>
<p>I try to subset my data based on the categorical variable &quot;modelcoef&quot;.
However, that above code does not seem to be work.
I want to subset the data-- I want to take a dataset that  modelcoef columns contains &quot;ev1&quot; or &quot;ev2&quot;.</p>
<p>I can do it manually with this example, but my real data is really huge, I cannot do that manually</p>
","<r><string><dplyr><subset><data-management>","2022-06-16 19:58:34","62","0","2","72687570","<p>You need to use a regex pattern, <code>ev(1|2)</code>in this case:</p>
<pre><code>library(dplyr)
library(stringr)
data %&gt;%
    filter(str_detect(modelcoef, &quot;ev(1|2)&quot;))
 modelcoef id value
1 model1_1_ev1  2     1
2 model1_1_ev2  3     4
3 model2_1_ev1  5     4
4 model2_1_ev2  6     6
</code></pre>
"
"72632065","Data Mapping In Typescript","<p>I have two static data sets. I would like to implement them dynamically with an API but I can not format them to the expected type.</p>
<p>This is the first data set:</p>
<pre><code>export let lineChartSeries = [
    {
        name: 'Cumulative',
        series: [
            {
                name: '2022-06-01',
                value: 50
            },
            {
                name: '2022-05-01',
                value: 80
            },
            {
                name: '2022-04-01',
                value: 85
            },
            {
                name: '2022-03-01',
                value: 90
            },
            {
                name: '2022-02-01',
                value: 100
            }
        ]
    }
];
</code></pre>
<p>This is the second data set :</p>
<pre><code>export let barChart: any = [
    {
        name: '2022-06-01',
        value: 50000
    },
    {
        name: '2022-05-01',
        value: 30000
    },
    {
        name: '2022-04-01',
        value: 10000
    },
    {
        name: '2022-03-01',
        value: 5000
    },
    {
        name: '2022-02-01',
        value: 500
    }
];
</code></pre>
<p>And this is my code for mapping the data.</p>
<pre><code>this.apiDashboard.api(&quot;test&quot;).toPromise().then((data: any) =&gt; {
            this.multi = [];
            this.single = [];
            data.list.forEach(element =&gt; {

                this.multi = data.list.map(datum =&gt; ({ name: &quot;Cumulative&quot;, series: [{ 
                name: datum.period, value: datum.cumulative }] }));
                this.single = data.list.map(datum =&gt; ({
                    name: datum.period, value: datum.amount
                }));
            });
        }
</code></pre>
<p>This is the console log for results. The first array is the expected result and the second array is mine data format type. I have to put the period and the value inside of the series[]. How could I deal with this data mapping?<br />
<a href=""https://i.stack.imgur.com/llw1b.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/llw1b.png"" alt=""Console"" /></a></p>
","<angular><typescript><data-mapping>","2022-06-15 13:11:16","578","0","1","72749692","<p>I found a reasonable way.</p>
<p>To map data like this: Create an array, map the data on this array and attend it to our data set.</p>
<p>Code:</p>
<pre><code>   let cumulative = [];

    this.apiDashboard.api(&quot;month-transacted&quot;).toPromise().then((data: any) =&gt; {
        this.dataSource = [];
        this.multi = [];
        this.single = [];
        data.list.forEach(element =&gt; {

            this.dataSource.push(element);

            cumulative = data.list.map(datum =&gt; ({ name: datum.period, value: datum.cumulative }));
            this.single = data.list.map(datum =&gt; ({
                name: datum.period, value: datum.amount
            }));
        });

        this.multi = [
            { name: &quot;Cumulative&quot;, series: cumulative }
        ];
    });
}
</code></pre>
"
"72627609","Unusual large table size in one data warehouse while with the same table, its size is significantly smaller in another data warehouse","<p>Currently, we have a strange issue in our data warehouse (Azure Synapse Analytics) as 1 table in production instance has 52 GB in size with 18 million records. I copied that table to our development instance (I exported the mentioned table to csv file in ADLS GEN2 and copy it into our development data warehouse using ADF)  to check why this table has large size and causing store procedures to run slow.</p>
<p>Strangely, the table size is just 17 GB while the tables between the two instances are the same in row count, contents and DDL. The two Data warehouses have the same DWU and other specifications. As I do not have much permission to research on production instance and I can not replicate the same table size on development instance.</p>
<p>Can someone help me to troubleshoot this issue or guide me in the right direction to rectify this ?</p>
<p>Kind regards,</p>
<p>Ken</p>
","<azure><azure-synapse>","2022-06-15 07:46:03","119","0","1","72857856","<p>Please recheck the process that you have followed for this and if it results same you can contact MS support team.</p>
<p>But before that I would suggest you try to copy the table <strong>directly from one synapse to another using the Copy activity</strong> in ADF and check the size of the table.</p>
<p>Please refer this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse?tabs=data-factory"" rel=""nofollow noreferrer"">Official Microsoft Documentation</a> for that.</p>
"
"72625354","DBFS FileStore Equivalent in Azure Synapse?","<p>Is there an equivalent to Databricks' DBFS FileStore system in Azure Synapse? Is it possible to upload csv files and read them into pandas dataframes within Azure Synapse notebooks? Ideally I'd like to not load the csv into a database; looking for something as simple as DBFS' FileStore folder.</p>
<p>In Databricks: <code>pd.read_csv('/dbfs/FileStore/name_of_file.csv')</code></p>
<p>In Synapse: ?</p>
<p>I don't see anywhere to upload csv files directly like in DBFS:</p>
<p><a href=""https://i.stack.imgur.com/4UxpQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4UxpQ.png"" alt=""enter image description here"" /></a></p>
","<azure-databricks><data-warehouse><azure-synapse>","2022-06-15 02:38:30","281","0","1","72627118","<p>The azure synapse equivalent of using <code>FileStore</code> in Databricks would be to use the data lake file system linked to your synapse workspace. Once you go to your synapse studio, navigate to <code>Data-&gt;Linked</code> where you can find the linked storage account. This storage account was created/assigned when you create your workspace.</p>
<p><a href=""https://i.stack.imgur.com/Sg1GN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Sg1GN.png"" alt=""enter image description here"" /></a></p>
<p>This primary data lake functions close to the FileStore in azure Databricks. You can use the UI shown in the above image to upload required files. You can right click on any of the files and load it into a Dataframe. As you can see in the image below, you can right click on the file and then choose <code>new notebook -&gt; Load to DataFrame</code>.</p>
<p><a href=""https://i.stack.imgur.com/5aBDS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5aBDS.png"" alt=""enter image description here"" /></a></p>
<p>The UI automatically provides a code which helps to load the csv file to a spark Dataframe. You can modify this code to load the file as a pandas Dataframe.</p>
<pre><code>'''
#this is provided by synapse when you select file and choose to load to Dataframe

df = spark.read.load('abfss://data@datalk1506.dfs.core.windows.net/sample_1.csv', format='csv'
## If header exists uncomment line below
##, header=True
)
display(df.limit(10))
'''
#Use this following code to load as pandas dataframe

import pandas as pd 
df = pd.read_csv('abfss://data@datalk1506.dfs.core.windows.net/sample_1.csv')
</code></pre>
<p>This data lake storage will be linked to the workspace with the help of the linked service (Can be viewed in <code>Manage-&gt;Linked services</code>). This is created by default from the data lake and file system information provided by the user (mandatory) while creating the synapse workspace.</p>
"
"72592807","How to add a new column by aggregating values from multiple columns in SSIS","<p>I have an excel file that I imported in SSIS and want to transfer it to SQL server as a new table in the destination DB, the table is shown in the screenshot below:</p>
<p><a href=""https://i.stack.imgur.com/baMSN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/baMSN.png"" alt=""enter image description here"" /></a></p>
<p>I want to calculate the Average of Scores for each player as new column as shown in the screenshot.</p>
<p><a href=""https://i.stack.imgur.com/cj6kf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cj6kf.png"" alt=""enter image description here"" /></a></p>
<p>I tried to accomplish this, but I couldn't success.</p>
","<ssis><etl><data-warehouse>","2022-06-12 13:50:35","109","0","1","72630230","<p>Rather let the database handle the transformation as your database will most likely do it faster than letting SSIS do it, insert your data into your destination but leave average column as <code>NULL</code>, write an <code>UPDATE</code> sql or <code>STORED PROCEDURE</code> to update your average column after the load, let SSIS call the update after doing the load</p>
<p><a href=""https://i.stack.imgur.com/ABXig.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ABXig.png"" alt=""Populate"" /></a></p>
"
"72567473","Data warehouse: difference between semi-additive and non-additive measures","<p>I have some difficulty to understand the difference between semi-additive and non-additive measures in a fact table. I've seen this example:</p>
<p><a href=""https://stackoverflow.com/questions/34295293/whats-the-difference-between-additive-semi-additive-and-non-additive-measures"">What&#39;s the difference between additive, semi-additive, and non-additive measures</a></p>
<p>But I don't understand it. I tried to read some of the Kimball's books but I don't understand it. In theory, you cannot sum over <strong>some</strong> of the dimensions but the examples that I see on the Internet seems to be similar than non-additive measures, what's the difference between them.</p>
<p>I need a good explanation with examples because I have an exam soon and I need to understand this :).</p>
<p>Thank you.</p>
","<data-warehouse><measure><fact>","2022-06-09 22:57:47","241","0","2","72567734","<p>Semi-additive measures can only be aggregated across some dimensions. E.g. quantity in stock. You can add it across products and warehouses to get the full stock. But you cannot aggregate across the time dimension.</p>
<p>Non—additive measures cannot be aggregated across any dimension. For example, sales tax percentage or unit price. It makes no sense adding the sales tax percentages of different facts. Or adding the unit prices.</p>
"
"72567473","Data warehouse: difference between semi-additive and non-additive measures","<p>I have some difficulty to understand the difference between semi-additive and non-additive measures in a fact table. I've seen this example:</p>
<p><a href=""https://stackoverflow.com/questions/34295293/whats-the-difference-between-additive-semi-additive-and-non-additive-measures"">What&#39;s the difference between additive, semi-additive, and non-additive measures</a></p>
<p>But I don't understand it. I tried to read some of the Kimball's books but I don't understand it. In theory, you cannot sum over <strong>some</strong> of the dimensions but the examples that I see on the Internet seems to be similar than non-additive measures, what's the difference between them.</p>
<p>I need a good explanation with examples because I have an exam soon and I need to understand this :).</p>
<p>Thank you.</p>
","<data-warehouse><measure><fact>","2022-06-09 22:57:47","241","0","2","72572640","<p>Just to be clear, when describing a measure as being semi- or non-additive we are talking about whether the operation makes logical/business sense. We are not saying that you cannot perform any/certain mathematical operations on these measures, just that if you do the result you would get would have no business meaning.</p>
<p><strong>Semi-additive Measures</strong></p>
<p>Say you have a fact table like this, showing monthly bank balances for customers :</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>#</th>
<th>Customer_id</th>
<th>Month</th>
<th>Balance</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>AAA</td>
<td>2022-01</td>
<td>100.00</td>
</tr>
<tr>
<td>2</td>
<td>AAA</td>
<td>2022-02</td>
<td>200.00</td>
</tr>
<tr>
<td>3</td>
<td>AAA</td>
<td>2022-03</td>
<td>90.00</td>
</tr>
<tr>
<td>4</td>
<td>AAA</td>
<td>2022-04</td>
<td>750.00</td>
</tr>
<tr>
<td>5</td>
<td>AAA</td>
<td>2022-05</td>
<td>400.00</td>
</tr>
<tr>
<td>6</td>
<td>BBB</td>
<td>2022-01</td>
<td>250.00</td>
</tr>
<tr>
<td>7</td>
<td>BBB</td>
<td>2022-02</td>
<td>68.00</td>
</tr>
<tr>
<td>8</td>
<td>BBB</td>
<td>2022-03</td>
<td>170.00</td>
</tr>
<tr>
<td>9</td>
<td>BBB</td>
<td>2022-04</td>
<td>98.00</td>
</tr>
<tr>
<td>10</td>
<td>BBB</td>
<td>2022-05</td>
<td>230.00</td>
</tr>
</tbody>
</table>
</div>
<p>The balance is additive across customers e.g. the total customer balance for 2022-01 was 350.</p>
<p>The balance is not additive across months e.g. saying the balance for customer A between 2022-01 and 2022-02 was 100+200=300 is meaningless. At no point was Customer A's month-end balance 300.</p>
<p>So because this balance measure is additive for some dimensions (Customer) but not for others (Month) it is described as semi-additive.</p>
<p><strong>Non-Additive</strong></p>
<p>Say we have a fact table that shows the ratio of the month-end balance to the balance at the end of 2021 (say for Customer A the 2021 balance was 1000 and for Customer B is was 500)</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>#</th>
<th>Customer_id</th>
<th>Month</th>
<th>Balance</th>
<th>Ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>AAA</td>
<td>2022-01</td>
<td>100.00</td>
<td>0.1</td>
</tr>
<tr>
<td>2</td>
<td>AAA</td>
<td>2022-02</td>
<td>200.00</td>
<td>0.2</td>
</tr>
<tr>
<td>3</td>
<td>AAA</td>
<td>2022-03</td>
<td>90.00</td>
<td>0.09</td>
</tr>
<tr>
<td>4</td>
<td>AAA</td>
<td>2022-04</td>
<td>750.00</td>
<td>0.75</td>
</tr>
<tr>
<td>5</td>
<td>AAA</td>
<td>2022-05</td>
<td>400.00</td>
<td>0.4</td>
</tr>
<tr>
<td>6</td>
<td>BBB</td>
<td>2022-01</td>
<td>250.00</td>
<td>0.5</td>
</tr>
<tr>
<td>7</td>
<td>BBB</td>
<td>2022-02</td>
<td>68.00</td>
<td>0.17</td>
</tr>
<tr>
<td>8</td>
<td>BBB</td>
<td>2022-03</td>
<td>170.00</td>
<td>0.34</td>
</tr>
<tr>
<td>9</td>
<td>BBB</td>
<td>2022-04</td>
<td>98.00</td>
<td>0.196</td>
</tr>
<tr>
<td>10</td>
<td>BBB</td>
<td>2022-05</td>
<td>230.00</td>
<td>0.46</td>
</tr>
</tbody>
</table>
</div>
<p>There are no aggregations you could make to the ratio that make any sense e.g. summing or averaging the ratio by customer or month would produce a meaningless figure. Therefore the ratio is a non-additive measure</p>
"
"72555404","Data warehousing relation between 2 fact tables with different grain for drill down in report tool?","<p>I've read through the DWH Toolkit and searched here on Stack Overflow.
There are different stores and HQ wants to have a report with their monthly revenue.
All data is normalized in the DWH and a periodic fact table is created for the reporting tool so that HQ can see the monthly revenue.</p>
<p>After half a year there is a request for change. HQ want to be able to drill down in the report to see more details for each store.</p>
<p>Since this is aggregated data a drill down is not possible. How to solve this?</p>
<ul>
<li>Create a fact with a lower grain and join via the DATE dimension to the lower grain (Kimball shows on p.81 in the DWH toolkit a date dimension) what I think is possbile to use to go from a higher level grain fact to a lower level grain fact with a join.</li>
<li>Create a new transactional star schema and do the aggregation in the reporting tool (for example PowerBI). Since there is a lower grain aggregation can be easily done.</li>
</ul>
","<data-warehouse><dm><kimball>","2022-06-09 06:04:34","74","-2","1","72585728","<p>You create a fact table at the lower grain. It is best practice to always create fact tables at the lowest possible grain - so that you don't get the exact issue you are now facing.</p>
<p>Your BI tool can do the aggregation or, if you are facing performance issues, you can create your own aggregated fact tables. How you drill down from from one fact table to another is normally dependent on your BI tool</p>
"
"72549899","Incorrect External column length in SSIS OLE DB source","<p>I am transferring data from database_1 to database_2, I created a table i the destination DB like the one in the source DB, but when I connect the OLE DB source to the table and open the Advanced editor I found that the External column length for a specific column is different, the length in the sql server is varchar (50) but in the SSIS in Advanced editor is DT_SRT (30). I tried to edit it but when I check again it turned into 30 again.</p>
<p>When I execute the package an error rise saying that truncatination occurred on the column and the execution stops.</p>
<pre><code>[Derived Column [2]] Error: The &quot;Derived Column&quot; failed because truncation occurred, and the 
truncation row disposition on &quot;Derived Column.Outputs[Derived Column 
Output].Columns[DBusinessTypeNameAR]&quot; specifies failure on truncation. A truncation error 
occurred on the specified object of the specified component.

[SSIS.Pipeline] Error: SSIS Error Code DTS_E_PROCESSINPUTFAILED.  The ProcessInput method on             
component &quot;Derived Column&quot; (2) failed with error code 0xC020902A while processing input 
&quot;Derived Column Input&quot; (3). The identified component returned an error from the ProcessInput 
method. The error is specific to the component, but the error is fatal and will cause the Data 
Flow task to stop running.  There may be error messages posted before this with more 
information about the failure.

[OLE DB Source [79]] Error: The attempt to add a row to the Data Flow task buffer failed with 
error code 0xC0047020.

[SSIS.Pipeline] Error: SSIS Error Code DTS_E_PRIMEOUTPUTFAILED.  The PrimeOutput method on OLE 
DB Source returned error code 0xC02020C4.  The component returned a failure code when the 
pipeline engine called PrimeOutput(). The meaning of the failure code is defined by the 
component, but the error is fatal and the pipeline stopped executing.  There may be error 
messages posted before this with more information about the failure.
</code></pre>
<p><strong>this is the source view :</strong></p>
<p><a href=""https://i.stack.imgur.com/WTsvM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WTsvM.png"" alt=""enter image description here"" /></a></p>
<p><strong>this is the destination table :</strong></p>
<p><a href=""https://i.stack.imgur.com/jiAwJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jiAwJ.png"" alt=""enter image description here"" /></a></p>
<p>this is the data flow :</p>
<p><a href=""https://i.stack.imgur.com/datLN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/datLN.png"" alt=""enter image description here"" /></a></p>
<p>this is the advance editor :</p>
<p><a href=""https://i.stack.imgur.com/DhmY1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DhmY1.png"" alt=""enter image description here"" /></a></p>
<p><strong>EDIT:</strong></p>
<pre><code>Microsoft SQL Server Data Tools for Visual Studio 2017 
Version 15.9.15
VisualStudio.15.Release/15.9.15+28307.812
Microsoft .NET Framework
Version 4.7.03062
****************************
SQL Server Data Tools   15.1.61906.03120
Microsoft SQL Server Data Tools

SQL Server Integration Services   15.0.1301.433
Microsoft SQL Server Integration Services Designer
Version 15.0.1301.433
</code></pre>
<hr />
<p>the Derived Column only contains code page correction as the screenshot below.</p>
<p><a href=""https://i.stack.imgur.com/9lXCS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9lXCS.png"" alt=""enter image description here"" /></a></p>
","<sql-server><ssis><data-warehouse>","2022-06-08 17:28:20","534","1","1","72581374","<p>There are two different things going on here.</p>
<p><strong>1. String truncation error from a Derived Column</strong></p>
<pre><code>[Derived Column [2]] Error: The &quot;Derived Column&quot; failed because truncation occurred, and the 
truncation row disposition on &quot;Derived Column.Outputs[Derived Column 
Output].Columns[DBusinessTypeNameAR]&quot; specifies failure on truncation. A truncation error 
occurred on the specified object of the specified component.
</code></pre>
<p>Note that the truncation message is coming from your Derived Column not the destination column. The derived column's length is 10, not 30 (or 50). Increasing the size of your DBusinessTypeNameAR derived column to 30 <code>(DT_STR,30,1252)</code> or 50 <code>(DT_STR,50,1252)</code> should remove the error.</p>
<p><a href=""https://i.stack.imgur.com/A3OfW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A3OfW.png"" alt=""enter image description here"" /></a></p>
<p><strong>2. Out-of-sync destination column metadata</strong></p>
<p>If your design-time database's column meta-data does not match the run-time database 's column meta-data (you said in the comments your target database connection string was set by an expression) this may explain the out-of-sync destination database column metadata.</p>
<p>If you've ensured that the two db 'scenarios' (design-time db and run-time db) have the same field meta-data, the simplest way to re-sync out-of-sync destination table column meta-data for OLEDB Database destination component is to:</p>
<ol>
<li>Inside the OLE DB Destination Editor, click the table drop-down list to expand the list of target tables.</li>
<li>Select a different table in the drop-down list.</li>
<li>Move to a different &quot;tab&quot; in the editor.</li>
<li>Back on the Connection Manager tab, select the original table from the table drop-down, and make sure that all mappings are in place (Mappings tab)</li>
<li>Click ok and Save changes.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/OYd7e.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OYd7e.png"" alt=""enter image description here"" /></a></p>
"
"72517763","What database should I use for storing and analyzing blog post data warehouse? Relational vs No-SQL?","<p>I plan to scrape blog posts from some sources and build a blog post data warehouse for analytics purpose. For example, listing the trending topics among millions of blog posts. I was wondering what type of database I should use for storing analyzing such big data. Relational or No-SQL? If No-SQL, which one?</p>
<p>I would appreciate your suggestions.</p>
","<database><nosql>","2022-06-06 12:33:18","54","0","1","74849717","<p>Because the analytics you plan on running depend on the words inside each article, that sounds like a use case that could be addressed nicely by Elasticsearch (NoSQL)</p>
"
"72488767","Cropping a large file of data","<p>I have a massive file of data that is to big to process in any way. The file is structured like below. To be able to process the data i would like to crop out steps of data so i only have every 50 step in the file. It would then end up with data from Direct configuration=     1, Direct configuration=     50 and so on. Does anyone have any idea where to start to achieve this?</p>
<pre><code>Generic Input
           1
     9.911879   -0.001977    0.001562
    -0.002156   10.766003    0.001154
     0.002104    0.001503   12.939863
   Si   Ge   Li   N    O
     8    24    32    32    32
Direct configuration=     1
   0.24672335  0.21343837  0.06644162
   0.25680732  0.21274439  0.56108128
   0.75202052  0.21114493  0.06961516
   .
   .
   .
Direct configuration=     2
   0.24650415  0.21279551  0.06873548
   0.25388539  0.21261272  0.55851287
   0.74968128  0.20877708  0.06763310
   .
   .
   .
</code></pre>
<p>and so on</p>
","<file><editing><data-management>","2022-06-03 11:13:59","17","0","1","72488831","<p>When you read the file you can make diffrent statements and change the destination if the mod of row is 0.
like:</p>
<pre><code>count = 0
with open(file,''r) as file:
  for row in file:
    count +=1
    if count % 50 = 0
      --change destination--
</code></pre>
"
"72483570","How to represent different business facts in a single fact table?","<p>I have the following dimensional tables:</p>
<ul>
<li>DimUser</li>
<li>DimClient</li>
<li>DimLocation</li>
<li>DimDate</li>
<li>DimTime</li>
<li>DimLog</li>
<li>DimStatuses</li>
</ul>
<p>How could I represent the following events in the fact table?</p>
<ol>
<li>Logs by user over time</li>
<li>User status change over time</li>
</ol>
<p>Let's say the fact table is something like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>application_id</th>
<th>location_id</th>
<th>user_id</th>
<th>client_id</th>
<th>log_id</th>
<th>date_id</th>
<th>time_id</th>
<th>status_id</th>
</tr>
</thead>
<tbody>
<tr>
<td>3</td>
<td>19</td>
<td>3</td>
<td>2</td>
<td>69</td>
<td>45</td>
<td>64</td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>23</td>
<td>1</td>
<td>1</td>
<td></td>
<td>10</td>
<td>207</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>Is it a valid representation where the first record refers to a log event and the second record refers to an user status change?</p>
","<data-modeling><data-warehouse><star-schema><fact-table>","2022-06-03 00:27:54","67","0","2","72487075","<p>Every measure needs to have its grain defined. If 2 measures have the same grain then they can, but don’t have to, be stored in the same fact table. If 2 measures don’t have the same grain then they can’t be stored in the same fact table</p>
"
"72483570","How to represent different business facts in a single fact table?","<p>I have the following dimensional tables:</p>
<ul>
<li>DimUser</li>
<li>DimClient</li>
<li>DimLocation</li>
<li>DimDate</li>
<li>DimTime</li>
<li>DimLog</li>
<li>DimStatuses</li>
</ul>
<p>How could I represent the following events in the fact table?</p>
<ol>
<li>Logs by user over time</li>
<li>User status change over time</li>
</ol>
<p>Let's say the fact table is something like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>application_id</th>
<th>location_id</th>
<th>user_id</th>
<th>client_id</th>
<th>log_id</th>
<th>date_id</th>
<th>time_id</th>
<th>status_id</th>
</tr>
</thead>
<tbody>
<tr>
<td>3</td>
<td>19</td>
<td>3</td>
<td>2</td>
<td>69</td>
<td>45</td>
<td>64</td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>23</td>
<td>1</td>
<td>1</td>
<td></td>
<td>10</td>
<td>207</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>Is it a valid representation where the first record refers to a log event and the second record refers to an user status change?</p>
","<data-modeling><data-warehouse><star-schema><fact-table>","2022-06-03 00:27:54","67","0","2","73082830","<p>Different facts should be represented in the schema under different fact tables. For this case, there should be a fact table for logs by user and another fact table for user status change over time, this is known as a galaxy schema or multi fact schema, so in other words it should look like this:</p>
<p><a href=""https://i.stack.imgur.com/cfYa8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cfYa8.jpg"" alt=""Galaxy schema with user status change over time and logs by user fact tables"" /></a></p>
<p>So, it's feasible because you can create your measures according your grain.</p>
<ul>
<li><p>Number of users by application (user status change over time fact
table)</p>
</li>
<li><p>Number of logs or events by user (logs by user fact table)</p>
</li>
<li><p>Active/Inactive users (user status change over time fact table) user</p>
</li>
<li><p>logs over time (logs by user fact table)</p>
</li>
</ul>
"
"72476769","AWS glue job (Pyspark) to AWS glue data catalog","<p>We know that,
the procedure of writing from pyspark script (aws glue job) to AWS data catalog is to write in s3 bucket (eg.csv) use a crawler and schedule it.</p>
<p>Is there any other way of writing to aws glue data catalog?
I am looking for a direct way to do this.Eg. writing as a s3 file and sync to the aws glue data catalog.</p>
","<amazon-web-services><aws-glue><aws-glue-data-catalog><aws-glue-spark>","2022-06-02 13:04:18","808","0","3","72903767","<p>You may manually specify the table. The crawler only discovers the schema. If you set the schema manually, you should be able to read your data when you run the AWS Glue Job.</p>
"
"72476769","AWS glue job (Pyspark) to AWS glue data catalog","<p>We know that,
the procedure of writing from pyspark script (aws glue job) to AWS data catalog is to write in s3 bucket (eg.csv) use a crawler and schedule it.</p>
<p>Is there any other way of writing to aws glue data catalog?
I am looking for a direct way to do this.Eg. writing as a s3 file and sync to the aws glue data catalog.</p>
","<amazon-web-services><aws-glue><aws-glue-data-catalog><aws-glue-spark>","2022-06-02 13:04:18","808","0","3","73579671","<p>We have had this same problem for one of our customers who had millions of small files within AWS S3. The crawler practically would stall and not proceed and continue to run infinitely. We came up with the following alternative approach :</p>
<ol>
<li>A Custom Glue Python Shell job was written which leveraged AWS Wrangler to fire queries towards AWS Athena.</li>
<li>The Python Shell job would List the contents of folder s3:///event_date=&lt;Put the Date Here from #2.1&gt;</li>
<li>The queries fired :</li>
</ol>
<p>alter table  add partition (event_date='&lt;event_date from above&gt;',eventname=’List derived from above S3 List output’)
4. This was triggered to run post the main Ingestion Job via Glue Workflows.</p>
"
"72476769","AWS glue job (Pyspark) to AWS glue data catalog","<p>We know that,
the procedure of writing from pyspark script (aws glue job) to AWS data catalog is to write in s3 bucket (eg.csv) use a crawler and schedule it.</p>
<p>Is there any other way of writing to aws glue data catalog?
I am looking for a direct way to do this.Eg. writing as a s3 file and sync to the aws glue data catalog.</p>
","<amazon-web-services><aws-glue><aws-glue-data-catalog><aws-glue-spark>","2022-06-02 13:04:18","808","0","3","74600979","<p>If you are not expecting schema to change, use Glue job directly after creating manually tables using Glue Database and Table.</p>
"
"72467058","Show Top Results But Limiting Ocurrences by Another Table in SQL","<p>Here's what I have:</p>
<ul>
<li>Table with all the sales.</li>
<li>Table with the vendors.</li>
<li>Table with the products.</li>
</ul>
<p>What I need to achieve, is to get a query that would show the top 3 products sold by each vendor along with the amount sold by that vendor for the product and then sort it from top to bottom by the sales made by the vendor in general while showing all the vendors.</p>
<p>So, for example, the vendors are: Mike, Lucas, Amy, Bob, Matt and Agatha and there are 10 products.</p>
<p>The expected output would be:</p>
<ul>
<li>Mike -cereal - $400</li>
<li>Mike -juice -$100</li>
<li>Mike -soap $50</li>
<li>Amy -soap $200</li>
<li>Amy -lettuce $150</li>
<li>Amy -cheese $100</li>
</ul>
<p>And so on...</p>
<p>I tried this code but only shows the top selling item for the vendor:</p>
<pre><code>SELECT v.vendor_name. p.product_name, sum(f.total) as total
FROM vendors v, sales f, products p
WHERE v.id_vendor = f.vendor and p.id_product = f.product
GROUP BY v.vendor_name, p.product_name
ORDER BY total DESC
</code></pre>
<p>Some samples of the tables:</p>
<p><a href=""https://i.stack.imgur.com/YpuTC.png"" rel=""nofollow noreferrer"">Sales table</a></p>
<p><a href=""https://i.stack.imgur.com/hDpDV.png"" rel=""nofollow noreferrer"">Vendor table</a></p>
<p><a href=""https://i.stack.imgur.com/aeh73.png"" rel=""nofollow noreferrer"">Products table</a></p>
<p><a href=""https://i.stack.imgur.com/08fBJ.png"" rel=""nofollow noreferrer"">Current Output</a></p>
<p><a href=""https://i.stack.imgur.com/8FIcf.png"" rel=""nofollow noreferrer"">These are the best vendors</a></p>
<p>Like the last image displays, that should be the order in which the vendors appear followed by the column of their 3 top sold products and the amount sold for each one.</p>
<p>So the first vendor would appear 3 times, along with the 3 top sold products and their respective amount.</p>
<p><a href=""https://i.stack.imgur.com/dkVBi.png"" rel=""nofollow noreferrer"">Best Vendor</a></p>
<p>Followed by <a href=""https://i.stack.imgur.com/aK27t.png"" rel=""nofollow noreferrer"">Second best</a>.</p>
","<sql><apache-spark><databricks><data-warehouse><azure-data-lake>","2022-06-01 19:18:52","65","0","1","72526708","<p><em>We tried to reproduce the same scenario and we did it with using <code>rank and partition by</code>.</em></p>
<p><em><strong>Example</strong> :-<br />
Sale Table</em></p>
<p><img src=""https://i.imgur.com/4wPS3GU.png"" alt=""enter image description here"" /></p>
<p><em>Product Table</em><br />
<img src=""https://i.imgur.com/RqReRnd.png"" alt=""enter image description here"" /></p>
<p><em>Vendor table</em><br />
<img src=""https://i.imgur.com/Bs3gaGW.png"" alt=""enter image description here"" /></p>
<pre class=""lang-sql prettyprint-override""><code>select ts.vendor_name,  
ts.product_name,  
ts.Total_amount from (  
select tbl_vendor.vendor_name,  
tbl_products.product_name,  
sum(tbl_products.product_amount) 'Total_amount',  
rank() over  
(partition by tbl_vendor.vendor_name order by sum(tbl_products.product_amount) desc) as 'rank'  
from tbl_sale  
inner join tbl_vendor  
on tbl_sale.vendor_id = tbl_vendor.vendor_id  
inner join tbl_products  
on tbl_sale.product_id = tbl_products.product_id  
  
group by tbl_products.product_name,tbl_vendor.vendor_name --order by tbl_vendor.vendor_name,Total_amount desc  
) ts where ts.rank &lt;= 3  
</code></pre>
<p><em>Output</em></p>
<p><img src=""https://i.imgur.com/c8sYHe3.png"" alt=""enter image description here"" /></p>
"
"72457676","Great Expectations list total unique values","<p>I have run Great Expectation check expect_column_values_to_be_unique check on one of the column. It produced the following result as below.Total There are 62 Duplicates but in the output list it is returning only 20 elements. How to retrieve all duplicate records in that column.
<code>df.expect_column_values_to_be_unique('A')</code></p>
<pre><code>  &quot;exception_info&quot;: null,
  &quot;expectation_config&quot;: {
    &quot;expectation_type&quot;: &quot;expect_column_values_to_be_unique&quot;,
    &quot;kwargs&quot;: {
      &quot;column&quot;: &quot;A&quot;,
      &quot;result_format&quot;: &quot;BASIC&quot;
    },
    &quot;meta&quot;: {}
  },
  &quot;meta&quot;: {},
  &quot;success&quot;: false,
  &quot;result&quot;: {
    &quot;element_count&quot;: 100,
    &quot;missing_count&quot;: 0,
    &quot;missing_percent&quot;: 0.0,
    &quot;unexpected_count&quot;: 62,
    &quot;unexpected_percent&quot;: 62.0,
    &quot;unexpected_percent_nonmissing&quot;: 62.0,
    &quot;partial_unexpected_list&quot;: [
      37,
      62,
      72,
      53,
      22,
      61,
      95,
      21,
      64,
      59,
      77,
      53,
      0,
      22,
      24,
      46,
      0,
      16,
      78,
      60
    ]
  }
}
</code></pre>
","<python><pyspark><apache-spark-sql><data-quality><great-expectations>","2022-06-01 07:11:03","703","0","2","72462496","<p>I think you are using &quot;<a href=""https://sparkbyexamples.com/spark/spark-show-full-column-content-dataframe/"" rel=""nofollow noreferrer"">show</a>&quot; without parameters.  By default this only shows the first 20 rows.  If you wish to see more you need to pass in how many rows you want to see: (This will show you 200 rows, and not truncate the length of the column)</p>
<pre><code>df.select( col(&quot;*&quot;) ).show(200,false)
</code></pre>
"
"72457676","Great Expectations list total unique values","<p>I have run Great Expectation check expect_column_values_to_be_unique check on one of the column. It produced the following result as below.Total There are 62 Duplicates but in the output list it is returning only 20 elements. How to retrieve all duplicate records in that column.
<code>df.expect_column_values_to_be_unique('A')</code></p>
<pre><code>  &quot;exception_info&quot;: null,
  &quot;expectation_config&quot;: {
    &quot;expectation_type&quot;: &quot;expect_column_values_to_be_unique&quot;,
    &quot;kwargs&quot;: {
      &quot;column&quot;: &quot;A&quot;,
      &quot;result_format&quot;: &quot;BASIC&quot;
    },
    &quot;meta&quot;: {}
  },
  &quot;meta&quot;: {},
  &quot;success&quot;: false,
  &quot;result&quot;: {
    &quot;element_count&quot;: 100,
    &quot;missing_count&quot;: 0,
    &quot;missing_percent&quot;: 0.0,
    &quot;unexpected_count&quot;: 62,
    &quot;unexpected_percent&quot;: 62.0,
    &quot;unexpected_percent_nonmissing&quot;: 62.0,
    &quot;partial_unexpected_list&quot;: [
      37,
      62,
      72,
      53,
      22,
      61,
      95,
      21,
      64,
      59,
      77,
      53,
      0,
      22,
      24,
      46,
      0,
      16,
      78,
      60
    ]
  }
}
</code></pre>
","<python><pyspark><apache-spark-sql><data-quality><great-expectations>","2022-06-01 07:11:03","703","0","2","72465801","<p>You're currently passing <code>result_format</code> as <code>BASIC</code>. To get the level of detail you're looking for, you'll want to instead pass <code>result_format</code> for this Expectation as <code>COMPLETE</code> to get the full list of unexpected values. For example:</p>
<pre><code>df.expect_column_values_to_be_unique(column=&quot;A&quot;, result_format=&quot;COMPLETE&quot;)
</code></pre>
<p>See <a href=""https://docs.greatexpectations.io/docs/reference/expectations/result_format"" rel=""nofollow noreferrer"">this documentation</a> for more on <code>result_format</code>.</p>
"
"72450396","DBT - write dbt test --store-failures to a specific table in my data warehouse","<p>Good afternoon,
I want to write the <em>dbt test</em> values to a specific table in my data warehouse.
I have tested multiple schema inclusions in all the possible .yml files and I am not really finding the correct place to specify to which database I want the tests to be recorded.
Nowadays, it always answers me with the error of not having the permissions to perform a <em>glue:CreateDatabase</em> action, which in fact is not what I want to do, but rather write to table specified by me.
To conclude, what I am asking here is <strong>how can I specify where the dbt tests results are being written, instead of letting dbt create and store the values in the default schemas?</strong>
If somebody could help me on this I would really appreciate it!</p>
","<amazon-web-services><yaml><dbt>","2022-05-31 15:21:41","2144","0","1","72450920","<p>Well, I managed to fix it, I was almost throwing my computer away, because I didn't have more air to pull off, but basically we can specify the target database in the dbt_project.yml. To do that, just add to the dbt_project.yml</p>
<pre><code>tests:
  +store_failures: true
  +schema: &quot;path that you want&quot;
</code></pre>
<p>I did not find any information in dbt documentation, neither community forums, so it was just an iterative process of testing possible approaches</p>
"
"72436777","Is the storage and compute decoupled in modern cloud data warehouses?","<ul>
<li><p>In Redshift, Snowflake, and Azure SQL DW, do we have storage and compute decoupled?</p>
<ul>
<li>If they are decoupled, is there any use of &quot;External Tables&quot; still or they are gone?</li>
</ul>
</li>
<li><p>When Compute and Storage were tightly coupled, and when we wanted to scale, we scaled both compute and storage. But under the hoods, was it a virtual machine and we scaled the compute and the VMs disks? Do you guys have maybe some readings on this?</p>
</li>
</ul>
<p>Massive thanks, I am confused now and it would be a blessing if someone could jump in to explain!</p>
","<amazon-redshift><data-warehouse>","2022-05-30 15:34:14","244","0","2","72437612","<p>You have reason to be confused as there is a heavy layer of marketing being applied in a lot of places.  Let's start with some facts:</p>
<p>All databases need local disk to operate.  This disk can store permanent versions of the tables (classic locally stored tables and is needed to store the local working set of data for the database to operate.  Even in cases where no tables are permanently stored on local disk the size of the local disks is critical as this allows for date fetched from remote storage to be worked upon and cached.</p>
<p>Remote storage of permanent tables comes in 2 &quot;flavors&quot; - defined external tables and transparent remote tables.  While there are lots of differences in how these flavors work and how each different database optimizes them they all store the permanent version of the table on disks that are remote from the database compute system(s).</p>
<p>Remote permanent storage comes with pros and cons.  &quot;Decoupling&quot; is the most often cited advantage for remote permanent storage.  This just means that you cannot fill up the local disks with the storage of &quot;cold&quot; data as only &quot;in use&quot; data is stored on the local disks in this case.  To be clear you can fill up (or brown out) the local disks even with remote permanent storage if the working set of data is too large.  The downside of remote permanent storage is that the data is remote.  Being across a network to some flexible storage solution means that getting to the data takes more time (with all the database systems having their own methods to hide this in as many cases as possible).  This also means that the coherency control for the data is also across the network (in some aspect) and also comes with impacts.</p>
<p>External tables and transparent remote tables are both permanently stored remotely but there are differences.  An external table isn't under the same coherency structure that a fully-owned table is under (whether local or remote).  Transparent remote just implies that the database is working with the remote table &quot;as if&quot; it is locally owned.</p>
<p>VMs don't change the local disk situation.  An amount of disk is apportioned to each VM in the box and an amount of local disk is allocated to each VM.  The disks are still local, it's just that only a portion of the physical disks are addressable by any one VM.</p>
<p>So leaving fact and moving to opinion.  While marketing will tell you why one type of database storage is better than the other in all cases this just isn't true.  Each has advantages and disadvantages and which is best for you will depend on what your needs are.  The database providers that offer only one data organization will tell you that this is the best option, and it is for some.</p>
<p>Local table storage will always be faster for those applications where speed of access to data is critical and caching doesn't work.  However, this means that DBAs will need to do the work to maintain the on-disk data is optimized and fits is the available local storage (for the compute size needed).  This is real work and takes time an energy.  What you gain in moving remote is the reduction of this work but it comes at the cost of some combination of database cost, hardware cost, and/or performance.  Sometimes worth the tradeoff, sometimes not.</p>
"
"72436777","Is the storage and compute decoupled in modern cloud data warehouses?","<ul>
<li><p>In Redshift, Snowflake, and Azure SQL DW, do we have storage and compute decoupled?</p>
<ul>
<li>If they are decoupled, is there any use of &quot;External Tables&quot; still or they are gone?</li>
</ul>
</li>
<li><p>When Compute and Storage were tightly coupled, and when we wanted to scale, we scaled both compute and storage. But under the hoods, was it a virtual machine and we scaled the compute and the VMs disks? Do you guys have maybe some readings on this?</p>
</li>
</ul>
<p>Massive thanks, I am confused now and it would be a blessing if someone could jump in to explain!</p>
","<amazon-redshift><data-warehouse>","2022-05-30 15:34:14","244","0","2","74056742","<p>When it comes to the concept of separating (or de-coupling) Cloud Compute vs. Cloud Storage, the concepts can become a little confusing. In short, true decoupling generally requires object level storage vs. faster traditional block storage (traditionally on-premises and also called local storage). The main reason for this is that object storage is flat, without a hierarchy and therefore scales linearly with the amount of data you add. It therefore winds up also being cheaper as it is extremely distributed, redundant, and easily re-distributed and duplicated.</p>
<p>This is all important because in order to decouple storage from compute in the cloud or any large distributed computing paradigm you need to shard (split) your data (storage) amongst your compute nodes... so as your storage grows linearly, object storage which is flat -- allows that to happen without any penalty in performance -- while you can (practically) instantly &quot;remaster&quot; your compute nodes so that they can evenly distribute the workload again as you scale your compute up or down or to withstand network/node failures.</p>
"
"72407823","Split Records in Table using SQL","<p>I have an table with the below records</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>PREFIX</th>
<th>BEGIN</th>
<th>END</th>
<th>DIV</th>
<th>INDUSTRY</th>
</tr>
</thead>
<tbody>
<tr>
<td>1AB</td>
<td>00001</td>
<td>99999</td>
<td>LPSD</td>
<td>RI</td>
</tr>
<tr>
<td>1AB</td>
<td>00527</td>
<td>00528</td>
<td>MHUG</td>
<td>RI</td>
</tr>
</tbody>
</table>
</div>
<p>I want to split it as below</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>PREFIX</th>
<th>BEGIN</th>
<th>END</th>
<th>DIV</th>
<th>INDUSTRY</th>
</tr>
</thead>
<tbody>
<tr>
<td>1AB</td>
<td>00001</td>
<td>99999</td>
<td>LPSD</td>
<td>RI</td>
</tr>
<tr>
<td>1AB</td>
<td>00527</td>
<td>00528</td>
<td>MHUG</td>
<td>RI</td>
</tr>
<tr>
<td>1AB</td>
<td>00529</td>
<td>99999</td>
<td>LPSD</td>
<td>RI</td>
</tr>
</tbody>
</table>
</div>
<p>If you see as because we have a record with the same prefix but begin and end are different, i want to split the first record into 2 records, one from 00001 to 00526 and 00529 to 99999.</p>
<p>Is it possible to perform the same using the SQL query</p>
","<sql><oracle><oracle11g><data-warehouse>","2022-05-27 15:36:38","73","0","2","72409128","<p>I think this works (but it would need a bigger data set to test it properly):</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT prefix,
       &quot;BEGIN&quot;,
       &quot;END&quot;,
       CASE type
       WHEN 1
       THEN div
       ELSE LAG(CASE type WHEN 1 THEN div END) IGNORE NULLS
              OVER (PARTITION BY prefix, industry, grp ORDER BY &quot;BEGIN&quot;)
       END AS div,
       industry
FROM   (
  SELECT prefix,
         div,
         industry,
         value AS &quot;BEGIN&quot;,
         type,
         SUM(type) OVER (PARTITION BY prefix, industry ORDER BY value) AS grp,
         LEAD(value) OVER (PARTITION BY prefix, industry ORDER BY value) AS &quot;END&quot;
  FROM   table_name
  UNPIVOT (value FOR type IN (&quot;BEGIN&quot; AS 1, &quot;END&quot; AS -1))
)
WHERE  grp &gt; 0
ORDER BY prefix, industry, &quot;BEGIN&quot;;
</code></pre>
<p>Which, for the sample data:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE table_name (prefix, &quot;BEGIN&quot;, &quot;END&quot;, div, industry) AS
SELECT '1AB', '00001', '99999', 'LPSD', 'RI' FROM DUAL UNION ALL
SELECT '1AB', '00527', '00528', 'MHUG', 'RI' FROM DUAL;
</code></pre>
<p>Outputs:</p>
<blockquote>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">PREFIX</th>
<th style=""text-align: left;"">BEGIN</th>
<th style=""text-align: left;"">END</th>
<th style=""text-align: left;"">DIV</th>
<th style=""text-align: left;"">INDUSTRY</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">1AB</td>
<td style=""text-align: left;"">00001</td>
<td style=""text-align: left;"">00527</td>
<td style=""text-align: left;"">LPSD</td>
<td style=""text-align: left;"">RI</td>
</tr>
<tr>
<td style=""text-align: left;"">1AB</td>
<td style=""text-align: left;"">00527</td>
<td style=""text-align: left;"">00528</td>
<td style=""text-align: left;"">MHUG</td>
<td style=""text-align: left;"">RI</td>
</tr>
<tr>
<td style=""text-align: left;"">1AB</td>
<td style=""text-align: left;"">00528</td>
<td style=""text-align: left;"">99999</td>
<td style=""text-align: left;"">LPSD</td>
<td style=""text-align: left;"">RI</td>
</tr>
</tbody>
</table>
</div></blockquote>
<p><em>db&lt;&gt;fiddle <a href=""https://dbfiddle.uk/?rdbms=oracle_21&amp;fiddle=19ccaeec0067fe099a3f630e04d8dbbb"" rel=""nofollow noreferrer"">here</a></em></p>
"
"72407823","Split Records in Table using SQL","<p>I have an table with the below records</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>PREFIX</th>
<th>BEGIN</th>
<th>END</th>
<th>DIV</th>
<th>INDUSTRY</th>
</tr>
</thead>
<tbody>
<tr>
<td>1AB</td>
<td>00001</td>
<td>99999</td>
<td>LPSD</td>
<td>RI</td>
</tr>
<tr>
<td>1AB</td>
<td>00527</td>
<td>00528</td>
<td>MHUG</td>
<td>RI</td>
</tr>
</tbody>
</table>
</div>
<p>I want to split it as below</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>PREFIX</th>
<th>BEGIN</th>
<th>END</th>
<th>DIV</th>
<th>INDUSTRY</th>
</tr>
</thead>
<tbody>
<tr>
<td>1AB</td>
<td>00001</td>
<td>99999</td>
<td>LPSD</td>
<td>RI</td>
</tr>
<tr>
<td>1AB</td>
<td>00527</td>
<td>00528</td>
<td>MHUG</td>
<td>RI</td>
</tr>
<tr>
<td>1AB</td>
<td>00529</td>
<td>99999</td>
<td>LPSD</td>
<td>RI</td>
</tr>
</tbody>
</table>
</div>
<p>If you see as because we have a record with the same prefix but begin and end are different, i want to split the first record into 2 records, one from 00001 to 00526 and 00529 to 99999.</p>
<p>Is it possible to perform the same using the SQL query</p>
","<sql><oracle><oracle11g><data-warehouse>","2022-05-27 15:36:38","73","0","2","72410355","<p>First of all it is not a good practice to name columns after reserved words (like Begin, End ...). This could be solved using MODEL clause like this...</p>
<pre><code>    WITH
        tbl AS
            (   Select '1AB' &quot;PRFX&quot;, '00001' &quot;BGN&quot;, '99999' &quot;ND&quot;, 'LPSD' &quot;DV&quot;, 'RI' &quot;IND&quot; From Dual UNION ALL
                Select '1AB' &quot;PRFX&quot;, '00527' &quot;BGN&quot;, '00528' &quot;ND&quot;, 'MHUG' &quot;DV&quot;, 'RI' &quot;IND&quot; From Dual
            ) 
    SELECT DISTINCT
        m.*,
        t.DV &quot;DV&quot;,
        t.IND &quot;IND&quot;
    FROM
        (
    SELECT
        INDX,
        PRFX, BGN, ND
    FROM
        (   SELECT
                PRFX &quot;PRFX&quot;,
                LISTAGG(BGN, ',') WITHIN GROUP (ORDER BY PRFX, BGN) &quot;BGN&quot;,
                LISTAGG(ND, ',') WITHIN GROUP (ORDER BY PRFX, ND) &quot;ND&quot;
            FROM
                tbl 
            GROUP BY
                PRFX
        )
    MODEL
        DIMENSION BY(0 as INDX)
        MEASURES (PRFX, BGN, ND)
            RULES ITERATE(3) 
                (   PRFX[ITERATION_NUMBER+1] = PRFX[0],
                    BGN[ITERATION_NUMBER+1] = Nvl(SubStr(REPLACE(BGN[0], ',', ''), (ITERATION_NUMBER*5) + 1, 5), LPAD(To_Number(SubStr(REPLACE(ND[0], ',', ''), ((ITERATION_NUMBER-2)*5) + 1, 5)) + 1, 5, '0')),
                    ND[ITERATION_NUMBER+1] = CASE 
                                                WHEN SubStr(REPLACE(BGN[0], ',', ''), ((ITERATION_NUMBER+1)*5) + 1, 5) &lt; SubStr(REPLACE(ND[0], ',', ''), (ITERATION_NUMBER*5) + 1, 5) THEN
                                                    LPAD(To_Number(SubStr(REPLACE(BGN[0], ',', ''), ((ITERATION_NUMBER+1)*5) + 1, 5)) - 1, 5, '0')
                                              ELSE 
                                                  SubStr(REPLACE(ND[0], ',', ''), ((ITERATION_NUMBER-1)*5) + 1, 5) 
                                              END
                )
        ) m
    LEFT JOIN
        (   SELECT 
                PRFX, DV, 
                IND, 
                Max(ND) OVER(PARTITION BY PRFX, DV ORDER BY PRFX, DV) &quot;MAX_ND&quot;,
                Min(BGN) OVER(PARTITION BY PRFX, DV ORDER BY PRFX, DV) &quot;MIN_BGN&quot;
            FROM 
                tbl
        )  t ON (t.PRFX = m.PRFX And (t.MAX_ND = m.ND OR t.MIN_BGN = m.BGN)) 
WHERE
    m.INDX &gt; 0 And ND Is Not Null
ORDER BY 
    m.INDX
    --
    -- Result
    -- INDX PRFX BGN    ND      DV      IND
    -- 1    1AB  00001  00526   LPSD    RI
    -- 2    1AB  00527  00528   MHUG    RI
    -- 3    1AB  00529  99999   LPSD    RI
</code></pre>
<p>Hopefully it will help. Regards...</p>
"
"72404368","Getting data quality in Delta Live Table (bronze, gold, silver..)","<p>How to check if Delta Live Table is in bronze, gold or silver layer(zone) with python? I have notebook for creating Delta Live Table pipeline, and I need to know what is quality of data(silver, bronze, gold). How to get that information with python?</p>
<p>In SQL exist something like <code>TBLPROPERTIES('BRONZE')</code>, is there anything like that for python?</p>
","<python><databricks><azure-databricks><delta-live-tables>","2022-05-27 11:01:21","533","1","2","72405660","<p>If you have a sql solution run it directly with</p>
<pre class=""lang-py prettyprint-override""><code>spark.sql('''SHOW TBLPROPERTIES table_name'')
</code></pre>
"
"72404368","Getting data quality in Delta Live Table (bronze, gold, silver..)","<p>How to check if Delta Live Table is in bronze, gold or silver layer(zone) with python? I have notebook for creating Delta Live Table pipeline, and I need to know what is quality of data(silver, bronze, gold). How to get that information with python?</p>
<p>In SQL exist something like <code>TBLPROPERTIES('BRONZE')</code>, is there anything like that for python?</p>
","<python><databricks><azure-databricks><delta-live-tables>","2022-05-27 11:01:21","533","1","2","72953171","<p>You can do something like this:</p>
<pre><code>@dlt.table(
  comment=&quot;Bronze live streaming table for Test data&quot;,
  name=&quot;bronze_test_table&quot;,
  table_properties={
    &quot;quality&quot;: &quot;bronze&quot;
  }
)
</code></pre>
"
"72393933","Primary and Foreign Key in DW tables","<p>I've read that dimension tables hold the primary key and and fact tables contain the foreign key which references the primary key of Dimension tables.</p>
<p>Now the confusion I am having is this  - suppose I have an ETL pipeline which populates the dimension table (let's say customer) from a source (say another DB). Let's assume this is a frequently changing table and has over 200 columns. How do I incorporate these changes in the dimension tables? I want to have only the latest record for each customer (type 1 SCD) in the DWH.</p>
<p>One thing what I could do is delete the row in the dimension table and re-insert the new updated row. But this approach won't work because of the primary key - foreign key constraint (which will not allow me to delete the record).</p>
<p>Should I write an update statement with all 200 columns in the ETL script? Or is there any other approach?</p>
","<data-warehouse>","2022-05-26 15:03:21","38","0","1","72413433","<p>Strictly speaking you just need to update the fields that changed. But the cost of updating all in a single record is probably similar (assuming it’s row based storage), and it’s probably easier to write.</p>
<p>You can’t delete and re-insert, as the new row will have a new PK and old facts will no longer be linked.</p>
"
"72374176","How to only replace NA with specific values based on a condition in another variable","<p>I need help to a solution. I have tried reading many earlier questions and answer but without a solution to my case. I want to keep the values in every variable but in the variable Udd I only want to vhange the NA values but keep the rest as it is.</p>
<p>Every time Udd is NA, I want it to change according to Edu.</p>
<p><em>When Edu is from 1:7, I want Udd to be 1.
When Edu is from 8:11, I want Udd to be 2.
When Edu is from 12 and above, I want Udd to be 3.</em></p>
<pre><code>library(dplyr)
ID &lt;- c(1,2,3,4,5,6,7,8,9,10)
Edu &lt;- c(4,7,9,13,15,18,11,10,12,8)
Udd &lt;- c(1,NA,2,NA,3,3,2,NA,3,2)
d &lt;- data.frame(ID, Edu, Udd)
d &lt;- d %&gt;%
  mutate(Udd = case_when(is.na(Udd) &amp; Edu &lt; 8 ~ 1,
                         is.na(Udd) &amp; Edu[c(8,11)] ~ 2,
                         is.na(Udd) &amp; Edu &gt; 11 ~ 3))
</code></pre>
<p>It changes NA but returns rest as NA. I only want it to change the NA values, and keep the Udd values that are not NA as they were.</p>
","<r><dplyr><conditional-statements><na><data-management>","2022-05-25 08:19:15","570","0","1","72383252","<p>Changed the vector to have an %in% statement and added an else statement.</p>
<pre><code>d %&gt;%
  mutate(Udd = case_when(is.na(Udd) &amp; Edu &lt; 8 ~ 1,
                         is.na(Udd) &amp; Edu %in% c(8:11) ~ 2,
                         is.na(Udd) &amp; Edu &gt; 11 ~ 3,
                         TRUE ~ Udd))
</code></pre>
"
"72324972","Variable scope in Synapse data warehouse - Must declare the scalar variable (very clearly declared)","<p>Why am I getting the error?  Working in synapse data warehouse.</p>
<blockquote>
<p>Msg 137, Level 15, State 2, Line 1<br />
Must declare the scalar variable &quot;@p_src_id&quot;</p>
</blockquote>
<p>I declared it. I tinkered with it and it will work if I plug <code>GETDATE()</code> into the query in place of the variable, but it will not work with the variable. I am capable of a work-around; I just want to understand the problem.</p>
<pre><code>DECLARE @p_src_id   INT          = 4389,
        @v_ts       DATETIME2(6) = GETDATE(),
        @v_rnk      INT;

SELECT @v_rnk = CASE WHEN @v_ts BETWEEN dt.eff_dt AND dt.exp_dt
                     THEN 1
                     ELSE 0
                END
FROM (SELECT MIN(eff_dt) AS eff_dt,
             MIN(exp_dt) AS exp_dt
      FROM dbo.my_dates
      WHERE src_id = @p_src_id) AS dt;

----THIS WORKS!!
--SELECT @v_rnk = CASE WHEN GETDATE() BETWEEN dt.eff_dt AND dt.exp_dt
--                     THEN 1
--                     ELSE 0
--                END
--FROM (SELECT MIN(eff_dt) AS eff_dt,
--             MIN(exp_dt) AS exp_dt
--      FROM dbo.my_dates
--      WHERE src_id = @p_src_id) AS dt;

SELECT @v_rnk;
</code></pre>
","<sql><azure-synapse>","2022-05-20 21:32:58","171","0","1","72333104","<p>Remove the semi-colon on line 3 after INT</p>
"
"72323830","Data Ingestion in Amazon Redshift","<p>I have multiple data source from which I need to build and implement a DWH in AWS. I have one challenge with respect to one of my unstructured data source (Data coming from different APIs). How can I ingest data from this source into the Amazon Redshift??? Can we first pull it into Amazon S3 bucket and then integrate S3 with Amazon redshift? What is a better approach?</p>
","<amazon-web-services><amazon-s3><amazon-redshift><etl><data-warehouse>","2022-05-20 19:13:01","281","0","2","72340093","<p>Yes, S3 first.  You APIs can write to S3 or/and if you like you can use a service like Kinesis (with or without firehose) to populate S3.  From there it is just work in Redshift.</p>
"
"72323830","Data Ingestion in Amazon Redshift","<p>I have multiple data source from which I need to build and implement a DWH in AWS. I have one challenge with respect to one of my unstructured data source (Data coming from different APIs). How can I ingest data from this source into the Amazon Redshift??? Can we first pull it into Amazon S3 bucket and then integrate S3 with Amazon redshift? What is a better approach?</p>
","<amazon-web-services><amazon-s3><amazon-redshift><etl><data-warehouse>","2022-05-20 19:13:01","281","0","2","72386278","<p>Without knowing more about the sources, yes S3 is likely the right approach - whether you require latency in seconds, minutes or hours will be an important consideration.</p>
<p>If latency is not a driving concern, simply:</p>
<ol>
<li>Set up an S3 bucket to use a destination from your initial source(s).</li>
<li>Create tables in your Redshift database (loading data from S3 to Redshift requires pre-existing destination table).</li>
<li>Use the <a href=""https://docs.aws.amazon.com/redshift/latest/dg/t_loading-tables-from-s3.html"" rel=""nofollow noreferrer"">COPY command</a> load from S3 to Redshift.</li>
</ol>
<p>As noted, there may be value in Kinesis, especially if you're working with real-time data streams (the service <a href=""https://aws.amazon.com/about-aws/whats-new/2022/02/amazon-redshift-public-preview-streaming-ingestion-kinesis-data-streams/"" rel=""nofollow noreferrer"">recently introduced support</a> for skipping S3 and streaming directly to Redshift).</p>
<p>S3 is probably the easier approach, if you're not trying to analyze real-time streams.</p>
"
"72279073","Data Warehousing for Defined schema and its invoices","<p>As part of use case, We generate some invoice documents that are transported in ships and cargo. Each Document contains details of the container and their contents. As part of this Ships and Cargo, We need to store these invoice documents for 15 years and retrieve them back.</p>
<p>Here are the details -</p>
<p>Container Name | Origin Port | Destination Port -&gt; Invoice Name</p>
<p>We need to able to retrieve the Invoice name using container name, origin port or destination port or combination of columns (Similar to SQL).</p>
<p>Each invoice will be at-least 40 to 70 MB.</p>
<p>Any suggestions on building this. We use AWS as cloud. I just need some pointers which can help me get Started.</p>
<p>One approach is to use RedShift + Athena backed by Spark Jobs.</p>
","<database><amazon-web-services><data-warehouse>","2022-05-17 18:33:24","35","0","1","72280000","<p>Invoices of that size likely include images and don't fit well in database but your retrieval needs sound very much like a database.  I've don't work with clients in the past with similar needs and utilized Redshift for the analytics  on the relational data and S3 for storage of large non-relational data (images).  The data table in Redshift can have a json (super type) column that contains pointers to any S3 objects and descriptors for what the S3 object is.    Any number (up to the max size of the json which is large) of S3 objects can be referenced by a single row in the data table.</p>
"
"72257524","How to add a column based on the table which the data come from in SSIS?","<p>I have two source tables:</p>
<ol>
<li>Ext_Agreements</li>
<li>ABS_Agreements</li>
</ol>
<p>both have the same columns : &quot;each table have different data this is just an example&quot;</p>
<pre><code>                          ID, START_DATE,  END_DATE, 
                          01, 28/02/2021, 04/05/2021
                          02, 11/10/2021, 09/01/2022
                          03, 08/01/2022, 03/05/2022
</code></pre>
<p>I want to merge them in one table in the destination Database, while maintaining the information of the type of the agreement by adding a columns &quot;AGREEMENT_TYPE&quot; that contain &quot;Ext&quot; or &quot;ABS&quot; based of the source table of the Agreement.</p>
<p>the destination table will have :</p>
<pre><code>                          ID, START_DATE,  END_DATE, AGREEMENT_TYPE
                          01, 28/02/2021, 04/05/2021,  ABS
                          02, 11/10/2021, 09/01/2022,  EXT
                          03, 08/01/2022, 03/05/2022,  ABS
</code></pre>
<p>I tried merge and Union All and derived columns, but I didn't succeed.
thank you</p>
","<ssis><data-warehouse><sql-server-data-tools><ssis-2012><sql-data-warehouse>","2022-05-16 10:09:54","210","0","2","72263399","<p>How about a UNION ALL in your source.</p>
<pre><code>select ID, START_DATE,  END_DATE, AGREEMENT_TYPE = 'EXT'
from Ext_Agreements
UNION ALL
select ID, START_DATE,  END_DATE, AGREEMENT_TYPE = 'ABS'
from ABS_Agreements
</code></pre>
"
"72257524","How to add a column based on the table which the data come from in SSIS?","<p>I have two source tables:</p>
<ol>
<li>Ext_Agreements</li>
<li>ABS_Agreements</li>
</ol>
<p>both have the same columns : &quot;each table have different data this is just an example&quot;</p>
<pre><code>                          ID, START_DATE,  END_DATE, 
                          01, 28/02/2021, 04/05/2021
                          02, 11/10/2021, 09/01/2022
                          03, 08/01/2022, 03/05/2022
</code></pre>
<p>I want to merge them in one table in the destination Database, while maintaining the information of the type of the agreement by adding a columns &quot;AGREEMENT_TYPE&quot; that contain &quot;Ext&quot; or &quot;ABS&quot; based of the source table of the Agreement.</p>
<p>the destination table will have :</p>
<pre><code>                          ID, START_DATE,  END_DATE, AGREEMENT_TYPE
                          01, 28/02/2021, 04/05/2021,  ABS
                          02, 11/10/2021, 09/01/2022,  EXT
                          03, 08/01/2022, 03/05/2022,  ABS
</code></pre>
<p>I tried merge and Union All and derived columns, but I didn't succeed.
thank you</p>
","<ssis><data-warehouse><sql-server-data-tools><ssis-2012><sql-data-warehouse>","2022-05-16 10:09:54","210","0","2","72275396","<p>If you want to use SSIS, then...</p>
<p>In data flow.</p>
<p>Create a source based on:</p>
<pre><code>select ID, START_DATE,  END_DATE --, AGREEMENT_TYPE = 'EXT'
from Ext_Agreements
</code></pre>
<p>Add a derived column and add:</p>
<pre><code>AgreementType and set (DT_WSTR, 3) &quot;EXT&quot;
</code></pre>
<p>Do the same this for ABS (source and der col).</p>
<p>Then put them together in a UnionAll.</p>
"
"72231489","Data warehouse, could a dimention be a fact?","<p>We have a data warehouse that, between other things, has to store the product sold by the user.
Everytime a user is added to our system, we have an ETL process that persist the user in .. a DM_CUSTOMER table.
Every time a user by a product, we have a row in FT_SALE, with a FK of the DM_CUSTOMER.
The problem here is that in our dashboard we have to show many KPI related to the number of the CUSTOMER in our DW; how many new customer with daily base, montly, hourly ... and we have to fetch this data from the DM_CUSTOMER.</p>
<p>So, ONE CUSTOMER is a business metric ... so it could be a FACT but ... it is a DIMENSION for SALE table so ... what's wrong with this? is it correct to perform query on DM Tables?
Should I create another FT_CLIENT with a FT of DM_CLIENT ?</p>
<p>NOTE: in out system EVERYTIME a user came into the system, for us is a NEW CUSTOMER, also if that user was already in our system.
We do not perform any kind of deduplication.</p>
","<data-warehouse>","2022-05-13 14:48:38","35","0","1","72251166","<p>If you just want to know information that can be calculated from just the Customer Dim table then there is no point building a fact table. However, if you want to slice-and-dice customer information by other dimensions (e.g. date) then it probably makes sense to construct a fact table for customer analytics.</p>
<p>Facts and dimensions are different object types so the fact that you have both a customer fact table and a customer dimension table is not an issue and is, in fact, a common pattern</p>
"
"72220283","Data warehouse design for parking lot - date and time dimensions","<p>I came across a data warehousing practice question. I tried to think of various ways to design this but I'm not sure what the best practices are. The question deals with designing a data warehouse for a parking lot and writing a SQL query for getting the parking fees.</p>
<p>The constraints are as follows:</p>
<p><strong>Weekday hourly rates</strong></p>
<p>Two wheeler - 1$</p>
<p>Four wheeler - 2$</p>
<p><strong>Weekend hourly rates</strong></p>
<p>Two wheeler - 2$</p>
<p>Four wheeler - 3$</p>
<p>A car is parked from Friday morning 9am till Saturday 10am. Design a data warehouse to stored this data and write a SQL to get the parking fees for a vehicle.</p>
<p>I could only think of below two ways of representing it,</p>
<ul>
<li>Approach 1</li>
</ul>
<p>Having a date_id, time_id and a type. Querying the parking fees can be difficult here since we do not have data at the grain of an hour. Difficult to calculate parking fees but consume less data</p>
<p>fact_parking_lot_data</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>fact_key</th>
<th>vehicle_id</th>
<th>date_id</th>
<th>time_id</th>
<th>type</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>20220506</td>
<td>9</td>
<td>in</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>20220507</td>
<td>22</td>
<td>out</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>Approach 2</li>
</ul>
<p>Having a date_id, time_id for each hour of the day. This would created multiple fact table entries for the vehicle, if the vehicle is parked for 2 days then it would have 48 records. Easy to calculate parking fees but consume lot of storage</p>
<p>fact_parking_lot_data</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>fact_key</th>
<th>vehicle_id</th>
<th>date_id</th>
<th>time_id</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>20220506</td>
<td>9</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>20220506</td>
<td>10</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>20220506</td>
<td>11</td>
</tr>
<tr>
<td>4</td>
<td>1</td>
<td>20220506</td>
<td>12</td>
</tr>
<tr>
<td>.</td>
<td>.</td>
<td>.</td>
<td>.</td>
</tr>
<tr>
<td>.</td>
<td>.</td>
<td>.</td>
<td>.</td>
</tr>
<tr>
<td>.</td>
<td>.</td>
<td>.</td>
<td>.</td>
</tr>
<tr>
<td>26</td>
<td>1</td>
<td>20220507</td>
<td>10</td>
</tr>
</tbody>
</table>
</div>
<p>Any thoughts or suggestion would be really appreciated. Thank you !</p>
","<data-modeling><data-warehouse><sql-data-warehouse>","2022-05-12 18:06:32","80","0","1","72226644","<p>Your model is a clear cut example of an accumulating snapshot table: foreign keys to dimensions would be vehicle_id, date_in_id, date_out_id, time_in_id and time_out_id. And as a measure the duration of parking.</p>
<p>When a car comes in date_in_id and time_in_id are populated, but not date_out_id, time_out_id nor duration. When the car leaves then the date_out_id, time_out_id and duration are populated.</p>
<p>That gives you a natural metric to calculate: the total duration across a day, or several days.</p>
<p>The disadvantage of an accumulating snapshot is that it requires lookups on the fact table and updates on &quot;out&quot; events, but I'm guessing your fact table won't be too large (you don't have a location_id in your model so I'm assuming we're talking a few hundred cars per day, maybe up to a couple thousand).</p>
<p>If you're not happy with the accumulating snapshot then my preference between the two models you suggest goes to the 2nd one, where each hour is populated while the car is parked.</p>
<p>Now, a few remarks:</p>
<ul>
<li>your model only takes into account the hours. You may want to have the time keys include minutes and seconds. Even if you're not using it now it gives you room to grow if in the future minute and second resolution are required (of course your second model gets a bit too large in this situation, so the accumulating snapshot becomes the obvious solution)</li>
<li>beware of daylight savings time and any other possible future timezone changes. If cars can be parked overnight and during the weekend you may have a parking event that starts on Saturday at 8pm and ends on Sunday at 8am that in fact lasts either 11 or 13 hours during the DST transitions. Choose a timezone and stick to it.</li>
<li>add a location_id. Sure, maybe now all your parking events happen on a single location, but in the future there may be others. Better to include now a dimension and not use it than having to add it later</li>
<li>add a vehicle class dimension. that dimension should be an SCD Type II with attributes such as the number of wheels and unit price. Upon price changes the new unit price is inserted and versioning ensures history is kept.</li>
</ul>
<p>A disadvantage of the accumulating snapshot vs your 2nd model (with 1 row per time period): there's no easy way to count how many cars are parked at a given time. You'll have to count rows across the entire fact table where</p>
<pre><code>(date_in_id &lt; X or (date_in_id = X and time_in_id &lt;= Y)) 
and 
(date_out_id &gt; X or (date_out_id = X and time_out_id &gt; Y)) 
</code></pre>
<p>Whereas your model allows you to quickly count how many cars are parked at any given moment by just counting all those with</p>
<pre><code>date_id = X and time_id = Y
</code></pre>
"
"72219731","I have a very large xml file (almost 1 gb) I need to split the xml file into 3 smaller files. All with the same headers. I would like to do in Python","<p>I'm opening the file with the code below, but it won't open because it is too big.</p>
<pre><code>from xml.dom import minidom
Test_file = open(&quot;C:\\Users\\samue\\OneDrive\\Desktop\\mopar.xml&quot;,&quot;r&quot;, encoding=&quot;utf8&quot;)
xmldox = minidom.parse(Test_file)

Test_file.close()

def printNode(node):
    print (node)
    for child in node.childNodes:
        printNode(child)
        
printNode(xmldoc.documentElement)
</code></pre>
","<python><xml><data-management>","2022-05-12 17:16:14","138","-1","1","72220025","<p>although I don't see the error messages like the call stack you pasted, I suppose your code maybe failed at the second or the third line.</p>
<p>Have you tried to parse your xml file by xml.etree.cElementTree?</p>
<p>For example, use the codes below and you can know how long ET parses your XML file.</p>
<pre><code>import os
import time
import xml.etree.cElementTree as ET

def read_xml_file(xml_file, element):
    &quot;&quot;&quot;
    Parse the xml file to xml.etree.cElementTree
    &quot;&quot;&quot;
    tree = ET.parse(xml_file)
    root = tree.getroot()
    number_of_element = len(root.findall(element))
    return '{:,.0f}'.format(number_of_element)

start_time = time.perf_counter()
counter = read_xml_file(xml_file_name, 'ProteinEntry/header') # the element here depends on your XML header tag 
end_time = time.perf_counter()
total_time = round(end_time - start_time, 2)
print(f'xml.etree.cElementTree - Total time taken:[{total_time}] seconds to identify the number of elements: [{counter}]')
</code></pre>
"
"72198976","Azure Purview Data Lineage with Databricks","<p>I am using Azure Purview for Data Governance, and Data Lineage. We use Databricks in our Data Architecture, but there isn't any native support for capturing Data Lineage with Databricks.</p>
<p>I found the following links that will allow you to create custom processes in Azure Purview.</p>
<p><a href=""https://stackoverflow.com/questions/69365553/databricks-notebooks-lineage-in-azure-purview"">Databricks notebooks lineage in Azure Purview</a></p>
<p>Can someone let me know if there is any recent methods of achieving Data Lineage in Azure Purview with Databricks?</p>
","<azure-databricks><azure-purview><data-governance>","2022-05-11 09:54:02","1301","0","1","72273266","<p>Data integration and ETL tools can push lineage into Microsoft Purview at execution time. Tools such as Data Factory, Data Share, Synapse, Azure Databricks, and so on, belong to this category of data processing systems. The data processing systems reference datasets as source from different databases and storage solutions to create target datasets. The list of data processing systems currently integrated with Microsoft Purview for lineage are listed in below table.</p>
<p><a href=""https://i.stack.imgur.com/FSaJd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FSaJd.png"" alt=""enter image description here"" /></a></p>
<p>Refer - <a href=""https://learn.microsoft.com/en-us/azure/purview/catalog-lineage-user-guide#data-processing-systems"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/purview/catalog-lineage-user-guide#data-processing-systems</a></p>
<hr />
<p><strong>EDIT:</strong> July 2022 - Since this question was answered, the Microsoft Purview team released an open source solution accelerator to extract lineage from Databricks and ingest it into Microsoft Purview: <a href=""https://github.com/microsoft/Purview-ADB-Lineage-Solution-Accelerator"" rel=""nofollow noreferrer"">A connector to ingest Azure Databricks lineage into Microsoft Purview</a> (github.com)</p>
<p>This solution accelerator, together with the <a href=""https://openlineage.io/"" rel=""nofollow noreferrer"">OpenLineage</a> project, provides a connector that will transfer lineage metadata from Spark operations in Azure Databricks to Microsoft Purview, allowing you to see a table-level lineage graph. It supports Delta, Azure SQL, Data Lake Gen 2, and more.</p>
"
"72184428","How to prevent dbt from rebuilding incremental tables when source view's columns change?","<p>We have the following structure in dbt:</p>
<pre><code>[events (view), with column1, column2, column3]‒‒‒&gt;[incremental1, using column1 from events]
                                             |
                                             |
                                             └‒‒‒‒&gt;[incremental2, using column2 from events]
</code></pre>
<p>In our continuous integration setup, we run <code>dbt run --models state:modified+</code> to avoid running models that were not changed in a PR. However, the problem is that if we change <code>column3</code> in <code>events</code> (e.g. rename it), both incremental tables, <code>incremental1</code> and <code>incremental2</code>, will be fully rebuilt which may take an excessively long time, sometimes more than a whole day. Note that neither incremental tables are actually affected by the change in their source as they do not use the changed column in any way.</p>
<p>Is there a way to avoid rebuilding these models while also not having to run all the other models (which is what a simple <code>dbt run</code> would do)?</p>
<p>Some additional details:</p>
<ul>
<li>our dbt version is 1.0</li>
<li>our data warehouse engine is Snowflake</li>
<li>our incremental strategy is <code>'delete+insert'</code></li>
</ul>
","<continuous-integration><bigdata><snowflake-cloud-data-platform><data-warehouse><dbt>","2022-05-10 10:11:40","1032","0","1","72188912","<p><code>dbt run</code> takes an <code>--exclude</code> option; you could exclude those long-running tables by name, or tag them. This obviously carries its own risks and maintenance burden.</p>
<p>Another option would be to filter the records in these large tables on a dev or CI run so that they build much more quickly. You could add something like this to the incremental model:</p>
<pre><code>{% if target.name != 'prod' %}
where updated_at &gt; current_timestamp() - interval '30 days'
{% endif %}
</code></pre>
"
"72150082","How do we optimise an incremental merge involving a very large target table (10 TB) and smaller incremental source table in a data lake environment?","<p>I came across this question recently in one of the interviews and haven't been able to find a satisfying answer to this question. The incremental merge could contain new records for inserts as well as updates to older records.</p>
<p>I proposed using merge statement but the interviewer said that would involve scanning the entire target table which is not desired. Any better way to merge without scanning the larger table completely?</p>
","<apache-spark><pyspark><merge><etl><data-warehouse>","2022-05-07 06:37:22","145","0","1","72150809","<p>The best way to deal with it is to use Boradcast Join in spark.
you can define property park.sql.autoBroadcastJoinThreshold and set its value to the approx size your smaller incremental source table.</p>
<p>So when your small Data (incremental record dataset) arrives , its copies are created and sent to each node containing large dataset.
In this way network shuffle cost os saved as Large data set does not have to shuffle in Network</p>
<p>example : df_join = Large_dataset.join(broadcast(small_dataset)).</p>
"
"72147046","Pentaho spoon search and replace especial character in rows","<p>I have a csv file with mime type US-ASCII and one column in the dataset look like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>V_name</th>
</tr>
</thead>
<tbody>
<tr>
<td>210001</td>
<td>cha?ne des Puys</td>
</tr>
<tr>
<td>210030</td>
<td>M?los</td>
</tr>
<tr>
<td>213004</td>
<td>G?ll?</td>
</tr>
<tr>
<td>213021</td>
<td>S?phan</td>
</tr>
<tr>
<td>221110</td>
<td>Afd?ra</td>
</tr>
</tbody>
</table>
</div>
<p>And so on.</p>
<p>I would like to change those characters to:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>id</th>
<th>V_name</th>
</tr>
</thead>
<tbody>
<tr>
<td>210001</td>
<td>chaine des Puys</td>
</tr>
<tr>
<td>210030</td>
<td>Milos</td>
</tr>
<tr>
<td>213004</td>
<td>Gollu</td>
</tr>
<tr>
<td>213021</td>
<td>Suphan</td>
</tr>
<tr>
<td>221110</td>
<td>Afdera</td>
</tr>
</tbody>
</table>
</div>
<p>The thing is that there are 95 rows of this kind, how can I search and replace those rows?
I using the suite PDI spoon.
Thanks in advance.</p>
","<data-cleaning><data-warehouse><pentaho-data-integration><stage><spoon>","2022-05-06 20:29:54","149","0","1","72170930","<p>As @Iłya Bursov has stated, the source file you are reading doesn't provide the correct characters, it is providing the ? in the source, so if you want to correct it, you'll have to do it manually.</p>
<p>I don't think it is worth it, unless you know you are going to get always the same set of V_name over time and different files. In that case you could create a file to correlate the <strong>V_name</strong> in your source with the ? characters to a <strong>V_name_corrected</strong> with the correct display for the characters. This seems to be an exercise, so I would let the names as they are. In real life, I would contact the provider of the file with the incorrect character set to tell them that they need to correct the generation of the file to provide the correct characters in the file.</p>
"
"72142600","SCD Start and End Date","<p><a href=""https://i.stack.imgur.com/UdH3p.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UdH3p.png"" alt=""enter image description here"" /></a></p>
<p>I am new to ssis and i am a little bit confused about the SCD_Start_Date and SCD_End_Date when we use the SCD dimension wizard. on many examples that i have searched online people always use the &quot;system start time&quot; or &quot;system container &quot; start time. but i am implementing a data warehouse from a scratch but our company have multiple data sources from two or three years ago. so when i upload our data source from these two years into our dw after running the package it will obviously show that, for example, a seller (seller Dimension) changed from department in the day i ran that package but this is wrong because i want to have a reference date that is in the data source (each row has a data_Ref) . isn't this the logic or am i missing something? how do you guys in BI upload old data source with a reference date?</p>
","<ssis><data-warehouse><business-intelligence><scd>","2022-05-06 13:50:40","353","0","1","72143779","<p>SCD Start and End dates are the dates that a version of the record became (or stopped being) the current version in your source system - they have nothing to do with when data was moved between environments (unless that movement was part of a transaction that changed the state of a record).</p>
<p>Going forward, it is relatively easy to create new SCD records and you insert a new version of a record (and update the previous current version of the record) every time there is a change in the source system.</p>
<p>Loading historic data is entirely dependent on your source system. If it only holds the current version of a record then you can only load that single version into your DWH. If the source system, contains a history of the changes made (or, for example, an audit trail of changes made) then you should be able to re-construct the history in a way that allows you to load multiple versions of a record into your DWH</p>
"
"72135361","Coding the number of days before and after a specific date in R","<p>I am working with a time series data in R. I want to code the number of days before and after a specific date.</p>
<p>Here is an example data frame I'm working on:</p>
<pre><code>df &lt;- data.frame(date = sample(seq(as.Date(&quot;2014-01-01&quot;, &quot;%Y-%m-%d&quot;), by = &quot;day&quot;, length.out = 200), 5000, replace = T))
df$Treatment[df$date &lt;= &quot;2014-04-02&quot;] &lt;- 0
df$Treatment[df$date &gt; &quot;2014-04-02&quot;] &lt;- 1

df &lt;- df[order(df$date),]
</code></pre>
<p>I would like day 0 to be 2014-04-02.</p>
<p>Then, have it count 1,2...n every day after 2014-04-02 and then I would like it count -1, -2... -n, every day before 2014-04-02.</p>
<p>Such that, 2014-04-01 would be coded -1 and 2014-04-03 would be coded as 1, so on and so forth.</p>
<p>How can I do this efficiently in R?</p>
<p>Thank you!</p>
","<r><time-series><data-management>","2022-05-06 01:57:31","153","0","1","72135436","<p>Found the Answer</p>
<p>This should do it.</p>
<pre><code>difftime(df$date, &quot;2014-04-02&quot;, units = &quot;days&quot;)
</code></pre>
<p>You can check the results here.</p>
<pre><code>#Time Since
df$Time_Since &lt;- as.numeric(round(difftime(df$date, &quot;2014-04-02&quot;, units = &quot;days&quot;)))

Check.1 &lt;- unique(with(df, data.frame(date, Time_Since)))
Check.2 &lt;- Check.1[order(Check.1$date),]

</code></pre>
"
"72123639","Is single table with multiple updates better than multiple tables inserted at different times in Redshift","<p>I have a task fact table with following columns</p>
<ul>
<li>Task ID (BIGINT)</li>
<li>User ID (BIGINT)</li>
<li>Created Timestamp (TIMESTAMP)</li>
<li>First assigned Timestamp (TIMESTAMP)</li>
<li>First assigned time taken (BIGINT)</li>
<li>First comment Timestamp (TIMESTAMP)</li>
<li>First comment time taken (BIGINT)</li>
<li>Closed Timestamp (TIMESTAMP)</li>
<li>Closed time taken (BIGINT)</li>
<li>First comment (VARCHAR(3000))</li>
</ul>
<p>As you can see, these timestamps can get updated at any point in time after the task is created. This would mean this column will keep getting updated in redshift.</p>
<p><strong>Q1:</strong> Do I create multiple tables with assignment, comment, close and join them when needed?</p>
<p>Following are the reports that is done</p>
<ul>
<li>Average time taken for different metrics like first-assigned, first comment etc for time range x and y. The time range filters on created/first-assigned/first-comment etc.</li>
<li>Export data as csv between time x and y which has received First comment in the time range</li>
<li>The exported data would need all the data that is present here including the first comment</li>
</ul>
<p><strong>Q2:</strong> Is it okay to store the First comment here as it's a columnar storage. This text field is only used to export as csv.</p>
","<amazon-redshift><data-warehouse><warehouse><parallel-data-warehouse>","2022-05-05 07:44:21","45","0","1","72124831","<p>You wouldn't create multiple tables, you would create a single fact table with an FK to an Event Type Dimension e.g.</p>
<ul>
<li>Task ID (BIGINT)</li>
<li>User ID (BIGINT)</li>
<li>Event ID (BIGINT)</li>
<li>Event Timestamp (TIMESTAMP)</li>
<li>Comment (VARCHAR(3000))</li>
</ul>
<p>But if you did this you obviously couldn't also hold the metrics (e.g. First assigned time taken).</p>
<p>Whether it is better for the fact table to be insert-only (and so be faster) and metrics to be calculated at query time, or whether it is better for the fact table to do updates (and so be slower) but have the metrics pre-calculated and therefore the queries run faster - only you can know, because:</p>
<p>a) Only you can find out how much faster/slower each approach is</p>
<p>b) Only you know what &quot;better&quot; means for your specific circumstances</p>
"
"72117403","Redshift design or configuration issue? - My Redshift datawarehouse seems much slower than my mysql database","<p>I have a Redshift datawarehouse that is pulling data in from multiple sources.
One is my from MySQL and the others are some cloud based databases that get pulled in.</p>
<p>When querying in redshift, the query response is significantly slower than the same mysql table(s).</p>
<p>Here is an example:
SELECT *
FROM leads
WHERE id = 10162064</p>
<p>In mysql this takes .4 seconds.  In Redshift it takes 4.4 seconds.</p>
<p>The table has 11 million rows.  &quot;id&quot; is indexed in mysql and in redshift it is not since it is a columnar system.</p>
<p>I know that Redshift is a columnar data warehouse (which is relatively new to me) and Mysql is a relational database that is able to utilize indexes.  I'm not sure if Redshift is the right tool for us for reporting, or if we need something else.  We have about 200 tables in it from 5 different systems and it is currently at 90 GB.</p>
<p>We have a reporting tool sitting on top that does native queries to pull data.  They are pretty slow but are also pulling a ton of data from multiple tables.  I would expect some slowness with these, but with a simple statement like above, I would expect it to be quicker.</p>
<p>I've tried some different DIST and SORT key configurations but see no real improvement.</p>
<p>I've run vacuum and analyze with no improvement.</p>
<p>We have 4 nodes, dc2.large.  Currently only using 14% storage.  CPU utilization is frequently near 100%.  Database connections averages about 10 at any given time.</p>
<p>The datawarehouse just has exact copies of the tables from our integration with the other sources. We are trying to do near real-time reporting with this.</p>
<p>Just looking for advice on how to improve performance of our redshift via configuration changes, some sort of view or dim table architecture, or any other tips to help me get the most out of redshift.</p>
","<mysql><sql><amazon-redshift><data-warehouse>","2022-05-04 17:50:13","177","1","2","72117957","<p>I've worked with clients on this type of issue many times and I'm happy to help but this may take some back and forth to narrow in on what is happening.</p>
<p>First I'm assuming that &quot;leads&quot; is a normal table, not a view and not an external table.  Please correct if this assumption isn't right.</p>
<p>Next I'm assuming that this table isn't very wide and that &quot;select *&quot; isn't contributing greatly to the speed concern.  Yes?</p>
<p>Next question is wide this size of cluster for a table of only 11M rows?  I'd guess it is that there are other much larger data sets on the database and that this table isn't setting the size.</p>
<p>The first step of narrowing this down is to go onto the AWS console for Redshift and find the query in question.  Look at the actual execution statistics and see where the query is spending its time.  I'd guess it will be in loading (scanning) the table but you never know.</p>
<p>You also should look at STL_WLM_QUERY for the query in question and see how much wait time there was with the running of this query.  Queueing can take time and if you have interactive queries that need faster response times then some WLM configuration may be needed.</p>
<p>It could also be compile time but given the simplicity of the query this seems unlikely.</p>
<p>My suspicion is that the table is spread too thin around the cluster and there are lots of mostly empty blocks being read but this is just based on assumptions.  Is &quot;id&quot; the distkey or sortkey for this table?  Other factors likely in play are cluster load - is the cluster busy when this query runs?  WLM is one place that things can interfere but disk IO bandwidth is a share resource and if some other queries are abusing the disks this will make every query's access to disk slow.  (Same is true of network bandwidth and leader node workload but these don't seem to be central to your issue at the moment.)</p>
<p>As I mentioned resolving this will likely take some back and forth so leave comments if you have additional information.</p>
"
"72117403","Redshift design or configuration issue? - My Redshift datawarehouse seems much slower than my mysql database","<p>I have a Redshift datawarehouse that is pulling data in from multiple sources.
One is my from MySQL and the others are some cloud based databases that get pulled in.</p>
<p>When querying in redshift, the query response is significantly slower than the same mysql table(s).</p>
<p>Here is an example:
SELECT *
FROM leads
WHERE id = 10162064</p>
<p>In mysql this takes .4 seconds.  In Redshift it takes 4.4 seconds.</p>
<p>The table has 11 million rows.  &quot;id&quot; is indexed in mysql and in redshift it is not since it is a columnar system.</p>
<p>I know that Redshift is a columnar data warehouse (which is relatively new to me) and Mysql is a relational database that is able to utilize indexes.  I'm not sure if Redshift is the right tool for us for reporting, or if we need something else.  We have about 200 tables in it from 5 different systems and it is currently at 90 GB.</p>
<p>We have a reporting tool sitting on top that does native queries to pull data.  They are pretty slow but are also pulling a ton of data from multiple tables.  I would expect some slowness with these, but with a simple statement like above, I would expect it to be quicker.</p>
<p>I've tried some different DIST and SORT key configurations but see no real improvement.</p>
<p>I've run vacuum and analyze with no improvement.</p>
<p>We have 4 nodes, dc2.large.  Currently only using 14% storage.  CPU utilization is frequently near 100%.  Database connections averages about 10 at any given time.</p>
<p>The datawarehouse just has exact copies of the tables from our integration with the other sources. We are trying to do near real-time reporting with this.</p>
<p>Just looking for advice on how to improve performance of our redshift via configuration changes, some sort of view or dim table architecture, or any other tips to help me get the most out of redshift.</p>
","<mysql><sql><amazon-redshift><data-warehouse>","2022-05-04 17:50:13","177","1","2","72164200","<p>(I am speaking from a knowledge of MySQL, not Redshift.)</p>
<pre><code>SELECT * FROM leads WHERE id = 10162064
</code></pre>
<p>If <code>id</code> is indexed, especially if it is a Unique (or Primary) key, 0.4 sec sounds like a long network delay.  I would expect 0.004 as a worst-case (with SSDs and `PRIMARY KEY(id)).</p>
<p>(If <code>leads</code> is a <code>VIEW</code>, then let's see the tables.  0.4s may be be reasonable!)</p>
<p>That query works well for a RDBMS, but <em>not</em> for a columnar database.  Face it.</p>
<p>I can understand using a columnar database to handle random queries on various columns.  See also MariaDB's implementation of &quot;Columnstore&quot; -- that would give you both RDBMS and Columnar in a single package.  Still, they are separate enough that you can't really intermix the two technologies.</p>
<p>If you are getting 100% CPU in MySQL, show us the query, its <code>EXPLAIN</code>, and <code>SHOW CREATE TABLE</code>.  Often, a better index and/or query formulation can solve that.</p>
<p>For &quot;real time reporting&quot; in a Data Warehouse, building and maintaining <a href=""http://mysql.rjweb.org/doc.php/summarytables"" rel=""nofollow noreferrer""><em>Summary Tables</em></a> is often the answer.</p>
<p>Tell us more about the &quot;exact copy&quot; of the DW data.  In some situations, the Summary tables can supplant one copy of the Fact table data.</p>
"
"72110716","Spark Compatible Data Quality Framework for Narrow Data","<p>I'm trying to find an appropriate data quality framework for very large amounts of time series data in a <a href=""https://en.wikipedia.org/wiki/Wide_and_narrow_data"" rel=""nofollow noreferrer"">narrow</a> format.</p>
<p>Image billions of rows of data that look kinda like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Sensor</th>
<th>Timestamp</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>12251</td>
<td>12</td>
</tr>
<tr>
<td>B</td>
<td>12262</td>
<td>&quot;A&quot;</td>
</tr>
<tr>
<td>A</td>
<td>12261</td>
<td>13</td>
</tr>
<tr>
<td>A</td>
<td>12271</td>
<td>13</td>
</tr>
<tr>
<td>C</td>
<td>12273</td>
<td>5.4545</td>
</tr>
</tbody>
</table>
</div>
<p>There are hundreds of thousands of sensors, but for each timestamp only a very small percentage send values.</p>
<p>I'm building  Data Quality Monitoring for this data that checks some expectations about the values (e.g. whether the value falls within the expected range for a given sensor, there are tens of thousands of different expectations). Due to the size of the data and existing infrastructure the solution has to be run on Spark.  I would like to build this solution on an (ideally open source) data quality framework, but cannot find anything appropriate.</p>
<p>I've looked into Great Expectations and Deequ, but these fundamentally seem to be build for &quot;wide data&quot; where the expectations are defined for columns. I could theoretically reshape (pivot) my data to this format, but it would be a very expensive operation and result in an extremly sparse table that is awkward to work with (or require sampling on the time and in this way a loss of information).</p>
<p>Does anyone know of an existing (spark compatible) framework for such time series data in narrow format? Or can point me to best practices how to apply Deequ/Great Expectations in such a setting?</p>
","<apache-spark><data-quality><great-expectations><amazon-deequ>","2022-05-04 09:29:37","177","0","1","74200703","<p>Have you tried <code>github.com/canimus/cuallee</code>
It is an open-source framework, that supports the Observation API to make testing on billions of records, super-fast, and less resource greedy as pydeequ. Is intuitive, and easy to use.</p>
"
"72085354","Using DimCalendar update days between two dates","<p>In DWH (SQL Server) i have two tables:</p>
<p>DWH.Days</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>DayStart</th>
<th>DayStop</th>
<th>DaysBetween</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022-04-21</td>
<td>2022-04-24</td>
<td>null</td>
</tr>
<tr>
<td>2022-03-12</td>
<td>2022-04-27</td>
<td>null</td>
</tr>
<tr>
<td>2022-04-21</td>
<td>2022-04-24</td>
<td>null</td>
</tr>
<tr>
<td>2022-03-01</td>
<td>2022-04-22</td>
<td>null</td>
</tr>
</tbody>
</table>
</div>
<p>and
DWH.Calendar</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Date</th>
<th>IsHoliday?</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022-05-11</td>
<td>yes</td>
</tr>
<tr>
<td>2022-05-12</td>
<td>no</td>
</tr>
<tr>
<td>2022-05-13</td>
<td>yes</td>
</tr>
<tr>
<td>2022-05-15</td>
<td>no</td>
</tr>
</tbody>
</table>
</div>
<p>I need to update DWH.Days.DaysBetween as a number of days between DayStart and DayStop where DWH.DimCalendar.IsHoliday?='no'. I don't have premission change the data model. I don't have any ideas how to do it, any ideas?</p>
","<sql><sql-server><tsql><data-warehouse>","2022-05-02 10:21:05","57","1","1","72086759","<p>your data</p>
<p>however there exist numerous issues regarding your sample data</p>
<pre><code>declare @Days table (
  DayStart DATE NOT NULL, 
  DayStop DATE NOT NULL, 
  DaysBetween VARCHAR(40)
);
INSERT INTO @Days (DayStart, DayStop, DaysBetween) 
VALUES 
  ('2022-04-21', '2022-04-24', NULL), 
  ('2022-03-12', '2022-04-27', NULL), 
  ('2022-04-21', '2022-04-24', NULL), 
  --repeated data it should be deleted
  ('2022-03-01', '2022-04-22', NULL);
declare @Calendar table(
  Date DATE NOT NULL, 
  IsHoliday VARCHAR(30) NOT NULL
);
INSERT INTO @Calendar (Date, IsHoliday) 
VALUES 
  ('2022-05-11', 'yes'), 
  --actual values
  ('2022-05-12', 'no'), 
  --actual values
  ('2022-05-13', 'yes'), 
  --actual values
  ('2022-05-15', 'no'), 
  --actual values
  ('2022-04-23', 'yes'), 
  --added values for test. it can be deleted.
  ('2022-03-10', 'no'), 
  --added values for test. it can be deleted.
  ('2022-03-14', 'yes'), 
  --added values for test. it can be deleted.
  ('2022-04-22', 'no'), 
  --added values for test. it can be deleted.
  ('2022-03-15', 'no');
</code></pre>
<p>first you should use <code>cross join</code> for satisfy the Condition <em>DayStart&lt;Date&lt;DayStop and IsHoliday='no'</em>. then use <code>subquery</code> and <code>DATEDIFF</code> and <code>group by</code> as follows</p>
<pre><code>SELECT Daystart,
       Daystop,
       Datediff(d, Daystart, Daystop) - Count(Isholiday) AS DaysBetween
FROM   (SELECT Daystart,
               Daystop,
               Daysbetween,
               Isholiday
        FROM   @days d,
               @Calendar c
        WHERE  d.Daystart &lt; c.Date
               AND d.Daystop &gt; c.Date
               AND c.Isholiday = 'no') a
GROUP  BY Daystart,
          Daystop  
</code></pre>
"
"72046309","DWH modelization - Relationship between 2 fact tables","<p>Hy,</p>
<p>I realize a DWH and i have a probleme with the relationship between my tables.</p>
<p>My context :</p>
<ul>
<li>&quot;job offer&quot; : contains the list of all job offer. A job offer is created by a company, have a validation date and can be publish to a jobboard (like a likedIn)</li>
<li>&quot;candidaties&quot; :  a candidate applies to a job offer published to a joboard</li>
</ul>
<p>&quot;Job offer&quot; is ambigous table, it be considered as a fact table (contain externals Id, have a Date property). can also consideres as dim table, have a many properties and id use another fact. job offer have a lot of row (several million)</p>
<p>I identified 3 facts : JobOffer, PublicationOffer, Candidaty. The fact table is a transactions and have only count measure (considered as factless)</p>
<p>I have difficulties to create relathionship because i have relationship between fact table (it's forbiden)</p>
<p>I think create two tables job offers, one dim table, the other fact table. Or considered job offer as a dim table. What the good choice?</p>
<p>Models representation:
<a href=""https://i.stack.imgur.com/zVsAH.png"" rel=""nofollow noreferrer"">model 1</a>
or
<a href=""https://i.stack.imgur.com/Je5Sq.png"" rel=""nofollow noreferrer"">model 2</a></p>
<p>My envisaged solution
<a href=""https://i.stack.imgur.com/6D8Fs.png"" rel=""nofollow noreferrer"">solution 1</a>
or
<a href=""https://i.stack.imgur.com/NAe4e.png"" rel=""nofollow noreferrer"">solution 2</a></p>
<p>THX</p>
","<relationship><data-warehouse><fact>","2022-04-28 15:18:27","32","0","1","72048683","<p>designing a DWH should follow these broad steps</p>
<ol>
<li>Identify your reporting requirements</li>
<li>Identify the measures that will deliver these reporting requirements</li>
<li>Identify the dimensions that will allow you to aggregate and filter your measures in accordance with your requirements</li>
<li>Define the grain of your fact table(s) which then drives which of your measures can be in the same fact table and which need their own fact table</li>
</ol>
<p>If you follow this approach you should be able to answer your own question</p>
"
"72037815","ClickHouse TTL settings with WHERE condition","<p>I have a table with the following DDL:</p>
<pre><code>CREATE TABLE default.pricing_rate_latest_state_union_test
(
    `organization_id`   Int32,
    `organization_name` String,
    `scraping_time`     AggregateFunction(max, DateTime),
    `amount_from_raw`   Float64,
    `amount_to_raw`     Float64,
    `key_param_id`      LowCardinality(String),
    `amount_from`       AggregateFunction(argMax, Float64, DateTime),
    `amount_to`         AggregateFunction(argMax, Float64, DateTime),
    `source`            LowCardinality(String)
)
ENGINE = ReplicatedAggregatingMergeTree('/clickhouse/tables/{shard}/*****/pricing_rate_latest_state_union_test',
 '{replica}')
ORDER BY (key_param_id,
 amount_from_raw,
 amount_to_raw)
SETTINGS index_granularity = 8192;
</code></pre>
<p>For which I would like to add a TTL expression on scraping_time column, so that after certain number of days the corresponding records are dropped. This I can do easily with the following:</p>
<pre><code>ALTER TABLE default.pricing_rate_latest_state_union_test 
MODIFY TTL finalizeAggregation(scraping_time) + INTERVAL 30 DAY;
</code></pre>
<p>This is works fine, but my end goal is to add condition describing which rows need to be deleted after expiration. What I'm trying is this:</p>
<pre><code>ALTER TABLE default.pricing_rate_latest_state_union_test 
MODIFY TTL finalizeAggregation(scraping_time) + INTERVAL 30 DAY 
DELETE WHERE source = 'some_value';
</code></pre>
<p>Unfortunately this lead to the following error:</p>
<pre><code> DB::Exception: Exception happened during execution of mutation '0000000000' with part 'all_0_0_0' reason: 'Code: 10, e.displayText() = DB::Exception: Not found column scraping_time in block. There are only columns: organization_id (version 21.8.13.6 (official build))'. This error maybe retryable or not. In case of unretryable error, mutation can be killed with KILL MUTATION query 
</code></pre>
<p>According to this <a href=""https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#use-of-index-for-partially-monotonic-primary-keys"" rel=""nofollow noreferrer"">documentation</a> it should be possible as there is the following example:</p>
<pre><code>CREATE TABLE table_with_where
(
    d DateTime,
    a Int
)
ENGINE = MergeTree
PARTITION BY toYYYYMM(d)
ORDER BY d
TTL d + INTERVAL 1 MONTH DELETE WHERE toDayOfWeek(d) = 1;
</code></pre>
<p>I believe my case doesn't work that smoothly due to the ReplicatedAggregatingMergeTree, so any help would be greatly appreciated here.</p>
<p>I'm also interested in the meaning of <em>Not found column scraping_time in block.</em> I know what part is in ClickHouse, but what is the block and why it doesn't see the <em>scraping_time</em>, but see the <em>organization_id</em>?</p>
","<data-warehouse><clickhouse><ttl>","2022-04-28 03:48:34","834","1","2","72062793","<p>It looks like a bug or unexpected behavior because TTL is not designed to work with AggregateFunction.</p>
<p>You can construct TTL without WHERE</p>
<pre><code>ALTER TABLE default.pricing_rate_latest_state_union_test  
  MODIFY TTL if (source = 'some_value', finalizeAggregation(scraping_time) + INTERVAL 30 DAY, toDateTime(0));
</code></pre>
"
"72037815","ClickHouse TTL settings with WHERE condition","<p>I have a table with the following DDL:</p>
<pre><code>CREATE TABLE default.pricing_rate_latest_state_union_test
(
    `organization_id`   Int32,
    `organization_name` String,
    `scraping_time`     AggregateFunction(max, DateTime),
    `amount_from_raw`   Float64,
    `amount_to_raw`     Float64,
    `key_param_id`      LowCardinality(String),
    `amount_from`       AggregateFunction(argMax, Float64, DateTime),
    `amount_to`         AggregateFunction(argMax, Float64, DateTime),
    `source`            LowCardinality(String)
)
ENGINE = ReplicatedAggregatingMergeTree('/clickhouse/tables/{shard}/*****/pricing_rate_latest_state_union_test',
 '{replica}')
ORDER BY (key_param_id,
 amount_from_raw,
 amount_to_raw)
SETTINGS index_granularity = 8192;
</code></pre>
<p>For which I would like to add a TTL expression on scraping_time column, so that after certain number of days the corresponding records are dropped. This I can do easily with the following:</p>
<pre><code>ALTER TABLE default.pricing_rate_latest_state_union_test 
MODIFY TTL finalizeAggregation(scraping_time) + INTERVAL 30 DAY;
</code></pre>
<p>This is works fine, but my end goal is to add condition describing which rows need to be deleted after expiration. What I'm trying is this:</p>
<pre><code>ALTER TABLE default.pricing_rate_latest_state_union_test 
MODIFY TTL finalizeAggregation(scraping_time) + INTERVAL 30 DAY 
DELETE WHERE source = 'some_value';
</code></pre>
<p>Unfortunately this lead to the following error:</p>
<pre><code> DB::Exception: Exception happened during execution of mutation '0000000000' with part 'all_0_0_0' reason: 'Code: 10, e.displayText() = DB::Exception: Not found column scraping_time in block. There are only columns: organization_id (version 21.8.13.6 (official build))'. This error maybe retryable or not. In case of unretryable error, mutation can be killed with KILL MUTATION query 
</code></pre>
<p>According to this <a href=""https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#use-of-index-for-partially-monotonic-primary-keys"" rel=""nofollow noreferrer"">documentation</a> it should be possible as there is the following example:</p>
<pre><code>CREATE TABLE table_with_where
(
    d DateTime,
    a Int
)
ENGINE = MergeTree
PARTITION BY toYYYYMM(d)
ORDER BY d
TTL d + INTERVAL 1 MONTH DELETE WHERE toDayOfWeek(d) = 1;
</code></pre>
<p>I believe my case doesn't work that smoothly due to the ReplicatedAggregatingMergeTree, so any help would be greatly appreciated here.</p>
<p>I'm also interested in the meaning of <em>Not found column scraping_time in block.</em> I know what part is in ClickHouse, but what is the block and why it doesn't see the <em>scraping_time</em>, but see the <em>organization_id</em>?</p>
","<data-warehouse><clickhouse><ttl>","2022-04-28 03:48:34","834","1","2","73603488","<p>While investigating this case found that problem is not in AggregateFunction. Problem is that Clickhouse's TTL doesn't support WHERE clause in MODIFY TTL statement when table is not empty.</p>
<pre><code>CREATE TABLE test
(
    `scraping_time` AggregateFunction(max,
 DateTime),
    `source` String
)
ENGINE = ReplicatedAggregatingMergeTree('/clickhouse/tables/{shard}/test',
 '{replica}')
ORDER BY (source)
TTL finalizeAggregation(scraping_time) + toIntervalDay(if(source='condition1', 30, 40)) where source &lt;&gt; 'condition2'
SETTINGS index_granularity = 8192;
</code></pre>
<p>This works. After fill that table. Then if you will try to remove TTL and add again through MODIFY it will raise error described above.</p>
<pre><code>ALTER TABLE test REMOVE TTL;

ALTER TABLE test MODIFY TTL scraping_time + toIntervalDay(if(source='scraper', 30, 40)) where source &lt;&gt; 'stable';
</code></pre>
<blockquote>
<p>SQL Error [341]: ClickHouse exception, code: 341, host:
*****************, port: ****; Code: 341,
e.displayText() = DB::Exception: Exception happened during execution
of mutation '0000000000' with part 'all_0_0_0' reason: 'Code: 10,
e.displayText() = DB::Exception: Not found column scraping_time in
block. There are only columns: organization_id (version 21.8.14.5
(official build))'. This error maybe retryable or not. In case of
unretryable error, mutation can be killed with KILL MUTATION query
(version 21.8.14.5 (official build))</p>
</blockquote>
<p>I've tried to remove WHERE and it works.</p>
<pre><code>ALTER TABLE test MODIFY TTL finalizeAggregation(scraping_time) + toIntervalDay(if(source='condition1', 30, 40));
</code></pre>
<p>So here are three <strong>SOLUTIONS</strong>:</p>
<p><strong>1. As described by Denny Crane to don't use WHERE when you are MODIFY'ing TTL, you can replace it with multiIf or multiple ifs</strong></p>
<p><strong>2. Copy data to temp table, truncate table in which you want to add TTL, add that TTL ( must work with WHERE statement), copy data back to main table</strong>.</p>
<p><strong>3. Not recommended. Drop table and then recreate with your TTL in CREATE statement</strong></p>
"
"72034274","Distinct aggregation in pre-calculated measure (MDX)","<p>There are two measures in one fact table \ dimension. Measure '<strong>YearTotal</strong>' should somehow be pre-calcuated as a distinct value for any futher summing (aggregating). And 'YearTotal' can't be derived from 'YearDetail' measure, so they are completely independent.</p>
<pre><code> +-------------+---------------+-----------+------------+
| AccountingID |      Date     | TotalYear | YearDetail |
+--------------+---------------+-----------+------------+
|   account1   |    31.12.2012 |       500 |          7 |
|   account1   |    31.12.2012 |       500 |          3 |
|   account1   |    31.12.2012 |       500 |          1 |
|   account2   |    31.12.2012 |       900 |         53 |
|   account2   |    31.12.2012 |       900 |          4 |
|   account2   |    31.12.2012 |       900 |          9 |
|   account3   |    31.12.2012 |       203 |         25 |
|   account3   |    31.12.2012 |       203 |         11 | 
|   account3   |    31.12.2012 |       203 |         17 |
+--------------+---------------+-----------+------------+
</code></pre>
<p><strong>So, the question:</strong> What should be in (pre)calculated measure expression to get such a result:</p>
<pre><code>select 

    (
    [Accounting Dim].[Account ID]
    [Dim Date].[Calendar Year].&amp;[2012]
    ) ON COLUMNS
    
    from [Cube]
    
    WHERE [Measures].[YearTotal]
</code></pre>
<p><strong>in case of correct expression the answer would be</strong> --&gt; (500+900+203) = <strong>1603</strong></p>
<p>(and optionaly): maybe there is a common distinct pattern solution for any other simple types of aggregation</p>
","<visual-studio><ssas><mdx><data-warehouse>","2022-04-27 19:26:10","44","0","1","72138807","<p>Maybe go with MAX at the column level [TotalYear] and enforce a specific level of calculation.</p>
<pre><code>CREATE MEMBER [Measures].[YearTotal] AS 
    MAX(
        (
            [Calendar Dim].[Year].[All].Children, 
            [Accounting Dim].[Account ID].[All].Children
        ),
        [Measures].[TotalYear]
    )
</code></pre>
"
"71973924","Azure Synapse, design questions of External tables or Internal tables","<p>I'm designing a Dataware house in Azure Synapse using SQL Pool, but I'm facing some design questions.</p>
<p>Context: My plan is to load Partitioned Parquet files using Azure Data Lake Storage (ADLS), then, with SQL pool create External Tables to query those files.</p>
<p>My questions are:</p>
<ul>
<li>Is it better in terms of performance to provide the solution just with the external tables? that is, with no create internal tables neither CTAS, BCP, or copy methods from the ADLS to storage in the database.</li>
<li>Is it possible to perform partitioning in external tables? is it enough to organize the parquet by folders named by date?</li>
<li>How does affect the user concurrency to the external tables and the internal tables? some experienced recommendations?.</li>
</ul>
<p>Thanks for your time.
Josh</p>
","<database><parquet><data-warehouse><azure-synapse>","2022-04-22 19:49:23","911","0","1","71974907","<blockquote>
<p>Is it better in terms of performance to provide the solution just with the external tables?</p>
</blockquote>
<p>No.  Internal Tables are distributed columnstores, with multiple levels of caching, and typically out-perform external parquet tables.  Internal tables additionally support batch-mode scanning, columnstore ordering, segment elimination, partition elimination, materialized views, and resultset caching.</p>
<blockquote>
<p>Is it possible to perform partitioning in external tables?</p>
</blockquote>
<p>This is not currently possible in Dedicated SQL Pools, see <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#folder-partition-elimination"" rel=""nofollow noreferrer"">Folder Partition Elimination</a></p>
<blockquote>
<p>How does affect the user concurrency to the external tables and the internal tables?</p>
</blockquote>
<p>Concurrency is a matter of query performance.  The faster your queries perform, the faster sessions give up their concurrency slot.  So anything that improves query performance improves the effective concurrency (the number of concurrent users you can support with reasonable query runtime).</p>
<p>Serverless SQL Pools currently have more advanced capabilities for working with data stored as Parquet or Delta in the Data Lake.</p>
"
"71962062","How to get sum of multiple rows in a table dynamically","<p>I am trying to get the total sum from columns of a specific data type(money) for multiple tables in a database. Currently I am able to get the list of columns from specific tables but I am unable to get the sums from those columns.</p>
<p>This is what I have now</p>
<pre><code>    use database 1 
Select + Column_Name
    From information_schema.columns
    Where TABLE_NAME = 'claimant'
    and data_type = 'money'
</code></pre>
<p>The result looks something like below</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">table_name</th>
<th>column_name</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">table_1</td>
<td>column_a</td>
</tr>
<tr>
<td style=""text-align: left;"">table_1</td>
<td>column_b</td>
</tr>
<tr>
<td style=""text-align: left;"">table_1</td>
<td>column_c</td>
</tr>
</tbody>
</table>
</div>
<p>what I would like</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">table_name</th>
<th>column_name</th>
<th>total_sum</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">table_1</td>
<td>column_a</td>
<td>66.20</td>
</tr>
<tr>
<td style=""text-align: left;"">table_1</td>
<td>column_b</td>
<td>300.50</td>
</tr>
<tr>
<td style=""text-align: left;"">table_1</td>
<td>column_c</td>
<td>5389.42</td>
</tr>
</tbody>
</table>
</div>
<p>update for @Squirrel Here is the code I have but it's still giving me issues with truncation.</p>
<pre><code>{

declare @sql nvarchar(max);

select  @sql = 'with cte as (' + char(13)
            + 'select' + char(13)
            + string_agg(char(9) + quotename(column_name) + ' = sum(' + quotename(COLUMN_NAME) + ')', ',' + char(13)) + char(13)
            + 'from ' + max(quotename(table_name)) + char(13)
            + ')' + char(13)
            + 'select a.table_name, a.column_name, a.total_sum ' + char(13)
            + 'from   cte ' + char(13)
            + 'cross apply (' + char(13)
            + char(9) + 'values' + char(13)
            + string_agg(char(9) + '(''' + table_name + ''',''' + column_name  + ''',' + quotename(COLUMN_NAME) + ')', ',' + char(13)) + char(13)
            + ') a (table_name, column_name, total_sum)'
from   information_schema.columns AS A
INNER JOIN EDL01.STAGING.TABLE_DETAILS B
ON A.TABLE_NAME = B.DEST_TABLE_NAME
where  A.table_name = B.DEST_TABLE_NAME
and    data_type  = 'money'



print @sql
exec sp_executesql @sql 
}
</code></pre>
<p>below is the create table</p>
<pre><code>CREATE TABLE [staging].[TABLE_DETAILS](
    [SOURCE_TABLE_NAME] [varchar](100) NULL,
    [DEST_TABLE_NAME] [varchar](100) NULL,
    [TYPE] [varchar](10) NULL,
    [PRIORITY] [int] NULL,
    [SOURCE_TABLE_DATABASE] [varchar](50) NULL,
    [SOURCE_TABLE_SCHEMA] [varchar](50) NULL,
    [DEST_TABLE_DATABASE] [varchar](50) NULL,
    [DEST_TABLE_SCHEMA] [varchar](50) NULL
) ON [PRIMARY]
GO
</code></pre>
<p>Below is part of the results</p>
<pre><code>select a.table_name, a.column_name, a.total_sum 
from   cte 
cross apply (
    values
('PAYMENT','BILLEDAMOUNT',[BILLEDAMOUNT]),
    ('PAYMENT','AMOUNT',[AMOUNT]),
    ('SIMS_PAYMENT','CHECKAMOUNT',[CHECKAMOUNT]),
    ('BILLREVIEWHEADER','JURISDICTIONAMOUNT1',[JURISDICTIONAMOUNT1]),
    ('BILLREVIEWHEADER','JURISDICTIONAMOUNT2',[JURISDICTIONAMOUNT2]),
    ('BILLREVIE
</code></pre>
","<sql><database><tsql><data-quality>","2022-04-22 00:04:16","276","0","1","71962417","<p>You need to form the query dynamically and then execute it using <code>sp_executesql</code> or <code>exec()</code></p>
<p>Note : <code>char(9)</code> is tab, <code>char(13)</code> is carriage return. These are added to format the query so that it is readable when you <code>print</code> it out for verification.</p>
<pre><code>declare @sql nvarchar(max);

select @sql = 'with cte as (' + char(13)
            + 'select' + char(13)
            + string_agg(char(9) + quotename(column_name) + ' = sum(' + quotename(column_name) + ')', ',' + char(13)) + char(13)
            + 'from ' + max(quotename(table_name)) + char(13)
            + ')' + char(13)
            + 'select a.table_name, a.column_name, a.total_sum ' + char(13)
            + 'from   cte ' + char(13)
            + 'cross apply (' + char(13)
            + char(9) + 'values' + char(13)
            + string_agg(char(9) + '(''' + table_name + ''', ''' + column_name + ''',' + quotename(column_name) + ')', ',' + char(13)) + char(13)
            + ') a (table_name, column_name, total_sum)'
from   information_schema.columns
where  table_name = 'table_1'
and    data_type  = 'money'

print @sql
exec sp_executesql @sql
</code></pre>
<p>For your sample table, the generated dynamic query is</p>
<pre><code>with cte as (
select
    [column_a] = sum([column_a]),
    [column_b] = sum([column_b]),
    [column_c] = sum([column_c])
from [table_1]
)
select a.table_name, a.column_name, a.total_sum 
from   cte 
cross apply (
    values
    ('table_1', 'column_a',[column_a]),
    ('table_1', 'column_b',[column_b]),
    ('table_1', 'column_c',[column_c])
) a (table_name, column_name, total_sum)
</code></pre>
<p>EDIT
using a loop to iterate each table. Basically it execute above query for each of the table and insert the result into a temp table</p>
<p>see <a href=""https://dbfiddle.uk/?rdbms=sqlserver_2019&amp;fiddle=0e10f3ac35c1b4b5c3258a4fd4ba9d1c"" rel=""nofollow noreferrer"">db&lt;&gt;fiddle demo</a></p>
<p>for earlier SQL Server version without string_agg(), use <code>for xml path</code></p>
<pre><code>select @sql  = 'with cte as (' + char(13)
             + 'select' + char(13)
             + stuff
               (
                  (
                      select ',' + quotename(COLUMN_NAME) + ' = sum(' + quotename(COLUMN_NAME) + ')'
                      from   INFORMATION_SCHEMA.COLUMNS
                      where  TABLE_NAME = @table
                      and    DATA_TYPE  = 'money'
                      for xml path('')
                  ), 
                  1, 1, ''
               ) + char(13)
            + 'from ' + max(quotename(@table)) + char(13)
            + ')' + char(13)
            + 'select a.table_name, a.column_name, a.total_sum ' + char(13)
            + 'from   cte ' + char(13)
            + 'cross apply (' + char(13)
            + char(9) + 'values' + char(13)
            + stuff
            (
              (
                  select    ',' + '(''' + TABLE_NAME + ''', ''' + COLUMN_NAME + ''',' + quotename(COLUMN_NAME) + ')'
                  from   INFORMATION_SCHEMA.COLUMNS
                  where  TABLE_NAME = @table
                  and    DATA_TYPE  = 'money'
                  for xml path('')
              ),
            1, 1, ''
            )
            + ') a (table_name, column_name, total_sum)' + char(13)
</code></pre>
"
"71922576","Star Schema - Unify data with varying structure from different sources","<p>I am currently designing a star schema for a reporting database where an online product's performance is measured. The challenge is, that I receive information which is in principle measuring the same facts (visits, purchases) and has the same dimensions (user gender, user age, day) but with varying granularity depending on the source, for example, given a total of 10 visits:</p>
<ul>
<li>Source A returns a single line per day for the performance in the format:
<ol>
<li>Visits, Purchases, Gender, Age Range, Day (total visits = 15)</li>
</ol>
</li>
<li>Source B returns two lines for a single day as it does not allow the combination of gender and age:
<ol>
<li>Visits, Purchases, Gender, Day  (total visits = 10)</li>
<li>Visits, Purchases, Age Range, Day  (total visits = 10)</li>
</ol>
</li>
</ul>
<p>The issues is, if I store them in the same fact table, I will have incorrect values when applying aggregate functions:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Day</th>
<th>Visits</th>
<th>Age</th>
<th>Gender</th>
<th>Source</th>
</tr>
</thead>
<tbody>
<tr>
<td>19/04/2022</td>
<td>5</td>
<td>18-24</td>
<td>Male</td>
<td>A</td>
</tr>
<tr>
<td>19/04/2022</td>
<td>10</td>
<td>18-24</td>
<td>Female</td>
<td>A</td>
</tr>
<tr>
<td>19/04/2022</td>
<td>2</td>
<td>NULL</td>
<td>Male</td>
<td>B</td>
</tr>
<tr>
<td>19/04/2022</td>
<td>8</td>
<td>NULL</td>
<td>Female</td>
<td>B</td>
</tr>
<tr>
<td>19/04/2022</td>
<td>10</td>
<td>18-24</td>
<td>NULL</td>
<td>B</td>
</tr>
</tbody>
</table>
</div>
<p><em>(The sum of the visits column would count 20 for source B even though we only have 10 visits for this source, they just appear double due to the different data structure)</em></p>
<p>Is there a best practice for cases where dimensions and facts are generally the same, but the raw data granularity is different?</p>
","<database><data-warehouse><dimensional-modeling><star-schema>","2022-04-19 09:24:13","113","0","1","71932343","<blockquote>
<p>Is there a best practice for cases where dimensions and facts are generally the same, but the raw data granularity is different?</p>
</blockquote>
<p>You typically can only present the combined data at a grain that's compatible with all the sources, so (Day), (Age,Day), or (Gender,Day).</p>
<p>Alternatively you could &quot;allocate&quot; the Source B data, say applying the gender split for the day to each age group.  The totals would work, but the drilldown wouldn't be meaningful.</p>
"
"71880537","Find All the tables related to a stored procedure in Azure Synapse Datawarehouse","<p>Is there a simple way to find out all the tables referenced in a stored procedure in azure analytics data warehouse other than parsing the stored procedure code? I tried few commands like <strong>sp_tables</strong>, <strong>sp_depends</strong> but none seems to be working in azure data warehouse.</p>
","<azure><tsql><data-warehouse><azure-synapse>","2022-04-15 05:49:54","961","2","2","71884229","<p><a href=""https://learn.microsoft.com/en-us/sql/relational-databases/system-catalog-views/sys-sql-expression-dependencies-transact-sql?view=sql-server-ver15"" rel=""nofollow noreferrer"">sys.sql_expression_dependencies</a> is supported in Azure Synapse Analytics, dedicated SQL pools, but only supports tables, views and functions at this time.  A simple example:</p>
<p><code>SELECT * FROM sys.sql_expression_dependencies;</code></p>
<p>So you are left either parsing <code>sys.sql_modules</code>.  Something like this is imperfect (ie doesn't deal with schema name, square brackets, partial matches etc) but could server as a starting point:</p>
<pre><code>SELECT 
    sm.[definition],
    OBJECT_SCHEMA_NAME(t.object_id) schemaName,
    OBJECT_NAME(t.object_id) tableName
FROM sys.sql_modules sm
    CROSS JOIN sys.tables t
WHERE sm.object_id = OBJECT_ID('dbo.usp_test')
  AND sm.[definition] Like '%' + t.name + '%';
</code></pre>
<p>I actually use SQL Server Data Tools (SSDT) with dedicated SQL pools so your dependencies can't get out of step and are trackable via the project.</p>
"
"71880537","Find All the tables related to a stored procedure in Azure Synapse Datawarehouse","<p>Is there a simple way to find out all the tables referenced in a stored procedure in azure analytics data warehouse other than parsing the stored procedure code? I tried few commands like <strong>sp_tables</strong>, <strong>sp_depends</strong> but none seems to be working in azure data warehouse.</p>
","<azure><tsql><data-warehouse><azure-synapse>","2022-04-15 05:49:54","961","2","2","75614938","<pre><code>SELECT OBJECT_NAME(OBJECT_ID),definition 
FROM sys.all_sql_modules 
WHERE definition LIKE '%your_table_name%' 
</code></pre>
"
"71878814","how to combine values of columns in the tidyverse?","<p>I have two variables, 'landcover' (categorical: 1-5) and 'field' (categorical: 1-2). Where there's a value for 'field', there should be an <code>na</code> in the 'landcover' column and vice versa. How would I go about combining these into a single variable in the <code>tidyverse</code>?</p>
<p>Example code looks something like this:</p>
<pre><code>dat &lt;- data.frame(landcover = c(1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, NA, 4, 5, 1, NA, 3, NA, 5),
fieldsampled = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA, NA, 1, NA, 2, NA))
</code></pre>
<p>I expect to start out, I'd need to change 'field' values to something different like 6 and 7:</p>
<pre><code>dat %&gt;% mutate(field = recode(field, 1 = 6, 2 = 7))
</code></pre>
<p>But unaware of a handy <code>dplyr</code> function to then combine the two columns into one. Suggestions are appreciated!</p>
","<r><dplyr><tidyverse><data-management>","2022-04-15 00:13:45","438","1","1","71878877","<p>The dplyr::coalesce function (like the SQL equivalent) is useful here for combining two vectors to get the first non-NA. We can also add 5 to the fieldsampled one to create a number that doesn't overlap with the landcover values.</p>
<pre><code>dat %&gt;% 
  mutate(field = coalesce(landcover, fieldsampled + 5))
</code></pre>
<p>Result</p>
<pre><code>   landcover fieldsampled field
1          1           NA     1
2          2           NA     2
3          3           NA     3
4          4           NA     4
5          5           NA     5
6          1           NA     1
7          2           NA     2
8          3           NA     3
9          4           NA     4
10         5           NA     5
11         1           NA     1
12         2           NA     2
13        NA            2     7
14         4           NA     4
15         5           NA     5
16         1           NA     1
17        NA            1     6
18         3           NA     3
19        NA            2     7
20         5           NA     5
</code></pre>
"
"71813893","How to fit a separate table in data warehouse model?","<p>I have a table named AWARENESS_MEETINGS with name and date of the meeting as an attributes, it's not related to any other table in the database.
I am designing a data warehouse model, I can't fit it into the model its only related to the date dimension, and I can't determine if it's a fact or dimension table, the only business question related to the table is the count of the AWARENESS_MEETINGS by time.</p>
<p>EDITE:
<a href=""https://i.stack.imgur.com/PCd5p.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PCd5p.png"" alt=""enter image description here"" /></a></p>
<p>EDITE 2:</p>
<p><a href=""https://i.stack.imgur.com/mKnIa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mKnIa.png"" alt=""enter image description here"" /></a></p>
","<data-modeling><data-warehouse><dimensional-modeling><sql-data-warehouse><star-schema-datawarehouse>","2022-04-10 05:03:46","121","-1","1","71817296","<p>If you have a business requirement to count the number of awareness meetings then you would need:</p>
<ul>
<li>a fact table to hold the counts</li>
<li>a meetings dimension to hold the meeting name</li>
<li>a date dimension</li>
<li>any other dimensions needed to slice/dice/aggregate the fact table</li>
</ul>
"
"71797857","Updating fact table in warehouse fact table if it represents the process","<p>I'm in the process of designing data warehouse structure and ETL process.</p>
<p>I've identified 'purchase' fact table, which would hold all the purchases done by the users.</p>
<p>Problem is, that there are multiple things in the process that user has to do:</p>
<ol>
<li>subscribe to newsletter (without registration, optional step)</li>
<li>register his account</li>
<li>purchase sth</li>
</ol>
<p>It's important to show on the dashboard number of users who subscribed, who registered, who purchased something and conversion rates.</p>
<p>I've created following fact table (there are other things there, I skipped them to be more clear):</p>
<pre><code>subscription_date
account_id
purchase_date
</code></pre>
<p>My idea was, that if someone subscribed, but didn't register, only subscription_date will be filled-in, rest of those will be null. If he subscribed and registered - subscription_date and account_id is filled in. If he did whole process, all three fields will be filled.</p>
<p>The problem is, that user can subscribe on day 1, register on day 2 and purchase on day 3.
I'm using AWS glue to create ETL process. Everything works fine - it's fetching records from multiple database tables (subscription, account, purchase), combine them together and insert into purchase table. I do not know how to handle updates though.</p>
<p>What happens is if user subscribes on day 1, ETL job runs and inserts this record. If on day 2 user registers and on day 3 makes a purchase, this row is not updated. No new records are added there either.</p>
<p>I can think of two solutions for this problem:</p>
<ol>
<li><p>Split fact table into three different fact tables and join them when populating the dashboard (I'm using quicksight for that). Then no updates would be needed. If new purchase for account is inserted in source database, it will be just inserted with proper account_id.</p>
</li>
<li><p>Somehow try add to AWS Glue Job check if this account_id for purchase already exists and update the record</p>
</li>
<li><p>Delete all records from fact table that are updated and re-insert them with all data.</p>
</li>
</ol>
","<database-design><aws-glue><data-warehouse>","2022-04-08 13:25:11","245","-1","1","71821003","<p>The easiest solution would be to use Glue to land your data in staging tables (insert only) and then use SQL/Stored Procedures to upsert that data to the target tables.</p>
<p>BTW dimensional keys in your fact tables should never be null. So if a customer has not yet registered then the fact table registered date column should reference the “unknown date” row in your date dimension</p>
"
"71797611","Node MVC user data modelling - linking records to the current user","<p>I've set up a user model for an application. I would like each user to have access to their own records after they've logged in, e.g. orders, customers, etc. How best to implement this?</p>
<p>My only idea is that I add a &quot;user&quot; field to the records which holds the id of the user it belongs to (i.e. through parent referencing). Then add some middleware which ensures that only records linked to the currently logged in user are returned. This doesn't seem very efficient but I don't really know.</p>
<p>Is there a better way to model the data here?</p>
","<node.js><mongodb><model-view-controller><data-modeling><user-data>","2022-04-08 13:09:18","22","0","1","71797792","<p>this situation depends on your DB's data relationships.
For instance; 1:many,many:many, 1:1 etc.</p>
<p>Let's assume you have a product and user who bought that product could check it.</p>
<p>1 user can buy more than 1 product. so it's best to use 1:many.</p>
<p>but everytime when you call an user you're going to upload product data as well, that's why you need to know approximately how many data will be inserted.</p>
<p>user - product</p>
<ol>
<li>UserID</li>
<li>ProductID</li>
<li>OrderID</li>
<li>other details...</li>
</ol>
<p>Seller can access order by querying product id or order id,
as well as Customer also can check what kind of product has been bought.</p>
<p>BTW, My english is not good enough, sorry for that.<br />
Happy Coding✋</p>
"
"71763961","Is it ok to have 2 dimensions that are the same but one is less deep?","<p>I have a fact table, with <strong>account number</strong> and some numbers associated..</p>
<p>I have my <strong>DimAccount</strong> which has a very long hierarchy of level1,sub-level2… up to sub-level20.</p>
<p>When reporting in PowerBI this makes it very hard to navigate…</p>
<p>My requirement is to have a sort of different/new <strong>DimAccount</strong> which is less deep (it will be similar to <strong>DimAccount</strong> but with a different grouping)</p>
<p>So, I want to create a different mapping. Where should this be done?</p>
<p>In the backend?</p>
<ul>
<li>Having some sort of DimAccount2, where it has less hierarchies or</li>
<li>Creating new table? Perhaps creating a mapping table, where I just map sublevels to a less deep hierarchy?</li>
</ul>
<p>Or should this be corrected in the cube/powerbi ? creating measures in DAX where one does the mapping manually there?</p>
<p>I am not sure where/how to do it... My Goal is to have a <strong>DimHighLevelAccount</strong>, but it is not that I just can remove sub-levels, the mapping will be also different, perhaps I group some categories from level5,6 and 7 together...</p>
","<data-modeling><data-warehouse><dimensional-modeling>","2022-04-06 09:06:26","24","0","1","71809278","<p>Power BI always has its own data model (called a &quot;dataset&quot; in Power BI docs), derived in this case from the data model in your data warehouse.  And the Power BI data model has some modeling capabilities that your DW does not have.</p>
<p>So the Power BI data model should load/expose only the tables and columns from your data warehouse that are useful for the use case (you may have a handful of different Power BI datasets for the same DW tables).  And then add additional modeling, like adding Measures, hiding columns, and declaring <a href=""https://www.youtube.com/watch?v=x6vXVJZ_eTY&amp;t=11s"" rel=""nofollow noreferrer"">Hierarcies</a>.</p>
<p>So in this case, have a single Account dimension table, but when you bring it in to Power BI, leave out hierarchy levels that you don't want, and add the remaining ones to a Hierarchy and hide the individual levels from the report view, so the report developer sees a single hierarchal property.</p>
"
"71750287","Orika data mapping error after installing cmsoccaddon on custom commercewebservices on Hybris 2005","<p>We are using the custom commercewebservice extended from ycommercewebservice. We are not using the CMS API as of now but we have we need CMS APIs for spartacus.</p>
<p>I have install <strong>cmsoccaddon</strong> on our customer webservice extension. Addon install successfully but when we try to access https://localhost:9002/rest/v2/{base_site}/cms/pages it will throws the error:</p>
<p><code>ma.glasnost.orika.MappingException: While attempting the following mapping: sourceType = de.hybris.platform.cmsfacades.data.PageContentSlotData sourceProperty = slotShared(boolean) destinationType = java.util.ArrayList&lt;ContentSlotWsDTO&gt; destinationProperty = 8(ContentSlotWsDTO) Error occurred: ma.glasnost.orika.MappingException: Encountered mapping of primitive to object (or vise-versa); sourceType=boolean, destinationType=ContentSlotWsDTO</code></p>
<p>Fetch component API is working fine with the cmsaddon (https://localhost:9002/rest/v2/{base_site}/cms/components?fields=DEFAULT&amp;componentIds={componentId})</p>
<p>We Just update hybris version to 2005</p>
","<sap-commerce-cloud><spartacus-storefront>","2022-04-05 10:35:45","176","0","1","71970530","<p>Since you migrated from 1905 to 2005, the endpoints prefix has been changed from <code>/rest/v2/</code> to <code>/occ/v2</code>.</p>
<p>More information can be found in the following:</p>
<p>help.sap (2005) <a href=""https://help.sap.com/docs/SAP_COMMERCE/e5d7cec9064f453b84235dc582b886da/d46d19516961438f8939718e87ed787b.html?version=2005"" rel=""nofollow noreferrer"">https://help.sap.com/docs/SAP_COMMERCE/e5d7cec9064f453b84235dc582b886da/d46d19516961438f8939718e87ed787b.html?version=2005</a>
<a href=""https://help.sap.com/docs/SAP_COMMERCE/9d346683b0084da2938be8a285c0c27a/9136088a207c4fc1bb5ec2b53c462b23.html?version=2005"" rel=""nofollow noreferrer"">https://help.sap.com/docs/SAP_COMMERCE/9d346683b0084da2938be8a285c0c27a/9136088a207c4fc1bb5ec2b53c462b23.html?version=2005</a></p>
"
"71733887","UDP packet receive errors continuously increasing while using logstash udp input and some filters and output to a PostgreSQL database","<p>Here I am processing massive logs that are originated from a router, and sending it to logstash which is receiving logs via UDP input method. Having some filter methods to it and finally inserting those fields to a PostgreSQL database using JDBC output. Below is the logstash pipeline.</p>
<pre><code>    input{
    udp{
        port =&gt; 9002
        host =&gt; &quot;10.10.10.10&quot;
        queue_size =&gt; 25000
        workers =&gt; 8
        receive_buffer_bytes =&gt; 2119999999
    }
}

filter{
    grok{
        match =&gt; {
                        &quot;message&quot; =&gt; &quot;%{DATA}%{TIMESTAMP_ISO8601:Local_Time}%{ISO8601_TIMEZONE:NTP} %{HOSTNAME:Bras}%{DATA}: application:%{DATA:Application}, %{DATA} %{IPV4:Src_Nat_IP}:%{INT:Src_Nat_Port} \[%{IPV4:Src_IP}:%{INT:Src_Port}\]%{DATA}%{IPV4:Dst_IP}:%{INT:Dst_Port} \(%{DATA:Protocol}\)&quot;
                }
        match =&gt; {
                        &quot;message&quot; =&gt; &quot;%{DATA}%{TIMESTAMP_ISO8601:Local_Time}%{ISO8601_TIMEZONE:NTP} %{HOSTNAME:Bras}%{DATA}: application:%{DATA:Application}, %{DATA} %{IPV4:Src_IP}:%{INT:Src_Port}%{DATA}%{IPV4:Dst_IP}:%{INT:Dst_Port} \(%{DATA:Protocol}\)&quot;
                }
    }
    if ! [Src_Nat_IP] or ! [Src_Nat_Port]{
        mutate{
            add_field =&gt; [&quot;Where&quot;,&quot;from outside&quot;]
        }
    }
    else{
        mutate{
            add_field =&gt; [&quot;Where&quot;,&quot;from inside&quot;]
        }
    }
    mutate{
        gsub =&gt; [ &quot;Local_Time&quot;, &quot;T&quot;, &quot; &quot; ]
        remove_field =&gt; [&quot;host&quot;, &quot;NTP&quot;, &quot;log&quot;, &quot;event&quot;]
    }
    date{
        match =&gt; [ &quot;Local_Time&quot;, &quot;yyyy-mm-dd HH:mm:ss&quot; ]
    }
    if [Application] == &quot;none&quot;{
        mutate{
            remove_field =&gt; &quot;Application&quot;
        }
    }
}

output{
    jdbc{
        connection_string =&gt; 'jdbc:postgresql://127.0.0.1:6432/postgres?user=username&amp;password=password'
        max_pool_size =&gt; 999
        statement =&gt; [ &quot;INSERT INTO todat_test (local_time, bras, application, src_nat_ip, src_nat_port, src_ip, src_port, dst_ip, dst_port) VALUES (CAST (? AS timestamp), ?, ?, CAST (? AS inet), CAST (? AS integer),CAST (? AS inet), CAST (? AS integer), CAST (? AS inet), CAST (? AS integer))&quot;, &quot;Local_Time&quot;, &quot;Bras&quot;, &quot;Application&quot;, &quot;Src_Nat_IP&quot;, &quot;Src_Nat_Port&quot;, &quot;Src_IP&quot;, &quot;Src_Port&quot;, &quot;Dst_IP&quot;, &quot;Dst_Port&quot;]
    }
}
</code></pre>
<p>The server is running Centos 7, with 16 core CPU and 16GB RAM. I have also increased the default buffer size of both logstash and the OS itself but still there is a UDP packer receive error. Any solution to how can I handle these logs? Approximately 30 million lines are generated every hour. I have also setup pgbouncer to pool PostgreSQL connections; however, that is also not working.</p>
<p>My question?</p>
<p>How can I process all the logs received at the system and get all the data into the database?
Please suggest things that need to be tuned.</p>
","<database><postgresql><udp><logstash><data-warehouse>","2022-04-04 08:09:11","231","0","1","71742493","<p>Those grok patterns with multiple occurrences for %{DATA} are going to be really, really expensive if they do not match. It has to look for the timestamp starting at every character in the message, then once it matches that it has to check for the IP at every following character and so on.</p>
<p>Break up your patterns. You could try</p>
<pre><code>grok {
    break_on_match =&gt; false
    match =&gt; {
        &quot;message&quot; =&gt; [
            &quot;%{TIMESTAMP_ISO8601:Local_Time}%{ISO8601_TIMEZONE:NTP} %{HOSTNAME:Bras}&quot;,
            &quot;application:%{DATA:Application},&quot;
        ]
    }
}
grok {
    match =&gt; {
        &quot;message&quot; =&gt; [
            &quot; %{IPV4:Src_Nat_IP}:%{INT:Src_Nat_Port} \[%{IPV4:Src_IP}:%{INT:Src_Port}\]%{DATA}%{IPV4:Dst_IP}:%{INT:Dst_Port} \(%{DATA:Protocol}\)&quot;,
            &quot; %{IPV4:Src_IP}:%{INT:Src_Port}%{DATA}%{IPV4:Dst_IP}:%{INT:Dst_Port} \(%{DATA:Protocol}\)&quot;
        ]
    }
}
</code></pre>
<p>If the ip: port (proto) is at the end of the line then extract the</p>
<pre><code>%{IPV4:Dst_IP}:%{INT:Dst_Port} \(%{DATA:Protocol}\)$
</code></pre>
<p>and put that as an additional pattern in the first grok. That would remove the remaining DATA field.</p>
"
"71696251","Does Azure Purview have a data lineage API?","<p>I know there are connectors for Purview that support data lineage data collection.  However, I'm wondering if Purview has any sort of API that allows any data processing (ETL) process to write a lineage record/document to the Purview lineage repository?</p>
","<azure><azure-purview><data-governance>","2022-03-31 16:39:19","127","1","1","71698220","<p>It's built on <a href=""https://medium.datadriveninvestor.com/azure-purview-catalog-is-based-on-apache-atlas-8e340830cabe"" rel=""nofollow noreferrer"">Apache Atlas</a> - Data Governance and Metadata framework for Hadoop.</p>
"
"71620131","How to Import xPath fields of XML to Pimcore DataDirector dataport raw data fields?","<p>Pimcore has powerfull plugin DataDirector for import automation.
It proposes manual configuration of fields to import and their mapping to Pimcore fields <a href=""https://www.youtube.com/watch?v=nyhKJTzTq-4&amp;list=PL4-QRNfdsdKIfzQIP-c9hRruXf0r48fjt"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=nyhKJTzTq-4&amp;list=PL4-QRNfdsdKIfzQIP-c9hRruXf0r48fjt</a></p>
<p>It works fine if you have 10-50 fields.
How to import that configuration from some csv file whan you have 700+ fields?</p>
","<import><pimcore><data-mapping><datadirector>","2022-03-25 16:21:15","42","0","1","71620132","<p>There is no ability to import using interface of commandline DataDirector API.</p>
<p>I tried to request such functionality creation from vendor -- it costs.</p>
<p>I tried to edit it in MySQL but it is strictly coupled to other data, see: SELECT sourceconfig FROM <code>plugin_pim_dataport</code>;</p>
<p><strong>Solution:</strong></p>
<p>Each dataport can be exported and imported to JSON. This is our chance.</p>
<ol>
<li><p>create sample dataport using your source XML</p>
</li>
<li><p>export it to sample.json</p>
</li>
<li><p>unserialize $json_a['plugin_pim_dataport']['sourceconfig'] and $json_a['plugin_pim_rawitemField'] containers</p>
<pre><code>     $string = file_get_contents(
         &quot;sample.json&quot;
     );
     $json_a = json_decode($string, true);
     $sourceConfig = $json_a['plugin_pim_dataport']['sourceconfig'];
     $sourceConfig = unserialize($sourceConfig, ['allowed_classes' =&gt; false]);

     $fieldsConfig = $json_a['plugin_pim_rawitemField']; 
</code></pre>
</li>
<li><p>add new fields to them from any source e.g. CSV</p>
<pre><code>     while (($data = fgetcsv($handle, 1000, &quot;,&quot;)) !== false) {
         $row++;
         $id = $max + 1 + $row;
         $data = $this-&gt;addField($id, $data[0], $data[1]);
         $sourceConfig['fields']['field_' . ($id + 1)] = $data['sourceConfig'];
         $fieldsConfig[$id] = $data['fieldsConfig'];
     }

 public function addField(
     $id,
     $name = 'ObjectBrick/Field',
     $xpath = '//*[.//MainContainer = &quot;systeme&quot;]//*[.//Description = &quot;SomeAttribute&quot;]/anyPath/SampleContainer'
 ) {
     $res = [];

     $res['fieldsConfig'] = [
         &quot;dataportId&quot; =&gt; &quot;1&quot;,
         &quot;fieldNo&quot;    =&gt; $id + 1,
         &quot;name&quot;       =&gt; $name,
         &quot;priority&quot;   =&gt; $id
     ];
     $res['sourceConfig'] = [
         'xpath'       =&gt; $xpath,
         'multiValues' =&gt; false,
     ];

     return $res;
 }
</code></pre>
</li>
<li><p>serialize, save to JSON, import to your dataport.</p>
</li>
</ol>
"
"71613988","How different fact tables are handled to share some common dimension tables in data marts in star model?","<p>I am quite new to DW and I am just learning the stuff. I read on the Internet that after the ETL process, DW data is then stored in some data marts for some reasons such as ease of use. Each data mart can use a structure. Let's say a data mart is using star structure. Now my questions arise:</p>
<ol>
<li>First of all, can a data mart use two structures, for instance, star and snowflake?</li>
</ol>
<p>Assume we have two data marts that are using star structure. Suppose that both of them have only one fact table and some dimension tables. The thing is, it turned out that some of the dimension tables in the first data mart are the same as the second one.</p>
<ol start=""2"">
<li>While considering they are in different data marts, what should we do? Should we duplicate the tables in different data marts?</li>
<li>What if the fact tables were in the same data mart? Should we duplicate dimension tables or just create a foreign key to the table we already have?</li>
</ol>
","<data-warehouse><snowflake-schema><star-schema><fact-table><datamart>","2022-03-25 08:27:07","126","0","1","71614915","<ol>
<li><p>Snowflake describes one or more objects in your model. So some parts of your model could be snowflaked and others not</p>
</li>
<li><p>Datamarts are logical groupings of your facts and dimensions, not physical ones. So you don’t duplicate these tables and they can appear in as many datamarts as necessary</p>
</li>
</ol>
"
"71600262","What is the difference between storing the changing value in fact vs dimension?","<p>I have customer dimension table and the location of customer can change.</p>
<p>The customerid filters the sales fact table.</p>
<p>I have 2 options:</p>
<ol>
<li>Slowly changing dimension type 2 to hold 1 new record for each customer's location changes</li>
</ol>
<p>Or</p>
<ol start=""2"">
<li>Store the location at the time of data load into the sales fact table.</li>
</ol>
<p>Both ways allow me to see sales by location (although it's a customer location, the etl will place it on fact table).</p>
<p>The later option saves me from implementing SCD on dim table.</p>
<p>What are factors to decide which of the 2 approaches is suitable?</p>
","<ssis><etl><data-warehouse><star-schema><scd>","2022-03-24 09:54:13","60","0","2","71601767","<p>How you model location depends on what it relates to. If it is an attribute of a sale then it belongs as its own dim related to the sale. If it is an attribute of a customer (such as their home address) then it belongs in the customer dim. If the location is an attribute of both a sale and a customer then it belongs in both</p>
"
"71600262","What is the difference between storing the changing value in fact vs dimension?","<p>I have customer dimension table and the location of customer can change.</p>
<p>The customerid filters the sales fact table.</p>
<p>I have 2 options:</p>
<ol>
<li>Slowly changing dimension type 2 to hold 1 new record for each customer's location changes</li>
</ol>
<p>Or</p>
<ol start=""2"">
<li>Store the location at the time of data load into the sales fact table.</li>
</ol>
<p>Both ways allow me to see sales by location (although it's a customer location, the etl will place it on fact table).</p>
<p>The later option saves me from implementing SCD on dim table.</p>
<p>What are factors to decide which of the 2 approaches is suitable?</p>
","<ssis><etl><data-warehouse><star-schema><scd>","2022-03-24 09:54:13","60","0","2","71619048","<p>Your fact table should contain things that we measure, count, total. Your dimensions should be descriptive elements that allow users to slice their data along an axis - basically answer the &quot;by&quot; part of their request</p>
<blockquote>
<p>I want to see total sales by year and month across this customer based regional hierarchy</p>
</blockquote>
<p>Don't take my word for it, grab a data warehousing book or go read the freely available information from the <a href=""https://www.kimballgroup.com"" rel=""nofollow noreferrer"">Kimball Group</a></p>
<p>Storing the customer data on the fact is a bad idea regardless of your database engine. To satisfy a query like the above, the storage engine needs to read in the entirety of your fact table and the supporting dimensions. It could read (Date, RegionId, CustomerId, SalesAmount) which likely costs something like 16 bytes per row times however many rows you have. Or, it can read (Date, RegionId, CustomerName, CustomerAddress, CustomerCity, CustomerState, CustomerPostalCode, SalesAmount) at a cost of what, 70 bytes per row? That's an inflation to</p>
<ul>
<li>store your data (disk is cheap but that's not the point)</li>
<li>read your data (basic physics, the more data you wrote to disk, the longer it takes to read it back out)</li>
<li>less available memory for other queries (you're in a multi-user/query environment, when you hog resources, there's less for others)</li>
<li>write data (ETL processing is going to take longer because you have to write more pages to disk than you should have)</li>
<li>inability to optimize (What if the business just wants to see &quot;Total Sales by Year and Month&quot; - no customer hierarchy. The database engine will still have to read all the pages with all that useless customer data just to get at the things the user actually wanted)</li>
</ul>
<p>Finally, the most important takeaway from the Data Warehouse Toolkit is on like page 1. The biggest reason that Data Warehouse projects fails is that IT drives the requirements and it sounds like you're thinking of doing that to avoid creating a SCD type 2 dimension. If the business problem you're attempting to solve is that they need to be able to see sales data associated to the customer data at the point of time it happened, you have a Type 2 customer dimension.</p>
<p>Yes, technologies like Columnstore Compression can reduce the amount of storage required but it's not free because now you're adding workload to the cpu. Maybe you have it, maybe you don't. Or, you model it correctly and then do the compression as well and you still come out ahead in a proper dimensional model.</p>
"
"71587448","Whether sales (fact) table should be made to have the userid from customer (dim) table in dimensional modeling?","<p>I have following relationships:</p>
<p>User (<strong>userid</strong>, username)</p>
<p>Customer (<strong>customerid</strong>, <strong>userid</strong>, customername)</p>
<p>Sales (salesid, <strong>customerid</strong>, saledate, saleamt)</p>
<p>In the DW I am loading the data as follows:</p>
<p>User (<strong>userid</strong>, username)</p>
<p>Customer (customerid, <strong>userid</strong>, customername)</p>
<p>Sales (salesid, <strong>customerid</strong>, saledate, saleamt, <strong>userid</strong>)</p>
<p>Note: user id is actually not on the fact table but I am adding it by looking it up from the respective customer record, in order to make the model a star schema: User filters Sales; customer filters Sales.</p>
<p>Is it better to not do this and have the model as a snow flake such that user filters customer; and customer filters sales. (that is - not have the userid in the sales fact table)?</p>
","<sql><data-warehouse><snowflake-schema><star-schema><star-schema-datawarehouse>","2022-03-23 12:43:29","23","0","1","71595726","<p>The standard practice is not to create a Snowflake  design - so all your Dims will be linked directly to the Fact and you don’t have FKs on Dims referencing other Dims.</p>
<p>So unless you have a very good reason not to do so, this is the pattern you should be using.</p>
"
"71549467","Data Model for different source data structures","<p>Question on Data Modeling</p>
<p>I am building a data warehouse on Redshift where I have fact data coming from multiple sources.</p>
<p>Source 1 -
Has different facts and Dimension tables they are maintained well with integrity etc. Dimension tables have dimension attributes, fact tables have fact attributes, they are connected by foreign keys,
Order table
Product table
Customer Table</p>
<p>Source 2-
Has only one wide fact table, which collects all fact and dimension information.
For example, it has only oner order table with all the order attributes, product attributes, customer attributes. This is manually entered data. There are no identifiers for products, customers, etc either.</p>
<p>There would be more source systems in the future I have no clue at this point how they are stored.</p>
<p>I have built a star schema in my data warehouse that can consume Source1 easily.
Can I use this model to consumer source 2? Would you have a different design?</p>
","<data-modeling><data-warehouse><datamodel><dimensional-modeling><star-schema>","2022-03-20 18:20:50","134","0","1","71867662","<p>You seem to have the common challenge of integration - blending together data from multiple source systems.</p>
<p>There are a couple of well known data modelling paradigms that are usually employed to deal with this sort situation.</p>
<p>If your warehouse is needing to integrate deeply, ie potentially to entity level, then I suggest using the data vault approach.</p>
<p>If you only need to integrate on specific activity type - visits or sales - then I wold suggest the activity schema modelling approach.</p>
<p>Indeed, as previous commented have mentioned, the data modelling in the warehouse is usually independent of source systems - you should design for use cases and down stream applications.</p>
"
"71512356","Copy JSON data from Snowflake into S3","<p>I am trying to COPY data (Variant data type - JSON) from Snowflake to S3. I am using below command:</p>
<pre><code>copy into @STG_SF_S3_DEV_JJC/sample_file1.json
from (select distinct RECORD_CONTENT from MYTABLE where LOAD_DT_TS &gt;= '2022-02-09 00:00:00')
FILE_FORMAT = (TYPE=JSON,COMPRESSION = NONE)
SINGLE=true
header=true
OVERWRITE = TRUE;
</code></pre>
<p>The data is getting copied. I am using STORAGE_INTEGRATION process &amp; creating STAGE table.
My question is:</p>
<blockquote>
<ol>
<li>For above query, I am getting 23 rows and I want to load these 23 rows as 23 separate files (JSON files) in S3. How to achieve this?</li>
<li>Each record has different LOAD_DT_TS and so for each 23 files, I need to give different names. I mean, the file name should end with LOAD_DT_TS. How to achieve this?</li>
<li><strong>MYTABLE</strong> in above query (COPY command), I have 4 columns. Is there any possible way to load all the 4 columns into a single JSON file?</li>
</ol>
</blockquote>
<p>Please share your thoughts.</p>
","<amazon-s3><snowflake-cloud-data-platform><data-warehouse><file-copying>","2022-03-17 12:33:45","641","0","3","71516173","<p>Please refer below (it covers point #1 and 2).</p>
<pre><code>SNOWFLAKE1#COMPUTE_WH@TEST_DB.PUBLIC&gt;select * from test_pivot;
+---------+-----------+-----------+-----------+
| COMPANY | BU        | EVAL_DESC | MEMBER_ID |
|---------+-----------+-----------+-----------|
| C1      | FINANCIAL | L1        | ID1       |
| C1      | FINANCIAL | L2        | ID2       |
| C1      | FINANCIAL | L3        | ID3       |
| C1      | HR        | L1        | ID4       |
| C1      | HR        | L2        | ID5       |
| C2      | FINANCIAL | L1        | ID6       |
| C2      | BUSINESS  | L1        | ID7       |
+---------+-----------+-----------+-----------+
7 Row(s) produced. Time Elapsed: 0.187s
SNOWFLAKE1#COMPUTE_WH@TEST_DB.PUBLIC&gt;list @test_row_stage;
+------+------+-----+---------------+
| name | size | md5 | last_modified |
|------+------+-----+---------------|
+------+------+-----+---------------+
0 Row(s) produced. Time Elapsed: 0.177s
SNOWFLAKE1#COMPUTE_WH@TEST_DB.PUBLIC&gt;EXECUTE IMMEDIATE $$
                                     DECLARE
                                       company varchar2(30);
                                       BU varchar2(30);
                                       eval_desc varchar2(30);
                                       member_id varchar2(30);
                                       file_name varchar2(30);
                                       c1 CURSOR FOR SELECT * FROM test_pivot;
                                     BEGIN
                                      // OPEN c1;
                                       for record in c1 do
                                       company:=record.company;
                                       BU:=record.BU;
                                       eval_desc:=record.eval_desc;
                                       member_id:=record.member_id;
                                       file_name:='load'||'_'||member_id||'.csv';
                                       create or replace temporary table temp_test_pvt(company varchar2(30),BU varchar2
                                     (30),eval_desc varchar2(30),member_id varchar2(30));
                                       insert into temp_test_pvt values (:company,:bu,:eval_desc,:member_id);
                                       execute immediate 'copy into @test_row_stage/'||:file_name||' from (select * fro
                                     m temp_test_pvt) overwrite=false';
                                       end for;
                                       RETURN 0;
                                     END;
                                     $$
                                     ;
+-----------------+
| anonymous block |
|-----------------|
|               0 |
+-----------------+
1 Row(s) produced. Time Elapsed: 9.803s
SNOWFLAKE1#COMPUTE_WH@TEST_DB.PUBLIC&gt;list @test_row_stage;
+------------------------------------------+------+----------------------------------+-------------------------------+
| name                                     | size | md5                              | last_modified                 |
|------------------------------------------+------+----------------------------------+-------------------------------|
| test_row_stage/load_ID1.csv_0_0_0.csv.gz |   48 | df314a0d95a771d5d81fa9b1cfb3a28e | Thu, 17 Mar 2022 16:47:42 GMT |
| test_row_stage/load_ID2.csv_0_0_0.csv.gz |   48 | 6be39868046f583b1c63d616faa9e7f6 | Thu, 17 Mar 2022 16:47:43 GMT |
| test_row_stage/load_ID3.csv_0_0_0.csv.gz |   48 | ecf9dbcb4e45fa29d6bcfe268c6ccae3 | Thu, 17 Mar 2022 16:47:44 GMT |
| test_row_stage/load_ID4.csv_0_0_0.csv.gz |   48 | 0fd3ff7e0a453e04be3aca22147a7d32 | Thu, 17 Mar 2022 16:47:45 GMT |
| test_row_stage/load_ID5.csv_0_0_0.csv.gz |   48 | c77985f8312a540816d82b4bf4ec5249 | Thu, 17 Mar 2022 16:47:46 GMT |
| test_row_stage/load_ID6.csv_0_0_0.csv.gz |   48 | c9e9d6fd613a8bdb76413dd3e9464cc4 | Thu, 17 Mar 2022 16:47:47 GMT |
| test_row_stage/load_ID7.csv_0_0_0.csv.gz |   48 | 4e4b999ed56059b44ee6bd15f28cafb8 | Thu, 17 Mar 2022 16:47:48 GMT |
+------------------------------------------+------+----------------------------------+-------------------------------+
7 Row(s) produced. Time Elapsed: 0.176s
SNOWFLAKE1#COMPUTE_WH@TEST_DB.PUBLIC&gt;select $1,$2,$3,$4 from @test_row_stage/load_ID1.csv_0_0_0.csv.gz;
+----+-----------+----+-----+
| $1 | $2        | $3 | $4  |
|----+-----------+----+-----|
| C1 | FINANCIAL | L1 | ID1 |
+----+-----------+----+-----+
1 Row(s) produced. Time Elapsed: 0.429s
SNOWFLAKE1#COMPUTE_WH@TEST_DB.PUBLIC&gt;select $1,$2,$3,$4 from @test_row_stage/load_ID2.csv_0_0_0.csv.gz;
+----+-----------+----+-----+
| $1 | $2        | $3 | $4  |
|----+-----------+----+-----|
| C1 | FINANCIAL | L2 | ID2 |
+----+-----------+----+-----+
1 Row(s) produced. Time Elapsed: 0.374s
SNOWFLAKE1#COMPUTE_WH@TEST_DB.PUBLIC&gt;select $1,$2,$3,$4 from @test_row_stage/load_ID3.csv_0_0_0.csv.gz;
+----+-----------+----+-----+
| $1 | $2        | $3 | $4  |
|----+-----------+----+-----|
| C1 | FINANCIAL | L3 | ID3 |
+----+-----------+----+-----+
1 Row(s) produced. Time Elapsed: 0.506s
SNOWFLAKE1#COMPUTE_WH@TEST_DB.PUBLIC&gt;select $1,$2,$3,$4 from @test_row_stage/load_ID4.csv_0_0_0.csv.gz;
+----+----+----+-----+
| $1 | $2 | $3 | $4  |
|----+----+----+-----|
| C1 | HR | L1 | ID4 |
+----+----+----+-----+
1 Row(s) produced. Time Elapsed: 0.281s
SNOWFLAKE1#COMPUTE_WH@TEST_DB.PUBLIC&gt;
</code></pre>
"
"71512356","Copy JSON data from Snowflake into S3","<p>I am trying to COPY data (Variant data type - JSON) from Snowflake to S3. I am using below command:</p>
<pre><code>copy into @STG_SF_S3_DEV_JJC/sample_file1.json
from (select distinct RECORD_CONTENT from MYTABLE where LOAD_DT_TS &gt;= '2022-02-09 00:00:00')
FILE_FORMAT = (TYPE=JSON,COMPRESSION = NONE)
SINGLE=true
header=true
OVERWRITE = TRUE;
</code></pre>
<p>The data is getting copied. I am using STORAGE_INTEGRATION process &amp; creating STAGE table.
My question is:</p>
<blockquote>
<ol>
<li>For above query, I am getting 23 rows and I want to load these 23 rows as 23 separate files (JSON files) in S3. How to achieve this?</li>
<li>Each record has different LOAD_DT_TS and so for each 23 files, I need to give different names. I mean, the file name should end with LOAD_DT_TS. How to achieve this?</li>
<li><strong>MYTABLE</strong> in above query (COPY command), I have 4 columns. Is there any possible way to load all the 4 columns into a single JSON file?</li>
</ol>
</blockquote>
<p>Please share your thoughts.</p>
","<amazon-s3><snowflake-cloud-data-platform><data-warehouse><file-copying>","2022-03-17 12:33:45","641","0","3","71531105","<p>This works for input a predicate value with quotes placement -</p>
<pre><code>SNOWFLAKE1#COMPUTE_WH@TEST_DB.PUBLIC&gt;EXECUTE IMMEDIATE $$
                                     DECLARE
                                      load_dt_tss timestamp;
                                      file_name varchar2(30);
                                     BEGIN
                                      file_name:='somefile'||'.csv';
                                      execute immediate 'copy into @test_row_stage/'||:file_name||' from (select LOAD_D
                                     T_TS from TEST_TS where LOAD_DT_TS &gt;=' || '''2022-02-09 00:00:00''' || ')' || ' FI
                                     LE_FORMAT = (TYPE=CSV,COMPRESSION = NONE) overwrite=FALSE';
                                      RETURN 0;
                                     END;
                                     $$
                                     ;
+-----------------+
| anonymous block |
|-----------------|
|               0 |
+-----------------+
1 Row(s) produced. Time Elapsed: 0.584s
</code></pre>
<p><a href=""https://i.stack.imgur.com/magya.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/magya.png"" alt=""enter image description here"" /></a></p>
"
"71512356","Copy JSON data from Snowflake into S3","<p>I am trying to COPY data (Variant data type - JSON) from Snowflake to S3. I am using below command:</p>
<pre><code>copy into @STG_SF_S3_DEV_JJC/sample_file1.json
from (select distinct RECORD_CONTENT from MYTABLE where LOAD_DT_TS &gt;= '2022-02-09 00:00:00')
FILE_FORMAT = (TYPE=JSON,COMPRESSION = NONE)
SINGLE=true
header=true
OVERWRITE = TRUE;
</code></pre>
<p>The data is getting copied. I am using STORAGE_INTEGRATION process &amp; creating STAGE table.
My question is:</p>
<blockquote>
<ol>
<li>For above query, I am getting 23 rows and I want to load these 23 rows as 23 separate files (JSON files) in S3. How to achieve this?</li>
<li>Each record has different LOAD_DT_TS and so for each 23 files, I need to give different names. I mean, the file name should end with LOAD_DT_TS. How to achieve this?</li>
<li><strong>MYTABLE</strong> in above query (COPY command), I have 4 columns. Is there any possible way to load all the 4 columns into a single JSON file?</li>
</ol>
</blockquote>
<p>Please share your thoughts.</p>
","<amazon-s3><snowflake-cloud-data-platform><data-warehouse><file-copying>","2022-03-17 12:33:45","641","0","3","72303955","<p>SINGLE=true - This option send the full SQL output to single file, if you remove this it will send the output to separate files.</p>
"
"71512191","Data conversion failed in Sql server/SSIS/Excel due to truncation","<p><a href=""https://i.stack.imgur.com/OdlC2.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OdlC2.jpg"" alt=""enter image description here"" /></a>Im getting error while loading excel file to <strong>Sql server</strong> through <strong>SSIS</strong>.for only two column ( client , rue ) . the problem is my text is not even long . i have been searching everywhere and tried every solution but no results . here what i tried :</p>
<p>1- solution 1 : in data conversion i changed the type <strong>unicode string  to string STR</strong> and increase the <strong>length</strong> =&gt; failed</p>
<p>2- solution 2 =&gt; i changed data type in sql server from varchar(255) to varchar(max) then in data conversion i changed it from <strong>unicode string to TEXT</strong> =&gt; failed</p>
<p>3-solution 3 =&gt; i went through advanced setting and changed the output type , as well as <strong>TruncationRowDiposit</strong> : Rdignore failre . but still getting the same error</p>
<p>4- slution 4 =&gt; set the <strong>TypeGuessRows</strong> registry value to 0 in <strong>HKEY_LOCAL_MACHINE</strong> , this was my last attempt but failed</p>
<p>any other solution ? because i get the same error :
[17/03/2022 13:00]
<strong>The conversion returned status value 4 and status text &quot;Le texte est tronqué ou un ou plusieurs caractères n'ont aucune correspondance dans la page de codes cible.&quot;.
The &quot;Conversion de données.Sorties[Sortie de conversion de données].Colonnes[Copie de Nom Client]&quot; failed because truncation occurred,</strong></p>
<p><a href=""https://i.stack.imgur.com/gGYTh.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gGYTh.jpg"" alt=""enter image description here"" /></a><a href=""https://i.stack.imgur.com/kgYe6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kgYe6.png"" alt=""enter image description here"" /></a><a href=""https://i.stack.imgur.com/MBz72.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MBz72.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/0XiqW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0XiqW.jpg"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/0XiqW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0XiqW.jpg"" alt=""enter image description here"" /></a></p>
","<sql-server><excel><ssis><etl><data-warehouse>","2022-03-17 12:20:08","445","0","1","71533349","<p>I have had the same issue and I ended up changing my SQL Server Data Type from VARCHAR to NVARCHAR and that fixed my issue. I don't know why SSIS was doing this but changing to NVARCHAR fixed my issue.</p>
"
"71505539","Changing organization of data so that each observation represents a new variable (I tried)","<p>I am working in Stata with a dataset on electric vehicle charging stations. Variables include</p>
<p><code>station_name</code> name of charging station</p>
<p><code>review_text</code> all of the customer reviews for a specific station delimited by <code>}{</code></p>
<p><code>num_reviews</code> number of customer reviews.</p>
<p>I'm trying to make a new file where each observation represents one customer review in a new variable <code>customer_review</code> and another variable <code>station_id</code> has the name of the corresponding station. So, if the original dataset had 100 observations (one per station) with 5 reviews each, the new file should have 500 observations.</p>
<p>How can I do this? I would include some code I have tried but I have no idea how to start.</p>
","<stata><data-analysis><data-management>","2022-03-16 23:53:35","53","0","1","71505683","<p>If your data look like this:</p>
<pre><code>       station              reviews   n  
  1.         1   {good}{bad}{great}   3  
  2.         2    {poor}{excellent}   2  

</code></pre>
<p>Then the following:</p>
<pre><code>split(reviews), parse(}{)
drop reviews n
reshape long reviews, i(station) j(review_num)
drop if reviews==&quot;&quot;
replace reviews = subinstr(reviews, &quot;}&quot;,&quot;&quot;,.)
replace reviews = subinstr(reviews, &quot;{&quot;,&quot;&quot;,.)
</code></pre>
<p>will produce:</p>
<pre><code>       station   review~m     reviews  
  1.         1          1        good  
  2.         1          2         bad  
  3.         1          3       great  
  4.         2          1        poor  
  5.         2          2   excellent  
</code></pre>
"
"71493460","What is the most optimal way of using sequence Generator in SQL","<p>I am using below SQL on snowflake database for creating random Sequence based on Date, but it takes almost 4 hours in 2XL warehouse and majority time is spent on window function,</p>
<pre><code>SELECT 
   DAY_DT,
   AGMT_ID, 
   COUNTRY_CD,
   CUSTOMER_ID,
   SUM(AMOUNT_YTD),
   SUM(AMOUNT_MTD)
   ROW_NUMBER() OVER( ORDER BY 1 ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS Contract_Id 
from DB_1.SCHEMA01.TABLE_Test 
where DAY_DT = Current_DATE()-1 
group by 1,2,3,4;
</code></pre>
<p>(163Million rows are there in table_test)
Is there any alternate method to write above window function efficiently so that runtime is optimized?</p>
","<sql><snowflake-cloud-data-platform><data-warehouse><snowflake-schema>","2022-03-16 07:58:46","124","1","1","71493733","<p>If you need a sequence with no gaps you will need <a href=""https://docs.snowflake.com/en/sql-reference/functions/row_number.html"" rel=""nofollow noreferrer"">ROW_NUMBER</a></p>
<p>but then you should use:</p>
<pre><code>ROW_NUMBER() OVER(ORDER BY null) AS Contract_Id 
</code></pre>
<p>but that has to all be done on one node. If you just need numbers that go up.</p>
<p>use <a href=""https://docs.snowflake.com/en/sql-reference/functions/seq1.html"" rel=""nofollow noreferrer"">SEQ8</a>()</p>
<pre><code>SEQ8() as Contract_Id
</code></pre>
<p>So using this CTE with some fake data to test those out:</p>
<pre><code>WITH TABLE_Test as (
    SELECT * 
    FROM VALUES
    (Current_DATE()-1, 10,100,1000, 1.23, 43.21),
    (Current_DATE()-1, 11,100,1000, 1.23, 43.21),
    (Current_DATE()-1, 10,100,1000, 1, 1),
    (Current_DATE()-1, 11,100,1000, 1, 1),
    (Current_DATE()-1, 12,100,1000, 1, 1)
    v(DAY_DT, AGMT_ID, COUNTRY_CD, CUSTOMER_ID, AMOUNT_YTD, AMOUNT_MTD)
)
</code></pre>
<p>you SQL gives:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>DAY_DT</th>
<th>AGMT_ID</th>
<th>COUNTRY_CD</th>
<th>CUSTOMER_ID</th>
<th>SUM(AMOUNT_YTD)</th>
<th>SUM(AMOUNT_MTD)</th>
<th>CONTRACT_ID</th>
</tr>
</thead>
<tbody>
<tr>
<td>2022-03-15</td>
<td>10</td>
<td>100</td>
<td>1000</td>
<td>2.23</td>
<td>44.21</td>
<td>1</td>
</tr>
<tr>
<td>2022-03-15</td>
<td>11</td>
<td>100</td>
<td>1000</td>
<td>2.23</td>
<td>44.21</td>
<td>2</td>
</tr>
<tr>
<td>2022-03-15</td>
<td>12</td>
<td>100</td>
<td>1000</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
</tbody>
</table>
</div>
<p>thus the SEQ option works a charm:</p>
<pre><code>SELECT 
   DAY_DT,
   AGMT_ID, 
   COUNTRY_CD,
   CUSTOMER_ID,
   SUM(AMOUNT_YTD),
   SUM(AMOUNT_MTD),
   SEQ8() + 1 AS Contract_Id 
from TABLE_Test 
where DAY_DT = Current_DATE()-1 
group by 1,2,3,4;
</code></pre>
<p>as does the ROW_NUMBER() OVER (ORDER BY null) version.</p>
<pre><code>SELECT 
   DAY_DT,
   AGMT_ID, 
   COUNTRY_CD,
   CUSTOMER_ID,
   SUM(AMOUNT_YTD),
   SUM(AMOUNT_MTD),
   ROW_NUMBER() OVER(ORDER BY null) AS Contract_Id 
UNBOUNDED FOLLOWING) AS Contract_Id 
from TABLE_Test 
where DAY_DT = Current_DATE()-1 
group by 1,2,3,4;
</code></pre>
<p>The SEQn() version will be much faster, but can have gaps in the numbers.</p>
<p>Given there is no ORDER BY sorting in the data, both methods (row number, seq) each query could/will have different allocation of values.</p>
"
"71470912","Using MERGE with analytic functions (like RANK) on target table","<p>I have a data pipeline where, at random intervals, the a staging table called <code>stg</code> is truncated and overwritten with records. Then, using <code>MERGE</code>, the records in <code>stg</code> should be merged into the dimension table <code>dim</code> according to the following rules (it's a slowly changing dimension of <a href=""https://en.wikipedia.org/wiki/Slowly_changing_dimension#Type_2:_add_new_row"" rel=""nofollow noreferrer"">type 2</a>):</p>
<ul>
<li>when there exists an email in <code>stg</code> that doesn't exist in <code>dim</code> insert the row corresponding to that email into <code>dim</code> with label <code>'INSERT'</code></li>
<li>when an email <em>does</em> exist in both <code>stg</code> and <code>dim</code> check to see if their corresponding data differs. If so, it's indicative of an <code>'UPDATE'</code>.</li>
<li>when an email is absent from <code>stg</code> but does exist in <code>dim</code>, this means it's been deleted so label with <code>'DELETE'</code>.</li>
</ul>
<p>Emails are unique to each user in this organization.</p>
<pre class=""lang-sql prettyprint-override""><code>MERGE dim
USING stg
ON stg.email = dim.email
WHEN NOT MATCHED
    INSERT(email, first_name, last_name, last_active, dml_type, extraction_timestamp, state_index)
    VALUES(stg.email, stg.first_name, stg.last_name, stg.last_active, 'INSERT', stg.extraction_timestamp))
WHEN MATCHED AND stg.row_hash &lt;&gt; dim.row_hash
    INSERT(email, first_name, last_name, last_active, dml_type, extraction_timestamp, state_index)
    VALUES(stg.email, stg.first_name, stg.last_name, stg.last_active, 'UPDATE', stg.extraction_timestamp)
WHEN NOT MATCHED BY SOURCE 
    INSERT(email, first_name, last_name, last_active, dml_type, extraction_timestamp, state_index)
    VALUES(stg.email, NULL, NULL, NULL, 'DELETE', stg.extraction_timestamp)
</code></pre>
<p>The problem? This query compares <code>stg</code> against the entirety of <code>dim</code>, when actually I only want to compare it with the following subset of <code>dim</code>:</p>
<pre class=""lang-sql prettyprint-override""><code>select *
from (
    select *, 
    RANK() OVER(PARTITION BY email ORDER BY extraction_timestamp DESC) as rnk 
    from dim
) as hist
where rnk = 1

</code></pre>
<p>Is it possible for me to <code>MERGE</code> with <code>dim</code> as my target table, and <code>stg</code> as my source, but based only on the <code>rnk=1</code> values in as computed in the <code>RANK()</code> analytical function shown above?</p>
<p>Something like this?:</p>
<pre class=""lang-sql prettyprint-override""><code>MERGE (
    select *, 
    RANK() OVER(PARTITION BY email ORDER BY extraction_timestamp DESC) as rnk 
    from dim
) as dim_with_rank
USING stg
ON stg.email = dim_with_rank.email
AND dim_with_rank.rnk = 1
WHEN NOT MATCHED
    INSERT(email, first_name, last_name, last_active, dml_type, extraction_timestamp, state_index)
    VALUES(stg.email, stg.first_name, stg.last_name, stg.last_active, 'INSERT', stg.extraction_timestamp))
WHEN MATCHED AND stg.row_hash &lt;&gt; dim.row_hash
    INSERT(email, first_name, last_name, last_active, dml_type, extraction_timestamp, state_index)
    VALUES(stg.email, stg.first_name, stg.last_name, stg.last_active, 'UPDATE', stg.extraction_timestamp)
WHEN NOT MATCHED BY SOURCE 
    INSERT(email, first_name, last_name, last_active, dml_type, extraction_timestamp, state_index)
    VALUES(stg.email, NULL, NULL, NULL, 'DELETE', stg.extraction_timestamp)
</code></pre>
","<sql><google-bigquery><data-warehouse><scd>","2022-03-14 16:12:51","267","2","1","71604569","<p>Unfortunately, you can’t run a merge and then launch a subquery as your example:</p>
<pre><code>MERGE (
      select *,
  RANK() OVER(PARTITION BY email ORDER BY extraction_timestamp DESC) as rnk
  from dim)
as dim_with_rank
USING stg
ON stg.email = dim_with_rank.email
AND dim_with_rank.rnk = 1
</code></pre>
<p>You would need to create a query like the next one:</p>
<pre><code>MERGE dim 
USING (
        select *, RANK() OVER(PARTITION BY email ORDER BY extraction_timestamp DESC) as rnk
        from stg )
 as stg_with_rank
 ON stg_with_rank.email = dim.email AND
 stg_with_rank.rnk = 1 [...]
</code></pre>
<p>You can see more information about this use case <a href=""https://cloud.google.com/bigquery/docs/reference/standard-sql/dml-syntax#merge_statement"" rel=""nofollow noreferrer"">here</a></p>
"
"71467762","using tryCatch to manage zero variance variables in R","<p>I need to use <code>cor</code> function inside a for-loop and it stops with this error:</p>
<blockquote>
<p>Error: from glmnet Fortran code (error code 7777); All used predictors
have zero variance</p>
</blockquote>
<p>Although throwing data away is an option when dealing with constant predictors across samples, I am looking for managing this so that in such cases, zero is assigned. So,I want use <code>tryCatch</code> . Here is the structure of two datasets that I use as 'x' and 'y' for <code>cor</code> function:</p>
<pre><code>my_list &lt;- list(ENSG0000014 = list(set_1 = structure(list(expr = c(&quot; 9.940670e-02&quot;, 
&quot; 1.289670e-01&quot;, &quot;-7.394904e-03&quot;, &quot; 9.940670e-02&quot;, &quot; 9.940670e-02&quot;
)), row.names = c(NA, -5L), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;
)), set_2 = structure(list(expr = c(&quot; 9.940670e-02&quot;, &quot; 1.289670e-01&quot;, 
&quot;-7.394904e-03&quot;, &quot; 9.940670e-02&quot;, &quot; 9.940670e-02&quot;)), row.names = c(NA, 
-5L), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;)), set_3 = structure(list(
    expr = c(&quot; 9.940670e-02&quot;, &quot; 1.289670e-01&quot;, &quot;-7.394904e-03&quot;, 
    &quot; 9.940670e-02&quot;, &quot; 9.940670e-02&quot;)), row.names = c(NA, -5L
), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;))), ENSG0000015 = list(
    set_1 = structure(list(expr = c(&quot; 9.940670e-02&quot;, &quot; 1.289670e-01&quot;, 
    &quot;-7.394904e-03&quot;, &quot; 9.940670e-02&quot;, &quot; 9.940670e-02&quot;)), row.names = c(NA, 
    -5L), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;)), set_2 = structure(list(
        expr = c(&quot; 9.940670e-02&quot;, &quot; 1.289670e-01&quot;, &quot;-7.394904e-03&quot;, 
        &quot; 9.940670e-02&quot;, &quot; 9.940670e-02&quot;)), row.names = c(NA, 
    -5L), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;)), set_3 = structure(list(
        expr = c(&quot; 9.940670e-02&quot;, &quot; 1.289670e-01&quot;, &quot;-7.394904e-03&quot;, 
        &quot; 9.940670e-02&quot;, &quot; 9.940670e-02&quot;)), row.names = c(NA, 
    -5L), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;))))

# class(my_list[[1]][[1]])
# [1] &quot;tbl_df&quot;     &quot;tbl&quot;        &quot;data.frame&quot;

alpha0 &lt;- structure(list(s1 = c(0.0187340226414619, 0.00908524361272445, 
0.00931968564730621, 0.0112482521491358, 0.00929048221018, 0.0126019919679288, 
0.0194020980563262, 0.00510680509885104, 0.0182302538181811, 
0.0070983818621192, 0.0111253617983923, 0.00820051118160036, 
0.0221411777890218, 0.0120777292746757, 0.00681949654210974, 
0.0119405765039798, 0.00730874406889384, 0.0194020980563262, 
0.00838540350315828), s1.1 = c(-0.0111797117112052, -0.0151154469367662, 
-0.0274839538246719, -0.0232526535738252, -0.0214438607379155, 
-0.0139148593878817, -0.0233534584833798, -0.0149056307836724, 
-0.014480345769519, -0.0180705053683703, -0.0226853642294842, 
-0.0189920966797582, -0.00967108125788073, -0.0254219105164431, 
-0.0217286557382764, -0.0151528980623231, -0.0176955630367825, 
-0.0178792733665001, -0.0229882892112302), s1.2 = c(-0.00657906147914681, 
-0.00161418712964399, 0.00549018150634929, -0.00563876737503981, 
0.00676109674439662, 0.0175951004499502, 0.000870549311249299, 
-0.00253963535899881, 0.00402394916896305, 0.00305742983988951, 
-0.00272125028295169, 0.00263511211988609, -0.00287396022316179, 
-0.000843642468373697, 0.00780145969696268, 0.00971427253061111, 
0.00155452869320207, 0.00897302942313759, 0.0000166335321619695
)), class = &quot;data.frame&quot;, row.names = c(NA, 19L))
</code></pre>
<p>And here is what I tied with no success.</p>
<pre><code>for (g in 1:2) { 
  for (f in 1:3) {
    ## correlation between actual expression value (testing set) and the predicted one.
    cor_r_sets[[f]] &lt;- cor(my_list[[g]][[f]], 
                            alpha0[, f])
  }
  cor_r_genes[[g]] &lt;- cor_r_sets
}

## Usigy tryCatch:

for (g in 1:2) { 
  for (f in 1:3) {
    ## tryatch to evaluate the zero variances and assign the exceptions.
cor_rg &lt;- tryCatch(
  {
    cor_r_sets[[f]] &lt;- cor(final_lists$expr.y[[g]][[f]], 
                           alpha0.predicted_final[, f]

     },
      error = function(e) {
        cor_r_sets[[f]] = 0
      }
    )

  }
      }
      cor_r_genes[[g]] &lt;- cor_r_sets
    }
</code></pre>
<p>Could you please guide me how to make it work!</p>
","<r><data-management>","2022-03-14 12:25:34","94","0","1","71468185","<p>As stated in the comments, you need to return a value for the error. Also, you data doesn't work as posted, so I <code>type.convert</code> the data to numeric and then I only use the first 5 rows in alpha0.</p>
<pre class=""lang-r prettyprint-override""><code>cor_r_sets &lt;- c()
cor_r_genes &lt;- list()

for (g in 1:2) { 
  for (f in 1:3) {
   
    cor_r_sets[[f]] &lt;- tryCatch(
      cor(
        type.convert(my_list[[g]][[f]], as.is = T),
        alpha0[1:5, f]),
      error = \(e) 0)
  }
  cor_r_genes[[g]] &lt;- cor_r_sets
}

cor_r_genes
#&gt; [[1]]
#&gt; [[1]][[1]]
#&gt;           [,1]
#&gt; expr 0.1893385
#&gt; 
#&gt; [[1]][[2]]
#&gt;           [,1]
#&gt; expr 0.7048062
#&gt; 
#&gt; [[1]][[3]]
#&gt;            [,1]
#&gt; expr -0.5055223
#&gt; 
#&gt; 
#&gt; [[2]]
#&gt; [[2]][[1]]
#&gt;           [,1]
#&gt; expr 0.1893385
#&gt; 
#&gt; [[2]][[2]]
#&gt;           [,1]
#&gt; expr 0.7048062
#&gt; 
#&gt; [[2]][[3]]
#&gt;            [,1]
#&gt; expr -0.5055223
</code></pre>
<p>I would recommend looking into <code>map</code> functions to avoid using loops. For this example it is relatively fast to use a loop, but indexing can be confusing as loops become nested and can quickly become slow. Example of mapping:</p>
<pre class=""lang-r prettyprint-override""><code>library(tidyverse)

map(my_list,
    \(x) map2(
      x, 1:3, 
      \(x, y) tryCatch(
        cor(
          type.convert(x[,1], as.is = T), 
          alpha0[1:5,y]
          ), 
        error = \(e) 0
        )
      )
    )
#&gt; $ENSG0000014
#&gt; $ENSG0000014$set_1
#&gt;           [,1]
#&gt; expr 0.1893385
#&gt; 
#&gt; $ENSG0000014$set_2
#&gt;           [,1]
#&gt; expr 0.7048062
#&gt; 
#&gt; $ENSG0000014$set_3
#&gt;            [,1]
#&gt; expr -0.5055223
#&gt; 
#&gt; 
#&gt; $ENSG0000015
#&gt; $ENSG0000015$set_1
#&gt;           [,1]
#&gt; expr 0.1893385
#&gt; 
#&gt; $ENSG0000015$set_2
#&gt;           [,1]
#&gt; expr 0.7048062
#&gt; 
#&gt; $ENSG0000015$set_3
#&gt;            [,1]
#&gt; expr -0.5055223
</code></pre>
"
"71456221","How to add multiple column dynamically based on filter condition","<p>I am trying to create multiple columns dynamically based on filter condition after comparing two data frame with below code</p>
<pre><code>source_df
+---+-----+-----+----+
|key|val11|val12|date|
+---+-----+-----+-----+
|abc|  1.1| john|2-3-21
|def|  3.0| dani|2-2-21
+---+-----+-----+------

dest_df
+---+-----+-----+------+
|key|val11|val12|date  |
+---+-----+-----+------
|abc|  2.1| jack|2-3-21|
|def|  3.0| dani|2-2-21|
-----------------------
</code></pre>
<pre><code>columns= source_df.columns[1:]
joined_df=source_df\
    .join(dest_df, 'key', 'full')
for column in columns:
     column_name=&quot;difference_in_&quot;+str(column)
     report = joined_df\
    .filter((source_df[column] != dest_df[column]))\
    .withColumn(column_name, F.concat(F.lit('[src:'), source_df[column], F.lit(',dst:'),dest_df[column],F.lit(']')))


</code></pre>
<p>The output I expect is</p>
<pre><code>#Expected
+---+-----------------+------------------+
|key| difference_in_val11| difference_in_val12 |
+---+-----------------+------------------+
|abc|[src:1.1,dst:2.1]|[src:john,dst:jack]|
+---+-----------------+-------------------+
</code></pre>
<p>I get only one column result</p>
<pre><code>#Actual
+---+-----------------+-
|key| difference_in_val12  |
+---+-----------------+-|
|abc|[src:john,dst:jack]|
+---+-----------------+-
</code></pre>
<p>How to generate multiple columns based on filter condition dynamically?</p>
","<dataframe><pyspark><apache-spark-sql><data-quality>","2022-03-13 11:18:03","419","-2","2","71456729","<p>Dataframes are immutable objects. Having said that, you need to create another dataframe using the one that got generated in the 1st iteration. Something like below -</p>
<pre><code>from pyspark.sql import functions as F

columns= source_df.columns[1:]
joined_df=source_df\
    .join(dest_df, 'key', 'full')
for column in columns:
  if column != columns[-1]:
       column_name=&quot;difference_in_&quot;+str(column)
       report = joined_df\
                    .filter((source_df[column] != dest_df[column]))\
                    .withColumn(column_name, F.concat(F.lit('[src:'), source_df[column], F.lit(',dst:'),dest_df[column],F.lit(']')))

  else:
    column_name=&quot;difference_in_&quot;+str(column)
    report1 = report.filter((source_df[column] != dest_df[column]))\
                    .withColumn(column_name, F.concat(F.lit('[src:'), source_df[column], F.lit(',dst:'),dest_df[column],F.lit(']')))
report1.show()
#report.show()
</code></pre>
<p><strong>Output</strong> -</p>
<pre><code>+---+-----+-----+-----+-----+-------------------+-------------------+
|key|val11|val12|val11|val12|difference_in_val11|difference_in_val12|
+---+-----+-----+-----+-----+-------------------+-------------------+
|abc|  1.1| john|  2.1| jack|  [src:1.1,dst:2.1]|[src:john,dst:jack]|
+---+-----+-----+-----+-----+-------------------+-------------------+
</code></pre>
"
"71456221","How to add multiple column dynamically based on filter condition","<p>I am trying to create multiple columns dynamically based on filter condition after comparing two data frame with below code</p>
<pre><code>source_df
+---+-----+-----+----+
|key|val11|val12|date|
+---+-----+-----+-----+
|abc|  1.1| john|2-3-21
|def|  3.0| dani|2-2-21
+---+-----+-----+------

dest_df
+---+-----+-----+------+
|key|val11|val12|date  |
+---+-----+-----+------
|abc|  2.1| jack|2-3-21|
|def|  3.0| dani|2-2-21|
-----------------------
</code></pre>
<pre><code>columns= source_df.columns[1:]
joined_df=source_df\
    .join(dest_df, 'key', 'full')
for column in columns:
     column_name=&quot;difference_in_&quot;+str(column)
     report = joined_df\
    .filter((source_df[column] != dest_df[column]))\
    .withColumn(column_name, F.concat(F.lit('[src:'), source_df[column], F.lit(',dst:'),dest_df[column],F.lit(']')))


</code></pre>
<p>The output I expect is</p>
<pre><code>#Expected
+---+-----------------+------------------+
|key| difference_in_val11| difference_in_val12 |
+---+-----------------+------------------+
|abc|[src:1.1,dst:2.1]|[src:john,dst:jack]|
+---+-----------------+-------------------+
</code></pre>
<p>I get only one column result</p>
<pre><code>#Actual
+---+-----------------+-
|key| difference_in_val12  |
+---+-----------------+-|
|abc|[src:john,dst:jack]|
+---+-----------------+-
</code></pre>
<p>How to generate multiple columns based on filter condition dynamically?</p>
","<dataframe><pyspark><apache-spark-sql><data-quality>","2022-03-13 11:18:03","419","-2","2","71458061","<p>You could also do this with a union of both dataframes and then collect list only if collect_set size is greater than 1 , this can avoid joining the dataframes:</p>
<pre><code>from pyspark.sql import functions as F
cols = source_df.drop(&quot;key&quot;).columns

output = (source_df.withColumn(&quot;ref&quot;,F.lit(&quot;src:&quot;))
          .unionByName(dest_df.withColumn(&quot;ref&quot;,F.lit(&quot;dst:&quot;))).groupBy(&quot;key&quot;)
.agg(*[F.when(F.size(F.collect_set(i))&gt;1,F.collect_list(F.concat(&quot;ref&quot;,i))).alias(i)
       for i in cols]).dropna(subset = cols, how='all')
         )
</code></pre>
<hr />
<pre><code>output.show()

+---+------------------+--------------------+
|key|             val11|               val12|
+---+------------------+--------------------+
|abc|[src:1.1, dst:2.1]|[src:john, dst:jack]|
+---+------------------+--------------------+
</code></pre>
"
"71439379","In this specific example, can I have more than one fact table?","<p>I have the following assignment for Business Intelligence Class:</p>
<ul>
<li><p>&quot;The CSUF Business Association is conducting a babysitter service as a fundraiser
for different clubs in college. When a customer is entered into the system, the
CSUF Club coordinator gets name, address, and phone. The coordinator also
records each babysitting job, the amount paid for it and the sitter assigned to
the job. Each person may sign up to credit only one club and the system keeps
the contact person and phone number for each participating club.</p>
</li>
<li><p>The treasurer wants a data warehouse for this. He would like to be able to
determine how much each customer was billed by week, month, or year. How
much each employee (sitter) earned, also summed by time periods. He is
interested in how much work is done on weekends, holidays or other special
days.</p>
</li>
<li><p>Develop a data warehouse using the methodology you prefer to provide this
information (including fact and dimension tables, their attributes, keys, and
relationships), referring the name of the schema that was adopted.&quot;</p>
</li>
</ul>
<p>I was thinking about the following fact tables:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Fact_CustBill</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dim_Time_ID</td>
</tr>
<tr>
<td>Dim_Club_ID</td>
</tr>
<tr>
<td>Dim_Cust_ID</td>
</tr>
<tr>
<td>Fact_BilledAmount</td>
</tr>
</tbody>
</table>
</div><div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Fact_EmployeeEarns</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dim_Time_ID</td>
</tr>
<tr>
<td>Dim_Club_ID</td>
</tr>
<tr>
<td>Dim_Emp_ID</td>
</tr>
<tr>
<td>Fact_EarnedAmount</td>
</tr>
</tbody>
</table>
</div><div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Fact_WorkSpecial</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dim_TimeSpecial_ID</td>
</tr>
<tr>
<td>Fact_HoursWorked</td>
</tr>
</tbody>
</table>
</div>
<p>I'm thinking about these fact tables just based on an example I was given during class. I'm not sure about Dim_Club_ID being necessary on the employee tables and I'm also not 100% about the &quot;Fact_WorkSpecial&quot;. Is my reasoning correct or not really?</p>
","<schema><data-warehouse><business-intelligence><fact-table>","2022-03-11 13:36:48","140","0","1","71444014","<p>You can have multiple fact tables. And you should have multiple fact tables if you’re recording data of different granularity.</p>
<p>But in this case one fact table is enough for both employee revenues and amounts biller to customers. The grain is the same: the babysitting job.</p>
<p>So you can have one fact table linking to club, date, customer and employee dimensions, with multiple measures: hours worked, amount billed to client and amount earned by employee.</p>
<p>When you want to report you can pick only one of those measures and query on it. But it will give you more info: whether some clients prefer to work with certain employees.</p>
<p>The trick here is your date dimension. It needs calendar year, month and day, but also year of week (iso 8601), week number and day of the week.</p>
"
"71430360","Extract and match multiple rows in google sheets","<p>I'm in Google sheets, attempting to assign the correct Salesforce Account IDs to a LONG list of accounts. I have exported all accounts with IDs, and in another sheet I have a list of opportunities I need to import with DataLoader. However, in order to attach these opportunities to their accounts, they need to have the Account ID.</p>
<p>Essentially I need to take Account Name from each sheet, take the corresponding ID and assign that ID to both Account Names. I am not a developer but I am a Salesforce Admin which does require some basic formula building and I'm more than willing to give anything a shot in order to not have to match them manually, one-by-one. Thanks.</p>
","<google-sheets><salesforce><data-manipulation><data-management>","2022-03-10 20:25:23","136","2","1","71430966","<p>You need <a href=""https://support.google.com/docs/answer/3093318?hl=en-GB"" rel=""nofollow noreferrer"">VLOOKUP</a>.</p>
<p>Reorder your columns so the Account Name comes first, then Id. Optionally sort by Name, performance will be better. Think what you'll want to do with duplicate accounts.</p>
<pre><code>=VLOOKUP(E2;$A$2:$B$14;2;false)
</code></pre>
<p><a href=""https://i.stack.imgur.com/3wpqN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3wpqN.png"" alt=""enter image description here"" /></a></p>
<p>the parameters are</p>
<ol>
<li>which value,</li>
<li>against what range (putting dollar signs helps fix the range, otherwise as you drag the formula down it might drag the range down too),</li>
<li>cell from which column to return,</li>
<li>is the reference table sorted</li>
</ol>
"
"71419439","How can I monitor a CSV file in Azure blob store for newly added records, similar to unix ""tail -f""?","<h2>Context:</h2>
<ul>
<li>I am an information architect (not a data engineer, was once a Unix and <a href=""https://en.wikipedia.org/wiki/Oracle_Database"" rel=""nofollow noreferrer"">Oracle</a> developer), so my technical knowledge in Azure is limited to browsing Microsoft documentation.</li>
<li>The context of this problem is ingesting data from a constantly growing CSV file, in Azure ADLS into an Azure SQL MI database.</li>
<li>I am designing an Azure data platform that includes a SQL data warehouse with the first source system being a <a href=""https://en.wikipedia.org/wiki/Microsoft_Dynamics_365"" rel=""nofollow noreferrer"">Dynamics 365</a> application.</li>
<li>The data warehouse is following Data Vault 2.0 patterns. This is well suited to the transaction log nature of the CSV files.</li>
<li>This platform is in early development - not in production.</li>
<li>The CSV files are created and updated (append mode) by an Azure Synapse Link that is exporting dataverse write operations on selected dataverse entities to our ADLS storage account. This service is configured in append mode, so all dataverse write operations (create, update and delate) produce an append action to the entities corresponding CSV file. Each CSV file is essentially a transaction log of the corresponding dataverse entity</li>
<li>Synapse Link operates in an event based fashion - creating a records in dataverse triggers a CSV append action. Latency is typically a few seconds. There aren't any SLAs (promises), and latency can be several minutes if the API caps are breached.</li>
<li>The CSV is partitioned Annually. This means the a new CSV file is created at the start of each year and continues to grow throughout the year.</li>
<li>We are currently trialling <a href=""https://en.wikipedia.org/wiki/Microsoft_Azure#Data_management"" rel=""nofollow noreferrer"">ADF</a> as the means of extracting records from the CSV for loading into the data warehouse. We are not wedded to ADF and can consider changing horses.</li>
</ul>
<h2>Request:</h2>
<p>I'm searching for an event based solution for ingesting that monitors a source CSV file for new records (appended to the end of the file) and extracts only those new records from the CSV file and then processes each record in sequence which result in one or more SQL insert operations for each new CSV record. If I was back in my old Unix days, I would build a process around the &quot;tail -f&quot; command as the start of the pipeline with the next step an <a href=""https://en.wikipedia.org/wiki/Extract,_transform,_load"" rel=""nofollow noreferrer"">ETL</a> process that processed each record served by the <a href=""https://en.wikipedia.org/wiki/Tail_(Unix)"" rel=""nofollow noreferrer"">tail</a> command. But I can't figure out how to do this in Azure.</p>
<p>This process will be the pattern for many more similar ingestion processes - there could be approximately one thousand CSV files that need to be processed in this event based - near real time process. I assume one process per CSV file.</p>
<p>Some nonfunctional requirements are speed and efficiency.</p>
<ul>
<li>My goal is for an event based solution (low latency = speed),</li>
<li>that doesn't need to read the entire file every 5 minutes to see if there are changes. This is an inefficient (micro) batch process that will be horribly inefficient (read: expensive - 15,000x redundant processing). This is where the desire for a process like Unix &quot;tail -f&quot; comes to mind. It watches the file for changes, emitting new data as it is appended to the source file. I'd hate to do something like a 'diff' every 5 minutes as this is inefficient and when scaled to thousands of tables will be prohibitively expensive.</li>
</ul>
","<azure-blob-storage><microsoft-dynamics><data-warehouse><tail><data-vault>","2022-03-10 05:56:45","293","0","1","71491304","<p>One possible solution to your problem is to store each new CSV record as a separate blob.</p>
<p>You will then be able to use Azure Event Grid to raise events when a new blob is created in Blob Storage i.e. use <a href=""https://learn.microsoft.com/en-us/azure/event-grid/event-schema-blob-storage?tabs=event-grid-event-schema"" rel=""nofollow noreferrer"">Azure Blob Storage as Event Grid source</a>.</p>
<p>The basic idea is to store the changed CSV data as new blob and have Event Grid wired to Blob Created event. An Azure Function can listen to these events and then only process the new data. For auditing purposes, you can save this data in a separate Append Blob once the CSV processing has been completed.</p>
"
"71404896","How to get the column names between two columns in R","<p>Is there a function in R that returns the names of columns, including and between two column names? This function would take the beginning column and the ending columns as inputs and return the names of the columns between and including the inputs.</p>
<p>For example, say I am using the following data frame:</p>
<pre><code>set.seed(1993)
DF.1 &lt;- data.frame(&quot;Q0&quot;= sample(seq(1,100), replace = T, 100))
for(i in seq(1,50, by =1)){
  
  x &lt;-sample(seq(1,100), replace = T, 100)
  DF &lt;- data.frame(x)
  
  names(DF)[names(DF) == &quot;x&quot;] &lt;- paste(&quot;Q&quot;, i, sep = &quot;&quot;)
  
  DF.1 &lt;- cbind(DF.1, DF)
  
}

colnames(DF.1)
</code></pre>
<p>A potential function might look like this</p>
<pre><code>function(Column Name 1, Column Name 2, data)
function(&quot;Q1&quot;, &quot;Q5&quot;, data = DF.1)
</code></pre>
<p>And it would output:</p>
<pre><code>&gt;&quot;Q1&quot;  &quot;Q2&quot;  &quot;Q3&quot;  &quot;Q4&quot;  &quot;Q5&quot;
</code></pre>
<p>Note: that the function would need to be generalizable to any set of column names.</p>
<p>Thank You!</p>
","<r><dataframe><data-management>","2022-03-09 05:55:12","645","1","3","71404937","<p>You can use the <code>dplyr</code> package like this:</p>
<pre><code>library(dplyr) 
df %&gt;% select(Q1:Q5) %&gt;% colnames() 
</code></pre>
<p>or as function:</p>
<pre><code>find_colnames &lt;- function(c1, c2, data) data %&gt;% select(c1:c2) %&gt;% colnames()
</code></pre>
"
"71404896","How to get the column names between two columns in R","<p>Is there a function in R that returns the names of columns, including and between two column names? This function would take the beginning column and the ending columns as inputs and return the names of the columns between and including the inputs.</p>
<p>For example, say I am using the following data frame:</p>
<pre><code>set.seed(1993)
DF.1 &lt;- data.frame(&quot;Q0&quot;= sample(seq(1,100), replace = T, 100))
for(i in seq(1,50, by =1)){
  
  x &lt;-sample(seq(1,100), replace = T, 100)
  DF &lt;- data.frame(x)
  
  names(DF)[names(DF) == &quot;x&quot;] &lt;- paste(&quot;Q&quot;, i, sep = &quot;&quot;)
  
  DF.1 &lt;- cbind(DF.1, DF)
  
}

colnames(DF.1)
</code></pre>
<p>A potential function might look like this</p>
<pre><code>function(Column Name 1, Column Name 2, data)
function(&quot;Q1&quot;, &quot;Q5&quot;, data = DF.1)
</code></pre>
<p>And it would output:</p>
<pre><code>&gt;&quot;Q1&quot;  &quot;Q2&quot;  &quot;Q3&quot;  &quot;Q4&quot;  &quot;Q5&quot;
</code></pre>
<p>Note: that the function would need to be generalizable to any set of column names.</p>
<p>Thank You!</p>
","<r><dataframe><data-management>","2022-03-09 05:55:12","645","1","3","71405552","<p>Without additional packages.</p>
<pre><code>names_between &lt;- function(col_1, col_2, data){
  names(data)[which(names(data) == col_1): which(names(data) == col_2)]
  }

names_between(&quot;Q1&quot;, &quot;Q5&quot;, data = DF.1)
[1] &quot;Q1&quot; &quot;Q2&quot; &quot;Q3&quot; &quot;Q4&quot; &quot;Q5&quot;
</code></pre>
"
"71404896","How to get the column names between two columns in R","<p>Is there a function in R that returns the names of columns, including and between two column names? This function would take the beginning column and the ending columns as inputs and return the names of the columns between and including the inputs.</p>
<p>For example, say I am using the following data frame:</p>
<pre><code>set.seed(1993)
DF.1 &lt;- data.frame(&quot;Q0&quot;= sample(seq(1,100), replace = T, 100))
for(i in seq(1,50, by =1)){
  
  x &lt;-sample(seq(1,100), replace = T, 100)
  DF &lt;- data.frame(x)
  
  names(DF)[names(DF) == &quot;x&quot;] &lt;- paste(&quot;Q&quot;, i, sep = &quot;&quot;)
  
  DF.1 &lt;- cbind(DF.1, DF)
  
}

colnames(DF.1)
</code></pre>
<p>A potential function might look like this</p>
<pre><code>function(Column Name 1, Column Name 2, data)
function(&quot;Q1&quot;, &quot;Q5&quot;, data = DF.1)
</code></pre>
<p>And it would output:</p>
<pre><code>&gt;&quot;Q1&quot;  &quot;Q2&quot;  &quot;Q3&quot;  &quot;Q4&quot;  &quot;Q5&quot;
</code></pre>
<p>Note: that the function would need to be generalizable to any set of column names.</p>
<p>Thank You!</p>
","<r><dataframe><data-management>","2022-03-09 05:55:12","645","1","3","71405749","<p>Using <code>grep</code> and <code>seq.int</code>.</p>
<pre><code>f &lt;- function(cn1, cn2, data) {
  pat &lt;- paste(paste0('^', c(cn1, cn2), '$'), collapse='|')
  g &lt;- grep(pat, colnames(data))
  colnames(data)[do.call(seq.int, as.list(g))]
}

f(&quot;Q1&quot;, &quot;Q5&quot;, data=DF.1)
# [1] &quot;Q1&quot; &quot;Q2&quot; &quot;Q3&quot; &quot;Q4&quot; &quot;Q5&quot;
</code></pre>
"
"71401864","How is includeProjectIds used in Google data catalog search?","<p>When <a href=""https://cloud.google.com/data-catalog/docs/reference/rest/v1/catalog/search"" rel=""nofollow noreferrer"">searching using Google's data catalog</a>, a <code>scope</code> parameter is required and one of its fields is <code>includeProjectIds</code>. According to the documentation, this parameter represents:</p>
<blockquote>
<p>The list of project IDs to search within.</p>
</blockquote>
<p>However, the results returned are not limited to that projectid. Am I understanding the parameter incorrectly? I can limit the results using <code>projectid=myproject</code> in the query, but I am trying to understand the <code>includeProjectIds</code> field.</p>
<p>Example request body:</p>
<pre><code>{
  &quot;scope&quot;: {
    &quot;includeProjectIds&quot;: [
      &quot;MY-PROJECT&quot;
    ]
  },
  &quot;query&quot;: &quot;type=dataset&quot;
}
</code></pre>
<p>Example response:</p>
<pre><code>{
  &quot;results&quot;: [
    # I expect this result:
    {
      &quot;searchResultType&quot;: &quot;ENTRY&quot;,
      &quot;searchResultSubtype&quot;: &quot;entry.dataset&quot;,
      &quot;relativeResourceName&quot;: &quot;projects/MY-PROJECT/locations/us/entryGroups/@bigquery/entries/....&quot;,
      &quot;linkedResource&quot;: &quot;//bigquery.googleapis.com/projects/MY-PROJECT/datasets/dataset_name&quot;,
      &quot;modifyTime&quot;: &quot;2000-01-01T00:00:00Z&quot;,
      &quot;integratedSystem&quot;: &quot;BIGQUERY&quot;,
      &quot;description&quot;: &quot;My description&quot;
    },
    # But I don't expect this:
    {
      &quot;searchResultType&quot;: &quot;ENTRY&quot;,
      &quot;searchResultSubtype&quot;: &quot;entry.dataset&quot;,
      &quot;relativeResourceName&quot;: &quot;projects/NOT-MY-PROJECT/locations/us/entryGroups/@bigquery/entries/....&quot;,
      &quot;linkedResource&quot;: &quot;//bigquery.googleapis.com/projects/NOT-MY-PROJECT/datasets/dataset_name&quot;,
      &quot;modifyTime&quot;: &quot;2000-01-01T00:00:00Z&quot;,
      &quot;integratedSystem&quot;: &quot;BIGQUERY&quot;,
      &quot;description&quot;: &quot;My description&quot;
    },
    ...
  ]
}
</code></pre>
","<google-cloud-platform><google-data-catalog>","2022-03-08 21:49:59","194","1","1","71406058","<p>You may try using curl method instead because it uses the service account key of your service account. For service account and keys creation you can refer to this <a href=""https://cloud.google.com/translate/docs/setup#creating_service_accounts_and_keys"" rel=""nofollow noreferrer"">documentation</a>. I tried this and the output was correct having only my project datasets displayed in the results.</p>
<p>Command:</p>
<pre><code>-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \
-H &quot;Content-Type: application/json; charset=utf-8&quot; \
-d @request.json \
&quot;https://datacatalog.googleapis.com/v1/catalog:search&quot;
</code></pre>
<p>Inside request.json:</p>
<pre><code>{
  &quot;scope&quot;: {
    &quot;includeProjectIds&quot;: [
      &quot;my-project&quot;
    ]
  },
  &quot;query&quot;: &quot;type=dataset&quot;
}
</code></pre>
<p>Output:</p>
<pre><code>a@cloudshell:~ (a)$ curl -X POST \
-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \
-H &quot;Content-Type: application/json; charset=utf-8&quot; \
-d @request.json \
&quot;https://datacatalog.googleapis.com/v1/catalog:search&quot;
{
  &quot;results&quot;: [
    {
      &quot;searchResultType&quot;: &quot;ENTRY&quot;,
      &quot;searchResultSubtype&quot;: &quot;entry.dataset&quot;,
      &quot;relativeResourceName&quot;: &quot;projects/my-project/locations/us/entryGroups/@bigquery/entries/cHJvamVjdHMvdGlwaC1hbmplbGFiL2RhdGFzZXRzLzIwMjIwMTEx&quot;,
      &quot;linkedResource&quot;: &quot;//bigquery.googleapis.com/projects/my-project/datasets/2&quot;,
      &quot;modifyTime&quot;: &quot;2022-01-11T01:15:11Z&quot;,
      &quot;integratedSystem&quot;: &quot;BIGQUERY&quot;
    },
    {
      &quot;searchResultType&quot;: &quot;ENTRY&quot;,
      &quot;searchResultSubtype&quot;: &quot;entry.dataset&quot;,
      &quot;relativeResourceName&quot;: &quot;projects/my-project/locations/us/entryGroups/@bigquery/entries/cHJvamVjdHMvdGlwaC1hbmplbGFiL2RhdGFzZXRzL0dDUFF1aWNrU3RhcnQ&quot;,
      &quot;linkedResource&quot;: &quot;//bigquery.googleapis.com/projects/my-project/datasets/G&quot;,
      &quot;modifyTime&quot;: &quot;2022-01-07T07:54:58Z&quot;,
      &quot;integratedSystem&quot;: &quot;BIGQUERY&quot;
    },
    {
      &quot;searchResultType&quot;: &quot;ENTRY&quot;,
      &quot;searchResultSubtype&quot;: &quot;entry.dataset&quot;,
      &quot;relativeResourceName&quot;: &quot;projects/my-project/locations/us-east1/entryGroups/@bigquery/entries/cHJvamVjdHMvdGlwaC1hbmplbGFiL2RhdGFzZXRzLzIwMjExMjI4&quot;,
      &quot;linkedResource&quot;: &quot;//bigquery.googleapis.com/projects/my-project/datasets/20&quot;,
      &quot;modifyTime&quot;: &quot;2021-12-28T02:00:47Z&quot;,
      &quot;integratedSystem&quot;: &quot;BIGQUERY&quot;
    },
    {
      &quot;searchResultType&quot;: &quot;ENTRY&quot;,
      &quot;searchResultSubtype&quot;: &quot;entry.dataset&quot;,
      &quot;relativeResourceName&quot;: &quot;projects/my-project/locations/us-central1/entryGroups/@bigquery/entries/cHJvamVjdHMvdGlwaC1hbmplbGFiL2RhdGFzZXRzL2JxbWxfdHV0b3JpYWw&quot;,
      &quot;linkedResource&quot;: &quot;//bigquery.googleapis.com/projects/my-project/datasets/b&quot;,
      &quot;modifyTime&quot;: &quot;2021-12-14T14:58:09Z&quot;,
      &quot;integratedSystem&quot;: &quot;BIGQUERY&quot;
    },
    {
      &quot;searchResultType&quot;: &quot;ENTRY&quot;,
      &quot;searchResultSubtype&quot;: &quot;entry.dataset&quot;,
      &quot;relativeResourceName&quot;: &quot;projects/my-project/locations/us-central1/entryGroups/@bigquery/entries/cHJvamVjdHMvdGlwaC1hbmplbGFiL2RhdGFzZXRzL2JhYnluYW1lcw&quot;,
      &quot;linkedResource&quot;: &quot;//bigquery.googleapis.com/projects/my-project/datasets/ba&quot;,
      &quot;modifyTime&quot;: &quot;2021-12-14T14:01:46Z&quot;,
      &quot;integratedSystem&quot;: &quot;BIGQUERY&quot;
    }
  ]
}
</code></pre>
"
"71401567","Star schema - splitting dimensions into multiple smaller dimensions","<p>I have been working in dashboarding for a long time and have always been using the traditional kimball star schema to build my reporting and dashboarding on. The architecture team at work have been working on automating the dimensional star schema. The automation brings a noticeable difference with what I am used to.</p>
<p>What they started doing: creating mini-dimensions based on each business combination of hub and link of the DV warehouse. For instance, the client dimension is splitted in 5 based on attributes affinity and change frequency (dim_client_address, dim_client_physical_attributes, dim_client_sociodemo, dim_client_common, ...). The product is splitted in 4 mini dimensions. And so on for all dimensions. All these mini dimensions are using the same NK but different SK. All mini dims have the same number of rows but different attributes.</p>
<p>I'm getting told that this:</p>
<ul>
<li>is more performance since I'm querying a subset of columns at a time (my queries are generated by reporting tools)</li>
<li>is more business relevant since the grouping are based on business concepts</li>
</ul>
<p>This looks like a good idea when presented to me but before going all in I want to make sure that we do not have some blond spots. For instance, to me this makes it a little less easier to query which is a basic principle of dimensional star schema.</p>
<p>It seems to me that it is a form of centipede fact table but not sure since I can't find examples on the web.</p>
<p>Is this something you have encountered?</p>
<p>Thank you!
(sorry if the english is not perfect, it's not my daily language)</p>
","<data-warehouse><dimensional-modeling><star-schema><kimball>","2022-03-08 21:19:39","239","0","1","71456493","<p>The article you've linked to conveniently stops before describing how a &quot;star schema&quot; built in this way would actually be queried.</p>
<p>The most &quot;expensive&quot; part of any SQL query is the joins. Imagine in your model if the Customer Dim has been split into 3 mini-Dims and you need to pick attributes in your query and they exist across those 3 mini-Dims; your query will now have to perform 3 joins. Multiply that by lots of &quot;big&quot; Dims and the number of joins in any query will explode and is highly likely to kill the performance of your queries.</p>
<p>Seems like someone has come up with a way of making it easier to build a &quot;star schema&quot; when the starting point is a Datavault model but hasn't actually thought it through to the end.</p>
<p>I would suggest going back to your architecture team and getting them to demonstrate that query performance using their approach will be acceptable - using realistic data volumes and real-world queries</p>
"
"71397763","Python: Is there a better way to work with ragged arrays than a list of arrays with dtype = object?","<p>I am collecting time series data, which can be separated into &quot;tasks&quot; based on a particular target value. These tasks can be numbered based on the associated target. However, the lengths of data associated with each task will differ because it may take less time or more time for a &quot;task&quot; to be completed. Right now in MATLAB, this data is separated by the target number into a MATLAB cell, which is extremely convenient as the analysis on this time-series data will be the same for each set of data associated with each target, and thus I can complete data analysis simply by using a for loop to go through each cell in the cell array. My knowledge on the closest equivalent of this in Python would be to generate a ragged array. However, through my research on answering this question, I have found that automatic setting of a ragged array has been deprecated, and that if you want to generate a ragged array you must set dtype = object. I have a few questions surrounding this scenario:</p>
<ol>
<li><p>Does setting dtype=object for the ragged array come with any inherent limitations on how one will access the data within the array?</p>
</li>
<li><p>Is there a more convenient way of saving these ragged arrays as numpy files besides reducing dimensionality from 3D to 2D and also saving a file of the associated index? This would be fairly inconvenient I think as I have thousands of files for which it would be convenient to save as a ragged array.</p>
</li>
<li><p>Related to 2, is saving the data as a .npz file any different in practice in terms of saving an associated index? More specifically, would I be able to unpack the ragged arrays automatically based on a technically separate .npy file for each one and being able to assume that each set of data associated with each target is stored in the same way for every file?</p>
</li>
<li><p>Most importantly, is using ragged arrays really the best equivalent set-up for my task, or do I get the deprecation warning about setting dtype=object because manipulating data in this way has become redundant and Python3 has a better method for dealing with stacked arrays of varying size?</p>
</li>
</ol>
","<python-3.x><deprecated><data-management><ragged>","2022-03-08 15:46:20","108","0","1","71588373","<p>I have decided to move forward with a known solution to my problem, and it seems to be adapting well.
I organize each set of separate data into it's own array, and then store them in a sequence in a list as I would with cells in MATLAB.
To save this information, when I separated out the data I stored the subsequent index value in a list. By this I mean that:</p>
<ol>
<li>I identify the location of the next separate set of data.</li>
<li>I copy the data up until that index value into an array that is appended to a list.</li>
<li>I store the index value that was the start of the next separate set of data.</li>
<li>I delete that information from a copy of my original array.</li>
<li>I repeat steps 1-4 until there is only one uniquely labelled sequence of data left. I append this set of data. There is no other index to record. Therefore the list of indices is equal to the length of the list of arrays -1.</li>
<li>When saving data, I take the original array and save it in a .npz file with the unpacking indices.</li>
<li>When I want to use and reload the data into it's separate arrays for analysis, I can 'pack' and 'unpack' the array into it's two different forms, from single numpy array to list of numpy arrays.</li>
</ol>
<p>This solution is working quite well. I hope this helps someone in the future.</p>
"
"71394862","Pyspark how can identify unmatched row value from two data frame","<p>I have below two data frame from which i am trying to identify the unmatched row value from data frame two. This is the part of migration where i want to see the difference after source data being migrated/moved to different destination.</p>
<pre><code>source_df
+---+-----+-----+
|key|val11|val12|
+---+-----+-----+
|abc|  1.1|  1.2|
|def|  3.0|  3.4|
+---+-----+-----+

dest_df
+---+-----+-----+
|key|val11|val12|
+---+-----+-----+
|abc|  2.1|  2.2|
|def|  3.0|  3.4|
+---+-----+-----+
</code></pre>
<p>i want to see the output something like below</p>
<pre><code>key: abc,

col:          val11                  val12

difference:  [src-1.1,dst:2.1]       [src:1.2,dst:2.2]

</code></pre>
<p>Any solution for this?</p>
","<pyspark><apache-spark-sql><data-quality>","2022-03-08 12:13:07","387","1","1","71395462","<pre><code>source_df  = spark.createDataFrame(
  [
('abc','1.1','1.2'),
('def','3.0','3.4'),
  ], ['key','val11','val12']
)

dest_df  = spark.createDataFrame(
  [
('abc','2.1','2.2'),
('def','3.0','3.4'),
  ], ['key','val11','val12']
)

report = source_df\
    .join(dest_df, 'key', 'full')\
    .filter((source_df.val11 != dest_df.val11) | (source_df.val12 != dest_df.val12))\
    .withColumn('difference_val11', F.concat(F.lit('[src:'), source_df.val11, F.lit(',dst:'),dest_df.val11,F.lit(']')))\
    .withColumn('difference_val12', F.concat(F.lit('[src:'), source_df.val12, F.lit(',dst:'),dest_df.val12,F.lit(']')))\
    .select('key', 'difference_val11', 'difference_val12')

report.show()

+---+-----------------+-----------------+
|key| difference_val11| difference_val12|
+---+-----------------+-----------------+
|abc|[src:1.1,dst:2.1]|[src:1.1,dst:2.1]|
+---+-----------------+-----------------+
</code></pre>
<p>Or, if you want exactally in that format:</p>
<pre><code>for x in report.select('key', 'difference_val11', 'difference_val12').collect():
    print(&quot;key: &quot; + str(x[0]) + &quot;,\n\n&quot; +\
          &quot;col:          val11                 val12\n\n&quot; +\
         &quot;difference:   &quot; + str(x[1]) + &quot;     &quot; + str(x[2]))
</code></pre>
<p>Output:</p>
<pre><code>key: abc,

col:          val11                 val12

difference:   [src:1.1,dst:2.1]     [src:1.2,dst:2.2]
</code></pre>
"
"71321055","Is there any snowflake data warehouse testing documentation/guide/tutorial and frameworks?","<p>Is there any snowflake data warehouse testing documentation/guide/tutorial?
Can someone plz suggest the course where we can learn how to test snowflake data warehouse?</p>
","<unit-testing><testing><automated-tests><snowflake-cloud-data-platform>","2022-03-02 10:24:59","261","0","2","71321782","<p>Please try the Snowflake University courses:</p>
<p><a href=""https://community.snowflake.com/s/snowflake-university"" rel=""nofollow noreferrer"">https://community.snowflake.com/s/snowflake-university</a></p>
"
"71321055","Is there any snowflake data warehouse testing documentation/guide/tutorial and frameworks?","<p>Is there any snowflake data warehouse testing documentation/guide/tutorial?
Can someone plz suggest the course where we can learn how to test snowflake data warehouse?</p>
","<unit-testing><testing><automated-tests><snowflake-cloud-data-platform>","2022-03-02 10:24:59","261","0","2","71323046","<p>Good starting points for learning Snowflake are:</p>
<ul>
<li><a href=""https://learn.snowflake.com/"" rel=""nofollow noreferrer"">https://learn.snowflake.com/</a></li>
<li><a href=""https://docs.snowflake.com/en/"" rel=""nofollow noreferrer"">https://docs.snowflake.com/en/</a></li>
</ul>
<p>In case of testing - are you using any additional tools? Most often they come along with additional database/etl-testing capabilities.</p>
"
"71318648","How to implement dml error logging in Postgresql?","<p>In the context of datawarehousing, ETL process must have <a href=""https://www.kimballgroup.com/2009/10/six-key-decisions-for-etl-architectures/"" rel=""nofollow noreferrer"">a strategy for error handling</a>. About that, Oracle has a great <a href=""https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/INSERT.html#GUID-903F8043-0254-4EE9-ACC1-CB8AC0AF3423"" rel=""nofollow noreferrer"">dml error logging</a> feature that lets you insert/merge/update a million records without failing or rolling back when constraint violation occurs with one or more rows, which can be logged in a dedicated error table. After that you can investigate what is wrong with each row and correct the errors before repeating the insert/merge/update.</p>
<p>Is there any way to implement this feature in Postgresql ?</p>
","<postgresql><logging><data-warehouse><bulkinsert>","2022-03-02 07:06:39","382","0","1","71376219","<p>Since there is nothing built in, nor any useful extension exists, I searched for a solution based on a pgsql procedure and eventually found it. It works well in my use case, where some csv files must be loaded once  a month into a staging db using foreign tables.
In the following test some records are inserted into the destination tables while other records that break an integrity constraint are inserted in an error table along with the error info.</p>
<pre><code>test=# create table t1(c1 int primary key);
create table t2( f1 int ,f2 int, f3 numeric);
insert into t1 values(2),(11),(5),(12);
insert into t2 values(100,2,234),(57,11,25),(5,5,1231),(2,2,173),(2,12,240),(11,22,101),(3,12,99);
create table t3 as select * from t2 where 1+1=11;
alter table t3 add constraint t3_pk primary key(f1),add foreign key (f2) references t1(c1),add constraint f3_ck check(f3&gt;100);
create table t3$err(f1 int,f2 int,f3 numeric,  error_code varchar, error_message varchar, constraint_name varchar);

test=# do
$$
    declare
        rec           Record;
        v_err_code    text;
        v_err_message text;
        v_constraint  text;
    begin
        for rec in
            select f1,
                   f2,
                   f3
            from t2 --in my use case this is the foreign table reading a csv file
            loop
                begin
                    insert
                    into t3
                    values (rec.f1,
                            rec.f2,
                            rec.f3);
                exception
                    when others then
                        get stacked diagnostics
                            v_err_code= returned_sqlstate,
                            v_err_message= MESSAGE_TEXT,
                            v_constraint= CONSTRAINT_NAME;
                        if left(v_err_code, 2) = '23' then --exception Class 23 — Integrity Constraint Violation
                            insert
                            into t3$err
                            values (rec.f1,
                                    rec.f2,
                                    rec.f3,
                                    v_err_code,
                                    v_err_message,
                                    v_constraint);
                            raise notice 'record % inserted in error table',rec;
                        end if;
                end;
            end loop;
    exception
        when others then --outer exceptions different from constraint violations
            get stacked diagnostics
                v_err_code= returned_sqlstate;
            raise notice 'sqlstate: %', v_err_code;
    end;
$$;
NOTICE:  record (57,11,25) inserted in error table
NOTICE:  record (2,12,240) inserted in error table
NOTICE:  record (11,22,101) inserted in error table
NOTICE:  record (3,12,99) inserted in error table

test=# select * from t3;
 f1  | f2 |  f3  
-----+----+------
 100 |  2 |  234
   5 |  5 | 1231
   2 |  2 |  173
(3 rows)

test=# select * from t3$err;
 f1 | f2 | f3  | error_code |                                error_message                                | constraint_name 
----+----+-----+------------+-----------------------------------------------------------------------------+-----------------
 57 | 11 |  25 | 23514      | new row for relation &quot;t3&quot; violates check constraint &quot;f3_ck&quot;                 | f3_ck
  2 | 12 | 240 | 23505      | duplicate key value violates unique constraint &quot;t3_pk&quot;                      | t3_pk
 11 | 22 | 101 | 23503      | insert or update on table &quot;t3&quot; violates foreign key constraint &quot;t3_f2_fkey&quot; | t3_f2_fkey
  3 | 12 |  99 | 23514      | new row for relation &quot;t3&quot; violates check constraint &quot;f3_ck&quot;                 | f3_ck
(4 rows)

</code></pre>
<p>All the magics is done within the nested BEGIN..END, where each row passing the constraints is inserted in the target table or else inserted in the error table.</p>
<p>The above solution has many limitations, such as:</p>
<ul>
<li>the oracle feature mentioned in the question is fully integrated with SQL (except for the plsql preliminaries in order to create the error table), while here a pgsql procedure is needed,</li>
<li>iterating over all the records is not exactly the most efficient way for data loading in comparison with a <a href=""https://blog.karatos.in/a?ID=01050-222a3144-8567-4b71-a37c-7b8924455cef"" rel=""nofollow noreferrer"">bulk loading executed through a pgsql procedure</a>,</li>
<li>moreover, the loop is accompanied by a overhead due to <a href=""https://asktom.oracle.com/pls/apex/f?p=100:11:0::::p11_question_id:60122715103602"" rel=""nofollow noreferrer"">the context switch</a> between the procedural environment and the sql environment,</li>
<li>the error handling is not generic but must be addressing specific errors</li>
<li>when a record has more than one error, only the last one is inserted in the error table (there could be a solution for this point).</li>
</ul>
"
"71316878","flutter user data management","<p>i'm developing an app with user system, my user data stores location information and with this information im showing data to user also i have a profile page.</p>
<p>i'm struggling figure out the fundamental of model system. The thing that i want to do is login to an account and retrieve all data and keep the data in the system. when i logged in store all my data somewhere and use that data from all pages.</p>
<p>im doing this task by retrieve data on every page is loaded.(when homepage is loaded, retrieve user data, when profile page is loaded retrieve user data. i dont think this is the right or efficient way.</p>
<p><strong>LONG STORY SHORT</strong></p>
<p>how to retrieve user data when logged in and use that same data on all of my pages(im retrieving data on every page)</p>
","<flutter><dart><google-cloud-firestore><model>","2022-03-02 02:48:23","327","0","1","71317451","<p>if you use Cloud Firestore then use</p>
<pre><code>final FirebaseAuth _auth = FirebaseAuth.instance;
getCurrentUser() async {
    final FirebaseUser user = await _auth.currentUser();
    final uid = user.uid;
    
   final uemail = user.email;
    print(uid);
   print(uemail);
  }
</code></pre>
"
"71305515","Using apache atlas as data catalog for snowflake","<p>I have snowflake as a data warehouse and want to use Apache atlas as a data catalog and lineage tool.
I went through the details but not sure it can be used with snowflake.
Is it possible to connect with a snowflake?</p>
","<snowflake-schema><apache-atlas>","2022-03-01 08:18:50","403","0","1","73309813","<p><em>Disclaimer: I work for Snowflake.</em></p>
<p>It is not possible, at this time, to use Atlas as the catalog. You can, however, use a <a href=""https://docs.snowflake.com/en/user-guide/tables-external-hive.html"" rel=""nofollow noreferrer"">Hive Metastore</a> with Snowflake. I have not tried to use things like the <a href=""https://atlas.apache.org/1.2.0/Hook-Hive.html"" rel=""nofollow noreferrer"">Atlas Hook</a> but perhaps that might be an option.</p>
"
"71257692","Scheduling a Cognos report based on Dates from a data warehouse table","<p>We have a report already written for Student Services, but we need to schedule it for specific times in the term; these times are from the date table in our data warehouse.  For example, we need it on the first day of the term (one of the MANY dates defined in our date table), and two weeks prior to the first day of the term.  If the current date is either one of these dates, we need the report to run; otherwise no.  Should I use trigger-based Cognos reporting?  Is there a way to do it in regular Cognos scheduling?  Should I schedule it out of an external (Oracle) stored procedure?</p>
","<cognos-11>","2022-02-24 20:10:43","239","0","1","71888359","<p>We were able to set up Event Studio to first have a daily check to see if it is 14 days before term (had to add that to the date table), and 2 weeks after start of term (also in our date table).  Set up the run condition, set up tasks for the reports required, then set up the email.  Could not set up Run Agent in Event studio (IBM was singularly unhelpful here) so we scheduled it in Cognos.  It runs like a charm.</p>
"
"71247805","Perform data mapping in GCP","<p>I have data coming from multiple hotels. These hotels are not using the same naming convention for storing the order information. I have a predefined dataset created in the bigquery(called hotel_order). I
want to map the data coming from different hotels to the single dataset in GCP, so it is easier for me to do comparisons in the bigquery.</p>
<p>If the column name(from hotel1) matches the bigquery dataset columnname, then the bigquery should load the data in the column, if the columnnames (from hotel orders data and dataset in bigquery) don't match, then column in the bigquery should have the null value. How do I do implement this in GCP? Problem of mapping in the GCP?</p>
","<google-cloud-platform><google-bigquery>","2022-02-24 06:07:39","79","0","1","71249854","<p>If you want to join tables together, and show a null value when a match doesn't exist, then you can do so using 'left join'.</p>
<p><strong>Rough example</strong>
<code>from hotel.orders as main left join hotel_number_one as Hotel_One on main.order_information = Hotel_One.order_information</code></p>
<p>Difficult to give a more detailed answer without more details or a working example using dbfiddle.</p>
"
"71245758","How to Parse column that have JSON Data using R","<p>The figure below show is the csv file that contain dataset that have 7 variable, this dataset contain jason data at column number 3 which is <em>order_list</em> variable</p>
<p><a href=""https://i.stack.imgur.com/xC78x.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xC78x.png"" alt=""enter image description here"" /></a></p>
<p>My question is how to parse the jarson data in column 3 which is <em>order_list</em> variable of this dataset.</p>
<p>what i already do :</p>
<pre><code>library(jsonlite)

df2 &lt;- purrr::map(df$order_list, jsonlite::fromJSON)

str(df2)
</code></pre>
<p><a href=""https://i.stack.imgur.com/UacA2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UacA2.png"" alt=""enter image description here"" /></a></p>
<p>the column 3 still remain as jason data.</p>
","<r><dataframe><data-structures><jsonlite><data-management>","2022-02-24 00:45:28","92","0","1","71245890","<p>As you saw, <code>fromJSON</code> returns a list. If you want it as a data.frame or tibble you can pipe its output into <code>as.data.frame()</code> or <code>as_tibble()</code>, from the <code>tibble</code> package.</p>
<pre><code>jsonlite::fromJSON(&quot;https://us.api.iheart.com/api/v3/live-meta/stream/2501/currentTrackMeta?defaultMetadata=true&quot;)
</code></pre>
"
"71226791","A table in my model records building valuations over time. Is it a slowly-changing dimension table or a fact fable?","<p>I'm building a data model for a report that allows users to analyze building valuations over time, and details about buildings and their current leases.</p>
<p>I have a fact table that contains leasing information, and a dimension table that contains building information.</p>
<p>I have a third table that measures building valuations recorded every other quarter. I want to know whether it should be considered a slowly-changing dimension table or as another fact table.</p>
<p>The structure of the tables is as follows.</p>
<p><strong>Fact_Leases</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Lease ID</th>
<th>Building ID</th>
<th>Floor</th>
<th>Customer</th>
<th>Start Date</th>
<th>End Date</th>
<th>Occupied Area (sq ft)</th>
<th>Yearly Rent</th>
</tr>
</thead>
<tbody>
<tr>
<td>101</td>
<td>1</td>
<td>1</td>
<td>Customer X</td>
<td>1/1/2000</td>
<td>1/1/2020</td>
<td>60</td>
<td>$10</td>
</tr>
<tr>
<td>102</td>
<td>1</td>
<td>2</td>
<td>Customer X</td>
<td>1/1/2010</td>
<td>31/12/2030</td>
<td>40</td>
<td>$25</td>
</tr>
<tr>
<td>103</td>
<td>2</td>
<td>3</td>
<td>Customer X</td>
<td>6/1/2015</td>
<td>5/8/2032</td>
<td>15</td>
<td>$17</td>
</tr>
<tr>
<td>104</td>
<td>2</td>
<td>1</td>
<td>Customer Y</td>
<td>5/6/2016</td>
<td>6/9/2028</td>
<td>5</td>
<td>$12</td>
</tr>
<tr>
<td>105</td>
<td>3</td>
<td>1</td>
<td>Customer Z</td>
<td>4/3/2017</td>
<td>12/2/2020</td>
<td>50</td>
<td>$19</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Dim_Buildings</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Building ID</th>
<th>Building Name</th>
<th>Sq Ft</th>
<th>Units</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Building 1</td>
<td>100</td>
<td>10</td>
</tr>
<tr>
<td>2</td>
<td>Building 2</td>
<td>150</td>
<td>20</td>
</tr>
<tr>
<td>3</td>
<td>Building 3</td>
<td>125</td>
<td>50</td>
</tr>
</tbody>
</table>
</div>
<p><strong>?_Valuations</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Building ID</th>
<th>Quarter</th>
<th>Valuation</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Q2</td>
<td>$50</td>
</tr>
<tr>
<td>1</td>
<td>Q4</td>
<td>$55</td>
</tr>
<tr>
<td>2</td>
<td>Q2</td>
<td>$40</td>
</tr>
<tr>
<td>2</td>
<td>Q4</td>
<td>$35</td>
</tr>
<tr>
<td>3</td>
<td>Q2</td>
<td>$32</td>
</tr>
<tr>
<td>3</td>
<td>Q4</td>
<td>$44</td>
</tr>
</tbody>
</table>
</div>
<p>At first, I thought the Valuations table was a dimension table because it relates to information about the building dimension. I considered joining the valuation data to the building dimension table but this would result in needlessly repeated rows, so I left it as a separate table.</p>
<p>However, the valuation table will not be used to filter the leases table, and the valuation column would be considered a measurement, which makes me think it is actually another fact table.</p>
<p>Can anybody clear this up for me?</p>
","<powerbi><data-modeling><data-warehouse><dimensional-modeling><star-schema>","2022-02-22 18:47:51","29","1","1","71230729","<p>Short answer: Yes. It is a fact table.</p>
<p>You have two fact tables that differ only in terms of <em>granularity</em>. Your <strong>Fact_Leases</strong> table, for example, is a fact table at the granularity of a <em>lease</em>. I can assume this quite safely because it appears the Lease ID column is a primary key. Each row of that table represents a lease.</p>
<p>On the other hand, your <strong>?_Valuations</strong> table is a fact table at the granularity of <em>quarter-time-building</em>. That is, each row not only represents a building but also a quarter time period. And one way you can sort of know that this is a fact table is by understanding that if you had a date-dimension table, you could relate the two on their Quarter columns (although it would be a many-to-many relationship). Therefore, your date-DIMENSION table would be explaining the facts of your valuations. (I'd recommend, however, replacing your Quarter column with actual dates, and allow the date-dimension table to inform the quarters. That's an aside, though.)</p>
<p>Now, the problem of repeating valuation metrics occurs because you are trying to combine two fact tables at different levels of granularity. When you try to apply the valuations to the Fact_Leases table, which is at the granularity of lease, Power BI (or any BI tool, for that matter) can't understand how to apportion the valuation at the BUILDING level down to the LEASE level of granularity. So it just repeats. And it's important to keep this in mind when developing your reporting. No visualizations built at the context level of lease will be able to include a valuation metric because valuations exist only at a higher level of granularity.</p>
"
"71217036","Mapping Synapse data flow with parameterized dynamic source need importing projection dynamically","<p><a href=""https://i.stack.imgur.com/CFvwV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CFvwV.png"" alt=""enter image description here"" /></a></p>
<p>I am trying to build a cloud data warehouse where I have staged the on-prem tables as parquet files in data lake.</p>
<p>I implemented the metadata driven incremental load.</p>
<p>In the above data flow I am trying to implement merge query passing the table name as parameter so that the data flow dynamically locate respective parquet files for full data and incremental data and then go through some ETL steps to implement merge query.</p>
<p>The merge query is working fine. But I found that projection is not correct. As the source files are dynamic, I also want to &quot;import projection&quot; dynamically during the runtime. So that the same data flow can be used to implement merge query for any table.</p>
<p>In the picture, you see it is showing 104 columns (which is a static projection that it imported at the development time). Actually for this table it should be 38 columns.</p>
<p>Can I dynamically (i.e run-time) assign the projection? If so how?
Or anyone has any suggestion regarding this?
Thanking</p>
<p>Muntasir Joarder</p>
","<azure-data-factory><data-warehouse><azure-synapse>","2022-02-22 06:53:05","357","1","1","71252417","<p>Enable Schema drift in your source transformation when the metadata is often changed. This removes or adds columns in the run time.</p>
<p>The source projection displays what has been imported at the run time but it changes based on the source schema at run time.</p>
<p>Refer to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-schema-drift#main"" rel=""nofollow noreferrer"">document</a> for more details with examples.</p>
"
"71203005","Data Quality check with Python Dask","<p>Currently trying to write code to check for data quality of a 7 gb data file. I tried googling exactly but to no avail. Initially, the purpose of the code is to check how many are nulls/NaNs and later on to join it with another datafile and compare the quality between each. We are expecting the second is the more reliable but I would like to later on automate the whole process. I was wondering if there is someone here willing to share their data quality python code using Dask. Thank you</p>
","<python><dask><data-quality>","2022-02-21 08:08:19","107","0","1","71207327","<p>I would suggest the following approach:</p>
<ul>
<li>try to define how you would check quality on small dataset and implement it in Pandas</li>
<li>try to generalize the process in a way that if each &quot;part of file&quot; or partition is of good quality, than whole dataset can be considered of good quality.</li>
<li>use Dask's map_partitions to parralelize this processing over your dataset's partition.</li>
</ul>
"
"71193490","How to set seed value and incremental value for identity column in Azure Synapse?","<p>I have a table with DDL as below in the Azure Synapse Data Warehouse:</p>
<pre><code>CREATE TABLE [test].[test_history]
(
    [id] [int] IDENTITY(1,1) NOT NULL,
    [test1] [varchar](500) NOT NULL,
    [test2] [varchar](100) NULL
)
WITH
(
    DISTRIBUTION = HASH ( [test1] ),
    CLUSTERED COLUMNSTORE INDEX
)
GO
</code></pre>
<p>I tried to append values into this table 3 times with the sql below, the id column has a seed value of 45 and incremental value of 60 inspite of the IDENTITY(1,1).</p>
<pre><code>DECLARE @now DATETIME2 = GETDATE()

INSERT INTO test.test_history
(test1, test2)
VALUES ('test', '2022-02-18-03')

SELECT * FROM test.test_history
</code></pre>
<p>Also, I check the seed value as 1 and incremental value as 1 with the SQL below. However, the table does not provide the id value as expected</p>
<p>How can I fix this issue ?</p>
<p>Kind regards,</p>
","<sql><data-warehouse><azure-synapse>","2022-02-20 10:29:13","800","0","1","71195235","<p><code>IDENTITY</code> columns in Azure Synapse Analytics dedicated SQL pools do guarantee unique values but <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-identity?#allocation-of-values"" rel=""nofollow noreferrer"">do not guarantee sequential values</a>. The reason is because data is split across 60 distributions; each distribution has a unique set of identity values.</p>
<p>If it's important for you to have a sequential column then recreate the table without the <code>IDENTITY</code> property and change your <code>INSERT</code> statement to the following code which will produce a sequential ID:</p>
<pre><code>
DECLARE @now DATETIME2 = GETDATE()

INSERT INTO trans_customer_cdm_ejkb.cdm_file_process_history
(id, layer, ingest_partition, [status], last_update_time, pipeline_run_id)
SELECT
(SELECT ISNULL(MAX(id),0) FROM trans_customer_cdm_ejkb.cdm_file_process_history) +1 as id, 
 'src2stg', '2022-02-18-03', 'success', @now, 'Test'

SELECT * FROM trans_customer_cdm_ejkb.cdm_file_process_history
</code></pre>
<p>Since your code was just inserting a single row I did <code>+1</code> but normally you would do <code> + ROW_NUMBER() OVER (ORDER BY [SomeColumn])</code>.</p>
"
"71162934","Full Text Search Find Exact Word","<p>I use full text search on my SynonymWords table. I need to find exact word.
I put my search word between quotes but it still to get result.
I expected there is no result. Where i do a mistake?</p>
<p>My query is</p>
<pre><code>SELECT * FROM SynonymWords WHERE CONTAINS(Words,'&quot;zebra&quot;')
</code></pre>
<p>The result is here</p>
<p><a href=""https://i.stack.imgur.com/i9iEU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i9iEU.png"" alt=""Full Text Search Result"" /></a></p>
","<sql><sql-server><database><data-mining><data-warehouse>","2022-02-17 17:39:13","543","0","1","71164684","<p>Fulltext index doesn't work that way</p>
<p>it would find Zebra as long as there is a space or an interpunct before and behind it, so that ozt can idenitfy it a a word.</p>
<p>That's the function of the full text search and you can't change it.</p>
<p>A word like zebrajsduerhn would not be found by your query</p>
<p>You would change it to</p>
<pre><code>SELECT * FROM SynonymWords WHERE CONTAINS( [words], '&quot;zebra*&quot;') 
</code></pre>
<p>It taes some time to get used to it</p>
<p>Iyou want to have an exact search for word i would recoment to read this thread <a href=""https://stackoverflow.com/questions/50949736/sql-server-query-to-match-the-exact-word"">SQL Server query to match the exact word</a> which handles this and also have an answer for regular expressions.</p>
<p>If you need something faster you should take a look at elasticsearch and opensearch, but these needs also some time to undertsand the concepts</p>
"
"71146509","Three-table join with NO EXISTS","<p>I have three tables and would like to answer the following question in SQL:</p>
<p><strong>&quot;Who has only certifications that do NOT have scores?&quot;</strong></p>
<p>For instance, in the setup below, the query would return &quot;John&quot; only.
Joana has the &quot;AWS Certification&quot;, which is in SCORE table (id 57).
Marry has the &quot;ITIL V3 Certification&quot; which is not in the SCORE table, but she also has &quot;Professional Python Dev&quot;, which is in thee SCORE table.</p>
<pre><code>PERSON
| PERSON_ID | PERSON_NAME |
| --------  | -------------- |
| 12    | John            |
| 23   | Mary            |
| 24   | Joana            |


**CERTIFICATION**
|CERTIFICATION_ID| CERTIFICATION_NAME | PERSON_ID|
| -------- |----------- | -------------- |
| 53 | Java Certification    |    12       |
| 54 | ITIL V3 Certification    |    23       |
| 55 | Professional Python Dev            | 23|
| 56 |GCP Certification            |23   | 
| 57 |AWS Certification            |24   | 

SCORES

|SCORE_ID| CERFITICATION_ID | SCORE_DETAILS |
| -------- |----------- | -------------- |
| 70 |55   | 80%            |
| 71 |56   | 90%            |
| 72 |57   | 95%           |
</code></pre>
<p>I am trying to implement this in SQL only without having to iterate over the records and without using stored procedures.</p>
<p>SQL for creating these tables and adding the data in case anyone needs it:</p>
<pre><code>create table person(
person_id integer,
person_name varchar(16)
);

create table certification(
CERTIFICATION_ID int,
  CERTIFICATION_NAME varchar(16),
  person_id int
);

create table scores(
  SCORE_ID int,
  CERTIFICATION_ID int,
 SCORE_DETAILS varchar(16));


insert into person(person_id, person_name) values(12, 'John');
insert into person(person_id, person_name) values(23, 'Mary');
insert into person(person_id, person_name) values(24, 'Joana');


insert into certification(CERTIFICATION_ID,CERTIFICATION_NAME,person_id) values(53,'A', 12);
insert into certification(CERTIFICATION_ID,CERTIFICATION_NAME,person_id) values(54,'B',23);
insert into certification(CERTIFICATION_ID,CERTIFICATION_NAME,person_id) values(55,'C', 23);
insert into certification(CERTIFICATION_ID,CERTIFICATION_NAME,person_id) values(56,'D', 23);
insert into certification(CERTIFICATION_ID,CERTIFICATION_NAME,person_id) values(57,'E', 24);

insert into scores (SCORE_ID,CERTIFICATION_ID, SCORE_DETAILS) values (70,55,'e');
insert into scores (SCORE_ID,CERTIFICATION_ID, SCORE_DETAILS) values (71,56,'f');
insert into scores (SCORE_ID,CERTIFICATION_ID, SCORE_DETAILS) values (72,57,'g');
</code></pre>
","<sql><database><oracle><analytics><data-warehouse>","2022-02-16 17:25:15","35","0","2","71146671","<p>we can use not in like below</p>
<pre><code>select p.* from person p
 where p.person_id not in (
select c.person_id from 
certification c join scores s on c.CERTIFICATION_ID=s.CERTIFICATION_ID
)
</code></pre>
<p><a href=""https://dbfiddle.uk/?rdbms=oracle_21&amp;fiddle=ca2d45d8900570851135bad1dfd040f1"" rel=""nofollow noreferrer"">demo link</a></p>
"
"71146509","Three-table join with NO EXISTS","<p>I have three tables and would like to answer the following question in SQL:</p>
<p><strong>&quot;Who has only certifications that do NOT have scores?&quot;</strong></p>
<p>For instance, in the setup below, the query would return &quot;John&quot; only.
Joana has the &quot;AWS Certification&quot;, which is in SCORE table (id 57).
Marry has the &quot;ITIL V3 Certification&quot; which is not in the SCORE table, but she also has &quot;Professional Python Dev&quot;, which is in thee SCORE table.</p>
<pre><code>PERSON
| PERSON_ID | PERSON_NAME |
| --------  | -------------- |
| 12    | John            |
| 23   | Mary            |
| 24   | Joana            |


**CERTIFICATION**
|CERTIFICATION_ID| CERTIFICATION_NAME | PERSON_ID|
| -------- |----------- | -------------- |
| 53 | Java Certification    |    12       |
| 54 | ITIL V3 Certification    |    23       |
| 55 | Professional Python Dev            | 23|
| 56 |GCP Certification            |23   | 
| 57 |AWS Certification            |24   | 

SCORES

|SCORE_ID| CERFITICATION_ID | SCORE_DETAILS |
| -------- |----------- | -------------- |
| 70 |55   | 80%            |
| 71 |56   | 90%            |
| 72 |57   | 95%           |
</code></pre>
<p>I am trying to implement this in SQL only without having to iterate over the records and without using stored procedures.</p>
<p>SQL for creating these tables and adding the data in case anyone needs it:</p>
<pre><code>create table person(
person_id integer,
person_name varchar(16)
);

create table certification(
CERTIFICATION_ID int,
  CERTIFICATION_NAME varchar(16),
  person_id int
);

create table scores(
  SCORE_ID int,
  CERTIFICATION_ID int,
 SCORE_DETAILS varchar(16));


insert into person(person_id, person_name) values(12, 'John');
insert into person(person_id, person_name) values(23, 'Mary');
insert into person(person_id, person_name) values(24, 'Joana');


insert into certification(CERTIFICATION_ID,CERTIFICATION_NAME,person_id) values(53,'A', 12);
insert into certification(CERTIFICATION_ID,CERTIFICATION_NAME,person_id) values(54,'B',23);
insert into certification(CERTIFICATION_ID,CERTIFICATION_NAME,person_id) values(55,'C', 23);
insert into certification(CERTIFICATION_ID,CERTIFICATION_NAME,person_id) values(56,'D', 23);
insert into certification(CERTIFICATION_ID,CERTIFICATION_NAME,person_id) values(57,'E', 24);

insert into scores (SCORE_ID,CERTIFICATION_ID, SCORE_DETAILS) values (70,55,'e');
insert into scores (SCORE_ID,CERTIFICATION_ID, SCORE_DETAILS) values (71,56,'f');
insert into scores (SCORE_ID,CERTIFICATION_ID, SCORE_DETAILS) values (72,57,'g');
</code></pre>
","<sql><database><oracle><analytics><data-warehouse>","2022-02-16 17:25:15","35","0","2","71146773","<p>You can use following query</p>
<pre><code>select *
  from person p
 where not exists (select 1
                 from certification c
                 join scores s on s.certification_id = c.certification_id
                where p.person_id = c.person_id
              )
</code></pre>
"
"71102069","Bigquery nested table resulting out different results for similar queries","<p>I have Bigquery table with Record type <em>'trigger_prop'</em> column and here are two ways using which I am querying but it is resulting out different output:</p>
<p><strong>First Query:</strong></p>
<pre><code>SELECT b.* FROM dataset.table a, unnest(trigger_prop) as b
where b.number=425600;
</code></pre>
<p><strong>Second Query:</strong></p>
<pre><code>SELECT a.trigger_prop FROM dataset.table a, unnest(trigger_prop) as b
where b.number=425600;
</code></pre>
<p><em>'Query1 is resulting out 1 record'</em>, while <em>'query2 is resulting out 2 records'</em> from table which are actual records. These both queries are doing same thing but resulting out different output. First query is returning less record while second query is returning actual records. Can someone please help me in understanding the difference in both queries?</p>
","<sql><google-cloud-platform><google-bigquery><data-warehouse>","2022-02-13 15:22:39","237","-1","1","71102672","<p>They are not doing the same thing, the difference is the SELECT <strong>a</strong>.trigger_prop vs SELECT <strong>b</strong>.* which are different tables.<br />
The first query selects the unnested translation of <code>trigger_prop</code>from table b which &quot;unfolds&quot; the array and keeps the single array item with <code>number=425600</code>.<br />
The second query selects the raw <code>trigger_prop</code> field from the source table and returns a single row, <strong>but</strong> this row has multiple array items in it so it <em>looks</em> like multiple rows because of the array in <code>trigger_prop</code>.</p>
<p>To demonstrate I ran the following query, note the <code>LIMIT 1</code> clause:</p>
<pre><code>WITH
  TABLE AS (
  SELECT
    ARRAY&lt;STRUCT&lt;number INTEGER,
    name STRING&gt;&gt;[(1,
      &quot;me&quot;),
    (2,
      &quot;you&quot;)] AS trigger_prop)
SELECT
  a.trigger_prop
FROM
  TABLE AS a,
  UNNEST(trigger_prop) AS b
WHERE
  b.number=1
LIMIT 1
</code></pre>
<p>This gives the following result, note how it's a single row which <em>looks</em> like multiple rows.
<a href=""https://i.stack.imgur.com/PEysb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PEysb.png"" alt=""enter image description here"" /></a></p>
"
"71079798","How to compare two tables having record type column in BigQuery","<p>I have two nested tables one is source and another is target table. I wanted to compare nested columns of source and target table. I am comparing two tables to check weather data is being updated in source table or not. Is there any sql in BigQuery to achieve the same?</p>
<p><a href=""https://i.stack.imgur.com/0srqH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0srqH.png"" alt=""enter image description here"" /></a></p>
<p>Here are my approaches which I was previously doing to compare two tables with nested record:</p>
<p><strong>1. This is the first approach:</strong></p>
<pre><code>SELECT to_json_string(info) FROM database.nested_table_source
except distinct
SELECT to_json_string(info) FROM nested_table_target
</code></pre>
<p>to_json_string() is not working, since this function returns different sequence of source rows and target rows sometimes, even though data is same in both tables, it results out different records.</p>
<p><strong>2. This is the second approach:</strong></p>
<pre><code>select name
from dataset.nested_table_source a
join dataset.nested_table_target b
using(name)
where
a.name!=b.name  and
(select string_agg(format('%t', s) order by key) from a.info s) 
!= (select string_agg(format('%t', s) order by key ) from b.info s)
</code></pre>
<p>In this approach I am using string_agg function to compare two nested records. But I am not sure if that is the correct way to compare record fields.</p>
<p>What should I do in this case?</p>
","<sql><google-cloud-platform><google-bigquery><data-science><data-warehouse>","2022-02-11 12:03:54","890","2","1","71106428","<p>Here is an approach in which you basically stringify an <em>ordered</em> set of objects (or the <code>info</code> column in your tables), and then compare them against each other.</p>
<p>Here is an example with some dummy data:</p>
<pre><code>with source_data as (
    select 
    &quot;VICTOR&quot; as name,
     array[
        struct(&quot;A&quot; as key, 3 as value),       
        struct(&quot;B&quot; as key, 5 as value)   
    ] as info

    union all 
    
    select 
    &quot;MAX&quot; as name,
     array[
        struct(&quot;A&quot; as key, 0 as value),       
        struct(&quot;B&quot; as key, 1 as value)   
    ] as info

    union all 
    
    select 
    &quot;SAIF&quot; as name,
     array[
        struct(&quot;A&quot; as key, 0 as value),       
        struct(&quot;B&quot; as key, 1 as value)   
    ] as info
),
target_data as (
    select 
    &quot;VICTOR&quot; as name,
     array[
        struct(&quot;A&quot; as key, 3 as value),       
        struct(&quot;B&quot; as key, 15 as value)   
    ] as info

    union all 
    
    select 
    &quot;MAX&quot; as name,
     array[
        struct(&quot;A&quot; as key, 0 as value),       
        struct(&quot;B&quot; as key, 1 as value)   
    ] as info
)

select name, stringified_source_set as info from (
    select 
        s.name, 
        array_to_string(array(select concat(cast(x.key as string), '|', cast(x.value as string)) from unnest(t.info) as x order by x.key), '|') AS stringified_target_set,
        array_to_string(array(select concat(cast(x.key as string), '|', cast(x.value as string)) from unnest(s.info) as x order by x.key), '|') AS stringified_source_set
    from source_data s
    left join target_data t on t.name = s.name 
)
where (stringified_source_set != stringified_target_set) or (name is null)

</code></pre>
<p>Note that the above approach does achieve both &quot;horizontal comparison&quot; (i.e. comparing both <code>info</code> objects) and &quot;vertical comparison&quot; (i.e. comparing missing entries in the target table which exist in the source table).</p>
"
"71044831","how to use --selector & --defer in getdbt. Please share some examples","<p>I am using getdbt on redshift for data analytics operation. Can anyone please suggest, how to use --selector &amp; --defer with &quot;dbt run&quot; commands.
What is the syntax ? What is the use of selectors.yml file?
Please share some examples.</p>
<p>Thanks</p>
","<analytics><data-warehouse><dbt>","2022-02-09 06:17:26","887","0","1","71115061","<p>My interpretation of <code>defer</code> is a way to utilize the dbt cli to work with unbuilt or differential versions of the current &amp; futures state defined versions of a model.</p>
<p>Example of why you may want to interact with that here: <a href=""https://github.com/dbt-labs/dbt-core/issues/2740"" rel=""nofollow noreferrer"">#2740 - Automating Non Regression Test</a></p>
<hr />
<p><code>selectors</code> being a relatively new feature, I also haven't seen much documentation to back this up but it is effectively a naming convention for a set of logical criteria (more than 1 tag, multiple directories, etc.)</p>
<p>I'd recommend this article in general for understanding the build path generation of a typical dbt run: <a href=""https://blog.getdbt.com/how-we-made-dbt-runs-30-faster/"" rel=""nofollow noreferrer"">How we made dbt runs 30% faster</a></p>
<p>From there, you can imagine that within a large project, there are huge interconnecting chains for each raw -&gt; analytics ready transformation pipeline that you have.</p>
<p>We'll use Gitlab's open <a href=""https://gitlab.com/gitlab-data/analytics/-/tree/master/transform/snowflake-dbt"" rel=""nofollow noreferrer"">dbt project</a> as an example.</p>
<p>Gitlab doesn't currently use selectors but they do make use of tags.
So they could build up a <code>selectors.yml</code> file using logical definitions like:</p>
<p><code>selectors.yml</code></p>
<pre><code>selectors:
  - name: sales_funnel
    definition: 
        tag: salesforce
        tag: sales_funnel
  - name: arr
    description: builds all arr models to current state + all upstream dependencies (zoho, zuora subscriptions, etc.)
    default: true
    definition: 
         tag: zuora_revenue
         tag: arr
  - name: month_end_process
    description: builds reporting models about customer segments based on subscription activity for latest closed month
    definition:
      - union:
          - method: fqn
            value: rpt_available_to_renew_month_end
            greedy: eager  # default: will include all tests that touch selected model
          - method: fqn
            value: rpt_possible_to_churn_month_end
            greedy: eager
</code></pre>
<p>Full list of valid selector definitions here: <a href=""https://docs.getdbt.com/reference/node-selection/yaml-selectors#default"" rel=""nofollow noreferrer"">https://docs.getdbt.com/reference/node-selection/yaml-selectors#default</a></p>
<p>What that gives them the ability to do is on a cron job, via airflow, or some other orchestrator simply execute:</p>
<p><code>dbt run --selector month_end_process --full-refresh</code></p>
<p>And have confidence that the logical selection of models to run for that process is 100% accurately reproduced instead of another more fallible  approach like assuming that all the models needed are in a single directory:</p>
<p><code>dbt run --models marts.finance.restricted_safe.reports --full-refresh</code></p>
<hr />
<p>Architecturally, you likely won't need selectors until you get to the level of having multiple layers of tags and / or multiple layers of use-case directories to be mindful of within a single run.</p>
<p>Example: tags for the models' function, tags for the sources, tags for the bi/analyst consumers, tags for the <a href=""https://www.getdbt.com/coalesce-2021/optimizing-query-run-time-with-materialization-schedules/"" rel=""nofollow noreferrer"">materialization schedule</a>, etc.</p>
"
"71043229","BI - How to design a data model to compare orders price vs cost in a star schema","<p>I'm struggling triying to make a star schema from a set of tables with different origins, two SQL databases, Excel files and CSV reports, it's a bit of a puzzle.</p>
<p>The initial tables that they provide me are set like this:</p>
<p><a href=""https://i.stack.imgur.com/1qyCR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1qyCR.png"" alt=""Initial tables"" /></a></p>
<p>The important points of this set of tables are:</p>
<ul>
<li><p>In Products table IdProduct is not unique, because one product can be make with one type of machine in factory A, and another type of machine in factory B, so it's one row for every Factory/Machine/IdProduct combination.</p>
</li>
<li><p>The OrderItems table has mixed rows with materials and products, so you have all the products in the Order and all the materials used in each product of the same Order.</p>
</li>
<li><p>The cost of the material changes daily and is updated in the system from where I get the OrderItems table.</p>
</li>
<li><p>The delivery cost is different for each order.</p>
</li>
<li><p>The packaging and fix costs are updated once a week.</p>
</li>
<li><p>The product price changes from order to order (it is set taking into account the client, day and size of the order).</p>
</li>
</ul>
<p>I got to this model dividing the OrderItems in products and costs (materials), and joining with them the fixed costs and the packaging costs, I haven't joined the delivery costs, but i end up with two fact tables and a snowflake schema:</p>
<p><a href=""https://i.stack.imgur.com/hO4Sa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hO4Sa.png"" alt=""Current model"" /></a></p>
<p>I am thinking in Region, Factory, Machine, Date, Product, and a compound of cost concepts (materials, fixed costs, etc.) as dimensions, and the total amounts and quantities as facts. This to compare the total sales to the total costs around the different dimensions.</p>
<p>I just wanna know if this is the correct path or there is a better way, I tried to search more about the subject but the case is too specific so I get nothing.</p>
<p>Thanks in advance for your answers.</p>
","<powerbi><powerquery><data-modeling><data-warehouse><business-intelligence>","2022-02-09 02:16:04","283","0","1","71184879","<p>Choosing a star schema over a snowflake schema ?
The Star schema is in a more de-normalized form and can be better for performance. Along the same records the Star schema uses less foreign keys so the query execution time is limited. In almost all cases the data retrieval speed of a Star schema has the Snowflake beat.</p>
<p>But you can split your work as data marts :</p>
<blockquote>
<p>A data mart is a simple form of data warehouse focused on a single
subject or line of business.</p>
</blockquote>
<p>A data mart can contain star schemas and other tables for more than one warehouse pack. For example, a single data mart might contain data for the your reporting needs related to costs</p>
"
"71041609","Relations between slowly changing dimensions in a data warehouse","<p>I’m designing a data warehouse and am struggling to work out how I should be modelling this scenario. I’ve got Users stored in a Slowly Changing Dimension Type 2 table along these lines:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>UserKey</th>
<th>UserID</th>
<th>Language</th>
<th>EffectiveDate</th>
<th>ExpiryDate</th>
<th>IsCurrent</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1001</td>
<td>EN</td>
<td>2021-01-01</td>
<td>9999-12-3</td>
<td>Y</td>
</tr>
<tr>
<td>2</td>
<td>1002</td>
<td>EN</td>
<td>2021-07-31</td>
<td>2022-01-06</td>
<td>N</td>
</tr>
<tr>
<td>3</td>
<td>1002</td>
<td>FR</td>
<td>2022-01-06</td>
<td>9999-12-31</td>
<td>Y</td>
</tr>
</tbody>
</table>
</div>
<p>And a Login fact table like:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>LoginKey</th>
<th>UserKey</th>
<th>LoginTime</th>
</tr>
</thead>
<tbody>
<tr>
<td>12345</td>
<td>2</td>
<td>2021-12-25 15:00</td>
</tr>
<tr>
<td>12399</td>
<td>3</td>
<td>2022-01-31 18:00</td>
</tr>
</tbody>
</table>
</div>
<p>Thereby allowing us to report on logins by date by user language setting at the time, etc.</p>
<p>Now I have to consider that each user may have one, none, or many concurrent subscriptions, which I was thinking of modelling in a Type 1 SCD thus:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>SubsKey</th>
<th>SubsID</th>
<th>SubsType</th>
<th>UserKey</th>
<th>StartDate</th>
<th>EndDate</th>
</tr>
</thead>
<tbody>
<tr>
<td>55501</td>
<td>SBP501</td>
<td>Premium</td>
<td>2</td>
<td>2021-08-01</td>
<td>2022-08-01</td>
</tr>
<tr>
<td>55502</td>
<td>SBB123</td>
<td>Bonus</td>
<td>3</td>
<td>2022-01-31</td>
<td>2023-01-31</td>
</tr>
</tbody>
</table>
</div>
<p>Is it right for one dimension table to reference the surrogate row key of another like this, or should it rather contain the UserID natural key? It seems unwieldy for the Subs table to have different UserKeys for the two concurrent Subscriptions for the same user like this. Or perhaps, when the third row was added to the Type 2 User table, should all the existing rows in Subs with UserKey=2 have been updated to UserKey=3?</p>
<p>The whole thing doesn't seem to fit comfortably into the classic snowflake pattern, which usually has the one-to-many relationship pointing the other way, as might be the case were Language to be a separate dimension table say, with a one-to-many relation on User.</p>
<h2>Edit</h2>
<p>I'm wrestling with not only in the one-to-many example described (one user has many subscriptions) but also many-to-one relations between SCDT2 tables e.g. If the user's language was stored in a SCDT2 table, should the User dimension use reference the Language ID or the LanguageKey for Language table's current row?</p>
","<data-warehouse>","2022-02-08 22:20:24","351","1","2","71042105","<p>A subscription is a fact and so should be stored in a fact table - though you might also have a subscription dimension that holds attributes of a subscription such as its name.
You relate dimensions through fact tables, so your subscription fact would have FKs to Subscription, User, Date etc dimensions.</p>
<p>Relating dimensions directly to each other is called snowflaking and is, generally a bad design.</p>
<p>BTW for an SCD2 table, having the expiry date of one row the same as the effective date of the next row is not a good design. In your example, you would need business logic to define which row was active on 2022-01-06, whereas if a row expires on 2022-01-06 and the next row starts on 2022-01-07 there can be no confusion.</p>
"
"71041609","Relations between slowly changing dimensions in a data warehouse","<p>I’m designing a data warehouse and am struggling to work out how I should be modelling this scenario. I’ve got Users stored in a Slowly Changing Dimension Type 2 table along these lines:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>UserKey</th>
<th>UserID</th>
<th>Language</th>
<th>EffectiveDate</th>
<th>ExpiryDate</th>
<th>IsCurrent</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1001</td>
<td>EN</td>
<td>2021-01-01</td>
<td>9999-12-3</td>
<td>Y</td>
</tr>
<tr>
<td>2</td>
<td>1002</td>
<td>EN</td>
<td>2021-07-31</td>
<td>2022-01-06</td>
<td>N</td>
</tr>
<tr>
<td>3</td>
<td>1002</td>
<td>FR</td>
<td>2022-01-06</td>
<td>9999-12-31</td>
<td>Y</td>
</tr>
</tbody>
</table>
</div>
<p>And a Login fact table like:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>LoginKey</th>
<th>UserKey</th>
<th>LoginTime</th>
</tr>
</thead>
<tbody>
<tr>
<td>12345</td>
<td>2</td>
<td>2021-12-25 15:00</td>
</tr>
<tr>
<td>12399</td>
<td>3</td>
<td>2022-01-31 18:00</td>
</tr>
</tbody>
</table>
</div>
<p>Thereby allowing us to report on logins by date by user language setting at the time, etc.</p>
<p>Now I have to consider that each user may have one, none, or many concurrent subscriptions, which I was thinking of modelling in a Type 1 SCD thus:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>SubsKey</th>
<th>SubsID</th>
<th>SubsType</th>
<th>UserKey</th>
<th>StartDate</th>
<th>EndDate</th>
</tr>
</thead>
<tbody>
<tr>
<td>55501</td>
<td>SBP501</td>
<td>Premium</td>
<td>2</td>
<td>2021-08-01</td>
<td>2022-08-01</td>
</tr>
<tr>
<td>55502</td>
<td>SBB123</td>
<td>Bonus</td>
<td>3</td>
<td>2022-01-31</td>
<td>2023-01-31</td>
</tr>
</tbody>
</table>
</div>
<p>Is it right for one dimension table to reference the surrogate row key of another like this, or should it rather contain the UserID natural key? It seems unwieldy for the Subs table to have different UserKeys for the two concurrent Subscriptions for the same user like this. Or perhaps, when the third row was added to the Type 2 User table, should all the existing rows in Subs with UserKey=2 have been updated to UserKey=3?</p>
<p>The whole thing doesn't seem to fit comfortably into the classic snowflake pattern, which usually has the one-to-many relationship pointing the other way, as might be the case were Language to be a separate dimension table say, with a one-to-many relation on User.</p>
<h2>Edit</h2>
<p>I'm wrestling with not only in the one-to-many example described (one user has many subscriptions) but also many-to-one relations between SCDT2 tables e.g. If the user's language was stored in a SCDT2 table, should the User dimension use reference the Language ID or the LanguageKey for Language table's current row?</p>
","<data-warehouse>","2022-02-08 22:20:24","351","1","2","71404201","<p>Based on your examples, the last table looks like more close to SLCD Type 4 than Type 1.</p>
<p>Indeed, I agree that subscriptions might be a Fact table and have a Dimension table.
Perhaps, an SLCD Type 2 can be the best option for the subscriptions dimension table but adding a flag column to set the current/active subscription with his associated effective date.</p>
"
"71024379","where to put dimensions in Kimball data warehouse","<p>I am creating a data warehouse following Kimball's theory.  So, I am aiming for the greatest amount of dimension table re-use.  In this theory, how should I physically organize the dimension tables?  I have used databases to organize my data marts (i.e., 1 mart per database, with possibly multiple fact tables per mart).  Since a given dimension can be used by multiple marts (and is what I want to aim for), I don't know where I should put my dimension tables.</p>
<p>For example, should I put them all under a certain schema in a certain database (e.g., schema 'Dimension' under database 'Dimensions')?  Or, perhaps, should I incrementally add them to each new database as I build out new data marts?</p>
","<data-warehouse>","2022-02-07 19:37:24","134","0","2","71024592","<p>A datamart is a logical subset of your data warehouse, not a physical one. Your data warehouse should (under most circumstances) reside in a single database</p>
"
"71024379","where to put dimensions in Kimball data warehouse","<p>I am creating a data warehouse following Kimball's theory.  So, I am aiming for the greatest amount of dimension table re-use.  In this theory, how should I physically organize the dimension tables?  I have used databases to organize my data marts (i.e., 1 mart per database, with possibly multiple fact tables per mart).  Since a given dimension can be used by multiple marts (and is what I want to aim for), I don't know where I should put my dimension tables.</p>
<p>For example, should I put them all under a certain schema in a certain database (e.g., schema 'Dimension' under database 'Dimensions')?  Or, perhaps, should I incrementally add them to each new database as I build out new data marts?</p>
","<data-warehouse>","2022-02-07 19:37:24","134","0","2","71035469","<p>Traditional data warehouses often use separate databases to create application boundaries based on either workload, domain or security.</p>
<p>As an example, a traditional SQL Server data warehouse might include a staging database, a data warehouse database, and some data mart databases. In this topology, each database operates as a workload and security boundary in the architecture.</p>
<p>You can create a schema for example for HR datamart and load all the related dimensions under it.</p>
<pre><code>CREATE SCHEMA [HR]; --   name for the data mart HR
CREATE TABLE [HR].[DimEmployee] -- create dimensions related to data mart HR in the HR schema
(       EmployeeSK BIGINT NOT NULL
,       ...
);
</code></pre>
"
"70986146","Unsure how to manage my variables/data using tkinter and openpyxl: fstring issue","<p>I am creating an app on a Raspberry Pi (if its at all relevent) which needs to access data from specific cells from a spreadsheet in order to display the settings as button text. There are multiple 'modes', settings and buttons. The button that may need to update varies and I have specified this with a Boolean variable. The text that needs to be displayed on the specified button can also vary by time and worksheet. I was able to create a variable to hold the time and then using a nested f-string was able to display data from the correct cell:</p>
<pre><code>current_time = datetime.now().strftime('%H')
cell_number = int(current_time)+2
bool = True

def update_button():
    if bool == True:
        button.config(text=f&quot;{ws_sheet[f'A{cell_number}'].value}&quot;)
    else:
        button1.config(text=f&quot;{ws_sheet[f'B{cell_number}'].value}&quot;)
</code></pre>
<p>This worked fine but I also need to alter the sheet now as well based on a different variable. I tried this but it produces an fstring syntax error.</p>
<pre><code>current_time = datetime.now().strftime('%H')
cell_number = int(current_time)+2
bool = True
worksheet = some_worksheet 

def update_button():
    if bool == True:
        button.config(text=f&quot;{{worksheet}[f'A{cell_number}'].value}&quot;)
    else:
        button1.config(text=f&quot;{{worksheet}[f'B{cell_number}'].value}&quot;)
</code></pre>
<p>The syntax error produced:</p>
<pre><code>button.config(text=f&quot;{{worksheet}[f'A{cell_number}'].value}&quot;)
                  ^
SyntaxError: f-string: single '}' is not allowed
</code></pre>
<p>This seems like a counter-intuitive error to me as well.</p>
<p>I am open to the fact I am probably going about this totally the wrong way and for my own ego I feel compelled to mention that I haven't been doing this very long. I know I could achieve what I want with a ton of if/else statements but I was hoping to do something a bit more elegant.</p>
<p>Any help is greatly appreciated.</p>
","<python><tkinter><openpyxl><f-string><data-management>","2022-02-04 12:08:28","66","0","1","70988271","<p>You can pass work sheet when calling your function.</p>
<pre><code>def update_button(ws_sheet):
    if bool == True:
        button.config(text=f&quot;{ws_sheet[f'A{cell_number}'].value}&quot;
    else:
        button1.config(text=f&quot;{ws_sheet[f'B{cell_number}'].value}&quot;
</code></pre>
"
"70984599","Relate an architecture to a Data Warehouse System Architecture","<p>Below you find from the company EC4U an architecture
for a a data warehouse system. (OBI stands for Oracle Business Intelligence).
And I want to relate this architecture to the data warehouse system reference architecture
(see the first image) Which parts of the EC4U architecture fulfill the task(s) of which
components of the reference architecture?</p>
<p>I tried this, below you can find my solution. Is it right? Is something missing?</p>
<p><strong>Data Warehouse System Architecture</strong></p>
<p><a href=""https://i.stack.imgur.com/QHKkP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QHKkP.png"" alt=""enter image description here"" /></a></p>
<p><strong>EC4U Architecture</strong>
<a href=""https://i.stack.imgur.com/w2Sym.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w2Sym.png"" alt=""enter image description here"" /></a></p>
<p><strong>Solution</strong></p>
<pre><code>Reference Architecture      |        EC4U Architecture
=========================================================================                 
Data Sources                |        OLTP Application Server (Operational Systems)
Data Warehouse              |        OLAP DWH 
Staging Area                |        Database Server
Transformation              |        ETL Server
DW Manager                  |        Information Server
Repository                  |        Config Files, RPD, Catalog, Log Files
Metadata Manager            |        BI Server

</code></pre>
<p><strong>Definitions</strong></p>
<p><em>Functional Components</em></p>
<ul>
<li>Data Warehouse Manager: Central component, starts, monitors and controls all
processes from extraction of data from the sources to analysis.</li>
<li>Monitors: detect and report to the data warehouse manager updates in the data
sources that are relevant for the DW. There is usually one Monitor per data source.</li>
<li>Extractors: select and transport data from the sources to the Staging Area</li>
<li>Transformators: Before loading into the Basic Database, data must be transformed
into a homogeneous internal format to be comparable =&gt; unify, integrate, consolidate,
aggregate, and supplement extracted data in the Staging Area.</li>
<li>Loading Components: Load acquired and transformed data from the Staging Area
to the Basic Database and from there finally into the DW</li>
<li>Analysis Components: Provide analysis and presentation of data in the DW.</li>
<li>Metadata Manager: A database application that controls the metadata management
of the DWS. It provides an interface for read and write access to the repository
where the metadata are stored.</li>
</ul>
<p><em>Data components</em></p>
<ul>
<li>Data sources: Operational systems with detailed data some or all of which are fed
to the DW</li>
<li>Staging Area: intermediate storage area where data is transformed and integrated
for the DW. Not necessarily a database system.</li>
<li>Basic Database: A database system with integrated data from which the DW is
updated periodically. Unlike in the DW, the data in the Basic Database can be changed
and deleted. It contains the data that has changed since the last loading of the
DW. Not every DWS has a Basic Database.</li>
<li>Repository: contains the metadata of the DW (comparable to a data dictionary).
Information from the respective data dictionaries of the source databases is also
imported into the Repository.</li>
<li>Data Warehouse (DW): a database with historical data designed in a model suited
to analytic needs.</li>
</ul>
","<architecture><data-warehouse>","2022-02-04 10:10:38","116","0","1","71005047","<p>Database server (RDBMS, OLTP, Excel,Files) is the Data Source.
ETL Server represents the gray area un the 1st diagram (Extraction, Transformation, Loading, Staging Arda, etc.)
DWH is the Data Warehouse.</p>
<p>Both diagram elements don't match with one-to-one correspondance.</p>
"
"70979578","Do I need to remove indexes on a table before a large load?","<p>I am working in a data warehouse, and the standard setup for loading a table is to insert rows into the table for the nightly load that only loads changes. Still, if a full history load is run, all the indexes on the dimension or fact table are dropped before the insert and then added back after. Does this make the load run faster? Is there another reason to drop all indexes and then rebuild when loading a large number of rows to a fact or dimension table?</p>
","<sql><sql-server><indexing><ssis><data-warehouse>","2022-02-03 23:12:05","1096","2","1","70991251","<p>Dropping and recreating indexes are done to increase the data insertion speed. This practice makes sense, especially when loading a huge amount of data. Since the more data volume increases, the total time of data insertion and index recreation becomes smaller than inserting data and updating indexes time.</p>
<p>You can learn more information in the following article:</p>
<ul>
<li><a href=""https://use-the-index-luke.com/sql/dml/insert"" rel=""nofollow noreferrer"">More Indexes, Slower Insert</a></li>
</ul>
"
"70967838","DD_INSERT in the update strategy transformation","<p>Informatica automatically inserts all the rows to the target...so why do we have to use update strategy transformation (DD_INSERT) to insert the records???</p>
","<data-warehouse><informatica><informatica-powercenter><data-management>","2022-02-03 08:16:14","753","1","3","70968147","<p>Good Question and you don't have to in case of insert only/update only case.</p>
<p>This assumption that <em>informatica automatically inserts all the rows to the target</em> is not true always. Yes, by default it inserts but there are many cases when we want to only update/only insert/delete/insert or update/reject the data.</p>
<p>Update strategy is used to control those scenarios.</p>
<p><code>DD_INSERT</code> - This option is only to insert data.<br />
<code>DD_UPDATE</code> - This updates the data based on key defined in target.<br />
<code>DD_REJECT</code> - This rejects the data.<br />
<code>DD_DELETE</code> - This deletes the data based on key defined in target.<br />
Session should be data driven.</p>
<p>So, in your case, you can either mention dd_insert or set the session properties to <code>insert only</code>.
if you want to insert new data but update old data, you need to use dd_insert or dd_update.<br />
if you want to insert new data but ignore old data, you need to use dd_insert or dd_reject.
if you want to only update old data, you need to use dd_update or set the session properties to <code>update only</code>..</p>
"
"70967838","DD_INSERT in the update strategy transformation","<p>Informatica automatically inserts all the rows to the target...so why do we have to use update strategy transformation (DD_INSERT) to insert the records???</p>
","<data-warehouse><informatica><informatica-powercenter><data-management>","2022-02-03 08:16:14","753","1","3","71013385","<p>Update strategy transformation is used when you want handy logic to check insert/update/delete records by comparing source key columns with target key columns. for each type of update strategy transformation, you can choose how you want to process your data. below link examples by Informatica will help you to understand this:</p>
<p><a href=""https://docs.informatica.com/data-integration/powercenter/10-5/transformation-language-reference/constants/dd_insert/examples.html"" rel=""nofollow noreferrer"">https://docs.informatica.com/data-integration/powercenter/10-5/transformation-language-reference/constants/dd_insert/examples.html</a></p>
<p>You can achieve the same by using a lookup and setting IUD Flag in expression transformation and then based on each flag you can route different types of records (ins, upd, del) into different target to update target in different ways.</p>
<p>Informatica is giving you read to use logic in the form of Update strategy transformation and dd_insert is one type of that.</p>
"
"70967838","DD_INSERT in the update strategy transformation","<p>Informatica automatically inserts all the rows to the target...so why do we have to use update strategy transformation (DD_INSERT) to insert the records???</p>
","<data-warehouse><informatica><informatica-powercenter><data-management>","2022-02-03 08:16:14","753","1","3","71024823","<p>While it is true that insert is the default behavior, it may be changed on session properties. See the <a href=""https://docs.informatica.com/data-integration/powercenter/10-4-0/workflow-basics-guide/sources/working-with-relational-sources/defining-the-treat-source-rows-as-property.html"" rel=""nofollow noreferrer"">Treat Source Rows As property documentation</a>.</p>
<p>Only when you want to build a logic and decide what to do with a particular row, you should use <code>Data Driven</code> property value and then use DD_INSERT and other values according to your needs.</p>
"
"70965915","how to decide if customer is a dimension","<p>I am creating a data warehouse (data mart) for a project-based (labor-centric) organization.  (That is, they sell labor-based &quot;projects&quot;; they don't sell physical products.)  They are interested in project- and customer-related dimension info.  I need to make a design decision about a certain dimension.  Should I make this dimension be &quot;Project&quot; (with customer info as attributes on this dimensions)?  Or, should I make two separate dimensions -- one for project and another for customer?  What are some questions to ask (or things to think about) to help me make this decision?</p>
","<data-warehouse>","2022-02-03 04:25:58","50","-1","1","71011100","<p>If the customer and the project represent axisis for analysis you can proceed with the following design :</p>
<p><a href=""https://i.stack.imgur.com/3IL9N.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3IL9N.png"" alt=""enter image description here"" /></a></p>
<p>The customer and the project can be Slowly Changing dimensions where you decide a type from the following :</p>
<pre><code>SCD Type    Summary
Type 1      Overwrite the changes
Type 2      History will be added as a new row.
</code></pre>
<p>The fact table can handle measures like the cost, the number of working days...</p>
"
"70958768","Service Token for External API | Databricks","<p>I am currently experimenting with Databricks. I'd like to leverage the <a href=""https://docs.databricks.com/dev-tools/python-sql-connector.html"" rel=""nofollow noreferrer"">Python SQL Connector</a> to allow external APIs to access certain tables through Databricks.</p>
<p>Using the Python SQL Connector requires the use of a <a href=""https://docs.databricks.com/dev-tools/api/latest/authentication.html#generate-a-personal-access-token"" rel=""nofollow noreferrer"">Personal Access Token</a> to authenticate with Databricks.</p>
<p>The issue here is I do not want the access token for a service to be tied to my personal identity, ideally I'd like the access token to be attached to a service identity. Databricks only allows me to create users with a first name, last name, and email. Is creating a service identity even possible to externally access tables?</p>
<p>Thanks.</p>
","<databricks><data-warehouse><delta-lake><aws-databricks><databricks-sql>","2022-02-02 15:58:15","389","1","1","70967828","<p>Yes, it's really possible.  You need to create a service principal (via <a href=""https://docs.databricks.com/dev-tools/api/latest/scim/scim-sp.html"" rel=""nofollow noreferrer"">REST API</a> or <a href=""https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/service_principal"" rel=""nofollow noreferrer"">Terraform provider</a>), add it to corresponding group that has access to data (via <a href=""https://docs.databricks.com/dev-tools/api/latest/scim/scim-sp.html#add-to-a-group"" rel=""nofollow noreferrer"">REST API</a>, UI, or <a href=""https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/group_member"" rel=""nofollow noreferrer"">Terraform provider</a>), give it permission to use personal access token (via <a href=""https://docs.databricks.com/dev-tools/api/latest/permissions.html#tag/Token-permissions"" rel=""nofollow noreferrer"">REST API</a> or <a href=""https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/permissions#token-usage"" rel=""nofollow noreferrer"">Terraform provider</a>), and then generate a token for it (via <a href=""https://docs.databricks.com/dev-tools/api/latest/token-management.html#operation/create-obo-token"" rel=""nofollow noreferrer"">REST API</a> or <a href=""https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/obo_token"" rel=""nofollow noreferrer"">Terraform provider</a>).</p>
"
"70950914","Sagemaker: How to debug Model monitoring(data quality and model quality)?","<p>I have created a Data Quality monitoring from Sagemaker Studio UI and also created using sagemaker SDK <a href=""https://stackoverflow.com/q/69179914/11844406"">code</a>, I referred to create model Data Quality monitoring job.</p>
<p><strong>Errors:</strong></p>
<ol>
<li>when there is no captured data (this is expected)</li>
</ol>
<blockquote>
<p>Monitoring job failure reason:</p>
<p>Job inputs had no data</p>
</blockquote>
<ol start=""2"">
<li>From logs, I can see that it is using <code>Java</code> in background. Not sure how to debug?</li>
</ol>
<blockquote>
<p>org.json4s.package$MappingException: Do not know how to convert
JObject(List(0,JDouble(38.0))) into class java.lang.String.</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/3da3i.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3da3i.png"" alt=""enter image description here"" /></a></p>
<p>Once we create the DataQuality monitoring job using Sagemaker Studio UI or Sagemkaer python sdk, it is taking a hour to start. I would like to know is there a way to debug monitoring job without waiting for a hour every time we get a error?</p>
","<python><amazon-web-services><amazon-sagemaker>","2022-02-02 05:34:02","190","0","1","71225027","<p>For development, it might be easier to trigger execution of the monitoring job manually. Take a look at this <a href=""https://github.com/aws-samples/amazon-sagemaker-mlops-workshop/blob/main/labs/05_model_monitor/monitoringjob_utils.py"" rel=""nofollow noreferrer"">python code</a></p>
<p>If you want to see how it's used, open the <a href=""https://github.com/aws-samples/amazon-sagemaker-mlops-workshop/blob/main/labs/05_model_monitor/05_model_monitor.ipynb"" rel=""nofollow noreferrer"">lab 5 notebook</a> of the workshop and scroll almost to the end, to the cells right after the &quot;Triggering execution manually&quot; title.</p>
"
"70939067","Test yaml great-expectations with Bigquery","<p>I am having troubles testing the yaml of great-expectation to bigquery.
I followed the official documentation and got to this code</p>
<pre><code>import os 
import great_expectations as ge 

datasource_yaml = &quot;&quot;&quot;
name: my_bigquery_datasource
class_name: Datasource
execution_engine:
  class_name: SqlAlchemyExecutionEngine
  connection_string: bigquery://&lt;GCP_PROJECT_NAME&gt;/&lt;BIGQUERY_DATASET&gt;
data_connectors:
  default_runtime_data_connector_name:
    class_name: RuntimeDataConnector
    batch_identifiers:
      - default_identifier_name
  default_inferred_data_connector_name:
    class_name: InferredAssetSqlDataConnector
    include_schema_name: true
&quot;&quot;&quot;
context = ge.get_context()

context.test_yaml_config(datasource_yaml)
</code></pre>
<p>The code works but it takes soo much time. I did deep debugging and see that the problem is that it wants to retrieve all the datasets of the project in bigquery and all the tables from all datasets. We have over 200 datasets and thousands of tables.
I haven't found a way to filter the only dataset that i need or more specifically the table. I thought the connection_string should do it but doesn't.</p>
<p>In my deep debugging, and got to the <code>inferred_asset_sql_data_connector.py</code> module. I saw that it should filter the schema_name problem is that always comes as None. And don't know how to pass it as the dataset I want.</p>
<p><a href=""https://i.stack.imgur.com/GIOwe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GIOwe.png"" alt=""enter image description here"" /></a></p>
<p>I followed this <a href=""https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/how_to_configure_a_dataconnector_to_introspect_and_partition_tables_in_sql/#configuring-introspection-and-tables"" rel=""nofollow noreferrer"">guide</a> as well of introspection but getting other errors.
<a href=""https://i.stack.imgur.com/DuuHW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DuuHW.png"" alt=""enter image description here"" /></a></p>
<p>If I put the SimpleSqlalchemyDatasource as class_name I get the following error. And I dont know how to initalize the engine for bq in sqlalchemy in the context of greatexpectations.</p>
<p><a href=""https://i.stack.imgur.com/NaE0M.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NaE0M.png"" alt=""enter image description here"" /></a></p>
","<python><google-bigquery><data-quality><great-expectations>","2022-02-01 10:33:54","519","2","1","72880705","<blockquote>
<p>default_inferred_data_connector_name</p>
</blockquote>
<p>tries to  fetch all dataset and table info from bigquery and it will create assets. You can remove the default_inferred_data_connector_name and use</p>
<blockquote>
<p>RuntimeBatchRequest</p>
</blockquote>
<p>and use query to validate the data.</p>
<p>Regarding authentication issue you can change the</p>
<blockquote>
<p>connection_string: bigquery://&lt;GCP_PROJECT_NAME&gt;/&lt;BIGQUERY_DATASET&gt;</p>
</blockquote>
<p>to</p>
<blockquote>
<p>connection_string: bigquery://&lt;GCP_PROJECT_NAME&gt;/&lt;BIGQUERY_DATASET&gt;?credentials_path=&lt;path_to_credential file &gt;</p>
</blockquote>
<p>More info on sql alchemy configuration can be found at <a href=""https://github.com/googleapis/python-bigquery-sqlalchemy"" rel=""nofollow noreferrer"">https://github.com/googleapis/python-bigquery-sqlalchemy</a></p>
"
"70938974","In a datawarehouse can a dimension be related to another dimension?","<p>I am currently developing a Data Warehouse and I was wondering if it makes sense to have a dimension connected to another via a foreign key.</p>
<p>For example, let's say we have two dimensions 'Country' and 'City' should we store only the City Dimension Key in the fact table. And the City is aware of it's Country.</p>
<p>Or does it make more sense two store both foreign keys in the fact table.</p>
<p>But the City dimension will have to know which Country it belongs to (it looks like it defies the star schema, since we now also have links between dimensions)</p>
<p>Or is this purely a design choice and will have no impact in querying, etc?</p>
","<sql><sql-server><powerbi><data-warehouse>","2022-02-01 10:28:01","1275","2","1","70941268","<p>Not a straight answerm but consider these two scenarios;</p>
<h2>A. You have one fact table at the grain of city</h2>
<ul>
<li>You may choose to have a star schema, which is
<ul>
<li>A single dimension containing city</li>
<li>This dimension contains a country column (which is repeated)</li>
</ul>
</li>
</ul>
<p><code>factTransactionA &gt;- dimCity</code></p>
<ul>
<li>Or you may choose to have a snowflake schema, which is
<ul>
<li>A city dimension table</li>
<li>a seperate country dimension table</li>
<li>these dimensions can be joined.</li>
</ul>
</li>
</ul>
<p><code>factTransactionA  &gt;- dimCity &gt;- dimCountry</code></p>
<p>Both are valid but consider....</p>
<h2>B. You have one fact table at the grain of city and another one at the grain of country</h2>
<p>When you're not sure of a design decision.... look for other constraints or requirements that help you make a decision</p>
<p>For case B you <em>have</em> to have a Country dimension. You shouldn't for example &quot;overload&quot; the city dimension and try to make it fit a fact table at the grain of Country. So you know you must have this:</p>
<p><code>factTransactionB &gt;- Country dimension table</code></p>
<p>So if I extend this explanation on the fly.... typically, you use &quot;conformed&quot; dimensions between fact tables, so when we consider both fact tables, I would actually suggest this type of schema:</p>
<p><code>factTransaction2 &gt;- dimCountry  -&lt; factTransaction1 &gt;- dimCity</code></p>
<p>Rather than this</p>
<p><code>factTransaction2 &gt;- dimCountry -&lt; dimCity -&lt; factTransaction1</code></p>
<p>This actually means baking the dimCountry surrogate key into <code>factTransaction1</code> which is actually at City level.</p>
<p>Because</p>
<ul>
<li>My gut feel tells me we should avoid two conformed dimensions between facts</li>
<li>If you have one fact at Country dimension, then Country is probably important enough in your business to bake into other facts to make it easy to compare metrics across facts.</li>
</ul>
<p>So I feel in this long winded explanation I have posed one reason to avoid snowflake schemas, but they are definitely valid</p>
"
"70912070","Why can't we use data warehouse directly instead of database?","<p>I am learning the Hadoop and came across the HIVE. And I learnt that HIVE acts a data warehouse for analytical queries.</p>
<p>If data warehouse is aggregation of multiple data sources, why do the companies use multiple data sources and aggregate them? Why can't they directly write to data warehouse as it would reduce the overhead of aggregation and processing?</p>
<p>Is it because the overhead that would occur if all the users read from same data warehouse?</p>
<p>I wanted to learn why do we need databases if we have data warehouses which are highly scalable?</p>
<p>(Only in context of big companies that generates huge volumes of data. Because I can understand that most of the companies can even manage with 2 databases.)</p>
","<database><hadoop><hive><sql-data-warehouse>","2022-01-30 05:24:55","125","0","1","70914948","<p>Data Warehouse (DWH) is a bigger layer on top of different databases, while the database is a namespace+storage for storing some tables/objects/procedures corresponding to some DWH stage.</p>
<p>The final purpose of analytical DWH is a provision of analytical data marts which can be used in different analytics/reporting tools: Tableu, QulickView, etc, etc</p>
<p>DWH contains data in different stages (databases) differently aggregated, for example LZ - a landing zone where the data loaded from different sources are stored, ODS - operational data store, where the data from different sources are combined into fact table and dimensions, cleared, transformed, usually 3NF is used and conformed dimensions which are being enriched from different sources.
And finally DM - a data mart where aggregated data are stored in dimensional model: fact tables(can be aggregated) and dimensions which are used in a star/snowflake schemas. Also some other databases can be used, like STG for intermediate data processing.</p>
<p>DWH is not only a storage consisted of multiple databases, it is also ETL processes for loading, extracting, transforming data on each stage, common architecture and policies.</p>
<p>You decide which layers the DWH will contain and how it should be designed: using top-down approach (starting from source systems) or bottom-up(starting from data marts dimensional modelling) or both.</p>
<p>DWH does not aggregate(process) any data itself. For each step and entity you create an ETL process which loads, extracts, transforms the data, between different stages(databases).</p>
<p>Conformed dimensions (same dimensions used in different datamarts/fact tables) are used as a single point of truth in the DWH.</p>
<p>For example if you have <code>User</code> table incrementally loaded from different source systems like Salesforce, GoogleAnalytics, etc, then different ETL processes are loading the data into LZ, then another ETL process combines and de duplicates it into ODS table, then another process is loading it into DM along with Transactions data into monthly/daily aggregated Transaction data mart, which is a daily or monthly aggregation star schema, using <code>User</code> table as a dimension.</p>
<p>There is another modern concept - a <a href=""https://stackoverflow.com/a/68406247/2700344""><code>data lake</code></a>, which additionally contains semi-structured or not structured data along with structured data in 3NF and data marts, this allows data engineers/data miners to mine the unstructured data and analyze, find some correlations and finally build new datamarts.</p>
"
"70885339","resolve the ambiguity for the dimensional model for one to many(fact to dimension) ratio","<p>I have a question regarding facts and dimensions.</p>
<p>suppose I have come across a fact table fact_trips - composed of columns like
trip_id ,
driver_id,
vehicle_id,
date ( in the int form - 'YYYYMMDD') ,
timestamp (in milliseconds - bigint ),
miles,
time_of_trip</p>
<p><a href=""https://i.stack.imgur.com/zCyG6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zCyG6.png"" alt=""enter image description here"" /></a></p>
<p>I have another dim_alerts - composed of columns like
trip_id,
Alert_id,
Alert_type,
Number_of_alerts_per_type_per_trip_id,
total_alerts_per_trip_id</p>
<p><a href=""https://i.stack.imgur.com/CboMQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CboMQ.png"" alt=""enter image description here"" /></a></p>
<p>important thing to note is that, there is one-to-many mapping between fact and dimension over here. So, a trip_id - 1 might have multiple entries in the dim_alerts table</p>
<p>How do I relate them? Do I use a surrogate key or what? cause the key here is that if i include alert_id in the fact table, then for every trip_id, there will multiple alert_id's , which will add more records in the fact.</p>
<p>The DWH necessarily gauzes the action the user takes on his phone_app
while he is driving his car. The act of driving the car is called as a trip. A single trip can consists of multiple alerts.</p>
<p><a href=""https://i.stack.imgur.com/won8S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/won8S.png"" alt=""enter image description here"" /></a></p>
","<data-warehouse><dimensional-modeling><fact>","2022-01-27 20:31:10","45","0","1","70901970","<p>TripAlerts is a fact, not a dimension.  So you have a table: FactTripAlert with primary key (TripId,AlertId).</p>
"
"70875633","The ambiguity w.r.t date field in Dim_time","<p>I have come across a fact table <code>fact_trips</code> - composed of columns like</p>
<pre><code> driver_id,
 vehicle_id,
 date ( in the int form  - 'YYYYMMDD')
 timestamp (in milliseconds -  bigint )
 miles,
 time_of_trip
</code></pre>
<p>I have another <code>dim_time</code> - composed of columns like</p>
<pre><code>  date ( in the int form  - 'YYYYMMDD'),
  timestamp (in milliseconds -  bigint ),
  month,
  year,
  day_of_week
  day
</code></pre>
<p>Now when I want to see the trips grouped based on year, I have to join the two tables based on <code>timestamp</code> (in bigint) and then group by year from <code>dim_time</code>.</p>
<p>Why the hell do we keep date in int form then? Because ultimately, I have to join on timestamp. What needs to be changed?</p>
<p>Also, the dim_time does not have a primary key, hence there are multiple entries for the same date. So, when I join the tables, I get more rows in return than expected.</p>
","<time><data-warehouse><dimensional-modeling><fact>","2022-01-27 08:49:40","29","0","1","70876246","<p>You should have 2 Dim tables:</p>
<ol>
<li>DIM_DATE: PK = YYYYMMDD</li>
<li>DIM_TIME: PK = number. Will hold the same number of records as however many milliseconds there are in a day (assuming you are holding time at the millisecond grain rather than second, minute, etc)</li>
</ol>
"
"70871564","Python code(boto3) insert SQL to table in AWS Glue data catalog","<p>Task - I have to insert some data into table which reside in AWS Glue data catalog.</p>
<p>I use boto3 to retrieve the data from table already but i can't write to glue catalog.</p>
<pre><code>client = boto3.client('glue', 'us-east-1')
client.put_item(tablename = 'abcd', item={'col1':{'S','goal'},'col2':{'S','goal1'})
job.commit()
</code></pre>
<p>got an error</p>
<pre><code> glue object has no attribute 'put_item'
</code></pre>
<p>Question - How to insert data into table in AWS Glue data catalog.
Please help!</p>
","<pyspark><boto3><aws-glue><aws-glue-data-catalog><aws-glue-spark>","2022-01-26 23:35:42","399","0","1","70873445","<p>AWS Glue does not have a putItem method - see here for
<a href=""https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue.html"" rel=""nofollow noreferrer"">Boto3 Glue documentation</a>. While invalid, it appears your code should be used for DynamoDB given the reference to the method name, and parameters (e.g. table, item).</p>
"
"70816580","Make a column based on conditions and previous columnb in R","<p>I have a dataset like this :</p>
<pre><code>structure(list(Q3.3 = c(&quot;Female&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Female&quot;, 
&quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, 
&quot;Female&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, 
&quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, 
&quot;Female&quot;, &quot;Male&quot;), Q6.2 = c(&quot;Yes&quot;, &quot;NE&quot;, &quot;Yes&quot;, &quot;NE&quot;, &quot;Yes&quot;, 
&quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;No&quot;, &quot;No&quot;, &quot;NE&quot;, 
&quot;Yes&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;No&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;No&quot;, &quot;NE&quot;, &quot;NE&quot;, 
&quot;No&quot;, &quot;NE&quot;, &quot;Yes&quot;, &quot;NE&quot;), Q6.3 = c(&quot;3&quot;, &quot;NE&quot;, &quot;2&quot;, &quot;NE&quot;, &quot;1&quot;, 
&quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, 
&quot;3&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, 
&quot;NE&quot;, &quot;2&quot;, &quot;NE&quot;), Q6.4 = c(&quot;Yes&quot;, &quot;NE&quot;, &quot;No&quot;, &quot;NE&quot;, &quot;No&quot;, &quot;NE&quot;, 
&quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;No&quot;, &quot;No&quot;, &quot;NE&quot;, &quot;No&quot;, 
&quot;NE&quot;, &quot;NE&quot;, &quot;No&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;No&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;No&quot;, &quot;NE&quot;, 
&quot;No&quot;, &quot;NE&quot;), Q6.5 = c(&quot;3&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, 
&quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, 
&quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, 
&quot;NE&quot;), Q7.5_1 = c(&quot;Yes&quot;, &quot;&quot;, &quot;Yes&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, 
&quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;No, this was available, but I did not use it&quot;, 
&quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;Yes&quot;, &quot;&quot;), dataname1 = c(&quot;Q&quot;, 
&quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, 
&quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, 
&quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;), Q7.5_1_bb = c(&quot;Yes&quot;, &quot;NE&quot;, &quot;Yes&quot;, &quot;NE&quot;, &quot;Yes&quot;, 
&quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;No&quot;, &quot;No&quot;, &quot;NE&quot;, 
&quot;No, this was available, but I did not use it&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;No&quot;, 
&quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;No&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;No&quot;, &quot;NE&quot;, &quot;Yes&quot;, &quot;NE&quot;)), row.names = c(NA, 
30L), class = &quot;data.frame&quot;)[![enter image description here][1]][1]
</code></pre>
<p>I want to mirror &quot;NE&quot; values to Q7.5_1 and also need to code  &quot;Q7.5_1&quot; column again &quot;NE&quot;</p>
<p>If <strong>NOT</strong> Q6.3 = 2 Or Q6.3 = 3 Or Q6.3 = 4 Or Q6.3 = 5 Or Q6.3 = 6 Or Q6.3 = 7 Or Q6.3 = 8 Or Q6.3 = 9 Or Q6.3 = 10</p>
<p><strong>Or If</strong> Q6.5 = 2 Or Q6.5 = 3 Or Q6.5 = 4 Or Q6.5 = 5 Or Q6.5 = 6 Or Q6.5 = 7 Or Q6.5 = 8 Or Q6.5 = 9 Or Q6.5 = 10</p>
<p><strong>Or If</strong> Q6.3 = 1 And Q6.5 = 1</p>
<pre><code>df&lt;-df %&gt;% mutate(Q7.5_1_bb = if_else(
  !(Q6.3 %in% c(2:10)) | Q6.5 %in% c(2:10) | Q6.3 == 1 &amp; Q6.5 == 1,
  Q6.2, Q7.5_1))

</code></pre>
<p>When I used the above code, some of the values were weritten, like the row 5</p>
<pre><code>    Q3.3 Q6.2 Q6.3 Q6.4 Q6.5 Q7.5_1  Q7.5_1_bb

1   Female  Yes 3   Yes 3   Yes     Yes
2   Male    NE  NE  NE  NE          NE
3   Female  Yes 2   No  NE  Yes     Yes
4   Male    NE  NE  NE  NE          NE
5   Female  Yes 1   No  NE          Yes
</code></pre>
<p>While I expect to  see NE in Q7.5_1_bb in row 5.</p>
","<r><dplyr><data-management>","2022-01-22 19:52:13","72","2","1","70816897","<p>Would it be this:</p>
<pre><code>newdf &lt;- structure(list(Q3.3 = c(&quot;Female&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Female&quot;, 
                        &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, 
                        &quot;Female&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, 
                        &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, 
                        &quot;Female&quot;, &quot;Male&quot;), Q6.2 = c(&quot;Yes&quot;, &quot;NE&quot;, &quot;Yes&quot;, &quot;NE&quot;, &quot;Yes&quot;, 
                                                    &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;No&quot;, &quot;No&quot;, &quot;NE&quot;, 
                                                    &quot;Yes&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;No&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;No&quot;, &quot;NE&quot;, &quot;NE&quot;, 
                                                    &quot;No&quot;, &quot;NE&quot;, &quot;Yes&quot;, &quot;NE&quot;), Q6.3 = c(&quot;3&quot;, NA, &quot;2&quot;, NA, &quot;1&quot;, NA, 
                                                                                       NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;3&quot;, NA, NA, NA, NA, 
                                                                                       NA, NA, NA, NA, NA, NA, NA, &quot;2&quot;, NA), Q6.4 = c(&quot;Yes&quot;, &quot;&quot;, &quot;No&quot;, 
                                                                                                                                      &quot;&quot;, &quot;No&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;No&quot;, &quot;No&quot;, &quot;&quot;, &quot;No&quot;, 
                                                                                                                                      &quot;&quot;, &quot;&quot;, &quot;No&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;No&quot;, &quot;&quot;, &quot;&quot;, &quot;No&quot;, &quot;&quot;, &quot;No&quot;, &quot;&quot;), 
               Q6.5 = c(&quot;3&quot;, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
                        NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
                        NA, NA, NA), Q7.5_1 = c(&quot;Yes&quot;, &quot;&quot;, &quot;Yes&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, 
                                                &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;No, this was available, but I did not use it&quot;, 
                                                &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;Yes&quot;, &quot;&quot;)), row.names = c(NA, 
     
# Conditional
newdf2 &lt;- newdf %&gt;% mutate(Q7.5_1_bb = case_when(
  !(Q6.3 %in% c(2:10)) ~ if_else(Q6.3 == 1, 
                            if_else(Q6.5 == 1, Q7.5_1, &quot;NE&quot;), Q7.5_1),
    Q6.5 %in% c(2:10) ~ Q7.5_1,
  TRUE ~ &quot;NE&quot;
  )
  )
</code></pre>
"
"70798814","Not able to get custom attribute data from Autodesk BIM360 for data management API","<p>Not able to get custom attribute data from Autodesk BIM360 for data management API - <a href=""https://forge.autodesk.com/en/docs/data/v2/reference/http/projects-project_id-folders-folder_id-contents-GET/"" rel=""nofollow noreferrer"">https://forge.autodesk.com/en/docs/data/v2/reference/http/projects-project_id-folders-folder_id-contents-GET/</a></p>
<p>When I was trying to get all data available in a specific project with a specific folder, but I didn't get the custom attribute data in that. I also need custom attribute data for the file included in that folder. Whereas I am able to get only data and included values with missing custom attribute values. Also in the API custom attribute values are missing. please find it below.</p>
<pre><code>data&quot;: [
    {
      &quot;type&quot;: &quot;folders&quot;,
      &quot;id&quot;: &quot;urn:adsk.wipprod:dm.folder:hC6k4hndRWaeIVhIjvHu8w&quot;,
      &quot;attributes&quot;: {
        &quot;name&quot;: &quot;Plans&quot;,
        &quot;displayName&quot;: &quot;Plans&quot;,
        &quot;createTime&quot;: &quot;2015-11-27T11:11:23.000Z&quot;,
        &quot;createUserId&quot;: &quot;BW9RM76WZBGL&quot;,
        &quot;createUserName&quot;: &quot;John Doe&quot;,
        &quot;lastModifiedTime&quot;: &quot;2015-11-27T11:11:27.000Z&quot;,
        &quot;lastModifiedTimeRollup&quot;: &quot;2015-11-27T11:11:27.000Z&quot;,
        &quot;lastModifiedUserId&quot;: &quot;BW9RM76WZBGL&quot;,
        &quot;lastModifiedUserName&quot;: &quot;John Doe&quot;,
        &quot;path&quot;: &quot;/dm-test-root/f0cb4ba0-7722-0133-9814-0eeb7bad1e3b&quot;,
        &quot;objectCount&quot;: 4,
        &quot;hidden&quot;: false,
        &quot;extension&quot;: {
          &quot;type&quot;: &quot;folders:autodesk.bim360:Folder&quot;,
          &quot;version&quot;: &quot;1.0&quot;,
          &quot;schema&quot;: {
            &quot;href&quot;: &quot;/schema/v1/versions/folders%3Aautodesk.bim360%3AFolder-1.0&quot;
          },
          &quot;data&quot;: {
            &quot;allowedTypes&quot;: [
              &quot;folders&quot;,
              &quot;items:autodesk.bim360:File&quot;,
              &quot;items:autodesk.bim360:Document&quot;,
              &quot;items:autodesk.bim360:TitleBlock&quot;
            ],
            &quot;visibleTypes&quot;: [
              &quot;folders&quot;,
              &quot;items:autodesk.bim360:Document&quot;
            ],
            &quot;namingStandardIds&quot;: []
          }
        }

</code></pre>
<p>I would like to save custom attributes with their values also.<a href=""https://i.stack.imgur.com/qUMqb.png"" rel=""nofollow noreferrer"">Please find the image of the custom attribute with their values in the format of rows and colums</a></p>
","<autodesk-forge><autodesk-viewer><autodesk><autodesk-data-management>","2022-01-21 09:09:23","202","0","1","70857859","<p>To get custom attributes of the document in BIM360, you will need to use another endpoint (Version:Batch:Get). This blog tells more.
<a href=""https://forge.autodesk.com/blog/custom-attributes-apis-bim-360-document-are-now-public-beta"" rel=""nofollow noreferrer"">https://forge.autodesk.com/blog/custom-attributes-apis-bim-360-document-are-now-public-beta</a></p>
<p>While please note, since March 23, 2021, in the newly created project, custom attributes definitions are created in project level (previously, in folder level), although you could still use the endpoint above to access the custom attributes, there is a gap that no endpoint yet to get definitions of project level. This blog tells more:
<a href=""https://forge.autodesk.com/blog/custom-attributes-projects-created-and-after-march-23-2021"" rel=""nofollow noreferrer"">https://forge.autodesk.com/blog/custom-attributes-projects-created-and-after-march-23-2021</a></p>
"
"70791465","TSQL - Get value between dates in data warehouse dimension","<p>I have the beginnngs of a data warehouse, that contains (among other tables)</p>
<ul>
<li>date table with 200 years worth of dates to join with, so no need to
build a &quot;dynamic&quot; date table</li>
<li>dbo.Dim_Items (see below)</li>
</ul>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ItemNumber</th>
<th>QOH</th>
<th>EffectiveDate</th>
<th>ExpirationDate</th>
<th>IsCurrentRecord</th>
</tr>
</thead>
<tbody>
<tr>
<td>372620</td>
<td>24</td>
<td>2021-12-11 05:34:09.000</td>
<td>9999-12-31 00:00:00.000</td>
<td>1</td>
</tr>
<tr>
<td>372620</td>
<td>11</td>
<td>2021-12-09 05:34:11.000</td>
<td>2021-12-11 05:34:09.000</td>
<td>0</td>
</tr>
<tr>
<td>372620</td>
<td>9</td>
<td>2021-12-07 05:34:20.000</td>
<td>2021-12-09 05:34:11.000</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>I would like to find the QOH for each day between 2021-12-07 and today (assume 2021-12-13, for brevity), so that my result looks like this</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Date</th>
<th>ItemNumber</th>
<th>QOH</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021-12-07</td>
<td>372620</td>
<td>9</td>
</tr>
<tr>
<td>2021-12-08</td>
<td>372620</td>
<td>9</td>
</tr>
<tr>
<td>2021-12-09</td>
<td>372620</td>
<td>11</td>
</tr>
<tr>
<td>2021-12-10</td>
<td>372620</td>
<td>11</td>
</tr>
<tr>
<td>2021-12-11</td>
<td>372620</td>
<td>24</td>
</tr>
<tr>
<td>2021-12-12</td>
<td>372620</td>
<td>24</td>
</tr>
<tr>
<td>2021-12-13</td>
<td>372620</td>
<td>24</td>
</tr>
</tbody>
</table>
</div>
<p>The closest I have come is the query below, but it is not working the way I want it to (note the zeros on dates that don't line up with the Items dimension)</p>
<pre><code>DECLARE @START_DATE date,
        @END_DATE date

SET @START_DATE = '20211207'
SET @END_DATE = GETDATE()   -- '20211213'

SELECT CAL.TheDate,
        I.ItemNumber,
        I.QOH
FROM dbo.Dim_Calendar CAL
    LEFT OUTER JOIN dbo.Dim_Items I ON CAL.TheDate &gt;= CAST(I.EffectiveDate as date)
        AND CAL.TheDate &lt;= CAST(I.EffectiveDate as date)
        AND I.ItemNumber = 372620
WHERE CAL.TheDate &gt;= @START_DATE
    AND CAL.TheDate &lt;= @END_DATE
ORDER BY CAL.TheDate,
            I.ItemNumber
        
        
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Date</th>
<th>ItemNumber</th>
<th>QOH</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021-12-07</td>
<td>372620</td>
<td>9</td>
</tr>
<tr>
<td>2021-12-08</td>
<td>372620</td>
<td>0</td>
</tr>
<tr>
<td>2021-12-09</td>
<td>372620</td>
<td>11</td>
</tr>
<tr>
<td>2021-12-10</td>
<td>372620</td>
<td>0</td>
</tr>
<tr>
<td>2021-12-11</td>
<td>372620</td>
<td>24</td>
</tr>
<tr>
<td>2021-12-12</td>
<td>372620</td>
<td>0</td>
</tr>
<tr>
<td>2021-12-13</td>
<td>372620</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>I am new to data warehousing as a whole, so I am not sure how to build this query correctly.</p>
","<tsql><sql-server-2017>","2022-01-20 18:20:56","32","0","1","70791768","<p><a href=""https://dbfiddle.uk/?rdbms=sqlserver_2019&amp;fiddle=c583ca3cfb4a987a176d79f4689a6923"" rel=""nofollow noreferrer"">DBFIDDLE</a></p>
<pre><code>DECLARE @START_DATE date,
        @END_DATE date

SET @START_DATE = '20211207'
SET @END_DATE = GETDATE()   -- '20211213'

SELECT
   TheDate,
   I.Itemnumber,
   I.QOH
FROM dbo.Dim_Calendar CAL
LEFT JOIN dbo.Dim_Items I On CAL.TheDate BETWEEN CAST(I.EffectiveDate as DATE) 
                                             AND I.ExpirationDate
                         AND I.Itemnumber = 372620
WHERE CAL.TheDate &gt;= @START_DATE
    AND CAL.TheDate &lt;= @END_DATE
ORDER BY CAL.TheDate,
            I.ItemNumber
</code></pre>
"
"70790829","Can a SCD 2 table have non SCD columns?","<p>Lets say there is a table schema where in the columns are a,b,c,d,start_time,end_time,current_status.</p>
<p>Can we have a,b,c as SCD columns and let d not be a part of SCD logic so that if d changes, it wont create a new SCD row?</p>
","<database><etl><data-warehouse><scd><scd2>","2022-01-20 17:34:46","179","0","2","70859463","<p>Yes, if d column value changes, the record is overwritten.</p>
"
"70790829","Can a SCD 2 table have non SCD columns?","<p>Lets say there is a table schema where in the columns are a,b,c,d,start_time,end_time,current_status.</p>
<p>Can we have a,b,c as SCD columns and let d not be a part of SCD logic so that if d changes, it wont create a new SCD row?</p>
","<database><etl><data-warehouse><scd><scd2>","2022-01-20 17:34:46","179","0","2","70862546","<p>The Type 2 SCD is called Row Versioning where you cab track changes as version records with current flag &amp; active dates and other metadata.
Do not forget that after you have implemented your chosen dimension type, you need to point your fact records at the relevant business or surrogate key. Surrogate keys in the SCD type 2 relate to a specific historical version of the record, removing join complexity from later data structures.</p>
"
"70787329","Reshaping multiple variables in wide format to nodes and edges for network analysis","<p>I am having trouble preparing a dataset in R for network analysis. I have a set of ten non-mutually exclusive race/ethnicity dummy variables from survey data in a wide format of just over 150K responses (see R code below for sample data)</p>
<pre><code>resp &lt;- 150000
race_01 &lt;- rbinom(n=resp, size=1, prob = runif(1,min = 0, max = 1))
race_02 &lt;- rbinom(n=resp, size=1, prob = runif(1,min = 0, max = 1))
race_03 &lt;- rbinom(n=resp, size=1, prob = runif(1,min = 0, max = 1))
race_04 &lt;- rbinom(n=resp, size=1, prob = runif(1,min = 0, max = 1))
race_05 &lt;- rbinom(n=resp, size=1, prob = runif(1,min = 0, max = 1))
race_06 &lt;- rbinom(n=resp, size=1, prob = runif(1,min = 0, max = 1))
race_07 &lt;- rbinom(n=resp, size=1, prob = runif(1,min = 0, max = 1))
race_08 &lt;- rbinom(n=resp, size=1, prob = runif(1,min = 0, max = 1))
race_09 &lt;- rbinom(n=resp, size=1, prob = runif(1,min = 0, max = 1))
race_10 &lt;- rbinom(n=resp, size=1, prob = runif(1,min = 0, max = 1))
dat &lt;- cbind(race_01, race_02, race_03, race_04, race_05, race_06, race_07, race_08, race_09, race_10)
dat
</code></pre>
<p>Googling around, I haven't been able to find a method to transition this dataset to one of nodes and edges for network analyses. Any assistance or direction would be appreciated.</p>
","<r><network-analysis><data-management>","2022-01-20 13:45:33","69","0","1","70787867","<pre><code>library(igraph)
g&lt;-graph_from_adjacency_matrix(dat)
plot(g, edge.arrow.size=.6)
</code></pre>
"
"70767609","When will the data quality function be released to the release version","<p>When will the dolphinscheduler data quality function be released to the release version?
Is there a time plan for this。</p>
","<data-quality><apache-dolphinscheduler>","2022-01-19 08:44:43","70","1","1","71985892","<p>3.0-alpha has supported this feature, you can download the version from the url[1]</p>
<p>1,https://dolphinscheduler.apache.org/en-us/download/download.html</p>
"
"70736458","How to copy all tables, stored procedures to another schema in Synapse data warehouse?","<p>Currently, I need to transfer all tables (DDL and data in the tables), stored procedures to another schema in Synapse data warehouse. I checked the documentation below, but it seems that I have to move all of them one by one.</p>
<p><a href=""https://learn.microsoft.com/en-us/sql/t-sql/statements/alter-schema-transact-sql?view=sql-server-ver15"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sql/t-sql/statements/alter-schema-transact-sql?view=sql-server-ver15</a></p>
<p>Is there a method, command or query which I can transfer all the contents of a schema to another in Synapse data warehouse ?</p>
<p>Kind regards,</p>
","<sql><azure><azure-synapse>","2022-01-17 04:14:42","1287","1","1","70739910","<p>There is no built-in method to do this, but depending on your skills there are a number of different options:</p>
<ul>
<li><p>use SQL Server Management Studio (SSMS) built-in scripting options.  Newer versions of SSMS (v18.x and onwards) are capable of producing DDL for Azure Synapse Analytics.  Simply point to your object (table, stored proc, view etc) in Object Explorer, right-click it, and view the scripting options.  eg for tables you will see 'Script Table as'</p>
</li>
<li><p>SQL Server Data Tools (SSDT) - SSDT now has support for Azure Synapse Analytics, dedicated SQL pools.  So you can import your schema, do a find and replace in the .sql scripts in the project, and generate the script.  You can also use the Data Compare and Schema Compare features.</p>
</li>
<li><p>command-line option <code>mssql-cli</code>.  This offers powerful command-line scripting options but you'll need to download and install it:  <a href=""https://learn.microsoft.com/en-us/sql/tools/mssql-cli?view=sql-server-ver15"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sql/tools/mssql-cli?view=sql-server-ver15</a></p>
</li>
<li><p>Use <code>CTAS</code> to transfer schema and data.  Create a simple CTAS template and run it for each of your tables:</p>
<pre><code> CREATE TABLE &lt;new schema&gt;.yourTable
 WITH
 (
     DISTRIBUTION = ROUND_ROBIN,
     CLUSTERED COLUMNSTORE INDEX
 )
 AS
 SELECT *
 FROM &lt;old schema&gt;.yourTable;
 OPTION ( LABEL = 'CTAS: copy yourTable to new schema' );
</code></pre>
</li>
</ul>
<p>So a few options for you.</p>
"
"70734449","How to optimize materialized views when data volume and velocity increase in redshift?","<p>I created materialized views in Redshift. Data is growing incrementally day by day, will the performance (such as duration of query result) of these materialized views be affected as the size of the data volume and velocity increase? What should I optimize if it is affected?</p>
","<amazon-web-services><amazon-redshift><data-warehouse><materialized-views>","2022-01-16 21:47:22","391","0","1","70734856","<p>As things get bigger, queries in general will get slower.  However, the real question is if the query time (view refresh time) will increase disproportionally to the input data size.  Since materialized views are often used when the generating query is complex it is often the case.  So it is likely that things will slow noticeably as the data grows but how much will depend on your situation.</p>
<p>The important thing to ensure that the generating query is optimized.  Materialized views get used, at times, when people don't want to do the work of optimizing the query and it doesn't matter that the data is slightly out of current.  Then the data grows the the refresh time gets out of hand and THEN they need to do the optimization.  They bought some time which is often good.  If the generating query has looped joins or creates new data in processing it likely can be significantly optimized.</p>
"
"70695226","Should I use multiple fact tables for each grain or just aggregate from lowest grain?","<p>Fairly new to data warehouse design and star schemas. We have designed a fact table which is storing various measures about Memberships, our grain is daily and some of the measures in this table are things like qty sold new, qty sold renewing, qty active, qty cancelled.</p>
<p>My question is this, the business will want to see the measures at other grains such as monthly, quarterly, yearly etc.. so would typically the approach here just be to aggregate the day level data for whatever time period was needed or would you recommend creating separate fact tables for  the &quot;key&quot; time periods for our business requirements e.g. monthly, quarterly, yearly? I have read some mixed information on this which is mainly why I'm seeking others views.</p>
<p>Some information I read had people embedding a hierarchy in the fact table to designate different grains which was then identified via a &quot;level&quot; type column, which was advised against by quite a few people and didn't seem good to me also, those advising against we're suggesting separate fact tables per grain but to be honest I don't see why we wouldn't just aggregate from the daily entries we have, what benefits would we get from a fact table for each grain other than some slight performance improvements maybe?</p>
","<etl><data-warehouse><dimensional-modeling><star-schema><star-schema-datawarehouse>","2022-01-13 10:42:59","399","0","1","70702661","<p>Each DataMart will have its own &quot;perspective&quot;, which may require an aggregated fact grain.</p>
<p>Star schema modeling is a &quot;top-down&quot; process, where you start from a set of questions or use cases and build a schema that makes those questions easy to answer.  Not a &quot;bottom-up&quot; process where you start with the source data and figure out the schema design from there.</p>
<p>You may end up with multiple data marts that share the same granular fact table, but which need to aggregate it in different ways, either for performance, or to have a gran to calculate and store a measure that only makes sense at the aggregated grain.</p>
<p>Eg</p>
<pre><code>SalesFact (store,day,customer,product,quantiy,price,cost)
</code></pre>
<p>and</p>
<pre><code>StoreSalesFact(store, week, revenue, payroll_expense, last_year_revenue)
</code></pre>
"
"70693971","Apache Kylin - how to solve cube build error problem","<p>When i built kylin_sales_cube in Apache Kylin,i get the error:</p>
<p><code>ERROR [main] org.apache.kylin.engine.mr.KylinReducer:java.lang.NoSuchMethodError: org.apache.hadoop.fs.FileSystem.setStoragePolicy(Lorg/apache/hadoop/fs/Path;Ljava/lang/String;)V</code></p>
<p>when i built my own cube,i got same error.</p>
<p>please help!</p>
<p>versions: kylin3.1.3 hadoop2.7.3 hbase1.7.1 hive2.3.9</p>
","<bigdata><data-analysis><data-warehouse><kylin>","2022-01-13 09:13:01","110","0","1","70737781","<p>It looks like kylin 3.1 and hadoop 2.7 are not compatible. Maybe try kylin &lt; 2.5 ?</p>
<p>You can find more info about compatibility here:</p>
<p><a href=""https://kylin.apache.org/docs31/install/index.html"" rel=""nofollow noreferrer"">https://kylin.apache.org/docs31/install/index.html</a></p>
"
"70692733","Linking SSIS packages to a table to all me to control which packages should run","<p>I am a newbie to SSIS and trying to replicate the current ELT process (which is configured via a set of store procedures) using SSIS.</p>
<p>The ELT process:
I have a config table that has all the table information stored.
The key column in this table is called 'Active'. This allows me to enable and deactivate tables that I don't want the ELT to run at any given time.
The ELT_SP scans the DWH_Process table for tables marked as active and thereafter executes the schedule for that specific job via server agent.</p>
<p>The SSIS Process
I have configured the SSIS packages to extract data from the CRMs and import into DWH.
I have used the 'sequence containers' control flow to replicate the ELT process, and this is working perfectly.</p>
<p>The problem is, I would then have to update 300+ tables and redeploy to the DWH.</p>
<p>In SSIS, what other options are available to me, and is there a simpler way to add in the 'check active status' before executing the package through a job agent?</p>
<p><a href=""https://i.stack.imgur.com/noCO6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/noCO6.png"" alt=""enter image description here"" /></a></p>
","<sql-server><visual-studio><ssis><package><data-warehouse>","2022-01-13 07:18:17","149","0","1","70694334","<p>A setup I've used in the past is something like this:
<a href=""https://i.stack.imgur.com/iYPsv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iYPsv.png"" alt=""Concept"" /></a></p>
<ol>
<li>Setup a table with the configuration, something with columns like PackageName, IsEnabled, Partition ...</li>
<li>Create a loop package, that you are going to call in the root packages. The loop package queries, the config table based on a parameter of the package (e.g. Partition=1) , for a specific partition. The query also filters on IsEnabled = 1 or a different condition for your case. After the query you starts a foreach loop for every result/package that's returned from the query. This foreach then calls the actual package with your logic. You use a variable from the foreach to start the <code>Execute package task</code><br />
<a href=""https://i.stack.imgur.com/W6NDJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W6NDJ.png"" alt=""Loop packages"" /></a></li>
<li>Create a root package</li>
<li>In a sequence container in the root package, add N execute package tasks that calls the loop package, where you pass a parameter which partition they are executing. (This is purely for performance to run multiple packages at once.)<br />
<a href=""https://i.stack.imgur.com/7j4U6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7j4U6.png"" alt=""Sequence container with execute package tasks"" /></a></li>
</ol>
<p>The benefit of this approach is you can enabled disable packages with a simple update statement, no need to redeploy. You can play with the partitions to get the best performance.</p>
"
"70687102","Firestore custom document data mapping","<p><a href=""https://i.stack.imgur.com/S6JK4.png"" rel=""nofollow noreferrer"">Firestore Model</a></p>
<p>I have the following data structure in Firestore. I am having a hard time mapping it to my ios app. I am using Swift. Could anyone provide the codable struct or something?</p>
<pre><code>struct CartModel: Codable {

    let cart: [CartItemModel]
   
    enum CodingKeys: String, CodingKey {
        case cart
    }    
}

extension CartModel{
    static var dummyCartData : CartModel {
        .init(cart: [CartItemModel.dummyCartData])
    }
}


struct CartItemModel: Codable {
    let brand: String
    let itemDescription: String
    let image: String
    let selection: [UserSelection]
    let title:String    
   
    enum CodingKeys: String, CodingKey {
        
        case brand
        case itemDescription = &quot;description&quot;
        case image
        case selection
        case title

    }
}


extension CartItemModel{
    static var dummyCartData : CartItemModel {

        .init(brand: &quot;gucci&quot;,
              itemDescription: &quot;adidas&quot;,
              image: &quot;yeezy&quot;,
              selection: [UserSelection.dummyData],
              title: &quot;123&quot;)
    }
}


struct UserSelection: Codable, Equatable{
    var size :String
    var count : Int

    enum CodingKeys: String, CodingKey {
        case size = &quot;size&quot;
        case count
    }

}

extension UserSelection{

    static var dummyData : UserSelection {

        .init(size: &quot;XS11&quot;, count: 20)
    }
}


    docRef.getDocument { (snapshot, error) in
        if error != nil {
            return
        }

        if let snapshot = snapshot, snapshot.exists {
            print(snapshot.data())
        let products = try! snapshot.data(as: CartModel.self)
            print(products)
       
        }
    }
</code></pre>
<p>Firestore returns</p>
<pre><code>Optional([&quot;cart&quot;: &lt;__NSArrayM 0x600002979380&gt;(
{
    brand = Nike;
    description = &quot;Find the Jordan Point Lane at . &quot;;
    image = &quot;https://static.nike.com/a/images/t_default/f105dd55-53d0-4a50-b43a-55bd9231c658/jordan-point-lane-mens-shoes-PPMHdC.png&quot;;
    selection =     {
        count = 1;
        size = &quot;&quot;;
    };
    title = &quot;Jordan Point Lane Men's Shoes&quot;;
}
)
])
</code></pre>
<p>Fatal error: 'try!' expression unexpectedly raised an error: Swift.DecodingError.typeMismatch(Swift.Array, Swift.DecodingError.Context(codingPath: [CodingKeys(stringValue: &quot;cart&quot;, intValue: nil),</p>
<p>_FirestoreKey(stringValue: &quot;Index 0&quot;, intValue: 0), CodingKeys(stringValue: &quot;selection&quot;, intValue: nil)], debugDescription: &quot;Not an array&quot;, underlyingError: nil))</p>
<p>Fatal error: 'try!' expression unexpectedly raised an error: Swift.DecodingError.typeMismatch(Swift.Array, Swift.DecodingError.Context(codingPath: [CodingKeys(stringValue: &quot;cart&quot;, intValue: nil), _FirestoreKey(stringValue: &quot;Index 0&quot;, intValue: 0), CodingKeys(stringValue: &quot;selection&quot;, intValue: nil)], debugDescription: &quot;Not an array&quot;, underlyingError: nil))</p>
","<ios><swift><iphone><google-cloud-firestore><mapping>","2022-01-12 19:08:30","42","0","1","70687213","<p>As the error says, <code>selection</code> is not an array in the Firestore document, but you've defined it as one in your model.</p>
<p>Change <code>let selection: [UserSelection]</code> to <code>let selection: UserSelection</code></p>
"
"70679997","what is the best way of handling data resubmission in a datawarehouse?","<p>Let's assume that we have a datawarehouse comprised of four components :</p>
<ul>
<li><strong>extract</strong> : source data is extracted from an oracle database to a flat file. there is a flat file per source table. Extraction date is kept as part of the flat file name. Each record contains a insert/update date from the source system.</li>
<li><strong>staging area</strong> : temporary tables used to load the extracted data into database tables</li>
<li><strong>operational data store</strong> : staged data will be loaded in the ODS. The ODS keeps all the history of all the loaded data and the data is typecast. Surrogate keys are not yet generated.</li>
<li><strong>datawarehouse</strong> : data is loaded from the ODS, surrogate keys are generated, dimensions are historized, and finally fact data is loaded and attached to the proper dimension.</li>
</ul>
<p>So far so good, and regarding regular delta loading I have no issue. However the question I ask myself is : I have regularly encountered in the past situations where, for whatever reason, you will want to resubmit extracted data into the loading pipeline. Let's assume for instance that we select all the extracted flat files over the last 15 days, and that we push them again to the ETL process.</p>
<ol>
<li>There is no new extraction from the source systems. Previously loaded files are re-used and fed into the ETL process.</li>
<li>Data is then reloaded into the staging tables, which will have been truncated previously</li>
<li>now data has to move to the ODS. And here I have a real headache on how to proceed.</li>
</ol>
<p><strong>Alternative 1</strong> : just insert the new rows. So we would have :
row 2, natural key : ID001, batch date : 12/1/2022 16:34, extraction date : 10/1/2022, source system modification timestamp : 10/1/2022 10:43:00
previous row : natural key : ID001, batch date : 10/1/2022 01:00, extraction date : 10/1/2022, source system modification timestamp : 10/1/2022</p>
<p>But then, when loading to the DWH, we need to have some kind of insert/update mechanism and we cannot do a straight insert as it will created duplicate facts.</p>
<p><strong>Alternative 2</strong> : apply an insert/update logic at ODS level. With the previous example we would have :</p>
<ul>
<li>check if the ODS table contains already a row with natural key : ID001 - extraction date : 10/1/2022, source system modification timestamp : 10/1/2022</li>
<li>insert if not found</li>
</ul>
<p><strong>Alternative 3</strong> : purge in the ODS the previously loaded data, i.e.</p>
<ul>
<li>purge all the data where extraction date in the last 15 days</li>
<li>load the data from the staging.</li>
</ul>
<p>Alternative 1 is performant but shifts the insert/update task at DWH level, so the performance-killer is still there.
Alternative 2 requires an insert update, which for millions of rows does not seem optimal.
Alternative 3 looks good but if feels wrong to delete data from the ODS.</p>
<p>What is your view on this ? In other words my question would be how to reconcile the recommandation to have insert-only processes in the datawarehouse, with the reality that from time to time you will need to reload previously extracted data to fix bugs or correct missing data.</p>
","<etl><data-warehouse>","2022-01-12 10:32:25","201","-1","1","70905853","<p>There are two primary methods to load data into your data warehouse:</p>
<p><strong>Full load:</strong> with a full load, the entire data staged is dumped, or loaded, and is then completely replaced with the new, updated data flow. No additional information, such as timestamps or audit technical columns, are needed.</p>
<p><strong>Incremental load/ Delta load:</strong> only the difference between the target and source data is loaded through the ETL process in data warehouse. There are 2 types of incremental loads, depending on the data volumetry , streaming incremental load and batch incremental load.</p>
"
"70642143","Recursive method for calculate percentual of repeated values for each column in my df with R","<p>I need to use lapply/sapply or other recursive methods for my real df for calculate how many repeated values have in each column/variable.</p>
<p>Here I used an small example to reproduce my case:</p>
<pre><code>library(dplyr)

df &lt;- data.frame(
var1 = c(1,2,3,4,5,6,7,8,9,10 ),
var2 = c(1,1,2,3,4,5,6,7,9,10 ),
var3 = c(1,1,1,2,3,4,5,6,7,8 ),
var4 = c(2,2,1,1,2,1,1,2,1,2 ),
var5 = c(1,1,1,1,1,4,5,5,6,7 ),
var6 = c(4,4,4,5,5,5,5,5,5,5 )   
)

</code></pre>
<p>I have <code>r nrow(df)</code> in my dataset and now I need to obtain the % of repeated values for each column. Suppose that my real <code>df</code> have a lot of columns, and I need to do it recursively. I tryed to use <code>lapply/sapply</code>, but it didn´t worked...</p>
<pre><code># create function that is used in lapply
perc_repeated &lt;- function(variables){
  
  paste(round((sum(table(df$variables)-1) / nrow(df))*100,2),&quot;%&quot;)
  
}

perce_repeated_values &lt;- lapply(df, perc_repeated) 
perce_repeated_values

</code></pre>
<p><img src=""https://github.com/rhozon/datasets/raw/master/perc_repeated_for_each_col.png"" alt="""" /></p>
<p>How to do this optimally if my dataframe increases in number of columns to something like 700, using some recursive function for each column and getting the results in an orderly way in a dataframe from largest to smallest ? (eg of the variable that has it 100% repeated values for the one that reaches 0%) in something like:</p>
<pre><code>df_repeated

variable      perc_repeated_values
var6                    80%
var4                    80%
var5                    50%
var3                    20%
var2                    20%
var1                     0%

</code></pre>
","<r><dataframe><function><recursion><data-quality>","2022-01-09 14:17:58","56","0","1","70642292","<p>This can easily be done with <code>dplyr::summarize()</code></p>
<pre class=""lang-r prettyprint-override""><code>library(tidyverse)

df &lt;- data.frame(
  var1 = c(1,2,3,4,5,6,7,8,9,10 ),
  var2 = c(1,1,2,3,4,5,6,7,9,10 ),
  var3 = c(1,1,1,2,3,4,5,6,7,8 ),
  var4 = c(2,2,1,1,2,1,1,2,1,2 ),
  var5 = c(1,1,1,1,1,4,5,5,6,7 ),
  var6 = c(4,4,4,5,5,5,5,5,5,5 )   
)

df %&gt;% 
  summarise(across(everything(),
                   ~100 * (1 - n_distinct(.)/n()))) %&gt;% 
  pivot_longer(everything(), 
               names_to = &quot;var&quot;, 
               values_to = &quot;percent_repeated&quot;) %&gt;% 
  arrange(desc(percent_repeated))
#&gt; # A tibble: 6 x 2
#&gt;   var   percent_repeated
#&gt;   &lt;chr&gt;            &lt;dbl&gt;
#&gt; 1 var4                80
#&gt; 2 var6                80
#&gt; 3 var5                50
#&gt; 4 var3                20
#&gt; 5 var2                10
#&gt; 6 var1                 0
</code></pre>
<p><sup>Created on 2022-01-09 by the <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex package</a> (v2.0.1)</sup></p>
"
"70627383","data warehouse - physical implementation best practice","<p>I have been tasked to design a Kimball-style data warehouse.  It will sit on-prem in SQL Server.  What is the best practice for the organization of the physical implementation?  That is, should the data warehouse be a single database, using schemas to separate each of the data marts (and also putting all dimensions in their own schema, to help &quot;drive&quot; re-use across marts)?  Or, should each data mart be its own database (forcing all dimensions to live in a separate database)?</p>
<p>Does the decision matter if I was to use a cloud platform for the data warehouse, say Azure SQL DB (e.g., use managed instance to allow for cross-database querying)?</p>
","<data-warehouse>","2022-01-07 21:30:09","65","1","1","70627846","<p>Both a multi-schema and a multi-database design are common.  The main benefits for a multi-database design is</p>
<ol>
<li><p>The ability to have different backup/recovery model/HA for different parts of the DW.</p>
</li>
<li><p>An easier security, admin, and DevOps story for having different teams own different parts of the DW.</p>
</li>
</ol>
<p>In Azure SQL Database (not managed instance), and Synapse SQL Pool (currently) first-class cross-database queries are not available, so you're more likely to use a multi-schema design there.  Managed Instance has some cost-of-perf issues for data warehouse workloads, and Hyperscale typically a better fit for Data Warehouses in the 100GB-10TB range.</p>
<p>But all Managed Instance databases are in  Full recovery with platform-managed backups.  And in MI you can't share storage resources between databases, so a multi-database design may be more expensive.</p>
<p>So not a clear best-practice there.  Columnstore for large fact tables is strong recommendation, combined with general Columnstore best-practices for loading and maintenance.</p>
"
"70596683","SSIS: Model design issue causing duplications - can two fact tables be connected?","<p>For a university group project, we are building a data warehouse using SQL Server and Visual Studio. We are currently in the staging area and want to fill the tables with data.
However, we noticed that in our fact sales table the sum of the price + the sum of the freight value is different from the payment_value as well as the values differ from the csv file. That is why we tried to additionally check the box “Remove rows with duplicate sort values” in the Sort 1 (see files). Without checking that option we end up with
117.216 rows in the end and with checking it, we end up with 102.727 rows. Yet, there is still a variation between all of the values. For a better overview and understanding, please refer to the following image:
<a href=""https://i.stack.imgur.com/2ul2S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2ul2S.png"" alt=""differences"" /></a></p>
<p>We found out that the duplications are due that there could be more than one payment per order, which means there is is a model design issue. Payment should actually be another fact table, separate from sales.</p>
<p>We were trying to create a fact table for payment, however we are not sure how to do it, since we thought it is not possible to have two fact tables connected to each other. Could you maybe help us out here, because Payments should be connected to Sales, however Sales is a fact table, so we are not sure how to proceed.</p>
<p>Another problem, we our facing is that if Payment will be a fact table, we are losing a dimension. In our guidelines we are required to have 5 dimensions and we would not know what other dimension to create (could also be made up). If you have any suggestions on how to solve that correlating problem, we would be more than happy.</p>
<p>For a better understanding and insight, we are providing you with our SQL scripts, flat files as well as visual studio data flow: <a href=""https://drive.google.com/file/d/1Sseh6LCigpY1eLpQEANqTjCjQKYT8INb/view?usp=sharing"" rel=""nofollow noreferrer"">Download files</a></p>
<p>Any help is much appreciated! Thank you very much in advance! :)</p>
","<sql-server><ssis><data-warehouse>","2022-01-05 16:59:01","161","0","1","70597798","<p>You can still have payments as a dimension to stay within the constraints of the project. One thing you can do to deal with the many to many relationship between orders and payments is what is called a table, so your schema would be like this:</p>
<pre><code>CREATE DATABASE [OLIST_STORE_STG]
GO

USE [OLIST_STORE_STG]
GO


/* the dimension table for Customers: stg_dim_customer */
CREATE TABLE Stg_Dim_Customer (
BK_Customer_unique NVARCHAR (50) PRIMARY KEY,
Customer_zip_code_prefix INT NOT NULL,
Customer_city NVARCHAR(50) NOT NULL,
Customer_state NVARCHAR(50) NOT NULL,
);


/* Object: The dimension table for products: Stg_Dim_Product */

CREATE TABLE Stg_Dim_Product (
BK_Product NVARCHAR(50) PRIMARY KEY,
Product_category_name_english NVARCHAR(50) NOT NULL,
Product_photos_qty INT NULL,
Product_name_lenght INT NULL,
Product_description_lenght INT NULL,
Product_weight_g INT NULL,
Product_length_cm INT NULL,
Product_height_cm INT NULL,
Product_width_cm INT NULL,
);


/* Object: The dimension table for sellers: Stg_Dim_Sellers */

CREATE TABLE Stg_Dim_Sellers(
BK_Sellers NVARCHAR (50) PRIMARY KEY,
Sellers_zip_code_prefix INT NOT NULL,
Sellers_city NVARCHAR (50) NOT NULL,
Sellers_state NVARCHAR (50) NOT NULL,
);


/* The dimension table for sellers: Stg_Dim_Payment */

CREATE TABLE Stg_Dim_Payment(
BK_Payment INT PRIMARY KEY,
Payment_sequential INT NOT NULL,
Payment_type NVARCHAR (50) NOT NULL,
Payment_installments INT NOT NULL
);


/* The dimension table for Date: Stg_Dim_Date */

CREATE TABLE Stg_Dim_Date(
SK_Date INT PRIMARY KEY,
Full_date DATE NOT NULL,
Day_number INT NOT NULL,
Day_name NVARCHAR (20) NOT NULL,
Month_number INT NOT NULL,
Month_name NVARCHAR (50) NOT NULL,
Trimester_number INT NOT NULL,
Trimester_name NVARCHAR (50) NOT NULL,
Year INT NOT NULL
);


/* The Facts Table for Sales: Stg_Fact_Sales */

CREATE TABLE Stg_Fact_Sales (
FK_Date DATE ,
FK_Product NVARCHAR(50) ,
FK_Sellers NVARCHAR(50),
FK_Customer_unique NVARCHAR(50) ,
Delays INT NULL,
Price DECIMAL(18, 2) NOT NULL,
Freight_value DECIMAL(18, 2) NOT NULL,
Payment_value DECIMAL (18,2) NOT NULL,
Order_status NVARCHAR(50) NOT NULL,
Order_reference NVARCHAR (50) NOT NULL,
Order_item_reference INT NOT NULL,
CONSTRAINT pk_Fact_Sales PRIMARY KEY (
Order_reference ASC,
Order_item_reference ASC
)
);



/* The Facts Table for Reviews: Stg_Fact_Reviews */

CREATE TABLE Stg_Fact_Reviews (
FK_Date DATE NOT NULL,
FK_Customer_unique NVARCHAR(50) NOT NULL,
Average_review_score DECIMAL(18,2) NOT NULL,
CONSTRAINT pk_Fact_Reviews PRIMARY KEY (
FK_Date ASC,
FK_Customer_unique ASC
)
);

/* Bridge table to relate orders to payments */

CREATE TABLE Stg_Order_Payments (
FK_Order_reference NVARCHAR (50),
FK_Payment NVARCHAR (50)
);
</code></pre>
<p>Of course, this will need to be reflected in the SSIS package: remove the merge between payments and orders when generating fact_sales and add a new data flow from payments to stg_order_payments</p>
<p><strong>Other Design Considerations:</strong></p>
<p>Payments are most commonly held in a fact table. If you need more dimensions for the constraints of the project, you can for example make a Zip Codes table, so you don't have to repeat city and state in Customers and Sellers dimensions, but just a foreign key to the table Zip codes. Also Categories could be another dimension, suppose that in the future, there also is translation to French or any other language; it would be better to have the Product category in Portuguese and a category table with the Portuguese name and all the translations. Even better, you can create a numeric Category Code as a FK in product to the table Category.</p>
"
"70522967","Max source dates from all joined tables during data warehouse incremental build","<p>I have a data warehouse query that builds a fact table by joining 14 source tables. Each source table has a source_timestamp field to indicate the time the record was inserted or updated.  I need to pull the max source_timestamp for each row of the query result from each of the 14 source tables.  This will allow me to know the max update date for each row of the fact table.</p>
<p>I wanted to do something like this for the last field in the query...</p>
<pre><code>(
    SELECT MAX(Source_Timestamp)
    FROM (
        VALUES a.source_timestamp, b.source_timestamp, c.source_timestamp, ...
    ) AS UpdateDate(Source_Timestamp)
) AS LastUpdateDate
</code></pre>
<p>However, I get an incorrect syntax error because the subquery doesn't know a., b., or c. in the query context.  I was hoping the VALUES clause would help me out but apparently not.</p>
<p>Any ideas on how to accomplish this?</p>
","<sql><sql-server><tsql><max><data-warehouse>","2021-12-29 17:36:33","29","0","1","70523901","<p>It was my fault for not being more careful with the coding.  I should've guessed from the fact that it was a syntax error.  I needed to enclose each of the items in the <code>VALUES</code> clause in () like:</p>
<pre><code>(
    SELECT MAX(Source_Timestamp)
    FROM (
        VALUES (a.source_timestamp), (b.source_timestamp), (c.source_timestamp),(...)
    ) AS UpdateDate(Source_Timestamp)
) AS LastUpdateDate
</code></pre>
"
"70502950","Data warehouse Fact table foreign key Generation","<p>I am pretty new to data warehouse designing.
I have a database like below.
<a href=""https://i.stack.imgur.com/LktNh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LktNh.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://webpages.charlotte.edu/mirsad/itcs6265/group1/domain.html"" rel=""nofollow noreferrer"">https://webpages.charlotte.edu/mirsad/itcs6265/group1/domain.html</a></p>
<p>I am planning to create a simple data warehouse like below.</p>
<p><a href=""https://i.stack.imgur.com/7G4r0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7G4r0.png"" alt=""enter image description here"" /></a></p>
<p>But the problem I am facing now is I don't have ClientId and LoanId in the Fact table. Because in the original database it was lnked via Account table.
I am trying to achieve this via SQL server. Can someone show me the direction how to approach this.</p>
","<sql-server><data-warehouse>","2021-12-28 04:59:29","105","-1","1","70560358","<p>If you want to create the FactTransaction fact table you don't need the Loan because it represents a fact so Transaction and Loan are two differnt actions like mentioned in the data dictionary :</p>
<blockquote>
<p>TRANSACTIONS (TRANS) Each record describes one transaction on an
account</p>
<p>LOANS Each record describes a loan granted for a given account</p>
</blockquote>
<p>the design can be like below if we follow a separate context for each :</p>
<p><strong>Datamart Transaction :</strong></p>
<p><a href=""https://i.stack.imgur.com/XDG8j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XDG8j.png"" alt=""enter image description here"" /></a></p>
<p><strong>Datamart Loan :</strong></p>
<p><a href=""https://i.stack.imgur.com/T9BlV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T9BlV.png"" alt=""enter image description here"" /></a></p>
<p>The query to populate the FactTransaction is like below :</p>
<pre><code>SELECT T.account_id AS TransactionID, C.client_id AS ClientID,
A.account_id AS AccountID
FROM Transactions AS T
LEFT JOIN Account AS A ON A.account_id=T.account_id
LEFT JOIN Disposition AS D ON A.account_id=D.account_id
LEFT JOIN Client AS C ON C.client_id=D.client_id
</code></pre>
<p>AccountID, TransactionID, ClientID represent a composite key to uniquely identify the tuple transaction in a the fact table.</p>
<p>The query to populate the FactLoan is like below :</p>
<pre><code> SELECT L.account_id AS LoanID, C.client_id AS ClientID,
A.account_id AS AccountID
    FROM Loan AS L
    LEFT JOIN Account AS A ON A.account_id=A.account_id
    LEFT JOIN Disposition AS D ON A.account_id=D.account_id
    LEFT JOIN Client AS C ON C.client_id=D.client_id
</code></pre>
<p>AccountID, LoanID, ClientID represent a composite key to uniquely identify the tuple transaction in a the fact table.</p>
<p>Do not forget to implement the DimDate.</p>
"
"70502064","Dates as an attribute in Star schema","<p>All,
It's been a while I built a DWH star schema, too much data vault nowadays :) Quick question. Can we use dates as an attributes in some dimensions? Such as in Product dimension, if we have product effective_date and retired_date, these can be included as attributes in product dimension right?</p>
<p>Have a separate date dimension as well.</p>
<p>Thanks.</p>
","<database-design><data-warehouse><snowflake-schema>","2021-12-28 01:51:25","341","-2","1","70566510","<p>Maybe the correct question is <strong>Should dates in the dimension tables be joined to dimDate?</strong>
<strong>First point :</strong>
These attributes can be considered facts because in general anything refering to a date is a fact since it is time-sensitive.
So if it is possible, we can add these into a fact table. This could be as simple as Fact.Product or Fact.ProductHistory for example, but it really depends on how you will be analysing the data. Sometimes you may even want to add them directly into the main fact table such as Fact.Sales or something similar.</p>
<p><strong>Second point :</strong></p>
<p>You must be aware that adding relationships will take time in term of maintenance of your data warehouse. Since the foreign keys may cause additional overhead during the ETL process, even when disabling them while loading the data, you need to recheck them once they are enabled again.</p>
<p><strong>Third point :</strong>
You can create a dedicated-seperated date dimensions for each one of the date attributes</p>
<p>if you do wish to link the dimensional attributes to a date dimension, then I find it is better to create separate date dimension tables for each one of your attributes. I will let you discover the inconvenience of this if your data model becomes complex.</p>
<p>You can link all date attributes to the same date dimension table, then you will discover that it will be difficult when applying multiple independent filters.</p>
<p>What if you only want to see products added in X month but you also want to aggregate the sales by the month they were purchased?</p>
<p>Using a single date dimension means that this query is a lot harder than it needs to be. If you have separate date dimensions it is pretty easy to do.</p>
<p>You can read more about <a href=""https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/role-playing-dimension/"" rel=""nofollow noreferrer"">role-playing dimensions</a>.</p>
"
"70378726","Draw dendrogram using agglomerative algorithm","<p><a href=""https://i.stack.imgur.com/Giq8L.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Giq8L.jpg"" alt=""Question"" /></a></p>
<p>I have tried making it dendrogram but it was seeming to me very wrong.</p>
<p>Pair I have made:
XB, XBZ, XBZY</p>
","<data-mining><data-warehouse><dendrogram>","2021-12-16 12:04:15","69","-1","1","70385224","<p>The obvious problem is, that your <em>distance</em> matrix is <em>inconsistent</em>.</p>
<p>If the distence from <code>A</code> to <code>B</code> is 3 and the distance from <code>B</code> to <code>Y</code> is 2, how is it possible that the distance from <code>A</code> to <code>Y</code> is 6?</p>
<p>If you slightly adapt your matrix, so that it corresponds to a distance of points in a real space, you get better results.</p>
<p><strong>Example Using R</strong></p>
<pre><code>data &lt;- c(0,3,3.3,1,3,
          3,0,3.3,1,3,
          3.3,3.3,0,3,6,         
          1,1,3,0,3, 
          3,3,6,3,0)          

dim_names &lt;- c(&quot;X&quot;,&quot;Z&quot;,&quot;Y&quot;,&quot;B&quot;,&quot;A&quot;)
mat &lt;- matrix(data,nrow=5,ncol=5,byrow=TRUE, dimnames = list(dim_names,dim_names) )

dist &lt;- as.dist(mat)

dist

    X   Z   Y   B
Z 3.0            
Y 3.3 3.3        
B 1.0 1.0 3.0    
A 3.0 3.0 6.0 3.0

hc &lt;- hclust(dist)

plot(hc, hang = -1)
</code></pre>
<p>which produce
<a href=""https://i.stack.imgur.com/EuBjb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EuBjb.png"" alt=""enter image description here"" /></a></p>
"
"70345051","Delete repeated occurrences of a value across two columns","<p>I have a dimension that stores salary awards by code and description. The Award_Code and Award_Desc combination forms a natural key.  Each code should have only one description and each description should have only one code, but over the years people have added the same award code but with a different description or the same description but with a different award code, resulting in a table like below.  In this example one of the award codes is found twice with different descriptions (Award_SK 6 and Award_SK 2270) and one of the Award descriptions is found twice with different codes (Award_SK 6 and Award_SK 2209).  Only Award_SK 6, is the correct Award_Code/ Award_Desc combo and I need to remove the others from the dimension.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Award_SK</th>
<th style=""text-align: center;"">Award_Code</th>
<th style=""text-align: left;"">Award_Desc</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">6</td>
<td style=""text-align: center;"">AWDTEA</td>
<td style=""text-align: left;"">Teachers Award</td>
</tr>
<tr>
<td style=""text-align: center;"">2209</td>
<td style=""text-align: center;"">TEAAWD</td>
<td style=""text-align: left;"">Teachers Award</td>
</tr>
<tr>
<td style=""text-align: center;"">2270</td>
<td style=""text-align: center;"">AWDTEA</td>
<td style=""text-align: left;"">Award for Teachers</td>
</tr>
</tbody>
</table>
</div>
<p>To work out which descriptions and codes are associated with each other as in the above table, I have run the following code that grabs rows that join more than once on the Award_Code or the Award_Description.</p>
<pre><code>--get the list of awards that are associated either by code or description, and put them in a temporary table
    SELECT * INTO #DuplicatedAwards
    FROM
    (
        SELECT Dim_Award_SK,AWARD_CODE, AWARD_DESC
        FROM
        (
            --Type 1: different Award codes, same award description
            SELECT  Dim_Award_SK, award_code,AWD.Award_Desc FROM    
            DM.DIM_AWARD AWD    
            INNER JOIN  
                (SELECT Award_Desc, COUNT(Dim_Award_SK) as total_of_Same_Description_different_code FROM DM.DIM_AWARD
                GROUP BY Award_Desc, Award_Class_Desc
                HAVING count(Award_Desc)&gt;1 
                ) A ON AWD.Award_Desc=A.Award_Desc 
            
            UNION ALL
    
            --Type 2: different award description, same award code
            SELECT  Dim_Award_SK, A.Award_Code,AWD.Award_Desc FROM
            DM.DIM_AWARD AWD
            INNER JOIN
                (SELECT Award_Code,COUNT(Dim_Award_SK) as Total_of_Same_Code_Different_Description FROM DM.DIM_AWARD
                GROUP BY Award_Code
                HAVING count(DISTINCT Award_Desc)&gt;1 
            ) A ON AWD.Award_Code=A.Award_Code 
        )B
    )C
    
    --Join the temporary table to the dimension on award code OR award description.  This will show an Award_SK in the first column 
    --and its matched Award_SK's in the second column
 --When a new SK starts in the first column we are looking at a new group of matched awards
    
    SELECT DISTINCT
    AW.Dim_Award_SK as Award_SK,
    DIM.Dim_Award_SK as Matching_Award_SK 
    FROM #DuplicatedAwards AW
    INNER JOIN DM.DIM_AWARD DIM
    ON DIM.Award_Code=AW.Award_Code OR DIM.Award_Desc=AW.Award_Desc
    --exclude rows where the affected SK is matched with itself
    WHERE DIM.Dim_Award_SK &lt;&gt; AW.Dim_Award_SK
    ORDER BY  AW.Dim_Award_SK, DIM.Dim_Award_SK
    
    DROP TABLE #DuplicatedAwards
</code></pre>
<p>This gives me a result like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Award_SK</th>
<th style=""text-align: left;"">Matched Award_SK</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">6</td>
<td style=""text-align: left;"">2209</td>
</tr>
<tr>
<td style=""text-align: center;"">6</td>
<td style=""text-align: left;"">2270</td>
</tr>
<tr>
<td style=""text-align: center;"">8</td>
<td style=""text-align: left;"">1853</td>
</tr>
<tr>
<td style=""text-align: center;"">8</td>
<td style=""text-align: left;"">2278</td>
</tr>
<tr>
<td style=""text-align: center;"">17</td>
<td style=""text-align: left;"">2052</td>
</tr>
<tr>
<td style=""text-align: center;"">17</td>
<td style=""text-align: left;"">2442</td>
</tr>
<tr>
<td style=""text-align: center;"">22</td>
<td style=""text-align: left;"">1895</td>
</tr>
<tr>
<td style=""text-align: center;"">22</td>
<td style=""text-align: left;"">2282</td>
</tr>
<tr>
<td style=""text-align: center;"">22</td>
<td style=""text-align: left;"">2428</td>
</tr>
<tr>
<td style=""text-align: center;"">1853</td>
<td style=""text-align: left;"">8</td>
</tr>
<tr>
<td style=""text-align: center;"">1853</td>
<td style=""text-align: left;"">2278</td>
</tr>
<tr>
<td style=""text-align: center;"">1895</td>
<td style=""text-align: left;"">22</td>
</tr>
<tr>
<td style=""text-align: center;"">1895</td>
<td style=""text-align: left;"">2282</td>
</tr>
<tr>
<td style=""text-align: center;"">1895</td>
<td style=""text-align: left;"">2428</td>
</tr>
<tr>
<td style=""text-align: center;"">2052</td>
<td style=""text-align: left;"">17</td>
</tr>
<tr>
<td style=""text-align: center;"">2052</td>
<td style=""text-align: left;"">2442</td>
</tr>
<tr>
<td style=""text-align: center;"">2209</td>
<td style=""text-align: left;"">6</td>
</tr>
<tr>
<td style=""text-align: center;"">2209</td>
<td style=""text-align: left;"">2270</td>
</tr>
<tr>
<td style=""text-align: center;"">2270</td>
<td style=""text-align: left;"">6</td>
</tr>
<tr>
<td style=""text-align: center;"">2270</td>
<td style=""text-align: left;"">2209</td>
</tr>
<tr>
<td style=""text-align: center;"">2278</td>
<td style=""text-align: left;"">8</td>
</tr>
<tr>
<td style=""text-align: center;"">2278</td>
<td style=""text-align: left;"">1853</td>
</tr>
<tr>
<td style=""text-align: center;"">2282</td>
<td style=""text-align: left;"">22</td>
</tr>
<tr>
<td style=""text-align: center;"">2282</td>
<td style=""text-align: left;"">1895</td>
</tr>
<tr>
<td style=""text-align: center;"">2282</td>
<td style=""text-align: left;"">2428</td>
</tr>
<tr>
<td style=""text-align: center;"">2428</td>
<td style=""text-align: left;"">22</td>
</tr>
<tr>
<td style=""text-align: center;"">2428</td>
<td style=""text-align: left;"">1895</td>
</tr>
<tr>
<td style=""text-align: center;"">2428</td>
<td style=""text-align: left;"">2282</td>
</tr>
<tr>
<td style=""text-align: center;"">2442</td>
<td style=""text-align: left;"">17</td>
</tr>
<tr>
<td style=""text-align: center;"">2442</td>
<td style=""text-align: left;"">2052</td>
</tr>
</tbody>
</table>
</div>
<p>The first two values in the left column are the same, so I know I need to look at the details of Award_SK 6, 2209 and 2270 in the dimension to work out business-wise, which is the right Award_SK to keep and which other two can be discarded.  Next, rows 3 and 4 both show Award_SK 8 so I know that I need to look at Award_SK 8, 1853 and 2278 together, and so on. However, these combinations will show up more than once in a different arrangement as I move through the table. Award_SK 1853 eventually shows up again in the first column with Award_SK 8 and Award_SK 2278 in the second column. I have 8000 rows in my table but if I stop the combinations recurring, the table will be significantly smaller and I'll end up with a table like this.  I'm not sure what to add to my code to make that happen. Maybe I could even do it in Excel, but again, not sure how.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Award_SK</th>
<th style=""text-align: left;"">Matched Award_SK</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">6</td>
<td style=""text-align: left;"">2209</td>
</tr>
<tr>
<td style=""text-align: center;"">6</td>
<td style=""text-align: left;"">2270</td>
</tr>
<tr>
<td style=""text-align: center;"">8</td>
<td style=""text-align: left;"">1853</td>
</tr>
<tr>
<td style=""text-align: center;"">8</td>
<td style=""text-align: left;"">2278</td>
</tr>
<tr>
<td style=""text-align: center;"">17</td>
<td style=""text-align: left;"">2052</td>
</tr>
<tr>
<td style=""text-align: center;"">17</td>
<td style=""text-align: left;"">2442</td>
</tr>
<tr>
<td style=""text-align: center;"">22</td>
<td style=""text-align: left;"">1895</td>
</tr>
<tr>
<td style=""text-align: center;"">22</td>
<td style=""text-align: left;"">2282</td>
</tr>
<tr>
<td style=""text-align: center;"">22</td>
<td style=""text-align: left;"">2428</td>
</tr>
</tbody>
</table>
</div>
<p>I'd really appreciate any help with this. Thanks.</p>
","<sql><sql-server><tsql><data-warehouse><olap>","2021-12-14 07:18:26","66","1","1","70362347","<p>Instead of avoiding matching to the same, you can use that as matching to higher.</p>
<pre><code>SELECT DISTINCT
  AW.Dim_Award_SK as Award_SK,
  DIM.Dim_Award_SK as Matching_Award_SK 
FROM #DuplicatedAwards AW
JOIN DM.DIM_AWARD DIM
  ON ( DIM.Award_Code = AW.Award_Code OR
       DIM.Award_Desc = AW.Award_Desc 
     ) 
 AND AW.Dim_Award_SK &lt; DIM.Dim_Award_SK
ORDER BY AW.Dim_Award_SK, DIM.Dim_Award_SK
</code></pre>
"
"70342114","How to set up a database for authenticate and data management","<p>I just finished the in deph course of vaadin fusion (@marcushellberg). It was really good.</p>
<p>Now Im trying to do the same but now I have a Postgres database with info, but I don't know how to load the information in the grid from the database</p>
<p>And how to authenticate an user from database instead of hardcode the credentials... Anyone can help?</p>
<p>I just created my personalized entities, endpoints, repositories and the view for one of them however the view do not load the info from database, also I have the application.properties file with database info and tested</p>
<p>Is there something that I am missing?</p>
<p>Thanks in advance and if you need more info from my project just let me know.</p>
<p>I notice that in my browser I am getting this</p>
<pre><code>Error: Endpoint 'SupermercadosEndpoint' method 'getSupermercadosData' request cannot be accessed, reason: 'Access denied'
</code></pre>
","<vaadin-grid><hilla><vaadin-fusion>","2021-12-13 23:32:59","189","0","1","70342214","<p>Vaadin Fusion uses Spring Security, you can follow any Spring Security tutorials on database authentication if you want to store your credentials there. Here's one for example <a href=""https://www.baeldung.com/spring-security-authentication-with-a-database"" rel=""nofollow noreferrer"">https://www.baeldung.com/spring-security-authentication-with-a-database</a></p>
<p>For the endpoint access, ensure you have a <code>@PermitAll</code> or another appropriate annotation on the Endpoint as they are deny all by default when you have security enabled <a href=""https://vaadin.com/docs/latest/fusion/security/configuring/#security-options"" rel=""nofollow noreferrer"">https://vaadin.com/docs/latest/fusion/security/configuring/#security-options</a></p>
"
"70308036","As a part of initial load testing or incremental load testing do we validate audit fields?","<p>We are performing initial load testing and incremental load testing on target tables in datawarehouse. so do we validate audit field values in initial load test or do we validate audit fields values  in incremental load testing ? As part of which testing do we validate audit field values</p>
","<etl><data-warehouse><sql-data-warehouse><uat>","2021-12-10 17:03:36","142","0","1","70310349","<p>Presumably audit fields get populated in both initial and incremental loads, in which case you need to test them in both types of load</p>
"
"70298480","how should I manage changes in a dimension field when using big flat table?","<p>Let's say that I have a sale (a fact entry) made by a user (a dimension).
initially I would insert this sale+user into my big flat table in my data warehouse. So far sounds very standard</p>
<p>But my doubt is about what should I do when a dimension entry have a change?, for example, the user change his address.</p>
<p>How should I proceed in this scenario?, should I introduce a new sale, using the same data as before, with the user information updated, into my big flat table?</p>
<p>NOTE: I'm speaking about dimensions, etc, but I'm not pretending to use at all an star-schema as an intermediate step (at least for my learning problem)</p>
","<data-modeling><data-warehouse>","2021-12-10 00:47:23","70","1","1","70303906","<p>The user dimension is usually called conformed/shared dimension.</p>
<p>A conformed dimension is a dimension that has exactly the same meaning and content when being referred from different fact tables.</p>
<p>In your case :</p>
<blockquote>
<p>But my doubt is about what should I do when a dimension entry have a
change?, for example, the user change his address.</p>
</blockquote>
<ol>
<li>If you have a need to store the User's address for analytic
purposes, you can opt for Slowly Changing Dimension Type 2 – Row
Versioning. Therefore you need to update your fact table and link it
to the last version of the User.</li>
<li>If you don't need to track the addresses changes of a User you can
use the Slowly Changing Dimension Type 1 – No History. Therefore no need to update the fact rows since they will always point to the unique version of a user</li>
</ol>
"
"70273474","Datawarehouse design","<p>I am going to design a Datawarehouse (although its not an easy process). I am wondering through out the ETL process , how the data in the Datawarehouse is going to extract/transform to Data Mart ?
Are there any model design within Datawarehouse vs Datamart ? Also usually starschema or snowflake?so should we place the table like in the following</p>
<p>In Datawarehouse
dim_tableA
dim_tableB
fact_tableA
fact_tableB</p>
<p>And in Datamart A
dim_tableA   (full copy from datawarehouse)
fact_tableA   (full copy from datawarehouse)</p>
<p>And in Datamart B
dim_tableB   (full copy from datawarehouse)
fact_tableB  (full copy from datawarehouse)</p>
<p>is it something real life example which can demonstrate the model difference between datawarehouse and datamart ?</p>
","<data-warehouse><datamart>","2021-12-08 10:30:23","93","-1","2","70277808","<p><em><strong>I echo with both Nick's responses  and in more technical way following Kimball methodology:</strong></em></p>
<pre><code>In my opinion and my experience. At high level ,we have data marts like Service Analytics , Financial Analytics , Sales Analytics , Marketing Analytics ,Customer Analytics etc. These were grouped as below
Subject Areas -&gt; Logical grouping(Star Modelling) -&gt;Data Marts -&gt; Dimension &amp;Fact (As per Kimball’s)
Example:
AP Real Time -&gt;Supplier, Supplier Transaction’s , GL Data -&gt; Financial Analytics + Customer Analytics-&gt;Physical Tables
</code></pre>
"
"70273474","Datawarehouse design","<p>I am going to design a Datawarehouse (although its not an easy process). I am wondering through out the ETL process , how the data in the Datawarehouse is going to extract/transform to Data Mart ?
Are there any model design within Datawarehouse vs Datamart ? Also usually starschema or snowflake?so should we place the table like in the following</p>
<p>In Datawarehouse
dim_tableA
dim_tableB
fact_tableA
fact_tableB</p>
<p>And in Datamart A
dim_tableA   (full copy from datawarehouse)
fact_tableA   (full copy from datawarehouse)</p>
<p>And in Datamart B
dim_tableB   (full copy from datawarehouse)
fact_tableB  (full copy from datawarehouse)</p>
<p>is it something real life example which can demonstrate the model difference between datawarehouse and datamart ?</p>
","<data-warehouse><datamart>","2021-12-08 10:30:23","93","-1","2","70282292","<p>Data marts contain repositories of summarized data collected for analysis on a specific section or unit within an organization, for example, the sales department. ... A data warehouse is a large centralized repository of data that contains information from many sources within an organization.</p>
<p>Depending on their needs, companies can use multiple data marts for different departments and opt for data mart consolidation by merging different marts to build a single data warehouse later. This approach is called the Kimball Dimensional Design Method. Another method, called The Inmon Approach, is to first design a data warehouse and then create multiple data marts for particular services as needed.</p>
<p><strong>An example:</strong> In a data warehouse, email clicks are recorded based on a click date, with the email address being just one of the click parameters. For a CRM expert, the e-mail address (or any other customer identifier) ​​will be the entry point: opposite each contact, the frequency of clicks, the date of the last click, etc.</p>
<p>The Datamart is a prism that adapts the data to the user. In this, its keys to success depend a lot on the way the data is organized. The more understandable it is to the user, the better the result. This is why the titles of each field and their method of calculation must stick as closely as possible to the uses of the trade.</p>
"
"70220549","How to modify the source file path of a BigQuery external table in Airflow?","<p>There is a process which exports some CSV files to GCS and puts the current datetime into the path, e.g.:</p>
<pre><code>gs://my_gcs_bucket/path/to/export/&lt;current_timestamp&gt;/exported_file_1.csv
gs://my_gcs_bucket/path/to/export/&lt;current_timestamp&gt;/exported_file_2.csv
</code></pre>
<p>I want to create an external table for these files before I cast its columns and merge with other tables.</p>
<p>The problem is that I can't implement a DAG in Airflow which can handle the changing timestamps.</p>
<hr />
<p>I can create an external table by specifying the path (with the current timestamp), but for tomorrow's exports this external table will not read the new files. But I will need tomorrow the same <code>project_name.dataset.tablename</code> for processing.</p>
<pre><code>from airflow.contrib.operators.bigquery_operator import BigQueryCreateExternalTableOperator

CreateExternalBqTable = BigQueryCreateExternalTableOperator(
    task_id                             = &quot;CreateExternalBqTable&quot;,
    field_delimiter                     = '|',
    destination_project_dataset_table   = f'{project_name}.{dataset}.{tablename}',
    bucket                              = BUCKET_URI,
    source_objects                      = [ 'path/to/export/2021-12-12-12-12-12/exported_file_*' ],
    schema_fields                       = generate_custom_struct()
)
</code></pre>
<p>If tomorrow I'll try to run the same table creation task again it will fail because the external table already exists.
I can delete the existing external table then recreate it again to ensure it does not fail the external table creation task, but if it's already deleted then this will fail:</p>
<pre><code>from airflow.providers.google.cloud.operators.bigquery import BigQueryDeleteTableOperator

DeleteExternalBqTable = BigQueryDeleteTableOperator(
    task_id                 = &quot;DeleteExternalBqTable&quot;,
    deletion_dataset_table  = f'{project_name}.{dataset}.{tablename}',
)
</code></pre>
<hr />
<p>What is the good pattern here?<br />
Should I <strong>always delete and recreate the external table</strong> just because of the new timestamps?<br />
It seems to me very bad and error prone pattern.</p>
<p>Or could I <strong>change the path under an existing external table</strong> somehow?</p>
<p>I would be much happier if I could <strong>separate the initialization and deletion phases of my external table</strong> from the daily runs and not always delete or create them. I plan to <strong>initialize these tables only once</strong> and move away the CSV files if I'm done with the processing and keep the external tables empty until the next run.</p>
<p>(I need a working solution for Airflow 1.x)</p>
","<google-bigquery><airflow><data-warehouse><external-tables>","2021-12-03 20:52:14","828","0","1","70234196","<p>In <code>BigQueryDeleteTableOperator </code> you can use <code>ignore_if_missing</code> argument as documented in <a href=""https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/bigquery/index.html#airflow.providers.google.cloud.operators.bigquery.BigQueryDeleteTableOperator"" rel=""nofollow noreferrer"">Airflow docs</a>.</p>
<p>But if you don't want to recreate the table every time consider using idempotent operator  <code>BigQueryCreateEmptyTableOperator</code> to create the table (it will skip creation if table already exists) and then <code>GoogleCloudStorageToBigQueryOperator</code> to load the data. In this way you will separate the creation and load</p>
"
"70174794","Where can I run MDX Queries on macOS?","<p>I searched and found that there is only SQL server (Microsoft) that can execute MDX queries, and unfortunately it is not compatible with macOS. Are there relative apps or environments that I could install on an IDE that could understand MDX query?</p>
","<mdx><data-warehouse><olap><olap-cube>","2021-11-30 19:10:22","167","0","1","70182189","<p>I think you could try <a href=""https://www.iccube.com/support/documentation/user_guide/running_iccube/server.php"" rel=""nofollow noreferrer"">icCube</a>. As I see they support MAC OS.</p>
"
"70132162","Check on data modelling","<p>I would like to know whether my data modeling is working for Power BI.</p>
<p>The dataset I am using is training course for students and corporate. The original data has 3 tables separated by individual program. The purpose of my visualization is to analyze the 3 programs from all students in a single dashboard.</p>
<p>Here is the original data after being imported to Power BI:
<a href=""https://i.stack.imgur.com/6KFqU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6KFqU.png"" alt=""original data"" /></a></p>
<p>Here is the data pre-processing:</p>
<ul>
<li>Remove unneeded column</li>
</ul>
<p>DPT table
Remove column – No, Date, Quarter
DTP table
Remove column – Count, Email, Date
LLD table
Remove column – Email, To calculate, Learning Hours</p>
<ul>
<li>Rename column name &amp; impute missing value with “Not given”</li>
</ul>
<p>DPT table
Trainee = Name, Training Provider = Provider, Course name = Course, Focus area = Domain
DTP table
Participant Name = Name, Event/Training Name = Course, Training providers = Provider</p>
<ul>
<li>Create new column and impute them with “No given” and put in same
position (for append tables later)</li>
</ul>
<p>DTP table
Level
LLD table
Company, Provider, Level</p>
<ul>
<li>Create a new column called Program and impute value as it program
name for each row.</li>
</ul>
<p>Post cleaned table:</p>
<p><a href=""https://i.stack.imgur.com/HKUmT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HKUmT.png"" alt=""post cleaned table"" /></a></p>
<p>After appending the 3 tables and calling it Master:</p>
<p><a href=""https://i.stack.imgur.com/t7u4C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t7u4C.png"" alt=""post append"" /></a></p>
<p>. Duplicate the Master table to create Student, Provider and Program table. In each table remove irrelevant columns, remove duplicates and create unique ID.</p>
<p>Final data model:</p>
<p><a href=""https://i.stack.imgur.com/XNP4f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XNP4f.png"" alt=""final data model"" /></a></p>
<p>The focus is the Program, Provider and Student tables. The rest of the tables will be deactivated the relationship when creating calculated columns and measures before I make any correction to the data model.</p>
<p>Is there any proper approach to build the data model?</p>
<p>From my data model in the last picture, does it mean that the Provider table is a fact table while the Student and Program tables are dimensions?</p>
","<powerbi><powerquery>","2021-11-27 04:58:56","69","0","1","70132747","<p>I agree with removing unneeded columns, renaming columns for a better look, and substituting 'Not Given' in place of NULLS (Caution here: Measures and Dimensions handle nulls differently, for dimensional values substituting is okay)</p>
<p>If modeling using PowerBI is a must, then the following strategy should happen:</p>
<ol>
<li>The dimensions can be Students, Programs, Providers</li>
<li>A Factless Fact table (FactProgram or something like that). It will have dimensional keys to Students, Programs, Providers ( and additional measures that you can create or can be taken from Master )</li>
<li>Remove unnecessary columns from the dimensions, so that the Remove Duplicates will give you what you want. For instance, Student and Program currently have same columns coming from master (Company, Course, Domain, Level, Program, Provider). Make it clear which columns belong to which dimensions and optionally create new dimensions (maybe, DimCompany)</li>
</ol>
"
"70115885","Data Modeling - Slow Changing Dimension type 2: How to deal with schema change (column added)?","<p>What is the best practice to deal with schema-changing when building a Slow Changing Dimension table?</p>
<p>For example, a column was added:</p>
<pre><code>First state:
+----------+---------------------+-------------------+
|customerId|address              |updated_at         |
+----------+---------------------+-------------------+
|1         |current address for 1|2018-02-01 00:00:00|
+----------+---------------------+-------------------+


New state with new column, but every other followed column constant:
+----------+---------------------+-------------------+------+
|customerId|address              |updated_at         |newCol|
+----------+---------------------+-------------------+------+
|1         |current address for 1|2018-03-03 00:00:00|1000  |
+----------+---------------------+-------------------+------+
</code></pre>
<p>My first approach is to think that schema-changing means the row has changed. So I would add a new row to my SCD table:</p>
<pre><code>+----------+---------------------+-------------------+------+-------------+-------------------+-------------------+
|customerId|address              |updated_at         |newCol|active_status|active_status_start|active_status_end  |
+----------+---------------------+-------------------+------+-------------+-------------------+-------------------+
|1         |current address for 1|2018-02-01 00:00:00|null  |false        |2018-02-01 00:00:00|2018-03-03 00:00:00|
|1         |current address for 1|2018-03-03 00:00:00|1000  |true         |2018-03-03 00:00:00|null               |
+----------+---------------------+-------------------+------+-------------+-------------------+-------------------+
</code></pre>
<p>But, what if the columns were added, but for some specific row the value is null? For example, for row with customerId = 2, it is null:</p>
<pre><code>+----------+---------------------+-------------------+------+
|customerId|address              |updated_at         |newCol|
+----------+---------------------+-------------------+------+
|2         |current address for 2|2018-03-03 00:00:00|null  |
+----------+---------------------+-------------------+------+

</code></pre>
<p>In this case, I can take two approaches:</p>
<ol>
<li>Consider every schema change as a row change, even for null rows (<strong>much easier to implement, but costlier from a storage perspective</strong>). It would result in:</li>
</ol>
<pre><code>+----------+---------------------+-------------------+-------------+-------------------+-------------------+------+
|customerId|address              |updated_at         |active_status|active_status_end  |active_status_start|newCol|
+----------+---------------------+-------------------+-------------+-------------------+-------------------+------+
|1         |current address for 1|2018-02-01 00:00:00|false        |2018-03-03 00:00:00|2018-02-01 00:00:00|null  |
|1         |current address for 1|2018-03-03 00:00:00|true         |null               |2018-03-03 00:00:00|1000  |
|2         |current address for 2|2018-02-01 00:00:00|false        |2018-03-03 00:00:00|2018-02-01 00:00:00|null  |
|2         |current address for 2|2018-03-03 00:00:00|true         |null               |2018-03-03 00:00:00|null  |
+----------+---------------------+-------------------+-------------+-------------------+-------------------+------+

</code></pre>
<ol start=""2"">
<li>Do a check for every row, and if it has an actual value for this new column, add it; otherwise, don't do anything to this row (<strong>for now, I didn't come up with implementation to it, but it is much more complicated and likely to be error-prone</strong>). The result in SCD table for row 2 would be 'row has not changed':</li>
</ol>
<pre><code>+----------+---------------------+-------------------+-------------+-------------------+-------------------+------+
|customerId|address              |updated_at         |active_status|active_status_end  |active_status_start|newCol|
+----------+---------------------+-------------------+-------------+-------------------+-------------------+------+
|1         |current address for 1|2018-02-01 00:00:00|false        |2018-03-03 00:00:00|2018-02-01 00:00:00|null  |
|1         |current address for 1|2018-03-03 00:00:00|true         |null               |2018-03-03 00:00:00|1000  |
|2         |current address for 2|2018-02-01 00:00:00|true         |null               |2018-02-01 00:00:00|null  |
+----------+---------------------+-------------------+-------------+-------------------+-------------------+------+


</code></pre>
<p>The second approuch seems more &quot;correct&quot;, but am I right? Also, implement approuch 1 is much simpler. Approuch 2 would need some more complicated and has other trade-offs, for example:
a) What if instead of adding a columns, a columnd was droped?
b) From a query persperctive it is much more costlier.</p>
<p>I have done research on the subject and didn't fount this kind of situation being treated.</p>
<p>What is the standard approach to it? Trade-offs? Is there another approach I am missing here?</p>
<p>Thank you all.</p>
","<sql><apache-spark><data-modeling><data-warehouse><dimensional-modeling>","2021-11-25 18:35:48","156","0","1","70432213","<p>Thanks for @MarmiteBomber and @MatBailie comments. Based on your comments I ended up implementing the second option, because (summary of your thoughts):</p>
<ol>
<li>The second approach is the only one meaningful.</li>
<li>Implementation is a consequence of business logic, not necessarily a standard practice. In our case, we didn't need to differentiate types of nulls, so the right approach was to encapsulate known non-existing values as null, as well as unknown values, etc.</li>
<li>Be explicit.</li>
</ol>
<p>The second approach also needed to add a check (is the new column present in the row?) in write time, but it saves complexity in query time, and storage. Since SCD is &quot;slow&quot; and this case is rare (schema changes happen, but not &quot;every day&quot;), adding the check in write time is better than in query time.</p>
"
"70099078","Mapping non-numeric factor to choose higher value between two columns in R","<p>I have a dataframe with two column: PathGroupStage, ClinGroupStage. I want to create a new column, OutputStage, that chooses the higher stage.</p>
<p>Valid value of stage: I, IA, IB, II, IIA, IIB, III, IIIA, IIIB, IIIC ,IV, IVA, IVB, IVC, Unknown.</p>
<ul>
<li>If both stages have values, then use the highest, e.g., IIIB &gt; IIIA &gt; III</li>
<li>If one is missing and the other has value, the use the one with value</li>
<li>If both are missing or unknown, then .= unknown</li>
</ul>
<p>How would I derive the OutputStage variable comparing the non-numeric values from the two columns? I am thinking I need to factor levels but how would I compare the factors between different columns?</p>
<p>Here is the sample dataset:</p>
<pre><code>   PathGroupStage       ClinGroupStage
1              II                 &lt;NA&gt;
2               I                   IA
3             IVB                  IVB
4            IIIA Unknown/Not Reported
5               I                  III
6              II                 &lt;NA&gt;
7            IIIA                  IIB
8              II                   II
9            &lt;NA&gt;                 &lt;NA&gt;
10           IIIB Unknown/Not Reported

 df &lt;- structure(list(PathGroupStage = c(&quot;II&quot;, &quot;I&quot;, &quot;IVB&quot;, &quot;IIIA&quot;, &quot;I&quot;, 
    &quot;II&quot;, &quot;IIIA&quot;, &quot;II&quot;, NA, &quot;IIIB&quot;), ClinGroupStage = c(NA, &quot;IA&quot;, 
    &quot;IVB&quot;, &quot;Unknown/Not Reported&quot;, &quot;III&quot;, NA, &quot;IIB&quot;, &quot;II&quot;, NA, &quot;Unknown/Not Reported&quot;
    )), row.names = c(NA, 10L), class = &quot;data.frame&quot;)
</code></pre>
","<r><dplyr><levels><data-mapping>","2021-11-24 15:51:33","54","2","2","70099237","<p>One option could be:</p>
<pre><code>stages &lt;- c(&quot;Unknown/Not Reported&quot;, &quot;I&quot;, &quot;IA&quot;, &quot;IB&quot;, &quot;II&quot;, &quot;IIA&quot;, &quot;IIB&quot;, &quot;III&quot;, &quot;IIIA&quot;, &quot;IIIB&quot;, &quot;IIIC&quot; ,&quot;IV&quot;, &quot;IVA&quot;, &quot;IVB&quot;, &quot;IVC&quot;)

df %&gt;%
    mutate(across(everything(), ~ factor(., levels = stages, ordered = TRUE)),
           OutputStage = pmax(PathGroupStage, ClinGroupStage, na.rm = TRUE))

   PathGroupStage       ClinGroupStage OutputStage
1              II                 &lt;NA&gt;          II
2               I                   IA          IA
3             IVB                  IVB         IVB
4            IIIA Unknown/Not Reported        IIIA
5               I                  III         III
6              II                 &lt;NA&gt;          II
7            IIIA                  IIB        IIIA
8              II                   II          II
9            &lt;NA&gt;                 &lt;NA&gt;        &lt;NA&gt;
10           IIIB Unknown/Not Reported        IIIB
</code></pre>
"
"70099078","Mapping non-numeric factor to choose higher value between two columns in R","<p>I have a dataframe with two column: PathGroupStage, ClinGroupStage. I want to create a new column, OutputStage, that chooses the higher stage.</p>
<p>Valid value of stage: I, IA, IB, II, IIA, IIB, III, IIIA, IIIB, IIIC ,IV, IVA, IVB, IVC, Unknown.</p>
<ul>
<li>If both stages have values, then use the highest, e.g., IIIB &gt; IIIA &gt; III</li>
<li>If one is missing and the other has value, the use the one with value</li>
<li>If both are missing or unknown, then .= unknown</li>
</ul>
<p>How would I derive the OutputStage variable comparing the non-numeric values from the two columns? I am thinking I need to factor levels but how would I compare the factors between different columns?</p>
<p>Here is the sample dataset:</p>
<pre><code>   PathGroupStage       ClinGroupStage
1              II                 &lt;NA&gt;
2               I                   IA
3             IVB                  IVB
4            IIIA Unknown/Not Reported
5               I                  III
6              II                 &lt;NA&gt;
7            IIIA                  IIB
8              II                   II
9            &lt;NA&gt;                 &lt;NA&gt;
10           IIIB Unknown/Not Reported

 df &lt;- structure(list(PathGroupStage = c(&quot;II&quot;, &quot;I&quot;, &quot;IVB&quot;, &quot;IIIA&quot;, &quot;I&quot;, 
    &quot;II&quot;, &quot;IIIA&quot;, &quot;II&quot;, NA, &quot;IIIB&quot;), ClinGroupStage = c(NA, &quot;IA&quot;, 
    &quot;IVB&quot;, &quot;Unknown/Not Reported&quot;, &quot;III&quot;, NA, &quot;IIB&quot;, &quot;II&quot;, NA, &quot;Unknown/Not Reported&quot;
    )), row.names = c(NA, 10L), class = &quot;data.frame&quot;)
</code></pre>
","<r><dplyr><levels><data-mapping>","2021-11-24 15:51:33","54","2","2","70099392","<pre><code>df &lt;- structure(
    list(
        PathGroupStage = c(&quot;II&quot;, &quot;I&quot;, &quot;IVB&quot;, &quot;IIIA&quot;, &quot;I&quot;, &quot;II&quot;, &quot;IIIA&quot;, &quot;II&quot;, NA, &quot;IIIB&quot;),
        ClinGroupStage = c(NA, &quot;IA&quot;, &quot;IVB&quot;, &quot;Unknown/Not Reported&quot;, &quot;III&quot;, NA, &quot;IIB&quot;, &quot;II&quot;, NA, &quot;Unknown/Not Reported&quot;)
    ),
    row.names = c(NA, 10L), class = &quot;data.frame&quot;
) 

# The variables are not yet factors as far as R is concerned as you can 
# see from the tibble print method
df %&gt;% as_tibble()

stages &lt;- c(&quot;I&quot;, &quot;IA&quot;, &quot;IB&quot;, &quot;II&quot;, &quot;IIA&quot;, &quot;IIB&quot;, &quot;III&quot;, &quot;IIIA&quot;, &quot;IIIB&quot;, &quot;IIIC&quot; ,&quot;IV&quot;, &quot;IVA&quot;, &quot;IVB&quot;, &quot;IVC&quot;, &quot;Unknown/Not Reported&quot;)

df %&gt;%
    as_tibble() %&gt;%
    dplyr::mutate(
        # if we make them ordered factors then they now have values you can do a mathematical operation on
        PathGroupStage = factor(PathGroupStage, levels = stages, ordered = TRUE),
        ClinGroupStage = factor(ClinGroupStage, levels = stages, ordered = TRUE),
        # case when is like a more general if_else() with multiple conditions
        # of the form: logical test ~ result if true
        OutputStage = case_when(
            (is.na(ClinGroupStage) | ClinGroupStage == &quot;Unknown/Not Reported&quot;) &amp; 
            (is.na(PathGroupStage) | PathGroupStage == &quot;Unknown/Not Reported&quot;) ~ 
                factor(&quot;Unknown/Not Reported&quot;, levels = stages, ordered = TRUE),
            is.na(PathGroupStage) ~ ClinGroupStage,
            is.na(ClinGroupStage) ~ PathGroupStage,
            PathGroupStage &gt;= ClinGroupStage ~ PathGroupStage,
            ClinGroupStage &gt;= PathGroupStage ~ ClinGroupStage
        )
    )
</code></pre>
"
"70087102","Designing a DW Model Diagram","<p>I am tasked with modeling out a rough design of a DW. Main purpose is for searching property information and files, orders that are opened on said properties.</p>
<p>I am struggling with the structure when it comes to A File(Parent level transaction) and order. Files are first opened and then orders are opened under those files. This can be a many to many relationship. I was informed that the main Fact table would be the order and the file would be a degenerate dimension.</p>
<p>But there are other tables like Parties on Orders and Files (i.e. property owner, real estate agent etc) OrderParty and FileParty that need to be considered too.</p>
<p>Any thoughts on how to start this model design?</p>
","<attributes><data-warehouse><dimensional-modeling><fact><star-schema-datawarehouse>","2021-11-23 19:58:32","33","0","1","70088880","<p>Dimensional models should be designed based on the queries that they need to answer, not on the structure of any source tables.</p>
<p>Define what you want to measure and this will give you your facts; then define how you want to filter and aggregate these facts and that will give you your dimensions</p>
"
"70067191","How to write a basic ETL using SQL without truncate/load at each ETL run?","<p>I have a OLTP database which I am loading into another database using SQL (ETL). For this, in the 2nd database database I have created the star schema model based on facts and dimensions.</p>
<p>Technology: MS-SQL</p>
<p>Fact: Sales</p>
<p>Dimensions: Customer, Client, Agent</p>
<p>Note: There is no date modified field in the source system, and rows can be deleted in the source system.</p>
<p>I have a agent job that runs every 15 minutes to do the following: For each table, I am loading the data from OLTP db into the respective staging tables (for each dimension and fact table). Then merging the staging dim table into the dim table. Then updating the staging fact table to include the surrogate key of the dimension. Then using merge to sync the staging fact table with the fact table. This is resource consuming because I have to load entire data into the staging tables at each etl run. So I am looking for a better way - if someone can point me to how its done it will help me.</p>
","<sql-server><tsql><ssis><etl><data-warehouse>","2021-11-22 14:14:18","379","0","1","70068283","<blockquote>
<p>This is resource consuming because I have to load entire data into the staging tables at each etl run</p>
</blockquote>
<ol>
<li>Turn on <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/track-changes/about-change-tracking-sql-server?view=sql-server-ver15"" rel=""nofollow noreferrer"">Change Tracking</a> or <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/track-changes/about-change-data-capture-sql-server?view=sql-server-ver15"" rel=""nofollow noreferrer"">Change Data Capture</a> at the source and extract only the changed rows</li>
</ol>
<p>or</p>
<ol start=""2"">
<li>Create a readable replica on the DW server with <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/replication/transactional/transactional-replication?view=sql-server-ver15"" rel=""nofollow noreferrer"">Transactional Replication</a> from the source to the staging tables or using <a href=""https://learn.microsoft.com/en-us/sql/database-engine/availability-groups/windows/configure-read-scale-availability-groups?view=sql-server-ver15"" rel=""nofollow noreferrer"">Read-Scale Replica</a> or <a href=""https://learn.microsoft.com/en-us/sql/database-engine/log-shipping/about-log-shipping-sql-server?view=sql-server-ver15"" rel=""nofollow noreferrer"">Log Shipping</a>.</li>
</ol>
"
"70027600","Why copying a parquet file into snowflake when the parquet schema is missed?","<p>I feel confuse about the next scenario:</p>
<ol>
<li><p>I have a parquet file into S3</p>
</li>
<li><p>I copy the parquet file into Snowflake (specifying compression Snappy and format Parquet).</p>
</li>
<li><p>The file is copied into my table and I can see the raw column, were the parquet value appear like a json. And this is my confusion, this value is json, the parquet schema is lost.</p>
</li>
</ol>
<p>So Im wondering. I know the benefits of using Parquet, but indeed I don't know why copying parquet files into Snowflake is good when you miss an important bit of information, like the schema.</p>
<p>My problem arises when I load raw data from different files with different structure. Then I have to extract and cast the fields of each data-structure, one by one, from multiple tables. I was wondering if there is a better way of doing this a bit more generic relying on schemas while using parquet.</p>
","<snowflake-cloud-data-platform><parquet><data-warehouse>","2021-11-18 22:33:51","980","1","1","70027763","<p>That's my favorite way of working with semi-structured data in Snowflake: Copy it into a VARIANT type.</p>
<p>But good news if you want to work directly with the schema — Snowflake is improving its capacities for auto-detection:</p>
<ul>
<li><a href=""https://www.snowflake.com/blog/schema-detection-public-preview/"" rel=""nofollow noreferrer"">https://www.snowflake.com/blog/schema-detection-public-preview/</a></li>
<li><a href=""https://medium.com/snowflake/snowflake-schema-detection-feature-overview-e24b0a999884"" rel=""nofollow noreferrer"">https://medium.com/snowflake/snowflake-schema-detection-feature-overview-e24b0a999884</a></li>
</ul>
<blockquote>
<p>INFER_SCHEMA retrieves and returns the schema from a set of staged files.</p>
</blockquote>
<blockquote>
<p>GENERATE_COLUMN_DESCRIPTION returns the list of columns necessary to create a table, external table, or view. Here you can modify the columns or data types before you complete the creation of the object.</p>
</blockquote>
<blockquote>
<p>CREATE TABLE … USING TEMPLATE expands upon Snowflake’s CREATE TABLE functionality to automatically create the structured table using the detected schema from the staged files with no additional input.</p>
</blockquote>
"
"69981737","Is Star Schema (data modelling) still relevant with the Lake House pattern using Databricks?","<p>The more I read about the Lake House architectural pattern and following the demos from Databricks I hardly see any discussion around Dimensional Modelling like in a traditional data warehouse (Kimball approach). I understand the compute and storage are much cheaper but are there any bigger impacts in terms of queries performance without the data modelling? In spark 3.0 onwards I see all the cool features like Adaptive Query Engine, Dynamic Partition Pruning etc., but is the dimensional modelling becoming obsolete because of that? If anyone implemented dimensional modelling with Databricks share your thoughts?</p>
","<apache-spark><bigdata><databricks><azure-databricks><apache-spark-3.0>","2021-11-15 22:40:13","1650","6","3","69995276","<p>In our use case we access the lakehouse using PowerBI + Spark SQL and being able to significantly reduce the data volume the queries return by using the star schema makes the experience faster for the end-user and saves compute resources.</p>
<p>However considering things like the columnar nature of parquet files and partition pruning which both also decrease the data volume per query, I can imagine scenarios in which a reasonable setup without star schema could work.</p>
"
"69981737","Is Star Schema (data modelling) still relevant with the Lake House pattern using Databricks?","<p>The more I read about the Lake House architectural pattern and following the demos from Databricks I hardly see any discussion around Dimensional Modelling like in a traditional data warehouse (Kimball approach). I understand the compute and storage are much cheaper but are there any bigger impacts in terms of queries performance without the data modelling? In spark 3.0 onwards I see all the cool features like Adaptive Query Engine, Dynamic Partition Pruning etc., but is the dimensional modelling becoming obsolete because of that? If anyone implemented dimensional modelling with Databricks share your thoughts?</p>
","<apache-spark><bigdata><databricks><azure-databricks><apache-spark-3.0>","2021-11-15 22:40:13","1650","6","3","70011323","<p>Not really a question for here, but interesting.</p>
<p>Of course Databricks et al are selling their Cloud solutions - I'm fine with that.</p>
<p>Taking this video  <a href=""https://go.incorta.com/recording-death-of-the-star-schema"" rel=""nofollow noreferrer"">https://go.incorta.com/recording-death-of-the-star-schema</a> into account - whether paid for or the real opinion of Imhoff:</p>
<ul>
<li>The computing power is higher at lower cost - if you manage it and you can more things on the fly.</li>
<li>That said, the same could be stated with SAP Hana, where you do ETL on the fly. I am not sure why every time I would want to have a virtual creation of a type 2 dimension.</li>
<li>Star schemas require thought and maintenance, but show focus. Performance is less of an issue.</li>
<li>It is true that ad hoc queries do not work well with star schemas over multiple fact tables. Try it.</li>
<li>Databricks has issues with sharing Clusters with SCALA, if you do it their way with pyspark it is OK.</li>
<li>It remains to be seen if querying via Tableau works well on Delta Lake - I need to see it for myself. In the past we had thrift server etc. for this and it did not work, but things are different now.</li>
</ul>
<blockquote>
<p>Where I am now we have Data Lake on HDP with delta format - and a
dimensional SQL Server DWH. The latter due to the on-premises aspects
of HDP.</p>
<p>Not having star schemas means people need more skills to query.</p>
<p>If I took ad hoc querying then I would elect the Lakehouse, but
actually I think you  need both. It's a akin to the discussion do you
need ETL tools if you have Spark.</p>
</blockquote>
"
"69981737","Is Star Schema (data modelling) still relevant with the Lake House pattern using Databricks?","<p>The more I read about the Lake House architectural pattern and following the demos from Databricks I hardly see any discussion around Dimensional Modelling like in a traditional data warehouse (Kimball approach). I understand the compute and storage are much cheaper but are there any bigger impacts in terms of queries performance without the data modelling? In spark 3.0 onwards I see all the cool features like Adaptive Query Engine, Dynamic Partition Pruning etc., but is the dimensional modelling becoming obsolete because of that? If anyone implemented dimensional modelling with Databricks share your thoughts?</p>
","<apache-spark><bigdata><databricks><azure-databricks><apache-spark-3.0>","2021-11-15 22:40:13","1650","6","3","73370167","<p>The Kimball's star schema and Data Vault modeling techniques are still relevant for Lakehouse patterns, and mentioned optimizations, like, Adaptive Query Execution, Dynamic Partition Pruning, etc., combined with Data Skipping, ZOrder, bloom filters, etc. are making queries very efficient.</p>
<p>Really, Databricks data warehousing specialists recently published two related blog posts:</p>
<ul>
<li><a href=""https://www.databricks.com/blog/2022/06/24/data-warehousing-modeling-techniques-and-their-implementation-on-the-databricks-lakehouse-platform.html"" rel=""nofollow noreferrer"">Data Warehousing Modeling Techniques and Their Implementation on the Databricks Lakehouse Platform: Using Data Vaults and Star Schemas on the Lakehouse</a></li>
<li><a href=""https://www.databricks.com/blog/2022/06/24/prescriptive-guidance-for-implementing-a-data-vault-model-on-the-databricks-lakehouse-platform.html"" rel=""nofollow noreferrer"">Prescriptive Guidance for Implementing a Data Vault Model on the Databricks Lakehouse Platform</a></li>
</ul>
"
"69973542","How can we use parallel loading in data warehouse ingest scripts to load into multiple tables at the same time without duplications?","<p>is it possible to load data into multiple tables using <code>INSERT ALL</code> without adding duplications or without using <code>overwrite</code> to accomplish it?</p>
<p>As <code>WHEN</code> clause doesn't support subqueries unless it returns a value to compare with something else, i am currently trying with the following:</p>
<pre><code>INSERT ALL
INTO TEMP (PKEY, TITLE) VALUES ( ID, TITLE_NAME )
SELECT
$1 AS ID,
$2 AS TITLE_NAME
FROM @azure_stg/mti_test.csv (FILE_FORMAT=&gt;'CSV')
WHERE $1 NOT IN (SELECT PKEY FROM TEMP)
</code></pre>
<p>my aim is to add another 3 tables to be filled with data in parallel within the same query without adding duplications. so I tried to add another <code>into table2 select ...</code> but it didn't work.</p>
","<sql><snowflake-cloud-data-platform><data-warehouse><data-ingestion>","2021-11-15 11:17:45","114","1","1","69974566","<p>If I got your question correct, you want to insert into several tables with one statement and without any condition. Main consideration is: Are all INSERT statements using the same source data or different source data?</p>
<p>If all statements are using the same source data, this is the syntax:</p>
<pre><code>insert all
  into t1
  into t1 (c1, c2, c3) values (n2, n1, default)
  into t2 (c1, c2, c3)
  into t2 values (n3, n2, n1)
select n1, n2, n3 from src;
</code></pre>
<p>See here: <a href=""https://docs.snowflake.com/en/sql-reference/sql/insert-multi-table.html"" rel=""nofollow noreferrer"">https://docs.snowflake.com/en/sql-reference/sql/insert-multi-table.html</a></p>
<p>If the statements are based on different SELECT-statements, you have to split them up into single INSERT statements.</p>
"
"69935501","Enable change tracking on all tables in snowflake cloud data warehouse","<p>How to enable change tracking = true on all tables in a schema in snowflake data warehouse.</p>
","<snowflake-cloud-data-platform><snowflake-schema>","2021-11-11 22:06:58","158","2","1","69936010","<p>By running &quot;ALTER TABLE … CHANGE_TRACKING = TRUE&quot; for each table</p>
"
"69909536","Snowflake how to parse any date or date-time values into YYYY-MM-DD?","<p>I am working with Snowflake into getting data from various data sources where date and date-time formats are not the same even within some systems.</p>
<p>We might have:</p>
<p><code>2021-11-04T00:00:00.000+06:00</code></p>
<p>or</p>
<p><code>2021-11-04</code></p>
<p>or even:</p>
<p><code>04/11/2021</code></p>
<p>I tried to use the following:</p>
<pre><code>SELECT ifnull(try_to_date('2021-11-04T00:00:00.000+06:00'), to_date('2021-11-04T00:00:00.000+06:00'))
</code></pre>
<p>But it doesn't work when the date is like <code>04/11/2021</code></p>
<p>I don't want to make the query bigger as it is within a huge ingest script and 1ms counts for us.</p>
<p>Is there way to parse any type of date and date-time into <code>YYYY-MM-DD</code> for a reason that's related to our warehouse ?</p>
","<sql><snowflake-cloud-data-platform><data-warehouse>","2021-11-10 07:52:54","313","2","1","69909964","<p>You can try different formats with a <code>COALESCE</code>, then the first non-null parsed will output the date:</p>
<pre class=""lang-sql prettyprint-override""><code>select coalesce(
  try_to_date(mydate)
  , try_to_date(mydate, 'yyyy-mm-dd')
  , try_to_date(mydate, 'mm/dd/yyyy')) as parsed_dae
</code></pre>
<p>Other specifiers supported:</p>
<ul>
<li><a href=""https://docs.snowflake.com/en/user-guide/date-time-input-output.html#about-the-format-specifiers-in-this-section"" rel=""nofollow noreferrer"">https://docs.snowflake.com/en/user-guide/date-time-input-output.html#about-the-format-specifiers-in-this-section</a></li>
</ul>
"
"69892050","SQL filtering counts in a fact table using a subquery or better alternative","<p>I am trying to summaries data from my Data Warehouse into a fact table and I want to be able to count up the number of transactions per type that there were per customer per month.</p>
<p>Columns being used are: Customer_id, Transaction_id, transaction_date, Transaction_type</p>
<p>Ideally what I want to get is.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Customer</th>
<th style=""text-align: center;"">Month</th>
<th style=""text-align: center;"">transaction_type_1</th>
<th style=""text-align: center;"">transaction_type_2</th>
<th style=""text-align: center;"">Total_transactions</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">12345</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">18</td>
<td style=""text-align: center;"">8</td>
<td style=""text-align: center;"">26</td>
</tr>
<tr>
<td style=""text-align: center;"">12345</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">23</td>
<td style=""text-align: center;"">14</td>
<td style=""text-align: center;"">37</td>
</tr>
<tr>
<td style=""text-align: center;"">67891</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">14</td>
<td style=""text-align: center;"">22</td>
<td style=""text-align: center;"">36</td>
</tr>
</tbody>
</table>
</div>
<p>I have to put it into a subquery but I get the total number of type 1 transactions for all customers in each month. I have tried unsuccessfully to using partition on top of that but now very far outside my level.</p>
<pre><code>Select 
 customer_id, 
 month, 
 count(transactions_id),
 (select count(transactions_id) from DWH where transaction_type = 1),
 (select count(transactions_id) from DWH where transaction_type = 2)
FROM DWH
GROUP BY customer_id, month
</code></pre>
<p>Incorrect table output looks something like this.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Customer</th>
<th style=""text-align: center;"">Month</th>
<th style=""text-align: center;"">transaction_type_1</th>
<th style=""text-align: center;"">transaction_type_2</th>
<th style=""text-align: center;"">Total_transactions</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">12345</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">432</td>
<td style=""text-align: center;"">564</td>
<td style=""text-align: center;"">26</td>
</tr>
<tr>
<td style=""text-align: center;"">12345</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">456</td>
<td style=""text-align: center;"">765</td>
<td style=""text-align: center;"">37</td>
</tr>
</tbody>
</table>
</div>
<p>In a standalone table I can get the information, but I can not incorporate it into the fact table view.</p>
<p>Standalone this works to get individual counts of each type, but I haven't been able to rework that into a select subquery:</p>
<pre><code>select customer_id, month, count(*) 
   FROM DWH 
   WHERE dwh.transaction_type = 1
   Group BY dwh.customer_id, month;
</code></pre>
<p>Any help would be much appreciated.</p>
","<sql><group-by><subquery><data-warehouse><fact-table>","2021-11-09 02:16:31","101","0","1","69892128","<p>You may be getting incorrect results because the filters in the where clause of your respective subqueries do not consider the group by columns i.e.</p>
<p>The table alias <code>d</code> helps us to distinguish between columns used in the outer/general query and the subqueries.</p>
<pre class=""lang-sql prettyprint-override""><code>select 
 d.customer_id, 
 d.month, 
 count(d.transactions_id),
 (
     select count(transactions_id) 
     from DWH 
     where transaction_type = 1 and 
           customer_id = d.customer_id and
           month = d.month
 ) as transaction_type_1,
 (
     select count(transactions_id) 
     from DWH 
     where transaction_type = 2 and 
           customer_id = d.customer_id and
           month = d.month
 ) as transaction_type_2
FROM DWH d
GROUP BY d.customer_id, d.month
</code></pre>
<p>However, while this approach may work, it would be best if you tested this performance wise on your respective database and evaluate the cost metrics/query plan.</p>
<p>Another approach that may be performant uses case expressions to achieve the result and has been included below:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT
     customer_id as Customer,
     month,
     COUNT(
         CASE WHEN transaction_type=1 THEN transactions_id END
     ) as transaction_type_1,
     COUNT(
         CASE WHEN transaction_type=2 THEN transactions_id END
     ) as transaction_type_2,
     COUNT(1) as Total_transactions
FROM
    DWH
GROUP BY
    customer_id, month
</code></pre>
"
"69888507","Dynamic SQL table validation for data quality dimension","<p>I have the following code to test for <code>nulls</code> in a whole table using dynamic sql:</p>
<pre><code>/*Completitud*/
--Housekeeping:
drop table if exists tmp_completitud;
--Declarar variables para el loop:
declare @custom_sql   VARCHAR(max)
declare @tablename as VARCHAR(255) = 'maestrodatoscriticos' --Nombre de tabla a usar.
--Reemplazar '_[dimension]' como &quot;apellido&quot; de la tabla por cada nueva dimension:
set @custom_sql = 'select ''' + @tablename + '_Completitud' + ''' as tabla'
select @custom_sql =
           --Reemplazar query de dimension aqui:
       @custom_sql + ', ' + 'sum(cast(iif(' + c.name + ' is null,0,1) as decimal)) / count(*) as ' + c.name
from sys.columns c
         inner join sys.tables t on c.object_id = t.object_id
where t.name = @tablename
set @custom_sql = @custom_sql + ' into tmp_completitud from ' + @tablename
--print @custom_sql
exec (@custom_sql);
--Poblar tabla de dimensiones con dimension actual:
insert into dimensiones
select *
from tmp_completitud;
</code></pre>
<p>I now want to test for unique values, but I'm having a hard time with aggregate functions inside a subquery. So far I have:</p>
<pre><code>select sum(cast(iif(
            ( select sum(cnt) from ( select count(distinct identificacion) as cnt from maestrodatoscriticos ) as x ) =
            ( select sum(cnt2) from ( select count(identificacion) as cnt2 from maestrodatoscriticos ) as y ), 0,
            1) as decimal)) / count(*)
from maestrodatoscriticos;
</code></pre>
<p>And I would like to somehow integrate the <code>select sum(cast(iif...</code> into the <code>select @custom_sql = ...</code> above. Any ideas?</p>
","<tsql><dynamic-sql><data-quality>","2021-11-08 18:51:35","51","0","1","69889583","<p>I actually resolved this with some help from a co-worker. The code is:</p>
<pre><code>/*Unicidad*/
--Housekeeping:
drop table if exists tmp_unicidad;
--Declarar variables para el loop:
declare @sqluni VARCHAR(max) declare @tableuni as VARCHAR(255) = 'maestrodatoscriticos' --Nombre de tabla a usar.
--Reemplazar '_[dimension]' como &quot;apellido&quot; de la tabla por cada nueva dimension:
set @sqluni = 'select ''' + @tableuni + '_Unicidad' + ''' as tabla'
select @sqluni =
           --Reemplazar query de dimension aqui:
       @sqluni + ', ' + 'count(distinct ' + c.name + ') * 1.00 / count(*) * 1.00 as ' + c.name
from sys.columns c
         inner join sys.tables t on c.object_id = t.object_id
where t.name = @tableuni
set @sqluni = @sqluni + ' into tmp_unicidad from ' + @tableuni
--print @custom_sql
exec (@sqluni);
--Poblar tabla de dimensiones con dimension actual:
insert into dimensiones
select *
from tmp_unicidad;
</code></pre>
"
"69883697","Postman request - how to get data from endpoint based on API documentation","<p>I want to retrieve data from a data warehouse that has a web-based API, I need to use an API key for authentication and use the GET / Customers command to retrieve the list of customers data, but when I am using that same thing in postman, it's returning the same documentation page of the data warehouse?</p>
<p>I am new to this any help will be really appreciated.</p>
<p><a href=""https://i.stack.imgur.com/Qo5Ht.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qo5Ht.png"" alt=""screenshot from the postman app"" /></a></p>
","<api><rest><postman><data-warehouse>","2021-11-08 12:48:02","278","0","1","69883906","<p>The URL doesn't look valid:</p>
<p><a href=""https://i.stack.imgur.com/V2BTA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V2BTA.png"" alt=""enter image description here"" /></a></p>
<p>You need a base URL, endpoint, http method, authentication scheme, and credential or a token etc.</p>
<p>I don't know details about your system and API, so let's see an example:</p>
<ul>
<li>base url is <code>https://stackoverflow.com</code>; your current base url is <code>localhost:4444</code>, is your server running on your machine? If so, it might be correct, but I assumer you're talking about a server running somewhere else, not on your computer</li>
<li>endpoint (path parameter) is <code>/questions/69883697</code>, in your case <code>/customers</code></li>
<li>http method is <code>GET</code> and you find it here in Postman; it also means it will not go into query parameters where you put it:
<a href=""https://i.stack.imgur.com/UOt3n.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UOt3n.png"" alt=""enter image description here"" /></a></li>
<li>authentication scheme - your docs mentions an api key that goes into a header called <code>Authorization</code>, so you have to set it in Headers, not as a query parameter:
<a href=""https://i.stack.imgur.com/AOqV1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AOqV1.png"" alt=""enter image description here"" /></a></li>
</ul>
<p>Read carefully what's on your screen, Postman uses the same language as your API documentation, so if your doc speaks about headers, you need to go into Headers tab in Postman.</p>
"
"69873640","What normal form is a Star schema model in Data Warehouse","<p>In Data warehouse - dimensional modelling, what kind of normal form is Star Schema?</p>
","<data-warehouse><dimensional-modeling><star-schema><star-schema-datawarehouse>","2021-11-07 15:07:28","244","-3","2","69874789","<p>It's not. It is a denormalised model</p>
"
"69873640","What normal form is a Star schema model in Data Warehouse","<p>In Data warehouse - dimensional modelling, what kind of normal form is Star Schema?</p>
","<data-warehouse><dimensional-modeling><star-schema><star-schema-datawarehouse>","2021-11-07 15:07:28","244","-3","2","70254019","<p>Logical all star schema will be 2nd normal form .</p>
<p>AS <strong>&quot;A relation that is in First Normal Form and every non-primary-key attribute is fully functionally dependent on the primary key, then the relation is in Second Normal Form&quot;</strong></p>
"
"69783045","Testing for unstamped minutes in time series data","<p>I have a df of minutely prices and want to establish if there are minutes missing (across a 5 year period). The price is only stamped when there is a transaction so there are some missing minutes.</p>
<p>There are 4 entities in a different column and I would like to know the entity that is missing the minute as well as when it was.</p>
<p>My first inclination is to resample and sum NaNs. What is the best way of doing this?</p>
","<python><time-series><bigdata><data-management>","2021-10-31 00:07:17","44","-1","1","69791763","<p>Until there is a better answer here is how I have dealt with this.
<a href=""https://stackoverflow.com/questions/54783713/merge-with-the-nearest-minute-using-pandas"">Merge with the nearest minute using pandas</a></p>
<p>Write the answer from this question out with the addition of printing all NaN values.</p>
<p><code>df_time = pd.DataFrame({'date':pd.date_range(start='yyyy/mm/dd',end='yyyy/mm/dd', freq='1T')}) df_time.info()</code> this with simple division will confirm you have the right data size</p>
<p><code>df_combined = pd.merge(df_time, df_price, on='date') print(df_combined.isna())</code></p>
<p>I then wanted to have the same price as the previous minute as no transactions of significant difference have occured, I did this through <code>df_combined.ffill()</code></p>
"
"69725702","Inconsistent cold start query performance in Postgres query","<p>We're having issues trying to build queries for a Postgres hosted datamart.  Our query is simple, and contains a modest amount of data.  We've seen some vast differences in the execution time of this query between runs- sometimes taking around 20 seconds, other times taking just 3 seconds- but we cannot seem to see what causes these differences and we're aiming to get consistent results.  There are only 2 tables involved in the query, one representing order rows (OrderItemTransactionFact 2,937,264 rows) and the other recording current stock levels for each item (stocklevels 62,353 rows).  There are no foreign keys due to this being a datamart which we run ETL processes against so require fast loading.</p>
<p>The query is;</p>
<pre><code>select 
    oitf.&quot;SKUId&quot;,
    sum(oitf.&quot;ConvertedLineTotal&quot;) as &quot;totalrevenue&quot;,
    sum(oitf.&quot;Quantity&quot;) as &quot;quantitysold&quot;,
    coalesce (sl.&quot;Available&quot;,0) as &quot;availablestock&quot;
from &quot;OrderItemTransactionFact&quot; oitf
left join stocklevels sl on sl.&quot;SKUId&quot; = oitf.&quot;SKUId&quot;
where 
    oitf.&quot;transactionTypeId&quot; = 2 
    and oitf.&quot;hasComposite&quot; = false
    and oitf.&quot;ReceivedDate&quot; &gt;= extract(epoch from timestamp '2020-07-01 00:00:00') 
    and oitf.&quot;ReceivedDate&quot; &lt;= extract(epoch from timestamp '2021-10-01 00:00:00')
group by 
    oitf.&quot;SKUId&quot;, sl.&quot;Available&quot;
order by oitf.&quot;SKUId&quot;;
</code></pre>
<p>The OrderItemTransactionFact table has a couple indexes;</p>
<pre><code>create index IX_OrderItemTransactionFact_ReceivedDate on public.&quot;OrderItemTransactionFact&quot; (&quot;ReceivedDate&quot; DESC);
create index IX_OrderItemTransactionFact_ReceivedDate_transactionTypeId on public.&quot;OrderItemTransactionFact&quot; (&quot;ReceivedDate&quot; desc, &quot;transactionTypeId&quot;);

</code></pre>
<p>Execution plan output for a 26 second run is;</p>
<pre><code>GroupAggregate  (cost=175096.24..195424.66 rows=813137 width=52) (actual time=24100.268..24874.065 rows=26591 loops=1)
  Group Key: oitf.&quot;SKUId&quot;, sl.&quot;Available&quot;
  Buffers: shared hit=659 read=43311 written=1042
  -&gt;  Sort  (cost=175096.24..177129.08 rows=813137 width=19) (actual time=24100.249..24275.594 rows=916772 loops=1)
        Sort Key: oitf.&quot;SKUId&quot;, sl.&quot;Available&quot;
        Sort Method: quicksort  Memory: 95471kB
        Buffers: shared hit=659 read=43311 written=1042
        -&gt;  Hash Left Join  (cost=20671.85..95274.08 rows=813137 width=19) (actual time=239.392..23127.993 rows=916772 loops=1)
              Hash Cond: (oitf.&quot;SKUId&quot; = sl.&quot;SKUId&quot;)
              Buffers: shared hit=659 read=43311 written=1042
              -&gt;  Bitmap Heap Scan on &quot;OrderItemTransactionFact&quot; oitf  (cost=18091.90..73485.91 rows=738457 width=15) (actual time=200.178..22413.601 rows=701397 loops=1)
                    Recheck Cond: ((&quot;ReceivedDate&quot; &gt;= '1585699200'::double precision) AND (&quot;ReceivedDate&quot; &lt;= '1625097600'::double precision))
                    Filter: ((NOT &quot;hasComposite&quot;) AND (&quot;transactionTypeId&quot; = 2))
                    Rows Removed by Filter: 166349
                    Heap Blocks: exact=40419
                    Buffers: shared hit=55 read=42738 written=1023
                    -&gt;  Bitmap Index Scan on ix_orderitemtransactionfact_receiveddate  (cost=0.00..17907.29 rows=853486 width=0) (actual time=191.274..191.274 rows=867746 loops=1)
                          Index Cond: ((&quot;ReceivedDate&quot; &gt;= '1585699200'::double precision) AND (&quot;ReceivedDate&quot; &lt;= '1625097600'::double precision))
                          Buffers: shared hit=9 read=2365 written=181
              -&gt;  Hash  (cost=1800.53..1800.53 rows=62353 width=8) (actual time=38.978..38.978 rows=62353 loops=1)
                    Buckets: 65536  Batches: 1  Memory Usage: 2948kB
                    Buffers: shared hit=604 read=573 written=19
                    -&gt;  Seq Scan on stocklevels sl  (cost=0.00..1800.53 rows=62353 width=8) (actual time=0.031..24.301 rows=62353 loops=1)
                          Buffers: shared hit=604 read=573 written=19
Planning Time: 0.543 ms
Execution Time: 24889.522 ms
</code></pre>
<p>But then execution plan for the same query when it took just 3 seconds;</p>
<pre><code>GroupAggregate  (cost=173586.52..193692.59 rows=804243 width=52) (actual time=2616.588..3220.394 rows=26848 loops=1)
  Group Key: oitf.&quot;SKUId&quot;, sl.&quot;Available&quot;
  Buffers: shared hit=2 read=43929
  -&gt;  Sort  (cost=173586.52..175597.13 rows=804243 width=19) (actual time=2616.570..2813.571 rows=889937 loops=1)
        Sort Key: oitf.&quot;SKUId&quot;, sl.&quot;Available&quot;
        Sort Method: quicksort  Memory: 93001kB
        Buffers: shared hit=2 read=43929
        -&gt;  Hash Left Join  (cost=20472.48..94701.25 rows=804243 width=19) (actual time=185.018..1512.626 rows=889937 loops=1)
              Hash Cond: (oitf.&quot;SKUId&quot; = sl.&quot;SKUId&quot;)
              Buffers: shared hit=2 read=43929
              -&gt;  Bitmap Heap Scan on &quot;OrderItemTransactionFact&quot; oitf  (cost=17892.54..73123.18 rows=730380 width=15) (actual time=144.000..960.232 rows=689090 loops=1)
                    Recheck Cond: ((&quot;ReceivedDate&quot; &gt;= '1593561600'::double precision) AND (&quot;ReceivedDate&quot; &lt;= '1633046400'::double precision))
                    Filter: ((NOT &quot;hasComposite&quot;) AND (&quot;transactionTypeId&quot; = 2))
                    Rows Removed by Filter: 159949
                    Heap Blocks: exact=40431
                    Buffers: shared read=42754
                    -&gt;  Bitmap Index Scan on ix_orderitemtransactionfact_receiveddate  (cost=0.00..17709.94 rows=844151 width=0) (actual time=134.806..134.806 rows=849039 loops=1)
                          Index Cond: ((&quot;ReceivedDate&quot; &gt;= '1593561600'::double precision) AND (&quot;ReceivedDate&quot; &lt;= '1633046400'::double precision))
                          Buffers: shared read=2323
              -&gt;  Hash  (cost=1800.53..1800.53 rows=62353 width=8) (actual time=40.500..40.500 rows=62353 loops=1)
                    Buckets: 65536  Batches: 1  Memory Usage: 2948kB
                    Buffers: shared hit=2 read=1175
                    -&gt;  Seq Scan on stocklevels sl  (cost=0.00..1800.53 rows=62353 width=8) (actual time=0.025..24.620 rows=62353 loops=1)
                          Buffers: shared hit=2 read=1175
Planning Time: 0.565 ms
Execution Time: 3235.300 ms
</code></pre>
<p>The server config is;
version: PostgreSQL 12.1, compiled by Visual C++ build 1914, 64-bit
work_mem : 1048576kb
shared_buffers : 16384 (x8kb)</p>
<p>Thanks in advance!</p>
","<sql><postgresql><performance><data-warehouse>","2021-10-26 15:14:31","342","-1","1","69730918","<p>It is the filesystem cache.  The slow one had to read the data off disk.  The fast one just had to fetch the data from memory, probably because the slow one already read it and left it there. You can make this show up explicitly in the plans by turning on track_io_timing.</p>
<p>It should help a little to have an index on <code>(&quot;transactionTypeId&quot;,&quot;hasComposite&quot;,&quot;ReceivedDate&quot;)</code>, perhaps a lot to crank up effective_io_concurrency (depending on your storage system).</p>
<p>But mostly, get faster disks.</p>
"
"69713921","Dimensional Modeling event Hierarchy","<p>In my current world, An employer Can Grant Stocks to an Employee under a stock plan. Not only stocks, other types like restricted stock units etc.. can be granted too. Each Grant record has its own attributes (Qty granted, fair market value when granted, etc..). Every Grant record has  multiple vesting information (eg: out of 100 shares granted, 50 can vest in 2021, 30 in 2022, 20 in 2023) . Finally each vesting record can have multiple &quot;planned distribution&quot; records (ie out of 50 vested ones in 2021, 20 can get exercised in Dec 2021 , 20 in Jan 2022 and remaining in Feb 2022). So the hierarchy looks like this:
Employee -&gt; n Grants -&gt; n Vesting -&gt; n Planned Distribution</p>
<p>Wondering what is the prescribed way of Dimensionally modeling this?</p>
<p>Option#1: Treat Grant, Vesting, Planned Distribution as separate Dimension and have a separate factless fact that relates all of these and the employee (However question here is, can they be treated independent dimensions, as child cannot exist/meaningless without the parent)</p>
<p>Option#2: Only have Planned Distribution Fact and collapse the Grant and Vesting into this fact (like kimball order-orderline concept). Thus employee and employer will be the only Dimensions (Drawback- What if Grants and Vesting are required on its own in other facts?)</p>
<p>Option#3: Treat Grant, Vesting, Planned Distribution as separate Dimension, relate them by using natural keys from parent to the child, but also have separate factless fact to relate the Dim Keys of each of this for point in time analysis</p>
<p>Database: snowflake cloud</p>
<p>Thanks in advance
Sunil</p>
","<data-modeling><data-warehouse><dimensional-modeling>","2021-10-25 19:47:49","90","1","1","69721353","<p>You are approaching this from the wrong direction. A dimensional model is based on the business questions you want to be able to answer, not on the data and its structures that you happen to have in your source system.</p>
<p>So you need to define the measures that you want to report on and their grain (which will give you your facts) and the entities that you want to use to filter and aggregate your fact (which will give you your dimensions).</p>
<p>Once you have this information it will become easier (though not necessarily easy!) to design your model and the answers to your questions will either become much more obvious or, possibly, irrelevant.</p>
"
"69702015","Implement Firebase data management in the Front-End vs. Back-End","<p>I'm a quite new JavaScript developer, coming from the Embedded Systems area, i.e., algorithms and programming basic concepts are not an issue for me, but some concepts of web development (front/back end) are still new to me, and many doubts often come up.</p>
<p>Going directly to my question, I'm currently developing a basic HTML + JS application where the user must login to get access to a &quot;private&quot; area; once logged-in, it is possible to perform many service requests and see other users requests, being more or less an &quot;intranet&quot; system.</p>
<p>I'm using <strong>Firebase Firestore</strong> as the database, and adopting two approaches to comunicate with the database:</p>
<p>-For <strong>user authentication management (log-in and sign-up processes)</strong>, I'm taking advantage of the <strong>&quot;front-end&quot;</strong> JS modules provided by Firebase, using the login/password method with the e-mail link confirmation feature activated. Once an auth request is sent, the firebase app returns a token, and based on this token a session cookie is generated, granting access to the private area. This way, I don't need to manage most of the auth / account validation steps in my back-end part.</p>
<p>-For <strong>any other database operations (firestore read &amp; write)</strong>, I'm using the 'firebase-admin' SDK in conjunction with my back-end implementation, in a way that all the requests are validated with the session cookie before being actually performed. The back-end has access to a service account JSON &quot;key&quot;, which gives it total read &amp; write access to the whole database.</p>
<p>Finally, the questions are:</p>
<p>-Is it acceptable or conventional to use both approaches, front-end and back-end, to manage the firebase application?</p>
<p>-Once all the firestore data is being managed in the back-end side, and all the read/write/privilege rules are managed there, is it correct the correct approach to set the security rules to block everything, as follows? (considering that firebase-admin bypasses everything from the back-end side, not being affected)</p>
<pre><code>service cloud.firestore {
  match /databases/{database}/documents {
    match /{document=**} {
      allow read, write: if false;
    }
  }
}
</code></pre>
<p>Best regards, and please let me know if I was clear in my doubts :)</p>
","<javascript><node.js><firebase><google-cloud-firestore><firebase-authentication>","2021-10-25 01:29:09","198","0","1","69702033","<p>Combining front-end and back-end code when using Firestore (or other Firebase modules) is quite normal. Performing all data access through the Admin SDK (while not necessary) is a completely acceptable approach too. Setting the rules to not allow any direct access from client-side SDKs (or with ID tokens through the REST APIs) is indeed a logical step in this scenario.</p>
<p>On thing to realize is that your <strong>code</strong> is now responsible for ensuring all data is authorized, while in cases where you use client-side Firebase SDKs it is the  <strong>security rules</strong> that perform access control.</p>
"
"69697935","Bash shell for data cleansing","<p>I'm new to bash scripting and I am learning scripting for data cleansing. I have a large file which I have managed to cut out the necessary columns and save it to a new file. Need help to achieve the outcome I am looking for.</p>
<pre><code>   2 Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 Media Server Enterprise
   7 Media Server lite
  10 Media server lite 1.0
 468 Media server lite 2.0
   8 Media server lite 2.3
   1 Media server lite 2.4
  40 Media server lite 3.0
   3 Media server lite 3.3
</code></pre>
<p>How could I edit this file to now make the csv file as</p>
<pre><code>   2 | Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 | Media Server Enterprise
   7 | Media Server lite
  10 | Media server lite 1.0
 468 | Media server lite 2.0
   8 | Media server lite 2.3
   1 | Media server lite 2.4
  40 | Media server lite 3.0
   3 | Media server lite 3.3
</code></pre>
","<bash><shell>","2021-10-24 14:54:57","132","-1","4","69698129","<p>I'd rather see you post (parts of) the original data file and show you how it's done all the way with awk, but here's what you asked for using GNU awk (<code>gensub</code>):</p>
<pre><code>$ gawk '{print gensub(/([0-9]+ )/,&quot;\\1| &quot;,1,$0)}' file
</code></pre>
<p>Output:</p>
<pre><code>   2 | Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 | Media Server Enterprise
   7 | Media Server lite
...
</code></pre>
<p><strong>Edit:</strong> Hmm, too much <code>gensub</code> lately I guess, just use awk:</p>
<pre><code>$ awk '{sub(/([0-9]+ )/,&quot;&amp;| &quot;)}1' file
</code></pre>
"
"69697935","Bash shell for data cleansing","<p>I'm new to bash scripting and I am learning scripting for data cleansing. I have a large file which I have managed to cut out the necessary columns and save it to a new file. Need help to achieve the outcome I am looking for.</p>
<pre><code>   2 Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 Media Server Enterprise
   7 Media Server lite
  10 Media server lite 1.0
 468 Media server lite 2.0
   8 Media server lite 2.3
   1 Media server lite 2.4
  40 Media server lite 3.0
   3 Media server lite 3.3
</code></pre>
<p>How could I edit this file to now make the csv file as</p>
<pre><code>   2 | Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 | Media Server Enterprise
   7 | Media Server lite
  10 | Media server lite 1.0
 468 | Media server lite 2.0
   8 | Media server lite 2.3
   1 | Media server lite 2.4
  40 | Media server lite 3.0
   3 | Media server lite 3.3
</code></pre>
","<bash><shell>","2021-10-24 14:54:57","132","-1","4","69699619","<p>Another approach with any <code>awk</code> is to use <code>match()</code> to locate where the first number and whitespace ends and then use <code>substr()</code> to print up to that point, add a <code>&quot;|&quot;</code> and then use <code>substr()</code> again to print from that point to the end, e.g.</p>
<pre><code>awk '{ 
    match($0,/^[ \t0-9]+/)
    print substr($0,0,RLENGTH-1), &quot;|&quot;, substr($0, RLENGTH+1)
}'
</code></pre>
<p><strong>Example Use/Output</strong></p>
<p>With your sample input in the file name <code>media</code>, you would do:</p>
<pre class=""lang-none prettyprint-override""><code>$ awk '{ match($0,/^[ \t0-9]+/); print substr($0,0,RLENGTH-1), &quot;|&quot;, substr($0, RLENGTH+1) }' media
   2 | Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 | Media Server Enterprise
   7 | Media Server lite
  10 | Media server lite 1.0
 468 | Media server lite 2.0
   8 | Media server lite 2.3
   1 | Media server lite 2.4
  40 | Media server lite 3.0
</code></pre>
"
"69697935","Bash shell for data cleansing","<p>I'm new to bash scripting and I am learning scripting for data cleansing. I have a large file which I have managed to cut out the necessary columns and save it to a new file. Need help to achieve the outcome I am looking for.</p>
<pre><code>   2 Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 Media Server Enterprise
   7 Media Server lite
  10 Media server lite 1.0
 468 Media server lite 2.0
   8 Media server lite 2.3
   1 Media server lite 2.4
  40 Media server lite 3.0
   3 Media server lite 3.3
</code></pre>
<p>How could I edit this file to now make the csv file as</p>
<pre><code>   2 | Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 | Media Server Enterprise
   7 | Media Server lite
  10 | Media server lite 1.0
 468 | Media server lite 2.0
   8 | Media server lite 2.3
   1 | Media server lite 2.4
  40 | Media server lite 3.0
   3 | Media server lite 3.3
</code></pre>
","<bash><shell>","2021-10-24 14:54:57","132","-1","4","69699836","<p>All <a href=""/questions/tagged/bash"" class=""post-tag"" title=""show questions tagged &#39;bash&#39;"" rel=""tag"">bash</a> / <a href=""/questions/tagged/zsh"" class=""post-tag"" title=""show questions tagged &#39;zsh&#39;"" rel=""tag"">zsh</a> answer.</p>
<p>If you are not married to the whitespace around the first field you can do:</p>
<pre><code>$ while read -r x rest; do printf &quot;%s|%s\n&quot; &quot;$x&quot; &quot;$rest&quot;; done &lt;file
2|Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
29|Media Server Enterprise
7|Media Server lite
10|Media server lite 1.0
468|Media server lite 2.0
8|Media server lite 2.3
1|Media server lite 2.4
40|Media server lite 3.0
3|Media server lite 3.3
</code></pre>
<p>That result is a single character <code>&quot;|&quot;</code> delimiter CSV file rather than having a three character <code>&quot; | &quot;</code> for a CSV delimiter (which is more difficult to deal with later...)</p>
<p>If you want the first field to be wider and a column, you can use a Bash regex to separate the first field from the rest:</p>
<pre><code>while IFS= read -r line || [[ -n $line ]]; do 
    if [[ $line =~ ^[[:blank:]]*([[:digit:]]+)[[:blank:]]+(.*) ]]; then
        printf &quot;%4s | %s\n&quot; &quot;${BASH_REMATCH[1]}&quot; &quot;${BASH_REMATCH[2]}&quot;
    fi  
done &lt;file 
   2 | Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 | Media Server Enterprise
   7 | Media Server lite
  10 | Media server lite 1.0
 468 | Media server lite 2.0
   8 | Media server lite 2.3
   1 | Media server lite 2.4
  40 | Media server lite 3.0
   3 | Media server lite 3.3
</code></pre>
<hr />
<p>For an <a href=""/questions/tagged/awk"" class=""post-tag"" title=""show questions tagged &#39;awk&#39;"" rel=""tag"">awk</a> answer, I would do:</p>
<pre><code>awk -v de=&quot; | &quot; '
FNR==NR{length($1)&gt;max ? max=length($1) : max=max; next}
{
    s=&quot;&quot;
    for (i=2;i&lt;=NF;i++) s=s ? s OFS $i : $i
    printf &quot; %*s%s%s\n&quot;, max, $1, de, s
}
' file file
   2 | Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 | Media Server Enterprise
   7 | Media Server lite
  10 | Media server lite 1.0
 468 | Media server lite 2.0
   8 | Media server lite 2.3
   1 | Media server lite 2.4
  40 | Media server lite 3.0
   3 | Media server lite 3.3
</code></pre>
"
"69697935","Bash shell for data cleansing","<p>I'm new to bash scripting and I am learning scripting for data cleansing. I have a large file which I have managed to cut out the necessary columns and save it to a new file. Need help to achieve the outcome I am looking for.</p>
<pre><code>   2 Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 Media Server Enterprise
   7 Media Server lite
  10 Media server lite 1.0
 468 Media server lite 2.0
   8 Media server lite 2.3
   1 Media server lite 2.4
  40 Media server lite 3.0
   3 Media server lite 3.3
</code></pre>
<p>How could I edit this file to now make the csv file as</p>
<pre><code>   2 | Media Server Community - WebRTC, MP4, HLS, RTMP&quot;
  29 | Media Server Enterprise
   7 | Media Server lite
  10 | Media server lite 1.0
 468 | Media server lite 2.0
   8 | Media server lite 2.3
   1 | Media server lite 2.4
  40 | Media server lite 3.0
   3 | Media server lite 3.3
</code></pre>
","<bash><shell>","2021-10-24 14:54:57","132","-1","4","69700530","<p>With <code>sed</code>(1)</p>
<pre><code>sed 's/^\([[:space:]]*[[:digit:]]\{1,\}\)/\1 |/' file.txt 
</code></pre>
<hr />
<ul>
<li><p>The <code>^</code> is an anchor which means the start/beginning.</p>
</li>
<li><p>The <code>( )</code> is a capture group, (the <code>(</code> and <code>)</code> needs to be escaped with B.R.E.) what ever pattern is inside it will be in <code>\1</code>. Which is the first capture group.</p>
</li>
<li><p><code>[[:space:]]</code> white space.</p>
</li>
<li><p><code>*</code> is a quantifier that means zero or more.</p>
</li>
<li><p><code>[[:digit:]]</code> is an int.</p>
</li>
<li><p><code>{1,}</code> is a quantifier that means one or more, but the <code>{</code> and <code>}</code> needs to be escape with B.R.E. which is the default regex engine by <code>sed</code>.</p>
</li>
</ul>
<hr />
<p>If the <code>-E</code> flag/option (E.R.E.) is used the escaping can be omitted in the pattern matching.</p>
<pre><code>sed -E 's/^([[:space:]]*[[:digit:]]{1,})/\1 |/' file.txt
</code></pre>
"
"69676027","Getting Error while reading a manual_test.csv file in Jupyter. (Data and warehouse mining, Machine Learning)","<p>I am trying to run a cell in Jupyter notebook and getting permission error number 13. The code is</p>
<pre><code>df_manual_testing = pd.concat([df_fake_manual_testing,df_true_manual_testing], axis = 0)
df_manual_testing.to_csv(&quot;manual_testing.csv&quot;)
</code></pre>
<p>The error which I am getting is:</p>
<pre><code>---------------------------------------------------------------------------
PermissionError                           Traceback (most recent call last)
&lt;ipython-input-16-f14a4d175882&gt; in &lt;module&gt;
      1 df_manual_testing = pd.concat([df_fake_manual_testing,df_true_manual_testing], axis = 0)
----&gt; 2 df_manual_testing.to_csv(&quot;manual_testing.csv&quot;)

~\anaconda\lib\site-packages\pandas\core\generic.py in to_csv(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)
   3202             decimal=decimal,
   3203         )
-&gt; 3204         formatter.save()
   3205 
   3206         if path_or_buf is None:

~\anaconda\lib\site-packages\pandas\io\formats\csvs.py in save(self)
    182             close = False
    183         else:
--&gt; 184             f, handles = get_handle(
    185                 self.path_or_buf,
    186                 self.mode,

~\anaconda\lib\site-packages\pandas\io\common.py in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text)
    426         if encoding:
    427             # Encoding
--&gt; 428             f = open(path_or_buf, mode, encoding=encoding, newline=&quot;&quot;)
    429         elif is_text:
    430             # No explicit encoding

PermissionError: [Errno 13] Permission denied: 'manual_testing.csv'
</code></pre>
<p><em>I cannot understand how to change the permission of my file. I think so I will need to change it from root to user, but I am not sure exactly how to do this.</em></p>
","<machine-learning><jupyter-notebook><logistic-regression><data-mining><data-warehouse>","2021-10-22 11:23:34","32","0","1","69679240","<p>If you are not interested in writing the csv file into the current working directory, you could simply specify the full path of a folder in which you are sure you have the permissions to write.</p>
<p>For example:</p>
<p><code>df_manual_testing.to_csv(&quot;C:/Users/../manual_testing.csv&quot;)</code></p>
<p>However, if you want to write in a particular folder, you can check from the terminal if you have the permissions to write here, using the command <code>ls -lh</code>. Eventually, you could change the permissions using the account of the owner of the folder, with the command <code>chmod 777 myfolder</code>.</p>
<p>If you need more information about file permissions, you could look at <a href=""https://www.linux.org/threads/file-permissions-chmod.4124/"" rel=""nofollow noreferrer"">this reference</a>.</p>
"
"69673664","Deleted_Flg and Effective End Date in Soft Delete","<p>I have implemented soft delete in warehouse and added an attribute called deleted_flg in the table and values would be 0 = Not Deleted, 1 = Deleted.</p>
<p>However i was thinking of another approach of keeping an attribute called effective_end_date which would be populated with the date incase of record is deleted and Null if record is active.</p>
<p>Should I keep both the attribute or only one of them.</p>
","<data-modeling><data-warehouse><datamodel><snowflake-schema>","2021-10-22 08:24:39","75","0","1","69824831","<p>There is no need, really, to implement soft delete in more than one way, you should decide which of those approaches suits you best and go forward with it, still remembering that instead of a DELETE you are making an UPDATE action, which has its own additional checks and tasks, such as maintaining referential integrity, changing some of the views etc.</p>
"
"69670565","Does the dimension table support concurrent writing?","<p>I have created a dimension table in <code>dolphindb</code> database. Now I want to write the data in it concurrently. Does a dimension table support the concurrent writing?</p>
","<data-warehouse><dolphindb>","2021-10-22 01:52:20","30","0","1","69733542","<p>Starting from the version 1.30.13 , a new configuration of enableConcurrentDimensionalTableWrite is added, which allows the dimension table to do parallel writing.</p>
"
"69666201","Generate list of distinct dimensions in a table","<p>I have a table with 3 dimensions- A, B, and C.
I essentially want values of all possible combinations for these dimensions and populate all measures(M) as 0 when a combination isn't present.</p>
<p>Suppose I have the table-</p>
<p><a href=""https://i.stack.imgur.com/2Lzuv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2Lzuv.png"" alt=""enter image description here"" /></a></p>
<p>If I do this I get -</p>
<pre><code>
select a,b,c from sum(m) fact group by a,b,c

</code></pre>
<p><a href=""https://i.stack.imgur.com/3NUDj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3NUDj.png"" alt=""enter image description here"" /></a></p>
<p>But I would like all possible combinations, -</p>
<p><a href=""https://i.stack.imgur.com/mppU7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mppU7.png"" alt=""enter image description here"" /></a></p>
<p>Currently, I a doing a cross join like below, but is there some faster way to do this (as my table has about ~1M records)?  -</p>
<pre><code>select * from (
select distinct f1.a, f2.b, f3.c 
from fact f1 
cross join fact f2 
cross join fact f3 ) all
left join 
( select a,b,c from sum(m) fact group by a,b,c) s
on all.a=s.a and all.b=s.b and all.c=s.c

</code></pre>
","<mysql><sql><database><oracle><data-warehouse>","2021-10-21 17:16:06","145","0","2","69668305","<p>If this is Oracle Database, then this is exactly what <code>cube</code> is for.</p>
<pre><code>select a, b, c, sum(m) 
from my_table 
group by cube(a,b,c)
</code></pre>
"
"69666201","Generate list of distinct dimensions in a table","<p>I have a table with 3 dimensions- A, B, and C.
I essentially want values of all possible combinations for these dimensions and populate all measures(M) as 0 when a combination isn't present.</p>
<p>Suppose I have the table-</p>
<p><a href=""https://i.stack.imgur.com/2Lzuv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2Lzuv.png"" alt=""enter image description here"" /></a></p>
<p>If I do this I get -</p>
<pre><code>
select a,b,c from sum(m) fact group by a,b,c

</code></pre>
<p><a href=""https://i.stack.imgur.com/3NUDj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3NUDj.png"" alt=""enter image description here"" /></a></p>
<p>But I would like all possible combinations, -</p>
<p><a href=""https://i.stack.imgur.com/mppU7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mppU7.png"" alt=""enter image description here"" /></a></p>
<p>Currently, I a doing a cross join like below, but is there some faster way to do this (as my table has about ~1M records)?  -</p>
<pre><code>select * from (
select distinct f1.a, f2.b, f3.c 
from fact f1 
cross join fact f2 
cross join fact f3 ) all
left join 
( select a,b,c from sum(m) fact group by a,b,c) s
on all.a=s.a and all.b=s.b and all.c=s.c

</code></pre>
","<mysql><sql><database><oracle><data-warehouse>","2021-10-21 17:16:06","145","0","2","69683873","<p>MySQL:</p>
<p><code>GROUP BY a,b,c</code> will produce 1 row per combination that exists in the table.</p>
<p>If you want all possible combinations, you need to build 1 (or 3) more tables to list all the possible values, then do a <code>LEFT JOIN</code> from them.  You may also want <code>COALESCE(col, 0)</code> to turn <code>NULLs</code> into zeros.</p>
"
"69650366","Can a Star Schema Contain Outriggers?","<p>My understanding from Star Schema modelling is that dimensions must directly link to a fact table. If this is true, then a star schema can't contain Outrigger dimension table. So, is this statement true that if a schema contains Outrigger dimension, it is snowflake?</p>
<p>Thank you</p>
","<data-modeling><data-warehouse><snowflake-schema><star-schema>","2021-10-20 17:12:32","77","2","1","69667192","<p>Neither of the terms “star schema” or “snowflake” have precise definitions that are generally accepted. If you want to describe your model as a star schema, a star schema with outriggers or a snowflake then that is entirely up to you.</p>
"
"69649256","Valid From and Valid To columns - Only Show Latest Row based on Value between Dates","<p>Tearing my hair out over this one and not 100% sure it's possible, but everything is possible in SQL so it must be?!</p>
<p>The aim is to be able to historically track how many active accounts there were at the end of each month.</p>
<p>I have two tables:</p>
<ul>
<li><p>star_UserAccounts</p>
<ul>
<li>Holds account information every hour.</li>
<li>If the data is the same, nothing happens.</li>
<li>If the data changes, the existing row is flagged as old and the new row is inserted.  This enables a nice little history to see what changes are being made.</li>
</ul>
</li>
<li><p>LastDayOfMonth</p>
<ul>
<li>Holds dates that I'd like the data for (just as an example)</li>
</ul>
</li>
</ul>
<p>Within the &quot;star_UserAccounts&quot; table I have (Ascending):</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>UserId</th>
<th>UserStatus</th>
<th>InsertedDate</th>
<th>ValidToDate</th>
<th>Row Version</th>
</tr>
</thead>
<tbody>
<tr>
<td>JoeBloggs</td>
<td>Active</td>
<td>2019-07-19 13:43:09.083</td>
<td>2019-10-31 16:08:27.633</td>
<td>1</td>
</tr>
<tr>
<td>JoeBloggs</td>
<td>Active</td>
<td>2019-10-31 16:08:28.027</td>
<td>2020-01-09 10:08:27.840</td>
<td>2</td>
</tr>
<tr>
<td>JoeBloggs</td>
<td>Active</td>
<td>2020-01-09 10:08:28.013</td>
<td>2020-01-09 11:08:28.813</td>
<td>3</td>
</tr>
<tr>
<td>JoeBloggs</td>
<td>Active</td>
<td>2020-01-09 11:08:28.970</td>
<td>2020-01-16 11:08:24.547</td>
<td>4</td>
</tr>
</tbody>
</table>
</div>
<p>Now - I would like to return the details that were valid on the 31st October 2019.  There were two updates on that day and therefore the LATEST RowVersion for that day should be returned.</p>
<p>I can get the individual row version for the account when hardcoding the date into my select script, however when I then try to apply this across multiple different dates in one go, it is picking up both rows for the 31st October 2019, but I only want it to fetch the latest row on the days needed.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>LastDayOfMonth</th>
<th>ActiveAtDate</th>
</tr>
</thead>
<tbody>
<tr>
<td>2019-07-31</td>
<td>1</td>
</tr>
<tr>
<td>2019-08-31</td>
<td>1</td>
</tr>
<tr>
<td>2019-09-30</td>
<td>1</td>
</tr>
<tr>
<td>2019-10-31</td>
<td>2  &lt;-- The problem row</td>
</tr>
<tr>
<td>2019-11-30</td>
<td>1</td>
</tr>
<tr>
<td>2019-12-31</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>How an earth can I make sure the latest row is pulled for each date that is being pushed in?</p>
<p>Here is a SQL Fiddle to play with:  <a href=""http://sqlfiddle.com/#!18/ed372/12"" rel=""nofollow noreferrer"">http://sqlfiddle.com/#!18/ed372/12</a></p>
<p>I have kept in the syntax error which is within the subquery, as this sort of shows what I am trying to achieve as I need to pass the date into the sub query that collects the MAX record for the date.</p>
<p>Appreciate any assistance.  Such a hard one to explain in an easy way, apologies if it's very confusing.</p>
<p>Thanks.</p>
","<sql><sql-server><tsql><data-warehouse>","2021-10-20 15:52:55","437","0","2","69650087","<p><strong>UPDATE</strong></p>
<p>After re-reading your post I realized I had not fully understood your question prior to answering. I will leave my original answer as it may prove to be helpful.</p>
<p>Your situation may be as simple as using <code>COUNT DISTINCT</code>, like so:</p>
<pre><code>SELECT
    LastDayOfMonth,
    COUNT ( DISTINCT UserId ) AS ActiveAtDate
FROM @star_UserAccounts AS ua
CROSS APPLY (

    SELECT LastDayOfMonth FROM @LastDayOfMonth AS ld
        WHERE ld.LastDayOfMonth BETWEEN CAST( ua.InsertedDate AS date ) AND CAST( ua.ValidToDate AS date )
        
) AS x
GROUP BY
    LastDayOfMonth
ORDER BY
    LastDayOfMonth;
</code></pre>
<p><strong>RETURNS</strong></p>
<pre><code>+----------------+--------------+
| LastDayOfMonth | ActiveAtDate |
+----------------+--------------+
| 2019-07-31     |            1 |
| 2019-08-31     |            1 |
| 2019-09-30     |            1 |
| 2019-10-31     |            1 |
| 2019-11-30     |            1 |
| 2019-12-31     |            1 |
+----------------+--------------+
</code></pre>
<p>Using <code>CROSS APPLY</code> still has the same effect in limiting the resultset to users with activity during the period in the LastDayOfMonth table.</p>
<p><strong>ORIGINAL ANSWER BELOW</strong></p>
<p>Try something like the following:</p>
<pre><code>DECLARE @star_UserAccounts table (
    [UserId] nvarchar (20) COLLATE Latin1_General_CI_AS NOT NULL,
    [InsertedDate] datetime NULL,
    [ValidToDate] datetime NULL,
    [RowVersion] int NOT NULL
);

INSERT INTO @star_UserAccounts ( [UserId], [InsertedDate], [ValidToDate], [RowVersion] )
VALUES
    ('JoeBloggs', '2020-01-09 11:08:28.970', '2020-01-16 11:08:24.547', 4 ), 
    ('JoeBloggs', '2020-01-09 10:08:28.013', '2020-01-09 11:08:28.813', 3 ), 
    ('JoeBloggs', '2019-10-31 16:08:28.027', '2020-01-09 10:08:27.840', 2 ), 
    ('JoeBloggs', '2019-07-19 13:43:09.083', '2019-10-31 16:08:27.633', 1 );

DECLARE @LastDayOFMonth table (
    LastDayOfMonth date
);

INSERT INTO @LastDayOFMonth ([LastDayOfMonth])
VALUES
    ('2019-07-31 00:00:00' ), 
    ('2019-08-31 00:00:00' ), 
    ('2019-09-30 00:00:00' ), 
    ('2019-10-31 00:00:00' ), 
    ('2019-11-30 00:00:00' ), 
    ('2019-12-31 00:00:00' );

;WITH current_activity AS (
    SELECT
        UserID, MAX( RowVersion ) AS CurrentVersion
    FROM @star_UserAccounts
    GROUP BY
        UserID
)
SELECT
    ua.*
FROM current_activity
INNER JOIN @star_UserAccounts AS ua
    ON current_activity.UserID = ua.UserID
    AND current_activity.CurrentVersion = ua.[RowVersion]
WHERE EXISTS (
    
    SELECT * FROM @star_UserAccounts AS u
    CROSS APPLY (
        SELECT * FROM @LastDayOfMonth AS ld
            WHERE CAST( ld.LastDayOfMonth AS date ) BETWEEN CAST( u.InsertedDate AS date ) AND CAST( u.ValidToDate AS date )
    ) AS d
    WHERE
        u.UserId = current_activity.UserId

)
ORDER BY
    current_activity.UserId;
</code></pre>
<p>RETURNS</p>
<pre><code>+-----------+-------------------------+-------------------------+------------+
|  UserId   |      InsertedDate       |       ValidToDate       | RowVersion |
+-----------+-------------------------+-------------------------+------------+
| JoeBloggs | 2020-01-09 11:08:28.970 | 2020-01-16 11:08:24.547 |          4 |
+-----------+-------------------------+-------------------------+------------+
</code></pre>
<p>The CTE gets the most current <strong>UserId</strong>'s <strong>RowVersion</strong> and then looks to see if the user had any activity during the dates listed in <strong>LastDayOfMonth</strong>.</p>
<p>Using <code>CROSS APPLY</code> restricts the results to users with any activity that includes the LastDayOfMonth.</p>
"
"69649256","Valid From and Valid To columns - Only Show Latest Row based on Value between Dates","<p>Tearing my hair out over this one and not 100% sure it's possible, but everything is possible in SQL so it must be?!</p>
<p>The aim is to be able to historically track how many active accounts there were at the end of each month.</p>
<p>I have two tables:</p>
<ul>
<li><p>star_UserAccounts</p>
<ul>
<li>Holds account information every hour.</li>
<li>If the data is the same, nothing happens.</li>
<li>If the data changes, the existing row is flagged as old and the new row is inserted.  This enables a nice little history to see what changes are being made.</li>
</ul>
</li>
<li><p>LastDayOfMonth</p>
<ul>
<li>Holds dates that I'd like the data for (just as an example)</li>
</ul>
</li>
</ul>
<p>Within the &quot;star_UserAccounts&quot; table I have (Ascending):</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>UserId</th>
<th>UserStatus</th>
<th>InsertedDate</th>
<th>ValidToDate</th>
<th>Row Version</th>
</tr>
</thead>
<tbody>
<tr>
<td>JoeBloggs</td>
<td>Active</td>
<td>2019-07-19 13:43:09.083</td>
<td>2019-10-31 16:08:27.633</td>
<td>1</td>
</tr>
<tr>
<td>JoeBloggs</td>
<td>Active</td>
<td>2019-10-31 16:08:28.027</td>
<td>2020-01-09 10:08:27.840</td>
<td>2</td>
</tr>
<tr>
<td>JoeBloggs</td>
<td>Active</td>
<td>2020-01-09 10:08:28.013</td>
<td>2020-01-09 11:08:28.813</td>
<td>3</td>
</tr>
<tr>
<td>JoeBloggs</td>
<td>Active</td>
<td>2020-01-09 11:08:28.970</td>
<td>2020-01-16 11:08:24.547</td>
<td>4</td>
</tr>
</tbody>
</table>
</div>
<p>Now - I would like to return the details that were valid on the 31st October 2019.  There were two updates on that day and therefore the LATEST RowVersion for that day should be returned.</p>
<p>I can get the individual row version for the account when hardcoding the date into my select script, however when I then try to apply this across multiple different dates in one go, it is picking up both rows for the 31st October 2019, but I only want it to fetch the latest row on the days needed.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>LastDayOfMonth</th>
<th>ActiveAtDate</th>
</tr>
</thead>
<tbody>
<tr>
<td>2019-07-31</td>
<td>1</td>
</tr>
<tr>
<td>2019-08-31</td>
<td>1</td>
</tr>
<tr>
<td>2019-09-30</td>
<td>1</td>
</tr>
<tr>
<td>2019-10-31</td>
<td>2  &lt;-- The problem row</td>
</tr>
<tr>
<td>2019-11-30</td>
<td>1</td>
</tr>
<tr>
<td>2019-12-31</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>How an earth can I make sure the latest row is pulled for each date that is being pushed in?</p>
<p>Here is a SQL Fiddle to play with:  <a href=""http://sqlfiddle.com/#!18/ed372/12"" rel=""nofollow noreferrer"">http://sqlfiddle.com/#!18/ed372/12</a></p>
<p>I have kept in the syntax error which is within the subquery, as this sort of shows what I am trying to achieve as I need to pass the date into the sub query that collects the MAX record for the date.</p>
<p>Appreciate any assistance.  Such a hard one to explain in an easy way, apologies if it's very confusing.</p>
<p>Thanks.</p>
","<sql><sql-server><tsql><data-warehouse>","2021-10-20 15:52:55","437","0","2","69652199","<p>You are over-complicating it. You can use simple row-numbering to do this</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT
    LastDayOfMonth,
    COUNT(*) ActiveAtDate
FROM (
    SELECT
      ld.LastDayOfMonth,
      ua.UserId,
      rn = ROW_NUMBER() OVER (PARTITION BY ld.LastDayOfMonth, ua.UserId ORDER BY ua.ValidToDate DESC)
    FROM dbo.star_UserAccounts ua
    INNER JOIN LastDayOfMonth ld
      ON ld.LastDayOfMonth &gt;= ua.InsertedDate
      AND DATEADD(day, 1, ld.LastDayOfMonth) &lt; ua.ValidToDate
    WHERE ua.UserId = 'JoeBloggs'
) latestRowForDate
WHERE rn = 1
GROUP BY LastDayOfMonth;
</code></pre>
<p><a href=""http://sqlfiddle.com/#!18/b56c9/1"" rel=""nofollow noreferrer"">SQL Fiddle</a></p>
"
"69608342","How to create the LINK in data vault without having strong relationship keys (Foreign Keys)?","<p>Our sales representatives call Leads to propose services. Leads are stored in Crm with attributes: LeadId, PrimaryContactNumber, SecondaryContactNumber. All calls are done via Teams and recorded. Calls could be extracted via Microsoft Graph Api and have attributes: CallId, UserId, CalleeNumber, CallerNumber, Duration. Users have these attributes: UserId, Username, Email. How can I generate LINK (relation between User, Lead) using data vault modeling when foreign keys are not known? I thought my design should look like this: User(hub)-Call(link)-Lead(hub), but my call has only userId, leadId could only be inferred from one of the Lead attributes(PrimaryContactNumber or SecondaryContactNumber) What is the best solution to this problem? Or should I model Call as a hub aswell and perform filter when loading data to datamart?</p>
","<data-modeling><data-warehouse><data-vault>","2021-10-17 20:30:16","552","0","1","69693960","<p>A call is not a business entities, it's rather a relation between a sales person and a lead. So, your first thought is right.</p>
<p>To answers your question, you might want take a look at the zero/ghost records concept.  <a href=""http://roelantvos.com/blog/unknown-keys-zero-keys-or-ghost-keys-in-hubs-for-dv2-0/"" rel=""nofollow noreferrer"">http://roelantvos.com/blog/unknown-keys-zero-keys-or-ghost-keys-in-hubs-for-dv2-0/</a></p>
<p>Basically, add a &quot;Unknown&quot; entry in your Lead(Hub). Then, when you generate the link and the Lead is not known, simply link it to the unknown entities.  Make sure you add a satellite to you link, so you can track the period it was unknown, and when it is known.</p>
"
"69595952","Joining date dimension multiple times? - Kimball's book on Data warehouse and Dimension Modeling","<p>I'm reading Ralph Kimball's book on Data warehouse and Dimension Modeling and in chapter 6, there is this part about dimension role playing.</p>
<blockquote>
<p>Sometimes you discover other dates associated with each transaction,
such as the requested ship date for the order. Each of the dates
should be a foreign key in the fact table...
<strong>However, you cannot simply join these two foreign keys to the same date dimension table. SQL would interpret this two-way simultaneous
join as requiring both the dates to be identical, which isn’t very
likely.</strong></p>
</blockquote>
<p>And I am not sure I understand the two last sentences. Does it mean you cannot join the date dimension multiple times if both dates in fact table have different value? Why?</p>
","<database><data-warehouse><dimension><dimensional-modeling><kimball>","2021-10-16 13:18:53","719","3","2","69596178","<p>It’s not very well expressed but all that it is saying is that you need to alias the date dimension if you are going to join to it multiple times from different FKs in the Fact table.</p>
<p>This is true of any SQL statement where 2 tables are joined together more than once, it’s not specific to dimensional modelling.</p>
"
"69595952","Joining date dimension multiple times? - Kimball's book on Data warehouse and Dimension Modeling","<p>I'm reading Ralph Kimball's book on Data warehouse and Dimension Modeling and in chapter 6, there is this part about dimension role playing.</p>
<blockquote>
<p>Sometimes you discover other dates associated with each transaction,
such as the requested ship date for the order. Each of the dates
should be a foreign key in the fact table...
<strong>However, you cannot simply join these two foreign keys to the same date dimension table. SQL would interpret this two-way simultaneous
join as requiring both the dates to be identical, which isn’t very
likely.</strong></p>
</blockquote>
<p>And I am not sure I understand the two last sentences. Does it mean you cannot join the date dimension multiple times if both dates in fact table have different value? Why?</p>
","<database><data-warehouse><dimension><dimensional-modeling><kimball>","2021-10-16 13:18:53","719","3","2","69598540","<p>The (well) hidden message here is that you need <em>multiple</em> joins - <em>one for each dimension role</em></p>
<p>Say you have a fact table with <em>entry date</em> <code>entry_d</code> and <em>booking date</em> <code>booking_d</code></p>
<p>This would be <strong>wrong</strong> and is <em>probably</em> meant in the text</p>
<pre><code>select * from fact
join time_dim tim
on fact.entry_d = tim.date_d and
   fact.book_d = tim.date_d;
</code></pre>
<p>And this is <strong>right</strong> using two independent joins to the time dimension</p>
<pre><code>select * from fact
join time_dim entr on fact.entry_d = entr.date_d  
join time_dim book on fact.book_d = book.date_d;
</code></pre>
<p>Note also that if you use an <em>inner join</em> as in the example above you shoud make some elementary validaton and clenaing of both dates. The point is: you should recognise <em>fact rows</em> with invalid dates (not covered in time dimension or <code>NULL</code>) and handle them propertly - not <em>discard them silently</em> in the join.</p>
<p>For non trivial setups, especially when the fact table is <em>partitioned</em> on a time column, you choose the native <code>DATE</code> format and not some <em>surrogate key</em> as for other dimensions. (This is a <em>practical</em> rule - not covered in the <em>theory</em>)</p>
"
"69585552","The difference between Kimball and Inmon modelling approach","<p>I know that Inmon is using <code>up-to-bottom</code> and 3NF approach, while Kimball is using <code>bottom-to-up</code> and dimensional approach.</p>
<p>I understand the 3NF and dimensional modelling, but I really can't understand what <code>bottom</code> and <code>up</code> really mean here, I have read many materials on the internet, it seems that no one has explained clearly what they mean.</p>
<p>What have I missed?</p>
","<data-modeling><data-warehouse>","2021-10-15 13:33:29","62","0","1","69585811","<p>from my understanding,bottom here means <code>part</code>, while up means <code>whole</code>, that they really mean from whole to part approach, and from part to whole approach.</p>
"
"69573524","Where in the stack to best merge analytical data-warehouse data with data scraped+cached from third-party APIs?","<h4>Background information</h4>
<p>We sell an API to users, that analyzes and presents corporate financial-portfolio data derived from public records.</p>
<p>We have an &quot;analytical data warehouse&quot; that contains all the raw data used to calculate the financial portfolios. This data warehouse is fed by an ETL pipeline, and so isn't &quot;owned&quot; by our API server per se. (E.g. the API server only has read-only permissions to the analytical data warehouse; the schema migrations for the data in the data warehouse live alongside the ETL pipeline rather than alongside the API server; etc.)</p>
<p>We also have a small document store (actually a Redis instance with persistence configured) that <em>is</em> owned by the API layer. The API layer runs various jobs to write into this store, and then queries data back as needed. You can think of this store as a shared persistent cache of various bits of the API layer's in-memory state. The API layer stores things like API-key blacklists in here.</p>
<h4>Problem statement</h4>
<p>All our input data is denominated in USD, and our calculations occur in USD. However, we give our customers the query-time option to convert the response just-in-time to another currency. We do this by having the API layer run a background job to scrape exchange-rate data, and then cache it in the document store. Individual API-layer nodes then do (in-memory-cached-with-TTL) fetches from this exchange-rates key in the store, whenever a query result needs to be translated into a specific currency.</p>
<p>At first, we thought that this unit conversion wasn't really &quot;about&quot; our data, just about the API's UX, and so we thought this was entirely an API-layer concern, where it made sense to store the exchange-rates data into our document store.</p>
<p>(Also, we noticed that, by not pre-converting our DB results into a specific currency on the DB side, the calculated results of a query for a particular portfolio became more cache-friendly; the way we're doing things, we can cache and reuse the portfolio query results between queries, even if the queries want the results in different currencies.)</p>
<p>But recently we've been expanding into also allowing partner clients to also execute complex data-science/Business Intelligence queries directly against our analytical data warehouse. And it turns out that they will also, often, need to do final exchange-rate conversions in their BI queries as well—despite there being no API layer involved here.</p>
<p>It seems like, to serve the needs of BI querying, the exchange-rate data &quot;should&quot; actually live in the analytical data warehouse alongside the financial data; and the ETL pipeline &quot;should&quot; be responsible for doing the API scraping required to fetch and feed in the exchange-rate data.</p>
<p>But this <em>feels</em> wrong: the exchange-rate data has a different lifecycle and integrity constraints than our financial data. The exchange rates are dirty and ephemeral point-in-time samples attained by scraping, whereas the financial data is a reliable historical event stream. The exchange rates get constantly updated/overwritten, while the financial data is append-only. Etc.</p>
<p>What is the best practice for serving the needs of analytical queries that need to access backend &quot;application state&quot; for &quot;query result presentation&quot; needs like this? Or am I wrong in thinking of this exchange-rate data as &quot;application state&quot; in the first place?</p>
","<caching><data-warehouse><cqrs>","2021-10-14 15:35:26","26","0","1","69574779","<p>What I find interesting about your scenario is about when the exchange rate data is applicable.</p>
<p>In the case of the API, it's all about the realtime value in the other currency and it makes sense to have the most recent value in your API app scope (Redis).</p>
<p>However, I assume your analytical data warehouse has tables with purchases that were made at a certain time. In those cases, the current exchange rate is not really relevant to the value of the transaction.</p>
<p>This might mean that you want to store the exchange rate history in your warehouse or expand the &quot;purchases&quot; table to store the values in all the currencies at that moment.</p>
"
"69547729","Netezza code to convert data ending with minus symbol and instead having it start with minus","<p>In a table I have numbers in the format of character datatype with both minus and plus figures. The issue is the ones having minus which looks like for instance 1.000- (instead of being -1.000)
The ones that are plus is looking like 1.000 which is all fine hence only the ones supposed to be minus I need to correct.</p>
<p>Below code is the closest I have reached but problem is that when testing it everything becomes minus even if there are plus figures as well. It have been casted to int datatype from being character in order to be able to sum it.</p>
<p><code>cast('-' || SUBSTR(Numbers, 1, LENGTH(Numbers) - 1)as int)</code></p>
<p>So to summarize my question how can I have the character data in &quot;Numbers&quot; column to become -1 if its 1- and do nothing if its 1 in IBM Netezza database?</p>
<p>Thanks!</p>
","<sql><data-warehouse><netezza>","2021-10-12 22:37:08","75","0","1","69549250","<p>The <a href=""https://www.ibm.com/docs/en/netezza?topic=extensions-conversion-functions"" rel=""nofollow noreferrer"">to_number</a> function in your friend in such cases</p>
<pre class=""lang-sql prettyprint-override""><code>select * from test;
   C1
--------
 1.000-
 21-
 1.2
 2
 100-
 1.23-
 213.01
(7 rows)
</code></pre>
<p>Now we can convert it to the right text by doing</p>
<pre class=""lang-sql prettyprint-override""><code> select to_number(c1, '9999D999S') from test;
 TO_NUMBER
-----------
    -1.000
   -21.000
     1.200
     2.000
  -100.000
    -1.230
   213.010
(7 rows)

</code></pre>
"
"69542388","Handling Deletes in SCD Type 2","<p>As a Modeler trying to find out what is the best way to handle deletes in SCD Type 2 tables.</p>
<p>As per principle an SCD Type 2 table is to track changes using ETL dates like START_DT and END_DT.</p>
<ol>
<li>START_DT will be the date the record is effective from.</li>
<li>END_DT will be the date it changed to another form or Null/High Date to denote current version of record.</li>
<li>At all point of time, for a Key Combination there will be a Current Version record with END_DT either Null or High Date.</li>
</ol>
<p>Now if a record is deleted from Source, what is the best option from below,</p>
<ol>
<li>Have additional column like SRC_DELETE_IND which is set 'N' as default and 'Y' if record is deleted from source.</li>
</ol>
<pre><code>Ex: Record came on 1st Oct
PK_ID START_DT    END_DT     VALUE SRC_DELETE_IND
1     2021-10-01  Null       ABC    N
Record had an update on 2nd Oct
PK_ID START_DT    END_DT     VALUE SRC_DELETE_IND
1     2021-10-01  2021-10-02  ABC    N
1     2021-10-02  Null        XYZ    N
Record got deleted on 3rd Oct
PK_ID START_DT    END_DT     VALUE SRC_DELETE_IND
1     2021-10-01  2021-10-02  ABC    N
1     2021-10-02  Null        XYZ    Y
</code></pre>
<ol start=""2"">
<li>Same as 1 but insert new duplicate row when Delete Came.</li>
</ol>
<pre><code>Record got deleted on 3rd Oct
PK_ID START_DT    END_DT     VALUE SRC_DELETE_IND
1     2021-10-01  2021-10-02  ABC    N
1     2021-10-02  2021-10-03  XYZ    N
1     2021-10-03  Null        XYZ    Y
</code></pre>
<ol start=""3"">
<li>Instead of SRC_DELETE_IND expire/end date the record</li>
</ol>
<pre><code>Record got deleted on 3rd Oct
PK_ID START_DT    END_DT     VALUE 
1     2021-10-01  2021-10-02  ABC   
1     2021-10-02  2021-10-03  XYZ   
</code></pre>
<p>But here we now dont have a Open record left.</p>
<p>Complexity is added if the record reappears in the Source stating as a Incorrect delete. Lets say on 10th
For Option 1,
Data will look like,</p>
<pre><code>PK_ID START_DT    END_DT     VALUE SRC_DELETE_IND
1     2021-10-01  2021-10-02  ABC    N
1     2021-10-02  Null        XYZ    N --Reversed
</code></pre>
<p>FOr Option 2</p>
<pre><code>PK_ID START_DT    END_DT     VALUE SRC_DELETE_IND
1     2021-10-01  2021-10-02  ABC    N
1     2021-10-02  2021-10-03  XYZ    N
1     2021-10-03  Null        XYZ    N  -- Reversed but now row is duplicate
</code></pre>
<p>For Option 3</p>
<pre><code>PK_ID START_DT    END_DT     VALUE 
1     2021-10-01  2021-10-02  ABC   
1     2021-10-02  2021-10-03  XYZ   
1     2021-10-10  Null        XYZ  --considered as New since no open record existed. Creates ETL gap
</code></pre>
<p>Which option makes more sense and is as per DWH best practices.</p>
","<database><snowflake-cloud-data-platform><data-warehouse><scd><scd2>","2021-10-12 14:32:11","2203","4","1","69570339","<p>I would go for a simpler way, put a default END_DATE for the deleted records like 1000-12-31 :</p>
<pre><code>PK_ID START_DT    END_DT     VALUE 
1     2021-10-01  2021-10-02  ABC   
1     2021-10-02  1000-12-31  XYZ --&gt; this row is deleted
</code></pre>
<p>Also, avoid using NULL values.
A NULL value indicates a lack of a value, which is not the same thing as a value of zero. SQL NULL is a state, not a value. This usage is quite different from most programming languages, where null value of a reference means it is not pointing to any object.</p>
<p>I recommend you to use a default date for END_DT for example 9999-12-31 so when inserting a row, your dimension will be like below :</p>
<pre><code> PK_ID START_DT    END_DT     VALUE 
 1     2021-10-01  9999-12-31  ABC 
</code></pre>
<p>instead of :</p>
<pre><code>PK_ID START_DT    END_DT     VALUE 
1     2021-10-01  NULL       ABC 
</code></pre>
<p>I recommend you to add a surrogate key to your dimensions.
A dimension table is designed with one column serving as a unique primary key. This primary key cannot be the operational system’s natural key because there will be multiple dimension rows for that natural key when changes are tracked over time. In addition, natural keys for a dimension may be created by more than one source system, and these natural keys may be incompatible or poorly administered.  The DW/BI system needs to claim control of the primary keys of all dimensions; rather than using explicit natural keys or natural keys with appended dates, you should create anonymous integer primary keys for every dimension. These dimension surrogate keys are simple integers, assigned in sequence, starting with the value 1, every time a new key is needed. The date dimension is exempt from the surrogate key rule; this highly predictable and stable dimension can use a more meaningful primary key.</p>
"
"69540884","Relation modeling vs normalization","<p>I have homework from my teacher and he say that which one is better (relation modeling or normalization) to make model of data in a hospital ? and why?</p>
","<data-modeling><data-management>","2021-10-12 12:51:12","57","0","1","69613168","<p>I am assuming by relation modelling you mean dimensional modelling (usually associated with Kimball).</p>
<p>Normalisation is associated with 3NF form or Inmon style.</p>
<p>You have to ask yourself what is the data model for?  You cant design anything unless you know the purpose of it or the requirements for the 'product' you build/design.</p>
<p>If you use the model as a backend database for the hospital administration application, then use Normalised data model.
If you want to use the model for analytics then you use the Kimball or denormalised model.</p>
"
"69519773","AWS athena giving error when trying to query files in S3 that have already been catalogued in Glue data catalog","<p>Trying to build a data lake using S3 for files that are in .csv.gz format and then further cleansing/processing data in AWS environment itself.
First used AWS Glue to create a data catalog\ (crawler was able to identify all tables).
The tables from catalog are also available in AWS Athena but when i try to run a Select * from the table it gives me following error.</p>
<p><em>Error opening Hive split s3://BUCKET_NAME/HEADER FOLDER/FILENAME.csv.gz (offset=0, length=44354) using org.apache.hadoop.mapred.TextInputFormat: Permission denied on S3 path: 3://BUCKET_NAME/HEADER FOLDER/FILENAME.csv.gz</em>.</p>
<p>Could it be that the file is in CSV.GZ format and that is why it cannot be accessed as is or do i need to give user or role a specific access for these files?</p>
","<amazon-s3><gzip><amazon-iam><amazon-athena>","2021-10-10 23:52:52","642","0","1","69531265","<p>You need to fix your permissions. The error says the principal (user/role) that ran the query does not have permission to read an object on S3.</p>
"
"69507707","Search Optimization Service in Snowflake is not used","<p>I have a Question on Search Optimization Service on Snowflake:</p>
<p>I have two tables Store_sales and Customers in my db and both are enabled with search optimization service, i have a query which does filter on date column in store_sales.date column as below by joining customer data:</p>
<pre><code>select ss.SS_SOLD_DATE_SK, ss.SS_ITEM_SK, c.C_FIRST_NAME from STORE_SALES ss
join SS_CUSTOMER c 
on ss.SS_CUSTOMER_SK = c.C_CUSTOMER_SK  
where ss.SS_SOLD_DATE_SK = 2451148;</code></pre>
<p>In the execution plan i am seeing Search optimization service used while scanning STORE_SALES table whereas Search Optimization is not used on SS_CUSTOMER table, due to this full table scan query is slowing down, similarly if i have point lookup filter on customer and querying STORE_SALES search optimization on STORE_SALES is not used. How to use search optimization service which is ON both tables to be used on both the tables?</p>
<p>Below is execution plan:</p>
<p><a href=""https://i.stack.imgur.com/fPqh8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fPqh8.png"" alt=""enter image description here"" /></a></p>
","<sql><snowflake-cloud-data-platform><data-warehouse>","2021-10-09 14:37:48","304","0","1","69507886","<h4>How Search Optimization Affects Joins</h4>
<p>The search optimization service does not directly improve the performance of joins. However, it can improve the performance of filtering rows from either table prior to the join, if the table has search optimization enabled and the predicate is selective.</p>
<p>Both tables do not need to have search optimization enabled. <strong>The decision to use search optimization is made for each table independently.</strong></p>
<p>The above is is taken from <a href=""https://docs.snowflake.com/en/user-guide/search-optimization-service.html"" rel=""nofollow noreferrer"">the documentation</a> where I have added my own <strong>bold</strong>.</p>
<p>So it would seem that the query optimizer/planner has decided that SO is not needed for the customer table in this instance. Unfortunately I didn't see anything about how (or if) it's possible to force it to use SO, nor anthing about <em>how</em> it decides whether or not to use SO. I have read guidlines about the the number of distinct values in the query predicate being large: at least one of the columns accessed through the query filter operation has at least 100k-200k distinct values.</p>
"
"69498656","Add a column that count number of rows until the first 1, by group in R","<p>I have the following dataset:</p>
<pre><code>test_df=data.frame(Group=c(1,1,1,1,2,2),var1=c(1,0,0,1,1,1),var2=c(0,0,1,1,0,0),var3=c(0,1,0,0,0,1))

</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Group</th>
<th style=""text-align: center;"">var1</th>
<th style=""text-align: center;"">var2</th>
<th style=""text-align: center;"">var3</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">0</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">1</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">0</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">1</td>
</tr>
</tbody>
</table>
</div>
<p>I want to add 3 columns (out1-3) for var1-3, which count number of rows until the first 1, by Group,</p>
<p>as shown below:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Group</th>
<th style=""text-align: center;"">var1</th>
<th style=""text-align: center;"">var2</th>
<th style=""text-align: center;"">var3</th>
<th style=""text-align: center;"">out1</th>
<th style=""text-align: center;"">out2</th>
<th style=""text-align: center;"">out3</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: center;"">2</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: center;"">2</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: center;"">2</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: center;"">2</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">2</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">2</td>
</tr>
</tbody>
</table>
</div>
<p>I used this R code, I repeated it for my 3 variables, and my actual dataset contains more than only 3 columns.
But it is not working:</p>
<pre><code>test_var1&lt;-select(test_df,Group,var1 )%&gt;% 
  group_by(Group) %&gt;% 
  mutate(out1 = row_number()) %&gt;% 
  filter(var1 != 0) %&gt;% 
  slice(1)
</code></pre>
","<r><dataframe><data-management>","2021-10-08 15:56:29","797","0","2","69498740","<p>If you only have 3 &quot;out&quot; variables then you can create three rows as follows</p>
<pre><code>#1- Your dataset
df=data.frame(Group=rep(1,4),var1=c(1,0,0,1),var2=c(0,0,1,1),var3=c(0,1,0,0))

#2- Count the first row number with &quot;1&quot; value
df$out1=min(rownames(df)[which(df$var1==1)])
df$out2=min(rownames(df)[which(df$var2==1)])
df$out3=min(rownames(df)[which(df$var3==1)])
</code></pre>
<p>If you have more than 3 columns, then it may be better to create a loop for example</p>
<pre><code>for(i in 1:3){
    df[paste(&quot;out&quot;,i,sep=&quot;&quot;)]=min(rownames(df)[which(df[,which(colnames(df)==paste(&quot;var&quot;,i,sep=&quot;&quot;))]==1)])
}
</code></pre>
"
"69498656","Add a column that count number of rows until the first 1, by group in R","<p>I have the following dataset:</p>
<pre><code>test_df=data.frame(Group=c(1,1,1,1,2,2),var1=c(1,0,0,1,1,1),var2=c(0,0,1,1,0,0),var3=c(0,1,0,0,0,1))

</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Group</th>
<th style=""text-align: center;"">var1</th>
<th style=""text-align: center;"">var2</th>
<th style=""text-align: center;"">var3</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">0</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">1</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">0</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">1</td>
</tr>
</tbody>
</table>
</div>
<p>I want to add 3 columns (out1-3) for var1-3, which count number of rows until the first 1, by Group,</p>
<p>as shown below:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Group</th>
<th style=""text-align: center;"">var1</th>
<th style=""text-align: center;"">var2</th>
<th style=""text-align: center;"">var3</th>
<th style=""text-align: center;"">out1</th>
<th style=""text-align: center;"">out2</th>
<th style=""text-align: center;"">out3</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: center;"">2</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: center;"">2</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: center;"">2</td>
</tr>
<tr>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">3</td>
<td style=""text-align: center;"">2</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">2</td>
</tr>
<tr>
<td style=""text-align: center;"">2</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">1</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: center;"">2</td>
</tr>
</tbody>
</table>
</div>
<p>I used this R code, I repeated it for my 3 variables, and my actual dataset contains more than only 3 columns.
But it is not working:</p>
<pre><code>test_var1&lt;-select(test_df,Group,var1 )%&gt;% 
  group_by(Group) %&gt;% 
  mutate(out1 = row_number()) %&gt;% 
  filter(var1 != 0) %&gt;% 
  slice(1)
</code></pre>
","<r><dataframe><data-management>","2021-10-08 15:56:29","797","0","2","69499189","<pre><code>df &lt;- data.frame(Group=c(1,1,1,1,2,2),
                 var1=c(1,0,0,1,1,1),
                 var2=c(0,0,1,1,0,0),
                 var3=c(0,1,0,0,0,1))
</code></pre>
<p>This works for any number of variables as long as the structure is the same as in the example (i.e. Group + many variables that are 0 or 1)</p>
<pre><code>df %&gt;% 
  mutate(rownr = row_number()) %&gt;%
  pivot_longer(-c(Group, rownr)) %&gt;%
  group_by(Group, name) %&gt;%
  mutate(out = cumsum(value != 1 &amp; (cumsum(value) &lt; 1)) + 1,
         out = ifelse(max(out) &gt; n(), 0, max(out))) %&gt;% 
  pivot_wider(names_from = c(name, name), values_from = c(value, out)) %&gt;% 
  select(-rownr)
</code></pre>
<p>Returns:</p>
<blockquote>
<pre><code>  Group value_var1 value_var2 value_var3 out_var1 out_var2 out_var3
  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
1     1          1          0          0        1        3        2
2     1          0          0          1        1        3        2
3     1          0          1          0        1        3        2
4     1          1          1          0        1        3        2
5     2          1          0          0        1        0        2
6     2          1          0          1        1        0        2
</code></pre>
</blockquote>
"
"69485548","Sum rows by group with condition in R","<p>I have a dataset in R like this one:</p>
<p><a href=""https://i.stack.imgur.com/eWrb6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eWrb6.png"" alt=""enter image description here"" /></a></p>
<p>and I want to keep the same dataset with adding a column that gives the sum rows by ID when A=B=1.</p>
<p>This is the required dataset:</p>
<p><a href=""https://i.stack.imgur.com/Kujvf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Kujvf.png"" alt=""enter image description here"" /></a></p>
<p>I tried the following R code but it doesn't give the result I want:</p>
<pre><code>library(dplyr)

data1&lt;-data%&gt;% group_by(ID) %&gt;% 
  mutate(result=case_when(A==1 &amp; B==1 ~ sum(A),TRUE ~ 0)) %&gt;% ungroup()
</code></pre>
","<r><dplyr><data-management>","2021-10-07 17:38:03","812","-3","2","69485562","<p>After grouping by 'ID', multiply the 'A' with 'B' (0 values in B returns 0 in A) and then get the <code>sum</code></p>
<pre><code>library(dplyr)
data  %&gt;%
     group_by(ID) %&gt;%
     mutate(result = sum(A*B)) %&gt;%
     ungroup
</code></pre>
<p>-output</p>
<pre><code># A tibble: 10 × 4
      ID     A     B result
   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
 1     1     1     0      3
 2     1     1     1      3
 3     1     0     1      3
 4     1     0     0      3
 5     1     1     1      3
 6     1     1     1      3
 7     2     1     0      2
 8     2     1     1      2
 9     2     1     1      2
10     2     0     0      2
</code></pre>
<h3>data</h3>
<pre><code>data &lt;- structure(list(ID = c(1, 1, 1, 1, 1, 1, 2, 2, 2, 2), A = c(1, 
1, 0, 0, 1, 1, 1, 1, 1, 0), B = c(0, 1, 1, 0, 1, 1, 0, 1, 1, 
0)), class = &quot;data.frame&quot;, row.names = c(NA, -10L))
</code></pre>
"
"69485548","Sum rows by group with condition in R","<p>I have a dataset in R like this one:</p>
<p><a href=""https://i.stack.imgur.com/eWrb6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eWrb6.png"" alt=""enter image description here"" /></a></p>
<p>and I want to keep the same dataset with adding a column that gives the sum rows by ID when A=B=1.</p>
<p>This is the required dataset:</p>
<p><a href=""https://i.stack.imgur.com/Kujvf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Kujvf.png"" alt=""enter image description here"" /></a></p>
<p>I tried the following R code but it doesn't give the result I want:</p>
<pre><code>library(dplyr)

data1&lt;-data%&gt;% group_by(ID) %&gt;% 
  mutate(result=case_when(A==1 &amp; B==1 ~ sum(A),TRUE ~ 0)) %&gt;% ungroup()
</code></pre>
","<r><dplyr><data-management>","2021-10-07 17:38:03","812","-3","2","69485644","<p>Not as neat and clean , but still:</p>
<pre><code>data %&gt;%
  mutate(row_sum = apply(across(A:B), 1, sum)) %&gt;%
  group_by(ID) %&gt;%
  mutate(result = sum(row_sum == 2)) %&gt;%
  ungroup() %&gt;%
  select(-row_sum)
</code></pre>
<p>which gives:</p>
<pre><code># A tibble: 10 x 4
      ID     A     B result
   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;int&gt;
 1     1     1     0      3
 2     1     1     1      3
 3     1     0     1      3
 4     1     0     0      3
 5     1     1     1      3
 6     1     1     1      3
 7     2     1     0      2
 8     2     1     1      2
 9     2     1     1      2
10     2     0     0      2
</code></pre>
"
"69470370","How to use putSQL in apache nifi","<p>I a beginner in data <code>warehousing</code> and <code>apache nifi</code>. I was trying taking the Mysql table data into nifi and then want to put that data into another mysql database table, I am successfully getting data from the database table one and I can also able to print that data into file using <code>putFile</code> processor.</p>
<p>But now I want to store that queued data into mysql database table, I know there is <code>putSQL</code> processor but it was not working for me.
Can anyone let me know how to do it correctly.</p>
<p>Here are the screenshots of my flow</p>
<p><a href=""https://i.stack.imgur.com/oaACy.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oaACy.jpg"" alt=""enter image description here"" /></a></p>
<p>PutSQL configuration-
<a href=""https://i.stack.imgur.com/SUOKL.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SUOKL.jpg"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/W0c46.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W0c46.jpg"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/rYMIz.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rYMIz.jpg"" alt=""enter image description here"" /></a></p>
<p>I converted data from Avro to JSON and then JSON to SQL in case if that would work, but this also not worked.</p>
","<etl><apache-nifi><data-warehouse>","2021-10-06 17:42:48","3016","0","1","69476265","<p>Use <code>PutDatabaseRecord</code> and remove the <code>Convert*</code> processors.</p>
<p>From nifi docs:</p>
<blockquote>
<p>The PutDatabaseRecord processor uses a specified RecordReader to input
(possibly multiple) records from an incoming flow file. These records
are translated to SQL statements and executed as a single transaction.
If any errors occur, the flow file is routed to failure or retry, and
if the records are transmitted successfully, the incoming flow file is
routed to success. The type of statement executed by the processor is
specified via the Statement Type property, which accepts some
hard-coded values such as INSERT, UPDATE, and DELETE, as well as 'Use
statement.type Attribute', which causes the processor to get the
statement type from a flow file attribute. IMPORTANT: If the Statement
Type is UPDATE, then the incoming records must not alter the value(s)
of the primary keys (or user-specified Update Keys). If such records
are encountered, the UPDATE statement issued to the database may do
nothing (if no existing records with the new primary key values are
found), or could inadvertently corrupt the existing data (by changing
records for which the new values of the primary keys exist).</p>
</blockquote>
<p>This should be more performant and cleaner.</p>
"
"69412707","Can I store time series data in data warehouse?","<p><strong>Background:</strong></p>
<p>I have an Airflow job that collects data (market share of the industry in the last 24 hours) from third-party endpoints every 10 minutes. I currently store the data in S3. We want to use this data to calculate our market share. I plan to keep the data in Data Warehouse as a time series standalone table( not a dimension or fact) to be used in queries to compare it with our data (stored in a star schema)to get daily market share.</p>
<p><strong>Questions:</strong></p>
<p>Is the data warehouse the right place to store time-series data? Can we store standalone tables similar to this? Does a table need to be part of the star schema to keep it in Data Warehouse?</p>
","<time-series><snowflake-cloud-data-platform><data-warehouse>","2021-10-01 23:05:43","483","4","2","69412890","<p>Snowflake has no restrictions on its use to store time series data, the data does not need to be stored in a star schema.
You have to consider how you load this data, it depends on how quickly you want to see it in the data warehouse.
You can use the Snowpipe functionality and load data on the fly.</p>
<p>The presentation: <a href=""https://www.youtube.com/watch?v=jKJTqfvwFOg"" rel=""nofollow noreferrer"">Snowpipe: Load data fast, analyze even faster</a></p>
<p>Documentation: <a href=""https://docs.snowflake.com/en/user-guide/data-load-snowpipe-intro.html"" rel=""nofollow noreferrer"">Introduction to Snowpipe</a></p>
<p>Or, load data at specified intervals using the STREAM and TASK functionality. Set e.g. a cyclic task every 10 minutes and download data if STREAM detected any new data on AWS S3.</p>
<p>The presentation: <a href=""https://www.youtube.com/watch?v=5AMf8xLJofs"" rel=""nofollow noreferrer"">Randy Pitcher Streams &amp; Tasks Hands on Lab</a></p>
<p>Documentation: <a href=""https://docs.snowflake.com/en/user-guide/tasks-intro.html"" rel=""nofollow noreferrer"">Introduction to Tasks</a> and
<a href=""https://docs.snowflake.com/en/sql-reference/sql/create-stream.html"" rel=""nofollow noreferrer"">CREATE STREAM</a></p>
"
"69412707","Can I store time series data in data warehouse?","<p><strong>Background:</strong></p>
<p>I have an Airflow job that collects data (market share of the industry in the last 24 hours) from third-party endpoints every 10 minutes. I currently store the data in S3. We want to use this data to calculate our market share. I plan to keep the data in Data Warehouse as a time series standalone table( not a dimension or fact) to be used in queries to compare it with our data (stored in a star schema)to get daily market share.</p>
<p><strong>Questions:</strong></p>
<p>Is the data warehouse the right place to store time-series data? Can we store standalone tables similar to this? Does a table need to be part of the star schema to keep it in Data Warehouse?</p>
","<time-series><snowflake-cloud-data-platform><data-warehouse>","2021-10-01 23:05:43","483","4","2","69413635","<p>By time series you're talking about real-time / near real-time data represented as JSON?</p>
<p>Yes, semi-structured data can be stored in the Variant data type and easily queried in place or materialised into structured columns in the same table as the variant data type</p>
"
"69360297","Calculated column in SSAS covering for faulty modeling in the backend?","<p>I have a legacy model in SSAS, where I have a Fact table with columns: <strong>Key_Parent_Segment</strong> and <strong>Key_Segment</strong>.</p>
<p>They both connect to a single role playing <strong>ClientSegment</strong> table.</p>
<p>And in the fact table there is a calculated column:</p>
<pre><code>=IF(
    FactIncome[Key_Parent_Segment] &lt; 0
    ,RELATED('Client Segment'[Segment])
    ,RELATED('Parent Client Segment'[Segment])
)
</code></pre>
<p>(In SSAS both <strong>Client Segment</strong> and <strong>Parent Client Segment</strong> have the same DimClientSegment as source)</p>
<p>Question: Isn't this a flaw in the model? Isn’t this calculated column just covering for some unfilled values at source/backend?</p>
","<data-warehouse><cube><ssas-tabular><dimensional-modeling>","2021-09-28 10:45:43","27","0","1","69363155","<blockquote>
<p>Isn't this a flaw in the model? Isn’t this calculated column just covering for some unfilled values at source/backend?</p>
</blockquote>
<p>As a practical matter a tabular model is always a little more specific and a little more complete than the data mart it's built on.  It's quite common to make little changes in the tabular model that might ideally belong as design changes to the relational data mart.  But practical considerations, like introducing breaking changes, or simple timing often make it more convenient to apply the change at the tabular model layer (at least initially).</p>
"
"69341791","Turn subquery result to array in Kylin","<p>How can I turn the result from subquery into an array in Kylin? In PostgresSQL, I can do it like the query below, but in Kylin it is an error.</p>
<pre><code>SELECT ARRAY(
    SELECT A.ID
    FROM A
    WHERE A.num = 123
)
</code></pre>
","<sql><data-warehouse><kylin>","2021-09-27 05:53:41","50","0","1","70144245","<p>Checked <a href=""https://kylin.apache.org/docs/tutorial/sql_reference.html"" rel=""nofollow noreferrer"">Kylin SQL reference</a>, there is no such ARRAY function or similar. Seems the feature you are looking for is not in Kylin yet.</p>
"
"69319982","Optimizing rendering of data received from getStaticProps (data occasionally not visible on initial render)","<p>I have a function I run on my NextJS application's homepage that takes in the props received from getStaticProps. The only problem is that on iOS and Mac, using the Safari/DuckDuckGo browser, the page will occasionally (about every 5/6 cacheless reloads in incognito) load -all- content....but none of it is visible.</p>
<p>You can still copy and paste the text, you can push down on the images and THEN see them, but the only things visible are the header background color and HTML/Body background color. HOWEVER, it works 100% of the time in Chrome, on all operating systems.</p>
<p>If you think that the problem lies w/i the length and/or methodology of this function, can you please provide me with guidance on how to condense it, or use better practices?</p>
<p>And if you think the problem lies elsewhere, I'd love to know where to start looking. The console doesn't show any problems whatsoever.</p>
<p>Here's the component that is being rendered:</p>
<pre><code>import React, { useState, useEffect } from &quot;react&quot;;
import organizeMenu from &quot;../models/orgMenu&quot;;
import {
  Box,
  Heading,
  SimpleGrid,
  Divider,
  Center,
  Container,
} from &quot;@chakra-ui/react&quot;;
import ItemCard from &quot;./molecules/ItemCard&quot;;
import { useMenuStore } from &quot;../state/store&quot;;
import SearchBar from './search/SearchBar';

const HomeContainer = ({props}) =&gt; {
 

  const { setStateModifierLists } = useMenuStore();
  const modifierLists = props.data.objects.filter(
    (object) =&gt; object.type === &quot;MODIFIER_LIST&quot;
  );

  useEffect(() =&gt; {
    setStateModifierLists(modifierLists);
  }, []);

  const itemList = props.data.objects.filter(
    (object) =&gt; object.type === &quot;ITEM&quot;
  );

  const categories = props.data.objects.filter(
    (object) =&gt; object.type === &quot;CATEGORY&quot;
  );

  const loadThis = organizeMenu(props);

  const bfast = loadThis.bfast;
  const entrees = loadThis.entrees;
  const drinks = loadThis.drinks;
  console.log(`bfast`, bfast);

  const [loaded, setLoaded] = useState(false);

  const handleLoad = (e) =&gt; {
    console.log(&quot;loaded&quot;);
    setLoaded(true);
  };
    return (
        
             &lt;Box w=&quot;100%&quot;&gt;
      &lt;Container&gt;
        &lt;SearchBar categories={categories} itemList={itemList} /&gt;
      &lt;/Container&gt;

      
      &lt;Heading ml={3}&gt;Breakfast&lt;/Heading&gt;
      &lt;Divider /&gt;
      &lt;Center&gt;
        &lt;SimpleGrid
          m=&quot;0 auto&quot;
          alignItems=&quot;center&quot;
          spacing={6}
          p=&quot;2&quot;
          columns={[1, null, 2, null, 3]}
        &gt;
          {bfast.map((b) =&gt; (
            &lt;ItemCard modifierLists={modifierLists} key={b.id} item={b} /&gt;
          ))}
        &lt;/SimpleGrid&gt;
      &lt;/Center&gt;
      &lt;Heading ml={3}&gt;Entrees&lt;/Heading&gt;
      &lt;Divider /&gt;
      &lt;Center&gt;
        &lt;SimpleGrid
          m=&quot;0 auto&quot;
          alignItems=&quot;baseline&quot;
          onLoad={handleLoad}
          spacing={6}
          p=&quot;2&quot;
          columns={[1, null, 2, null, 3]}
        &gt;
          {entrees.map((e) =&gt; (
            &lt;&gt;
              &lt;ItemCard modifierLists={modifierLists} key={e.id} item={e} /&gt;
            &lt;/&gt;
          ))}
        &lt;/SimpleGrid&gt;
      &lt;/Center&gt;
      &lt;Heading ml={3}&gt;Drinks&lt;/Heading&gt;
      &lt;Divider /&gt;
      &lt;Center&gt;
        &lt;SimpleGrid
          m=&quot;0 auto&quot;
          alignItems=&quot;stretch&quot;
          onLoad={handleLoad}
          spacing={6}
          p=&quot;2&quot;
          columns={[1, null, 2, null, 3]}
        &gt;
          {drinks.map((d) =&gt; (
            &lt;ItemCard modifierLists={modifierLists} key={d.id} item={d} /&gt;
          ))}
        &lt;/SimpleGrid&gt;
      &lt;/Center&gt;
    &lt;/Box&gt;
        
    )
}

export default HomeContainer

</code></pre>
<p>This is the function I wrote to organize the data for rendering:</p>
<pre><code>export default function organizeMenu(props) {
  
    // Segment menu items
    let menuItems = [];
  
    menuItems = props.data.objects.filter((object) =&gt; object.type === &quot;ITEM&quot;);

    //Segment menu item images
    let itemImages = [];
    itemImages = props.data.objects.filter((object) =&gt; object.type === &quot;IMAGE&quot;);

    //Segment Categories
    let categories = [];
    categories = props.data.objects.filter(
      (object) =&gt; object.type === &quot;CATEGORY&quot;
    );

    //Segment Modifier Lists
    let modifierLists = [];
    modifierLists = props.data.objects.filter(
      (object) =&gt; object.type === &quot;MODIFIER_LIST&quot;
    );
    
  
    // Merge data to provide better mapping and ordering process
  
    //Looping through menuItems and itemImages to combine fields into menuItems for ease of mapping data to components
  
    for (let x = 0; x &lt; menuItems.length; x++) {
      menuItems[x].modifiers = [];
      menuItems[x].imageData = {
        url: &quot;https://via.placeholder.com/250&quot;,
      };
      for (let y = 0; y &lt; itemImages.length; y++) {
        if (menuItems[x].imageId === itemImages[y].id) {
          // console.log(`Match: ${menuItems[x].imageId}`);
          menuItems[x].imageData = itemImages[y].imageData;
        } else {
          // console.log(&quot;No match&quot;);
        }
      }
    }
  

  
    // Next, we're going to tie the actual modifiers to the menuItem objects, rather than having to map them separately.
  
    for (let mm = 0; mm &lt; menuItems.length; mm++) {
      menuItems[mm].availableModifiers = [];
      if (menuItems[mm].itemData.modifierListInfo) {
        for (
          let xx = 0;
          xx &lt; menuItems[mm].itemData.modifierListInfo.length;
          xx++
        ) {
          if (menuItems[mm].itemData.modifierListInfo[xx].enabled === true) {
            // console.log(&quot;enabled&quot;);
            for (let zz = 0; zz &lt; modifierLists.length; zz++) {
              if (
                menuItems[mm].itemData.modifierListInfo[xx].modifierListId ===
                modifierLists[zz].id
              ) {

                for (
                  let xo = 0;
                  xo &lt; modifierLists[zz].modifierListData.modifiers.length;
                  xo++
                ) {
  
                  menuItems[mm].availableModifiers.push(
                    modifierLists[zz].modifierListData.modifiers[xo]
                  );
                }
  

              }
            }
          } else {
            // console.log(&quot;no mods&quot;);
          }
        }
      }
    }
  
//If modifier has a price, map the price according to the needs of square's api.
  
    for (let qu = 0; qu &lt; menuItems.length; qu++) {
      for (let xz = 0; xz &lt; menuItems[qu].availableModifiers.length; xz++) {
        if (menuItems[qu].availableModifiers[xz].modifierData.priceMoney) {
          menuItems[qu].availableModifiers[xz].basePriceMoney = {
            ...menuItems[qu].availableModifiers[xz].modifierData.priceMoney,
          };
        } else {
          menuItems[qu].availableModifiers[xz].modifierData.basePriceMoney = {
            amount: &quot;0&quot;,
            currency: &quot;USD&quot;,
          };
          menuItems[qu].availableModifiers[xz].basePriceMoney = {
            amount: &quot;0&quot;,
            currency: &quot;USD&quot;,
          };
        }
      }
    }
  
  
// Set primary variation (default)
  
    for (let h = 0; h &lt; menuItems.length; h++) {
      if (
        menuItems[h].itemData.variations[0] &amp;&amp;
        menuItems[h].itemData.variations[0].isDeleted === false
      ) {
        menuItems[h].primaryVariation = {
          ...menuItems[h].itemData.variations[0],
          isChosen: false,
        };
      }
    }
  

  
    // Merging &quot;CATEGORIES&quot; with menuItems
  
    for (let q = 0; q &lt; menuItems.length; q++) {
      menuItems[q].categoryName;
      for (let w = 0; w &lt; categories.length; w++) {
        if (menuItems[q].itemData.categoryId === categories[w].id) {
          menuItems[q].categoryName = categories[w].categoryData.name;
        }
      }
    }
  
    // Separating items into arrays based on category...
  
    let breakfastItems = [];
    breakfastItems = menuItems.filter(
      (object) =&gt; object.categoryName === &quot;Breakfast&quot;
    );

  
    let entreeItems = [];
    entreeItems = menuItems.filter((object) =&gt; object.categoryName === &quot;Entree&quot;);
  
    let drinkItems = [];
    drinkItems = menuItems.filter((object) =&gt; object.categoryName === &quot;Drinks&quot;);

// The object to be returned... Items are rendered from these objects.
    const catalog = {
      bfast: breakfastItems,
      entrees: entreeItems,
      drinks: drinkItems,
    };
  
 
    return catalog;
  }
</code></pre>
<p>When it works:
<a href=""https://i.stack.imgur.com/oG4z1.jpg"" rel=""nofollow noreferrer"">Image 1</a></p>
<p>When it doesn't:
<a href=""https://i.stack.imgur.com/nbGPm.png"" rel=""nofollow noreferrer"">Image 1</a>
<a href=""https://i.stack.imgur.com/f42hw.jpg"" rel=""nofollow noreferrer"">Image 2</a></p>
","<reactjs><function><next.js><rendering><data-mapping>","2021-09-24 19:02:51","92","0","1","69320717","<p>Your issue seems due an understanding of the rendering cycle and what causes components to render. Any of these changes will cause the component to render again:</p>
<ol>
<li>Update to props</li>
<li>Update to state</li>
<li><a href=""https://reactjs.org/docs/context.html"" rel=""nofollow noreferrer"">Context</a> value changes</li>
</ol>
<p>Your <code>const loadThis = organizeMenu(props);</code> is being run synchronously <strong>every time</strong> when one of the above changes (could be triggered by parent component too) and is wasteful and causing UI components to render unnecessarily.</p>
<pre><code>  const loadThis = organizeMenu(props);

  const bfast = loadThis.bfast;
  const entrees = loadThis.entrees;
  const drinks = loadThis.drinks;
  console.log(`bfast`, bfast);
</code></pre>
<p>Move these into <code>state</code>, <code>useEffect</code>, or <strong>ideally</strong> run the <code>organizeMenu</code> functionality at build time using <code>getStaticProps</code> and pass the data to the component as props.</p>
<p>See the simplified example with comments below. Every time you click the button, <code>organizeMenu(props)</code> is run and will output to the console and result in the list rendering again.</p>
<pre><code>export default function IndexPage({ categories, menu }) {
  const [selected, setSelected] = useState(0);
  const [menuList, setMenuList] = useState();

  const organizeMenu = (data) =&gt; {
    console.info('running organizeMenu');

    return data.map((category) =&gt; {
      return {
        id: category,
        name: category.toString()
      };
    });
  };

  // bad, this will be run every time there is a state change i.e. when the button is clicked and setSelected
  const foo = organizeMenu(categories);

  const handleOnClick = () =&gt; {
    setSelected(selected + 1);
  };

  useEffect(() =&gt; {
    console.info('running useEffect');
    setMenuList(
      categories.map((category) =&gt; {
        return {
          id: category,
          name: category.toString()
        };
      })
    );
  }, [categories]);

  return (
    &lt;div&gt;
      Hello World. {selected}
      &lt;br /&gt;
      {menu &amp;&amp; menu.map((menuItem) =&gt; &lt;li key={menuItem.id}&gt;{menuItem.name}&lt;/li&gt;)}
      &lt;br /&gt;
      {menuList &amp;&amp; menuList.map((menuItem) =&gt; &lt;li key={menuItem.id}&gt;{menuItem.name}&lt;/li&gt;)}
      &lt;hr /&gt;
      &lt;button onClick={handleOnClick}&gt;Make it so&lt;/button&gt;
    &lt;/div&gt;
  );
}

export async function getStaticProps(context) {
  const categories = [1, 2, 3];

  // prep menu here so run at build time
  const menu = categories.map((category) =&gt; {
    return {
      id: category,
      name: category.toString()
    };
  });

  return {
    props: {
      categories,
      menu
    }
  };
}
</code></pre>
<p><a href=""https://codesandbox.io/s/so-69319982-c9b24?file=/pages/index.js"" rel=""nofollow noreferrer"">CodesandBox Demo</a></p>
"
"69291596","Changing number of observation in a dataset by IDs according to a given value","<p>I have this dataset in R:</p>
<p><a href=""https://i.stack.imgur.com/uFOzB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uFOzB.png"" alt=""enter image description here"" /></a></p>
<p>and I want to change the data according to <code>nb</code> variable, it means <code>ID = 1</code> will have 5 rows and <code>ID=2</code> will have 12 rows as shown below:</p>
<p><a href=""https://i.stack.imgur.com/yG5I7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yG5I7.png"" alt=""enter image description here"" /></a></p>
<p>is there any R function that I could use it to transform my data :) ?</p>
<p>Thanks in advance</p>
","<r><data-management>","2021-09-22 21:41:31","133","1","3","69291610","<p>We need <code>uncount</code> from <code>tidyr</code> to expand based on the 'nb' column, by default, it removes the column as <code>.remove = TRUE</code>, change it to <code>FALSE</code> and then create the nb_long by doing a group by <code>row_number()</code></p>
<pre><code>library(dplyr)
library(tidyr)
df1 %&gt;%
   uncount(nb, .remove = FALSE) %&gt;%
   group_by(ID) %&gt;%
   mutate(nb_long = row_number()) %&gt;%
   ungroup
</code></pre>
<p>-output</p>
<pre><code># A tibble: 17 x 3
      ID    nb nb_long
   &lt;int&gt; &lt;dbl&gt;   &lt;int&gt;
 1     1     5       1
 2     1     5       2
 3     1     5       3
 4     1     5       4
 5     1     5       5
 6     2    12       1
 7     2    12       2
 8     2    12       3
 9     2    12       4
10     2    12       5
11     2    12       6
12     2    12       7
13     2    12       8
14     2    12       9
15     2    12      10
16     2    12      11
17     2    12      12
</code></pre>
<h3>data</h3>
<pre><code>df1 &lt;- structure(list(ID = 1:2, nb = c(5, 12)), 
class = &quot;data.frame&quot;, row.names = c(NA, 
-2L))
</code></pre>
"
"69291596","Changing number of observation in a dataset by IDs according to a given value","<p>I have this dataset in R:</p>
<p><a href=""https://i.stack.imgur.com/uFOzB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uFOzB.png"" alt=""enter image description here"" /></a></p>
<p>and I want to change the data according to <code>nb</code> variable, it means <code>ID = 1</code> will have 5 rows and <code>ID=2</code> will have 12 rows as shown below:</p>
<p><a href=""https://i.stack.imgur.com/yG5I7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yG5I7.png"" alt=""enter image description here"" /></a></p>
<p>is there any R function that I could use it to transform my data :) ?</p>
<p>Thanks in advance</p>
","<r><data-management>","2021-09-22 21:41:31","133","1","3","69291711","<p>Here is another option. we just map out the values from 1 to nb and then we unnest the vector longer.</p>
<pre class=""lang-r prettyprint-override""><code>#packages
library(tidyverse)

#data
df1 &lt;- structure(list(ID = 1:2, nb = c(5, 12)), 
class = &quot;data.frame&quot;, row.names = c(NA, 
-2L))

#solution
df1 %&gt;% 
  mutate(nums = map(nb, ~seq(1, .x, by = 1))) %&gt;%
  unnest_longer(nums)
#&gt; # A tibble: 17 x 3
#&gt;       ID    nb  nums
#&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;
#&gt;  1     1     5     1
#&gt;  2     1     5     2
#&gt;  3     1     5     3
#&gt;  4     1     5     4
#&gt;  5     1     5     5
#&gt;  6     2    12     1
#&gt;  7     2    12     2
#&gt;  8     2    12     3
#&gt;  9     2    12     4
#&gt; 10     2    12     5
#&gt; 11     2    12     6
#&gt; 12     2    12     7
#&gt; 13     2    12     8
#&gt; 14     2    12     9
#&gt; 15     2    12    10
#&gt; 16     2    12    11
#&gt; 17     2    12    12
</code></pre>
"
"69291596","Changing number of observation in a dataset by IDs according to a given value","<p>I have this dataset in R:</p>
<p><a href=""https://i.stack.imgur.com/uFOzB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uFOzB.png"" alt=""enter image description here"" /></a></p>
<p>and I want to change the data according to <code>nb</code> variable, it means <code>ID = 1</code> will have 5 rows and <code>ID=2</code> will have 12 rows as shown below:</p>
<p><a href=""https://i.stack.imgur.com/yG5I7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yG5I7.png"" alt=""enter image description here"" /></a></p>
<p>is there any R function that I could use it to transform my data :) ?</p>
<p>Thanks in advance</p>
","<r><data-management>","2021-09-22 21:41:31","133","1","3","69291745","<p>We can try the following <code>data.table</code> option</p>
<pre><code>&gt; setDT(df)[,.(nb_long = 1:nb),.(ID,nb)]
    ID nb nb_long
 1:  1  5       1
 2:  1  5       2
 3:  1  5       3
 4:  1  5       4
 5:  1  5       5
 6:  2 12       1
 7:  2 12       2
 8:  2 12       3
 9:  2 12       4
10:  2 12       5
11:  2 12       6
12:  2 12       7
13:  2 12       8
14:  2 12       9
15:  2 12      10
16:  2 12      11
17:  2 12      12
</code></pre>
"
"69287871","Extract/Get the table name and their respective columns used in a view from a query in BigQuery","<p>Let us take the following MYSQL query written in BigQuery for creating a view.</p>
<pre><code>SELECT a.col1,
       a.col2,
       a.col3,
       b.col1,
       b.col2,
       b.col3,
       c.col1,
       c.col2
FROM project_name_dataset_table_a a
INNER JOIN project_name_dataset_table_b b ON a.col1 = b.col1
INNER JOIN project_name_dataset_table_c c ON a.col2 = c.col2
</code></pre>
<p>Below is the required output format or similar.
<a href=""https://i.stack.imgur.com/z4xSL.png"" rel=""nofollow noreferrer"">Final Format Required or any similar will be fine</a></p>
<p>Basically i have to prepare a document which will contain all the information of the views created and the tables and their respective columns used in it.</p>
","<mysql><sql><database><google-bigquery><data-warehouse>","2021-09-22 16:17:05","154","0","2","69302203","<p>I have created a script for the usage at my end you can tweak the regex according to you</p>
<pre><code>import re

query = &quot;&quot;&quot;
SELECT a.col1, a.col2, a.col3, b.col1, b.col2, b.col3, c.col1, c.col2 
FROM `project_name.dataset.table_a` a
JOIN `project_name.dataset.table_b` b
ON ...
JOIN `project_name.dataset.table_c` c
ON ...
&quot;&quot;&quot;
tabsandcols = re.findall(r'([A-Za-z0-9\-\_]*)\.([a-zA-Z0-9\.\_]*)', query)

alias_names = re.findall(r'\`([A-Za-z\-\_]*)\.([a-zA-Z\.\_]*)\` ([a-z]{1,3})', query)

dic = {}

print(alias_names)

for i, j, k in alias_names:
    if k in dic:
        pass
    else:
        dic[k] = j

l = set()
for i, j in tabsandcols:
    if i != &quot;project_name&quot; and dic.get(i, &quot;&quot;) != &quot;&quot;:
        l.add(dic.get(i, &quot;&quot;) + &quot; &quot; + j)

for i in l:
    print(&quot;project_name&quot;, i)
</code></pre>
<p>It will provide following output</p>
<pre><code>project_name dataset.table_a col1
project_name dataset.table_b col2
project_name dataset.table_c col1
project_name dataset.table_c col2
project_name dataset.table_a col3
project_name dataset.table_a col2
project_name dataset.table_b col1
project_name dataset.table_b col3
</code></pre>
"
"69287871","Extract/Get the table name and their respective columns used in a view from a query in BigQuery","<p>Let us take the following MYSQL query written in BigQuery for creating a view.</p>
<pre><code>SELECT a.col1,
       a.col2,
       a.col3,
       b.col1,
       b.col2,
       b.col3,
       c.col1,
       c.col2
FROM project_name_dataset_table_a a
INNER JOIN project_name_dataset_table_b b ON a.col1 = b.col1
INNER JOIN project_name_dataset_table_c c ON a.col2 = c.col2
</code></pre>
<p>Below is the required output format or similar.
<a href=""https://i.stack.imgur.com/z4xSL.png"" rel=""nofollow noreferrer"">Final Format Required or any similar will be fine</a></p>
<p>Basically i have to prepare a document which will contain all the information of the views created and the tables and their respective columns used in it.</p>
","<mysql><sql><database><google-bigquery><data-warehouse>","2021-09-22 16:17:05","154","0","2","69307790","<p>I could replicate the code. I suggest that you use this command from BigQuery (INFORMATION_SCHEMA.COLUMNS).
This command brings you the information that you need.</p>
<p>This is a sample code:</p>
<pre><code>SELECT A.table_catalog, CONCAT(A.table_schema, '.' , A.table_name) as table,A.column_name
FROM myproject_name.mydataset_name.INFORMATION_SCHEMA.COLUMNS A
where A.table_catalog=myproject_name
</code></pre>
<p>If you need more information about this command you can <a href=""https://cloud.google.com/bigquery/docs/information-schema-intro#syntax"" rel=""nofollow noreferrer"">click here</a>:</p>
"
"69278232","How to change BI Answers user's password in OBIEE12c","<p>BI Answers users manage in WebLogic 12c. I did find any options in BI answers that allow users to change his/her password. BI answers users need to communicate with WebLogic administrator to change his/her password. In OBIEE10g, there is an option in configuration file to enable change password for BI Answers. Is there any option in OBIEE12c that enable users to change password.</p>
","<oracle><data-warehouse><obiee>","2021-09-22 04:32:45","306","0","1","69278708","<p>It isn’t an out of the box feature. WebLogic is acting as a lightweight LDAP. See this thread for further information- <a href=""https://community.oracle.com/tech/apps-infra/discussion/4477102/how-to-enable-user-to-change-the-password-for-their-account-in-obiee-12c"" rel=""nofollow noreferrer"">https://community.oracle.com/tech/apps-infra/discussion/4477102/how-to-enable-user-to-change-the-password-for-their-account-in-obiee-12c</a></p>
"
"69240011","Configuration Assistant (biee.py) Failed with Exit Value 1 at OBIEE12c","<p>During installation and configuration of obiee12c(12.2.1.4), configuration assistant failed with an error.
The following is the installation log.</p>
<blockquote>
<p>[2021-09-19T16:31:35.218+09:00] [bi] [NOTIFICATION] [] [oracle.bi.install.config.basesteps] [tid: 42] [ecid: 0000MTUM^f91Vctsoc7EDm1RztE2000004,0] Failed single shot step: BIEE with: Execution of [/u01/app/oracle/middleware/oracle_common/common/bin/wlst.sh, /u01/app/oracle/middleware/bi/modules/oracle.bi.configassistant/biee.py, /u01/app/oracle/middleware, /u01/app/oracle/middleware/user_projects/domains/bi, weblogic, Expanded, hxfpr371, 9502, 9503, ORACLE, oracle.jdbc.OracleDriver, jdbc:oracle:thin:@//:1521/, TST1, jdbc:oracle:thin:@//:1521/, ] failed with exit value 1
[2021-09-19T16:31:35.219+09:00] [bi] [ERROR] [] [oracle.bi.install.config.actions] [tid: 42] [ecid: 0000MTUM^f91Vctsoc7EDm1RztE2000004,0] Non-skipped failure during configuration action: Execution of [/u01/app/oracle/middleware/oracle_common/common/bin/wlst.sh, /u01/app/oracle/middleware/bi/modules/oracle.bi.configassistant/biee.py, /u01/app/oracle/middleware, /u01/app/oracle/middleware/user_projects/domains/bi, weblogic, Expanded, hxfpr371, 9502, 9503, ORACLE, oracle.jdbc.OracleDriver, jdbc:oracle:thin:@//:1521/, TST1, jdbc:oracle:thin:@//:1521/, ] failed with exit value 1[[
oracle.bi.exec.ExecutionStatusException: Execution of [/u01/app/oracle/middleware/oracle_common/common/bin/wlst.sh, /u01/app/oracle/middleware/bi/modules/oracle.bi.configassistant/biee.py, /u01/app/oracle/middleware, /u01/app/oracle/middleware/user_projects/domains/bi, weblogic, Expanded, hxfpr371, 9502, 9503, ORACLE, oracle.jdbc.OracleDriver, jdbc:oracle:thin:@//:1521/, TST1, jdbc:oracle:thin:@//:1521/, ] failed with exit value 1</p>
</blockquote>
<pre><code>at oracle.bi.exec.StdinProcess.runProcess(StdinProcess.java:106)
at oracle.bi.exec.ExecScript.executeScript(ExecScript.java:191)
at oracle.bi.exec.ExecScript.executeSynchronousScript(ExecScript.java:95)
at oracle.bi.exec.ExecWLST.executeWLSTScript(ExecWLST.java:62)
at oracle.bi.install.config.steps.WLSTStep.executeSingleShot(WLSTStep.java:55)
at oracle.bi.install.config.basesteps.SingleShotActionStep.execute(SingleShotActionStep.java:31)
at oracle.bi.install.config.basesteps.StepList.execute(StepList.java:85)
at oracle.bi.install.config.actions.BIConfigAction.doExecute(BIConfigAction.java:127)
at oracle.as.install.engine.modules.configuration.client.ConfigAction.execute(ConfigAction.java:405)
at oracle.as.install.engine.modules.configuration.action.TaskPerformer.run(TaskPerformer.java:88)
at oracle.as.install.engine.modules.configuration.action.TaskPerformer.startConfigAction(TaskPerformer.java:108)
at oracle.as.install.engine.modules.configuration.action.ActionRequest.perform(ActionRequest.java:15)
at oracle.as.install.engine.modules.configuration.action.RequestQueue.performSequentialExecution(RequestQueue.java:284)
at oracle.as.install.engine.modules.configuration.action.RequestQueue.perform(RequestQueue.java:260)
at oracle.as.install.engine.modules.configuration.standard.StandardConfigActionManager.start(StandardConfigActionManager.java:185)
at oracle.as.install.engine.modules.configuration.boot.ConfigurationExtension.kickstart(ConfigurationExtension.java:82)
at oracle.as.install.engine.modules.configuration.ConfigurationModule.run(ConfigurationModule.java:87)
at java.lang.Thread.run(Thread.java:748)
</code></pre>
","<oracle><data-warehouse><obiee>","2021-09-19 03:20:13","522","0","1","69880297","<p>the issue is due to a java configuration parameter i guess, try this before installation :</p>
<ul>
<li>go to « C:\Java\jdk1.8.0_192\jre\lib\security »
** replace this path with your own path</li>
<li>edit the file « java.security » and replace this line :</li>
</ul>
<p>securerandom.source=file:/dev/random</p>
<p>by this line :</p>
<pre><code>securerandom.source=file:/dev/urandom
</code></pre>
<p>for more details : Doc ID 2221805.1</p>
"
"69234532","Which is most efficient and failproof way to fetch a table which has almost 50 million records using python?","<p>I have a requirement where I need to fetch a huge table that is presently stored in MySQL database using python.
I have tried with Limit and offset, but that keeps on getting slowed down with the number of iterations. <br>
I am using <code>mysqlclient</code> for making connections. <br>
My current requirement is to write it into a file after each iteration.
For testing, I am using CSV but I will switch to <code>parquet</code> file format once I find a good solution</p>
<pre><code>select * from table_name  limit {} ,{} 
</code></pre>
<p>Please suggest me some of the methods that you are using and it works flawlessly.</p>
<p>code snippet that i am using :</p>
<pre><code>i = 1
limit = 1000000
curs.execute(&quot;select * from table_name limit 1&quot;)
col = [col_[0] for col_ in curs.description]
print(col)

while True:
    start_ = time.time()
    query = f'select * from table_name    limit {i} ,{limit} '
    print(query)
    curs.execute(query)
    rows = curs.fetchall()
    if not len(rows):
        print(&quot;done&quot;)
        break
    print(&quot;###########&quot;)
    print(i)
    print(&quot;###########&quot;)
    end_ = time.time()
    print(f&quot;Runtime of the program is {end_ - start_}&quot;)

    df_temp = pd.DataFrame(rows, columns=col)
    df_temp.to_csv(f&quot;test_{i}.csv&quot;, header=True, index=False)
    # df_total.append(df_temp)
    i = i + limit
</code></pre>
<p>Or should I consider using different languages like scala?</p>
","<python><mysql><etl><data-warehouse>","2021-09-18 12:11:43","2051","1","5","69235376","<p>First: rows in Mysql and other RDBMSs <em>have no inherent order</em>. So the result set from LIMIT 100 OFFSET 100 is not guaranteed to give the next rows after the result set from LIMIT 100 OFFSET 0. Unless your query includes an ORDER BY clause, the RDBMS gives results in an <em>unpredictable</em> order. <em>Unpredictable</em> is worse than <em>random</em>; with random ordering you have a chance of catching the problem in test. So your current batched approach is incorrect.</p>
<p>Second: Do not ORDER a result set like this except maybe on the primary key.  Your server will take a looooong time to order fifty megarows otherwise. And it will have to do it again for every batch. See you next year! -:)</p>
<p>Third: A very fast way to generate a .csv file is <a href=""https://dev.mysql.com/doc/refman/8.0/en/select-into.html"" rel=""nofollow noreferrer"">SELECT ... INTO OUTFILE ...</a>. You should consider doing this, then getting your python program to read the .csv file. You may even be able to find a software tool or script to convert the .csv file to the parquet format.</p>
<p>Fourth: You want to <em>stream</em> this result set, not <em>slurp</em> it. You want to process it row by row so you don't blow out your python program's RAM. Use a <a href=""https://dev.mysql.com/doc/connector-python/en/connector-python-api-mysqlcursorraw.html"" rel=""nofollow noreferrer"">Raw curso</a>r, and use <code>for (...) in cursor:</code> to fetch the rows one by one. Process each row before you fetch the next one.</p>
<p>Fifth: Don't worry about the fifty million iterations of your for-loop; your program must read and write that many lines anyhow.</p>
<p>Sixth: if your database is busy as you run this, other users may be blocked. To remediate that blocking execute the SQL statement <a href=""https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html#isolevel_read-uncommitted"" rel=""nofollow noreferrer"">SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED</a> right before your SELECT statement.</p>
<p>One of the points of all this RDBMS technology is to allow programs to use data sets that are orders of magnitude larger than available RAM. Streaming the data is the only way to do that when you need to process the entire data set.</p>
"
"69234532","Which is most efficient and failproof way to fetch a table which has almost 50 million records using python?","<p>I have a requirement where I need to fetch a huge table that is presently stored in MySQL database using python.
I have tried with Limit and offset, but that keeps on getting slowed down with the number of iterations. <br>
I am using <code>mysqlclient</code> for making connections. <br>
My current requirement is to write it into a file after each iteration.
For testing, I am using CSV but I will switch to <code>parquet</code> file format once I find a good solution</p>
<pre><code>select * from table_name  limit {} ,{} 
</code></pre>
<p>Please suggest me some of the methods that you are using and it works flawlessly.</p>
<p>code snippet that i am using :</p>
<pre><code>i = 1
limit = 1000000
curs.execute(&quot;select * from table_name limit 1&quot;)
col = [col_[0] for col_ in curs.description]
print(col)

while True:
    start_ = time.time()
    query = f'select * from table_name    limit {i} ,{limit} '
    print(query)
    curs.execute(query)
    rows = curs.fetchall()
    if not len(rows):
        print(&quot;done&quot;)
        break
    print(&quot;###########&quot;)
    print(i)
    print(&quot;###########&quot;)
    end_ = time.time()
    print(f&quot;Runtime of the program is {end_ - start_}&quot;)

    df_temp = pd.DataFrame(rows, columns=col)
    df_temp.to_csv(f&quot;test_{i}.csv&quot;, header=True, index=False)
    # df_total.append(df_temp)
    i = i + limit
</code></pre>
<p>Or should I consider using different languages like scala?</p>
","<python><mysql><etl><data-warehouse>","2021-09-18 12:11:43","2051","1","5","69236091","<p>Here is an example of how one might efficiently extract the entire contents of a MySQL database table and create a CSV file without the aid of pandas. My table (<strong>ips</strong>) has just 5 columns 3 of which each are declared as VARCHAR(255), one double and one int. The table contains 1 million rows. There are no constraints on the table. This code executes in ~1.75 seconds:-</p>
<pre><code>from MySQLdb import _mysql

CSVFILE = '/Users/andy/PrivateStuff/iplist.csv'
TABLE = 'IPS'
CHUNK = 10_000

CONFIG = {
    'user': 'andy',
    'passwd': 'monster',
    'db': 'andy'
}


class Connection():
    def __init__(self, config):
        self._config = config
        self._db = None

    @property
    def db(self):
        if not self._db:
            self._db = _mysql.connection(**self._config)
        return self._db

    def __enter__(self):
        return self

    def __exit__(self, *args):
        if self._db:
            self._db.close()
            self._db = None


def decode(items):
    return ', '.join([item.decode() if isinstance(item, bytes) else item if item else 'NULL' for item in items])


with open(CSVFILE, 'w') as csvfile:
    with Connection(CONFIG) as conn:
        conn.db.query(f'SHOW COLUMNS FROM {TABLE}')
        r = conn.db.store_result()
        cols = [col[0] for col in r.fetch_row(maxrows=0)]
        print(decode(cols), file=csvfile)
        conn.db.query(f'SELECT * FROM {TABLE}')
        r = conn.db.use_result()
        while (rows := r.fetch_row(maxrows=CHUNK)):
            for row in rows:
                print(decode(row), file=csvfile)
</code></pre>
"
"69234532","Which is most efficient and failproof way to fetch a table which has almost 50 million records using python?","<p>I have a requirement where I need to fetch a huge table that is presently stored in MySQL database using python.
I have tried with Limit and offset, but that keeps on getting slowed down with the number of iterations. <br>
I am using <code>mysqlclient</code> for making connections. <br>
My current requirement is to write it into a file after each iteration.
For testing, I am using CSV but I will switch to <code>parquet</code> file format once I find a good solution</p>
<pre><code>select * from table_name  limit {} ,{} 
</code></pre>
<p>Please suggest me some of the methods that you are using and it works flawlessly.</p>
<p>code snippet that i am using :</p>
<pre><code>i = 1
limit = 1000000
curs.execute(&quot;select * from table_name limit 1&quot;)
col = [col_[0] for col_ in curs.description]
print(col)

while True:
    start_ = time.time()
    query = f'select * from table_name    limit {i} ,{limit} '
    print(query)
    curs.execute(query)
    rows = curs.fetchall()
    if not len(rows):
        print(&quot;done&quot;)
        break
    print(&quot;###########&quot;)
    print(i)
    print(&quot;###########&quot;)
    end_ = time.time()
    print(f&quot;Runtime of the program is {end_ - start_}&quot;)

    df_temp = pd.DataFrame(rows, columns=col)
    df_temp.to_csv(f&quot;test_{i}.csv&quot;, header=True, index=False)
    # df_total.append(df_temp)
    i = i + limit
</code></pre>
<p>Or should I consider using different languages like scala?</p>
","<python><mysql><etl><data-warehouse>","2021-09-18 12:11:43","2051","1","5","69239823","<blockquote>
<p>with Limit and offset, but that keeps on getting slowed down with the number of iterations.</p>
</blockquote>
<p>Yes, that is a fact.</p>
<p>Instead &quot;remember where you left off&quot;; it will much faster.  More details:  <a href=""http://mysql.rjweb.org/doc.php/pagination"" rel=""nofollow noreferrer"">http://mysql.rjweb.org/doc.php/pagination</a></p>
<p>Meanwhile, a <code>LIMIT</code> without <code>ORDER BY</code> is asking for unpredictable results.  Furthermore, if Inserts and Deletes going such chunking can lead to duplicate and/or missing rows!</p>
<p>If your ultimate goal is to copy it to a CSV file, then simply do</p>
<pre><code>SELECT ... INTO OUTFILE &quot;...&quot; ...
</code></pre>
<p>That will take a single pass -- faster than anything you can possibly do with loops in Python or any other language.</p>
<p>Also consider running this remotely (that is, from your client machine, not the AWS server).</p>
<pre><code>mysql -h ... --batch -e &quot;SELECT * FROM tbl&quot; &gt; foo.csv
</code></pre>
<p>(It may need more parameters.)  (That will produce tabs between columns and an initial line with column names.)</p>
"
"69234532","Which is most efficient and failproof way to fetch a table which has almost 50 million records using python?","<p>I have a requirement where I need to fetch a huge table that is presently stored in MySQL database using python.
I have tried with Limit and offset, but that keeps on getting slowed down with the number of iterations. <br>
I am using <code>mysqlclient</code> for making connections. <br>
My current requirement is to write it into a file after each iteration.
For testing, I am using CSV but I will switch to <code>parquet</code> file format once I find a good solution</p>
<pre><code>select * from table_name  limit {} ,{} 
</code></pre>
<p>Please suggest me some of the methods that you are using and it works flawlessly.</p>
<p>code snippet that i am using :</p>
<pre><code>i = 1
limit = 1000000
curs.execute(&quot;select * from table_name limit 1&quot;)
col = [col_[0] for col_ in curs.description]
print(col)

while True:
    start_ = time.time()
    query = f'select * from table_name    limit {i} ,{limit} '
    print(query)
    curs.execute(query)
    rows = curs.fetchall()
    if not len(rows):
        print(&quot;done&quot;)
        break
    print(&quot;###########&quot;)
    print(i)
    print(&quot;###########&quot;)
    end_ = time.time()
    print(f&quot;Runtime of the program is {end_ - start_}&quot;)

    df_temp = pd.DataFrame(rows, columns=col)
    df_temp.to_csv(f&quot;test_{i}.csv&quot;, header=True, index=False)
    # df_total.append(df_temp)
    i = i + limit
</code></pre>
<p>Or should I consider using different languages like scala?</p>
","<python><mysql><etl><data-warehouse>","2021-09-18 12:11:43","2051","1","5","69265444","<p>Why do you want to dump to Parquet? To use it in a data lake situation? If so, consider using Presto or Spark to connect to MySQL and build a Parquet file for you.</p>
<p>With that said, I have built a Python script to dump a query result to CSV before. If you do it right, you should be able to dump 50 million rows of data into a CSV file in a few minutes (depending on the average size of your rows in bytes).</p>
<p>Pandas is probably slowing you down here. To be honest, I wouldn't even use Python's CSV package. I would just do some basic string manipulation, like this:</p>
<pre><code># create a file to append to
my_file = with open(&quot;mytable.csv&quot;, &quot;a&quot;) as my_file:

    # get sql result
    my_result = my_cursor.fetchall()

    # get amount of columns in result
    row_len = len(my_result[0])

    # iterate over results
    for row in my_result:

        # reset string to append to file
        row_string = &quot;&quot;

        # iterate over all columns except last one
        for col_num in range(row_len - 1):

            # escape double quotes with two double quotes (common CSV pattern)
            escaped_col_val = col.replace(&quot;\&quot;&quot;, &quot;\&quot;\&quot;&quot;)

            # encapsulate the column within double quotes, separate with a comma
            row_string = row_string + &quot;\&quot;{escaped_col_val}\&quot;&quot; + &quot;,&quot;

        # same escaping as before, just for last column
        escaped_col_val = col.replace(&quot;\&quot;&quot;, &quot;\&quot;\&quot;&quot;)

        # last column uses a newline instead of comma, to indicate a new row
        row_string = row_string + &quot;\&quot;{escaped_col_val}\&quot;&quot; + &quot;\n&quot;

        # append the result to a string
        my_file.write(row_string)

</code></pre>
<p>For what it's worth, I am building a new product <a href=""https://sqlpipe.com"" rel=""nofollow noreferrer"">SQLPipe</a> that could easily be extended to dump the result of a query to CSV. Perhaps check my profile for contact information.</p>
"
"69234532","Which is most efficient and failproof way to fetch a table which has almost 50 million records using python?","<p>I have a requirement where I need to fetch a huge table that is presently stored in MySQL database using python.
I have tried with Limit and offset, but that keeps on getting slowed down with the number of iterations. <br>
I am using <code>mysqlclient</code> for making connections. <br>
My current requirement is to write it into a file after each iteration.
For testing, I am using CSV but I will switch to <code>parquet</code> file format once I find a good solution</p>
<pre><code>select * from table_name  limit {} ,{} 
</code></pre>
<p>Please suggest me some of the methods that you are using and it works flawlessly.</p>
<p>code snippet that i am using :</p>
<pre><code>i = 1
limit = 1000000
curs.execute(&quot;select * from table_name limit 1&quot;)
col = [col_[0] for col_ in curs.description]
print(col)

while True:
    start_ = time.time()
    query = f'select * from table_name    limit {i} ,{limit} '
    print(query)
    curs.execute(query)
    rows = curs.fetchall()
    if not len(rows):
        print(&quot;done&quot;)
        break
    print(&quot;###########&quot;)
    print(i)
    print(&quot;###########&quot;)
    end_ = time.time()
    print(f&quot;Runtime of the program is {end_ - start_}&quot;)

    df_temp = pd.DataFrame(rows, columns=col)
    df_temp.to_csv(f&quot;test_{i}.csv&quot;, header=True, index=False)
    # df_total.append(df_temp)
    i = i + limit
</code></pre>
<p>Or should I consider using different languages like scala?</p>
","<python><mysql><etl><data-warehouse>","2021-09-18 12:11:43","2051","1","5","70016531","<p>You can order by primary key and query like:</p>
<ul>
<li><p><code>select * from table order by id limit 1000</code> the first time</p>
</li>
<li><p>and then perform <code>select * from table where id &gt; %(last_id_you_saw)s order by id limit 1000</code></p>
</li>
</ul>
<p>This should not slow down over time.</p>
"
"69202848","OWB (Oracle Warehouse Builder) migration assistant encounter an invalid substitution error","<p>Currently, we am using OWB11gR2(11.2.0.4) for the ETL process. We are planning to upgrade our oracle database into database 19c. Since, OWB is no longer exist after 11gR2, we need to migrate OWB into ODI (Oracle Data Integrator). After go through different document, we found an way to migrate owb to odi using owb migration assistant.
We are trying to run the migration assistant in AIX 7.2 in the following way.</p>
<pre><code>$OWB_HOME/owb/bin/unix/
./migration.sh &lt;odi_master_password&gt; &lt;odi_user_password&gt; &lt;owb_workspace_owner_password&gt; &lt;configuration_file&gt; 
</code></pre>
<p>However, migration assistant failed with the following error.</p>
<blockquote>
<p>migraiton.sh[24]: &quot;${1:0:13}&quot;: 0403-011 the Specified substitution is not valid for this command.</p>
</blockquote>
","<database><oracle><data-warehouse>","2021-09-16 05:36:51","268","1","1","69205061","<p>The following is a workaround to fix the issue.</p>
<pre><code>$OWB_HOME/owb/bin/unix/ ~/.bashrc
./migration.sh &lt;odi_master_password&gt; &lt;odi_user_password&gt; &lt;owb_workspace_owner_password&gt; &lt;configuration_file&gt;
</code></pre>
<p>Or use full path of the bash before the migration assistant script.</p>
<pre><code>/usr/bin/bash migration.sh &lt;odi_master_password&gt; &lt;odi_user_password&gt; &lt;owb_workspace_owner_password&gt; &lt;configuration_file&gt;
</code></pre>
"
"69172415","OBIEE12c configuration assistant failed with Cannot find identity keystore file","<p>I am getting the following error while run the obiee12c configuration assistant.</p>
<blockquote>
<p>weblogic.nodemanager.common.ConfigException: Identity key store file not found DemoIdentity.jks</p>
</blockquote>
<p>The following is the error log:</p>
<blockquote>
<p>&lt;Feb 6, 2021 10:16:38,503 PM JST&gt;    &lt;Cannot find identity keystore file &lt;DOMAIN_HOME&gt;/security/DemoIdentity.jks on server AdminServer&gt;
&lt;Feb 6, 2021 10:16:38,504 PM JST&gt;    &lt;Inconsistent security configuration, weblogic.management.configuration.ConfigurationException: Cannot find identity keystore file &lt;DOMAIN_HOME&gt;/security/DemoIdentity.jks on server AdminServer&gt;
&lt;Feb 6, 2021 10:16:38,504 PM JST&gt;    &lt;Not listening for SSL, weblogic.management.configuration.ConfigurationException: Cannot find identity keystore file &lt;DOMAIN_HOME&gt;/security/DemoIdentity.jks on server AdminServer.&gt;
&lt;Feb 6, 2021 10:16:38,505 PM JST&gt;    &lt;The server is unable to create a server socket for listening on channel &quot;DefaultSecure[iiops][5]&quot;. The address 127.0.0.1 might be incorrect or another process is using port 7002: java.io.IOException: Cannot find identity keystore file &lt;DOMAIN_HOME&gt;/security/DemoIdentity.jks on server AdminServer&gt;</p>
</blockquote>
","<oracle><data-warehouse><obiee>","2021-09-14 05:51:03","391","0","1","69174397","<p>The following is the workaround to resolve the issue.</p>
<p>Step 1: Browse to the location '&lt;DOMAIN_HOME&gt;\security'</p>
<p>Step 2: Copy the file &quot;DemoIdentity.jks&quot; from '&lt;DOMAIN_HOME&gt;\security' location and paste to '&lt;NODEMANAGER_HOME&gt;\security' location</p>
<p>Step 3: Re-run the obiee12c configuration assistant.</p>
"
"69170025","How to resolve invalid JAVA CLASS in OWBSYS user","<p>I am getting some invalid java object after running the following select statement.</p>
<pre><code>select OBJECT_NAME, OBJECT_TYPE, STATUS  from ALL_OBJECTS where status='INVALID' and owner='OWBSYS';
</code></pre>
<pre><code>OBJECT_NAME OBJECT_TYPE STATUS
--------------- ------------------------------ ------------------- -------
/11d72cdd_TaskScheduler JAVA CLASS INVALID
/12b895ae_LexiComparator JAVA CLASS INVALID
/1327ed5a_LRUCacheLRUNode JAVA CLASS INVALID
/1aabcc3f_ServiceDoctor JAVA CLASS INVALID
/1dad730_NAMsgUtilComponentInf JAVA CLASS INVALID
/1ef21df6_NameAddrRes_ko JAVA CLASS INVALID
/20dd1937_ImmutableArray JAVA CLASS INVALID
/235a039f_NameAddrExceptions_e JAVA CLASS INVALID
/23c25276_UnhandledException JAVA CLASS INVALID
</code></pre>
","<oracle><etl><data-warehouse>","2021-09-13 23:12:12","622","1","1","69170502","<p>Alter the java class with RESOLVE clause</p>
<pre><code>Connect to OWBSYS user
</code></pre>
<p>Then execute the following statement.</p>
<pre><code>SQL&gt; ALTER JAVA CLASS &quot;&lt;java class&gt;&quot; RESOLVE;
</code></pre>
"
"69160682","Implicit conversion on hash key column is causes very slow insert","<p>I am trying to insert some data into some stage tables where the insert is taking far too long. For example a table containing up 600000 records is taking nearly an hour to complete. In the select part of the query we are are creating a hash of the columns which is later used for change detection. Due to the Data Warehouse methodology we are using, DataVault, we cannot remove the change hash. When I look at the execution plan I  see warnings of type conversion in expression. and example is shown below.</p>
<pre><code>&lt;Warnings&gt;
              &lt;PlanAffectingConvert ConvertIssue=&quot;Cardinality Estimate&quot; Expression=&quot;CONVERT(varchar(max),[load].[load_acbs_loan_product_dimension].[product_month],0)&quot; /&gt;
</code></pre>
<p>I have also included a example of an insert statement that we are using:</p>
<pre><code>      INSERT INTO [stage_acbs_balance_category_dimension]
      ( hk_h_balance_category_dimension
      , balance_category_key
      , balance_category_code
      , balance_category_description
      , balance_class_code
      , balance_class_description
      , user_define_code_1
      , user_define_code_2
      , user_define_code_3
      , user_define_code_4
      , user_define_code_5
      , bal_cat_short_name
      , include_bal_in_tax_reporting
      , include_in_billings_statements
      , include_in_past_due_reporting
      , dss_change_hash_acbs_balance_category_dimension_lroc
      , dss_record_source
      , dss_load_date
      , dss_create_time)
      SELECT  CAST(HASHBYTES('sha2_256',
               COALESCE(CAST(load_acbs_balance_category_dimension.balance_category_key AS VARCHAR(MAX)),'null')
               ) AS BINARY(32)) AS hk_h_balance_category_dimension 
           , load_acbs_balance_category_dimension.balance_category_key AS balance_category_key 
           , load_acbs_balance_category_dimension.balance_category_code AS balance_category_code 
           , load_acbs_balance_category_dimension.balance_category_description AS balance_category_description 
           , load_acbs_balance_category_dimension.balance_class_code AS balance_class_code 
           , load_acbs_balance_category_dimension.balance_class_description AS balance_class_description 
           , load_acbs_balance_category_dimension.user_define_code_1 AS user_define_code_1 
           , load_acbs_balance_category_dimension.user_define_code_2 AS user_define_code_2 
           , load_acbs_balance_category_dimension.user_define_code_3 AS user_define_code_3 
           , load_acbs_balance_category_dimension.user_define_code_4 AS user_define_code_4 
           , load_acbs_balance_category_dimension.user_define_code_5 AS user_define_code_5 
           , load_acbs_balance_category_dimension.bal_cat_short_name AS bal_cat_short_name 
           , load_acbs_balance_category_dimension.include_bal_in_tax_reporting AS include_bal_in_tax_reporting 
           , load_acbs_balance_category_dimension.include_in_billings_statements AS include_in_billings_statements 
           , load_acbs_balance_category_dimension.include_in_past_due_reporting AS include_in_past_due_reporting 
           , CAST(HASHBYTES('SHA2_256',
               COALESCE(CAST(load_acbs_balance_category_dimension.balance_category_code AS VARCHAR(MAX)),'null') +'||'+
               COALESCE(CAST(load_acbs_balance_category_dimension.balance_category_description AS VARCHAR(MAX)),'null') +'||'+
               COALESCE(CAST(load_acbs_balance_category_dimension.balance_class_code AS VARCHAR(MAX)),'null') +'||'+
               COALESCE(CAST(load_acbs_balance_category_dimension.balance_class_description AS VARCHAR(MAX)),'null') +'||'+
               COALESCE(CAST(load_acbs_balance_category_dimension.user_define_code_1 AS VARCHAR(MAX)),'null') +'||'+
               COALESCE(CAST(load_acbs_balance_category_dimension.user_define_code_2 AS VARCHAR(MAX)),'null') +'||'+
               COALESCE(CAST(load_acbs_balance_category_dimension.user_define_code_3 AS VARCHAR(MAX)),'null') +'||'+
               COALESCE(CAST(load_acbs_balance_category_dimension.user_define_code_4 AS VARCHAR(MAX)),'null') +'||'+
               COALESCE(CAST(load_acbs_balance_category_dimension.user_define_code_5 AS VARCHAR(MAX)),'null') +'||'+
               COALESCE(CAST(load_acbs_balance_category_dimension.bal_cat_short_name AS VARCHAR(MAX)),'null') +'||'+
               COALESCE(CAST(load_acbs_balance_category_dimension.include_bal_in_tax_reporting AS VARCHAR(MAX)),'null') +'||'+
               COALESCE(CAST(load_acbs_balance_category_dimension.include_in_billings_statements AS VARCHAR(MAX)),'null') +'||'+
               COALESCE(CAST(load_acbs_balance_category_dimension.include_in_past_due_reporting AS VARCHAR(MAX)),'null')
               ) AS BINARY(32)) AS dss_change_hash_acbs_balance_category_dimension_lroc 
           , load_acbs_balance_category_dimension.dss_record_source AS dss_record_source 
           , load_acbs_balance_category_dimension.dss_load_date AS dss_load_date 
           , getdate() AS dss_create_time 
      FROM [load_acbs_balance_category_dimension] load_acbs_balance_category_dimension
</code></pre>
<p>I am looking for a way to get rid of the implicit conversions and get the insert to perform. I initially thought of making the column a computed persisted hash but the hash is non-deterministic. Any ideas would be much appreciated.</p>
","<sql-server><performance><data-warehouse>","2021-09-13 09:53:59","222","0","1","69174323","<p>I opted to change VARCHAR(MAX) TO VARBINARY(MAX which gave a significant boost in performance, 1 hour down to 40 seconds. I am also looking to change the hashing algorithm.</p>
"
"69157000","Repository creation fails while upgrading OWB11gR1(11.1.0.7) to OWB11gR2(11.2.0.4)","<p>I need to a new workspace in OWB11gR2(11.2.0.4) to upgrade OWB11gR1(11.1.0.7). Repository Assistant fails after processing 64%. The following is the error log.</p>
<pre><code>main.TaskScheduler timer[5]20200714@08:45:58.058: 00&gt; oracle.wh.service.impl.assistant.ProcessEngine.display(ProcessEngine.java:2122): % = 0.8051529790660225
main.TaskScheduler timer[5]20200714@08:45:58.058: 00&gt; oracle.wh.service.impl.assistant.ProcessEngine.display(ProcessEngine.java:2122): -token name = LOADJAVA; -token type = 13
main.TaskScheduler timer[5]20200714@08:45:58.058: 00&gt; oracle.wh.service.impl.assistant.ProcessEngine.display(ProcessEngine.java:2122): ProcessEngine.token_db_min_ver =
main.TaskScheduler timer[5]20200714@08:45:58.058: 00&gt; oracle.wh.service.impl.assistant.ProcessEngine.display(ProcessEngine.java:2122): Before processing LOADJAVA Token
main.TaskScheduler timer[5]20200714@08:45:58.058: 00&gt; oracle.wh.service.impl.assistant.ProcessEngine.display(ProcessEngine.java:2122): ... I am in processLoadJavaToken ...
main.AWT-EventQueue-0[6]20200714@08:48:36.036: 00&gt; oracle.wh.ui.jcommon.WhButton@424c414: WhButton setLabel rtsString = Yes
main.AWT-EventQueue-0[6]20200714@08:48:36.036: 00&gt; oracle.wh.ui.jcommon.WhButton@424c414: WhButton setLabel rtsString = No
</code></pre>
<p>The following is the list of database patches.</p>
<pre><code>Patch  17906774: applied on Wed Aug 04 11:21:52 BDT 2021
Unique Patch ID:  17692968
   Created on 14 May 2014, 22:56:54 hrs PST8PDT
   Bugs fixed:
     17607032, 17974168, 17669786, 17561509, 16885825, 18274560, 17613052
     17461930, 16829998, 17251918, 17435868, 17279666, 17328020, 17006987
     18260620, 16833468, 18180599, 17292119, 17340242, 17296559, 15990966
     17438322, 17939651, 17359696, 18385759, 17820353, 17939225, 17715818
     18192446, 16960088, 17191248, 17422695

Patch  31668908     : applied on Mon Jul 12 16:13:02 BDT 2021
Unique Patch ID:  23822194
Patch description:  &quot;OJVM PATCH SET UPDATE 11.2.0.4.201020&quot;
   Created on 18 Sep 2020, 03:30:45 hrs PST8PDT
   Bugs fixed:
     23727132, 19554117, 19006757, 14774730, 18933818, 18458318, 18166577
     19231857, 19153980, 19058059, 19007266, 17285560, 17201047, 17056813
     19223010, 19852360, 19909862, 19895326, 19374518, 20408829, 21047766
     21566944, 19176885, 17804361, 17528315, 21811517, 22253904, 19187988
     21911849, 22118835, 22670385, 23265914, 22675136, 24448240, 25067795
     24534298, 25076732, 25494379, 26023002, 19699946, 26637592, 27000663
     25649873, 27461842, 27952577, 27642235, 28502128, 28915933, 29254615
     29774367, 29992392, 29448234, 30160639, 30534664, 30855121, 31306274
     30772207, 31476032, 30561292, 28394726, 26716835, 24817447, 23082876
     31668867

Patch  31537677     : applied on Thu Jul 08 11:53:10 BDT 2021
Unique Patch ID:  23852314
Patch description:  &quot;Database Patch Set Update : 11.2.0.4.201020 (31537677)&quot;
   
</code></pre>
","<oracle><etl><data-warehouse>","2021-09-13 03:40:35","36","0","1","69160608","<p>The following is the workaround to fix the issue.</p>
<p>Step 1: Rollback the patches 31668908 and 31537677. OWB does not support with OJVM patch newer than December 2018</p>
<p>Step 2:Re-run the repository assistant.</p>
"
"69149028","How to rollback applied patch in OWB (Oracle Warehouse Builder)","<p>I was wrongly applied the oracle generic patch(p24615704_112040_Generic) to my current OWB. However, I need to rollback this patch because for apply another patch.</p>
","<oracle><data-warehouse>","2021-09-12 06:30:46","75","2","1","69149173","<p>Run the following commands step by step.</p>
<p>Step 1: Export the following variables</p>
<pre><code>    export ORACLE_HOME=$ORACLE_BASE/product/11.2.0/dbhome_1
    export PATH=$ORACLE_HOME/perl/bin:$PATH
    export PATH=$ORACLE_HOME/OPatch:$PATH
</code></pre>
<p>Step 2: opatch rollback -id 24615704</p>
"
"69149015","What's the difference between master data and dimension data","<p>I am kind of confused with the difference between master data and dimension  data. Both of them are said to be relatively stable data, eg, organization information, employee information, producti information， compared with transactional data，such as order.</p>
<p>I would ask what't the difference betweeen master data and dimension data, I think that most dimension table are from master data when doing data analysis?</p>
<p>Thanks</p>
","<database><data-analysis><data-warehouse><olap><dimensional-modeling>","2021-09-12 06:27:55","152","0","1","69154790","<p>Master data is a classification of the type of data.</p>
<p>Dimensional data is a classification of the way of organising/structuring data</p>
"
"69148298","How to apply patch in OWB11gR2(Oracle Warehouse Builder)","<p>I need to apply patch in the current oracle warehouse builder. My database version is oracle database 11gR2 and OWB version is also 11gR2.</p>
","<oracle><etl><data-warehouse>","2021-09-12 03:49:09","80","2","1","69148415","<p>The following is the workaround</p>
<p>Step 1: Download required patch</p>
<p>Step 2: Extract and read readme file.</p>
<p>Step 3: Export the following variables</p>
<pre><code>    export ORACLE_HOME=$ORACLE_BASE/product/11.2.0/dbhome_1

    export PATH=$ORACLE_HOME/perl/bin:$PATH

    export PATH=$ORACLE_HOME/OPatch:$PATH
</code></pre>
<p>Step 4: Go to the patch folder and run the following command</p>
<pre><code>    opatch apply
</code></pre>
"
"69133675","Loading a .docx file into ETL/ELT tool?","<p>Could someone please guide me on how to extract a <code>.docx</code> file and load it onto a database using an <code>ETL</code>(Extract-Transform-Load) or <code>ELT</code>(Extract-Load-Transform) tool?</p>
<p>Assuming that the <code>.docx</code> file contains mostly unstructured data, isn't it an <code>ELT</code> tool I should go for instead of <code>ETL</code>?</p>
<p>The <code>ETL</code> and <code>ELT</code> tools I found this far didn't support the <code>MS Word</code> component. What other way is there to extract and store the content in a <code>.docx</code> file onto a database?</p>
<p>My requirement is to:</p>
<ol>
<li>Extract the data inside the <code>.docx</code> file,</li>
<li>Convert them into meaningful data, and</li>
<li>Store them onto a <code>data lake</code> so I can perform <code>data analysis</code>, and take productive decisions based on those results.</li>
</ol>
<p>It's just like how e-commerce companies convert customer reviews into meaningful data so they can take decisions to boost their sales. In my case, it's <code>Word</code> files I need to analyze.</p>
<p>I'm asking this because I've searched for so many <code>ETL</code> and <code>ELT</code> tools but couldn't find anything that supported <code>Word</code> files. Maybe it's because I haven't been searching for the right tool or the right way to do it?</p>
<p>If somebody knows a way, please guide me through the process. What should I start looking for? A tool, or a way to code the entire thing?</p>
<p>I've been looking for an answer for weeks now but didn't find a helpful answer. And it's starting to get really frustrating to see all the tools supporting every other component like social media, MongoDB, or whatever EXCEPT <code>Word</code> files.</p>
","<ms-word><etl><data-analysis><data-warehouse><data-extraction>","2021-09-10 14:22:25","94","0","1","69133934","<p>You have to do this in 2 steps:</p>
<ol>
<li>Extract the data from the .docx file to <code>txt</code> or <code>xml</code></li>
<li>Now use SSIS to import. (Azure Data Factory if you are in the cloud)</li>
</ol>
"
"69124544","SSAS and calculated dimention from multiple other dims","<p>Given 3 dimentions DimA and DimB and DimC with some DimSk, DimId and DimName as attributes the problem is defined as &quot;add to cube new attribute which is calculated as&quot;:</p>
<pre><code>NewAttr =
 CASE
  WHEN DimC.DimId IN (1, 2, 3) THEN 'A ' + DimA.DimName
  WHEN DimC.DimId IN (4, 5, 6) THEN 'B ' + DimB.DimName
 END
</code></pre>
<p>All dimentions are referenced directly from Fact by SKs.
How would You solve this in multidim SSAS cube?</p>
<p>!Warning! Spoilers below - try to think about solution before reading about my!</p>
<p>My current approach is to calculate CROSS JOIN (~100x100x100) beetween Dims IDs.
Then I can calculate composite NK for DimNew as ID <code>~ DimA.DimId+|+DimB.DimId+|+DimC.DimId</code>.
Then I can add this ID to Fact ETLs too, and build new ETL for new dim with NewAttr as expected.
Then I can add in cube new dim and add new fact column and join them by ID/SK.
Should work, or is there 10x better solution?</p>
<p>Final Fact can be like:</p>
<pre><code>FactId,  DimASK, DimBSK, DimCSK, DimNewSK
or
FactId,  DimASK, DimBSK, DimCSK, NewAttr
</code></pre>
<ul>
<li>second one is fast and dirty without NewDim on db - but fact can be processed partially so distinct can produce different NewAttr for same composite NK when DimNamein dims will change in time...</li>
</ul>
","<ssas><data-warehouse>","2021-09-09 20:47:45","29","0","1","69185260","<p>Okey dokey!
Seems like if You have all SKs in Fact then add new dimention with all needed, then calculate NewAtr, and then make new Dim in cube with all SKs as composite key - its key step.</p>
<p>Its important to not use DimNewSK from new dim from identity or something as key in cube.</p>
<p>Composite Key allows to make simple regular relation to fact and works without additional dummy keys.</p>
<p>Pictures are 1000s of words - its as simple as looks after all:
<a href=""https://i.stack.imgur.com/eppEG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eppEG.png"" alt=""relationship"" /></a>
<a href=""https://i.stack.imgur.com/MIPoo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MIPoo.png"" alt=""composite key"" /></a></p>
"
"69104269","No active warehouse selected in the current session - Select an active warehouse with the 'use warehouse' command","<p>I am following this tutorial: <a href=""https://quickstarts.snowflake.com/guide/data_engineering_with_dbt/#4"" rel=""nofollow noreferrer"">https://quickstarts.snowflake.com/guide/data_engineering_with_dbt/#4</a></p>
<p>when I try to run this in a worksheet:</p>
<pre><code>SELECT * 
  FROM &quot;KNOEMA_ECONOMY_DATA_ATLAS&quot;.&quot;ECONOMY&quot;.&quot;DATASETS&quot;
 WHERE &quot;DatasetName&quot; ILIKE 'US Stock%'
    OR &quot;DatasetName&quot; ILIKE 'Exchange%Rates%';
</code></pre>
<p>I get an error that <code>No active warehouse selected in the current session. Select an active warehouse with the 'use warehouse' command.</code></p>
<p>I am logged in as TESTUSER and as SECURITYADMIN role. I have also given necessary permissions:</p>
<p><a href=""https://i.stack.imgur.com/rgw7f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rgw7f.png"" alt=""enter image description here"" /></a></p>
<p>What can I try? I tried to run this <code>USE WAREHOUSE DBT_PROD_WH</code> in a separate worksheet but it doesn't make any difference. I am not sure which warehouse I am supposed to use and with which role specifically</p>
","<snowflake-cloud-data-platform><etl><data-warehouse><snowflake-schema><dbt>","2021-09-08 13:45:27","3743","0","2","69104426","<p>The warehouse has to be set incontext of current session:</p>
<pre><code>SELECT CURRENT_WAREHOUSE();
</code></pre>
<p>If it returns null then, it could be set up in WebUI.
<a href=""https://docs.snowflake.com/en/user-guide/ui-worksheet.html#overview-of-features"" rel=""nofollow noreferrer"">Using Worksheets for Queries - Overview of Features</a>, setting the warehouse using item number 4</p>
<blockquote>
<p>Dropdown menu:</p>
<p>Change the current database, schema, or warehouse for the current worksheet without losing your work.</p>
<p>Resume/suspend or resize your current warehouse.</p>
</blockquote>
<hr />
<p>Alternatively in the same worksheet:</p>
<pre><code>USE WAREHOUSE &lt;warehouse_name&gt;;

SELECT * 
  FROM &quot;KNOEMA_ECONOMY_DATA_ATLAS&quot;.&quot;ECONOMY&quot;.&quot;DATASETS&quot;
 WHERE &quot;DatasetName&quot; ILIKE 'US Stock%'
    OR &quot;DatasetName&quot; ILIKE 'Exchange%Rates%';
</code></pre>
"
"69104269","No active warehouse selected in the current session - Select an active warehouse with the 'use warehouse' command","<p>I am following this tutorial: <a href=""https://quickstarts.snowflake.com/guide/data_engineering_with_dbt/#4"" rel=""nofollow noreferrer"">https://quickstarts.snowflake.com/guide/data_engineering_with_dbt/#4</a></p>
<p>when I try to run this in a worksheet:</p>
<pre><code>SELECT * 
  FROM &quot;KNOEMA_ECONOMY_DATA_ATLAS&quot;.&quot;ECONOMY&quot;.&quot;DATASETS&quot;
 WHERE &quot;DatasetName&quot; ILIKE 'US Stock%'
    OR &quot;DatasetName&quot; ILIKE 'Exchange%Rates%';
</code></pre>
<p>I get an error that <code>No active warehouse selected in the current session. Select an active warehouse with the 'use warehouse' command.</code></p>
<p>I am logged in as TESTUSER and as SECURITYADMIN role. I have also given necessary permissions:</p>
<p><a href=""https://i.stack.imgur.com/rgw7f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rgw7f.png"" alt=""enter image description here"" /></a></p>
<p>What can I try? I tried to run this <code>USE WAREHOUSE DBT_PROD_WH</code> in a separate worksheet but it doesn't make any difference. I am not sure which warehouse I am supposed to use and with which role specifically</p>
","<snowflake-cloud-data-platform><etl><data-warehouse><snowflake-schema><dbt>","2021-09-08 13:45:27","3743","0","2","74085824","<p>I've hit the same issue, following the same tutorial... basically there are no warehouses by default on the Snowflake account.</p>
<p>My approach was to identify how I can create/manage the warehouse in this case.</p>
<p>I've used the <a href=""https://docs.snowflake.com/en/sql-reference/sql/create-warehouse.html"" rel=""nofollow noreferrer"">CREATE WAREHOUSE</a> command (i.e. <code>CREATE WAREHOUSE FIRST_WH</code>)</p>
<p>This got automatically selected as my current warehouse after it was created:
<a href=""https://i.stack.imgur.com/tgBa7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tgBa7.png"" alt=""enter image description here"" /></a></p>
<p>From there, just continue the tutorial.</p>
"
"69088081","Oracle SQL, Combine Update or Merge with Join, Insert Surrogate Keys into Fact table using 2 dimensions tables","<p>I have populated my dimension tables (<strong>Oracle SQL Dev</strong>.) For the next step I created the <strong><code>facts table</code> (F_Orders)</strong>, I <strong>loaded quantity, price</strong> , the <strong>Order_ID's</strong> and <code>surrogate keys</code> into the <code>facts table</code>.</p>
<p><strong>I want to Insert the latest Surrogate_ID's (customer, location, etc.) with the specific <code>record</code>.</strong>
In <strong>SA_ORDERS</strong> I can see which Order_ID is connected with which Customer_ID. Using Customer_ID in <strong>D_CUSTOMERS</strong> I can find the <code>Surrogate key</code>.</p>
<p>To perform this task, I used this query:</p>
<pre><code>INSERT INTO F_Orders 
(order_id
,quanity
,price
,cust_s_key)
       
SELECT
sa_order_items.order_id
,sa_order_items.quantity
,sa_order_items.unit_price
,d_customers.s_key
FROM
sa_order_items
JOIN sa_orders ON sa_order_items.order_id = sa_orders.order_id
JOIN d_customers ONd_customers.customer_id=sa_orders.customer_id
WHERE 
d_customers.&quot;Latest&quot; = 'Y'
AND d_customers.flag = 'I'
</code></pre>
<p>Now I was wondering, can I perform <code>update</code> for <code>Surrogate key</code> (cust_s_key) as I have 2 <code>joints</code> to use?
Unfortunately, I cant combine <code>update</code> with the <code>join</code> successfully. How could I <strong>combine</strong> both or <strong>may find another way</strong> to do it? Thanks a lot in advance.
PS. I know that in the <code>ETL</code> we won't need this update often.</p>
","<sql><oracle><etl><data-warehouse>","2021-09-07 12:15:56","153","0","1","69113004","<p>we can update the fact table using the same query</p>
<pre><code>UPDATE F_Orders fo SET fo.cust_s_key = (SELECT
distinct d_customers.s_key
FROM
sa_order_items
JOIN sa_orders ON sa_order_items.order_id = sa_orders.order_id
JOIN d_customers ONd_customers.customer_id=sa_orders.customer_id
WHERE 
d_customers.&quot;Latest&quot; = 'Y'
AND d_customers.flag = 'I'
AND sa_order_items.order_id = fo.order_id)
WHERE EXISTS (SELECT 1 FROM
sa_order_items
JOIN sa_orders ON sa_order_items.order_id = sa_orders.order_id
JOIN d_customers ONd_customers.customer_id=sa_orders.customer_id
WHERE 
d_customers.&quot;Latest&quot; = 'Y'
AND d_customers.flag = 'I'
AND sa_order_items.order_id = fo.order_id)
</code></pre>
"
"69086064","Role 'DBT_DEV_ROLE' specified in the connect string does not exist or not authorized","<p>I am following this tutorial: <a href=""https://quickstarts.snowflake.com/guide/data_engineering_with_dbt/#1"" rel=""nofollow noreferrer"">https://quickstarts.snowflake.com/guide/data_engineering_with_dbt/#1</a></p>
<p>I ran this in my worksheet after selecting the securityadmin role and then sysadmin role,</p>
<pre><code>-------------------------------------------
-- dbt credentials
-------------------------------------------
USE ROLE securityadmin;
-- dbt roles
CREATE OR REPLACE ROLE dbt_dev_role;
CREATE OR REPLACE ROLE dbt_prod_role;
------------------------------------------- Please replace with your dbt user password
CREATE OR REPLACE USER dbt_user PASSWORD = &quot;&lt;mysecretpassword&gt;&quot;;

GRANT ROLE dbt_dev_role,dbt_prod_role TO USER dbt_user;
GRANT ROLE dbt_dev_role,dbt_prod_role TO ROLE sysadmin;

-------------------------------------------
-- dbt objects
-------------------------------------------
USE ROLE sysadmin;

CREATE OR REPLACE WAREHOUSE dbt_dev_wh  WITH WAREHOUSE_SIZE = 'XSMALL' AUTO_SUSPEND = 60 AUTO_RESUME = TRUE MIN_CLUSTER_COUNT = 1 MAX_CLUSTER_COUNT = 1 INITIALLY_SUSPENDED = TRUE;
CREATE OR REPLACE WAREHOUSE dbt_dev_heavy_wh  WITH WAREHOUSE_SIZE = 'LARGE' AUTO_SUSPEND = 60 AUTO_RESUME = TRUE MIN_CLUSTER_COUNT = 1 MAX_CLUSTER_COUNT = 1 INITIALLY_SUSPENDED = TRUE;
CREATE OR REPLACE WAREHOUSE dbt_prod_wh WITH WAREHOUSE_SIZE = 'XSMALL' AUTO_SUSPEND = 60 AUTO_RESUME = TRUE MIN_CLUSTER_COUNT = 1 MAX_CLUSTER_COUNT = 1 INITIALLY_SUSPENDED = TRUE;
CREATE OR REPLACE WAREHOUSE dbt_prod_heavy_wh  WITH WAREHOUSE_SIZE = 'LARGE' AUTO_SUSPEND = 60 AUTO_RESUME = TRUE MIN_CLUSTER_COUNT = 1 MAX_CLUSTER_COUNT = 1 INITIALLY_SUSPENDED = TRUE;

GRANT ALL ON WAREHOUSE dbt_dev_wh  TO ROLE dbt_dev_role;
GRANT ALL ON WAREHOUSE dbt_dev_heavy_wh  TO ROLE dbt_dev_role;
GRANT ALL ON WAREHOUSE dbt_prod_wh TO ROLE dbt_prod_role;
GRANT ALL ON WAREHOUSE dbt_prod_heavy_wh  TO ROLE dbt_prod_role;

CREATE OR REPLACE DATABASE dbt_hol_dev; 
CREATE OR REPLACE DATABASE dbt_hol_prod; 
GRANT ALL ON DATABASE dbt_hol_dev  TO ROLE dbt_dev_role;
GRANT ALL ON DATABASE dbt_hol_prod TO ROLE dbt_prod_role;
GRANT ALL ON ALL SCHEMAS IN DATABASE dbt_hol_dev   TO ROLE dbt_dev_role;
GRANT ALL ON ALL SCHEMAS IN DATABASE dbt_hol_prod  TO ROLE dbt_prod_role;
</code></pre>
<p>I have this in my profiles.yml file:</p>
<pre><code>dbt_hol:
  target: dev
  outputs:
    dev:
      type: snowflake
      ######## Please replace with your Snowflake account name
      account: xyz.eu-central-1

      user: TEST
      ######## Please replace with your Snowflake dbt user password
      password: password

 role: dbt_dev_role
      database: dbt_hol_dev
      warehouse: dbt_dev_wh
      schema: public
      threads: 200
    prod:
      type: snowflake
      ######## Please replace with your Snowflake account name
      account: xyz.eu-central-1

      user: TEST
      ######## Please replace with your Snowflake dbt user password
      password: password

      role: dbt_prod_role
      database: dbt_hol_prod
      warehouse: dbt_prod_wh
      schema: public
threads: 200
</code></pre>
<p>Although I am following the tutorial, when I run <code>dbt debug</code>, I get an error that:</p>
<pre><code>Connection:
  account: xyz.eu-central-1
  user: TEST
  database: dbt_hol_dev
  schema: public
  warehouse: dbt_dev_wh
  role: dbt_dev_role
  client_session_keep_alive: False
  Connection test: ERROR

dbt was unable to connect to the specified database.
The database returned the following error:

  &gt;Database Error
  250001 (08001): Failed to connect to DB: xyz.eu-central-1.snowflakecomputing.com:443. Role 'DBT_DEV_ROLE' specified in the connect string does not exist or not authorized. Contact your local system administrator, or attempt to login with another role, e.g. PUBLIC.
</code></pre>
<p>What could I be doing wrong?</p>
","<database><snowflake-cloud-data-platform><etl><data-warehouse><dbt>","2021-09-07 09:55:39","474","1","1","69086749","<p>As I see, you try to connect using the user TEST:</p>
<pre><code>Connection:
  account: xyz.eu-central-1
  user: TEST
  database: dbt_hol_dev
  schema: public
  warehouse: dbt_dev_wh
  role: dbt_dev_role
  client_session_keep_alive: False
  Connection test: ERROR
</code></pre>
<p>On the other hand, you granted the dbt_dev_role to the following users:</p>
<pre><code>GRANT ROLE dbt_dev_role,dbt_prod_role TO USER dbt_user;
GRANT ROLE dbt_dev_role,dbt_prod_role TO ROLE sysadmin;
</code></pre>
<p>You need to grant the role to the user TEST.</p>
"
"69054224","many to many relationship in oracle analytics","<p>I'm using Oracle Analytics v5.9 , I have a datawarehouse including a bridge table for implementing a hierarchical dimension. I cannot find a way to join this table with my fact table and my dimension table. i even created a view joining bridge table with Dim table but still no luck.
does this version even support what i'm trying to do..?
thank you in advance.</p>
","<database-design><data-warehouse><oracle-analytics>","2021-09-04 09:53:22","51","0","1","69058524","<p>Multi-table dataset and Join many to many is available in 6.0.</p>
<p>You can find a video on the YouTube Analytics channel:</p>
<p><a href=""https://youtu.be/AaMR80PXl18"" rel=""nofollow noreferrer"">https://youtu.be/AaMR80PXl18</a></p>
<p>You can find also more information on features here:</p>
<p><a href=""https://docs.oracle.com/en/cloud/paas/analytics-cloud/acswn/index.html#ACSWN-GUID-CFF90F44-BCEB-49EE-B40B-8D040F02D476"" rel=""nofollow noreferrer"">https://docs.oracle.com/en/cloud/paas/analytics-cloud/acswn/index.html#ACSWN-GUID-CFF90F44-BCEB-49EE-B40B-8D040F02D476</a></p>
<p>Hopefully this is helping you.</p>
<p>Ben</p>
"
"69026880","Data warehouses and atomicity rarely coexist","<p>This came up in the context of a <a href=""https://twitter.com/BrentO/status/1433138838050836482"" rel=""nofollow noreferrer"">humorous Tweet</a> about wrapping a data warehouse overnight load in a transaction and how this would bloat the log file and eat up disk space. I'm not trying to disprove it but rather to understand it better - as to me it seems to imply that a partial load (due to error) should be allowed to complete which would mean that the DW would not accurately reflect the source system(s).</p>
<p>The only way that I can understand it is if the incomplete records would be loaded into an intermediate staging layer in the DW but not be processed further until completed by a subsequent overnight load and only then would be processed further.</p>
<p>I tried to research it further but without success so would be really grateful for any advice.</p>
","<etl><data-warehouse>","2021-09-02 08:40:32","33","0","1","69029628","<p>When an error happens during the loading of a DW you could:</p>
<ul>
<li>stop the load and rollback (either to the start of the load of a single target object, of a group of objects or the whole DW)</li>
<li>Stop the load and leave the DW as it is at that point</li>
<li>log the error and continue the load (either of the failing target or of other objects in the DW)</li>
</ul>
<p>Which option you choose is entirely dependent on your particular circumstances and you might have many different strategies in use at different points in your etl pipeline and depending on the number of errors. For example:</p>
<ul>
<li>The error may allow you to continue to load other dims/facts without affecting them</li>
<li>Your business might prefer a fact table to be loaded minus one erroring record rather than missing a complete day’s data until the error is fixed</li>
</ul>
"
"69016693","JSON data mapping in Java with Jackson","<p>I got stuck in mapping a json into my data structures &quot;the easy way&quot; using just Jackson decorators and I was wondering if there is a way to do this ...</p>
<p>The json that I try to read has the following structure:</p>
<pre><code>{
   &quot;animals&quot;: [
       {&quot;data_info&quot;:{&quot;ns&quot;:&quot;dog&quot;}, &quot;sound&quot;:&quot;bowwow&quot;, &quot;bites&quot;:True},
       {&quot;data_info&quot;:{&quot;ns&quot;:&quot;dog&quot;}, &quot;sound&quot;:&quot;woofWoof&quot;, &quot;bites&quot;:False},
       {&quot;data_info&quot;:{&quot;ns&quot;:&quot;cat&quot;}, &quot;sound&quot;:&quot;meeeOwww&quot;, &quot;age&quot;:5}
    ],
    &quot;data_info&quot;:{&quot;ns&quot;:&quot;animal&quot;}
}
</code></pre>
<p>So basically every data entity has a &quot;data_info&quot; object (mapped in my code from below to  DataTypeInfo) that has a property &quot;ns&quot; object (mapped in my code TypeInfo) which contains the object type. So this means that the discriminator for object types is always under data_info.ns</p>
<p>Here are my data entities:</p>
<pre><code>public class Animals extends DataTypeInfo {
    @JsonProperty(&quot;animals&quot;)
    List&lt;Mamals&gt; animals;
}

@JsonTypeInfo( use = JsonTypeInfo.Id.NAME, include = JsonTypeInfo.As.PROPERTY, property = &quot;data_info.ns&quot;)
@JsonSubTypes({
        @JsonSubTypes.Type(value = Cat.class, name = &quot;cat&quot;),
        @JsonSubTypes.Type(value = Dog.class, name = &quot;dog&quot;),
})

public abstract class Mamals extends DataTypeInfo {    
}

public class Cat extends Mammals {
    @JsonProperty(&quot;sound&quot;)
    private String sound;
    @JsonProperty(&quot;age&quot;)
    private in age;
}

public class Dog extends Mammals {    
    @JsonProperty(&quot;sound&quot;)
    private String sound;
    @JsonProperty(&quot;bites&quot;)
    boolean bites    
}

public class DataTypeInfo {
    @JsonProperty(&quot;data_info&quot;)
    TypeInfo typeInfo;
}

public class TypeInfo {
    @JsonProperty(&quot;ns&quot;)
    String nameSpace;
}
</code></pre>
<p>The error in my code is in the discriminator from the Mammals class: property = &quot;data_info.ns&quot; since this is intended to work with properties but I try to use a sub property ...
Is there a way to correctly declare the discriminator of the Mammal abstract class so that the correct Dog or Cat objects are instantiated ?</p>
","<java><json><jackson><jackson-databind>","2021-09-01 15:28:44","103","0","1","69103964","<p>The solution that I ended up with was to use a custom builder (JsonCreator) in the abstract class (for the example from above Mammals).</p>
<p>My updated Mammals class looks like this:</p>
<pre><code>    @JsonCreator
public static Mammals create(Map&lt;String,Object&gt; jsonMap) throws JsonProcessingException {
    Mammals result;
    Map type_info = (Map&lt;String,String&gt;) jsonMap.get(&quot;data_info&quot;);
    String type = (String) type_info.get(&quot;ns&quot;);

    ObjectMapper mapper = new ObjectMapper();
    String json = mapper.writeValueAsString(jsonMap);
    if (type.equals(&quot;cat&quot;)) {
        result = mapper.readValue(json, Cat.class);
    } else if (type.equals(&quot;dog&quot;)) {
        result = mapper.readValue(json, Dog.class);
    } else {
        throw new RuntimeException(&quot;Unknown entity type&quot;);
    }
    return result;
}
</code></pre>
<p>Since my root class (Animals) contains a list of Mammals, for every element of this list this creator is executed to build the proper instance of Mammals (Cat or Dog in my example).</p>
"
"68982843","Is it OK to store transactional primary key on data warehouse dimension table to relate between fact-dim?","<p>I have data source (postgres transactional system) like this (simplified, the actual tables has more fields than this) :
<a href=""https://i.stack.imgur.com/eSnaI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eSnaI.png"" alt=""ERD"" /></a></p>
<p>Then I need to create an ETL pipeline, where the required report is something like this :</p>
<ul>
<li>order number (from <code>sales_order_header</code>)</li>
<li>item name (from <code>sales_order_lines</code>)</li>
<li>batch shift start &amp; end (from <code>receiving_batches</code>)</li>
<li>delivered quantity, approved received quantity, rejected received quantity (from <code>receiving_inventories</code>)</li>
</ul>
<p>My design for fact-dim tables is this (simplified).</p>
<p><a href=""https://i.stack.imgur.com/T07vv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T07vv.png"" alt=""enter image description here"" /></a></p>
<p>What I don't know about, is the optimal ETL design.
Let's focus on how to insert the fact, and relationship between fact with <code>dim_sales_orders</code></p>
<p>If I have staging tables like these:</p>
<p><a href=""https://i.stack.imgur.com/B0j05.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B0j05.png"" alt=""enter image description here"" /></a></p>
<p>The ETL runs daily. After 22:00, there will be no more receiving, so I can run the ETL at 23:00.
Then I can just fetch data from <code>sales_order_header</code> and <code>sales_order_lines</code>, so at 23:00, the script can runs, kind of :</p>
<pre class=""lang-sql prettyprint-override""><code>INSERT
    INTO
    staging_sales_orders (
    SELECT
        order_number,
        item_name
    FROM
        sales_order_header soh,
        sales_order_lines sol
    WHERE
        soh.sales_order_id = sol.sales_order_header_id
        and date_trunc('day', sol.created_timestamp) = date_trunc('day', now())
    );
</code></pre>
<p>And for the fact table, can runs at 23:30, with query</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT
    soh.order_number,
    rb.batch_shift_start,
    rb.batch_shift_end,
    sol.item_name,
    ri.delivered_quantity,
    ri.approved_received_quantity,
    ri.rejected_received_quantity
FROM
    receiving_batches rb,
    receiving_inventories ri,
    sales_order_lines sol,
    sales_order_header soh
WHERE
    rb.batch_id = ri.batch_id
    AND ri.sales_order_line_id = sol.sales_order_line_id
    AND sol.sales_order_header_id = soh.sales_order_id
    AND date_trunc('day', sol.created_timestamp) = date_trunc('day', now())
</code></pre>
<p>But how to optimally load the data into fact table, particulary the fact table?</p>
<p>My approach</p>
<ol>
<li>select from <code>staging_sales_orders</code> and insert them into <code>dim_sales_orders</code>, using auto increment primary key.</li>
<li>before inserting into <code>fact_receiving_inventories</code>, I need to know the <code>dim_sales_order_id</code>. So  in that case, I select :</li>
</ol>
<pre class=""lang-sql prettyprint-override""><code>SELECT
    dim_sales_order_id
FROM
    dim_sales_orders dso
WHERE
    order_number = staging_row.order_number
    AND item_name = staging_row.item_name
</code></pre>
<ol start=""3"">
<li>then insert to fact table.</li>
</ol>
<p>Now what I doubt, is on point 2 (selecting from existing dim). In here, I select based on 2 varchar columns, which should be performance hit. Since in the normalized form, I'm thinking of modifying the staging tables, adding <code>sales_order_line_id</code> on both staging tables. Hence, during point 2 above, I can just do</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT
    dim_sales_order_id
FROM
    dim_sales_orders dso
WHERE
    sales_order_line_id = staging_row.sales_order_line_id
</code></pre>
<p>But as consequences, I will need to add <code>sales_order_line_id</code> into <code>dim_sales_orders</code>, which I don't find common on tutorials. I mean, adding transactional table PK, is technically can be done since I can access the data source. But is it a good DW fact-dim dimension, to add such transactional field (especially since it is PK)?
Or there is any other approach, rather than selecting the existing dim based on 2 varchars?</p>
<p>How to optimally select dimension id for fact tables?</p>
<p>Thanks</p>
","<postgresql><data-warehouse>","2021-08-30 10:48:32","74","0","1","68984074","<p>It is practically mandatory to include the source PK/BK in a dimension.</p>
<p>The standard process is to load your Dims and then load your facts. For the fact loads you translate the source data to the appropriate Dim SKs with lookups to the Dims using the PK/BK</p>
"
"68919134","SSIS type 2 SCD","<p>I have a fact and dimension tables, fact is 3NF table.</p>
<p>My fact table:</p>
<p><a href=""https://i.stack.imgur.com/HCfQy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HCfQy.png"" alt=""enter image description here"" /></a></p>
<p>My Dimension table(total 250 rows, unique ids):</p>
<p><a href=""https://i.stack.imgur.com/rqmKm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rqmKm.png"" alt=""enter image description here"" /></a></p>
<p>SCD on SSIS:</p>
<p><a href=""https://i.stack.imgur.com/YzEVf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YzEVf.png"" alt=""enter image description here"" /></a></p>
<p>When fact's a metascore changed, i want to change dimension's metascore value of related id.
When i execute ssis, scd component overwrites my dimension 250 rows to 1519 rows(fact's row amount) or doesn't detect changed rows, can you help me doing scd type 2-3? i worked a lot but cant do it...</p>
","<ssis><etl><data-warehouse>","2021-08-25 08:08:18","153","0","1","68919227","<p>Why does the dimension change by the metascore value of the fact table?
Sounds more like a 1:1 relationship.</p>
"
"68913977","Azure Data Factory vs Azure Logic Apps","<p>Trying to self teach some ETL skills and need some help with the best way to do certain tasks.  I am trying to bring in data from our customer service platform - Freshdesk - into our SQL data warehouse.  We are a microsoft house and I have done somewhat similar tasks using Microsoft Azure Data Factory and Microsoft logic apps.  The logic apps are low/no code and a bit more intuitive so I am trying to use that now.  However, I feel like Data Factory might be the most efficient way to do this (although currently not 100% sure how to do it).  Any help or point in the right direction is appreciated.</p>
","<azure><azure-data-factory><azure-logic-apps><data-warehouse>","2021-08-24 20:54:56","5026","2","1","68917021","<p><strong>Azure Data Factory</strong></p>
<blockquote>
<p>If a large volume of data has to be moved from a data lake or an
enterprise data warehouse (EDW) to Azure. For big data analytics, you
may need to import large amounts of data into Azure from several
sources. It is critical to achieve optimal performance and scalability
in each circumstance.</p>
</blockquote>
<p>Also, inside copy activity, you may consider boosting DTUs/Parallel copy choices, which can help you <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance"" rel=""nofollow noreferrer"">increase copy performance</a>.</p>
<p><strong>Logic Apps</strong></p>
<p>When it comes to conducting repeating activities, modern data platform Logic Apps are fantastic. There are several connectors available, and it appears that the only limit to what you can accomplish with logic applications is your own imagination.</p>
<p>The <a href=""https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger?tabs=data-factory"" rel=""nofollow noreferrer"">Event-Driven Architecture (EDA)</a> paradigm involves the generation-</p>
<blockquote>
<p>Detection, consumption, and reaction of events. Users commonly need to
activate pipelines based on storage account events, such as the
arrival or deletion of a file in an Azure Blob Storage account, in
data integration scenarios.</p>
</blockquote>
<p>Hence, Azure Data Factory would be a better option over Azure  Logic Apps, but when it comes to repetitive tasks we could use Azure Logic Apps to orchestrate ETL via ADF. The decision of which to employ would be greatly influenced by the case at hand, and it would be worthwhile to explore combining ADF, Logic Apps, and other services such as Functions.</p>
"
"68804949","Can we use data warehouse to store time series data?","<p>We have an airflow job that captures data from an API every 15 mins. Can I store this time series data in Data Warehouse? It's not fit the grain of any of our star schemas but will be used with the aggregated results from star schema when used with the BI tools. Is this the right place to store it?</p>
","<data-warehouse><star-schema>","2021-08-16 14:53:56","146","0","1","70282320","<p>Many Data Warehouse Solutions like Cloudera or Snowflake can support the most demanding time series analytics needs. They have the capability to handle everything from real-time data ingestion at scale to enriching the data from the data warehouse to making easily consumable visualizations.</p>
"
"68769883","mule3 to mule 4 expression to dataweave 2.0","<p>I'm new to migrating the mule 3 apps to mule 4 I have done almost conversion but one expression stopped my flow and not able to achieve the logic for it if anyone has an idea regarding the expression to transform please help me</p>
<p><strong>Expression:</strong></p>
<pre><code>if(flowVars.maindata.keySet().contains(payload.idCaseNumber))
{
flowVars.temporary=[];
flowVars.maindata.get(payload.idCaseNumber).add(map);
}
else
{
flowVars.temporary.add(previousdata);
vars.maindata.put(payload.idCaseNumber,temporary);
}
</code></pre>
<p>I have tried up to my knowledge on the above code but still I'm getting problem</p>
<pre><code>flowVars.maindata.get(payload.idCaseNumber).add(map);
</code></pre>
","<mule><dataweave><payload><mule4><data-mapping>","2021-08-13 09:27:04","354","0","1","68771970","<p>In Mule 3 the expression language is MEL. In Mule 4 it is DataWeave 2.0. You can't just translate directly. MEL is an imperative scripting language, similar to a subset of Java and it is easy to call Java methods. DataWeave 2.0 is a functional language. Furthermore Mule 4 operations (example: a , , etc) can only return one value, which can be assigned to the payload or to one variable.</p>
<p>For your snippet I'll assume that maindata is a map. You can use two set-variable to assign each variable:</p>
<pre><code>&lt;set-variable variableName=&quot;temporary&quot; value=&quot;#[ if( namesOf(vars.maindata) contains payload.idCaseNumber ) [] else vars.temporary ++ **previousdata** ]&quot; /&gt;

</code></pre>
<p>I don't know exactly what do you use for previousdata.</p>
<p>To update the variable maindata it is probably a good match for the <a href=""https://docs.mulesoft.com/mule-runtime/4.3/dw-operators#update-operator"" rel=""nofollow noreferrer"">update operator</a>, in a separate  or Transform operation, with the same condition than for vars.temporary.</p>
<p>Update:
I'll assume vars.maindata is a map, which DataWeave will consider an object, and each element is a list. As an example of doing an <a href=""https://docs.mulesoft.com/mule-runtime/4.3/dw-operators#upserting"" rel=""nofollow noreferrer"">'upsert'</a> operation with a <a href=""https://docs.mulesoft.com/mule-runtime/4.3/dw-operators#dynamic-selector"" rel=""nofollow noreferrer"">dynamic selector</a>:</p>
<pre><code>%dw 2.0
output application/java
var temporary=[5]
var maindata={ a:[1,2,3,4] }
var myKey=&quot;a&quot;
---
maindata  update {
        case data at .&quot;$(myKey)&quot;! -&gt;  if (data != null) data ++ temporary else temporary
}
</code></pre>
<p>You could replace in above script the DataWeave var <code>temporary</code> with the expression from my example above, and the other DataWeave variables with the Mule variables (<code>vars.name</code>) or payload. If you change in above example myKey to have value &quot;b&quot; you will see that key being added.</p>
"
"68731961","Best solution for weather data warehouse netcdf or grib","<p><em>Problem:</em></p>
<p>Looking for best solution to store and make easily available big amount of weather data for the machine learning specialists team.
Initially I'm fetching data from cds.climate.copernicus.eu in <strong>netCDF</strong> or <strong>grib</strong> format. There will be some around <strong>10-20Tb</strong> in grib or netCDF.</p>
<p><em>Requirements:</em></p>
<ul>
<li>ML specialists can easily query data for given location (point, polygon) in given time range.</li>
<li>Results are returned in reasonable time.</li>
</ul>
<p><em>Ideas:</em></p>
<ul>
<li><strong>Postgres</strong>. I thought that maybe pg would handle that amount of data. But the problem I encoutered with this is that loading data into postgres will take ages additionally it would take much more space than 10-20Tb (Because I planned to store that in row like format where you have two tables Point and WeatherMeasurement) Is it a good idea? Have anyone experience with this kind of data and pg?</li>
<li><strong>Amazon Redshift</strong>. Would it be good approach to use this solution for weather data. How to load netCDf or grib into it? I have zero experience with could solutions like this.</li>
<li><strong>Files</strong>. Just store data in the grib or netCDF files. I would write some simplified Python interface to fetch data from those files? But the questions is will the queries be fast enough? Have anyone experience with those?</li>
</ul>
","<postgresql><bigdata><data-warehouse><netcdf><grib>","2021-08-10 18:34:34","398","-1","1","68745885","<p>For data this size that you want to sub-select quickly along multiple dimensions I'd lean toward Redshift.  You will want to pay attention to how you want to query the data and establish the data model to provide the fastest access for the needed subsets.  You may want to get some help setting this up initially as trial-and-error approach will take a while with this data size.  Also Redshift isn't cheap at this scale so ask the budget questions too.  This can be reduced if the database only needs to be up part of the time.</p>
<p>Files isn't a terrible idea as long as you can partition the data such that only a subset of files need to be accessed for any query.  A partitioning strategy based on YEAR, MONTH, LAT-Decade, and LON-Decade might work - you'll need to understand what queries need to be performed and how fast (what's reasonable time?).  This approach will be the least cost.</p>
<p>There is also a combo option - Redshift Spectrum.  Redshift can utilize on database information AND in S3 stored data in the same queries.  Again setting up the Redshift data model and S3 partitioning will be critical but this combo could give you attributes that will be valuable.</p>
<p>For any of these options you will want to convert to a more database friendly format like Parquet (or even CSV).  This conversion process along with how to merge new data will need to be understood.  There are lots of cloud tools to help with this processing.</p>
<p>Given the size of data you are working with I'll stress again that learning as you go will be time consuming.  You will likely want to find experts in the tools you are working with (and at the data sizes you have) to get up quickly.</p>
"
"68700113","From operational data store to warehouse","<p>I have 3 ODS, so in order to create DWH, do I need a DWH for each ODSs, or the DWH select the tables from any of the ODSs?</p>
","<data-warehouse><snowflake-schema><fact-table><star-schema-datawarehouse>","2021-08-08 11:01:08","68","2","1","68700408","<p>The design of a DWH is based on your reporting requirements. The number and locations of any data sources is entirely irrelevant.</p>
<p>Also, please can you explain what you mean by the phrase &quot;the DWH select Dim table for any of the ODSs&quot;? It doesn't make much sense to me</p>
"
"68662071","How to Define Data Warehouse units in Synapse?","<p>We have DWUs option in Azure synapse for Dedicated SQL Pool, which start from 100 DWUs which as per document consists of Compute +Memory +IO.</p>
<p>but how to check what type of compute node it is ? because in document it says 100 DWU consists of 1 compute with 60 distributions and 60 GB of Memory.</p>
<p>but here what is the configuration of Compute Node ?</p>
<p>or if we can't find the configuration, how to calculate the required DWUs to process 10GB of Data.</p>
","<data-warehouse><azure-synapse>","2021-08-05 07:10:09","1010","1","2","68888539","<p>A dedicated SQL pool (formerly SQL DW) represents a collection of analytic resources that are being provisioned. Analytic resources are defined as a combination of CPU, memory, and IO.</p>
<p>These three resources are bundled into units of compute scale called Data Warehouse Units (DWUs). A DWU represents an abstract, normalized measure of compute resources and performance.</p>
<blockquote>
<p>What is the configuration of Compute Node ? And how to check what type of compute node it is?</p>
</blockquote>
<p>Each Compute node has a node ID that is visible in system views. You can see the Compute node ID by looking for the node_id column in system views whose names begin with sys.pdw_nodes.</p>
<p>For a list of these system views, see <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/system-catalog-views/sql-data-warehouse-and-parallel-data-warehouse-catalog-views?toc=/azure/synapse-analytics/sql-data-warehouse/toc.json&amp;bc=/azure/synapse-analytics/sql-data-warehouse/breadcrumb/toc.json&amp;view=azure-sqldw-latest&amp;preserve-view=true"" rel=""nofollow noreferrer"">Synapse SQL system views</a>.</p>
<blockquote>
<p>How to calculate the required DWUs to process 10GB of Data.</p>
</blockquote>
<p>Dedicated SQL pool (formerly SQL DW) is a <strong>scale-out</strong> system that can provision <strong>vast amounts of compute</strong> and <strong>query sizeable quantities of data</strong>.</p>
<p>To see its true capabilities for scaling, especially at larger DWUs, we recommend scaling the data set as you scale to ensure that you have enough data to feed the CPUs.</p>
<p>For more information, refer to <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/what-is-a-data-warehouse-unit-dwu-cdwu#how-many-data-warehouse-units-do-i-need"" rel=""nofollow noreferrer"">Data Warehouse Units (DWUs) for dedicated SQL pool (formerly SQL DW) in Azure Synapse Analytics.</a></p>
"
"68662071","How to Define Data Warehouse units in Synapse?","<p>We have DWUs option in Azure synapse for Dedicated SQL Pool, which start from 100 DWUs which as per document consists of Compute +Memory +IO.</p>
<p>but how to check what type of compute node it is ? because in document it says 100 DWU consists of 1 compute with 60 distributions and 60 GB of Memory.</p>
<p>but here what is the configuration of Compute Node ?</p>
<p>or if we can't find the configuration, how to calculate the required DWUs to process 10GB of Data.</p>
","<data-warehouse><azure-synapse>","2021-08-05 07:10:09","1010","1","2","69128391","<p>How many cores do you think you need to process 10GB data? It's a pretty complicated question to answer.
What are your queries doing? Are you doing 20 self joins? How much tempdb space is needed?
Breast way to find out will be to run experiments for your particular workload and resize such that you use 80% of available resources.</p>
"
"68645947","How do you answer a question when asked what is the granularity of the data you worked on?","<p>I have been asked the above question but i know only its meaning that its the finest level of data. for example, if you have name in fact table, then its detail such email, phone number,etc can be found in dimensions table. I have sample dataset and its area level analysis which i have worked on, Please explain granularity of data based upon this data.</p>
<p><strong>Dataset:</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>itemid</th>
<th>item</th>
<th>RID</th>
<th>Rname</th>
<th>Area</th>
<th>Time_Availability&gt;70%</th>
</tr>
</thead>
<tbody>
<tr>
<td>6222589</td>
<td>peanut banana</td>
<td>1000</td>
<td>Cafe adda</td>
<td>gachibowli</td>
<td>True</td>
</tr>
<tr>
<td>6355784</td>
<td>chocolate fudge</td>
<td>2000</td>
<td>Santosh hotel</td>
<td>Attapur</td>
<td>False</td>
</tr>
</tbody>
</table>
</div>
<p><em><strong>Area level of analysis of restaurant on boarding to a platform</strong></em></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Area</th>
<th>Total Ingested restaurants</th>
<th>Available</th>
<th>items_Available &gt;=5</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gachibowli</td>
<td>5</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>Attapur</td>
<td>5</td>
<td>4</td>
<td>2</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Thank you</strong></p>
","<database><database-design><data-modeling><data-warehouse>","2021-08-04 05:42:32","193","0","1","68648014","<p>The granularity of a fact table is the minimum set of attributes that will uniquely identify a measure.</p>
<p>For example (and I'm not saying this is a real world example), if you had a sales fact table and there could only be one sale per customer per day then &quot;per customer per day&quot; would be the granularity of that fact table. You might have other dimensions such as the store that the sale occurred in or the country where the transaction took place - but these would not affect the granularity if you could still only have one sale per customer per day, regardless of which store or country that transaction took place in</p>
"
"68640663","Snowflake BLANK_IF?","<p>Is there a way to load a particular value as blank or empty string in Snowflake.
For example, I have hex value x98 written in the file which represents blank, and I want to load this value in the column as an empty string (e.g. '').</p>
","<snowflake-cloud-data-platform><data-warehouse>","2021-08-03 17:47:22","776","0","1","68641565","<p><a href=""https://docs.snowflake.com/en/sql-reference/functions/nullif.html"" rel=""nofollow noreferrer"">NULLIF()</a> might be what you're after. Takes 2 values ... if they're equal then returns null. Other functions used  <a href=""https://docs.snowflake.com/en/sql-reference/functions/hex_decode_string.html"" rel=""nofollow noreferrer"">HEX_DECODE_STRING()</a> <a href=""https://docs.snowflake.com/en/sql-reference/functions/hex_encode.html"" rel=""nofollow noreferrer"">HEX_ENCODE()</a></p>
<pre><code>select 
 NULLIF('Snowflake',HEX_DECODE_STRING('536E6F77666C616B65'))
,HEX_ENCODE('Snowball')
</code></pre>
<p><a href=""https://i.stack.imgur.com/zIt0l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zIt0l.png"" alt=""enter image description here"" /></a></p>
<p>Side note there's not HEX to INT in snowflake but here's a <a href=""https://community.snowflake.com/s/article/faq-does-snowflake-have-a-hex-to-int-type-function"" rel=""nofollow noreferrer"">link</a> to showing you a simple UDF to do the conversion. As <a href=""https://www.hexdictionary.com/hex/0098"" rel=""nofollow noreferrer"">0098 hex -&gt; 152 decimal</a> you might be better doing this first?</p>
"
"68576430","What's the difference between Data Mesh and multiple Data Warehouses in a technical perspective?","<p>I've come across with the new concept &quot;Data Mesh&quot; recently.
After reading some blogs and watched introduction videos about Data Mesh, it's not clear to me what is the difference between Data Mesh and multiple Data Warehouses in an organisation from a technical perspective.</p>
<p>If anyone is familiar with this concept, could you please share with me:</p>
<ol>
<li>Except the &quot;domain oriented&quot; principle, what's the difference of a Data Mesh and multiple Data Warehouses for different domains?</li>
<li>How does Data Mesh solves the problem of integrating data from different apartments(meshes)?</li>
</ol>
<p>Thanks :)!</p>
<p>Here are some links for Data Mesh introduction:</p>
<p><a href=""https://martinfowler.com/articles/data-monolith-to-mesh.html"" rel=""nofollow noreferrer"">How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh</a></p>
<p><a href=""https://www.youtube.com/watch?v=_bmYXWCxF_Q"" rel=""nofollow noreferrer"">Introduction to Data Mesh</a></p>
","<bigdata><data-modeling><data-warehouse>","2021-07-29 13:02:04","114","1","1","73027992","<p>They are many differences, but one is the <strong>standardization of API</strong> to access the data and metadata. By nature, the data quantum, which is the &quot;atomic&quot; element of the data mesh is agnostic of its data store (or data stores). So when you are thinking about Observability, Dictionary, and Control access of your data quanta, you want uniformity.</p>
"
"68574949","How to map the data from a nested json object in flutter","<p>Here I get this JSON object from the API and I need to add it to a list and return so that I can get it from the snapshot to display the data.But i get the snapshot.data as null.Please help me to solve this issue.</p>
<pre><code>...
{
    &quot;Data&quot;: [
        {
            &quot;product_name&quot;: &quot;MACC Tea Master Blend 40 Bags&quot;,
            &quot;img_url&quot;: &quot;1605262901.jpg&quot;,
            &quot;order_no&quot;: &quot;1625809545122&quot;,
            &quot;category&quot;: [
                {
                    &quot;category_name&quot;: &quot;01 Box (40 Bags)&quot;,
                    &quot;order_no&quot;: &quot;1625809545122&quot;,
                    &quot;qty&quot;: &quot;1&quot;,
                    &quot;line_total&quot;: &quot;1.79&quot;
                }
            ]
        }
    ],
    &quot;ID&quot;: &quot;200&quot;
}
...
</code></pre>
<p>This is the code on how i tried so far.</p>
<pre><code>...
Future&lt;List&lt;OrderDetails&gt;&gt; fetchMyOrderDetails(order_no) async {
  var body = jsonEncode({&quot;order_no&quot;: order_no});
  print(&quot;order_no : &quot; + order_no);
  http.Response response = await http.post(
      Uri.encodeFull(api + &quot;get_order_details_by_orderno.php&quot;), //url
      headers: {&quot;Accept&quot;: &quot;application/json&quot;},
      body: body);
  if (response.statusCode == 200) {
    Map&lt;String, dynamic&gt; map = json.decode(response.body);
    // var map = json.decode(response.body);
    print(&quot;response.body : &quot; + &quot;${response.body}&quot;);
    print(&quot;map : &quot; + &quot;${map['Data']}&quot;);

    List&lt;OrderDetails&gt; orderDetailsList;
    orderDetailsList = (json.decode(response.body)['Data'] as List)
        .map((i) =&gt; OrderDetails.fromJson(i))
        .toList();

    return orderDetailsList;
  } else {
    // print(&quot;Failed to load categories&quot;);
    throw Exception('Failed to load the Orders');
  }
}

class OrderDetails {
  final String product_name;
  final String img_url;
  final String order_no;
  final List&lt;Category&gt; category;

  OrderDetails({
    this.product_name,
    this.img_url,
    this.order_no,
    this.category,
  });

  factory OrderDetails.fromJson(Map&lt;String, dynamic&gt; json) {
    return OrderDetails(
      product_name: json['product_name'] as String,
      img_url: json['img_url'] as String,
      order_no: json['order_no'] as String,
      category: json['category'] as List,
    );
  }
}

class Category {
  final String category_name;
  final String qty;
  final String line_total;

  Category({this.category_name, this.qty, this.line_total});

  factory Category.fromJson(Map&lt;String, dynamic&gt; json) {
    return Category(
      category_name: json['category_name'] as String,
      qty: json['qty'] as String,
      line_total: json['line_total'] as String,
    );
  }
}
...
</code></pre>
<p>From the below code i try to access the data but the snapshot.data get null and the page is loading.</p>
<pre><code>...
    child: FutureBuilder&lt;List&lt;OrderDetails&gt;&gt;(
        future: fetchMyOrderDetails(order_no),
        builder: (BuildContext context, AsyncSnapshot snapshot) {
          print(&quot;snapshot data : &quot; + &quot;${snapshot.data}&quot;);
          if (snapshot.data == null) {
            return Container(
              child: Center(
                child: CircularProgressIndicator(),
              ),
            );
          } else {
            return Center(
              child: Text(snapshot.data.product_name),
            );
          }
        },
      ),
...
</code></pre>
","<flutter><dart><null><snapshot><data-mapping>","2021-07-29 11:18:32","1178","0","1","68575994","<p>Please update <code>OrderDetails</code> class.</p>
<p><code>json['category'] as List</code> is <code>List&lt;dynamic&gt;</code> , not <code>List&lt;Category&gt;</code></p>
<pre><code>factory OrderDetails.fromJson(Map&lt;String, dynamic&gt; json) {
    return OrderDetails(
      product_name: json['product_name'] as String,
      img_url: json['img_url'] as String,
      order_no: json['order_no'] as String,
      category: (json['category'] == null) 
                    ? null 
                    : (json['category'] as List).map(e =&gt; Category.fromJson(e)).toList(),
    );
}


Future&lt;List&lt;OrderDetails&gt;&gt; fetchMyOrderDetails(order_no) async {
  var body = jsonEncode({&quot;order_no&quot;: order_no});
  print(&quot;order_no : &quot; + order_no);
  http.Response response = await http.post(
      Uri.encodeFull(api + &quot;get_order_details_by_orderno.php&quot;), //url
      headers: {&quot;Accept&quot;: &quot;application/json&quot;},
      body: body);
  if (response.statusCode == 200) {
    Map&lt;String, dynamic&gt; map = json.decode(response.body);        
    print(&quot;response.body : &quot; + &quot;${response.body}&quot;);
    print(&quot;map : &quot; + &quot;${map['Data']}&quot;);

    List&lt;OrderDetails&gt; orderDetailsList;
    orderDetailsList = (map['Data'] as List)
        .map((i) =&gt; OrderDetails.fromJson(i))
        .toList();

    return orderDetailsList;
  } else {
    // print(&quot;Failed to load categories&quot;);
    throw Exception('Failed to load the Orders');
  }
}
</code></pre>
"
"68558270","SCD-2 in data modelling: how do I detect changes?","<p>I know the concept of SCD-2 and I'm trying to improve my skills about it doing some practices.</p>
<p>I have the next scenario/experiment:</p>
<ol>
<li>I'm calling daily to a rest API to extract information about companies.
In my initial load to the DB everything is new, so everything is very easy.</li>
<li>Next day I call to the same rest API, which might returns the same companies, but some of them might have (or not) some changes (i.e., they changed the size, the profits, the location, ...)</li>
</ol>
<p>I know SCD-2 might be really simple if the rest API returns just records with changes, but in this case it might returns as well records without changes.</p>
<p>In this scenario, how people detect if the data of a company has changes or not in order to apply SCD-2?, do they compare all the fields?.</p>
<p>Is there any example out there that I can see?</p>
","<database><data-modeling><data-warehouse><scd2>","2021-07-28 09:59:56","334","1","1","68558939","<p>There is no standard SCD-2 nor even a unique concept of it. It is a general term for large number of possible approaches. The only chance is to practice and see what is suitable for your use case.</p>
<p>In any case you must identify the <strong>natural key</strong> of the dimension and the <strong>set of the attributes</strong> you want to keep the history.</p>
<p>You may of course make it more complex by the decision to use your own <strong>surrogate key</strong>.</p>
<p>You mentioned that there are <strong>two main types of the interface</strong> for the process:</p>
<p>•   You get periodically a full set of the dimension data</p>
<p>•   You get the “changes only” (aka delta interface)</p>
<p>Paradoxically the former is <em>much simple</em> to handle than the latter.</p>
<p>First of all, in the full <em>dimensional snapshot</em> the <em>natural key</em> holds, contrary to the delta interface (where you may get more changes for one entity).</p>
<p>Additionally you have to handle the case of <em>late change delivery</em> or even the <em>wrong order of changes</em> delivery.</p>
<p>Next important decision is if you expect <strong>deletes</strong> to occur. This is again trivial in the full interface, you must define some convention, how this information would be passed in the delta interface.
Connected is the question whether a previously deleted entity can be <strong>reused</strong> (i.e. reappear in the data).</p>
<p>If you support <em>delete/reuse</em> you'll have to thing about how to show them in your dimension table.</p>
<p>In any case you will need some additional columns in the dimension to cover the historical information.</p>
<p>Some implementation use a <em>change_timestamp</em>, some other use validity interval <em>valid_from</em> and <em>valid_to</em>.</p>
<p>Even other implementation claim that additional <em>sequence number</em> is required – so you avoid the trap of more changes with the identical  timestamp.</p>
<p><em>So you see that before you look for some particular implementation you need carefully decide the options above.</em> For example the <em>full</em> and <em>delta</em> interface leads to a completely different implementations.</p>
"
"68555327","Watson Knowledge Catalogue missing the categories selection","<p>I'm trying to use WKC within my trial IBM Cloud account, I have the instance created but when I look to create the categories I am unable to see the &quot;categories&quot; option under the &quot;governance&quot; area within the menus. Specifically I only see under governance the following 3 options: Data dashboard, policy manager, business gloassary. Anyone know why I'm not able to see the full set?</p>
","<watson-knowledge-catalog><data-governance>","2021-07-28 06:20:42","66","0","1","68579935","<p>CPD menu items are dependent on the roles the current logged in user has.  If your exploring CPD, try using an ADMIN role.  If you are logged in as admin, ensure you have access to the &quot;category&quot; name in question.</p>
"
"68538522","Create household head based on age and member id","<p>I have a data frame of household members containing 3 integer columns, 'hid', 'sub', and 'age'. I'd like to create a new logical variable in the data frame called 'hh' representing the household head, defined as follows:</p>
<ol>
<li>If there is only 1 member in the household, then the value is TRUE,</li>
<li>If there are 2 or more members in the household, then the household head is the one who is aged between 18 and 65 (inclusive) and has the smallest subject id ('sub') among those aged between 18 and 65.</li>
<li>If there are no members in the household aged between 18 and 65, then the household head is the one with the smallest subject id.</li>
</ol>
<p>There must be 1 and only 1 household head per household.</p>
<p>My data looks something like this:</p>
<pre><code># A tibble: 10 x 3
     hid   sub   age
   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
 1     1     1    75
 2     1     2    55
 3     2     1    35
 4     3     1    69
 5     3     2    72
 6     4     1    69
 7     5     1    15
 8     5     2    17
 9     5     3    42
10     6     1    72
</code></pre>
<p>And I'd like the result to be like this:</p>
<pre><code>&gt; result
# A tibble: 10 x 4
     hid   sub   age hh   
   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;
 1     1     1    75 FALSE  # Not 18-65 &amp; there is another aged 18-65 within this household.
 2     1     2    55 TRUE   # Aged 18-65 and the smallest sub id within this household.
 3     2     1    35 TRUE   # Only 1 in this household.
 4     3     1    69 TRUE   # Not aged 18-65, but no other member is and smallest sub id.
 5     3     2    72 FALSE  # Not aged 18-65, and not the smallest sub id.
 6     4     1    69 TRUE   # Only 1 in this household.
 7     5     1    15 FALSE  # Not aged 18-65 and others in this household qualify.
 8     5     2    17 FALSE  # Not aged 18-65 and others in this household qualify.
 9     5     3    42 TRUE   # Aged 18-65 and the smallest sub id among those aged 18-65 within this household.
10     5     4    62 FALSE  # Aged 18-65 but not the smallest sub id among those aged 18-65 within this household.
</code></pre>
<p>Thank you!</p>
<hr />
<pre><code>d &lt;- structure(list(hid = c(1, 1, 2, 3, 3, 4, 5, 5, 5, 5), 
                      sub = c(1, 2, 1, 1, 2, 1, 1, 2, 3, 4),
                      age = c(75, 55, 35, 69, 72, 69, 15, 17, 42, 62)), 
                 row.names = c(NA, -10L), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;))
</code></pre>
","<r><dplyr><data-manipulation><data-management>","2021-07-27 03:15:56","83","1","3","68538556","<p>Here is an option</p>
<pre><code>library(dplyr)
d %&gt;% 
    group_by(hid) %&gt;%
     mutate(hh = if(n() == 1) TRUE else if(n() &gt; 1 &amp; 
         !any(between(age, 18, 65))) age == min(age) else
        age == min(age[between(age, 18, 65)])) %&gt;%
    ungroup
</code></pre>
<p>-output</p>
<pre><code># A tibble: 10 x 4
     hid   sub   age hh   
   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;
 1     1     1    75 FALSE
 2     1     2    55 TRUE 
 3     2     1    35 TRUE 
 4     3     1    69 TRUE 
 5     3     2    72 FALSE
 6     4     1    69 TRUE 
 7     5     1    15 FALSE
 8     5     2    17 FALSE
 9     5     3    42 TRUE 
10     5     4    62 FALSE
</code></pre>
<hr />
<p>Or another simplified option is</p>
<pre><code>d %&gt;% 
    mutate(rn = row_number()) %&gt;%
    arrange(hid, sub, age) %&gt;%
    group_by(hid) %&gt;% 
    mutate(hh = age == coalesce(age[between(age, 18, 65)][1], 
           first(age))) %&gt;% 
    ungroup %&gt;%
    arrange(rn) %&gt;%
    select(-rn)
</code></pre>
<p>-output</p>
<pre><code># A tibble: 10 x 4
     hid   sub   age hh   
   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;
 1     1     1    75 FALSE
 2     1     2    55 TRUE 
 3     2     1    35 TRUE 
 4     3     1    69 TRUE 
 5     3     2    72 FALSE
 6     4     1    69 TRUE 
 7     5     1    15 FALSE
 8     5     2    17 FALSE
 9     5     3    42 TRUE 
10     5     4    62 FALSE
</code></pre>
"
"68538522","Create household head based on age and member id","<p>I have a data frame of household members containing 3 integer columns, 'hid', 'sub', and 'age'. I'd like to create a new logical variable in the data frame called 'hh' representing the household head, defined as follows:</p>
<ol>
<li>If there is only 1 member in the household, then the value is TRUE,</li>
<li>If there are 2 or more members in the household, then the household head is the one who is aged between 18 and 65 (inclusive) and has the smallest subject id ('sub') among those aged between 18 and 65.</li>
<li>If there are no members in the household aged between 18 and 65, then the household head is the one with the smallest subject id.</li>
</ol>
<p>There must be 1 and only 1 household head per household.</p>
<p>My data looks something like this:</p>
<pre><code># A tibble: 10 x 3
     hid   sub   age
   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
 1     1     1    75
 2     1     2    55
 3     2     1    35
 4     3     1    69
 5     3     2    72
 6     4     1    69
 7     5     1    15
 8     5     2    17
 9     5     3    42
10     6     1    72
</code></pre>
<p>And I'd like the result to be like this:</p>
<pre><code>&gt; result
# A tibble: 10 x 4
     hid   sub   age hh   
   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;
 1     1     1    75 FALSE  # Not 18-65 &amp; there is another aged 18-65 within this household.
 2     1     2    55 TRUE   # Aged 18-65 and the smallest sub id within this household.
 3     2     1    35 TRUE   # Only 1 in this household.
 4     3     1    69 TRUE   # Not aged 18-65, but no other member is and smallest sub id.
 5     3     2    72 FALSE  # Not aged 18-65, and not the smallest sub id.
 6     4     1    69 TRUE   # Only 1 in this household.
 7     5     1    15 FALSE  # Not aged 18-65 and others in this household qualify.
 8     5     2    17 FALSE  # Not aged 18-65 and others in this household qualify.
 9     5     3    42 TRUE   # Aged 18-65 and the smallest sub id among those aged 18-65 within this household.
10     5     4    62 FALSE  # Aged 18-65 but not the smallest sub id among those aged 18-65 within this household.
</code></pre>
<p>Thank you!</p>
<hr />
<pre><code>d &lt;- structure(list(hid = c(1, 1, 2, 3, 3, 4, 5, 5, 5, 5), 
                      sub = c(1, 2, 1, 1, 2, 1, 1, 2, 3, 4),
                      age = c(75, 55, 35, 69, 72, 69, 15, 17, 42, 62)), 
                 row.names = c(NA, -10L), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;))
</code></pre>
","<r><dplyr><data-manipulation><data-management>","2021-07-27 03:15:56","83","1","3","68539108","<p>You can <code>arrange</code> the data in such a way that the first row of each group is the <code>hh</code> value you are looking for.</p>
<pre><code>library(dplyr)

d %&gt;%
  arrange(hid, !between(age, 18, 65), sub) %&gt;%
  mutate(hh = !duplicated(hid)) 

#     hid   sub   age hh   
#   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;
# 1     1     2    55 TRUE 
# 2     1     1    75 FALSE
# 3     2     1    35 TRUE 
# 4     3     1    69 TRUE 
# 5     3     2    72 FALSE
# 6     4     1    69 TRUE 
# 7     5     3    42 TRUE 
# 8     5     4    62 FALSE
# 9     5     1    15 FALSE
#10     5     2    17 FALSE          
</code></pre>
<p><code>!between(age, 18, 65)</code> would arrange the data keeping the individuals aged 18-65 first before others who are outside the range.</p>
"
"68538522","Create household head based on age and member id","<p>I have a data frame of household members containing 3 integer columns, 'hid', 'sub', and 'age'. I'd like to create a new logical variable in the data frame called 'hh' representing the household head, defined as follows:</p>
<ol>
<li>If there is only 1 member in the household, then the value is TRUE,</li>
<li>If there are 2 or more members in the household, then the household head is the one who is aged between 18 and 65 (inclusive) and has the smallest subject id ('sub') among those aged between 18 and 65.</li>
<li>If there are no members in the household aged between 18 and 65, then the household head is the one with the smallest subject id.</li>
</ol>
<p>There must be 1 and only 1 household head per household.</p>
<p>My data looks something like this:</p>
<pre><code># A tibble: 10 x 3
     hid   sub   age
   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
 1     1     1    75
 2     1     2    55
 3     2     1    35
 4     3     1    69
 5     3     2    72
 6     4     1    69
 7     5     1    15
 8     5     2    17
 9     5     3    42
10     6     1    72
</code></pre>
<p>And I'd like the result to be like this:</p>
<pre><code>&gt; result
# A tibble: 10 x 4
     hid   sub   age hh   
   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;
 1     1     1    75 FALSE  # Not 18-65 &amp; there is another aged 18-65 within this household.
 2     1     2    55 TRUE   # Aged 18-65 and the smallest sub id within this household.
 3     2     1    35 TRUE   # Only 1 in this household.
 4     3     1    69 TRUE   # Not aged 18-65, but no other member is and smallest sub id.
 5     3     2    72 FALSE  # Not aged 18-65, and not the smallest sub id.
 6     4     1    69 TRUE   # Only 1 in this household.
 7     5     1    15 FALSE  # Not aged 18-65 and others in this household qualify.
 8     5     2    17 FALSE  # Not aged 18-65 and others in this household qualify.
 9     5     3    42 TRUE   # Aged 18-65 and the smallest sub id among those aged 18-65 within this household.
10     5     4    62 FALSE  # Aged 18-65 but not the smallest sub id among those aged 18-65 within this household.
</code></pre>
<p>Thank you!</p>
<hr />
<pre><code>d &lt;- structure(list(hid = c(1, 1, 2, 3, 3, 4, 5, 5, 5, 5), 
                      sub = c(1, 2, 1, 1, 2, 1, 1, 2, 3, 4),
                      age = c(75, 55, 35, 69, 72, 69, 15, 17, 42, 62)), 
                 row.names = c(NA, -10L), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;))
</code></pre>
","<r><dplyr><data-manipulation><data-management>","2021-07-27 03:15:56","83","1","3","68539794","<p>An option with <code>case_when</code>,
each case_when is translating your conditions 1 to 3 into code:</p>
<pre><code>library(dplyr)

d %&gt;% 
    group_by(hid) %&gt;% 
    mutate(hh = case_when(max(sub) == 1 ~ TRUE,
                          max(sub) &gt; 1 &amp; 
                              between(age, 18, 65) &amp;
                              sub == min(sub[between(age, 18, 65)]) ~ TRUE,
                          max(between(age, 18, 65)) &lt; 1 &amp; 
                              sub == min(sub[max(between(age, 18, 65)) &lt; 1]) ~ TRUE,
                          TRUE ~ FALSE))
</code></pre>
<p>Output:</p>
<pre><code>     hid   sub   age hh   
   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;
 1     1     1    75 FALSE
 2     1     2    55 TRUE 
 3     2     1    35 TRUE 
 4     3     1    69 TRUE 
 5     3     2    72 FALSE
 6     4     1    69 TRUE 
 7     5     1    15 FALSE
 8     5     2    17 FALSE
 9     5     3    42 TRUE 
10     5     4    62 FALSE
</code></pre>
"
"68503167","OLAP architecture?","<p>I'm learning the architecture and design of datawarehouses and have a question on OLAP systems. So as I understand, historically (when computing power and memory weren't a commodity), data would be sourced and stored in a datawarehouse. And for each analytical/BI task, an OLAP system would need to be configured and the query results from the warehouse would be stored in individual cubes which were complex and high-maintenance. Fast-forward to modern age of cloud computing, data is stored either in optimized OLAP cubes, data marts or MPP solutions (columnar stores).</p>
<p>Couple of questions...</p>
<ol>
<li><p>Are OLAP cubes virtual/temporary or do they take up physical space on the server? Does data stored in data marts or columnar stores take up the same space as in the datawarehouse? If yes, I assume this isn't an issue now because memory space is readily available.</p>
</li>
<li><p>Is any part of how I understand this wrong? Feel free to correct me, like I said, I'm just learning.</p>
</li>
</ol>
","<database><data-warehouse><olap><oltp>","2021-07-23 17:36:46","187","1","1","68774310","<ol>
<li><p>Are OLAP cubes virtual/temporary or do they take up physical space on the server?
<br/>-&gt;OLAP cubes do take physical space on server. Read about processing cubes. This pulls the source data and stores it in proprietary format.</p>
</li>
<li><p>Does data stored in data marts or columnar stores take up the same space as in the datawarehouse?
<br/> -&gt; I think, columnar stores do reduce the space taken compared to row oriented database</p>
</li>
</ol>
"
"68502661","MongoDb data modelling, one large collection VS multiple smaller collections","<p>I have trouble deciding what data model structure i should have on my mongodb data. I have read that some says that one large collection is the best option, and some say that multiple smaller collection is the best option.</p>
<p>I have &quot;Users&quot; which will be stored in a collection.</p>
<p>The options i have looked at is:</p>
<p>1.</p>
<pre><code>Users (the collection):
  _id (what the documents include)
  name
  ...

Posts:
  _id
  user_id
  title
  body
</code></pre>
<p>To find all posts by the user I will query .collection(&quot;Posts&quot;).find({user_id: &quot;123test&quot;})</p>
<p>2.</p>
<pre><code>Users:
  _id
  name
  ...

(user1)Posts:
  _id
  title
  body

(user2)Posts:
  _id
  title
  body

(user3)Posts:
  _id
  title
  body

...
</code></pre>
<p>To find all posts by the user I will query .collection(user + &quot;Posts&quot;).find({})</p>
<p>The users and Posts will be growing constantly and i want something good for large amount of data. A user might have 1000-10000 Posts for example.
And there might be around 100 users at the end.
That will equal 10 000 000 Posts.
I will also be using multiple filters with multiple indexes on the Posts collection(s).</p>
<p>Im using Mongodb atlas serverless so what option is the most performant and cheapest?</p>
<p>Thank you for your time :)</p>
","<database><mongodb><data-structures><data-modeling><mongodb-atlas>","2021-07-23 16:52:57","105","0","1","68525417","<p>IMO the best solution here would be to go with the first approach, avoiding having abnormally bloated documents in the <code>Users</code> collection, on the lines of <a href=""https://www.mongodb.com/blog/post/building-with-patterns-the-subset-pattern"" rel=""nofollow noreferrer"">The Subset Pattern</a>. The reason being optimal usage of the <a href=""https://docs.mongodb.com/manual/reference/glossary/#std-term-working-set"" rel=""nofollow noreferrer"">Working Set</a>, which is stored in the RAM. Whenever you fire a query, it first tries to get resolved from the RAM, and if it is unable to, it goes to the disk to fetch data, thereby increasing latency.</p>
<p>Thus, if you have larger documents, the storage engine will be able to accommodate lesser number of those in the working set stored in RAM, vs., if you have smaller documents, it will be able to accommodate more number of documents, thereby increasing the probability of your queries getting resolved from the RAM itself, and not having to travel to disk or only for a limited set of data.</p>
<p>Apart from this, maintainability &amp; scalability is also better when you store <code>Posts</code> in a separate collection with a field called <code>user_id</code>, as you've shown in the first example. Also, if there's a use case wherein you have to show a user's top 10 posts or something like that, then you can store a <code>posts</code> field of type <code>Array&lt;Posts&gt;</code>, <a href=""https://docs.atlas.mongodb.com/schema-suggestions/avoid-unbounded-arrays/"" rel=""nofollow noreferrer"">making sure it is a bounded array with limited items, and not unbounded</a>.</p>
"
"68497022","Fact and Dim tables in Database or Data Warehouse?","<p>I am designing the schema for a database that takes in transactional user data, and the goal is to store the data, transform it, and display it back to the user in graphs on a dashboard.</p>
<p>My understanding is that we normalize the data to optimize storage in the Database and denormalize the data from the Database in the Data Warehouse into a format that can be used for data visualization/analysis.</p>
<p>Online, I have seen that Fact and Dim tables belong in the Data Warehouse, but I currently have them in my Database because of the nature of my data. Is my understanding wrong? I would think that the data warehouse joins the Fact and Dim tables in the Database and aggregates or performs other calculations for analysis?</p>
<p>I'm a newb with database design so any guidance would be massively appreciated!</p>
","<mysql><database><database-design><data-warehouse><database-normalization>","2021-07-23 09:26:58","235","1","1","68504659","<p>Yes, storing data in a &quot;Data Warehouse&quot; for later graphing is good.</p>
<p>Yes, &quot;normalization&quot; (into 'Dim' tables) is good, but it can be over-done.  Don't, for example, normalize DATEs or DATETIMEs; the downsides outweigh the benefits.</p>
<p>The typical &quot;Fact&quot; table needs very few indexes, possibly just a <code>PRIMARY KEY</code>.</p>
<p>Often, the data used in the Fact table needs to be summarized.  Then may happen even for graphing -- there is no use in plotting a million points.  You can boil it down to a thousand so that you don't choke the graphing software.</p>
<p>&quot;Summary table(s)&quot; should be built, then maintained incrementally (or continually).  Typically a summary table contains daily counts and subtotals for slices of the Fact table.  (&quot;aggregation&quot;)  Then the &quot;reports&quot; or fetches for graphs work against the Summary table.  Such tables have index(es) that make the queries efficient, in addition to having many fewer rows than the Fact table.</p>
<p>More discussion:  <a href=""http://mysql.rjweb.org/doc.php/datawarehouse"" rel=""nofollow noreferrer"">http://mysql.rjweb.org/doc.php/datawarehouse</a>  (and 3 related articles).</p>
<p>For further discussion, let's see an attempt at <code>CREATE TABLE</code> for your Fact table and one of the graphs that you need.</p>
"
"68490227","Flutter: How can i separate network and local data operations?","<p>If user want to register or login to application i want to fetch data from network. But if user want to work local application get data from local data. I use provider for data management by the way. Can someone give me some idea?</p>
","<flutter><flutter-provider><data-management>","2021-07-22 19:00:03","74","1","1","68604099","<ol>
<li><p>Create a parent <code>ChangeNotifier</code> and mark it abstract.</p>
</li>
<li><p>Include in it all common member variables and declare methods
without implementing them.</p>
</li>
<li><p>Now create two <code>ChangeNotifier</code> that extends the parent</p>
</li>
<li><p>Implement in each one the missing code for the methods in their
parent <code>ChangeNotifier</code></p>
</li>
<li><p>Use the correct child ChangeNotifier depending on the online/offline
state</p>
</li>
</ol>
<p>Here is an example :</p>
<pre><code>abstract class DataManagerNotifier extends ChangeNotifier {

  List currentData;
  String uID;

  void getData();

  void changeData();
}
</code></pre>
<p>and the 2 children will be like this :</p>
<pre><code>class OfflineDataManagerNotifier extends DataManagerNotifier {
  @override
  void changeData() {
    // TODO: implement changeData locally
  }

  @override
  void getData() {
    // TODO: implement getData locally
  }
  
}
</code></pre>
<p>and</p>
<pre><code>class OnlineDataManagerNotifier extends DataManagerNotifier {
  @override
  void changeData() {
    // TODO: implement changeData from network
  }

  @override
  void getData() {
    // TODO: implement getData from network
  }
  
}
</code></pre>
<p>finally, use them according to the state of user</p>
<pre><code>class Screen extends StatelessWidget {
  @override
  Widget build(BuildContext context) {

    if(isOnlineMode){
      return ChangeNotifierProvider&lt;DataManagerNotifier&gt;(
          create: (context) =&gt; OnlineDataManagerNotifier(),
          child: _ScreenContent()
      );
    }
    return ChangeNotifierProvider&lt;DataManagerNotifier&gt;(
        create: (context) =&gt; OfflineDataManagerNotifier(),
        child: _ScreenContent()
    );
  }
}
</code></pre>
<p>And don't forget to make <strong>ChangeNotifierProvider</strong> as <code>&lt;DataManagerNotifier&gt;</code></p>
"
"68488609","AWS Redshift or RDS for a Data warehouse?","<p>Right now we have an ETL that extracts info from an API, transforms, and Store in one big table in our OLTP database we want to migrate this table to some OLAP solution. This table is only read to do some calculations that we store on our OLTP database.</p>
<p>Which service fits the most here?</p>
<p>We are currently evaluating Redshift but never used the service before. Also, we thought of some snowflake schema(some kind of fact table with dimensions) in an RDS because is intended to store 10GB to 100GB but don't know how much this approach can scale.</p>
","<amazon-web-services><amazon-redshift><amazon-rds><data-warehouse>","2021-07-22 16:41:10","859","0","2","68489576","<p>AWS Redshift is really great when you only want to read the data from the database. Basically, Redshift in the backend is a column-oriented database that is more suitable for analytics. You can transfer all your existing data to redshift using the AWS DMS. AWS DMS is a service that basically needs your bin logs of the existing database and it will automatically transfer your data we don't have to do anything. From my Personal experience Redshift is really great.</p>
"
"68488609","AWS Redshift or RDS for a Data warehouse?","<p>Right now we have an ETL that extracts info from an API, transforms, and Store in one big table in our OLTP database we want to migrate this table to some OLAP solution. This table is only read to do some calculations that we store on our OLTP database.</p>
<p>Which service fits the most here?</p>
<p>We are currently evaluating Redshift but never used the service before. Also, we thought of some snowflake schema(some kind of fact table with dimensions) in an RDS because is intended to store 10GB to 100GB but don't know how much this approach can scale.</p>
","<amazon-web-services><amazon-redshift><amazon-rds><data-warehouse>","2021-07-22 16:41:10","859","0","2","68489656","<blockquote>
<p>Which service fits the most here?</p>
</blockquote>
<p>imho you could do a PoC to see which service is more feasible for you. It really depends on how much data you have, what queries and what load you plan to execute.</p>
<p>AWS Redshift is intended for OLAP on top of peta- or exa-bytes scale handling heavy parallel workload. RS can as well aggregate data from other data sources (jdbc, s3,..). However RS is not OLTP, it requires more static server overhead and extra skills for managing the deployment.</p>
<p>So without more numbers and use cases one cannot advice anything. Cloud is great that you can try and see what fits you.</p>
"
"68471957","Joining and Aggregating a Large Number of Fact Tables Efficiently in Redshift","<p>I have a number of (10M+ rows) fact tables in Redshift, each with a natural key <code>memberid</code> and each with a column <code>timestamp</code>. Let's say I have three tables: <code>transactions</code>, <code>messages</code>, <code>app_opens</code>, with <code>transactions</code> looking like this (all the other tables have similar structure):</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">memberid</th>
<th style=""text-align: center;"">revenue</th>
<th style=""text-align: right;"">timestamp</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">374893978</td>
<td style=""text-align: center;"">3.99</td>
<td style=""text-align: right;"">2021-02-08 18:34:01</td>
</tr>
<tr>
<td style=""text-align: left;"">374893943</td>
<td style=""text-align: center;"">7.99</td>
<td style=""text-align: right;"">2021-02-08 19:34:01</td>
</tr>
</tbody>
</table>
</div>
<p>My goal is to create a daily per-memberid aggregation table that looks likes this, with a row for each memberid and date:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">memberid</th>
<th style=""text-align: center;"">date</th>
<th style=""text-align: right;"">daily_revenue</th>
<th style=""text-align: right;"">daily_app_opens</th>
<th style=""text-align: right;"">daily_messages</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">374893978</td>
<td style=""text-align: center;"">2021-02-08</td>
<td style=""text-align: right;"">4.95</td>
<td style=""text-align: right;"">31</td>
<td style=""text-align: right;"">45</td>
</tr>
<tr>
<td style=""text-align: left;"">374893943</td>
<td style=""text-align: center;"">2021-02-08</td>
<td style=""text-align: right;"">7.89</td>
<td style=""text-align: right;"">23</td>
<td style=""text-align: right;"">7</td>
</tr>
</tbody>
</table>
</div>
<p>The SQL I'm currently using for this is the following, which involves unioning separate subqueries:</p>
<pre><code>SELECT memberid,
       date,
       max(NVL(daily_revenue,0)) daily_revenue,
       max(NVL(daily_app_opens,0)) daily_app_opens,
       max(NVL(daily_messages,0)) daily_messages
FROM 
 (
 SELECT memberid,
        trunc(timestamp) as date,
        sum(revenue) daily_revenue,
        NULL AS daily_app_opens,
        NULL AS daily_messages
 FROM transactions
 GROUP BY 1,2

 UNION ALL

 SELECT memberid,
        trunc(timestamp) as date,
        NULL AS daily_revenue,
        count(*) daily_app_opens,
        NULL AS daily_messages
 FROM app_opens
 GROUP BY 1,2

 UNION ALL

 SELECT memberid,
        trunc(timestamp) as date,
        NULL AS daily_revenue,
        NULL AS daily_app_opens,
        count(*) daily_messages
 FROM messages
 GROUP BY 1,2
)
GROUP BY memberid, date
</code></pre>
<p>This works fine and produces the expected output, but I'm wondering if this is the most efficient way to carry out this kind of query. I have also using <code>FULL OUTER JOIN</code> in place of <code>UNION ALL</code>, but the performance is essentially identical.</p>
<p>What's the most efficient way to achieve this in Redshift?</p>
","<sql><database><amazon-redshift><data-warehouse>","2021-07-21 15:03:23","181","0","1","68472799","<p>Seeing the EXPLAIN plan would help as it would let us see what the most costly parts of the query are.  Based on a quick read of the SQL it looks pretty good.  The cost of scanning the fact tables is likely meaningful but this is a cost you have to endure.  If you can restrict the amount of data read with a where clause this can be reduced but doing this may not meet your needs.</p>
<p>One place that you should review is the distribution of these tables.  Since you are grouping by accountid having this as the distribution key will make this process faster.  Grouping will need bring rows of the same accountid value together, distributing on these values will greatly cut down on network traffic within the cluster.</p>
<p>At large data sizes and with everything else optimized I'd expect UNION ALL to out perform FULL OUTER JOIN but this will depend on a number of factors (like how much the data size is reduced by the accountid aggregation).  10M rows is not very big in Redshift terms (I have 160M rows of wide data on a minimal cluster) so I don't think you will see much difference between these plans at these sizes.</p>
"
"68469604","Writing out group_by with length > 1 to individual text files in R","<p>Apologies, I am still getting acquainted with the world of dplyr and data.table, and trying to figure out its full capabilities!</p>
<p>I have a dataset where I am interested in grouping on a specific variable (locus):</p>
<pre><code>DF &lt;- structure(list(Gene = c(&quot;GeneA&quot;, &quot;GeneB&quot;, &quot;GeneC&quot;, &quot;GeneD&quot;, &quot;GeneE&quot;), 
                Locus = c(&quot;1&quot;,&quot;2&quot;,&quot;2&quot;,&quot;3&quot;,&quot;3&quot;),
                Chromosome = c(&quot;1&quot;,&quot;1&quot;,&quot;1&quot;,&quot;1&quot;,&quot;1&quot;),
                Start = c(&quot;100&quot;,&quot;500&quot;,&quot;600&quot;,&quot;1000&quot;,&quot;1500&quot;),
                Stop = c(&quot;200&quot;,&quot;550&quot;,&quot;700&quot;,&quot;1400&quot;,&quot;1750&quot;)),
                .Names = c(&quot;Gene&quot;,&quot;Locus&quot;,&quot;Chromosome&quot;,&quot;Start&quot;,&quot;Stop&quot;), 
                row.names = c(NA, 5L), 
                class = &quot;data.frame&quot;)

&gt; DF
   Gene Locus Chromosome Start Stop
GeneA     1          1   100  200
GeneB     2          1   500  550
GeneC     2          1   600  700
GeneD     3          1  1000 1400
GeneE     3          1  1500 1750
</code></pre>
<p>I was wondering whether it was possible to write out &quot;per locus&quot; files containing the values from the Gene, Chromosome, Start, Stop columns in instances where there are more than one value for the locus column. So Locus==1 would have no text file written out, but the values in the Gene column for Locus==2 and Locus==3 would be written to individual files?
e.g.</p>
<pre><code>&lt;loc2.txt&gt;
   Gene Chromosome Start Stop
GeneB           1   500  550
GeneC           1   600  700

&lt;loc3.txt&gt;
   Gene Chromosome Start Stop
GeneD           1  1000 1400
GeneE           1  1500 1750

</code></pre>
<p>Thanks in advance for any help!</p>
","<r><dplyr><data.table><data-management>","2021-07-21 12:37:06","58","0","1","68470254","<h2 id=""dplyr-bd28"">dplyr</h2>
<pre class=""lang-r prettyprint-override""><code>library(dplyr)
newDF &lt;- DF %&gt;%
  group_by(Locus) %&gt;%
  filter(n() &gt; 1) %&gt;%
  nest_by()
newDF
# # A tibble: 2 x 2
# # Rowwise:  Locus
#   Locus               data
#   &lt;chr&gt; &lt;list&lt;tbl_df[,4]&gt;&gt;
# 1 2                [2 x 4]
# 2 3                [2 x 4]
mapply(function(x, nm) write.csv(x, nm),
       newDF$data, paste0(&quot;loc&quot;, newDF$Locus, &quot;.csv&quot;))
# [[1]]
# NULL
# [[2]]
# NULL
</code></pre>
<p>The files are created in the current directory. You can safely ignore the <code>NULL</code> output from <code>mapply</code>.</p>
<h2 id=""data.table-d7mq"">data.table</h2>
<pre class=""lang-r prettyprint-override""><code>library(data.table)
DT &lt;- as.data.table(DF)
newDT &lt;- DT[, .SD[.N &gt; 1, .(data = list(.SD))], by = Locus]
newDT
#     Locus              data
#    &lt;char&gt;            &lt;list&gt;
# 1:      2 &lt;data.table[2x4]&gt;
# 2:      3 &lt;data.table[2x4]&gt;
mapply(function(x, nm) write.csv(x, nm),
       newDF$data, paste0(&quot;loc&quot;, newDF$Locus, &quot;.csv&quot;))
</code></pre>
"
"68464326","Star schema design feedback","<p>I work for an OTC company.</p>
<p>Here is the background on the workflow.
Taker (Buyer) --&gt; Requests a quote on derivatives --&gt; Maker(Quoter) responds with quotes (bid/ask) ---&gt; Taker either buys/sells ( traded) or takes no action on the quote. The quote can be sent and received from multiple makers.</p>
<p>Let me know if my star schema is correct.</p>
<p>I am thinking of takers, makers as dimension tables. Requests as one fact table and quotes as another fact table. Is this approach correct?</p>
","<data-warehouse><dimensional-modeling><star-schema><star-schema-datawarehouse>","2021-07-21 05:19:53","65","0","1","68489980","<p>So you’ve started to identify your measures, once you’ve identified them all the next steps are:</p>
<ol>
<li>Define the grain if each measure; measures with the same grain can exist in the sane fact table; measures with different grains need to be in different fact tables</li>
<li>For each fact table, identify the entities you need to filter or aggregate the facts; this gives you your dimensions</li>
</ol>
"
"68418208","aws glue studio inner join gives error when one of data catalogue has no records","<p>I am new to aws glue studio. I have created two tables in the AWS glue database with partition as the current date.</p>
<p>I am doing inner join &amp; left anti join to process the job.</p>
<p>If there is no match my glue job fails with the error</p>
<blockquote>
<p>AnalysisException: 'Cannot resolve column name &quot;column name&quot; among ();'</p>
</blockquote>
<p>I tried custom node &amp; select it from the collection before joining.. but no help. There are very limited options available in glue studio to play with. Let me know if there is a way in the glue studio itself to figure it out.</p>
","<amazon-web-services><aws-glue><aws-glue-data-catalog><aws-glue-spark><aws-glue-workflow>","2021-07-17 07:05:36","370","0","1","69125318","<p>I think you should deactivate the job bookmarks. If one of your underlying table does not have any new data, then it will return nothing. Therefore one, or many columns cannot be resolved.
Cheers</p>
"
"68405433","Can we store multiple types of data in a data warehouse?","<p>I want to ask that can we store various types of data in a Hadoop data warehouse? Data like RDBMS, JSON Doc, Cassandra Keyspace, txt, CSV, etc? Are they all stored in HDFS?</p>
","<database><hive><rdbms><data-warehouse><hadoop2>","2021-07-16 07:52:28","119","1","1","68406247","<p>Classic <code>DWH</code> is a repository for structured, filtered data that has already been processed for a specific purpose and all the data is being stored in the same format except landing zone (LZ or RAW) where data can be stored in the same format as it is loaded from source systems. DHW building process is based on Kimball or Inmon theory.</p>
<p>What you are asking about is a Data Lake - a modern concept - is a vast pool of raw data, the purpose for which can be not completely defined yet. In a DL you can store all structured alond with semi-structured data and data analysts can access both RAW semi-structured data and structured data in 3NF or dimentional form.</p>
<p>RDBMS normally add the abstraction layer between internal storage representation and means how it can be accessed, though storing data in external files in HDFS is possible for many RDBMS, this is used for convenient integration with Data Lake.</p>
<p>Yes, you can store everything in the same DL: semi-structured data, data in different storage formats like AVRO, CSV, Parquet, ORC, ETC, build Hive tables on it as well as different RDBMs tables, all can be stored in the same HDFS/S3/Azure/GCS/etc</p>
<p>Some layers are also can be created in DL like RAW/LZ/DM or based on domain event/business event model, this means that DL is not an absence of architecture constraints, normally you have some architecture design, and architecture constraints to follow in DL as well as in classic DWH.</p>
"
"68378084","How to generate point in time snapshot table from a transaction fact table?","<p>I have a transaction table that records the change in status (A,B,C,D) of a customer by closing the end date of the previous record and also opening a new record with the current system time and the end date for the new record will be set to a high open date.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>FactID</th>
<th>Cust_ID</th>
<th>Status</th>
<th>EffectiveDate</th>
<th>EndDate</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>A</td>
<td>20/05/2021 8:52:29 PM</td>
<td>21/05/2021 3:08:22 PM</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>B</td>
<td>21/05/2021 3:08:22 PM</td>
<td>24/05/2021 2:47:28 PM</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>C</td>
<td>24/05/2021 2:47:28 PM</td>
<td>24/05/2021 4:15:45 PM</td>
</tr>
<tr>
<td>4</td>
<td>1</td>
<td>A</td>
<td>24/05/2021 4:15:45 PM</td>
<td>24/05/2021 8:05:09 PM</td>
</tr>
<tr>
<td>5</td>
<td>1</td>
<td>D</td>
<td>24/05/2021 8:05:09 PM</td>
<td>31/12/9000</td>
</tr>
</tbody>
</table>
</div>
<p>I'm trying to build a snapshot at a point in time table (end of the day reporting) based on the above transaction table.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ReportDate</th>
<th>Cust_ID</th>
<th>EODStatus</th>
<th>A_SDate</th>
<th>A_EDate</th>
<th>B_SDate</th>
<th>B_EDate</th>
<th>C_SDate</th>
<th>C_EDate</th>
<th>D_SDate</th>
<th>D_EDate</th>
</tr>
</thead>
<tbody>
<tr>
<td>20/05/2021 11:59:59 PM</td>
<td>1</td>
<td>A</td>
<td>20/05/2021 8:52:29 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
</tr>
<tr>
<td>21/05/2021 11:59:59 PM</td>
<td>1</td>
<td>B</td>
<td>20/05/2021 8:52:29 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
</tr>
<tr>
<td>22/05/2021 11:59:59 PM</td>
<td>1</td>
<td>B</td>
<td>20/05/2021 8:52:29 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
</tr>
<tr>
<td>23/05/2021 11:59:59 PM</td>
<td>1</td>
<td>B</td>
<td>20/05/2021 8:52:29 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
</tr>
<tr>
<td>24/05/2021  11:59:59 PM</td>
<td>1</td>
<td>D</td>
<td>20/05/2021 8:52:29 PM</td>
<td>24/05/2021 8:05:09 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>24/05/2021 4:15:45 PM</td>
<td>24/05/2021 8:05:09 PM</td>
<td>31/12/9000</td>
</tr>
<tr>
<td>25/05/2021  11:59:59 PM</td>
<td>1</td>
<td>D</td>
<td>20/05/2021 8:52:29 PM</td>
<td>24/05/2021 8:05:09 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>24/05/2021 4:15:45 PM</td>
<td>24/05/2021 8:05:09 PM</td>
<td>31/12/9000</td>
</tr>
</tbody>
</table>
</div>
<p>I'm now stuck when I try to expand the transaction table before building the snapshot. Any pointers would be greatly appreciated.</p>
<pre><code>WITH
    date_ranges
    AS
        (SELECT ROWNUM, TO_DATE ('21-05-2021', 'dd-mm-yyyy') + ROWNUM - 1.00001 reportdate
           FROM all_objects
          WHERE ROWNUM &lt;= 6),
    transactions (factid, cust_id, status, effectivedate, enddate)
    AS
        (SELECT 1, 1, 'A', TO_DATE ('20/05/2021 8:52:29 PM', 'DD/MM/YYYY HH12:MI:SS AM'), TO_DATE ('21/05/2021 3:08:22 PM', 'DD/MM/YYYY HH12:MI:SS AM') FROM DUAL
         UNION ALL
         SELECT 2, 1, 'B', TO_DATE ('21/05/2021 3:08:22 PM', 'DD/MM/YYYY HH12:MI:SS AM'), TO_DATE ('24/05/2021 2:47:28 PM', 'DD/MM/YYYY HH12:MI:SS AM') FROM DUAL
         UNION ALL
         SELECT 3, 1, 'C', TO_DATE ('24/05/2021 2:47:28 PM', 'DD/MM/YYYY HH12:MI:SS AM'), TO_DATE ('24/05/2021 4:15:45 PM', 'DD/MM/YYYY HH12:MI:SS AM') FROM DUAL
         UNION ALL
         SELECT 4, 1, 'A', TO_DATE ('24/05/2021 4:15:45 PM', 'DD/MM/YYYY HH12:MI:SS AM'), TO_DATE ('24/05/2021 8:05:09 PM', 'DD/MM/YYYY HH12:MI:SS AM') FROM DUAL
         UNION ALL
         SELECT 5, 1, 'D', TO_DATE ('24/05/2021 8:05:09 PM', 'DD/MM/YYYY HH12:MI:SS AM'), TO_DATE ('31/12/9000', 'DD/MM/YYYY') FROM DUAL),
    dataset
    AS
        (SELECT DISTINCT reportdate,
                         cust_id,
                         status     AS eodstatus,
                         effectivedate,
                         enddate
           FROM transactions CROSS JOIN date_ranges)
  SELECT reportdate,
         cust_id,
         eodstatus,
         effectivedate,
         enddate,
         CASE
             WHEN eodstatus = 'A' THEN MIN (effectivedate)
             ELSE TO_DATE ('31/12/9000', 'DD/MM/YYYY')
         END             AS a_sdate,
         CASE WHEN eodstatus = 'A' THEN MAX (enddate) ELSE TO_DATE ('31/12/9000', 'DD/MM/YYYY') 
         END             AS a_edate,
         CASE
             WHEN eodstatus = 'B' THEN MIN (effectivedate)
             ELSE TO_DATE ('31/12/9000', 'DD/MM/YYYY')
         END             AS b_sdate,
         CASE WHEN eodstatus = 'B' THEN MAX (enddate) ELSE TO_DATE ('31/12/9000', 'DD/MM/YYYY') 
         END             AS b_edate,
         CASE
             WHEN eodstatus = 'C' THEN MIN (effectivedate)
             ELSE TO_DATE ('31/12/9000', 'DD/MM/YYYY')
         END             AS c_sdate,
         CASE WHEN eodstatus = 'C' THEN MAX (enddate) ELSE TO_DATE ('31/12/9000', 'DD/MM/YYYY') 
         END             AS c_edate,
         CASE
             WHEN eodstatus = 'D' THEN MIN (effectivedate)
             ELSE TO_DATE ('31/12/9000', 'DD/MM/YYYY')
         END             AS d_sdate,
         CASE WHEN eodstatus = 'D' THEN MAX (enddate) ELSE TO_DATE ('31/12/9000', 'DD/MM/YYYY') 
          END             AS d_edate
    FROM dataset t
   WHERE reportdate BETWEEN effectivedate AND enddate
GROUP BY reportdate, cust_id, eodstatus, effectivedate, enddate
ORDER BY reportdate, cust_id, eodstatus;
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>REPORTDATE</th>
<th>CUST_ID</th>
<th>EODSTATUS</th>
<th>EFFECTIVEDATE</th>
<th>ENDDATE</th>
<th>A_SDATE</th>
<th>A_EDATE</th>
<th>B_SDATE</th>
<th>B_EDATE</th>
<th>C_SDATE</th>
<th>C_EDATE</th>
<th>D_SDATE</th>
<th>D_EDATE</th>
</tr>
</thead>
<tbody>
<tr>
<td>20/05/2021 11:59:59 PM</td>
<td>1</td>
<td>&quot;A&quot;</td>
<td>20/05/2021 8:52:29 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>20/05/2021 8:52:29 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
</tr>
<tr>
<td>21/05/2021 11:59:59 PM</td>
<td>1</td>
<td>&quot;B&quot;</td>
<td>21/05/2021 3:08:22 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>21/05/2021 3:08:22 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
</tr>
<tr>
<td>22/05/2021 11:59:59 PM</td>
<td>1</td>
<td>&quot;B&quot;</td>
<td>21/05/2021 3:08:22 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>21/05/2021 3:08:22 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
</tr>
<tr>
<td>23/05/2021 11:59:59 PM</td>
<td>1</td>
<td>&quot;B&quot;</td>
<td>21/05/2021 3:08:22 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>21/05/2021 3:08:22 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
</tr>
<tr>
<td>24/05/2021 11:59:59 PM</td>
<td>1</td>
<td>&quot;D&quot;</td>
<td>24/05/2021 8:05:09 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>24/05/2021 8:05:09 PM</td>
<td>31/12/9000</td>
</tr>
<tr>
<td>25/05/2021 11:59:59 PM</td>
<td>1</td>
<td>&quot;D&quot;</td>
<td>24/05/2021 8:05:09 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>24/05/2021 8:05:09 PM</td>
<td>31/12/9000</td>
</tr>
</tbody>
</table>
</div>
<p>SQLFiddle <a href=""http://sqlfiddle.com/#!4/c50f8/7"" rel=""nofollow noreferrer"">here</a></p>
<p><strong>PS:</strong> I had a look at another <a href=""https://stackoverflow.com/questions/27327374/sql-to-generate-periodic-snapshots-from-transactions-table"">thread in SO</a> which had almost the same title but wasn't much helpful.</p>
<p><strong>Update 1:</strong></p>
<p>I am now able to get a daily status for all reporting dates, yet the start &amp; end date calculations and carrying forward the values to the subsequent rows is still not happening (as I am yet to figure it out).</p>
<ul>
<li>Start date - MIN (effectivedate for a given status)</li>
<li>End date -   MAX(Enddate for a given status)</li>
</ul>
<p><strong>Update 2:</strong>
The start date and end date calculated must not be greater than the reporting date. Refer to the SQL output which showcases the current issue</p>
","<sql><oracle><data-warehouse>","2021-07-14 12:26:54","429","2","2","68386867","<p>I haven't done Oracle for a while but you're after two components:</p>
<ol>
<li>The current snapshot</li>
<li>A fixed historical snapshot</li>
</ol>
<p>This generates a snapshot for a given hardcoded date. I don't have Oracle to check how variables work so you'll have to do the date variable part yourself.</p>
<p>Note:</p>
<ul>
<li>I'm assuming that a Cust_ID can only have one state at a time</li>
<li>real world data is more complicated than this and there are always edge cases</li>
<li>If there is no current state for the Cust, there will be no row</li>
<li>Just noticed you have overlapping dates. That is a problem because a customer is in two states at the same time</li>
</ul>
<p>You could join to a calendar table to run this for all dates but that can be very performance intensive, and ongoing you usually just want to generate every day to add to an existing table.</p>
<p>Below is the code copied from a fiddle</p>
<h2 id=""setup-code"">Setup code</h2>
<pre><code>CREATE TABLE t
    (FactID int, Cust_ID int, Status varchar2(1), EffectiveDate DATE, EndDate DATE)
;

INSERT ALL 
    INTO t (FactID, Cust_ID, Status, EffectiveDate, EndDate)
         VALUES (1, 1, 'A', TIMESTAMP'2021-05-20 08:52:29.000', TIMESTAMP'2021-05-21 03:08:22.000')
    INTO t (FactID, Cust_ID, Status, EffectiveDate, EndDate)
         VALUES (2, 1, 'B', TIMESTAMP'2021-05-21 03:08:22.000', TIMESTAMP'2021-05-24 02:47:28.000')
    INTO t (FactID, Cust_ID, Status, EffectiveDate, EndDate)
         VALUES (3, 1, 'C', TIMESTAMP'2021-05-24 02:47:28.000', TIMESTAMP'2021-05-24 04:15:45.000')
    INTO t (FactID, Cust_ID, Status, EffectiveDate, EndDate)
         VALUES (4, 1, 'A', TIMESTAMP'2021-05-24 04:15:45.000', TIMESTAMP'2021-05-24 08:05:09.000')
    INTO t (FactID, Cust_ID, Status, EffectiveDate, EndDate)
         VALUES (5, 1, 'D', TIMESTAMP'2021-05-24 08:05:09.000', TIMESTAMP'9000-12-31 00:00:00.000')         

SELECT * FROM dual
;
</code></pre>
<h2 id=""query"">Query</h2>
<pre><code>SELECT
T.Cust_ID, DATE '2021-05-25' ReportDate, T.Status, T.EffectiveDate,T.EndDate,
H.A_SDATE, H.A_EDATE, H.B_SDATE, H.B_EDATE, H.C_SDATE, H.C_EDATE
FROM
(
    -- Todays snapshot
    SELECT Cust_ID,Status, EffectiveDate,EndDate
    FROM t 
    WHERE DATE '2021-05-25' BETWEEN EffectiveDate AND EndDate 
) T
LEFT OUTER JOIN
(
-- Static capture of all states
    SELECT Cust_ID, 
    MIN(CASE WHEN Status = 'A' THEN EffectiveDate ELSE NULL END) A_SDATE, 
    MAX(CASE WHEN Status = 'A' THEN LEAST(DATE '2021-07-10',EndDate) ELSE NULL END) A_EDATE,
    MIN(CASE WHEN Status = 'B' THEN EffectiveDate ELSE NULL END) B_SDATE, 
    MAX(CASE WHEN Status = 'B' THEN LEAST(DATE '2021-05-25',EndDate) ELSE NULL END) B_EDATE,
    MIN(CASE WHEN Status = 'C' THEN EffectiveDate ELSE NULL END) C_SDATE, 
    MAX(CASE WHEN Status = 'C' THEN LEAST(DATE '2021-05-25',EndDate) ELSE NULL END) C_EDATE

    FROM t 
    -- Exclude state changes after the process date
    WHERE EffectiveDate &lt; DATE '2021-05-25'
    GROUP BY Cust_ID
) H
ON T.Cust_ID = H.Cust_ID
</code></pre>
"
"68378084","How to generate point in time snapshot table from a transaction fact table?","<p>I have a transaction table that records the change in status (A,B,C,D) of a customer by closing the end date of the previous record and also opening a new record with the current system time and the end date for the new record will be set to a high open date.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>FactID</th>
<th>Cust_ID</th>
<th>Status</th>
<th>EffectiveDate</th>
<th>EndDate</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>A</td>
<td>20/05/2021 8:52:29 PM</td>
<td>21/05/2021 3:08:22 PM</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>B</td>
<td>21/05/2021 3:08:22 PM</td>
<td>24/05/2021 2:47:28 PM</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>C</td>
<td>24/05/2021 2:47:28 PM</td>
<td>24/05/2021 4:15:45 PM</td>
</tr>
<tr>
<td>4</td>
<td>1</td>
<td>A</td>
<td>24/05/2021 4:15:45 PM</td>
<td>24/05/2021 8:05:09 PM</td>
</tr>
<tr>
<td>5</td>
<td>1</td>
<td>D</td>
<td>24/05/2021 8:05:09 PM</td>
<td>31/12/9000</td>
</tr>
</tbody>
</table>
</div>
<p>I'm trying to build a snapshot at a point in time table (end of the day reporting) based on the above transaction table.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ReportDate</th>
<th>Cust_ID</th>
<th>EODStatus</th>
<th>A_SDate</th>
<th>A_EDate</th>
<th>B_SDate</th>
<th>B_EDate</th>
<th>C_SDate</th>
<th>C_EDate</th>
<th>D_SDate</th>
<th>D_EDate</th>
</tr>
</thead>
<tbody>
<tr>
<td>20/05/2021 11:59:59 PM</td>
<td>1</td>
<td>A</td>
<td>20/05/2021 8:52:29 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
</tr>
<tr>
<td>21/05/2021 11:59:59 PM</td>
<td>1</td>
<td>B</td>
<td>20/05/2021 8:52:29 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
</tr>
<tr>
<td>22/05/2021 11:59:59 PM</td>
<td>1</td>
<td>B</td>
<td>20/05/2021 8:52:29 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
</tr>
<tr>
<td>23/05/2021 11:59:59 PM</td>
<td>1</td>
<td>B</td>
<td>20/05/2021 8:52:29 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
</tr>
<tr>
<td>24/05/2021  11:59:59 PM</td>
<td>1</td>
<td>D</td>
<td>20/05/2021 8:52:29 PM</td>
<td>24/05/2021 8:05:09 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>24/05/2021 4:15:45 PM</td>
<td>24/05/2021 8:05:09 PM</td>
<td>31/12/9000</td>
</tr>
<tr>
<td>25/05/2021  11:59:59 PM</td>
<td>1</td>
<td>D</td>
<td>20/05/2021 8:52:29 PM</td>
<td>24/05/2021 8:05:09 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>24/05/2021 4:15:45 PM</td>
<td>24/05/2021 8:05:09 PM</td>
<td>31/12/9000</td>
</tr>
</tbody>
</table>
</div>
<p>I'm now stuck when I try to expand the transaction table before building the snapshot. Any pointers would be greatly appreciated.</p>
<pre><code>WITH
    date_ranges
    AS
        (SELECT ROWNUM, TO_DATE ('21-05-2021', 'dd-mm-yyyy') + ROWNUM - 1.00001 reportdate
           FROM all_objects
          WHERE ROWNUM &lt;= 6),
    transactions (factid, cust_id, status, effectivedate, enddate)
    AS
        (SELECT 1, 1, 'A', TO_DATE ('20/05/2021 8:52:29 PM', 'DD/MM/YYYY HH12:MI:SS AM'), TO_DATE ('21/05/2021 3:08:22 PM', 'DD/MM/YYYY HH12:MI:SS AM') FROM DUAL
         UNION ALL
         SELECT 2, 1, 'B', TO_DATE ('21/05/2021 3:08:22 PM', 'DD/MM/YYYY HH12:MI:SS AM'), TO_DATE ('24/05/2021 2:47:28 PM', 'DD/MM/YYYY HH12:MI:SS AM') FROM DUAL
         UNION ALL
         SELECT 3, 1, 'C', TO_DATE ('24/05/2021 2:47:28 PM', 'DD/MM/YYYY HH12:MI:SS AM'), TO_DATE ('24/05/2021 4:15:45 PM', 'DD/MM/YYYY HH12:MI:SS AM') FROM DUAL
         UNION ALL
         SELECT 4, 1, 'A', TO_DATE ('24/05/2021 4:15:45 PM', 'DD/MM/YYYY HH12:MI:SS AM'), TO_DATE ('24/05/2021 8:05:09 PM', 'DD/MM/YYYY HH12:MI:SS AM') FROM DUAL
         UNION ALL
         SELECT 5, 1, 'D', TO_DATE ('24/05/2021 8:05:09 PM', 'DD/MM/YYYY HH12:MI:SS AM'), TO_DATE ('31/12/9000', 'DD/MM/YYYY') FROM DUAL),
    dataset
    AS
        (SELECT DISTINCT reportdate,
                         cust_id,
                         status     AS eodstatus,
                         effectivedate,
                         enddate
           FROM transactions CROSS JOIN date_ranges)
  SELECT reportdate,
         cust_id,
         eodstatus,
         effectivedate,
         enddate,
         CASE
             WHEN eodstatus = 'A' THEN MIN (effectivedate)
             ELSE TO_DATE ('31/12/9000', 'DD/MM/YYYY')
         END             AS a_sdate,
         CASE WHEN eodstatus = 'A' THEN MAX (enddate) ELSE TO_DATE ('31/12/9000', 'DD/MM/YYYY') 
         END             AS a_edate,
         CASE
             WHEN eodstatus = 'B' THEN MIN (effectivedate)
             ELSE TO_DATE ('31/12/9000', 'DD/MM/YYYY')
         END             AS b_sdate,
         CASE WHEN eodstatus = 'B' THEN MAX (enddate) ELSE TO_DATE ('31/12/9000', 'DD/MM/YYYY') 
         END             AS b_edate,
         CASE
             WHEN eodstatus = 'C' THEN MIN (effectivedate)
             ELSE TO_DATE ('31/12/9000', 'DD/MM/YYYY')
         END             AS c_sdate,
         CASE WHEN eodstatus = 'C' THEN MAX (enddate) ELSE TO_DATE ('31/12/9000', 'DD/MM/YYYY') 
         END             AS c_edate,
         CASE
             WHEN eodstatus = 'D' THEN MIN (effectivedate)
             ELSE TO_DATE ('31/12/9000', 'DD/MM/YYYY')
         END             AS d_sdate,
         CASE WHEN eodstatus = 'D' THEN MAX (enddate) ELSE TO_DATE ('31/12/9000', 'DD/MM/YYYY') 
          END             AS d_edate
    FROM dataset t
   WHERE reportdate BETWEEN effectivedate AND enddate
GROUP BY reportdate, cust_id, eodstatus, effectivedate, enddate
ORDER BY reportdate, cust_id, eodstatus;
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>REPORTDATE</th>
<th>CUST_ID</th>
<th>EODSTATUS</th>
<th>EFFECTIVEDATE</th>
<th>ENDDATE</th>
<th>A_SDATE</th>
<th>A_EDATE</th>
<th>B_SDATE</th>
<th>B_EDATE</th>
<th>C_SDATE</th>
<th>C_EDATE</th>
<th>D_SDATE</th>
<th>D_EDATE</th>
</tr>
</thead>
<tbody>
<tr>
<td>20/05/2021 11:59:59 PM</td>
<td>1</td>
<td>&quot;A&quot;</td>
<td>20/05/2021 8:52:29 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>20/05/2021 8:52:29 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
</tr>
<tr>
<td>21/05/2021 11:59:59 PM</td>
<td>1</td>
<td>&quot;B&quot;</td>
<td>21/05/2021 3:08:22 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>21/05/2021 3:08:22 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
</tr>
<tr>
<td>22/05/2021 11:59:59 PM</td>
<td>1</td>
<td>&quot;B&quot;</td>
<td>21/05/2021 3:08:22 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>21/05/2021 3:08:22 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
</tr>
<tr>
<td>23/05/2021 11:59:59 PM</td>
<td>1</td>
<td>&quot;B&quot;</td>
<td>21/05/2021 3:08:22 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>21/05/2021 3:08:22 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
</tr>
<tr>
<td>24/05/2021 11:59:59 PM</td>
<td>1</td>
<td>&quot;D&quot;</td>
<td>24/05/2021 8:05:09 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>24/05/2021 8:05:09 PM</td>
<td>31/12/9000</td>
</tr>
<tr>
<td>25/05/2021 11:59:59 PM</td>
<td>1</td>
<td>&quot;D&quot;</td>
<td>24/05/2021 8:05:09 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>24/05/2021 8:05:09 PM</td>
<td>31/12/9000</td>
</tr>
</tbody>
</table>
</div>
<p>SQLFiddle <a href=""http://sqlfiddle.com/#!4/c50f8/7"" rel=""nofollow noreferrer"">here</a></p>
<p><strong>PS:</strong> I had a look at another <a href=""https://stackoverflow.com/questions/27327374/sql-to-generate-periodic-snapshots-from-transactions-table"">thread in SO</a> which had almost the same title but wasn't much helpful.</p>
<p><strong>Update 1:</strong></p>
<p>I am now able to get a daily status for all reporting dates, yet the start &amp; end date calculations and carrying forward the values to the subsequent rows is still not happening (as I am yet to figure it out).</p>
<ul>
<li>Start date - MIN (effectivedate for a given status)</li>
<li>End date -   MAX(Enddate for a given status)</li>
</ul>
<p><strong>Update 2:</strong>
The start date and end date calculated must not be greater than the reporting date. Refer to the SQL output which showcases the current issue</p>
","<sql><oracle><data-warehouse>","2021-07-14 12:26:54","429","2","2","68449955","<p>First of all my sincere thanks to all the people who tried to help me. I somehow managed to nail this near-impossible task with some convoluted logic (but it works).  I tried providing inline comments to explain the derivations. A special mention to @Wernfried Domscheit who has written the PIVOT logic and deleted the answer and it helped me to a great extent.</p>
<pre><code>WITH
    date_ranges
-- Generate dates 
    AS
        (SELECT ROWNUM, TO_DATE ('21-05-2021', 'dd-mm-yyyy') + ROWNUM - 1.00001 reportdate
           FROM all_objects
          WHERE ROWNUM &lt;= 6),
-- Mock up source records
    transactions (factid, cust_id,status,effectivedate,enddate)
    AS
        (SELECT 1,1,'A',
                TO_DATE ('20/05/2021 8:52:29 PM', 'DD/MM/YYYY HH12:MI:SS AM'),
                TO_DATE ('21/05/2021 3:08:22 PM', 'DD/MM/YYYY HH12:MI:SS AM') FROM DUAL
         UNION ALL
         SELECT 2,1,'B',
                TO_DATE ('21/05/2021 3:08:22 PM', 'DD/MM/YYYY HH12:MI:SS AM'),
                TO_DATE ('24/05/2021 2:47:28 PM', 'DD/MM/YYYY HH12:MI:SS AM') FROM DUAL
         UNION ALL
         SELECT 3,1,'C',
                TO_DATE ('24/05/2021 2:47:28 PM', 'DD/MM/YYYY HH12:MI:SS AM'),
                TO_DATE ('24/05/2021 4:15:45 PM', 'DD/MM/YYYY HH12:MI:SS AM') FROM DUAL
         UNION ALL
         SELECT 4,1,'A',
                TO_DATE ('24/05/2021 4:15:45 PM', 'DD/MM/YYYY HH12:MI:SS AM'),
                TO_DATE ('24/05/2021 8:05:09 PM', 'DD/MM/YYYY HH12:MI:SS AM') FROM DUAL
         UNION ALL
         SELECT 5,1,'D',
                TO_DATE ('24/05/2021 8:05:09 PM', 'DD/MM/YYYY HH12:MI:SS AM'),
                TO_DATE ('31/12/9000', 'DD/MM/YYYY') FROM DUAL),
    dataset
-- Apply cross join to get report date into transactions
-- Could've been much better; time crunched
    AS
        (SELECT DISTINCT reportdate,cust_id,status     AS eodstatus,effectivedate,enddate
           FROM transactions CROSS JOIN date_ranges),
    dataset1
-- Ignore start and end dates if they are older than the reporting date
    AS
        (  SELECT reportdate,
                  cust_id,
                  eodstatus,
                  CASE
                      WHEN reportdate &gt; effectivedate THEN effectivedate
                      ELSE TO_DATE ('31/12/9000', 'DD/MM/YYYY')
                  END    AS effectivedate,
                  CASE
                      WHEN reportdate &gt; enddate THEN enddate
                      ELSE TO_DATE ('31/12/9000', 'DD/MM/YYYY')
                  END    AS enddate
             FROM dataset
            WHERE reportdate &gt; effectivedate),
    dataset2
-- Grab the min of start and max of end for all reporting days
    AS
        (  SELECT reportdate,
                  cust_id,
                  eodstatus,
                  eodstatus               AS status,
                  MIN (effectivedate)     effectivedate,
                  MAX (enddate)           enddate
             FROM dataset1
         GROUP BY reportdate, cust_id, eodstatus),
    dataset_new
-- Apply PIVOT to capture the start and end date per known statues and replacing NULLs with high open end dates
    AS
        (  SELECT reportdate,
                  cust_id,
                  eodstatus,
                  COALESCE ('A','B','C','D')                           AS status,
                  NVL (a_sdate, TO_DATE ('31/12/9000', 'DD/MM/YYYY'))    a_sdate,
                  NVL (a_edate, TO_DATE ('31/12/9000', 'DD/MM/YYYY'))    a_edate,
                  NVL (b_sdate, TO_DATE ('31/12/9000', 'DD/MM/YYYY'))    b_sdate,
                  NVL (b_edate, TO_DATE ('31/12/9000', 'DD/MM/YYYY'))    b_edate,
                  NVL (c_sdate, TO_DATE ('31/12/9000', 'DD/MM/YYYY'))    c_sdate,
                  NVL (c_edate, TO_DATE ('31/12/9000', 'DD/MM/YYYY'))    c_edate,
                  NVL (d_sdate, TO_DATE ('31/12/9000', 'DD/MM/YYYY'))    d_sdate,
                  NVL (d_edate, TO_DATE ('31/12/9000', 'DD/MM/YYYY'))    d_edate
             FROM dataset2
                  PIVOT (MIN (effectivedate) AS &quot;SDATE&quot;, MAX (enddate) AS &quot;EDATE&quot;
                        FOR status
                        IN ('A' AS &quot;A&quot;, 'B' AS &quot;B&quot;, 'C' AS &quot;C&quot;, 'D' AS &quot;D&quot;))
         ORDER BY reportdate),
    date_manipulations
-- Merging multiple entries into one record a day
    AS
        (  SELECT reportdate,
                  cust_id,
                  MIN (a_sdate)     a_sdate,
                  MIN (a_edate)     a_edate,
                  MIN (b_sdate)     b_sdate,
                  MIN (b_edate)     b_edate,
                  MIN (c_sdate)     c_sdate,
                  MIN (c_edate)     c_edate,
                  MIN (d_sdate)     d_sdate,
                  MIN (d_edate)     d_edate
             FROM dataset_new
         GROUP BY reportdate, cust_id
         ORDER BY 1)
-- JOIN with transaction to report the original status 
SELECT a.*, b.status
  FROM date_manipulations a JOIN transactions b ON reportdate BETWEEN effectivedate AND enddate;
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>REPORTDATE</th>
<th>CUST_ID</th>
<th>A_SDATE</th>
<th>A_EDATE</th>
<th>B_SDATE</th>
<th>B_EDATE</th>
<th>C_SDATE</th>
<th>C_EDATE</th>
<th>D_SDATE</th>
<th>D_EDATE</th>
<th>STATUS</th>
</tr>
</thead>
<tbody>
<tr>
<td>20/05/2021 11:59:59 PM</td>
<td>1</td>
<td>20/05/2021 8:52:29 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>&quot;A&quot;</td>
</tr>
<tr>
<td>21/05/2021 11:59:59 PM</td>
<td>1</td>
<td>20/05/2021 8:52:29 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>&quot;B&quot;</td>
</tr>
<tr>
<td>22/05/2021 11:59:59 PM</td>
<td>1</td>
<td>20/05/2021 8:52:29 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>&quot;B&quot;</td>
</tr>
<tr>
<td>23/05/2021 11:59:59 PM</td>
<td>1</td>
<td>20/05/2021 8:52:29 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>31/12/9000</td>
<td>&quot;B&quot;</td>
</tr>
<tr>
<td>24/05/2021 11:59:59 PM</td>
<td>1</td>
<td>20/05/2021 8:52:29 PM</td>
<td>24/05/2021 8:05:09 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>24/05/2021 4:15:45 PM</td>
<td>24/05/2021 8:05:09 PM</td>
<td>31/12/9000</td>
<td>&quot;D&quot;</td>
</tr>
<tr>
<td>25/05/2021 11:59:59 PM</td>
<td>1</td>
<td>20/05/2021 8:52:29 PM</td>
<td>24/05/2021 8:05:09 PM</td>
<td>21/05/2021 3:08:22 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>24/05/2021 2:47:28 PM</td>
<td>24/05/2021 4:15:45 PM</td>
<td>24/05/2021 8:05:09 PM</td>
<td>31/12/9000</td>
<td>&quot;D&quot;</td>
</tr>
</tbody>
</table>
</div>"
"68346272","how to combine two dataframes in two different lists having same names into a single list with dataframes using R","<p>I have a list with a set of dataframes called 'a' and the structure is as shown in the following picture</p>
<p><a href=""https://i.stack.imgur.com/Bfecc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Bfecc.png"" alt=""enter image description here"" /></a></p>
<p>I have another list with a set of dataframes called 'b' and having the same names as in 'a' and the structure is as shown in the following picture</p>
<p><a href=""https://i.stack.imgur.com/yzYOT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yzYOT.png"" alt=""enter image description here"" /></a></p>
<p>i would like to combine the two dataframes in two lists as per their names into one list with dataframes.</p>
<p>for this, I am using the following code</p>
<pre><code>c &lt;-  Map(cbind, a, b)
</code></pre>
<p>but, the code is not working as it is giving the following output</p>
<p><a href=""https://i.stack.imgur.com/UF5sn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UF5sn.png"" alt=""enter image description here"" /></a></p>
<p>the expected output will look very similar to as shown in the following picture</p>
<p><a href=""https://i.stack.imgur.com/LtDe7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LtDe7.png"" alt=""enter image description here"" /></a></p>
<p>for instance in a list of length 49, a dataframe with 241 rows and 14 columns</p>
<p>I don't know what is the issue with the code...... looking to solve this problem
I have tried all the options avalibale in stactoverflow to solve the problem</p>
","<r><dataframe><data-manipulation><data-management>","2021-07-12 11:05:23","67","-1","1","68694987","<p>the problem is with the structure of the two dataframes are different and the problem is solved by converting the structure of two lists into the same structure.</p>
<p>i have converted the structure of list 'b' into tibble as</p>
<pre><code>b &lt;- tibble::as_tibble(b)
</code></pre>
<p>then we can use the code as following</p>
<pre><code>c &lt;-  Map(cbind, a, b)
</code></pre>
<p>this has worked solved my problem</p>
"
"68336260","Linux tool for triggering data updates and transformations (low-end ETL/data warehousing tool)","<p>I have a bunch of scripts collecting data from internet and local services, writing them to disk, scripts transforming the data and writing it into a database, scripts reading data from the database and generating new data, etc, written in bash, Python, SQL, ... (Linux).</p>
<p>Apart from a few time-triggered scripts, the glue between the scripts is currently me, running the scripts now and then in a particular order to update everything.</p>
<p><em>What is the simplest way to replace me by a tool that observes dependencies and triggers the next step as soon as the preconditions are met?</em></p>
<p>I've found many ETL and data warehousing tools, but these seem too heavy weight for my simple setting. I'd prefer a CLI solution with text-based configuration (maybe able to visualise the graph of dependencies). Any suggestions?</p>
","<triggers><dependencies><etl><data-warehouse>","2021-07-11 12:32:04","34","-1","1","68336530","<p>Try airflow: <a href=""https://airflow.apache.org/docs/apache-airflow/stable/index.html"" rel=""nofollow noreferrer"">airflow.apache.org</a></p>
"
"68314192","Getting different cor() output on same data?","<p>I have a problem when calculating correlations.
I have two datasets:
d3: One with 1259 observations and 264 variables.
d4: One with 1196 observations and 185 variables - these are only blood tests.
Both data have same unique ID for participants, so that they can be merged.</p>
<p>When merging (called: d) I have 1150 observations (because them without bloodtests are out, and the bloodtest data had many control samples and so on. When remowing participants without information x (since this is inclusion criteria) we end up with 1144 observations.</p>
<p>In d4 I had two samples with missing values in all the variables. They are included in the 1144 participants.</p>
<p>So after sum datamanagement I want to calculate correlations between informations x (we call it Edu) and all the blood samples (184).</p>
<p>I make a new dataset by subsetting from d:</p>
<pre><code>dbio &lt;- d %&gt;%
  select(Edu, BMP_6:CCL16)
</code></pre>
<p>I use these codes:
First two functions:</p>
<pre><code>cor.prob &lt;- function (X, dfr = nrow(X) - 2) {
  R &lt;- cor(X, use=&quot;complete.obs&quot;, method = c(&quot;pearson&quot;))
  above &lt;- row(R) &lt; col(R)
  r2 &lt;- R[above]^2
  Fstat &lt;- r2 * dfr/(1 - r2)
  R[above] &lt;- 1 - pf(Fstat, 1, dfr)
  R[row(R) == col(R)] &lt;- NA
  R
}

flattenSquareMatrix &lt;- function(m) {
  if( (class(m) != &quot;matrix&quot;) | (nrow(m) != ncol(m))) stop(&quot;Must be a square matrix.&quot;) 
  if(!identical(rownames(m), colnames(m))) stop(&quot;Row and column names must be equal.&quot;)
  ut &lt;- upper.tri(m)
  data.frame(i = rownames(m)[row(m)[ut]],
             j = rownames(m)[col(m)[ut]],
             cor=t(m)[ut],
             p=m[ut])
}
</code></pre>
<p>Then I start making correlations:</p>
<pre><code>kor_masterlist_dbio &lt;- flattenSquareMatrix (cor.prob(dbio))

kor_masterlist_dbio_ordnet &lt;- kor_masterlist_dbio[order(-abs(kor_masterlist_dbio$cor)),]

kor_masterlist_dbio$threshold &lt;- as.factor(kor_masterlist_dbio$p &lt; 0.05)

kor_masterlist_dbio_Edu &lt;- subset(kor_masterlist_dbio_ordnet, i == &quot;Edu&quot; &amp; j != &quot;ID&quot;)

kor_masterlist_dbio_Edu$threshold &lt;- as.factor(kor_masterlist_dbio_Edu$p &lt; 0.05)


kor_masterlist_dbio_Edu[kor_masterlist_dbio_Edu$threshold==T,]

kor_masterlist_dbio_Edu
</code></pre>
<p>Now the question:
I use &quot;complete.obs&quot; in cor(). The two participants with all blood test missing(NA), if i take them out of the data by following code:</p>
<pre><code>d &lt;- d[rowSums(is.na(d[,3:6]))!=4,]
And end up with d: n = 1142.
</code></pre>
<p>I then get different results from the correlations. I end up with two less blood test as signifcant, as in p-value above 0.05. I dont understand the difference when the values are missing for those two participants.</p>
<p>When using &quot;pariwise&quot; ind cor() i get same results with 1142 or 1144 participants. But the last 4 blood tests are different from them I get with &quot;complete.obs&quot;</p>
<p>And the rest with slightly different correlationcoefficients and p-values. But stille same ranking.</p>
<p>I hope someone can help me. I should have same results wether the to participants are included or not.</p>
<p>I've tried to make following small reproducible example that shows my problem. You will get different results if you run it with/without:</p>
<pre><code>d &lt;- d[rowSums(is.na(d[,3:6]))!=4,]
</code></pre>
<pre class=""lang-r prettyprint-override""><code>ID &lt;- c(1,2,3,4,5,6,7,8,9,10)
Edu &lt;- c(4,7,9,13,15,18,11,10,12,8)
CLL &lt;- c(NA,0.33,0.45,NA,0.76,0.34,0.49,0.86,0.43,0.26)
BLL &lt;- c(NA,0.93,0.15,NA,0.86,0.14,0.29,0.36,0.93,0.76)
ALL &lt;- c(NA,0.53,0.65,NA,0.26,0.54,0.99,0.76,0.63,NA)
DLL &lt;- c(NA,0.13,0.95,NA,0.36,0.74,0.19,NA,0.83,0.56)

d &lt;- data.frame(ID, Edu, CLL, BLL, ALL, DLL)
d &lt;- d[rowSums(is.na(d[,3:6]))!=4,]

cor.prob &lt;- function (X, dfr = nrow(X) - 2) {
  R &lt;- cor(X, use=&quot;complete.obs&quot;, method = c(&quot;pearson&quot;))
  above &lt;- row(R) &lt; col(R)
  r2 &lt;- R[above]^2
  Fstat &lt;- r2 * dfr/(1 - r2)
  R[above] &lt;- 1 - pf(Fstat, 1, dfr)
  R[row(R) == col(R)] &lt;- NA
  R
}

flattenSquareMatrix &lt;- function(m) {
  if( (class(m) != &quot;matrix&quot;) | (nrow(m) != ncol(m))) stop(&quot;Must be a square matrix.&quot;) 
  if(!identical(rownames(m), colnames(m))) stop(&quot;Row and column names must be equal.&quot;)
  ut &lt;- upper.tri(m)
  data.frame(i = rownames(m)[row(m)[ut]],
             j = rownames(m)[col(m)[ut]],
             cor=t(m)[ut],
             p=m[ut])
}

kor_masterlist_d &lt;- flattenSquareMatrix (cor.prob(d))
kor_masterlist_d_order &lt;- kor_masterlist_d[order(-abs(kor_masterlist_d$cor)),]
kor_masterlist_d_Edu &lt;- subset(kor_masterlist_d_order, i == &quot;Edu&quot; &amp; j != &quot;ID&quot;)
kor_masterlist_d_Edu
#&gt;      i   j        cor         p
#&gt; 8  Edu ALL -0.3319602 0.4217894
#&gt; 3  Edu CLL  0.2646661 0.5264383
#&gt; 12 Edu DLL  0.2609108 0.5325405
#&gt; 5  Edu BLL -0.2492912 0.5515835
</code></pre>
<p><sup>Created on 2021-07-09 by the <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex package</a> (v2.0.0)</sup></p>
<pre class=""lang-r prettyprint-override""><code>ID &lt;- c(1,2,3,4,5,6,7,8,9,10)
Edu &lt;- c(4,7,9,13,15,18,11,10,12,8)
CLL &lt;- c(NA,0.33,0.45,NA,0.76,0.34,0.49,0.86,0.43,0.26)
BLL &lt;- c(NA,0.93,0.15,NA,0.86,0.14,0.29,0.36,0.93,0.76)
ALL &lt;- c(NA,0.53,0.65,NA,0.26,0.54,0.99,0.76,0.63,NA)
DLL &lt;- c(NA,0.13,0.95,NA,0.36,0.74,0.19,NA,0.83,0.56)

d &lt;- data.frame(ID, Edu, CLL, BLL, ALL, DLL)

cor.prob &lt;- function (X, dfr = nrow(X) - 2) {
  R &lt;- cor(X, use=&quot;complete.obs&quot;, method = c(&quot;pearson&quot;))
  above &lt;- row(R) &lt; col(R)
  r2 &lt;- R[above]^2
  Fstat &lt;- r2 * dfr/(1 - r2)
  R[above] &lt;- 1 - pf(Fstat, 1, dfr)
  R[row(R) == col(R)] &lt;- NA
  R
}

flattenSquareMatrix &lt;- function(m) {
  if( (class(m) != &quot;matrix&quot;) | (nrow(m) != ncol(m))) stop(&quot;Must be a square matrix.&quot;) 
  if(!identical(rownames(m), colnames(m))) stop(&quot;Row and column names must be equal.&quot;)
  ut &lt;- upper.tri(m)
  data.frame(i = rownames(m)[row(m)[ut]],
             j = rownames(m)[col(m)[ut]],
             cor=t(m)[ut],
             p=m[ut])
}

kor_masterlist_d &lt;- flattenSquareMatrix (cor.prob(d))
kor_masterlist_d_order &lt;- kor_masterlist_d[order(-abs(kor_masterlist_d$cor)),]
kor_masterlist_d_Edu &lt;- subset(kor_masterlist_d_order, i == &quot;Edu&quot; &amp; j != &quot;ID&quot;)
kor_masterlist_d_Edu
#&gt;      i   j        cor         p
#&gt; 8  Edu ALL -0.3319602 0.3487063
#&gt; 3  Edu CLL  0.2646661 0.4599218
#&gt; 12 Edu DLL  0.2609108 0.4665494
#&gt; 5  Edu BLL -0.2492912 0.4873203
</code></pre>
<p><sup>Created on 2021-07-09 by the <a href=""https://reprex.tidyverse.org"" rel=""nofollow noreferrer"">reprex package</a> (v2.0.0)</sup></p>
","<r><correlation><data-management>","2021-07-09 09:08:16","302","1","1","68326244","<p>The difference is introduced with</p>
<pre><code>  Fstat &lt;- r2 * dfr/(1 - r2)
</code></pre>
<p>due to different values of <code>dfr</code> (8 and 6).</p>
<pre><code>&gt; r2
 [1] 0.234375000 0.011456074 0.070048129 0.002401998 0.062146106 0.062751663
 [7] 0.096834764 0.110197604 0.165400138 0.216547595 0.057255529 0.068074453
[13] 0.030140179 0.136955494 0.005027238
&gt; r2*8/(1-r2)
 [1] 2.44897959 0.09271069 0.60259574 0.01926226 0.53011332 0.53562464
 [7] 0.85773686 0.99076023 1.58543173 2.21121379 0.48586255 0.58437675
[13] 0.24861473 1.26951037 0.04042111
&gt; r2*6/(1-r2)
 [1] 1.83673469 0.06953302 0.45194680 0.01444669 0.39758499 0.40171848
 [7] 0.64330264 0.74307017 1.18907380 1.65841034 0.36439691 0.43828256
[13] 0.18646105 0.95213278 0.03031583
</code></pre>
<p>Neither of 8 or 6 is correct, since it's based on <code>nrow(X)</code>, while for <code>use=&quot;complete.obs&quot;</code> it has to be based on the number of complete observations. This can be accomplished by changing the function definition to <code>cor.prob &lt;- function (X, dfr = sum(complete.cases(X)) - 2) { …</code>. Therewith, the same results are produced with and without the prior<br />
<code>d &lt;- d[rowSums(is.na(d[,3:6]))!=4,]</code></p>
<blockquote>
<p>But if I choose to use <code>pairwise.complete.obs</code> instead of <code>complete.obs</code>, then i'll keep the code as it was? And further, since i have <code>NA</code> values different places in my data then i will benefite from <code>&quot;pairwise&quot;</code> rather than <code>&quot;complete.obs&quot;</code>?</p>
</blockquote>
<p>Indeed if we use <code>pairwise.complete.obs</code>, we gain the observations where only part of the columns are <code>NA</code>. But, since we then have different numbers of observations for the individual columns, a single <code>dfr</code> value is not appropriate; instead, we can use a <code>dfr</code> matrix:</p>
<pre><code>library(psych)
cor.prob &lt;- function (X, dfr = pairwiseCount(X) - 2)
{
  …
  Fstat &lt;- r2 * dfr[above]/(1 - r2)
  R[above] &lt;- 1 - pf(Fstat, 1, dfr[above])
  …
</code></pre>
"
"68306060","Actively create Insights Table from Raw table in Bigquery","<p>I have a raw Table in Bigquery that ingest streaming data from on-prem table. The data can be of 3 types Insert, update or delete so to overcome preforming DML operation I did create the schema with appending creation_timestamp and operation(I,U,D) and insert into Bigquery table in streaming fashion.</p>
<p>Now, I wanted an insight table on top of this raw table that I can get basically using below query so that I can take extracts if required due to which creating views wont help.</p>
<pre><code> select * except(rn, creation_timestamp, operation ) from(select *, row_number() over(partition by &lt;primary_key&gt; order by creation_timestamp desc, operation desc ) rn
    from  &lt;Table_name&gt; )
    where rn =1 and operation!='D'
</code></pre>
<p>Please suggest a way so that I can get data flowing into Insights table with only active records from Raw table.</p>
","<sql><data-structures><google-bigquery><data-warehouse>","2021-07-08 17:14:12","66","0","1","68330678","<p>Got this problem solved using stored procedures.</p>
<p>I first create a stored procedure with below query</p>
<p>And scheduled it to run using cloud function and cloud schedule for every five minutes.</p>
<p>Note: took 10 min window data from raw table in every 5 min considering creation_timestammp delay than actual data insertion timestamp. Since, creation_timestmap is created another cloud function that is used to store data into raw table.</p>
<pre><code>    CREATE OR REPLACE PROCEDURE
  &lt;stored_procedure&gt;()
BEGIN
INSERT INTO
  &lt;insights_table&gt;
SELECT
  *
FROM
  &lt;raw_table&gt;
WHERE
  creation_timestamp&gt;current_timesamp-10mins;
DELETE
  &lt;insights_table&gt; outside
WHERE
  NOT EXISTS(
  SELECT
    *
  FROM (
    SELECT
      * EXCEPT(rn )
    FROM (
      SELECT
        *,
        ROW_NUMBER() OVER(PARTITION BY &lt;primary_key&gt; ORDER BY creation_timestamp DESC, operation DESC ) rn
      FROM
        &lt;insights_table&gt; )
    WHERE
      rn =1
      AND operation!='D') inside
  WHERE
    outside.&lt;primary_key&gt;=inside.&lt;primary_key&gt;
    AND outside.creation_timestamp=inside.creation_timestamp
    AND outside.operation=inside.operation );
END
  ;
</code></pre>
<p>Please, suggest any better way or improvements with the current approach if there are any.</p>
"
"68287830","Python : Changing the original data using a for loop","<p>I have some really big txt files (&gt; 2 gb) where the quality of the data is not good.
In some columns (that should be integer), for values below 1000.00 , '.' is used as the decimal point (e.g. 473.71886) but for values above 1000.00 then the form is like that 7.541,72419. So ',' is used as the decimal point and '.' for the thousands separator.</p>
<p>I have already read the text file using pd.read_csv with the below command</p>
<pre><code>df = pd.read_csv('mseg.txt',delimiter=(&quot;#|#&quot;),nrows=(1000),engine = 'python')
</code></pre>
<p>I tried to build the regular expression to be used but it doesn't work
<code>pattern = &quot;[0-9]+[\.][0-9]+[,][0-9]+&quot;</code></p>
<p>I was thinking of using the below code to correct the above problem but it doesn't work. (in the below code I used as <code>pattern2 = &quot;,&quot;</code> to test the code)</p>
<pre><code>for i in df.iloc[:,-5]:
    df3 = []
    if re.search(pattern2,i):
        k= i.replace(&quot;.&quot;,&quot;&quot;)
        print(k)
        df3.append(k)
    else:
        df3.append(k)
return dfe3
</code></pre>
<p>The <code>print(k)</code> in the loop seems to work fine but when I run df3 then I get the below output</p>
<pre><code>['\x00 \x003\x004\x00\x006\x006\x005\x00,\x002\x001\x007\x006\x000\x00']
</code></pre>
<p>Could anyone help?</p>
","<python><pandas><for-loop><data-quality>","2021-07-07 14:20:53","71","0","2","68288365","<p>You can try this:</p>
<pre><code>&gt;&gt;&gt; df
             0
0    473.71886
1  7.541,72419
</code></pre>
<pre><code>&gt;&gt;&gt; df[0].str.split(r'[^\d]') \
         .apply(lambda x: f&quot;{''.join(x[:-1])}.{x[-1]}&quot;)

0      473.75410
1    71886.72419
dtype: float64
</code></pre>
"
"68287830","Python : Changing the original data using a for loop","<p>I have some really big txt files (&gt; 2 gb) where the quality of the data is not good.
In some columns (that should be integer), for values below 1000.00 , '.' is used as the decimal point (e.g. 473.71886) but for values above 1000.00 then the form is like that 7.541,72419. So ',' is used as the decimal point and '.' for the thousands separator.</p>
<p>I have already read the text file using pd.read_csv with the below command</p>
<pre><code>df = pd.read_csv('mseg.txt',delimiter=(&quot;#|#&quot;),nrows=(1000),engine = 'python')
</code></pre>
<p>I tried to build the regular expression to be used but it doesn't work
<code>pattern = &quot;[0-9]+[\.][0-9]+[,][0-9]+&quot;</code></p>
<p>I was thinking of using the below code to correct the above problem but it doesn't work. (in the below code I used as <code>pattern2 = &quot;,&quot;</code> to test the code)</p>
<pre><code>for i in df.iloc[:,-5]:
    df3 = []
    if re.search(pattern2,i):
        k= i.replace(&quot;.&quot;,&quot;&quot;)
        print(k)
        df3.append(k)
    else:
        df3.append(k)
return dfe3
</code></pre>
<p>The <code>print(k)</code> in the loop seems to work fine but when I run df3 then I get the below output</p>
<pre><code>['\x00 \x003\x004\x00\x006\x006\x005\x00,\x002\x001\x007\x006\x000\x00']
</code></pre>
<p>Could anyone help?</p>
","<python><pandas><for-loop><data-quality>","2021-07-07 14:20:53","71","0","2","68288405","<p>I would suggest to do the following:</p>
<p>If there is a ',' in the number replace it with a '.' but get rid of the ',' before.
So you would change a 1.234,567 to 1234,567 and then to 1234.567.
Then all of your numbers should be in the same format.</p>
<pre><code>df3 = []
for index,i in df.iloc[:,-5]:  
    if ',' in i:
        i= i.replace(&quot;.&quot;,&quot;&quot;).replace(',','.')
    df3[index] = i
</code></pre>
"
"68284822","Dependent scheduled SQL queries in BigQuery","<p>We have a data pipeline with ELT in BigQuery. We have several transformations. Some of those transformations depend on other transformations happening before.</p>
<p>With BigQuery scheduled queries we can only set a time, so either a lot of time the system is idle if we have large buffers, or when dependent scheduled queries are too near to each other, they overlap. How would one model a transformation pipeline in BigQuery with dependencies?</p>
<p>[Edit] I know about external tools like AirFlow but would like to use only Google services.</p>
","<google-bigquery><data-warehouse>","2021-07-07 11:03:44","284","-1","1","68289991","<p>We can use workflow orchestrator solutions like Composer Airflow(Costly) or Cloud serverless workflows to manage the dependencies and the time of execution.</p>
"
"68229033","Redshift Query Scheduler is State: Enabled but the queries are not running. IAM Permission problem?","<p>I am attempting to use the Redshift Scheduler to run an UNLOAD to S3. The database is connected properly and when I connect and run the query from the Console it runs. I have enabled a scheduled query to run every hour, but it is not executing.</p>
<p>Am I missing something? I have set up the IAM role with the AmazonRedshiftFullAccess and Secret Manager access. The Trust Policy has scheduler.redshift.amazonaws.com and redshift.amazonaws.com as trusted entity providers and can assume role as my user.</p>
<p>I'm not getting any errors, it just is not running. State is enabled, but the S3 bucket is not updating and there are no query records in the history.</p>
","<amazon-redshift><amazon-iam><data-warehouse>","2021-07-02 17:12:06","792","1","2","68230269","<p>I needed to add the EventBridge permissions - events.amazonaws.com as a trusted provider and AmazonEventBridgeFullAccess policy.</p>
"
"68229033","Redshift Query Scheduler is State: Enabled but the queries are not running. IAM Permission problem?","<p>I am attempting to use the Redshift Scheduler to run an UNLOAD to S3. The database is connected properly and when I connect and run the query from the Console it runs. I have enabled a scheduled query to run every hour, but it is not executing.</p>
<p>Am I missing something? I have set up the IAM role with the AmazonRedshiftFullAccess and Secret Manager access. The Trust Policy has scheduler.redshift.amazonaws.com and redshift.amazonaws.com as trusted entity providers and can assume role as my user.</p>
<p>I'm not getting any errors, it just is not running. State is enabled, but the S3 bucket is not updating and there are no query records in the history.</p>
","<amazon-redshift><amazon-iam><data-warehouse>","2021-07-02 17:12:06","792","1","2","73409025","<p>Using CloudFormation I had to set the <code>StatementName</code> in <code>RedshiftDataParameters</code> to the same <code>Name</code> as the <code>Rule</code></p>
<pre><code>  MyRule:
    Type: AWS::Events::Rule
    Properties:
      ScheduleExpression: &quot;cron(0/1 * ? * MON,TUE,WED,THU,FRI,SAT,SUN *)&quot;
      Name: &quot;my-rule&quot;
      Targets:
        - Arn: !Sub &quot;arn:aws:redshift:${AWS::Region}:${AWS::AccountId}:cluster:${RedshiftClusterName}&quot;
          Id: &quot;my-target-id&quot;
          RoleArn: !GetAtt RoleFromUpTop.Arn
          RedshiftDataParameters:
            Database: &quot;my_db&quot;
            DbUser: &quot;my_user&quot;
            Sql: &quot;SELECT * FROM some_table;&quot;
            StatementName: &quot;my-rule&quot;
</code></pre>
"
"68219157","How does sortkey in Redshift work internally?","<p>I'm a beginner to Redshift and Data Warehouses in general.</p>
<p>When a numeric or timestamp column is specified as sortkey, does the Redshift DBMS use binary search during a query to find the desired row as efficiently as possible?</p>
<p>I feel that knowing more about this would improve my table design skill.</p>
","<database><amazon-web-services><amazon-redshift><data-warehouse>","2021-07-02 03:09:58","203","2","1","68219544","<p>Amazon Redshift is a <strong>columnar datastore</strong>, which means that each column is stored separately. This is great for wide tables because Redshift only needs to read in the columns that are specifically used in the query. The most time-consuming part of database queries is disk access, so anything that reduces/avoids disk access is a good thing.</p>
<p>When data is stored on disk, it is stored in 1MB disk blocks. Each column can consume multiple blocks, and each block only contains data relating to one column. Redshift keeps a <strong>Zone Map</strong> of each block, which stores the minimum and maximum values stored in the block. For example, if a query is searching for data from 2021 and Redshift knows that the timestamp column for a particular block has a maximum value in 2018, <strong>it does not need to read the block from disk</strong> to examine the contents. This greatly reduces query time.</p>
<p>Data is stored in the blocks based upon the selected <a href=""https://docs.aws.amazon.com/redshift/latest/dg/c_Compression_encodings.html"" rel=""nofollow noreferrer"">Compression Encoding</a>. These are very clever techniques for reducing the storage space for data. For example, if a column contains a list of Countries and the rows are sorted in alphabetical order by country, then Redshift could simply store the fact that the block contains <code>Jamaica x 63, then Japan x 104, then Jordan x 26</code>. This might only require 24 bytes to store 193 rows of data, and don't forget that each block is 1MB in size. Thus, <strong>compression reduces the amount of disk access</strong> required to retrieve data, again making queries faster.</p>
<p>To answer your question about how Redshift would find the desired rows:</p>
<ul>
<li>If the SORTKEY is used in a <code>WHERE</code> statement, then Redshift can quickly find the relevant blocks that potentially contain the desired data. I'm not sure if it does that with a binary search.</li>
<li>If <code>WHERE</code> statement does <em>not</em> use the SORTKEY, then finding the right rows is not as efficient because many blocks on disk might contain the rows that match the <code>WHERE</code> statement in various columns since they are not sorted together. This makes the query less efficient.</li>
<li>Redshift can still 'skip-over' blocks that do not contain matching data in the Zone Maps for all columns, avoiding the need to read those blocks from disk. Plus, compression on various columns can reduce the number of blocks that need to be read from disk.</li>
</ul>
<p>The <strong>general rules for Amazon Redshift</strong> are:</p>
<ul>
<li>Set the <code>DISTKEY</code> to the column that is most frequently used in <code>JOIN</code></li>
<li>Set the <code>SORTKEY</code> to the column that is most frequently used in <code>WHERE</code></li>
</ul>
<p>See: <a href=""https://docs.aws.amazon.com/redshift/latest/dg/c-optimizing-query-performance.html"" rel=""nofollow noreferrer"">Tuning query performance - Amazon Redshift</a></p>
"
"68211019","How to keep a copy of the deleted rows as archive before deleting?","<p>I have designed a table which will be used for log purpose. This is the table structure and I will use MySQL (InnoDB engine) and it will be hosted in AWS RDS.</p>
<pre><code>| id (primary_key auto_increment) | entity_id (foreign key) | previous_value | current_value | calculation_done_with_this_row (boolean) | created_time |
</code></pre>
<p>This table will be also used for some critical business calculations and <strong>as soon as the calculation is done for a certain day the log table's data is no more needed.</strong> So as of now, I planned to  <strong>delete</strong> the data from the log table as soon as the calculation is done for such rows.</p>
<p>But my concern is that if something goes wrong and I need to look into the historical data what will I do then? As I am deleting the records I won't have anything to back me up for such situations? On the other hand, if I won't delete the data from the log table it will be full of unnecessary data that won't contribute to the business calculations; thus slowing down the performance of the calculation query.</p>
<p>My requirements is to keep the log table size small and also keep the unnecessary logs for future reference as historical data and also import the historical data back to the main database if needed.</p>
","<mysql><logging><data-warehouse><database-administration>","2021-07-01 13:21:22","201","0","1","68218139","<p>This is one of the few cases where <code>PARTITIONing</code> is useful.</p>
<p>Change the table to be <code>PARTITION BY RANGE(TO_DAYS(...))</code>.  Add a new partition each night; drop really old partitions when you feel there is no chance to need them.</p>
<p>More details:  <a href=""http://mysql.rjweb.org/doc.php/partitionmaint"" rel=""nofollow noreferrer"">http://mysql.rjweb.org/doc.php/partitionmaint</a> .  Also you might be interested in summarization techniques: <a href=""http://mysql.rjweb.org/doc.php/summarytables"" rel=""nofollow noreferrer"">http://mysql.rjweb.org/doc.php/summarytables</a></p>
"
"68169507","SSIS: Multiple Sources to One Destination table","<p>I am new SSIS &amp; ETL. And trying to extract &amp; load data into single destination table in sql server.
I have 4 sources - text file, csv file, excel file and some data in sql server. Please find the pictures attached that I have done so far. In one package, I have created 2 data flows, not connected (highlighted in red color boxes): one for .txt &amp; .csv files and another for .xls &amp; data in sql server.</p>
<p>Data is getting inserted but not in correct way. Here are the screenshots attached:</p>
<p><a href=""https://i.stack.imgur.com/q7oiX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q7oiX.png"" alt=""enter image description here"" /></a></p>
<p>And the screenshot for output in destination table is shown below: Customer_ID is the auto increment.</p>
<p><a href=""https://i.stack.imgur.com/dnwtS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dnwtS.png"" alt=""enter image description here"" /></a></p>
<p>Can anyone please let me know what am I missing and how to do it in correct way.</p>
<p>Thanks in advance
Raj</p>
","<sql-server><ssis><etl><data-warehouse>","2021-06-28 20:13:32","322","0","1","68170576","<p>With the example, it seems that the merges join are not working.
Are you sure that the joining columns are ok?</p>
<p>If not, you can try to add derived columns to format the data for each source. SSIS is case sensitive and if you work with varchar columns be careful of: start and end Space, unicode and non-unicode columns, accent, ...</p>
"
"68146559","Is AdventureworksDW's FactInternetSales an accumulating snapshot table?","<p>I always wonder if the FactInternetSale table of the AdventureworksDW is a accumulating snapshot table. It has a ShipDateKey in it.</p>
<p><a href=""https://i.stack.imgur.com/jf8cy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jf8cy.png"" alt=""enter image description here"" /></a></p>
<p>According to the AdventureWorks OLTP documentation, it says that the ShipDate of the SalesOrderHeader is the date that &quot;the order was shipped to customer&quot;. I interpret this line as, when the order is shipped, the ship date will be updated.</p>
<p><a href=""https://i.stack.imgur.com/WFq9a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WFq9a.png"" alt=""enter image description here"" /></a></p>
<p>That also means the rows in the DW FactInternetSale will also need to be updated as well. The ship date marks the an important milestone of an order and this is clearly the behavior of an accumulating snapshot fact table.</p>
<p>So should this table be considered an accumulating snapshot fact table? If so then is there any problem that there is no real transaction fact table?</p>
<p>In the Kimball's data warehouse toolkit book, in this kind of problem, he separates the Order transaction fact table and the Shipping Fact table very strictly, with the Order Transaction Fact table only contains only the information which is recorded when the order is made, and will not be updated. The dates in the Order Transaction Fact table are always expected date, not the real date. The shipping fact table contains the true ship date of an item. After that there is an accumulating snapshot fact table that contains all the important milestones of an order. Not only the ship date, but also other important milestones... By having dates of important milestones, we of course can know the current status of the order.</p>
<p>In my personal opinion, I consider that the Order Fact Table that does not contain the current status of it is totally useless. What is the point of knowing the total amount of orders but cannot know how much is from fulfilled (shipped) ones and how much is from unfulfilled ones? In my experience, users (data analysts) will always just use the accumulating snapshot table to do their job all the time, as the search predicate of &quot;current status&quot; is never absent in their query.</p>
<p>In my real world, I usually design this Order (information) fact table as a accumulating snapshot straightforwardly, skipping the transaction fact table (like what Kimball does, strictly separates things), as I feel that is very time-consuming and have no use. The transaction fact tables are usually just the actions done on the order (for example: shipping).</p>
<p>How do you think about this?</p>
","<data-warehouse><dimensional-modeling><star-schema><kimball>","2021-06-26 22:04:11","244","1","1","68152169","<p>No, it's not an accumulating snapshot fact table</p>
"
"68120534","creating donuts chart in oracle OBIEE","<p><strong>hello all</strong></p>
<p>I'm trying to create a donut chart in <strong>OBIEE</strong> from oracle , the version I'm using is : 12.2.1.4.0 .</p>
<p>please note that our OBIEE is located on our organization network and there is no internet access.
so i can't use any solutions that need internet like Google API.</p>
<p>donuts chart sample:</p>
<p><a href=""https://i.stack.imgur.com/sA9pz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sA9pz.png"" alt=""enter image description here"" /></a></p>
<p>only pie chart is available:</p>
<p><a href=""https://i.stack.imgur.com/H9CoY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H9CoY.png"" alt=""enter image description here"" /></a></p>
<p>Many thanks all ..</p>
","<oracle><data-visualization><data-warehouse><business-intelligence><obiee>","2021-06-24 17:48:16","365","0","2","68121728","<p>Data Visualization has the donut chart type out of the box. It's available in 12.2.1.4.</p>
"
"68120534","creating donuts chart in oracle OBIEE","<p><strong>hello all</strong></p>
<p>I'm trying to create a donut chart in <strong>OBIEE</strong> from oracle , the version I'm using is : 12.2.1.4.0 .</p>
<p>please note that our OBIEE is located on our organization network and there is no internet access.
so i can't use any solutions that need internet like Google API.</p>
<p>donuts chart sample:</p>
<p><a href=""https://i.stack.imgur.com/sA9pz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sA9pz.png"" alt=""enter image description here"" /></a></p>
<p>only pie chart is available:</p>
<p><a href=""https://i.stack.imgur.com/H9CoY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H9CoY.png"" alt=""enter image description here"" /></a></p>
<p>Many thanks all ..</p>
","<oracle><data-visualization><data-warehouse><business-intelligence><obiee>","2021-06-24 17:48:16","365","0","2","68297499","<p>You can use D3 to and create this via a narrative but you will need to code it yourself.</p>
<p>To do this you need to</p>
<ol>
<li>Make d3 available on your BI server (I can't remember whether d3.js is already there in 12.2.1.4 or not)</li>
<li>Create analysis you want to chart</li>
<li>Take code from <a href=""https://www.d3-graph-gallery.com/graph/donut_basic.html"" rel=""nofollow noreferrer"">https://www.d3-graph-gallery.com/graph/donut_basic.html</a> and put into narative (remembering to tick the contains HTML check box)</li>
</ol>
<p>Assuming you have a table that looks something like
| Name | Value |
| ---- | ---- |
| a | 9 |
| b | 20 |
| c | 30 |
| d | 8 |
| e | 12 |</p>
<h2>Prefix</h2>
<pre class=""lang-html prettyprint-override""><code>&lt;!-- Load D3 library, need to change URL acccordingly --&gt;
&lt;script src=&quot;/analytics/res/[release string]/d3.js&quot; /&gt;
&lt;!-- Create div to hold your chart --&gt;
&lt;div id=&quot;my_dataviz&quot;&gt;&lt;/div&gt;

&lt;!-- Start my donut chart script --&gt;
&lt;Script&gt;
var data = {
</code></pre>
<h2>Narrative</h2>
<pre class=""lang-html prettyprint-override""><code>@1: @2
</code></pre>
<h2>Row Separator</h2>
<pre><code>,
</code></pre>
<h2>Postfix</h2>
<pre class=""lang-js prettyprint-override""><code>}
// the d3 script references above minus the starting &lt;script&gt; tag and the section which sets data
&lt;/script&gt;
</code></pre>
<hr />
<h2>How Does It Work</h2>
<p>The key is the narrative bit.   @1 references the first column, @2 the second and so on.
This just builds up the script dynamically so the output would end up looking like
<code>var data = {a: 9, b: 20, c:30, d:8, e:12}</code></p>
<p>You can change this as necessary, add tooltips to display values when you hover over, display a total in the middle etc.</p>
<p>If D3 is not already there (and it is a bit of a faff to find but go to domain home and <code>find . -name &quot;d3*js&quot;</code> and see what pops up) then you will need somewhere to make the d3 library available (easy if you have OHS in front of BI) have a look at <a href=""https://gianniceresa.com/2016/12/deploy-custom-folder-obiee-12c-analyticsres/"" rel=""nofollow noreferrer"">https://gianniceresa.com/2016/12/deploy-custom-folder-obiee-12c-analyticsres/</a> to create somewhere to access script locally</p>
<h2>Cons</h2>
<p>This approach isn't without cons.   The first one is that you need to code this up in javascript.   It isn't too difficult and there are plenty of examples on the internet but it is obviously more difficult than just creating a normal chart.</p>
<p>Another one is exporting.   As these charts are rendered in the browser and BI just thinks they are html they will not export properly (you will see the raw HTML / javascript)</p>
"
"68059407","Can a Kusto table have multiple update policy?","<p>I have a Kusto table and want to ingest data from two azure blob locations. Data from both sources need some transformation, hence I defined two update policies. So now I have two source tables and one destination table. The first update policy was working fine, and then I went on defining the second update policy which also worked fine. But after that when I observe, data was not getting ingested from the first source(It is present in the temporary source table) to the target table. I wonder if defining the second update policy somehow disabled the first update policy? Is it possible to have two update policies on one destination table?</p>
","<bigdata><azure-data-explorer><kql><data-mapping>","2021-06-20 19:33:20","491","2","1","68059565","<blockquote>
<p>Is it possible to have two update policies on one destination table?</p>
</blockquote>
<p>Yes, it is possible.</p>
<p>You may want to verify that you have an array of 2 update policies defined on the target table, and that you didn't overwrite the first with the second using the <code>.alter</code> policy command.</p>
"
"68047420","group by all non-aggregate columns","<p>I'm currently using postgres for a datawarehouse and are running into some big queries which are set up the following way:</p>
<pre><code>SELECT
    col_1,
    col_2,
    col_3,
    ...,
    col_41,
    SUM(col_42)
FROM table
GROUP BY
1,2,3,4,..., 41
</code></pre>
<p>I'm not fond of the group by syntax using the numbers, but with the sheer number of columns I can see why it would be used.</p>
<p>My main question is:
Is there a way to simply group by all columns without an aggregating function?</p>
<p>Something along the lines of:</p>
<pre><code>SELECT
    col_1,
    col_2,
    col_3,
    ...,
    col_41,
    SUM(col_42)
FROM table
GROUP BY ALL
</code></pre>
<p>or even</p>
<pre><code>SELECT
    col_1,
    col_2,
    col_3,
    ...,
    col_41,
    SUM(col_42)
FROM table
GROUP BY 1 to 41
</code></pre>
<p>would be an improvement.</p>
","<sql><postgresql><data-warehouse>","2021-06-19 13:52:41","50","1","1","68047654","<p>You can almost do what you want . . . if you are content to have the first column be a <em>tuple</em> rather than a column reference:</p>
<pre><code>SELECT (col1, col2, col3), SUM(x)
FROM t
GROUP BY 1
</code></pre>
<p>I don't think there is a way to extract the columns from a tuple without listing them.  For instance, the following does not work:</p>
<pre><code>SELECT t.cols.*, sum_x
FROM (SELECT (col1, col2, col3) as cols, SUM(x) as sum_x
      FROM t
      GROUP BY 1
     ) t;
</code></pre>
<p>You <em>can</em> reconstruct the columns -- but you are back to verbosity.  For an anonymous row type:</p>
<pre><code>SELECT (cols).f1 as col1, (cols).f2 as col2, (cols).f3 as f3, sum_x
FROM (SELECT (col1, col2, col3) as cols, SUM(x) as sum_x
      FROM t
      GROUP BY 1
     ) t;
</code></pre>
"
"68043640","How to set ingestion mapping for TSV or W3CLOGFILE format in Kusto database in Azure Data Explorer?","<p>I am relatively new to Kusto. I was wondering how do we specify ingestion mappings in Kusto for formats other than CSV, JSON, AVRO, ORC, or Parquet. I could see examples only for these data formats in the documentation. I want to create an ingestion mapping for TSV or W3CLOGFILE format, and ingest data through the built-in pipeline in ADX from azure blob storage.</p>
","<json><csv><azure-data-explorer><kql><data-mapping>","2021-06-19 04:34:04","211","0","1","68045120","<p>For TSV it is the same as CSV, see relevant <a href=""https://learn.microsoft.com/en-us/azure/data-explorer/kusto/management/mappings#csv-mapping"" rel=""nofollow noreferrer"">doc</a> comment:</p>
<blockquote>
<p>CSV mapping can be applied on all the delimiter-separated formats:
CSV, TSV, PSV, SCSV, and SOHsv.</p>
</blockquote>
<p>As for W3CLOGFILE, the mapping format is the same as for <a href=""https://learn.microsoft.com/en-us/azure/data-explorer/kusto/management/mappings#avro-mapping"" rel=""nofollow noreferrer"">AVRO mapping</a></p>
"
"68036753","Daily time steps interpolation between several netcdf layers over time in R or CDO","<p>I have several observations over time of a spatial variable (let's say var_A_1985.nc, var_A_2000.nc and var_A_2010.nc) with only one time layer. Would it be possible to efficiently reconstruct the daily values from 01/01/1985 to 01/01/2010 by linearly interpolating these values?</p>
<p>I currently explored two options:</p>
<ul>
<li><p>Climate Data Operator (CDO): I saw the operators inttime and intyear, but these do not seem to work for my case.</p>
</li>
<li><p>R as RasterBrick element manipulation</p>
</li>
</ul>
<p>something like:</p>
<pre><code>library(raster)
library(ncdf4)
setwd(mypath)

var_A_1985 &lt;- raster(&quot;var_A_1985.nc&quot;)
var_A_2000 &lt;- raster(&quot;var_A_2000.nc&quot;)

var_A_brick &lt;- brick(var_A_1985, var_A_2000)
brick_2&lt;- brick(lapply(1:n_days, function(x) raster::raster(matrix(NA, nrow(var_A_1985), ncol(var_A_1985)))) #with n_days representing the number of days between 01/01/1985 and 01/01/2000
extremes&lt;- c(1, n_days)

for (i in 1:2) {
    brick_2[[extremes[i]]]=var_A_brick[[1]]
}
var_A_brick_filled &lt;- approxNA(brick_2, method = 'linear')
#and then write this the new netcdf with daily values 
</code></pre>
<p>This second method actually works, but it takes forever. Is there a more efficient way to do this?</p>
","<r><netcdf><data-management><cdo-climate>","2021-06-18 14:15:38","436","1","1","68037125","<p>You can do this with CDO as follows:</p>
<pre><code>cdo -inttime,1985-01-01,12:00:00,1day -mergetime var*.nc outfile
</code></pre>
<p>Adjust the start time, of course.</p>
"
"67938232","In Azure Synpase, how can I check how a table is distributed","<p>In Azure Synapse, how can I check how a table is distributed. For example whether it is distributed in a round robin manner or with hash keys.</p>
","<sql-server><azure><data-warehouse><azure-synapse>","2021-06-11 13:51:34","1640","1","2","67939352","<p>You can use the Dynamic Management View (DMV) <code>sys.pdw_table_distribution_properties</code> in a dedicated SQL pool to determine if a table is distributed via round robin, hash or replicated, eg</p>
<pre><code>SELECT 
       OBJECT_SCHEMA_NAME( object_id ) schemaName, 
       OBJECT_NAME( object_id ) tableName,
       *
FROM sys.pdw_table_distribution_properties;
</code></pre>
<p>It's the <code>distribution_policy_desc</code> column.  Some sample results:</p>
<p><a href=""https://i.stack.imgur.com/HFlCh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HFlCh.png"" alt=""Results"" /></a></p>
"
"67938232","In Azure Synpase, how can I check how a table is distributed","<p>In Azure Synapse, how can I check how a table is distributed. For example whether it is distributed in a round robin manner or with hash keys.</p>
","<sql-server><azure><data-warehouse><azure-synapse>","2021-06-11 13:51:34","1640","1","2","67940854","<p>Don't confuse <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/massively-parallel-processing-mpp-architecture#distributions"" rel=""nofollow noreferrer"">distribution</a> and <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition"" rel=""nofollow noreferrer"">partitioning</a>. I've updated the question.</p>
<hr />
<p><code>pdw_table_distribution_properties</code> is certainly a possibility as mentioned.</p>
<p>Or just generate the create DDL for that table using any Client (Data Studio, SSMS, VS Code with plugin, ...).</p>
<p>E.g. in Azure Data Studio, right click on the table and click &quot;Script as Create&quot;.</p>
<p><a href=""https://i.stack.imgur.com/2knV8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2knV8.png"" alt=""enter image description here"" /></a></p>
<p>Look for <code>DISTRIBUTION</code> in <code>WITH</code> clause.</p>
<pre><code>SET ANSI_NULLS ON
GO
SET QUOTED_IDENTIFIER ON
GO
CREATE TABLE [dbo].[sa_logs]
(
    [version_number] [float] NULL,
    [request_start_time] [datetimeoffset](7) NULL,
    ...
    [referrer_header] [varchar](256) NULL,
)
WITH
(
    DISTRIBUTION = ROUND_ROBIN,
    HEAP
)
GO
</code></pre>
<p>or for a table with <code>HASH</code>:</p>
<pre><code>...
    DISTRIBUTION = HASH ( [hash_column] ),
...
</code></pre>
<hr />
"
"67930166","Can you audit user exports from Snowflake's UI?","<p>Snowflake makes it easy to audit queries users make against the data warehouse by providing a set of <code>QUERY_HISTORY</code> views. Is there an analogous table that keeps track of data export actions, too?</p>
<p>The <code>ROWS_UNLOADED</code> column and its equivalents seem to point in the direction of the query itself writing to an external system, but not a UI initiated data download.</p>
","<snowflake-cloud-data-platform><data-warehouse>","2021-06-11 01:15:31","119","0","1","67940670","<p>No such table exists, plus once the data has reached the client it is effectively exported anyway, right?  The user could copy/paste it, transcribe it, screenshot it, access it via Javascript, or any multiple of &quot;exports&quot; besides the &quot;Export&quot; button.</p>
"
"67928401","Accessing Aurora Postgres Materialized Views from Glue data catalog for Glue Jobs","<p>I have an Aurora Serverless instance which has data loaded across 3 tables (mixture of standard and jsonb data types). We currently use traditional views where some of the deeply nested elements are surfaced along with other columns for aggregations and such.</p>
<p>We have two materialized views that we'd like to send to Redshift. Both the Aurora Postgres and Redshift are in Glue Catalog and while I can see Postgres views as a selectable table, the crawler does not pick up the materialized views.</p>
<p>Currently exploring two options to get the data to redshift.</p>
<ol>
<li>Output to parquet and use copy to load</li>
<li>Point the Materialized view to jdbc sink specifying redshift.</li>
</ol>
<p>Wanted recommendations on what might be most efficient approach if anyone has done a similar use case.</p>
<p>Questions:</p>
<ol>
<li>In option 1, would I be able to handle incremental loads?</li>
<li>Is bookmarking supported for JDBC (Aurora Postgres) to JDBC (Redshift) transactions even if through Glue?</li>
<li>Is there a better way (other than the options I am considering) to move the data from Aurora Postgres Serverless (10.14) to Redshift.</li>
</ol>
<p>Thanks in advance for any guidance provided.</p>
","<amazon-redshift><aws-glue><amazon-aurora><aws-glue-data-catalog>","2021-06-10 20:59:12","631","0","1","67987608","<p>Went with option 2. The Redshift Copy/Load process writes csv with manifest to S3 in any case so duplicating that is pointless.</p>
<p>Regarding the Questions:</p>
<ol>
<li><p>N/A</p>
</li>
<li><p>Job Bookmarking does work. There is some gotchas though - ensure Connections both to RDS and Redshift are present in Glue Pyspark job, IAM self ref rules are in place and to identify a row that is unique [I chose the primary key of underlying table as an additional column in my materialized view] to use as the bookmark.</p>
</li>
<li><p>Using the primary key of core table may buy efficiencies in pruning materialized views during maintenance cycles. Just retrieve latest bookmark from cli using <code>aws glue get-job-bookmark --job-name yourjobname</code> and then just that in the where clause of the mv as <code>where id &gt;= idinbookmark</code></p>
<p><code>conn = glueContext.extract_jdbc_conf(&quot;yourGlueCatalogdBConnection&quot;)</code>
<code>connection_options_source = { &quot;url&quot;: conn['url'] + &quot;/yourdB&quot;, &quot;dbtable&quot;: &quot;table in dB&quot;, &quot;user&quot;: conn['user'], &quot;password&quot;: conn['password'], &quot;jobBookmarkKeys&quot;:[&quot;unique identifier from source table&quot;], &quot;jobBookmarkKeysSortOrder&quot;:&quot;asc&quot;}</code></p>
</li>
</ol>
<p><code>datasource0 = glueContext.create_dynamic_frame.from_options(connection_type=&quot;postgresql&quot;, connection_options=connection_options_source, transformation_ctx=&quot;datasource0&quot;)</code></p>
<p>That's all, folks</p>
"
"67855395","Import Fact Table Data from Multiple Tables and Queries","<p>I am trying to build the fact table in this snowflake schema: <a href=""https://i.stack.imgur.com/rCHaF.png"" rel=""nofollow noreferrer"">!</a><a href=""https://i.stack.imgur.com/rCHaF.png"" rel=""nofollow noreferrer"">1</a></p>
<p>I am using this database: <a href=""https://github.com/scottsimpson/restaurant-database"" rel=""nofollow noreferrer"">enter link description here</a></p>
<p>I am using SSMS, and when I right click my warehouse and choose import data then the query approach, I am left with the issue of having to fit all those queries for my measures into 1 big query which I had issues writing, but ended up with this only solution that had no errors:</p>
<pre><code>SELECT Orders.OrderID AS OrderID,
Events.EventID AS EventID,
Customers.CustomerID AS CustomerID,
Dishes.DishID AS DishID,
NormAvgTable.AverageNormalCustomerCount,
EveAvgTable.AverageEventCustomerCount,
FavDishTable.CustomerFavDishOrderCount,
EveRevTable.TotalEventsRevenue,
DishOrderCtTable.DishOrderCount,
CustomerRev.CustomerOrderRevenue,
BdayCtTable.CustomerBirthdayOrderCount
FROM Orders, Events, Customers, Dishes
CROSS JOIN (
SELECT AVG(count) AS AverageNormalCustomerCount
    FROM (
    SELECT Events.EventID, COUNT(Orders.CustomerID) AS count
    FROM Customers
    INNER JOIN Orders
    ON Orders.CustomerID = Customers.CustomerID
    INNER JOIN OrdersDishes
    ON Orders.OrderID = OrdersDishes.OrderID
    INNER JOIN Events
    ON CAST(Orders.OrderDate AS DATE) &lt;&gt; CAST(Events.Date AS DATE)
    GROUP BY Events.EventID
) AS CtTable
) AS NormAvgTable
CROSS JOIN (
SELECT AVG(count) AS AverageEventCustomerCount
    FROM (
    SELECT Events.EventID, COUNT(Orders.CustomerID) AS count
    FROM Customers
    INNER JOIN Orders
    ON Orders.CustomerID = Customers.CustomerID
    INNER JOIN OrdersDishes
    ON Orders.OrderID = OrdersDishes.OrderID
    INNER JOIN Events
    ON CAST(Orders.OrderDate AS DATE) = CAST(Events.Date AS DATE)
    GROUP BY Events.EventID
)AS EveCtTable
) EveAvgTable
CROSS JOIN(
SELECT Customers.CustomerID AS CustomerID, COUNT(Orders.OrderID)
AS CustomerFavDishOrderCount
FROM Orders
INNER JOIN Customers
ON Orders.CustomerID = Customers.CustomerID
INNER JOIN OrdersDishes
ON Customers.FavoriteDish = OrdersDishes.DishID
GROUP BY Customers.CustomerID
) AS FavDishTable
CROSS JOIN (
SELECT SUM(Dishes.Price)
AS TotalEventsRevenue
FROM Events
INNER JOIN Orders
ON CAST(Orders.OrderDate AS DATE) = CAST(Events.Date AS DATE)
INNER JOIN OrdersDishes
ON Orders.OrderID = OrdersDishes.OrderID
INNER JOIN Dishes
ON OrdersDishes.DishID = Dishes.DishID
GROUP BY Events.EventID
) AS EveRevTable
CROSS JOIN(
SELECT  Dishes.DishID AS DishID, COUNT(OrdersDishes.OrdersDishesID) 
AS DishOrderCount
FROM OrdersDishes
INNER JOIN Dishes
ON Dishes.DishID = OrdersDishes.DishID
GROUP BY Dishes.DishID
) AS DishOrderCtTable
CROSS JOIN(
SELECT Orders.CustomerID AS CustomerID, SUM(Dishes.Price)
AS CustomerOrderRevenue
FROM OrdersDishes
INNER JOIN Orders
ON Orders.OrderID = OrdersDishes.OrderID
INNER JOIN Dishes
ON OrdersDishes.DishID = Dishes.DishID
GROUP BY Orders.CustomerID
) AS CustomerRev
CROSS JOIN(
SELECT Customers.CustomerID AS CustomerID, COUNT(Orders.OrderID) 
AS CustomerBirthdayOrderCount
FROM Orders, Customers
WHERE DAY(Orders.OrderDate) = DAY(Customers.Birthday)
AND MONTH(Orders.OrderDate) = MONTH(Customers.Birthday)
GROUP BY Customers.CustomerID
) AS BdayCtTable
</code></pre>
<p>Aside from the fact that it looks very ugly, it gave me no errors and kept copying up to <strong>50 Million</strong> records, which I had to stop the wizard at because it kept going even further.</p>
<p>What is a better query I can use/how to optimize mine (what are mistakes in it).</p>
","<sql><sql-server><data-warehouse>","2021-06-06 02:37:23","291","0","1","67855823","<p>Two issues - Record count high and SQL looks complex/unreadable.
Record count is high because of cross join. You need to set some join condition between subqueries and tables. For example, you mentioned below tables but no joins <code>FROM Orders, Events, Customers, Dishes</code>. I am not sure about requirements so, here are my thoughts/solutions. -</p>
<ol>
<li>Make a bare minimum SQL with only Orders table. Then keep on adding joins and check count, time consumed. if higher than expectation try to tune that join. proceed till last table.</li>
<li>Cross join is always expensive - try using inner or left. Also few tables doenst have join conditions which produces Cartesian join and thus making result count huge.</li>
<li>Use sqlformat.org/ to make your sql beautiful :)</li>
<li>If point#1 above becomes complex, make two SQLs and create an intermediate table to do this. You dont have to put all logic in one SQL. Can break it as well.</li>
<li>Check index in each table and see if you can use them in your SQL.</li>
</ol>
"
"67811062","How to SUM distinct out of of GROUP BY that have duplicates","<p>I am calculating average number of orders per person within four groups, and I have the following SQL</p>
<pre><code>SELECT      TO_DATE(ORDER_TIME AT TIME ZONE 'UTC' AT TIME ZONE 'US/Pacific','YYYY-MM-DD'),
            GROUP_P,
            COUNT(CASE WHEN ORDER_TIME IS NOT null
                       THEN ORDER_ID END) AS Numerator,
            COUNT(DISTINCT
                  CASE WHEN ORDER_TIME IS NOT null
                       THEN PERSON_ID END) AS Denominator
FROM        ORDER_TAB 
WHERE       TO_DATE(ORDER_TIME AT TIME ZONE 'UTC' AT TIME ZONE 'US/Pacific','YYYY-MM-DD')
BETWEEN     DATEADD( day, -1, current_Date) AND current_Date
GROUP BY    1, 2
</code></pre>
<p>My result looks like this</p>
<pre><code>to_date     group_p    numerator   denominator
2021-06-01  A          28          20
2021-06-01  B          5           5
2021-06-01  C          1577        1025
2021-06-01  D          1010        765
</code></pre>
<p>Because one <code>person_id</code> can be in different groups, if I sum up from the above table, I wouldn't have the accurate denominator value due to potential duplicates. In my below code, I removed the <code>group by</code>, and the <code>numerator</code> sums up the same, but the <code>denominator</code> is getting less.</p>
<pre><code>SELECT      TO_DATE(ORDER_TIME AT TIME ZONE 'UTC' AT TIME ZONE 'US/Pacific','YYYY-MM-DD'),

            COUNT(CASE WHEN ORDER_TIME IS NOT null
                       THEN ORDER_ID END) AS Numerator,
            COUNT(DISTINCT
                  CASE WHEN ORDER_TIME IS NOT null
                       THEN PERSON_ID END) AS Denominator
FROM        ORDER_TAB 
WHERE       TO_DATE(ORDER_TIME AT TIME ZONE 'UTC' AT TIME ZONE 'US/Pacific','YYYY-MM-DD')
BETWEEN     DATEADD( day, -1, current_Date) AND current_Date
GROUP BY    1
</code></pre>
<pre><code>to_date     numerator   denominator
2021-06-01  2620        1699
</code></pre>
<p>Because of our reporting requirement, I must keep the output in the way that showed with <code>group_p</code> column.</p>
<p>What is the best way to use the accurate <code>1699</code> when summing up from the output?</p>
","<sql><database><reporting><data-warehouse>","2021-06-02 19:05:43","42","1","1","67811425","<p>Below query will count person_id once (first group in ascending order) for Denominator column. I have used row_number() window ranking function with common table expression for that.</p>
<pre><code>with cte as
(
    SELECT      GROUP_P,
                ORDER_TIME,
                row_number()over(partition by person_id order by group_p) rnk            
    FROM        ORDER_TAB 
    WHERE       TO_DATE(ORDER_TIME AT TIME ZONE 'UTC' AT TIME ZONE 'US/Pacific','YYYY-MM-DD')
    BETWEEN     DATEADD( day, -1, current_Date) AND current_Date
)
select    TO_DATE(ORDER_TIME AT TIME ZONE 'UTC' AT TIME ZONE 'US/Pacific','YYYY-MM-DD'),
          GROUP_P,
          COUNT(CASE WHEN ORDER_TIME IS NOT null
                     THEN ORDER_ID END) AS Numerator,
          COUNT(CASE WHEN ORDER_TIME IS NOT null and rnk=1
                     THEN PERSON_ID END) AS Denominator
from cte
</code></pre>
"
"67810005","Wide Fact Table vs Narrow Fact Table (Power BI)","<p>First off I realize that narrow fact tables are the ideal situation.</p>
<p>I am designing a healthcare data warehouse specifically for ingestion into Power BI. The problem I'm having is that I have over 100 different metrics that are included in just one report. Most of the data comes from the source like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Hospital</th>
<th>HospitalID</th>
<th>Date</th>
<th>Description</th>
<th>Number</th>
</tr>
</thead>
<tbody>
<tr>
<td>Children's Hospital</td>
<td>20192</td>
<td>1/2/2021</td>
<td>Beds Needed</td>
<td>8</td>
</tr>
<tr>
<td>Children's Hospital</td>
<td>20192</td>
<td>1/2/2021</td>
<td>Covid Patients</td>
<td>2</td>
</tr>
</tbody>
</table>
</div>
<p>We currently use logic to pull each metric out like this in PowerBI:</p>
<p><em>Beds Needed=IF(Description=&quot;Beds Needed&quot;, Number,0)</em></p>
<p>We do this for over 100 metrics that are needed according to business leaders. My question is, there are two ways Im thinking of doing this:</p>
<p><strong>Option 1:</strong></p>
<p>We put the logic like above into the database and have every metric be it's own column.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Date</th>
<th>Hospitalid</th>
<th>Beds Needed</th>
<th>Covid Patients</th>
</tr>
</thead>
<tbody>
<tr>
<td>1/2/2021</td>
<td>20192</td>
<td>8</td>
<td>2</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Option 2:</strong></p>
<p>I setup the fact table like so:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Date</th>
<th>HospitalID</th>
<th>Descriptionid</th>
<th>Number</th>
</tr>
</thead>
<tbody>
<tr>
<td>1/2/2021</td>
<td>20192</td>
<td>12</td>
<td>8</td>
</tr>
<tr>
<td>1/2/2021</td>
<td>20192</td>
<td>11</td>
<td>2</td>
</tr>
</tbody>
</table>
</div>
<p>And then create a dimension table like so:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Description</th>
<th>DescriptionID</th>
</tr>
</thead>
<tbody>
<tr>
<td>Beds Needed</td>
<td>12</td>
</tr>
<tr>
<td>Covid Patients</td>
<td>11</td>
</tr>
</tbody>
</table>
</div>
<p>The tables that I have currently (in the format of the first table) each are around 200k rows and there are 4 of them. There is one table that supplies metrics that is around 20 million rows.</p>
","<sql-server><powerbi><data-modeling><data-warehouse>","2021-06-02 17:39:17","341","1","1","75995746","<p>Option 2 is the cleaner version. A large fact table will store numbers better than text.</p>
<p>For the 20mm row fact table you could use incremental refresh to pull it in.</p>
"
"67784156","Getting the distinct count of rows that are in one dataset which dont exist in the other dataset, COGNOS BI report building","<p>I'm working with semi-additive data, so account balances. I'm trying to create a report where I select a time period, from this time period I'm interested in account status at the start and end of the time period. With these two 'datasets' I want to do the following:</p>
<ul>
<li>Get the distinct count of account IDs that exist in the first dataset, but not the last</li>
<li>Get the distinct count of account IDs that exist in the last dataset, but not the first</li>
</ul>
<p>Cognos has built in this except/intersect functionality, but I'm a little concerned about using it. Worried that it will get very messy quickly. What is the best practice when it comes to creating what I'm describing here?</p>
","<reporting><data-warehouse><business-intelligence><cognos>","2021-06-01 07:05:58","156","0","1","67794823","<p>There may be more elegant solutions using window functions, but I like to use many, simple pieces so that the next person has some hope of supporting what I have done.</p>
<p>I think you'll need 4 queries:</p>
<p>Query1<br />
Data item: AccountId<br />
Filters: [date] = ?startdate? (+ as needed)</p>
<p>Query2<br />
Data item: AccountId<br />
Filters: [date] = ?enddate? (+ as needed)</p>
<p>Query3 = Query1 EXCEPT Query2  (lost)<br />
Data item:  count(AccountId)</p>
<p>Query4 = Query2 EXCEPT Query1  (gained)<br />
Data item:  count(AccountId)</p>
"
"67756732","Fact Table with Primary Key and Foreign Keys","<p>I have been going through Kimball-The-Data warehouse-Tookit 3rd edition to understand about facts and dimension; I couldn't understand the following paragraph information. Would anyone please give me possible understanding:</p>
<p><strong>The fact table generally has its own primary key composed of a subset of the foreign keys. This key is often called a composite key. Every table that has a composite key is a fact table. Fact tables express many-to-many relationships. All others are dimension tables.
There are usually a handful of dimensions that together uniquely identify each fact table row. After this subset of the overall dimension list has been identified, the rest of the dimensions take on a single value in the context of the fact table row’s primary key. In other words, they go along for the ride.</strong></p>
<p>Thank you for giving your valuable time.</p>
","<ssis><data-warehouse>","2021-05-30 00:36:34","1060","0","1","67761294","<p>To completely understand this we may need to break it down and help us with a couple of pictures.</p>
<p>Let's start by the first definition:</p>
<ul>
<li>The fact table generally has its own primary key composed of a subset of the foreign keys. This key is often called a composite key.</li>
</ul>
<p><a href=""https://i.stack.imgur.com/Ckd1k.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ckd1k.png"" alt=""enter image description here"" /></a></p>
<p>In this very simple diagram we can see that there is a fact table (Fact_Sales) which hasn't got its own primary key per se but a set of foreign keys pointing to the dimension tables (which do have primary keys). This set of foreign keys in sales (Date_Id/Store_Id/Product_Id) is a composite key, acting as the primary key of the fact table. Which bring us to the definition:</p>
<ul>
<li>Every table that has a composite key is a fact table.</li>
</ul>
<p>For the next definition:</p>
<ul>
<li>Fact tables express many-to-many relationships</li>
</ul>
<p>Again, as it can be seen in the ERD of the example, the fact table Fact_Sales can also be seen as a junction table expressing a many-to-many relationship between Dim_Store and Dim_Product, i.e a store can sell many products and a product can be sold in many stores.</p>
<p>Now, if a table is not a Fact table as defined above then we come to the next definition:</p>
<ul>
<li>All others are dimension tables</li>
</ul>
<p>Ok, so far so good so let's carry on with the subsequent definitions:</p>
<ul>
<li>There are usually a handful of dimensions that together uniquely identify each fact table row.</li>
</ul>
<p>As we make our model more complex we may reach the point where the set of foreign keys of a fact becomes a unique key. For example if we'd added another dimension Dim_Customer and therefore a new foreign key to Fact_Sales we'll then have the composite key as
(Date_Id/Store_Id/Product_Id/Customer_Id), also lets say we add time to the dimension date (this could bring a separate discussion but for the sake of this analysis let's assume we have it in date dimension) then this composite key would be unique, since is not possible to sell the same thing to the same customer in the same store at the same time.</p>
<p>Finally, is time to extend our model a bit further so we are moving to a snowflake schema like this:</p>
<p><a href=""https://i.stack.imgur.com/Hmjmw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Hmjmw.png"" alt=""enter image description here"" /></a></p>
<p>This last sentence:</p>
<ul>
<li>After this subset of the overall dimension list has been identified, the rest of the dimensions take on a single value in the context of the fact table row’s primary key. In other words, they go along for the ride.</li>
</ul>
<p>means that once we identify the dimensions corresponding to a fact through its foreign keys (fact table row's primary key), for example in the above diagram these would be Date_Id/Store_Id/Product_Id, the other dimensions get uniquely identified by the dimension tables foreign keys; for instance, Dim_Geography is &quot;going along for the ride&quot;, invited by Dim_Store through its foreign key Geography_Id</p>
<p><strong>Image Attribution:</strong>
SqlPac at English Wikipedia, CC BY-SA 3.0 <a href=""https://creativecommons.org/licenses/by-sa/3.0"" rel=""nofollow noreferrer"">https://creativecommons.org/licenses/by-sa/3.0</a>, via Wikimedia Commons</p>
"
"67737588","Is it allowed to update a row of a fact table?","<p>I am confused. Is an update for a row in a fact table recommended or just don't do it in general?</p>
<p>I have read that it can slow the process, because an update is slower than an insert. Other say that it is not that bad as some people think.</p>
<p>Is there an advice for 2021 with state of the art Ram/cpu/...?</p>
","<database-design><data-warehouse><fact>","2021-05-28 10:55:35","291","-1","1","67745448","<p>In general, fact records are a moment in time and shouldn't need to be updated, so if you regularly need to update fact records then there may be an issue with your design.</p>
<p>The 2 main cases where fact updates are justified are:</p>
<ul>
<li>Accumulating snapshot facts</li>
<li>Data fixes, which obviously would be an infrequent activity</li>
</ul>
<p>From a technical perspective, updates are slower than inserts but whether they are &quot;too slow&quot; depends on your specific circumstances - though given the available technology these days I doubt whether having to do updates would be an insurmountable problem in 99% of circumstances.</p>
<p>If you do need to regularly update a fact table you should ensure it has a single PK column, rather than a composite PK made up of many dimensional keys, as referencing this in your updates will be much faster than using a composite key.</p>
"
"67676327","PowerBI: Advanced filtering using data modelling to create dependent dropdowns","<p>I have created a <strong>dimension table filter</strong> which is modelled from 2 tables</p>
<p><a href=""https://i.stack.imgur.com/2biXV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2biXV.png"" alt=""enter image description here"" /></a></p>
<p>Query for someone who has the same problem in the future -</p>
<pre><code>calcProjectNameFilter = DISTINCT(
UNION(
VALUES('Vani_Trends actual'[Project Name]),
VALUES('Vani_Trends2 - planned'[Project Name])
)
)
</code></pre>
<p>Now I want to create another filter- phase name but it should be dependent on the selected project name. My attempt was to do something like -</p>
<pre><code>calcProjectNameFilter = CALCULATETABLE( DISTINCT(
UNION(
VALUES('Vani_Trends actual'[Project Name]),
VALUES('Vani_Trends2 - planned'[Project Name])
),
ALLSELECTED(calcProjectNameFilter)
))
</code></pre>
<p>but that didn't work. Can anybody help me with creating dependent dropdowns using data modelling?</p>
","<powerbi><dax><powerquery>","2021-05-24 17:21:40","50","0","1","67686015","<p><a href=""https://www.youtube.com/watch?v=o8K4Px_6AYQ"" rel=""nofollow noreferrer"">Try this video</a>, maybe it can help you. Let me know if it does.</p>
<p>Single Vs Both Cross filtering:-</p>
<ol>
<li>Single cross-filtering is enabled only one table can filter out the contents from the other table.</li>
</ol>
<p>Table A can filter out the Table B</p>
<p><a href=""https://i.stack.imgur.com/QPjfF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QPjfF.png"" alt=""A filters B"" /></a></p>
<p>But Table B can not filter Table A</p>
<p><a href=""https://i.stack.imgur.com/E3VLA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E3VLA.png"" alt=""B cannot filters A"" /></a></p>
<ol start=""2"">
<li>In Both Cross-filtering Both Tables can filter out each other</li>
</ol>
<p>Table A filters Table B</p>
<p><a href=""https://i.stack.imgur.com/7kPHD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7kPHD.png"" alt=""A filters B"" /></a></p>
<p>Table B filters Table A</p>
<p><a href=""https://i.stack.imgur.com/3UGRH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3UGRH.png"" alt=""B filters A"" /></a></p>
"
"67634758","Updating of Fact tables","<p>I have a flatfile resources that were extracted into facts and dimensions.
Some dimensions also comes from db resources.
The transformation process is set on as needed basis (if there are new/updated from flatfiles).
The problem is this, some data reference doesn't exist or match on the dimension based on db resources so the foreign key id value on the fact is set to default (zero if no matching data).</p>
<p>How can i perform an update on the facts if the said dimension (db resource) has been updated?
What was the best practice/routine for this kind of scenario?</p>
<p>This is the sample illustration</p>
<pre><code>Flatfile source                           product list (db source)
--------------------------------          ------------------------------
| product name | year | volume |          | prodcode |  name           |
--------------------------------          ------------------------------
| apple        | 2020 |  1000  |          | 001      | apple           |
| watermelon   | 2020 |  2000  |          | 002      | mango           |
--------------------------------          ------------------------------
</code></pre>
<p>Fact/Dimension</p>
<pre><code>production_fact                           dim_product
-------------------------------          ---------------------------
| fk_product| fk_date| volume |          | id | prodcode |  name   |
-------------------------------          --------------------------|
| 2         |  d001  |  1000  |          |  1 |  n/a      | n/a    |
| 1         |  d001  |  2000  |          |  2 |  001      | apple  |
-------------------------------          |  3 |  002      | mango  |
                                         ---------------------------
</code></pre>
<p>If the product list will be updated (003 watermelon), should i replace the dim_product row#1 with the new value?</p>
","<database><etl><data-warehouse><data-ingestion><sql-data-warehouse>","2021-05-21 10:03:28","116","1","1","67646866","<p>Based on your example, this is the way it should work:</p>
<p>Note: I would expect prodcode to be be in flatfile, not product name. Is this really how your data looks? Anyway I will proceed.</p>
<p>First set of data arrives. Watermelon is in fact but not dimension.</p>
<pre><code>Flatfile source                           product list (db source)
--------------------------------          ------------------------------
| product name | year | volume |          | prodcode |  name           |
--------------------------------          ------------------------------
| apple        | 2020 |  1000  |          | 001      | apple           |
| watermelon   | 2020 |  2000  |          | 002      | mango           |
--------------------------------          ------------------------------
</code></pre>
<p>We load a dimension record but it won't have any attribute values. (As I said I would normally expect the code to be in the fact input data but that's fine we'll go with description). This will of course require some logic to find dimensions that are in fact but not in dimensions.</p>
<pre><code>production_fact                           dim_product
-------------------------------      ------------------------------------------------
| fk_product| fk_date| volume |      | id | prodcode |  name       | weight |colour |
-------------------------------      ------------------------------------------------
| 2         |  d001  |  1000  |      |  1 |  n/a      | n/a        | n/a    | n/a   |
| 4         |  d001  |  2000  |      |  2 |  001      | apple      | 200mg  | red   |
-------------------------------      |  3 |  002      | mango      | 400mg  | yellow|
                                     |  4 |  ?        | watermelon | ?      |   ?   |
                                     ------------------------------------------------
</code></pre>
<p>So we have dimension SK 4 which is a legitimate dimension record except it's missing a load of attributes.</p>
<p>Later, the dimension arrives. We know what it's meant to match on so we update the existing dimension which was missing data.</p>
<pre><code> product list (db source)
-----------------------------------------------
| prodcode |  name           | weight |colour |
--------------------------------------|-------|
| 003      | watermelon      | 1kg    | green |
-----------------------------------------------


------------------------------------------------
| id | prodcode |  name       | weight |colour |
------------------------------------------------
|  1 |  n/a      | n/a        | n/a    | n/a   |
|  2 |  001      | apple      | 200mg  | red   |
|  3 |  002      | mango      | 400mg  | yellow|
|  4 |  003      | watermelon | 1kg    | green |
------------------------------------------------
</code></pre>
<p>You want to avoid ever updating large facts. Updating smaller dimensions is a much better idea</p>
<p>BTW this is a type 1 dimension. You can take the same appriach with a SCD except that you wouldn't count the first version of the dimension, you'd just overwrite it.</p>
"
"67596340","How to breakdown denormalized table(M:M) to data model (from same table)?","<p>I'd like to ask question about how to model data from the same table</p>
<p><strong>Source Table : Customer</strong></p>
<pre><code>Fields : cust_id , Firstname , LastName , Address No , Street , Province , PostalCode
</code></pre>
<p><strong>TO BE MODEL</strong></p>
<p>TABLE : Customer</p>
<pre><code>Fields : cust_id , Firstname , LastName
</code></pre>
<p>TABLE : House</p>
<pre><code>Fields : House_id, Address No , Street , Province , PostalCode
</code></pre>
<p>TABLE : CUST_HOUSE_RELATION</p>
<pre><code>FIELD : CUST_ID,HOUSE_ID,RELATE_TYPE
</code></pre>
<p><strong>INFO</strong></p>
<ul>
<li>1 Customer M Houses(address) and 1 House(address) M Customers</li>
<li>Create CUST_HOUSE_RELATION to solve M:M relationship</li>
<li>There is no Table Address (separated)</li>
</ul>
<p><strong>Problem</strong></p>
<ul>
<li>I cannot define HouseID because there is no addressid from Source (Table customer contains customer info and address info (with no key of address info). Please suggest.</li>
</ul>
","<database><data-modeling><data-warehouse><datamodel><azure-synapse>","2021-05-19 03:04:26","49","0","1","67602925","<p>Here is one approach but whether this is suitable depends on what you are doing.</p>
<ol>
<li>Add <code>cust_id</code> column field to <code>house</code></li>
<li>Load <code>customer</code> and <code>house</code> from source table, ensuring you populate <code>house.cust_id</code></li>
<li>I assume <code>house.house_id</code> will autogenerate, but you don't provide enough info to tell</li>
<li>Load <code>cust_hous_relation</code> from <code>house</code> (it has both keys in it)</li>
<li>Drop <code>cust_id</code> from <code>house</code></li>
</ol>
<p>You haven't provided enough info to give a proper solution.</p>
"
"67587175","Snowflake task is returning a usage error that must be granted to owner role","<p>I am having a test procedure owned by prod_admin:</p>
<pre><code>CREATE OR REPLACE PROCEDURE test()
RETURNS VARIANT
LANGUAGE JAVASCRIPT
EXECUTE AS OWNER
AS
$$
query = &quot;SELECT * FROM DWH_OPS.CHANGE_HISTORY&quot;;
stmt = snowflake.createStatement({sqlText: query}).execute();
stmt.next();
return stmt;
$$;
</code></pre>
<p>And a task to run this procedure and owned by task_admin:</p>
<pre><code>CREATE OR REPLACE TASK test_procedure
    WAREHOUSE = 'COMPUTE_WH'
    TIMESTAMP_INPUT_FORMAT = 'YYYY-MM-DD HH24'
    SCHEDULE = '5 MINUTE'
AS
   call PROD_DB.DWH_OPS.TEST();
</code></pre>
<p>Once the task is running, we are receiving the following error:</p>
<blockquote>
<p>USAGE privilege on the task's warehouse must be granted to owner role</p>
</blockquote>
<p>Both roles are having usage privileges over the same warehouse. We've ran <code>show grants</code> to both roles, and they do have access.</p>
<p>When running the procedure using <code>CALL test()</code>, it worked using both roles.</p>
","<scheduled-tasks><snowflake-cloud-data-platform><data-warehouse>","2021-05-18 13:24:28","937","0","1","67652034","<p><strong>There was 2 issues with my task:</strong></p>
<p><strong>Timestamp session format:</strong></p>
<p>It turned out that this line in our task:</p>
<p><code>TIMESTAMP_INPUT_FORMAT = 'YYYY-MM-DD HH24'</code></p>
<p>is setting the timestamp of the session into a format that is different than the format within our data.</p>
<pre><code>CREATE OR REPLACE TASK DATA_LOAD_TASK
    WAREHOUSE = 'COMPUTE_WH'
    // TIMESTAMP_INPUT_FORMAT = 'YYYY-MM-DD HH24'
    SCHEDULE = 'USING CRON 00 18 * * * Europe/Zurich'
AS
    CALL proc1();
</code></pre>
<p>As mentioned in the <a href=""https://docs.snowflake.com/en/sql-reference/parameters.html"" rel=""nofollow noreferrer"">Snowflake parameters</a> documentation, this line will set the format during the whole session into the specified format in the task.</p>
<p>By not specifying the format of this parameter, it will set to <code>AUTO</code>, which will leave to Snowflake to make necessary timestamp conversions according to field types.</p>
<p><strong>Ownership and usage over procedures and tasks:</strong></p>
<p>To work properly with task that is calling a procedure which calling many other, we should grant usage over all used procedures to the role owning the <code>task</code>. Even if we are calling one procedure.</p>
<pre><code>grant usage on procedure proc1(varchar, varchar, varchar, array) to role prod_taskadmin;
grant usage on procedure proc2(varchar, varchar, varchar, varchar) to role prod_taskadmin;
grant usage on procedure proc3(varchar, varchar, varchar, varchar) to role prod_taskadmin;
</code></pre>
<p>And the procedures should be owned by a role who have permissions over all the objects in a schema:</p>
<pre><code>grant ownership on procedure proc1(string, string, string, array) to role prod_sysadmin;
grant ownership on procedure proc2(string, string, string, array) to role prod_sysadmin;
grant ownership on procedure proc3(string, string, string, array) to role prod_sysadmin;
</code></pre>
"
"67585498","Materialized view is not showing up in plan table output from explain plan statement spool?","<p>I have created a materialized view on SH2 but, when I run my explain plan statement I can't see the materialized view being used in the plan table output. I'm not sure if it's a more complex materialized view with additional key columns to join to other dimensions, so I'm a bit confused why the materialized view isn't being utilized as I'm refering to the SH2 prefix in my select query.</p>
<pre><code>CREATE MATERIALIZED VIEW fweek_pscat_sales_mv
PCTFREE 5
BUILD IMMEDIATE
REFRESH COMPLETE
ENABLE QUERY REWRITE
AS
SELECT   t.week_ending_day
,        p.prod_subcategory
,        sum(s.amount_sold) AS Money
,        s.channel_id
,        s.promo_id
FROM     sales s
,        times t
,        products p
WHERE    s.time_id = t.time_id
AND      s.prod_id = p.prod_id
GROUP BY t.week_ending_day
,        p.prod_subcategory
,        s.channel_id
,        s.promo_id;

CREATE BITMAP INDEX FW_PSC_S_MV_SUBCAT_BIX  
ON fweek_pscat_sales_mv(prod_subcategory);

CREATE BITMAP INDEX FW_PSC_S_MV_CHAN_BIX
ON fweek_pscat_sales_mv(channel_id);

CREATE BITMAP INDEX FW_PSC_S_MV_PROMO_BIX   
ON fweek_pscat_sales_mv(promo_id);

CREATE BITMAP INDEX FW_PSC_S_MV_WD_BIX
ON fweek_pscat_sales_mv(week_ending_day);
</code></pre>
<pre><code>spool &amp;data_dir.EXP_query_on_SH2_2.txt

alter session set query_rewrite_integrity = TRUSTED;
alter session set query_rewrite_enabled = TRUE;

set timing on

EXPLAIN PLAN FOR
SELECT   t.week_ending_day
,        p.prod_subcategory
,        sum(s.amount_sold) AS Money
,        s.channel_id
,        s.promo_id
FROM     SH2.sales s
,        SH2.times t
,        SH2.products p
WHERE    s.time_id = t.time_id
AND      s.prod_id = p.prod_id
GROUP BY t.week_ending_day
,        p.prod_subcategory
,        s.channel_id
,        s.promo_id;

REM Now Let us Display the Output of the Explain Plan
SET pagesize 9999
set linesize 250
set markup html preformat on
select * from table(dbms_xplan.display());
set linesize 80

spool off
</code></pre>
<pre><code>----------------------------------------------------------------------------------------------------------------------------------
| Id  | Operation                      | Name                    | Rows  | Bytes |TempSpc| Cost (%CPU)| Time     | Pstart| Pstop |
----------------------------------------------------------------------------------------------------------------------------------
|   0 | SELECT STATEMENT               |                         |  1016K|    60M|       | 17365   (1)| 00:00:01 |       |       |
|   1 |  HASH GROUP BY                 |                         |  1016K|    60M|    70M| 17365   (1)| 00:00:01 |       |       |
|*  2 |   HASH JOIN                    |                         |  1016K|    60M|       |  2178   (1)| 00:00:01 |       |       |
|   3 |    VIEW                        | index$_join$_003        | 10000 |   224K|       |    74   (0)| 00:00:01 |       |       |
|*  4 |     HASH JOIN                  |                         |       |       |       |            |          |       |       |
|   5 |      INDEX FAST FULL SCAN      | PRODUCTS_PK             | 10000 |   224K|       |    41   (0)| 00:00:01 |       |       |
|   6 |      INDEX FAST FULL SCAN      | PRODUCTS_PROD_SUBCAT_IX | 10000 |   224K|       |    51   (0)| 00:00:01 |       |       |
|*  7 |    HASH JOIN                   |                         |  1016K|    37M|       |  2101   (1)| 00:00:01 |       |       |
|   8 |     PART JOIN FILTER CREATE    | :BF0000                 |  1016K|    37M|       |  2101   (1)| 00:00:01 |       |       |
|   9 |      TABLE ACCESS FULL         | TIMES                   |  1461 | 23376 |       |    13   (0)| 00:00:01 |       |       |
|  10 |     PARTITION RANGE JOIN-FILTER|                         |  1016K|    22M|       |  2086   (1)| 00:00:01 |:BF0000|:BF0000|
|  11 |      TABLE ACCESS FULL         | SALES                   |  1016K|    22M|       |  2086   (1)| 00:00:01 |:BF0000|:BF0000|
----------------------------------------------------------------------------------------------------------------------------------
</code></pre>
","<oracle><plsql><oracle11g><data-warehouse>","2021-05-18 11:44:23","143","1","1","67643352","<p>Why would you expect reference to the MV from the baseline query. For that to happen Oracle would have to compare the query to all MVs to find a match. Even further it would require every query be compared to every MV. MVs are typically created to avoid running the baseline query by accessing the MV directly (that is why MVs are the <strong>stored results of a query</strong>). If you want the MV just select from it directly.</p>
<pre><code>SELECT  week_ending_day                                                                                                    
     ,  prod_subcategory                                                                                                   
     ,  Money                                                                                                              
     ,  channel_id                                                                                                         
     ,  promo_id                                                                                                           
  from fweek_pscat_sales_mv;

                                                                                           
</code></pre>
"
"67571299","How to create collision-less unique key in Data Warehouse?","<p>We are integrating tables from a lot of different systems in a relational warehouse, before making it dimensional in a Data Mart. But right now, we are really focusing on the relational warehouse step. We are working with Snowflake Cloud Platform.</p>
<p>Data is comming from different tables and we decided to create a table for localization (translations) in our warehouse. All necessary translations will be in this table. If we only use the given ID for each entities, we might get id collision and non-unique ID.</p>
<p>As an example, we might have Countries and Companies in the same localization table sharing the same id which were unique in their respective tables, but not anymore in this joint table :</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>origin_table</th>
<th>language_id</th>
<th>name</th>
<th>id_in_origin_table</th>
</tr>
</thead>
<tbody>
<tr>
<td>Country</td>
<td>100</td>
<td>Germany</td>
<td>101</td>
</tr>
<tr>
<td>Country</td>
<td>200</td>
<td>Allemagne</td>
<td>101</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr>
<td>Company</td>
<td>100</td>
<td>Mecanic Shop</td>
<td>101</td>
</tr>
<tr>
<td>Company</td>
<td>200</td>
<td>Garage Mécanique</td>
<td>101</td>
</tr>
</tbody>
</table>
</div>
<p>Obviously we can't use <strong>id_in_origin_table</strong> as a unique key, neither can we <strong>CONCAT(language_id, id_in_origin_table)</strong>.</p>
<p>So we thought of adding a column which would try to be unique by hashing the origin table name + the value.</p>
<p>Simplified example : hash('Country', country.id)</p>
<p>We do it both in the origin table and localization table to be able to have a relationship. But as you may know, most hash functions produces alphanumeric values which are less efficient to join than only numeric values.
We tried swaping hex value by their decimal counterpart, then doing an arithmetic operation to shrink the 128 bits CHAR value to 64 bits BIGINT. But we figure there would we quite high risks of collisions.</p>
<p><em>Really important for us : We want the keys to be <strong>numerical</strong> values only.</em></p>
<p>So the questions are:</p>
<ul>
<li>Is there an efficient way to do generate numerical unique keys?</li>
<li>If it implies hashing, which algorithm should be favored?</li>
<li>Is there natively a function in Snowflake that you know of, that could help us in this scenario?</li>
<li>Does anyone have a similar problem, or are we overthinking / missing the whole picture somehow?</li>
</ul>
<p>Thanks a lot! I appreciate you feedback!</p>
","<database><snowflake-cloud-data-platform><data-modeling><data-warehouse>","2021-05-17 14:06:00","262","0","2","67572329","<p>What you describe is not really a <em>surrogate key</em>. A surrogate key should remain unchanged if you alter a dimension attribute (e.g. name) which is not happening in your case.</p>
<p>A classical approach to maintain the <em>surrogate keys</em> is a mapping table that contain the natural key (table_name, lang Id and Id in your case) and the surrogate_id. The table is used for the transformation natural key -&gt; surrogate key via lookup and you insert a record for each new natural key with a sequence assigned surrogate key.</p>
<p><strong>This should be your benchmark</strong>, use an alternative only if you find a solution better than the above one.</p>
<p><em>Natural key hashing</em> (which BTW does not produce a <em>surrogate key</em>, as per definition a surrogate key must be able to remain unchanged if the natural key changes) was propagated by Data Vault. But if you opt for <a href=""https://stackoverflow.com/questions/tagged/data-vault"">Data</a> <a href=""https://en.wikipedia.org/wiki/Data_vault_modeling"" rel=""nofollow noreferrer"">Vault</a> you should not care for such details as the hash format.</p>
<p>The argument, that you get an independence between the key <em>definition</em> in the dimension and the <em>usage</em> in a fact table is valid, but you have to pay a price for it.</p>
"
"67571299","How to create collision-less unique key in Data Warehouse?","<p>We are integrating tables from a lot of different systems in a relational warehouse, before making it dimensional in a Data Mart. But right now, we are really focusing on the relational warehouse step. We are working with Snowflake Cloud Platform.</p>
<p>Data is comming from different tables and we decided to create a table for localization (translations) in our warehouse. All necessary translations will be in this table. If we only use the given ID for each entities, we might get id collision and non-unique ID.</p>
<p>As an example, we might have Countries and Companies in the same localization table sharing the same id which were unique in their respective tables, but not anymore in this joint table :</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>origin_table</th>
<th>language_id</th>
<th>name</th>
<th>id_in_origin_table</th>
</tr>
</thead>
<tbody>
<tr>
<td>Country</td>
<td>100</td>
<td>Germany</td>
<td>101</td>
</tr>
<tr>
<td>Country</td>
<td>200</td>
<td>Allemagne</td>
<td>101</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr>
<td>Company</td>
<td>100</td>
<td>Mecanic Shop</td>
<td>101</td>
</tr>
<tr>
<td>Company</td>
<td>200</td>
<td>Garage Mécanique</td>
<td>101</td>
</tr>
</tbody>
</table>
</div>
<p>Obviously we can't use <strong>id_in_origin_table</strong> as a unique key, neither can we <strong>CONCAT(language_id, id_in_origin_table)</strong>.</p>
<p>So we thought of adding a column which would try to be unique by hashing the origin table name + the value.</p>
<p>Simplified example : hash('Country', country.id)</p>
<p>We do it both in the origin table and localization table to be able to have a relationship. But as you may know, most hash functions produces alphanumeric values which are less efficient to join than only numeric values.
We tried swaping hex value by their decimal counterpart, then doing an arithmetic operation to shrink the 128 bits CHAR value to 64 bits BIGINT. But we figure there would we quite high risks of collisions.</p>
<p><em>Really important for us : We want the keys to be <strong>numerical</strong> values only.</em></p>
<p>So the questions are:</p>
<ul>
<li>Is there an efficient way to do generate numerical unique keys?</li>
<li>If it implies hashing, which algorithm should be favored?</li>
<li>Is there natively a function in Snowflake that you know of, that could help us in this scenario?</li>
<li>Does anyone have a similar problem, or are we overthinking / missing the whole picture somehow?</li>
</ul>
<p>Thanks a lot! I appreciate you feedback!</p>
","<database><snowflake-cloud-data-platform><data-modeling><data-warehouse>","2021-05-17 14:06:00","262","0","2","67573148","<p>Take a look at using sequences.  If you use the same sequence across multiple tables you will have a unique ID.  Depending on the details, this may eliminate the need to create a table to manage your keys.</p>
<p><a href=""https://docs.snowflake.com/en/user-guide/querying-sequences.html"" rel=""nofollow noreferrer"">https://docs.snowflake.com/en/user-guide/querying-sequences.html</a></p>
"
"67535622","""Cursor"" and ""FOR XML"" clause in Azure data warehouse","<p>While creating stored procedures in Azure data warehouse, I have got some error on &quot;Cursor&quot; and &quot;FOR XML&quot;. So wanted to know if they are supported by Azure data warehouse or not. If not then what are the alternatives.</p>
<p>sample code with error msg pictures are attached herein.</p>
<p><a href=""https://i.stack.imgur.com/FX7hI.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FX7hI.jpg"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/c5MTy.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c5MTy.jpg"" alt=""enter image description here"" /></a></p>
","<azure-synapse><azure-sql>","2021-05-14 13:58:55","357","1","1","67535838","<p>Neither <code>FOR XML</code> or cursors are supported in Azure Synapse dedicated SQL pools (formerly known as Azure SQL Data Warehouse) as <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-develop-loops"" rel=""nofollow noreferrer"">per the documentation</a>.  For cursors, either convert them to use a <code>WHILE</code> loop which is supported or refactor the code to use a set-based approach.  Another alternative is to use something external, like Azure Data Factory or Synapse Pipelines and use a For Each loop.  Another alternative is to use a nearby Azure SQL DB to do some pre-processing.  You should be aware the the MPP architecture of Azure Synapse Analytics does not lend itself well to this kind of row-based processing and you should remember it's a big data platform meant for large volumes of data, millions, billions of rows and set-based approaches should be preferred.</p>
<p>If you are just using <code>FOR XML</code> to do that sleazy string concatenation trick then you should use <code>STRING_AGG</code> instead which is fully supported in Synapse.  See <a href=""https://stackoverflow.com/questions/67417173/select-column-values-as-comma-separated-string-in-azure-synapse-sql-server/67417690#67417690"">this answer</a> for a recent example.  If you are actually producing XML then you will need to find an alternative method, eg a nearby Azure SQL DB.</p>
"
"67534845","What should update when an SCD1 change happens on an SCD2 aware table","<p>I am not sure if this is answered in the Kimball Data-Warehouse literature - I could not find an answer.</p>
<p>Lets say you have a table with a couple of SCD2 aware columns, and some SCD1 columns. The question is simply this: Which records are updated when a change to an SCD1 column happens?</p>
<p>Do you change ALL records, the current one and all historic ones? Or do you change only the most recent (open) record? I tend to go by &quot;all&quot;, while all my colleagues insist on &quot;most recent only&quot;.</p>
<p>The only reason I can give for my opinion is, that I want a <code>COUNT(DISTINCT ...)</code> to give the same result when I run it on the business key column, or on an SCD1 column.</p>
<p>Thinking of it, there are two special cases: What if a record does not have a current version, if the most recent one went out of scope and there was no new instance? What if I use my SCD2 logic to prepare records that will become active automatically in the future?</p>
<p>If I only update the most recent open version, than that would mean I do not change anything in the first special case, while in the other case I need to change all future (prepared) records. Thinking of it... I am confident &quot;all&quot; is the correct answer.</p>
<p>Can anyone point to a well known white-paper website (like Kimball, Microsoft) that gives an answer?</p>
","<sql><sql-server><data-warehouse><olap><kimball>","2021-05-14 13:04:52","642","0","3","67535051","<p>All.  The intent of type 1 is that facts are associated with the attribute's current value:</p>
<p><a href=""https://i.stack.imgur.com/SXbuY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SXbuY.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://www.kimballgroup.com/2013/02/design-tip-152-slowly-changing-dimension-types-0-4-5-6-7/"" rel=""nofollow noreferrer"">https://www.kimballgroup.com/2013/02/design-tip-152-slowly-changing-dimension-types-0-4-5-6-7/</a></p>
<p>In a type 2 SCD facts are associated with a &quot;version&quot; of the dimension that was active when the fact occurred.  In order for a fact that's associated with a version to be &quot;associated with the attribute's current value&quot; that current value must be updated on all versions of the dimension.</p>
<p>For instance if you have</p>
<pre><code>SalesFact(Date,CustomerId,Amount)
</code></pre>
<p>and</p>
<pre><code>CustomerDim(CustomerId, CustomerBK, EffectiveDate, EndEffectiveDate, IsCurrent, Name, SalesPersonId)
</code></pre>
<p>And you want to associate historical Sales facts with the customer's <em>current</em> SalesPerson rather than the SalesPerson active at the time of the sale, you must update CustomerDim for all versions of that Customer.  Eg</p>
<pre><code>update CustomerDim set SalesPersonId = 1234 where CustomerBk = 'Acme'
</code></pre>
<p>Or else a query like</p>
<pre><code>select sum(Amount)
from SalesFact s
join CustomerDim c
  on s.CustomerId = c.CustomerId
where c.SalesPersonId = 1234
</code></pre>
<p>won't return the older sales of that SalesPerson's customers.</p>
"
"67534845","What should update when an SCD1 change happens on an SCD2 aware table","<p>I am not sure if this is answered in the Kimball Data-Warehouse literature - I could not find an answer.</p>
<p>Lets say you have a table with a couple of SCD2 aware columns, and some SCD1 columns. The question is simply this: Which records are updated when a change to an SCD1 column happens?</p>
<p>Do you change ALL records, the current one and all historic ones? Or do you change only the most recent (open) record? I tend to go by &quot;all&quot;, while all my colleagues insist on &quot;most recent only&quot;.</p>
<p>The only reason I can give for my opinion is, that I want a <code>COUNT(DISTINCT ...)</code> to give the same result when I run it on the business key column, or on an SCD1 column.</p>
<p>Thinking of it, there are two special cases: What if a record does not have a current version, if the most recent one went out of scope and there was no new instance? What if I use my SCD2 logic to prepare records that will become active automatically in the future?</p>
<p>If I only update the most recent open version, than that would mean I do not change anything in the first special case, while in the other case I need to change all future (prepared) records. Thinking of it... I am confident &quot;all&quot; is the correct answer.</p>
<p>Can anyone point to a well known white-paper website (like Kimball, Microsoft) that gives an answer?</p>
","<sql><sql-server><data-warehouse><olap><kimball>","2021-05-14 13:04:52","642","0","3","67536209","<p>By definition, all rows must be updated when an SCD1 attribute changes.  If you only change the most recent version, it is an SCD2 update.</p>
<p>If you have a large number of version rows, you may consider splitting your SCD1 and SCD2 dimensions into separate tables.  That does add an additional reference from the fact (Type 4) or require snowflaking the new SCD2 dimension to the new SCD1 dimension. Both options address issues with updates to a mixed dimension.</p>
"
"67534845","What should update when an SCD1 change happens on an SCD2 aware table","<p>I am not sure if this is answered in the Kimball Data-Warehouse literature - I could not find an answer.</p>
<p>Lets say you have a table with a couple of SCD2 aware columns, and some SCD1 columns. The question is simply this: Which records are updated when a change to an SCD1 column happens?</p>
<p>Do you change ALL records, the current one and all historic ones? Or do you change only the most recent (open) record? I tend to go by &quot;all&quot;, while all my colleagues insist on &quot;most recent only&quot;.</p>
<p>The only reason I can give for my opinion is, that I want a <code>COUNT(DISTINCT ...)</code> to give the same result when I run it on the business key column, or on an SCD1 column.</p>
<p>Thinking of it, there are two special cases: What if a record does not have a current version, if the most recent one went out of scope and there was no new instance? What if I use my SCD2 logic to prepare records that will become active automatically in the future?</p>
<p>If I only update the most recent open version, than that would mean I do not change anything in the first special case, while in the other case I need to change all future (prepared) records. Thinking of it... I am confident &quot;all&quot; is the correct answer.</p>
<p>Can anyone point to a well known white-paper website (like Kimball, Microsoft) that gives an answer?</p>
","<sql><sql-server><data-warehouse><olap><kimball>","2021-05-14 13:04:52","642","0","3","67561249","<p>The approach is called Slowly Changing Dimensions, not Slowly Changing Attributes. Ignoring the complexity of higher levels, a dimension is either SCD0,1 or 2 - so if any attribute in an SCD2 dimension changes then you create a new record.</p>
<p>Trying to make attributes within a dimension have different SCD levels just gets you into the mess you've described.</p>
"
"67531163","Beginner guide to omics data warehousing","<p>I work in bioscience field, mainly involved in data analysis. Lately, numbers are growing and things are getting more complicated by the use of multiple analysis techniques (most of them &quot;omics&quot; type) on various biological samples from the same set of individuals/patients/animals.</p>
<p>I would like to implement a better way of locally storing data and meta-data (here I refer to meta-data as the general data about the individuals/patients/animals, but also to meta-data about instrument used in assay) which would also allow me to perform meta-analysis (mainly using R, but I would like to have a solution that can also work with SPSS). I am searching for some guides to learn the basics of building, managing and using databases, optimally tailored to biology and &quot;omics&quot;application.</p>
<p>I could summarize my situation in the following image</p>
<p><a href=""https://i.stack.imgur.com/6YgEb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6YgEb.png"" alt=""enter image description here"" /></a></p>
<p>In summary, over the same set of samples (individual - S1 to Sn), that would be the main entry in the database, we could perform a series of experimental assays, each of which resulting in some numeric data generally organized in a csv like format with the same id, accompanied by some meta data about the assay (instrument used and similar). The creation of new entries in the database would usually be via bulk upload of those csv files.</p>
<p>Essentially, I would like to collect and connect everything in one place, instead of having 1 folder for every project, with related R script and raw data. From R, I would then retrieve from the general database the data relevant to a certain project, and perform a set of analyses. As of now, I am interested in a local solution, but I would like to leave the eventual predisposition for remote access open</p>
<p>I have no background in databases, so I am open to any solution which would better fit my needs. For example, I have read that there are relational databases and graph databases (I do have some experience with ontologies) and can't decide which would be better.  Any &quot;digested&quot; source of general information from users who have handled similar issues, any beginner tips, or any suggestion on best solution, would be of great benefit for me to try and start something.</p>
","<r><database><bioinformatics><data-warehouse>","2021-05-14 08:18:29","137","0","1","67694404","<p>Actually, I disagree with the commenters who are criticizing this question, though I agree it is not specific to R or R-related programming. Maybe I just sympathize because I have been in a similar position. A better venue to ask something like this might be BioStars.</p>
<p>That said, I also work in academia, and I also had a similar problem. No one in my circles had a great answer.</p>
<p>From your diagram, it seems like you know something about relational databases, which is good. If you aren't familiar with sql-like syntax or relational database ideas, then definitely start there. I don't have a great suggestion on how to learn about these -- I had a class on mysql in college, and then started using sqlite and postgresql on my own. I very much appreciated the mysql class, so if you feel like you don't know sql-like syntax or relational database topics well, maybe you could find an online course (or take one at your university, if you are located at a school).</p>
<p>Specifically in R, I would start reading about connecting to a database from R/Rstudio</p>
<p><a href=""https://db.rstudio.com/"" rel=""nofollow noreferrer"">https://db.rstudio.com/</a></p>
<p>I use this predominantly through the RPostgresql package, which is an extension of the DBI package, I think.</p>
<p>Obviously get very comfortable with the tidyverse packages, if you aren't already. This is a great resource:</p>
<p><a href=""https://r4ds.had.co.nz/"" rel=""nofollow noreferrer"">https://r4ds.had.co.nz/</a></p>
<p>Since we're on the topic of Hadley Wickham, and since the topic of Hadley Wickam is related to R and R-programming, I don't feel bad saying you should read this, too:</p>
<p><a href=""https://vita.had.co.nz/papers/tidy-data.pdf"" rel=""nofollow noreferrer"">https://vita.had.co.nz/papers/tidy-data.pdf</a></p>
<p>You'll need to learn some basics about servers. I understand that you're specifically interested in doing this locally, but I suspect that there will come a time that you'll need to be able to both host locally and remotely. At least, that has been my experience. In any case, you should be using linux (I hope this isn't too contentious a statement) on your local computer, which means essentially that dealing with a local database is more or less the same as dealing with one remotely (minus some security concerns). I find Nginx easier than Apache, but that is likely a matter of taste. I use Amazon AWS when I need a public server, though if your university has hosting services, you could do a price comparison. AWS has been cheaper and easier in my experience. To manage a served database, I use Django, which is a python package. If you choose to build a django managed database, I suggest using this cookiecutter (a python package template):</p>
<p><a href=""https://github.com/agconti/cookiecutter-django-rest"" rel=""nofollow noreferrer"">https://github.com/agconti/cookiecutter-django-rest</a></p>
<p>Finally, posted below is a link to a django database framework for a currently active project for which I'm managing the data. I'm going to include another link to a R package that I'm also messing around with, which is meant to absorb some of the database data, process it, and spit it out. The latter is very much under development. It isn't in a share-able state, but it would have helped me to see something like this, I think, when I started asking similar questions to yours, so I'm going to include it.</p>
<p><a href=""https://github.com/BrentLab/S288CR64_database"" rel=""nofollow noreferrer"">https://github.com/BrentLab/S288CR64_database</a></p>
<p><a href=""https://github.com/cmatKhan/brentlabRnaSeqTools"" rel=""nofollow noreferrer"">https://github.com/cmatKhan/brentlabRnaSeqTools</a></p>
<p>If you have questions specifically related to genomics data management, feel free to ask via my email. You'll find it on github.</p>
"
"67525320","Cassandra data modelling for group by","<p>I have a data, where columns looks like:</p>
<pre><code>group, item, date, some_metric_column, another_metric_column
</code></pre>
<p>A bit about the columns:</p>
<p>Instead of metric_column substitute clicks/sales/temperature/etc...</p>
<p><code>item</code> belongs to a <code>group</code>. So when <code>item=18 and group=6</code> - then never would be situation when <code>item=18 and group=10</code> (different group for same item). Group may have up to 100K items inside (daily).</p>
<p>date - it's a date when those metrics occurred.</p>
<p>So we have data like:</p>
<pre><code>groupA, itemA1, 2021-05-10, temp=32
groupA, itemA1, 2021-05-11, temp=36
groupA, itemA2, 2021-05-11, temp=37
groupB, itemB1, 2021-05-10, temp=18
....
</code></pre>
<p>The result I'm trying to query from that data is:</p>
<blockquote>
<p>give me all items from specified group between some dates and AVG(temp).  (or SUM or whatever)</p>
</blockquote>
<p>So for dates between May 10 and 11, and group=groupA would have something like:</p>
<pre><code>item,  avg(temp)
itemA1, 34
itemA2, 37
</code></pre>
<p><strong>Question:</strong></p>
<p>How should I organise table primary key, what would be partition key, what clustering,
and how better to query it?</p>
<p>Ideally I want to filter out results and return only subset of it that matches some requirements, like when AVG temp is greater than X. (having clause?)</p>
<p>Thanks in advance.</p>
<p><strong>EDIT1:</strong></p>
<p>The one and single SQL query, that I want to run over this table looks like:</p>
<pre><code>SELECT item, avg(temp) temp, avg(humidity) humidity 
FROM table_name
WHERE group = 8
  and date &gt;= '2021-04-01' 
  and date &lt;= '2021-04-30'
group by item 
having temp &gt; 50
</code></pre>
<p>But CQL, is slightly different with more restrictions on keys composition.</p>
<p>Imagine, the group is like some <strong>organisation_id</strong>, so we want to see all weird sensors from that organisation (it would not be all 100K cause of having clause, but expect to get up to 1K as output).</p>
<p>As I said above, org can handle up to 100K sensors (usually it's about 10-20K but may be 100K or slightly more in some rare cases). And we have to save all those 100K on daily basis.</p>
<p>And of course - that table contains many organisations.</p>
<p><strong>EDIT2:</strong></p>
<p>About cardinality.</p>
<p>There are some more interim relations between group and item.
Exact data struct is:
account -&gt; profile -&gt; group -&gt; subgroup -&gt; item.
My initial example from above columns looks like:</p>
<pre><code>account, item, date, some_metric_column, another_metric_column
</code></pre>
<p>but we can easily use remaining columns to more granularly identify the item (partition it). Then columns in table would looks like:</p>
<pre><code>account, profile, group, subgroup, item, date, some_metric_column, another_metric_column
</code></pre>
<p>But anyway the question is:</p>
<p>how to get the all items from account=X for some specific date range and aggregate some metrics for every item. The range is most recent 60 or 90 days (it's not infinity or too wide range like years of data).</p>
<p>And last one point the data is should be updatable, so I can remove data for account=X for day=Y and insert updated data back again. (so solution to use data type list for metric column is not reasonable)</p>
","<cassandra><data-modeling>","2021-05-13 19:47:25","113","1","1","67526095","<blockquote>
<p>How should I organize table primary key, what would be partition key, what clustering, and how better to query it?</p>
</blockquote>
<p>Without understanding the cardinalities of each column and in the interest of addressing the specific question, I'd build the PK/CK definition like this:</p>
<pre><code>PRIMARY KEY (group,item,date))
WITH CLUSTERING ORDER BY (item asc, date desc);
</code></pre>
<p>After loading the data above, you could run a <code>SELECT</code> like this:</p>
<pre><code>&gt; SELECT item, avg(temp) FROM temps_by_item
  WHERE group='groupA'
  GROUP BY group,item;

 item   | system.avg(temp)
--------+------------------
 itemA1 |               34
 itemA2 |               37

(2 rows)
</code></pre>
<p>Again though, the other main thing to think about, is the cardinalities of <code>group</code> and <code>item</code>, just to make sure that the partitions by <code>group</code> don't get too big.</p>
<p><strong>Edit</strong></p>
<p>Ugh, I just saw this.</p>
<blockquote>
<p>Group may have up to 100K items inside (daily).</p>
</blockquote>
<p>So maybe there's a way to split that up?  Although, if we're only talking about a couple of columns on each, it might be ok as long as the partition sizes stay between 1MB and 10MB.</p>
"
"67488372","Star snd snowflake schema in OLAP systems","<p>I was of the impression that in OLAP , we try to store data in a denormalized fashion to reduce the number of joins and make query processing faster. Normalization that avoids data redundancy was more for OLTP systems.<br />
But then again, 2 of the common modelling approaches (star and snowflake schema) are essentially normalized schemas.<br />
Can you help me connect the dots?</p>
","<database><data-warehouse><snowflake-schema>","2021-05-11 14:03:23","154","0","2","67491911","<p>When people use the term &quot;normalised&quot; they normally mean something that is in, or at least close to, 3rd normal form (3NF).</p>
<p>Unless you mean something significantly different by the term normalised then neither Star or Snowflake schemas are normalised. Why do you think they are normalised?</p>
"
"67488372","Star snd snowflake schema in OLAP systems","<p>I was of the impression that in OLAP , we try to store data in a denormalized fashion to reduce the number of joins and make query processing faster. Normalization that avoids data redundancy was more for OLTP systems.<br />
But then again, 2 of the common modelling approaches (star and snowflake schema) are essentially normalized schemas.<br />
Can you help me connect the dots?</p>
","<database><data-warehouse><snowflake-schema>","2021-05-11 14:03:23","154","0","2","67492055","<p>Actually, that's very perceptive and the vast majority of people accept it. The truth is that a star is partially denormalized - the dimension tables are highly denormalized; they typically come from joining together a lot of related tables into one. A well designed fact table, however, is normalized - Each record is a bunch of values identified by a single, unique, primary key which is composed of the intersection of a set of foreign keys.</p>
<p>Snowflake schemas are, as you surmised, even more normalized. They effectively take the dimension tables and break them into small values that are all joined together when needed. While there are constant arguments over whether this is better or worse than a star, many folks believe that these are inexpensive joins and, depending on your thinking, may be worth it.</p>
<p>Initially, snowflakes were sold as a way of saving disk space because they do take up less room than dimension tables but disk space is rarely an issue nowadays.</p>
<p>I personally prefer a hybrid approach that allows me to build a few levels of dimension table that ultimately can provide referential integrity to both my atomic level data but also to my aggregate fact tables.</p>
"
"67483294","Designing fact table with mix granularities","<p>I am designing a data model for reporting. In the source there are three tables which has to be considered.</p>
<p>Invoice table
Test table
Services table</p>
<p>Test and services are against an invoice. One invoice can have multiple test as well as multiple services.</p>
<p>eg:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">InvoiceId</th>
<th style=""text-align: center;"">TestCount</th>
<th style=""text-align: right;"">ServicCount</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">123</td>
<td style=""text-align: center;"">0</td>
<td style=""text-align: right;"">4</td>
</tr>
<tr>
<td style=""text-align: left;"">435</td>
<td style=""text-align: center;"">2</td>
<td style=""text-align: right;"">5</td>
</tr>
<tr>
<td style=""text-align: left;"">435</td>
<td style=""text-align: center;"">4</td>
<td style=""text-align: right;"">2</td>
</tr>
</tbody>
</table>
</div>
<p>To calculate total revenue total amount column in the invoice table can be referred. But for the net revenue. Sum of the Testcost column in Test table and Servicescost column in service cost has to be calculated.</p>
<p>When designing a fact table for Tests and services. I think it's better to go with two fact tables one for test and one for service as they have different granularities and when reporting create a measure to calculate both and get the total net revenue.</p>
<p>Are there any possible other way to keep both test and service detail in same fact? ie. mix of granularities in one single fact?</p>
<p>Any valuable insight please</p>
<p>Thanks in advance</p>
","<database-design><data-warehouse><business-intelligence><star-schema><star-schema-datawarehouse>","2021-05-11 08:40:07","247","0","1","67485055","<p>Mixing facts of different grains in the same fact table is about as big a mistake as it is possible to make when designing a fact table.</p>
<p>So, no, it is not possible to keep facts with different grains in the same fact table.</p>
"
"67469662","Combining additive and semi-additive facts in a single report","<p>I'm working on a quarterly report. The report should look something like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>col</th>
<th>Calculation</th>
<th>Source table</th>
</tr>
</thead>
<tbody>
<tr>
<td>Start_Balance</td>
<td>Sum at start of time period</td>
<td>Account_balance</td>
</tr>
<tr>
<td>Sell Transactions</td>
<td>Sum of all sell values between the two time periods</td>
<td>Transactions</td>
</tr>
<tr>
<td>Buy Transactions</td>
<td>Sum of all buy values between the two time periods</td>
<td>Transactions</td>
</tr>
<tr>
<td>End Balance</td>
<td>Sum at the end of time period</td>
<td>Account_balance</td>
</tr>
</tbody>
</table>
</div>
<p>so e.g.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Calculation</th>
<th>sum</th>
</tr>
</thead>
<tbody>
<tr>
<td>Start_Balance</td>
<td>1000</td>
</tr>
<tr>
<td>Sell Transactions</td>
<td>500</td>
</tr>
<tr>
<td>Buy Transactions</td>
<td>750</td>
</tr>
<tr>
<td>End Balance</td>
<td>1250</td>
</tr>
</tbody>
</table>
</div>
<p>The problem here is that I'm working with a relational star schema, one of the facts is semi-additive and the other is additive, so they behave differently on the time dimension.</p>
<p>In my case I'm using Cognos analytics, but I think this problem goes for any BI tool. What would be best practice to deal with this issue? I'm certain I can come up with some sql query that combines these two tables into one table which the report reads from, but this doesn't seem like best practice, or is it? Another approach would be to create some measures in the BI tool, I'm not a big fan of this approach because it seems to be least sustainable approach, and I'm unfamiliar with it.</p>
","<data-warehouse><business-intelligence><cognos><fact><kimball>","2021-05-10 11:32:20","161","1","2","67470408","<p>You'll  probably need to setup determinants in the Framework Manager model.  The following does a good job in explaining this:
<a href=""https://www.ibm.com/docs/en/cognos-analytics/11.0.0?topic=concepts-multiple-fact-multiple-grain-queries"" rel=""nofollow noreferrer"">https://www.ibm.com/docs/en/cognos-analytics/11.0.0?topic=concepts-multiple-fact-multiple-grain-queries</a></p>
"
"67469662","Combining additive and semi-additive facts in a single report","<p>I'm working on a quarterly report. The report should look something like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>col</th>
<th>Calculation</th>
<th>Source table</th>
</tr>
</thead>
<tbody>
<tr>
<td>Start_Balance</td>
<td>Sum at start of time period</td>
<td>Account_balance</td>
</tr>
<tr>
<td>Sell Transactions</td>
<td>Sum of all sell values between the two time periods</td>
<td>Transactions</td>
</tr>
<tr>
<td>Buy Transactions</td>
<td>Sum of all buy values between the two time periods</td>
<td>Transactions</td>
</tr>
<tr>
<td>End Balance</td>
<td>Sum at the end of time period</td>
<td>Account_balance</td>
</tr>
</tbody>
</table>
</div>
<p>so e.g.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Calculation</th>
<th>sum</th>
</tr>
</thead>
<tbody>
<tr>
<td>Start_Balance</td>
<td>1000</td>
</tr>
<tr>
<td>Sell Transactions</td>
<td>500</td>
</tr>
<tr>
<td>Buy Transactions</td>
<td>750</td>
</tr>
<tr>
<td>End Balance</td>
<td>1250</td>
</tr>
</tbody>
</table>
</div>
<p>The problem here is that I'm working with a relational star schema, one of the facts is semi-additive and the other is additive, so they behave differently on the time dimension.</p>
<p>In my case I'm using Cognos analytics, but I think this problem goes for any BI tool. What would be best practice to deal with this issue? I'm certain I can come up with some sql query that combines these two tables into one table which the report reads from, but this doesn't seem like best practice, or is it? Another approach would be to create some measures in the BI tool, I'm not a big fan of this approach because it seems to be least sustainable approach, and I'm unfamiliar with it.</p>
","<data-warehouse><business-intelligence><cognos><fact><kimball>","2021-05-10 11:32:20","161","1","2","67504535","<p>For Cognos you can stitch the tables</p>
<p>The technique has to do with how Cognos aggregates</p>
<p>Framework manager joins are typically 1 to n for describing the relationship</p>
<p>A star schema having the fact table in the middle and representing the N
with all of the outer tables describing/grouping the data, representing the 1</p>
<ul>
<li>Fact tables, quantitative data, the stuff you want to sum should be on the many side of the relationship</li>
<li>Descriptive tables, qualitative data, the stuff you want to describe or group by should be on the 1 (instead of the many)</li>
</ul>
<p>To stitch we have multiple tables we want to be facts</p>
<p>Take the common tables that you would use for grouping, like the period (there are probably some others like company, or customer, etc)</p>
<p>Connect each of the fact tables with the common table (aka dimension) like this:</p>
<ul>
<li>Account_balance N  to 1 Company</li>
<li>Account_balance N  to 1 Period</li>
<li>Account_balance N  to 1 Customer</li>
<li>Transactions N to 1 Company</li>
<li>Transactions N to 1 Period</li>
<li>Transactions N to 1 Customer</li>
</ul>
<p>This will cause Cognos to perform a full outer join with a coalesce</p>
<p>Allowing you to handle the fact tables even though they have different levels of granularity</p>
<p>Remember with an outer join you may have to handle nulls and you may need to use the summary filter depending on your reporting needs</p>
<p>You want to include the common tables on your report which might conflict with how you want the report to look</p>
<p>An easy work around is to add them to the layout and then set the property to box type none so the sql behaves you want and the report looks the way you want</p>
"