QuestionId,QuestionTitle,QuestionBody,QuestionTags,Date,ViewCount,Score,NumberOfAnswers,AnswerId,AnswerBody
"76024216","How to track data flow inside a pipelined processor?","<p>Let's say we have a canonical in-order 5 stage processor, thus there will be 4 groups of pipeline registers, which are IF/ID, ID/EX, EX/ME, ME/WB. And I want to create a template as below:</p>
<ol>
<li>instruction (type)</li>
<li>stage i</li>
<li>observation i in stage i (potencially the value in the pipeline register)</li>
<li>stage j (j &gt; i)</li>
<li>observation j in stage j (potencially the value in the pipeline register)</li>
<li>(more if necessary)</li>
</ol>
<p>I want to confirm that for a specific instruction (type), observation i in stage i is always same as observation j in stage j, therefore, we can say there is always data leak between these two registers.</p>
<p>Of course there will be countless instances of such a templace, but also a multitude of them will not satisfy our condition. The problem is what method can we use to eliminate all those unsatisfying ones but keep the satisfying ones, such that we will have a set of &quot;invariants&quot; of data leaks inside such a pipelined processor? I am thining hashing, if it works?</p>
<p>Thank you very much for your help in advance.</p>
<p>Above is a definition I want to establish.</p>
","<hash><hardware><cpu-registers><riscv><processor>","2023-04-15 18:59:30","25","0","1","76024371","<p>I might use careful naming and analysis of the pipeline register fields.</p>
<p>Let's divide the pipeline register field names into two parts: a prefix that identifies the pipeline stage, and a suffix that refers to the meaning and value of the field in that pipeline stage.  It could be your system uses an access description like <code>stageName.fieldName</code> or just a naming convention like <code>stageName_fieldName</code> — where <code>stageName</code> is the prefix and <code>fieldName</code> is the suffix.</p>
<p>The rule then is to use the same name suffix for a naming a field in a later stage, as in an earlier stage where that value of the field will be the same value in both stages.</p>
<p>While also avoiding use of the same name suffix (requiring a new suffix) for a field in a later stage where the name suffix is already in use in an earlier stage though not guaranteed to have the same value in both stages.</p>
<p>Using this approach all unique value can be identified by the suffix name alone, and commonalty between stages by count of stages having any given name suffix.</p>
<p>To apply automation, we would need formulas for identifying the name of a field, such as instruction field n:m, or &quot;if this condition then that formula otherwise other formula&quot;,</p>
<p>The idea of <a href=""https://en.wikipedia.org/wiki/Value_numbering"" rel=""nofollow noreferrer"">value numbering</a>, which is like your hashing idea, might do the job for that.  The idea of value numbering goes to compiler technology for identifying (so as to later eliminate) common subexpressions.</p>
"
"76023141","How data flows to executor in spark job processing","<p>I am running spark on top of hadoop cluster .Hadoop has data blocks on which we need to run spark job. My query is what if the partition is in node2 and executor is in node3. How executor in node3 will process data block of node2. Do the data in the partition in node2 is transferred to node3 .
How internally the spark makes sure executor has partition in the same machine to process?</p>
","<apache-spark>","2023-04-15 15:17:54","17","0","1","76024623","<p>Any HDFS Client can read remote data blocks. They don't need to be &quot;node local&quot; lookups; that's just an optimization that YARN tries to accomplish.</p>
<p>In the Spark UI, you should be able to see what tasks from the execution timeline are local or remote</p>
"
"75984270","parameterize the count of input file rows in azure data flow","<ol>
<li><p>I have input file as csv now i want to generate valid and invalid records as csv with same input file name as output file in azure data flow,</p>
</li>
<li><p>Now i want to get the count of valid and invalid records as parameter value by using azure data factory data flow.</p>
</li>
</ol>
<p>Please suggest the way for both requirements.</p>
","<azure><azure-data-factory>","2023-04-11 09:19:58","42","0","1","75995504","<p>I have csv file in my blob storage named &quot;dbo.Customer.csv&quot;</p>
<p><img src=""https://i.imgur.com/4yaNIyB.png"" alt=""enter image description here"" /></p>
<p>Created dataset with above data in data flow and added that as source in dataflow, added derived column activity to the source and created two new columns named &quot;Validation&quot; with value <code>iif(Id % 2 == 0, 'InValid', 'Valid')</code> and filename with &quot;dbo.Customer.csv&quot;</p>
<p><img src=""https://i.imgur.com/PKDnnNS.png"" alt=""enter image description here"" /></p>
<p>Added window activity to count the validation count and selected <code>Validation</code> column in <code>over</code> tab and created windows column named <code>validation_count</code> with value <code>count(Valdation)</code></p>
<p><img src=""https://i.imgur.com/BOI0pfa.png"" alt=""enter image description here"" /></p>
<p>Added sink to the windows activity selected same dataset which selected to the source and selected file Name option as &quot;Name file as column data&quot; and selected &quot;filename&quot; column:</p>
<p><img src=""https://i.imgur.com/RBJVZ3G.png"" alt=""enter image description here"" /></p>
<p>Created pipeline and performed dataflow activity by selecting created data flow and debug it</p>
<p><img src=""https://i.imgur.com/ON6eLys.png"" alt=""enter image description here"" /></p>
<p>My output after debugging dataflow saved in the blob storage account with file name as inut file name</p>
<p><img src=""https://i.imgur.com/8juPDFv.png"" alt=""enter image description here"" /></p>
"
"75976877","Handling Unstructured Data from Excel in SSIS","<p>I need help with Handling Unstructured Data from Excel. The merged cell data needs to be splitted and set to the respective columns in this case. I have attached the source and target.
The Merged Data need to be unmerged and the data need to be populated to those unmerged columns.
In SSIS when i imported this excel it unmerges and places the data in the first column then leaves the other cell as NULL.</p>
<p>Source: tbl_e1 - <img src=""https://i.stack.imgur.com/DGbtJ.jpg"" alt=""Source"" /><br />
Target: tbl_e2 <img src=""https://i.stack.imgur.com/jQxQz.jpg"" alt=""Target"" /><br />
target to be loaded in SSMS.</p>
<p>Detailed Explanation of Requirement:</p>
<p>My Excel Source which has vertically merged cells on Dept1,Dept2 and horizontally merged cells on row2 and row3 on dept2 column as you can see which has the value Banking. similarly row2 and row3 on value column as you can see which has the value 20000. Note that when i imported this source in ssis I can see that this is how it arranges things,</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th>Name</th>
<th>Dept1</th>
<th>Dept2</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Harish</td>
<td>HR</td>
<td>NULL</td>
<td>10000</td>
</tr>
<tr>
<td>2</td>
<td>Kano</td>
<td>Finance</td>
<td>Banking</td>
<td>20000</td>
</tr>
<tr>
<td>3</td>
<td>Tangiro</td>
<td>HR</td>
<td>NULL</td>
<td>NULL</td>
</tr>
</tbody>
</table>
</div>
<p>I want to populate the unmerged cells with the data which was inside those cells when it was in merged state (ie. Source).</p>
<p>I want the horizontally merged cells Dept1 and Dept2 of ID 1 to have HR as value in Dept1 and Dept2.</p>
<p>For the Vertically merged cells on Dept2 column for ID 2 and ID 3 I want them to have Banking in Dept2 for Both Id2 and Id 3. Similarly for Column Value which has Vertically Merged cells on ID 2 and ID 3 I want them to have 20000 in Value for Both Id2 and Id 3.</p>
<p>I thought of trying this using the window functions which is not working and other logical things with script task.</p>
","<sql-server><tsql><ssis><data-integration>","2023-04-10 11:54:56","49","-3","1","75980576","<p>You need to clean the file before attempting to do anything with it in SSIS, it's much simpler to fix it first.</p>
<p>The following Excel VBA sub will find and replace all merged cells by unmerging them and then replacing them with the same value as the source cell.</p>
<pre><code>Sub UnmergeCellsAndDuplicateValues()
    
    Dim ws As Worksheet
    Dim r As Range
    Dim RangeAddress As String
    Dim CellValue

    For Each ws In Worksheets
        ws.Activate
        For Each r In ws.UsedRange
            If r.MergeCells Then
               RangeAddress = r.MergeArea.Address
               CellValue = r
               r.MergeCells = False
               Range(RangeAddress) = CellValue
            End If
        Next
    Next ws
    
End Sub
</code></pre>
<p>You could probably build a small executable that takes a filename as a parameter, opens the workbook and runs this VBA over it but I've not done that as I rarely use it. However, that would mean you could use this in your SSIS workflow , I suppose it depends on how often this will need to be run and/or will you need to run batches of files etc...</p>
"
"75971638","How to change the column data types of a data flow in Azure Data Factory","<p>I'm running a dataflow activity in ADF which uses a REST API response as its source. The response has integer values for some key values like the following example.</p>
<p><code>&quot;ValuatedBy&quot;: 0,</code></p>
<p>These values are interpreted as Booleans by ADF and displays as follows in the inspect tab for the source.</p>
<ol>
<li>How to change these values to have the correct data type?</li>
<li>Why does this happen?</li>
</ol>
<p><a href=""https://i.stack.imgur.com/OX2eQ.png"" rel=""nofollow noreferrer"">Wrong interpretation of the data types</a></p>
<p>I tried Casting but casting activity won't help as Boolean casted to String displayed as &quot;true&quot; or &quot;false&quot;.</p>
","<azure-data-factory>","2023-04-09 16:24:36","65","0","1","75974150","<ul>
<li>You can change the type of the column manually in source transformation.</li>
<li>Click the <strong>Projection</strong> tab in the source transformation of data flow.</li>
<li>In the column name which contains <strong>ValuatedBy</strong> field, select <em>Define Complex Type</em>.</li>
</ul>
<p><img src=""https://i.imgur.com/0A0tIbr.png"" alt=""img1"" /></p>
<ul>
<li>In dataflow expression builder, change the type of <code>ValuatedBy</code> field from <code>boolean</code> to <code>Integer</code> or any other required type. Then click Save and Finish.</li>
</ul>
<p><img src=""https://user-images.githubusercontent.com/113445679/230813084-7ecbd55c-8562-424e-ac7a-2758e3711301.gif"" alt=""gif1"" /></p>
<ul>
<li>Once changed, ValuatedBy is reflected as Integer type in <em>Inspect</em> tab.
<img src=""https://i.imgur.com/opOVziS.png"" alt=""img2"" /></li>
</ul>
<blockquote>
<ol start=""2"">
<li>Why does this happen?</li>
</ol>
</blockquote>
<p>This happens because ADF automatically infers the data types of the columns in the source based on the first few rows of data. If the first few rows of data contain only 0s and 1s, ADF may infer that the column as a Boolean column.</p>
"
"75961897","How to sync in realtime a legacy system with a new one?","<p>I have a legacy grails based system with a MySQL database. I rewrite incrementally the system in a modern Spring Boot 3 and MongoDB Atlas database. I can't just shutdown the legacy system and replace it and I want to rewrite only the write operations (create, update and delete) in the legacy system. So, I write a synchronization mechanism between the legacy system and the new system. To do this, I write a sync project and I use MongoDB Atlas triggers + functions.</p>
<p>For now, the process is as follow:</p>
<ol>
<li>I call the modern Spring project from the legacy project controller with a REST API call for all write operation (create, update and delete).</li>
<li>I save the new entity in the new project database based on MongoDB Atlas.</li>
<li>A trigger is executed on the entity write and execute a MongoDB Atlas function. This function sends the write event to a sync project.</li>
<li>The sync project receives the write event, transform the new entity into MySQL insert, update or delete executed directly in the legacy database to update it. The sync project keeps also a Link between the legacy entity and the new entity for further calls.</li>
</ol>
<p>With this mechanism I don't need to update any read operations in the legacy system because the legacy database is updated by the sync project. All the system works as expected but I must do a Thread.sleep in the legacy controller in order to wait the MongoDB Atlas trigger + function execution and the legacy database update.</p>
<p>This waiting is not really acceptable but I don't see what I can do to resolve the problem.</p>
<p>Do you have any idea?</p>
","<mongodb><spring-boot><data-synchronization>","2023-04-07 20:58:30","19","0","1","75994127","<p>I have resolved my problem with AOP instead of MongoDB Atlas function.</p>
"
"75954131","Unable to delete a data flow in Azure Data factory in git mode","<p>I have an azure data factory data flow that I have to delete. I have to because in order to publish my latest data factory changes, I need to clear the validations on this data flow. However, I can't delete it nor can I update it to another name or fix the validations. When I save any of those changes, it says it failed to save, or gives the below error:</p>
<pre><code>Failed to save All resources.
Error: {&quot;$id&quot;:&quot;1&quot;,&quot;innerException&quot;:null,&quot;message&quot;:&quot;The path '//dataflow/currentplan.json' does not exist at commit 'b04d8869c860afc6ac801503d7a76cd438a8f91f'\r\nParameter name:
</code></pre>
<p>Yet when I refresh, I see that data flow there and I can click into it and see the flow objects.</p>
<p>Any tips on how I can get around this? I'd try an earlier commit but I can't find a way to check out an earlier one like you would with normal git. I also would just copy my good data flows to another data factory and start there, but I have no idea how to do that. Has anyone encountered this issue with data factory?</p>
<p>Thank you!</p>
<p>Tried deleting as normal, tried editing the json file for the data flow.</p>
","<azure><azure-data-factory><etl>","2023-04-06 22:08:01","26","0","1","75961780","<p>Go the git branch---&gt; dataflow ---&gt; yourfile.json ( delete directly from there )</p>
<p>Now if you refresh datafactory ,you wont see the dataflow anymore and if you continue to work on same branch ,adf_publish also gets updated if you are in collab branch .</p>
"
"75918967","Execute google data flow pipeline on cloud via dataFlowRunner but not use GOOGLE_APPLICATION_CREDENTIALS for security","<p>I have a google data flow pipeline which I run from my local using the dataFlowRunner. However I have the GOOGLE_APPLICATION_CREDENTIALS stored in a file and I export it. I want to avoid storing the credentials for security reasons. Also I have saved the service-account with appropriate IAM roles in my properties file as well. Is there any way we can use Google security Manager? I have a use case to ensure code is credentials free.</p>
","<google-oauth><google-cloud-dataflow><google-secret-manager>","2023-04-03 11:27:52","39","0","1","75919937","<p>If you launch your <code>Dataflow</code> job from your local machine, your have to export the  <code>GOOGLE_APPLICATION_CREDENTIALS</code> env var, unfortunately there is no choice in this case.</p>
<p>However if you launch your job via a DAG orchestrator like <code>Airflow</code> and Cloud Composer no need to pass a SA key file. The authentication is handled with <code>Airflow</code> and the SA used by <code>Cloud Composer</code>.</p>
<p>You can also explore other solutions with <code>Cloud Shell</code> or <code>Cloud Build</code> to launch your job, but I think it's better for the CI CD part to deploy the job and delegate the responsability of job execution to a pipeline orchestration tool like <code>Airflow</code>.</p>
<p>In production environment, you can also use <code>Dataflow Flex Template</code> to standardize the deployment of your <code>Dataflow</code> jobs based on a <code>bucket</code> and a <code>Docker</code> image.</p>
<p>In this case, if you use a tool like <code>Cloud Build</code>, no need to pass a Service Account key file.</p>
<p>You can check this <a href=""https://medium.com/google-cloud/ci-cd-for-dataflow-java-with-flex-templates-and-cloud-build-e3c584b8e564"" rel=""nofollow noreferrer"">article</a> I written, that shows a complete example of CI CD pipeline with <code>Dataflow Flex Template</code>.</p>
"
"75889868","Azure Data Factory data flow not reading JSON object due to null value","<p>I'm working on an Azure Data Factory (ADF) data flow that reads JSON data from a file. However, I'm encountering an issue where the data flow is failing to read JSON objects due to a null value.</p>
<p>Here's a sample JSON data with the null value:</p>
<pre><code>[
    null,
    {
        &quot;Column2&quot;: &quot;KM120&quot;,
        &quot;Mon&quot;: 8,
        &quot;Tue&quot;: 8,
        &quot;Wed&quot;: 8,
        &quot;Thu&quot;: 8,
        &quot;Fri&quot;: 8
    },
    {
        &quot;Column2&quot;: &quot;KM121&quot;,
        &quot;Mon&quot;: 8,
        &quot;Tue&quot;: 8,
        &quot;Wed&quot;: 8,
        &quot;Thu&quot;: 8,
        &quot;Fri&quot;: 8
    }
]
</code></pre>
<p>The error message I'm getting is:</p>
<pre><code>Error occurred when deserializing source JSON file. Check if the data is in valid JSON object format. Error reading JObject from JsonReader. Current JsonReader item is not an object: Null. Path '[0]', line 2, position 5.
</code></pre>
<p>I have checked the JSON file and it is in the correct format. I suspect the issue is with the null value in the first element of the array.</p>
<p>Can anyone suggest how to resolve this issue and make the data flow read the JSON objects even with null values?</p>
<p>Thanks in advance.</p>
<p>I removed the null value from the JSON file and tested the data flow, which resolved the issue. However, I'm looking for a way to automate this process since manually editing each file is time-consuming. I'm exploring the option of using a custom JSON schema in the data flow that can handle null values, but I haven't been able to get it to work so far. I'm still investigating this solution and open to other suggestions.</p>
","<azure-data-lake-gen2><azure-data-factory>","2023-03-30 15:01:54","85","0","1","75896127","<blockquote>
<p>Can anyone suggest how to resolve this issue and make the data flow read the JSON objects even with null values?</p>
</blockquote>
<p>If your Json file is in <code>Azure Blob Storage</code> or <code>ADLS gen 2</code> you can access that file using the rest API ang read the Json object with Null values.</p>
<p>To achieve this follow below procedure:</p>
<ul>
<li>First Go to your storage account and <code>change access level</code> of container to <strong>container</strong> then copy the file path of your Json file.
<img src=""https://user-images.githubusercontent.com/98518545/229049786-1a49c68d-4a1b-43bc-b18b-0176a004d7e4.gif"" alt=""31-1 1"" /></li>
<li>Then create a linked service as rest api and access this file from there <strong>paste the copied url from blob storage as base url and select authentication as Anonymous.</strong><img src=""https://i.imgur.com/WwAwSjS.png"" alt=""enter image description here"" /></li>
<li>Then create data flow and select rest api linked service as source and import the schema.
<img src=""https://user-images.githubusercontent.com/98518545/229053666-de92d7e9-d7af-4d18-9197-a06b9b830d85.gif"" alt=""31-1 2"" /></li>
<li>take select transformation and do mapping ass below:
<img src=""https://i.imgur.com/sHRYKY7.png"" alt=""enter image description here"" /></li>
</ul>
<p><em><strong>Output:</strong></em></p>
<p><img src=""https://i.imgur.com/HoM6Tv7.png"" alt=""enter image description here"" /></p>
<blockquote>
<p><strong>Note: It is possible only if your file is in <code>Azure Blob Storage</code> or <code>ADLS gen 2</code></strong></p>
</blockquote>
"
"75845853","Using parameters and dynamic content in pre-SQL script for Azure Data Factory data flow sink transformation","<p>I have a pipeline parameter called query_sink (type string) it comes from a database and the posible values for the parameter could be</p>
<p><a href=""https://i.stack.imgur.com/0zQoG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0zQoG.png"" alt=""possible values"" /></a></p>
<p>The second record is something like <code>IF EXISTS(...) DELETE FROM table1 WHERE country = 1</code></p>
<p>So, I want to use a dataflow where in the sink transformation use the parameter query_sink.
<a href=""https://i.stack.imgur.com/bSfSB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bSfSB.png"" alt="""" /></a></p>
<p>Then use it in the pre SQL script in the sink transformation, For now I just pass the parameter without changes like this</p>
<p><a href=""https://i.stack.imgur.com/NyKpK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NyKpK.png"" alt="""" /></a></p>
<p>but I have problems when the value in the database is <strong>null</strong></p>
<blockquote>
<p>'sink1': There are no batches in the input script.&quot;</p>
</blockquote>
<p>I'm not sure if the parameter in that case is null or is it as if the parameter didn't receive anything.
What I need is in the case the parameter value is a query(second record of the first image) execut it, but in the other case(first record of the first image) do nothing.</p>
<p>Edit:
the input for the pipeline parameter receive this value</p>
","<azure-data-factory>","2023-03-26 04:25:08","90","0","1","75863532","<blockquote>
<p>What I need is in the case the parameter value is a query(second record of the first image) execut it, but in the other case(first record of the first image) do nothing.</p>
</blockquote>
<p>You can pass the below dynamic content to the dataflow parameter where <code>query</code> is a pipeline parameter.</p>
<pre><code>@if(greater(length(pipeline().parameters.query),4), concat('''',pipeline().parameters.query,''''),concat('''','DECLARE @id AS INT;',''''))
</code></pre>
<p>Give this to a string variable and pass the string to the dataflow parameter like below.</p>
<p><img src=""https://i.imgur.com/LJcwZZN.png"" alt=""enter image description here"" /></p>
<p><strong>This is my source table:</strong></p>
<p><img src=""https://i.imgur.com/S6JX4q1.png"" alt=""enter image description here"" /></p>
<p><strong>target table <code>final_target2</code> before deleting:</strong></p>
<p><img src=""https://i.imgur.com/f7hpZh6.png"" alt=""enter image description here"" /></p>
<p>Target table when query parameter value is <code>delete from final_target2 where country=1;</code>.</p>
<p><img src=""https://i.imgur.com/5PrJCAn.png"" alt=""enter image description here"" /></p>
<p>You can see the records with <code>country=1</code> were deleted and new records from <code>final_source1</code> were inserted above.</p>
<p>Target table when query parameter value is <code>NULL</code>. Here in this case I am Executing a sample query <code>DECLARE @id AS INT;</code> which does nothing to target table.</p>
<p><img src=""https://i.imgur.com/ZnRuu0F.png"" alt=""enter image description here"" /></p>
<p><strong>My pipeline JSON:</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;pipeline1&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Set variable1&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;var&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@if(greater(length(pipeline().parameters.query),4), concat('''',pipeline().parameters.query,''''),concat('''','DECLARE @id AS INT;',''''))&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;Data flow1&quot;,
                &quot;type&quot;: &quot;ExecuteDataFlow&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Set variable1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;policy&quot;: {
                    &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                    &quot;retry&quot;: 0,
                    &quot;retryIntervalInSeconds&quot;: 30,
                    &quot;secureOutput&quot;: false,
                    &quot;secureInput&quot;: false
                },
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;dataflow&quot;: {
                        &quot;referenceName&quot;: &quot;dataflow1&quot;,
                        &quot;type&quot;: &quot;DataFlowReference&quot;,
                        &quot;parameters&quot;: {
                            &quot;query_sink&quot;: {
                                &quot;value&quot;: &quot;@variables('var')&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    },
                    &quot;compute&quot;: {
                        &quot;coreCount&quot;: 8,
                        &quot;computeType&quot;: &quot;General&quot;
                    },
                    &quot;traceLevel&quot;: &quot;Fine&quot;
                }
            }
        ],
        &quot;parameters&quot;: {
            &quot;query&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;defaultValue&quot;: &quot;NULL&quot;
            }
        },
        &quot;variables&quot;: {
            &quot;var&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: [],
        &quot;lastPublishTime&quot;: &quot;2023-03-27T15:35:50Z&quot;
    },
    &quot;type&quot;: &quot;Microsoft.DataFactory/factories/pipelines&quot;
}
</code></pre>
<p><strong>My dataflow JSON:</strong></p>
<pre><code>{
    &quot;name&quot;: &quot;dataflow1&quot;,
    &quot;properties&quot;: {
        &quot;type&quot;: &quot;MappingDataFlow&quot;,
        &quot;typeProperties&quot;: {
            &quot;sources&quot;: [
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;source2_table&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;source1&quot;
                }
            ],
            &quot;sinks&quot;: [
                {
                    &quot;dataset&quot;: {
                        &quot;referenceName&quot;: &quot;final_target&quot;,
                        &quot;type&quot;: &quot;DatasetReference&quot;
                    },
                    &quot;name&quot;: &quot;sink1&quot;
                }
            ],
            &quot;transformations&quot;: [],
            &quot;scriptLines&quot;: [
                &quot;parameters{&quot;,
                &quot;     query_sink as string&quot;,
                &quot;}&quot;,
                &quot;source(output(&quot;,
                &quot;          id as integer,&quot;,
                &quot;          country as integer&quot;,
                &quot;     ),&quot;,
                &quot;     allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     isolationLevel: 'READ_UNCOMMITTED',&quot;,
                &quot;     format: 'table') ~&gt; source1&quot;,
                &quot;source1 sink(allowSchemaDrift: true,&quot;,
                &quot;     validateSchema: false,&quot;,
                &quot;     input(&quot;,
                &quot;          name as string&quot;,
                &quot;     ),&quot;,
                &quot;     deletable:false,&quot;,
                &quot;     insertable:true,&quot;,
                &quot;     updateable:false,&quot;,
                &quot;     upsertable:false,&quot;,
                &quot;     format: 'table',&quot;,
                &quot;     preSQLs:[($query_sink)],&quot;,
                &quot;     skipDuplicateMapInputs: true,&quot;,
                &quot;     skipDuplicateMapOutputs: true,&quot;,
                &quot;     errorHandlingOption: 'stopOnFirstError') ~&gt; sink1&quot;
            ]
        }
    }
}
</code></pre>
"
"75793322","How to Union By name for the dynamic drop of files into blob in Azure data flow","<p>My File 1 has
a,b,c
1,2,3</p>
<p>File 2 has
e
23
I am doing the Wildcard path as &quot;*.txt&quot;</p>
<p>My Expected Result should be :<br />
a,b,c,e<br />
1,2,3,NULL <br />
NULL,NULL,NULL,23</p>
<p>But it came as :<br />
a,b,c<br />
1,2,3<br />
23,NULL,NULL</p>
<p>I have Schema drift is ON. Even though Schema is not drifted.</p>
<p><strong>Scenario added :
I can get 100 files a day or 2 files a day which get dropped on a daily basis in the ADLS .
How to union by name dynamically in dataflow</strong></p>
<p><a href=""https://i.stack.imgur.com/fUE01.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fUE01.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/h3fty.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/h3fty.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/OiuqL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OiuqL.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/dM9hn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dM9hn.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2023-03-20 17:12:30","48","0","1","75797267","<p>Unioning files using wildcard character will union all files by the column position only. Instead, you can use union transformation to achieve your requirement. In Union settings, select <strong>Union by</strong> as <strong>Name</strong>.</p>
<p>I have reproduced this with sample datasets.</p>
<p><strong>Input Dataset- source1:</strong>
<img src=""https://i.imgur.com/FV6Byi1.png"" alt=""enter image description here"" /></p>
<p><strong>Input dataset- source2</strong></p>
<p><img src=""https://i.imgur.com/sZqaZwI.png"" alt=""enter image description here"" /></p>
<ul>
<li>Union transformation settings is given as in below image. Select Union by as Name.
<img src=""https://i.imgur.com/OqSASMR.png"" alt=""enter image description here"" /></li>
</ul>
<p><strong>Output of UNION transformation:</strong>
Output data is as expected
<img src=""https://i.imgur.com/nPgaMTo.png"" alt=""enter image description here"" /></p>
"
"75790744","JSON column does not appear in flatten activity in data flow in Azure Data Factory","<p>Using Azure Data Factory, I've copied data from an open-source API that requires no authorisation (<a href=""https://directory.spineservices.nhs.uk/ORD/2-0-0/organisations/q73"" rel=""nofollow noreferrer"">https://directory.spineservices.nhs.uk/ORD/2-0-0/organisations/q73</a>) and put into into an Azure blob storage account as a JSON file. Now I'm trying to flatten some columns in a data flow but I'm having issues.  In my source, I have a particular column called End (Organisation.Roles.Role.Date.End).  It exists in my source when I preview the data:</p>
<p><a href=""https://i.stack.imgur.com/U9WAN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/U9WAN.png"" alt=""enter image description here"" /></a></p>
<p>One thing I've noticed is that for some reason the column isn't being recognised as a date but the main issue is that in the flatten activity, the column does not exist when trying to set it as an input column for mapping:</p>
<p><a href=""https://i.stack.imgur.com/S9Ub5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S9Ub5.png"" alt=""enter image description here"" /></a></p>
<p>I have set the <code>Unroll by</code> array to <code>Organisation.Roles.Role.Date</code>.</p>
<p>Does anyone have any idea about:</p>
<ol>
<li>Why this column is being read as a string rather than a date in my source activity?</li>
<li>Why this column does not appear in the flatten activity?</li>
</ol>
<p>Any help is appreciated! :) Apologies in advance if I've missed any information- I'm very new to this!</p>
<p>Edit:</p>
<p>If I add the below array to the input mapping:</p>
<p><a href=""https://i.stack.imgur.com/aOqVB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aOqVB.png"" alt=""enter image description here"" /></a></p>
<p>The End column appears in the data preview (but I still can't select it specifically in the input data mapping):</p>
<p><a href=""https://i.stack.imgur.com/To5xY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/To5xY.png"" alt=""enter image description here"" /></a></p>
<p>So it is clearly there!  I just can't map it as a separate column, which is what I want.</p>
","<arrays><json><azure><azure-data-factory>","2023-03-20 13:14:45","49","0","1","75791547","<p>I've figured it out!  A silly mistake of course...</p>
<p>A previous version of the API endpoint I was using did not have an End column included (it was null for that particular OdsCode).  Under the Projection section of the source activity in the data flow, I had imported the schema when only the previous version of the API endpoint was being included in my pipeline.  I didn't re-import the schema when I included the new version of the API endpoint with the new column.  Under the source activity, all I needed to do was hit this button and it auto-magically worked:</p>
<p><a href=""https://i.stack.imgur.com/bSE6Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bSE6Q.png"" alt=""enter image description here"" /></a></p>
"
"75758672","Getting ""Malformed records are detected in schema inference"" when trying to flatten JSON using a data flow in ADF","<p>Using Azure Data Factory, I've copied data from an open-source API that requires no authorisation (<a href=""https://directory.spineservices.nhs.uk/ORD/2-0-0/organisations/rxl"" rel=""nofollow noreferrer"">https://directory.spineservices.nhs.uk/ORD/2-0-0/organisations/rxl</a>) and put into into an Azure blob storage account as a JSON file.  Now I'm trying to flatten some columns in a data flow but I'm having issues with my source activity.  When I try to preview the data, I'm getting this error:</p>
<blockquote>
<p>Malformed records are detected in schema inference. Parse Mode: FAILFAST. It could be because of a wrong selection in document form to parse json file(s). Please try a different 'Document form' (Single document/Document per line/Array of documents) on the json source.</p>
</blockquote>
<p>I have tried changing the 'Document form' option to every option and I'm still getting the same error.</p>
<p>I can preview the data ok when looking at the dataset.  But I can't look at it in the data flow... Any help appreciated!</p>
<p>Thanks.</p>
<p>Here is a sample of the data:</p>
<pre><code>[
    {
        &quot;Organisation&quot;: {
            &quot;Rels&quot;: {
                &quot;Rel&quot;: [
                    {
                        &quot;Date&quot;: [
                            {
                                &quot;Start&quot;: &quot;2020-04-01&quot;,
                                &quot;Type&quot;: &quot;Operational&quot;
                            }
                        ],
                        &quot;id&quot;: &quot;RE5&quot;,
                        &quot;Status&quot;: &quot;Active&quot;,
                        &quot;Target&quot;: {
                            &quot;OrgId&quot;: {
                                &quot;assigningAuthorityName&quot;: &quot;HSCIC&quot;,
                                &quot;extension&quot;: &quot;QE1&quot;,
                                &quot;root&quot;: &quot;2.16.840.1.113883.2.1.3.2.4.18.48&quot;
                            },
                            &quot;PrimaryRoleId&quot;: {
                                &quot;id&quot;: &quot;RO261&quot;,
                                &quot;uniqueRoleId&quot;: 300734
                            }
                        },
                        &quot;uniqueRelId&quot;: 666658
                    },
                    {
                        &quot;Date&quot;: [
                            {
                                &quot;End&quot;: &quot;2020-03-31&quot;,
                                &quot;Start&quot;: &quot;2016-04-01&quot;,
                                &quot;Type&quot;: &quot;Operational&quot;
                            }
                        ],
                        &quot;id&quot;: &quot;RE5&quot;,
                        &quot;Status&quot;: &quot;Inactive&quot;,
                        &quot;Target&quot;: {
                            &quot;OrgId&quot;: {
                                &quot;assigningAuthorityName&quot;: &quot;HSCIC&quot;,
                                &quot;extension&quot;: &quot;Q84&quot;,
                                &quot;root&quot;: &quot;2.16.840.1.113883.2.1.3.2.4.18.48&quot;
                            },
                            &quot;PrimaryRoleId&quot;: {
                                &quot;id&quot;: &quot;RO210&quot;,
                                &quot;uniqueRoleId&quot;: 278955
                            }
                        },
                        &quot;uniqueRelId&quot;: 464825
                    }
...
</code></pre>
","<arrays><json><azure-data-factory>","2023-03-16 15:58:25","83","0","1","75763713","<ul>
<li>I got a similar error when I try to copy the file to blob storage as JSON file and then try to read it in dataflow (while importing projection)</li>
</ul>
<p><img src=""https://i.imgur.com/1W7QHpt.png"" alt=""enter image description here"" /></p>
<ul>
<li>In copy data activity sink settings, using file pattern as <code>Array of objects</code> is causing this error to occur (dataset preview is working fine same as in your case).  Instead, set this to file pattern as <code>Set of Objects</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/OKvB48E.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now reading this file after importing projection would give the following schema using JSON setting as <code>Document per line</code>:</li>
</ul>
<p><img src=""https://i.imgur.com/okWT7U9.png"" alt=""enter image description here"" /></p>
<ul>
<li>The Dataflow's data preview would look as shown in the below image:</li>
</ul>
<p><img src=""https://i.imgur.com/FgnvFms.png"" alt=""enter image description here"" /></p>
<p>NOTE: Instead of copying the response to a JSON file, use REST linked service to access this open-source API to directly read the data. However the data preview would be slightly different from what you see above.</p>
<p><img src=""https://i.imgur.com/QTQXS99.png"" alt=""enter image description here"" /></p>
"
"75749908","How to get the first day of the month in Azure data flow","<p>I have &quot;Date&quot; column with yyyy-MM-dd format .I want to fetch the 1st day i.e,
if date is 2023-05-28 ----&gt; 2023-05-01
I have tried method below
and apply it in</p>
<p>iif(toString(Date)&gt;=substring(toString(currentDate()),1,8)+'01',substring(toString(Date),1,8)+'01',toString(null()))</p>
<p>Its working but not for all dates .I dont know wh.Is there any alternative ?</p>
","<azure><azure-data-factory>","2023-03-15 20:44:30","83","0","1","75752917","<ul>
<li>According to the condition written, you would get null values in every case where the date value belongs to the previous months. The required result would be generated only for dates that belong to current or future dates.</li>
<li>Look at the following example. I have the following data in my date column.</li>
</ul>
<p><img src=""https://i.imgur.com/L8hxY2d.png"" alt=""enter image description here"" /></p>
<ul>
<li>When I use the expression similar to yours, then it gives the result as shown in the below image:</li>
</ul>
<p><img src=""https://i.imgur.com/jzn879J.png"" alt=""enter image description here"" /></p>
<ul>
<li>If you want to convert any date to get <code>yyyy-MM-01</code>, then you can directly use the dynamic content <code>substring(toString(date),1,8)+'01'</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/2faFJFD.png"" alt=""enter image description here"" /></p>
"
"75733994","Azure synpase get output data from lookup activity to data flow source","<p>I have a problem. I got the data from oracle on premise by query in LOOKUP ACTIVITY now I want to get this data into the DATA FLOW source so I can append this data into the my master CSV file in BLOb storage.</p>
<p>please help out this</p>
<p>Azure pipeline get output value data from lookup activity to data flow source</p>
","<azure><azure-devops><azure-functions><azure-data-factory><azure-synapse>","2023-03-14 13:46:07","74","0","2","75734772","<p>You can parameterize based on</p>
<p>@activity('LookupActivityName').output.value</p>
<p>Check this out:</p>
<p><a href=""https://learn.microsoft.com/en-us/answers/questions/1136282/pass-lookup-activity-output-to-dataflow"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/answers/questions/1136282/pass-lookup-activity-output-to-dataflow</a></p>
"
"75733994","Azure synpase get output data from lookup activity to data flow source","<p>I have a problem. I got the data from oracle on premise by query in LOOKUP ACTIVITY now I want to get this data into the DATA FLOW source so I can append this data into the my master CSV file in BLOb storage.</p>
<p>please help out this</p>
<p>Azure pipeline get output value data from lookup activity to data flow source</p>
","<azure><azure-devops><azure-functions><azure-data-factory><azure-synapse>","2023-03-14 13:46:07","74","0","2","75747142","<blockquote>
<p>Azure pipeline get output value data from lookup activity to data flow source.</p>
</blockquote>
<p>If you want to do it with only lookup, first get the column values array using a ForEach. then pass that array as an array parameter to dataflow as suggested by <strong>@Ziya Mert Karakas</strong>.</p>
<p>In Dataflow use a dummy source and add derived column and unfold the parameter array to get it as a column. You can go through this <a href=""https://stackoverflow.com/questions/75621518/azure-data-flow-source-from-output-in-pipeline/75625419#75625419"">SO answer</a> by <strong>@RithwikBojja</strong> to learn more about it.<br>
But this method only works if there is only a single column in your data. If there are multiple columns in it, unfolding multiple column arrays at a time might not be possible in dataflow.</p>
<p>As an alternative you can use the below method which involves a temporary file. <strong>First copy the Oracle data to a temporary Blob file using copy activity</strong>. Now use dataflow after it.</p>
<blockquote>
<p>DATA FLOW source so I can append this data into the my master CSV file in BLOb storage</p>
</blockquote>
<p>But, dataflow will overwrite the sink blob csv file. If you want append to your master csv file, then use two sources first is your master csv file and second one is the temporary blob file. <strong>Do a union transformation on these two by <code>Name</code></strong> like below sample.</p>
<p><strong>My master csv file with some data:</strong></p>
<p><img src=""https://i.imgur.com/6vZc27m.png"" alt=""enter image description here"" /></p>
<p><strong>Temporary file with new data which we will get from copy activity:</strong></p>
<p><img src=""https://i.imgur.com/VvXGFA8.png"" alt=""enter image description here"" /></p>
<p><strong>Union transformation by Name:</strong></p>
<p><img src=""https://i.imgur.com/wbUiwg3.png"" alt=""enter image description here"" /></p>
<p>Then give the same master csv file dataset as sink. Check on output to single file and give the master csv file name here.</p>
<p><img src=""https://i.imgur.com/FQCb8Dm.png"" alt=""enter image description here"" /></p>
<p><strong>Sink result with appended data:</strong></p>
<p><img src=""https://i.imgur.com/4K8ewp3.png"" alt=""enter image description here"" /></p>
<p>After copy activity to temporary file, use the dataflow activity and execute the pipeline to get the data in the master csv file.</p>
"
"75733779","How to do not in ( ) concept in Azure data flow","<p>I have given an expression like</p>
<pre><code>not(in([&quot;FIX&quot;,&quot;ANGELA&quot;,&quot;ZERO&quot;,&quot;$0&quot;,&quot;INSTALLED&quot;,toString(null())],upper(CUSTOMER_PO_NUMBER))in([&quot;FIX&quot;,&quot;ANGELA&quot;,&quot;ZERO&quot;,&quot;$0&quot;,&quot;INSTALLED&quot;,upper(CUSTOMER_PO_NUMBER)))
</code></pre>
<p>in my filter Stream.</p>
<p>Here iam getting wrong no of rows filtered because nulls also been taken out.
My other idea was to do a &quot;Conditional Split&quot; ,in conditional split i mentioned</p>
<pre><code>in([&quot;FIX&quot;,&quot;ANGELA&quot;,&quot;ZERO&quot;,&quot;$0&quot;,&quot;INSTALLED&quot;,toString(null())],upper(CUSTOMER_PO_NUMBER))in([&quot;FIX&quot;,&quot;ANGELA&quot;,&quot;ZERO&quot;,&quot;$0&quot;,&quot;INSTALLED&quot;,upper(CUSTOMER_PO_NUMBER))
</code></pre>
<p>Now when i see no of rows in where the condition is false :Am getting the right no of rows using Conditional Split .</p>
<p>So I thought of including toString(null()) as :</p>
<pre><code>not(in([&quot;FIX&quot;,&quot;ANGELA&quot;,&quot;ZERO&quot;,&quot;$0&quot;,&quot;INSTALLED&quot;,toString(null())],upper(CUSTOMER_PO_NUMBER))in([&quot;FIX&quot;,&quot;ANGELA&quot;,&quot;ZERO&quot;,&quot;$0&quot;,&quot;INSTALLED&quot;,toString(null()),upper(CUSTOMER_PO_NUMBER)))
</code></pre>
<p>in filter .But that giving me 0 rows .</p>
<p>How to get the same output as conditional split False condition in filter ? not(in()) is not working in this scenario.</p>
","<azure><azure-data-factory>","2023-03-14 13:28:15","59","0","1","75741565","<p>I tried to replicate the scenario in my local.</p>
<p><strong>Input used</strong></p>
<p><img src=""https://i.imgur.com/Y2Sug8E.png"" alt=""enter image description here"" /></p>
<p><strong>Dataflow</strong></p>
<p><img src=""https://i.imgur.com/EvAZz5Z.png"" alt=""enter image description here"" /></p>
<p><strong>Expression for filter1</strong></p>
<p><img src=""https://i.imgur.com/IgioVCS.png"" alt=""enter image description here"" /></p>
<p><strong>Code used in filter expression</strong>   <code>in([&quot;FIX&quot;,&quot;ANGELA&quot;,&quot;ZERO&quot;,&quot;$0&quot;,&quot;INSTALLED&quot;],upper(CUSTOMER_PO_NUMBER))||isNull(CUSTOMER_PO_NUMBER)</code></p>
<p><strong>Explanation</strong></p>
<p>Using <code>toString(null())</code> into <code>in()</code> was not checking for null values and returning only matching records except null/empty value. I have removed <code>toString(null())</code> from <code>in()</code> and used  <code>||</code> with <code>isNull()</code> and it gave below result including null records.</p>
<p><strong>Result/Data preview</strong></p>
<p><img src=""https://i.imgur.com/TAC3YEp.png"" alt=""enter image description here"" /></p>
<p>To get not matching records you can use <code>not()</code> as mentioned below.    <code>not(in([&quot;FIX&quot;,&quot;ANGELA&quot;,&quot;ZERO&quot;,&quot;$0&quot;,&quot;INSTALLED&quot;],upper(CUSTOMER_PO_NUMBER))||isNull(CUSTOMER_PO_NUMBER))</code></p>
<p><strong>Result/Data preview</strong></p>
<p><img src=""https://i.imgur.com/bmQDPhv.png"" alt=""enter image description here"" /></p>
"
"75685794","Populate an Intune/MS Endpoint Manager Device Group based on a User Group","<p>How can you create a device group, based on membership in a user group in Azure/Intune/MS Endpoint Manager?</p>
<p>Ideally, there should be a means of using a dynamic group query in Azure/Intune/MS Endpoint Manager to populate a device group based on a user group.  Unfortunately, the fields needed to do this do not appear to be exposed to the group dynamic membership query environment/engine.  I've submitted the feature request to MS, but in the interim was looking for another pathway to solve this need.</p>
","<powershell><azure-active-directory><microsoft-graph-api><mdm><intune>","2023-03-09 14:15:33","91","0","1","75686292","<p>Unfortunately, this functionality does not exist as of yet.  I've submitted the feature request to MS, but in the interim developed a Powershell Script to solve this need, and wanted to share it as I found no posting of such a script anywhere I searched.  It could be tailored for other uses, and could have other functionality added, but for now it's a good start for someone needing a base for this type of need (IE the folks in accounting need App XYZ available on their device with this specific configuration).</p>
<p>I hope this helps others, and feel free to post updated versions with expanded capabilities that you extend from what I'm posting.  Below is also a link to the feature request that I've submitted for this to be available within the dynamic membership query functionality within Azure AD/MS Endpoint Manager/Intune, as upvoting there would be very helpful to get this ultimately solved the right way, natively within Azure/MSEM/Intune.</p>
<p><a href=""https://feedbackportal.microsoft.com/feedback/idea/75f632df-92b7-ed11-a81b-002248519701"" rel=""nofollow noreferrer"">https://feedbackportal.microsoft.com/feedback/idea/75f632df-92b7-ed11-a81b-002248519701</a></p>
<pre><code>#This could be run via automation to update the group membership at an interval to maintain groups    
#Connect to mggraph beta environment with priviledges.  This may work with read access for User and Directory.
Connect-MgGraph -Scopes &quot;User.ReadWrite.All&quot;, &quot;Directory.ReadWrite.All&quot;, &quot;DeviceManagementManagedDevices.PrivilegedOperations.All&quot;
Select-MgProfile -Name &quot;beta&quot;

#Get the User members of this group
#Replace the user group id below with your user group id
$grpMembers = Get-MgGroupMember -GroupId &quot;12345ab1-a1ab-123a-a123-123a4567b890&quot; -All | Where {$_}
$grpUsers = ($grpMembers.AdditionalProperties).userPrincipalName

#Get list of devices for users in group
$uDevices = $grpUsers | ForEach-Object {Get-MgUserRegisteredDevice -UserId $_}

#Get list of personal devices from the full list of devices for the users in group
$pDevices = $uDevices.AdditionalProperties | Where {$_.deviceOwnership -eq &quot;Personal&quot;} 

#There is a bug in what ID is returned or used with different mggraph commands so we need to translate
#the ID returned above to the ID needed for adding device group membership.  
#Fixing this is a breaking change, so will not be fixed until a major version update of MgGraph environment  
#At that time, this step of translating the ID returned will/can be removed

#Translate DeviceId returned from Get-MgUserRegisteredDevice to the DeviceID needed to add devices to device group
$gDevices = $pDevices.deviceId | ForEach-Object {get-mgdevice -Filter &quot;DeviceId eq '$($_)'&quot;}

#Get current device group membership
#Replace the group ID below with your device group ID.
$eDevices = Get-MgGroupMember -GroupId &quot;a123456b-12ab-12a3-abc1-123abcd34efg&quot; -All

If($eDevices -ne $null){ #If the group isn't empty...
#Compare devices from the existing devices in the group with the current devices that should be in the group
$cDevices = Compare-Object -ReferenceObject $eDevices.Id -DifferenceObject $gDevices.Id -IncludeEqual

#Based on comparison flag of results for each object in existing or current devices lists, do nothing, add new devices, or remove non-current devices
$cDevices | ForEach-Object {If ($($_.SideIndicator) -eq &quot;==&quot;) {Write-Host &quot;No change for $($_.InputObject)&quot;}}
#Replace the group ID below with your device group ID.
$cDevices | ForEach-Object {If ($($_.SideIndicator) -eq &quot;=&gt;&quot;) { New-MgGroupMember -GroupId &quot;a123456b-12ab-12a3-abc1-123abcd34efg&quot; -DirectoryObjectId $($_.InputObject); Write-Host &quot;Added $($_.InputObject)&quot;}}
#Replace the group ID below with your device group ID.
$cDevices | ForEach-Object {If ($($_.SideIndicator) -eq &quot;&lt;=&quot;) { Remove-MgGroupMemberByRef -GroupId &quot;a123456b-12ab-12a3-abc1-123abcd34efg&quot; -DirectoryObjectId $($_.InputObject); Write-Host &quot;Removed $($_.InputObject)&quot;}}

} Else {
    #Add all devices for users to the empty group
    #Replace the group ID below with your device group ID.
    $gDevices | ForEach-Object {New-MgGroupMember -GroupId &quot;a123456b-12ab-12a3-abc1-123abcd34efg&quot; -DirectoryObjectId $($_.Id); Write-Host &quot;Added $($_.Id)&quot;}
}
</code></pre>
"
"75619692","QT Android application crashes when Back Button pressed while running MDM in Kiosk Mode","<p>Anyone have any experience with integrating Android Mobile Device Management solutions with their QT Application? I’ve been vetting one vendor but my application freezes every time I hit the Back Button while my application is in Kiosk Mode. I thought the issue was specific to my application but I built a trivial Android QT application from the ground up and sure enough I ran into the same issue. Do you know any solutions? Are their just some Android MDMs that work better with QT than others?</p>
","<android><qt><mdm>","2023-03-02 18:48:49","20","0","1","75669816","<p>By default, clicking Back finishes the activity, and it's up to the MDM what's going further.</p>
<p>I would expect restarting the kiosk mode activity (we do that in our Headwind MDM), but the MDM you're using may behave differently.</p>
<p>I would recommend simply blocking the Back button from finishing the activity by overriding <code>onBackPressed()</code>.</p>
"
"75616772","React Trypescript Axios response data mapping","<p>I'm having a problem with using the response that I get with axios from a simple weather API - specifically, I'm having trouble mapping the response.</p>
<p>The <code>.map()</code> function does not work since my response is an object, I get that. My WeatherApi component returns the response - an object.</p>
<p>I then want to import the WeatherApi component into my App component and then access the data inside it like: <code>weather.location.name</code> and either map everything in advance to variables (like in the interface) or just do it on the fly when rendering the HTML like <code>&lt;h1&gt;{weather.location.name}&lt;/h1&gt;</code>.</p>
<p>If I do this, I either get errors in the app that sometimes weather is undefined or build / type errors that <code>.location</code> does not exist on type <code>WeatherData[]</code> which it does (check below).</p>
<p>How can I map everything nice and tidy to use in my App component?</p>
<p><strong>WeatherApi.ts</strong></p>
<pre><code>const WeatherApi = () =&gt; {
  const [weather, setWeather] = useState&lt;WeatherData[] | null&gt;([])
  
  useEffect(() =&gt; {
    const url:string = 'https://api.weatherapi.com/v1/current.json?key=d7db22f333fc4e6aaf3110311222601&amp;q=London&amp;aqi=no';

    axios
      .get&lt;WeatherData[]&gt;(url)
      .then(res =&gt; {
       setWeather(res.data)
      })
      .catch(err =&gt; {
        console.log(err)
      })
  },[])

  return weather ? weather : null
}

export default WeatherApi
</code></pre>
<p><strong>WeatherData.ts</strong></p>
<pre><code>export interface WeatherData {
    location: Location
    current: Current
  }
  
  export interface Location {
    name: string
    country: string
    localtime_epoch: number
    localtime: string
  }
  
  export interface Current {
    temp_c: number
    is_day: number
    condition: Condition
    feelslike_c: number
  }
  
  export interface Condition {
    text: string
    icon: string
    code: number
  }
</code></pre>
<p><strong>App.tsx</strong></p>
<pre><code>import WeatherApi from '../api/WeatherApi'

const Weather = () =&gt; {
    const weatherData = WeatherApi()

    // map weatherData here like in interface
    // const name = weatherData.location.name
    // const temp = weatherData.current.temp_c
    // etc.

  return (
    &lt;div&gt;
      // or add the data directly here as {weatherData.location.name} etc.
    &lt;/div&gt;
  )
}

export default Weather
</code></pre>
","<javascript><reactjs><typescript>","2023-03-02 14:24:29","28","0","1","75616915","<p>I think since you use a hook into <code>WeatherApi()</code> (the <code>useEffect</code>) you actually need to name it something like <code>useWeatherApi()</code> in order for react to understand it is actually a hook.</p>
<p>And use it like so</p>
<pre><code>const Weather = () =&gt; {
    const weatherData = useWeatherApi()

    // map weatherData here like in interface
    // const name = weatherData.location.name
    // const temp = weatherData.current.temp_c
    // etc.

  return (
    &lt;div&gt;
      // or add the data directly here as {weatherData.location.name} etc.
    &lt;/div&gt;
  )
}
</code></pre>
"
"75606832","Is there a local Graph DB (e.g. Neo4j) for Flutter that optionally syncs with the cloud?","<p>do you have a smart approach to tackle the following challenge? A flutter app wants to model its data in a graph structure. This graph structure includes meta information about objects, text based information and links to files (e.g. images, videos, audios, documents). These files are stored in a seperate file storage (to keep the graph light). This solution should work locally and optionally sync (as fast as possible) with the cloud. So I suppose there had to be some sort of local server that hosts both a graph database and file storage as well as some mechanism that can sync (ideally in the background?) these data with an instance in a cloud (to enable syncronization across devices). How would you go about developing a solution in flutter for this? Please let me know if you need more information.</p>
<p>Thank you and kind regards,
Niklas</p>
<p>I already did my own research on this but don't find a graph db integration for flutter where I can run a local instance that syncs with the cloud. I would expect that my problem is not that unique and there's some frameworks for it. I'd like to understand this better by hearing how you would tackle the problem. Thank you.</p>
","<flutter><graph><cloud><local><data-synchronization>","2023-03-01 17:02:13","51","1","1","75745723","<p>It's a little unclear to me based on your question what problem you are actually trying to solve here, but for my answer I'm going to assume you are looking for some kind of realtime sync that stores locally while offline, and then automatically detects a network connection and syncs again.</p>
<p>There really isn't a native neo4j solution here currently, nor, honestly, do I expect there ever will be.</p>
<p>There are, however similar tools for other types of databases, so I suppose it may happen one day.
<a href=""https://www.mongodb.com/docs/realm/sdk/flutter/"" rel=""nofollow noreferrer"">https://www.mongodb.com/docs/realm/sdk/flutter/</a>
<a href=""https://firebase.google.com/docs/database/flutter/offline-capabilities"" rel=""nofollow noreferrer"">https://firebase.google.com/docs/database/flutter/offline-capabilities</a></p>
<p>My team isn't exactly solving for this exact problem, because we aren't really at a point where we're thinking about offline functionality, so I apologize if this question isn't a perfect match. But what we do is use GraphQL to interface with our Neo4j database. By doing so, we have an opportunity to model our local data however we need to on the client and easily convert to JSON objects to interface with our API in a graph-like structure.</p>
<p>Unfortunately, given the complexities of syncing offline/online in realtime, and especially because neo4j/graphql doesn't really have good subscription functionality yet, you may find yourself having to either roll out some very challenging custom solutions, lean on something like the libraries referenced above, or making compromises on your functionality.</p>
<p>Alternatively, if realtime sync isn't the point, I would argue that your client database really doesn't need to use the same tools as your server db  (that's what apis are for) and that you should consider using sqlite, one of the above db tools, or one of the many other mobile client databases available. You can find many of them here: <a href=""https://pub.dev/packages?q=database"" rel=""nofollow noreferrer"">https://pub.dev/packages?q=database</a></p>
<p>updating to add: Depending on your goals and experience level you may also want to consider the question of if you actually even need a database on the client at all.</p>
"
"75590411","Multiple Users editing the same (DT) table at the same time","<p>Is it possible to have multiple users modifying the same DT table simultaneously (like a google doc / excel ...) ? We have an app with a table where users that are connected to the app need to add comments within the table. But in the case of multiple users that have loaded the page before anyone has added a comment, the last one to add one deletes all previously entered comments. I've done this kind of thing with a MongoDB but for this app we would like to stay inside shiny. Would you know how to circumvent this issue ? If Shiny package exists, even if it's not DT related ?
Thanks a lot, let me know if I can clarify the question !!</p>
","<r><shiny><dt><data-synchronization>","2023-02-28 09:56:56","54","1","1","75592442","<p>This can be done using a global <code>reactiveVal</code> (defined outside of the <code>server</code> function), which is shared among sessions.</p>
<p>The following is based on the example given <a href=""https://github.com/rstudio/DT/pull/480#issue-289146755"" rel=""nofollow noreferrer"">here</a>:</p>
<pre><code>library(shiny)
library(DT)

DF &lt;- reactiveVal(iris) # cross session reactiveVal

ui &lt;- fluidPage(DTOutput('myTable'))

server &lt;- function(input, output, session) {
  observeEvent(input$myTable_cell_edit, {
    info = input$myTable_cell_edit
    str(info)
    i = info$row
    j = info$col + 1  # column index offset by 1
    v = info$value
    tmpDF &lt;- DF()
    tmpDF[i, j] &lt;- DT::coerceValue(v, DF()[i, j])
    DF(tmpDF)
  })
  output$myTable &lt;- renderDT({
    DF()
  }, selection = 'none', rownames = FALSE, editable = TRUE)
}

shinyApp(ui, server)
</code></pre>
<p><em>DT edit in the first session appears in the second and vice versa:</em>
<a href=""https://i.stack.imgur.com/c81MH.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c81MH.gif"" alt=""result"" /></a></p>
"
"75582602","Using Apple MDM, I can't get ByPass Activation Code","<p>Is it possible to create Bypass Code for macOS 10.15+ devices without DEP? The machine is on macOS 10.15.7. The enrollment is a self-installed MDM profile.</p>
<p>I prefer to get the ByPass Code with the help of Apple MDM.</p>
<p>Note, when I upgraded the macOS to 13.2 Ventura, this command ActivationLockBypassCodeCommand can get back the bypass code. But on the page <a href=""https://developer.apple.com/documentation/devicemanagement/get_the_bypass_code_for_activation_lock"" rel=""nofollow noreferrer"">get_the_bypass_code_for_activation_lock</a>, it mentioned the minimum supported version is macOS 10.15+</p>
<ul>
<li><p><a href=""https://developer.apple.com/documentation/devicemanagement/activation_lock_a_device"" rel=""nofollow noreferrer"">activation_lock_a_device</a></p>
</li>
<li><p><a href=""https://developer.apple.com/documentation/devicemanagement/device_assignment/activation_lock_a_device/creating_and_using_bypass_codes"" rel=""nofollow noreferrer"">creating_and_using_bypass_codes</a></p>
</li>
<li><p><a href=""https://developer.apple.com/documentation/devicemanagement/get_the_bypass_code_for_activation_lock"" rel=""nofollow noreferrer"">get_the_bypass_code_for_activation_lock</a></p>
</li>
</ul>
","<mdm><apple-developer>","2023-02-27 15:46:21","44","2","1","75860610","<p>Requesting that code from a device requires that it’s supervised, so your description appears to be an expected response. Supervision (as a title) for computers was introduced in macOS 10.15, but in that release only applied to computers that were enrolled via Automated Device Enrollment. In macOS 11+ that all expanded, where any user-approved MDM enrollment was considered supervised, which would explain how it worked after upgrading.</p>
"
"75558596","How to use data flow activity and derived column transformation to do column mapping in ADF and load it in SQL Server table","<p>I have fixed length .DAT files in a ftp server and I need to use data flow activity and derived column transformation to do column mapping using ADF to be able to transform data before loading into a SQL Server table.</p>
<ol>
<li>There is no delimiter in the file.</li>
<li>The target table in SQL server has columns same as mapping photo below.</li>
<li>Need to do column mapping on dataset created from fixed length .DAT file for each line and land it in target SQL server table.</li>
</ol>
<p>I need a solution where without any delimiter specified I am able to convert .DAT file to a dataset and use derived column transformation and then take substring for every column to do column mapping in a similar way as shown below:[![Column Mapping][1]][1]</p>
<p>I would need to update the dataset daily because new file would be added everyday to the ftp server.</p>
<p>Any help/snapshots are appreciate. Thank you.</p>
","<azure><mapping><azure-data-factory>","2023-02-24 15:48:49","95","0","1","75561004","<p>Fixed width format is not natively supported as a Dataset. In order to process it, you'll need to parse the rows first. Here is a rough outline:</p>
<ol>
<li>Create a Dataset with no schema and no delimiter. This will read in each row as a single column named &quot;Prop_0&quot;.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/uBoQs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uBoQs.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>Use Derived Column and substring to parse the columns out of the row. When you name the columns, name them the same as the target SQL columns.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/yRjZ3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yRjZ3.png"" alt=""enter image description here"" /></a></p>
<ol start=""3"">
<li><p>Perform any additional operations like trims (recommended) or type conversions.</p>
</li>
<li><p>Write the data to your Sink.</p>
</li>
</ol>
<p>If the data file is really large, you might want to save the data to an intermediate location, like Parquet files, and then perform the write in another Dataflow. This should make the write to SQL perform better because it will be able to parallelize. It also gives you the opportunity to validate the converted data prior to writing it to SQL.</p>
"
"75540958","Jitterbit real-time data integration","<p>We use Jitterbit Studio and currently we do batch data movement from salesforce to Epicor and other way also. These data syncs are scheduled overnight, for this purpose native jitterbit salesforce connector and rest api's on Epicor side are leveraged.</p>
<p>For a new business requirement, we are looking to see if real-time data sync mechanism can be established. Within salesforce i am aware it has a outbound message mechanism where it can call an external web endpoint and pass data as xml (soap).</p>
<p>For other systems that might have a publish/ subscribe kind of system where jitterbit is a subscriber, does jitterbit have any feature that allows it to become an event subscriber for an external system publisher.</p>
<p>We use Jitterbit Studio to craft our integrations which is a non-cloud version within their suite of integration solutions and within this landscape i have researched within the jitterbit community with no answers hence coming here to stackoverflow as a last resort.</p>
","<jitterbit>","2023-02-23 05:38:44","19","0","1","75546472","<p>Jitterbit absolutely supports real-time data flow. The most common ways to handle this are through APIs and SOAP web services. Jitterbit can receive an API call (e.g., a webhook notification) or a SOAP call and run an operation using the data received. There are potentially other options, depending on exactly what you need, such as having the &quot;message&quot; saved as a file in a folder and having Jitterbit check that folder for new files as often as every minute (not truly real-time, but close enough).</p>
"
"75458856","How to perform mTLS in a flutter or iOS application","<p>I have an enterprise Flutter application that needs to launch the login page for the user's Identity Provider (IdP) inside a webview within the app. The IdP supports <a href=""https://www.pingidentity.com/en/resources/identity-fundamentals/authentication/certificate-authentication.html"" rel=""nofollow noreferrer"">certificate-based authentication</a> using a certificate present on the user's device (through MDM) to authenticate the user without needing to provide any credentials.</p>
<p>When launching a safari browser to launch this page, it works fine. Safari prompts the user to select a certificate the first time from the ones available and Safari sends it to the server and page successfully proceeds to present the protected resource.</p>
<p><a href=""https://i.stack.imgur.com/sB7jO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sB7jO.png"" alt=""enter image description here"" /></a></p>
<p>Another requirement is that we need to open the IdP page using a specific user-agent string so that their firewall can be configured to only allow selected apps and not any random Safari page.</p>
<p>Unfortunately, the Safari In-App Browser does not allow overriding user-agent. While using an in-app web view using <a href=""https://pub.dev/packages/flutter_inappwebview"" rel=""nofollow noreferrer"">https://pub.dev/packages/flutter_inappwebview</a> or <a href=""https://pub.dev/packages/webview_flutter"" rel=""nofollow noreferrer"">https://pub.dev/packages/webview_flutter</a> we are unable to get the certificate prompt like the one we see in safari and the communication fails with the following SSL error, with code -1200</p>
<blockquote>
<p>An SSL error has occurred and a secure connection to the server cannot be made</p>
</blockquote>
<p>When using InAppWebView from <a href=""https://pub.dev/packages/flutter_inappwebview"" rel=""nofollow noreferrer"">https://pub.dev/packages/flutter_inappwebview</a> and accessing the protected site, the <code>onReceivedClientCertRequest</code> handler gets invoked, but I am not sure how to pass the device certificate back in the response. The <code>ClientCertResponse</code> expects a path to the certificate, what should this be?</p>
<p>Is there a way to retrieve the appropriate certificate from the OS's secure storage (keychain?) and send it to the web page? Either Flutter or native iOS code is ok. We are only focusing on iOS for this use case.</p>
","<ios><flutter><client-certificates><mdm><mtls>","2023-02-15 11:05:34","232","1","2","75525844","<p>i don't have a good solution/workaround for your problem but at least i can link you what (i guess) the root problem is:</p>
<p><a href=""https://developer.apple.com/library/archive/qa/qa1745/_index.html"" rel=""nofollow noreferrer"">https://developer.apple.com/library/archive/qa/qa1745/_index.html</a> :</p>
<blockquote>
<p>Apps can only access keychain items in their own keychain access groups. This means that items in the Apple access group are only available to Apple-provided apps such as Safari or Mail.</p>
</blockquote>
<p>With that in mind you might be able to</p>
<ol>
<li>get your certificate into your app's 'keychain access group'</li>
<li>use something like/ similar to <a href=""https://pub.dev/packages/flutter_keychain"" rel=""nofollow noreferrer"">https://pub.dev/packages/flutter_keychain</a> to <code>.get()</code> the certificate</li>
<li>craft your http request using flutters http (no web-view required)</li>
</ol>
"
"75458856","How to perform mTLS in a flutter or iOS application","<p>I have an enterprise Flutter application that needs to launch the login page for the user's Identity Provider (IdP) inside a webview within the app. The IdP supports <a href=""https://www.pingidentity.com/en/resources/identity-fundamentals/authentication/certificate-authentication.html"" rel=""nofollow noreferrer"">certificate-based authentication</a> using a certificate present on the user's device (through MDM) to authenticate the user without needing to provide any credentials.</p>
<p>When launching a safari browser to launch this page, it works fine. Safari prompts the user to select a certificate the first time from the ones available and Safari sends it to the server and page successfully proceeds to present the protected resource.</p>
<p><a href=""https://i.stack.imgur.com/sB7jO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sB7jO.png"" alt=""enter image description here"" /></a></p>
<p>Another requirement is that we need to open the IdP page using a specific user-agent string so that their firewall can be configured to only allow selected apps and not any random Safari page.</p>
<p>Unfortunately, the Safari In-App Browser does not allow overriding user-agent. While using an in-app web view using <a href=""https://pub.dev/packages/flutter_inappwebview"" rel=""nofollow noreferrer"">https://pub.dev/packages/flutter_inappwebview</a> or <a href=""https://pub.dev/packages/webview_flutter"" rel=""nofollow noreferrer"">https://pub.dev/packages/webview_flutter</a> we are unable to get the certificate prompt like the one we see in safari and the communication fails with the following SSL error, with code -1200</p>
<blockquote>
<p>An SSL error has occurred and a secure connection to the server cannot be made</p>
</blockquote>
<p>When using InAppWebView from <a href=""https://pub.dev/packages/flutter_inappwebview"" rel=""nofollow noreferrer"">https://pub.dev/packages/flutter_inappwebview</a> and accessing the protected site, the <code>onReceivedClientCertRequest</code> handler gets invoked, but I am not sure how to pass the device certificate back in the response. The <code>ClientCertResponse</code> expects a path to the certificate, what should this be?</p>
<p>Is there a way to retrieve the appropriate certificate from the OS's secure storage (keychain?) and send it to the web page? Either Flutter or native iOS code is ok. We are only focusing on iOS for this use case.</p>
","<ios><flutter><client-certificates><mdm><mtls>","2023-02-15 11:05:34","232","1","2","75556663","<p>in case of ...</p>
<p>I wrote a flutter PoC that can connect to a HTTPS Server with mtls self-signed certificates on <a href=""https://github.com/ysimonx/mtls-ssl-generator"" rel=""nofollow noreferrer"">https://github.com/ysimonx/mtls-ssl-generator</a></p>
<p>As my PoC is using Flutter'Dio package, you should be able to force the user-agent .</p>
<p>And may be,  you won't need a webview anymore :)</p>
"
"75446597","Understanding streamlit data flow and how to submit form in a sequential way","<p>Below is a simple reproducible example that works to illustrate the problem in its simple form. You can jump to the code and expected behaviour as the problem description can be long.</p>
<h2>The main concept</h2>
<p>There are 3 dataframes stored in a list, and a form on the sidebar shows the <code>supplier_name</code> and <code>po_number</code> from the relevant dataframe. When the user clicks the <code>Next</code> button, the information inside the <code>supplier_name</code> and <code>po_number</code> <strong>text_input</strong> will be saved (in this example, they basically got printed out on top of the sidebar).<br />
<a href=""https://i.stack.imgur.com/4x3oP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4x3oP.png"" alt=""enter image description here"" /></a></p>
<h2>Problem</h2>
<p>This app works well when the user don't change anything inside the <strong>text_input</strong>, but if the user changes something, it breaks the app. See below pic for example, when I change the <code>po_number</code> to <code>somethingrandom</code>, the saved information is not <code>somethingrandom</code> but <code>p123</code> from the first dataframe.</p>
<p><a href=""https://i.stack.imgur.com/qpl90.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qpl90.png"" alt=""enter image description here"" /></a></p>
<p>What's more, if the information from the next dataframe is the same as the first dataframe, the changed value inside the text_input will be unchanged for the next display. For example, because the first and second dataframe's supplier name are both <code>S1</code>, if I change the supplier name to <code>S10</code>, then click next, the <code>supplier_name</code> is still <code>S10</code> on the second dataframe, while the second dataframe's supplier_name should be <code>S1</code>. But if the supplier name for the next dataframe changed, the information inside the <code>text_input</code> will be changed.</p>
<h2>Justification</h2>
<p>If you are struggling to understand why I want to do this, the original use for this is for the sidebar input area to extract information from each PDFs, then when the user confirms the information are all correct, they click next to review the next PDF. But if something is wrong, they can change the information inside the text_input, then click next, and the information of the changed value will be recorded, and for the next pdf, the extracted information should reflect on what the next pdf is. I did this in R shiny quite simply, but can't figure out how the data flow works here in streamlit, please help.</p>
<h2>Reproducible Example</h2>
<pre><code>import streamlit as st
import pandas as pd

# 3 dataframes that are stored in a list
data1 = {
    &quot;supplier_name&quot;: [&quot;S1&quot;],
    &quot;po_number&quot;: [&quot;P123&quot;],
}
data2 = {
    &quot;supplier_name&quot;: [&quot;S1&quot;],
    &quot;po_number&quot;: [&quot;P124&quot;],
}
data3 = {
    &quot;supplier_name&quot;: [&quot;S2&quot;],
    &quot;po_number&quot;: [&quot;P125&quot;],
}
df1 = pd.DataFrame(data1)
df2 = pd.DataFrame(data2)
df3 = pd.DataFrame(data3)

list1 = [df1, df2, df3]

# initiate a page session state, every time next button is clicked
# it will go to the next dataframe in the list
if 'page' not in st.session_state:
    st.session_state.page = 0

def next_page():
    st.sidebar.write(f&quot;Submitted! supplier_name: {supplier_name} po_number: {po_number}&quot;)
    st.session_state.page += 1

supplier_name_value = list1[st.session_state.page][&quot;supplier_name&quot;][0]
po_number_value = list1[st.session_state.page][&quot;po_number&quot;][0]

# main area
list1[st.session_state.page]

# sidebar form

with st.sidebar.form(&quot;form&quot;):
   supplier_name = st.text_input(label=&quot;Supplier Name&quot;, value=supplier_name_value)
   po_number = st.text_input(label=&quot;PO Number&quot;, value=po_number_value)
   next_button = st.form_submit_button(&quot;Next&quot;, on_click=next_page)
</code></pre>
<h2>Expected behaviour</h2>
<p>The dataframe's info are extracted into the sidebar input area. The user can change the input if they wish, then click next, and the values inside the input areas will be saved. When it goes to the next dataframe, the values inside the text input will be refreshed to extract from the next dataframe, and repeats.</p>
","<python><streamlit>","2023-02-14 10:34:41","256","1","2","75467150","<p>I'm not sure I quite understand what you're trying to accomplish, but it seems like you're never updating the supplier name in <code>list1</code> after the user updates the name via the text input widget.</p>
"
"75446597","Understanding streamlit data flow and how to submit form in a sequential way","<p>Below is a simple reproducible example that works to illustrate the problem in its simple form. You can jump to the code and expected behaviour as the problem description can be long.</p>
<h2>The main concept</h2>
<p>There are 3 dataframes stored in a list, and a form on the sidebar shows the <code>supplier_name</code> and <code>po_number</code> from the relevant dataframe. When the user clicks the <code>Next</code> button, the information inside the <code>supplier_name</code> and <code>po_number</code> <strong>text_input</strong> will be saved (in this example, they basically got printed out on top of the sidebar).<br />
<a href=""https://i.stack.imgur.com/4x3oP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4x3oP.png"" alt=""enter image description here"" /></a></p>
<h2>Problem</h2>
<p>This app works well when the user don't change anything inside the <strong>text_input</strong>, but if the user changes something, it breaks the app. See below pic for example, when I change the <code>po_number</code> to <code>somethingrandom</code>, the saved information is not <code>somethingrandom</code> but <code>p123</code> from the first dataframe.</p>
<p><a href=""https://i.stack.imgur.com/qpl90.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qpl90.png"" alt=""enter image description here"" /></a></p>
<p>What's more, if the information from the next dataframe is the same as the first dataframe, the changed value inside the text_input will be unchanged for the next display. For example, because the first and second dataframe's supplier name are both <code>S1</code>, if I change the supplier name to <code>S10</code>, then click next, the <code>supplier_name</code> is still <code>S10</code> on the second dataframe, while the second dataframe's supplier_name should be <code>S1</code>. But if the supplier name for the next dataframe changed, the information inside the <code>text_input</code> will be changed.</p>
<h2>Justification</h2>
<p>If you are struggling to understand why I want to do this, the original use for this is for the sidebar input area to extract information from each PDFs, then when the user confirms the information are all correct, they click next to review the next PDF. But if something is wrong, they can change the information inside the text_input, then click next, and the information of the changed value will be recorded, and for the next pdf, the extracted information should reflect on what the next pdf is. I did this in R shiny quite simply, but can't figure out how the data flow works here in streamlit, please help.</p>
<h2>Reproducible Example</h2>
<pre><code>import streamlit as st
import pandas as pd

# 3 dataframes that are stored in a list
data1 = {
    &quot;supplier_name&quot;: [&quot;S1&quot;],
    &quot;po_number&quot;: [&quot;P123&quot;],
}
data2 = {
    &quot;supplier_name&quot;: [&quot;S1&quot;],
    &quot;po_number&quot;: [&quot;P124&quot;],
}
data3 = {
    &quot;supplier_name&quot;: [&quot;S2&quot;],
    &quot;po_number&quot;: [&quot;P125&quot;],
}
df1 = pd.DataFrame(data1)
df2 = pd.DataFrame(data2)
df3 = pd.DataFrame(data3)

list1 = [df1, df2, df3]

# initiate a page session state, every time next button is clicked
# it will go to the next dataframe in the list
if 'page' not in st.session_state:
    st.session_state.page = 0

def next_page():
    st.sidebar.write(f&quot;Submitted! supplier_name: {supplier_name} po_number: {po_number}&quot;)
    st.session_state.page += 1

supplier_name_value = list1[st.session_state.page][&quot;supplier_name&quot;][0]
po_number_value = list1[st.session_state.page][&quot;po_number&quot;][0]

# main area
list1[st.session_state.page]

# sidebar form

with st.sidebar.form(&quot;form&quot;):
   supplier_name = st.text_input(label=&quot;Supplier Name&quot;, value=supplier_name_value)
   po_number = st.text_input(label=&quot;PO Number&quot;, value=po_number_value)
   next_button = st.form_submit_button(&quot;Next&quot;, on_click=next_page)
</code></pre>
<h2>Expected behaviour</h2>
<p>The dataframe's info are extracted into the sidebar input area. The user can change the input if they wish, then click next, and the values inside the input areas will be saved. When it goes to the next dataframe, the values inside the text input will be refreshed to extract from the next dataframe, and repeats.</p>
","<python><streamlit>","2023-02-14 10:34:41","256","1","2","75467448","<p>I'm not totally sure what you're going for, but after some messing around, the only way I was able to achieve this sort of sequential form submission handling is with <a href=""https://docs.streamlit.io/library/api-reference/control-flow/st.experimental_rerun"" rel=""nofollow noreferrer""><code>st.experimental_rerun()</code></a>. I hate to resort to that since it may be removed any time, so hopefully there's a better way.</p>
<p>Without <code>experimental_rerun()</code>, forms take two submits to actually update state. I wasn't able to find a &quot;correct&quot; way to achieve an immediate update to support the expected behavior.</p>
<p>Here's my attempt:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd  # 1.5.1
import streamlit as st  # 1.18.1


def initialize_state():
    data = [
        {
            &quot;supplier_name&quot;: [&quot;S1&quot;],
            &quot;po_number&quot;: [&quot;P123&quot;],
        },
        {
            &quot;supplier_name&quot;: [&quot;S1&quot;],
            &quot;po_number&quot;: [&quot;P124&quot;],
        },
        {
            &quot;supplier_name&quot;: [&quot;S2&quot;],
            &quot;po_number&quot;: [&quot;P125&quot;],
        },
    ]
    state.dfs = state.get(&quot;dfs&quot;, [pd.DataFrame(x) for x in data])
    first_vals = [{x: df[x][0] for x in df.columns} for df in state.dfs]
    state.selections = state.get(&quot;selections&quot;, first_vals)
    state.pages_expanded = state.get(&quot;pages_expanded&quot;, 0)
    state.current_page = state.get(&quot;current_page&quot;, 0)
    state.just_modified_page = state.get(&quot;just_modified_page&quot;, -1)


def handle_submit(i):
    st.session_state.selections[i] = {
        &quot;supplier_name&quot;: state.new_supplier_name,
        &quot;po_number&quot;: state.new_po_number,
    }
    state.current_page = i
    state.just_modified_page = i

    if i &lt; len(state.dfs) - 1 and state.pages_expanded == i:
        state.pages_expanded += 1

    st.experimental_rerun()


def render_form(i):
    with st.sidebar.form(key=f&quot;form-{i}&quot;):
        supplier_name = state.selections[i][&quot;supplier_name&quot;]
        po_number = state.selections[i][&quot;po_number&quot;]

        if i == state.just_modified_page:
            st.sidebar.write(
                f&quot;Submitted! supplier_name: {supplier_name} &quot;
                f&quot;po_number: {po_number}&quot;
            )
            state.just_modified_page = -1

        state.new_supplier_name = st.text_input(
            label=&quot;Supplier Name&quot;,
            value=supplier_name,
        )
        state.new_po_number = st.text_input(
            label=&quot;PO Number&quot;,
            value=po_number,
        )

        if st.form_submit_button(&quot;Next&quot;):
            handle_submit(i)


state = st.session_state
initialize_state()

for i in range(state.pages_expanded + 1):
    render_form(i)

# debug
st.write(&quot;state.pages_expanded&quot;, state.pages_expanded)
st.write(&quot;state.current_page&quot;, state.current_page)
st.write(&quot;state.just_modified_page&quot;, state.just_modified_page)
st.write(&quot;state.dfs[state.current_page]&quot;, state.dfs[state.current_page])
st.write(&quot;state.selections&quot;, state.selections)
</code></pre>
<p>I'm assuming you want to keep track of the user's selections, but not actually modify the dataframes. If you do want to modify the dataframes, that's simpler: replace <code>state.selections</code> with actual writes to <code>dfs</code> by index and column:</p>
<pre class=""lang-py prettyprint-override""><code># ...
def handle_submit(i):
    st.session_state.dfs[i][&quot;supplier_name&quot;] = state.new_supplier_name,
    st.session_state.dfs[i][&quot;po_number&quot;] = state.new_po_number,
    #st.session_state.selections[i] = {
    #    &quot;supplier_name&quot;: state.new_supplier_name,
    #    &quot;po_number&quot;: state.new_po_number,
    #}

# ...

def render_form(i):
    with st.sidebar.form(key=f&quot;form-{i}&quot;):
        supplier_name = state.dfs[i][&quot;supplier_name&quot;][0]
        po_number = state.dfs[i][&quot;po_number&quot;][0]
        #supplier_name = state.selections[i][&quot;supplier_name&quot;]
        #po_number = state.selections[i][&quot;po_number&quot;]
# ...
</code></pre>
<p>Now, it's possible to make this 100% dynamic, but I hardcoded <code>supplier_name</code> and <code>po_number</code> to avoid premature generalization that you may not need. If you do want to generalize, use <code>df.columns</code> like <code>initialize_state</code> does throughout the code.</p>
"
"75406850","How to modify dynamic complex data type fields in azure data factory data flow","<p>I have a complex data type (fraudData) that undesirably has hyphen characters in the field names I need to remove or change the hypens to some other character.</p>
<p>The input schema of the complex object looks like:
<a href=""https://i.stack.imgur.com/Zoz2r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Zoz2r.png"" alt=""enter image description here"" /></a></p>
<p>I have tried using the &quot;<strong>Select</strong>&quot; and &quot;<strong>Derive Column</strong>&quot; data flow functions and adding a custom mapping. It seems both functions have the same mapping interface. My current attempt with <strong>Select</strong> is:
<a href=""https://i.stack.imgur.com/eb96B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eb96B.png"" alt=""enter image description here"" /></a></p>
<p>This gets me close to the desired results. I can use the <code>replace</code> expression to convert hypens to underscores.</p>
<p>The problem here is that this mapping creates new root level columns outside of the <code>fraudData</code> structure. I would like to preserve the hierarchy of the <code>fraudData</code> structure and modify the column names in place.</p>
<p>If I am unable to modify the <code>fraudData</code> in place. Is there any way I can take the new columns and merge them into another complex data type?</p>
<p><strong>Update:</strong>. I do not know the fields of the complex data type in advance. This is a schema drift problem. This is why I have tried using the pattern matching solution. I will not be able to hardcode out kown sub-column names.</p>
","<azure><azure-data-factory>","2023-02-10 04:08:23","387","0","1","75408034","<p>You can rename the sub-columns of complex data type using derived column transformation and convert them as a complex data type again. I tried this with sample data and below is the approach.</p>
<ul>
<li><p>Sample complex data type column with two sub fields are taken as in below image.
<img src=""https://i.imgur.com/VEFkMOk.png"" alt=""enter image description here"" />
img:1 source data preview</p>
</li>
<li><p>In Derived column transformation, For the column <code>fraudData</code>, expression is given as</p>
</li>
</ul>
<pre><code>@(fraudData_1_chn=fraudData.{fraudData-1-chn},
fraudData_2_chn=fraudData.{fraudData-2-chn})
</code></pre>
<p><img src=""https://i.imgur.com/ZvSxod2.png"" alt=""enter image description here"" />
img:2 Derived column settings</p>
<ul>
<li>This expression renames the subfields and nests them under the parent column <code>fraudData</code>.</li>
</ul>
<p><img src=""https://i.imgur.com/tYMX629.png"" alt=""enter image description here"" />
img:3 Transformed data- Fields are renamed.</p>
<p><strong>Update: To rename sub columns dynamically</strong></p>
<ul>
<li>You can use below expression to rename all the fields under the root column <code>fraudData</code>.</li>
</ul>
<pre><code>@(each(fraudData, match(true()), replace($$,'-','_') =  $$))
</code></pre>
<p><img src=""https://i.imgur.com/vz34WOU.png"" alt=""enter image description here"" /></p>
<p>This will replace fields which has <code>-</code> with <code>_</code>.</p>
<p><img src=""https://user-images.githubusercontent.com/113445679/219585616-9050ca8b-0dbe-4645-aac3-a97fae81f086.gif"" alt=""gif112"" /></p>
<p>You can also use pattern match in the expression.</p>
<pre><code>@(each(fraudData, patternMatch(`fraudData-.+` ), replace($$,'-','_') = $$))
</code></pre>
<p>This expression will take fields with pattern <code>fraudData-.+</code> and replace <code>-</code> with <code>_</code> in those fields only.</p>
<p><em><strong>Reference:</strong></em></p>
<ol>
<li><strong>Microsoft document</strong> on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-json#sample-manual-script-for-complete-hierarchical-definition"" rel=""nofollow noreferrer"">script for hierarchical definition in data flow</a>.</li>
<li><strong>Microsoft document</strong> on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column#building-schemas-using-the-expression-builder"" rel=""nofollow noreferrer"">building schemas using derived column transformation</a> .</li>
</ol>
"
"75398724","Azure Data Factory - How to transform object with dynamic keys to array in a data flow?","<p>After spending many hours of reading the documentation, following some tutorials and trial &amp; error, i just can't figure it out; how can I transform the following complex object with key objects to an array using a data flow in Azure Data Factory?</p>
<p><strong>Input</strong></p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;headers&quot;: {
    &quot;Content-Length&quot;: 1234
  },
  &quot;body&quot;: {
    &quot;00b50a39-8591-3db3-88f7-635e2ec5c65a&quot;: {
      &quot;id&quot;: &quot;00b50a39-8591-3db3-88f7-635e2ec5c65a&quot;,
      &quot;name&quot;: &quot;Example 1&quot;,
      &quot;date&quot;: &quot;2023-02-09&quot;
    },
    &quot;0c206312-2348-391b-99f0-261323a94d95&quot;: {
      &quot;id&quot;: &quot;0c206312-2348-391b-99f0-261323a94d95&quot;,
      &quot;name&quot;: &quot;Example 2&quot;,
      &quot;date&quot;: &quot;2023-02-09&quot;
    },
    &quot;0c82d1e4-a897-32f2-88db-6830a21b0a43&quot;: {
      &quot;id&quot;: &quot;00b50a39-8591-3db3-88f7-635e2ec5c65a&quot;,
      &quot;name&quot;: &quot;Example 3&quot;,
      &quot;date&quot;: &quot;2023-02-09&quot;
    },
  }
}
</code></pre>
<p><strong>Expected output</strong></p>
<pre class=""lang-json prettyprint-override""><code>[
  {
    &quot;id&quot;: &quot;00b50a39-8591-3db3-88f7-635e2ec5c65a&quot;,
    &quot;name&quot;: &quot;Example 1&quot;,
    &quot;date&quot;: &quot;2023-02-09&quot;
  },
  {
    &quot;id&quot;: &quot;0c206312-2348-391b-99f0-261323a94d95&quot;,
    &quot;name&quot;: &quot;Example 2&quot;,
    &quot;date&quot;: &quot;2023-02-09&quot;
  },
  {
    &quot;id&quot;: &quot;00b50a39-8591-3db3-88f7-635e2ec5c65a&quot;,
    &quot;name&quot;: &quot;Example 3&quot;,
    &quot;date&quot;: &quot;2023-02-09&quot;
  }
]
</code></pre>
","<arrays><json><azure><azure-data-factory>","2023-02-09 12:39:16","204","0","1","75407474","<p><strong>AFAIK</strong>, Your JSON keys are dynamic. So, getting the desired result using dataflow might not be possible.</p>
<p>In this case, you can try the below approach as a workaround. This will work only if all of your key's length is same.</p>
<p><strong>This is my Pipeline:</strong></p>
<p><img src=""https://i.imgur.com/SBwXhv7.png"" alt=""enter image description here"" /></p>
<ul>
<li><p>First I have used a <strong>lookup activity</strong> to get the JSON file and converted the lookup output to a string and stored in a variable using below expression.
<code>@substring(string(activity('Lookup1').output.value[0].body),2,sub(length(string(activity('Lookup1').output.value[0].body)),4))</code>.</p>
</li>
<li><p>Then I have used <strong>split on that String variable</strong> with <code>'},&quot;'</code> and stored in an array variable using below expression.
<code>@split(variables('res_str'),'},&quot;')</code>
It will give the array like below.</p>
<p><img src=""https://i.imgur.com/ZzOCDfo.png"" alt=""enter image description here"" /></p>
</li>
<li><p>Give that array to a <strong>ForEach</strong> and inside ForEach use an <strong>append variable</strong> activity to store the keys into an array with below expression.
<code>@take(item(), 36)</code></p>
</li>
<li><p>Now, I got the list of keys in an array, after the above ForEach use another ForEach activity to get the desired array of objects. Use append variable actvity inside ForEach and give the below expression for it.
<code>@activity('Lookup1').output.value[0].body[item()]</code></p>
</li>
</ul>
<p><strong>Result array after ForEach will be:</strong></p>
<p><img src=""https://i.imgur.com/P5qhwv8.png"" alt=""enter image description here"" /></p>
<p>If you want to store the above JSON into a file, you need to use <code>OPENJSON</code> from SQL. This is because copy activity additonal column only supports string type not an array type.</p>
<p>Use a SQL dataset on copy activity source and give the below SQL script in the query.</p>
<pre><code>DECLARE @json NVARCHAR(MAX)
SET @json =   
  N'@{variables('json_arr')}'  
   
SELECT * FROM  
 OPENJSON ( @json )  
WITH (   
              id varchar(200) '$.id' ,  
              name varchar(32)     '$.name',  
              date varchar(32) '$.date'
 )
</code></pre>
<p>In Sink, give a JSON dataset and select Array of Objects as File pattern.</p>
<p><img src=""https://i.imgur.com/PVGyfzC.png"" alt=""enter image description here"" /></p>
<p>Execute the pipeline and you will get the above array inside a file.</p>
<p><strong>This is my Pipeline JSON:</strong></p>
<pre><code>{
&quot;name&quot;: &quot;pipeline1&quot;,
&quot;properties&quot;: {
    &quot;activities&quot;: [
        {
            &quot;name&quot;: &quot;Lookup1&quot;,
            &quot;type&quot;: &quot;Lookup&quot;,
            &quot;dependsOn&quot;: [],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;JsonSource&quot;,
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSReadSettings&quot;,
                        &quot;recursive&quot;: true,
                        &quot;enablePartitionDiscovery&quot;: false
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;JsonReadSettings&quot;
                    }
                },
                &quot;dataset&quot;: {
                    &quot;referenceName&quot;: &quot;Json1&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                },
                &quot;firstRowOnly&quot;: false
            }
        },
        {
            &quot;name&quot;: &quot;Lookup output to Str&quot;,
            &quot;description&quot;: &quot;&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Lookup1&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;res_str&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@substring(string(activity('Lookup1').output.value[0].body),2,sub(length(string(activity('Lookup1').output.value[0].body)),4))&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;Split Str to array&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Lookup output to Str&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;split_arr&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@split(variables('res_str'),'},\&quot;')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;build keys array using split array&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Split Str to array&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@variables('split_arr')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;isSequential&quot;: true,
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;take first 36 chars of every item&quot;,
                        &quot;type&quot;: &quot;AppendVariable&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;keys_array&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@take(item(), 36)&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    }
                ]
            }
        },
        {
            &quot;name&quot;: &quot;build final array using keys array&quot;,
            &quot;type&quot;: &quot;ForEach&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;build keys array using split array&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;items&quot;: {
                    &quot;value&quot;: &quot;@variables('keys_array')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                },
                &quot;isSequential&quot;: true,
                &quot;activities&quot;: [
                    {
                        &quot;name&quot;: &quot;Append variable1&quot;,
                        &quot;description&quot;: &quot;append every object to array&quot;,
                        &quot;type&quot;: &quot;AppendVariable&quot;,
                        &quot;dependsOn&quot;: [],
                        &quot;userProperties&quot;: [],
                        &quot;typeProperties&quot;: {
                            &quot;variableName&quot;: &quot;json_arr&quot;,
                            &quot;value&quot;: {
                                &quot;value&quot;: &quot;@activity('Lookup1').output.value[0].body[item()]&quot;,
                                &quot;type&quot;: &quot;Expression&quot;
                            }
                        }
                    }
                ]
            }
        },
        {
            &quot;name&quot;: &quot;Just for Res show&quot;,
            &quot;type&quot;: &quot;SetVariable&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;build final array using keys array&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;variableName&quot;: &quot;final_res_show&quot;,
                &quot;value&quot;: {
                    &quot;value&quot;: &quot;@variables('json_arr')&quot;,
                    &quot;type&quot;: &quot;Expression&quot;
                }
            }
        },
        {
            &quot;name&quot;: &quot;Copy data1&quot;,
            &quot;type&quot;: &quot;Copy&quot;,
            &quot;dependsOn&quot;: [
                {
                    &quot;activity&quot;: &quot;Just for Res show&quot;,
                    &quot;dependencyConditions&quot;: [
                        &quot;Succeeded&quot;
                    ]
                }
            ],
            &quot;policy&quot;: {
                &quot;timeout&quot;: &quot;0.12:00:00&quot;,
                &quot;retry&quot;: 0,
                &quot;retryIntervalInSeconds&quot;: 30,
                &quot;secureOutput&quot;: false,
                &quot;secureInput&quot;: false
            },
            &quot;userProperties&quot;: [],
            &quot;typeProperties&quot;: {
                &quot;source&quot;: {
                    &quot;type&quot;: &quot;AzureSqlSource&quot;,
                    &quot;sqlReaderQuery&quot;: &quot;DECLARE @json NVARCHAR(MAX)\nSET @json =   \n  N'@{variables('json_arr')}'  \n   \nSELECT * FROM  \n OPENJSON ( @json )  \nWITH (   \n              id varchar(200) '$.id' ,  \n              name varchar(32)     '$.name',  \n              date varchar(32) '$.date'\n )&quot;,
                    &quot;queryTimeout&quot;: &quot;02:00:00&quot;,
                    &quot;partitionOption&quot;: &quot;None&quot;
                },
                &quot;sink&quot;: {
                    &quot;type&quot;: &quot;JsonSink&quot;,
                    &quot;storeSettings&quot;: {
                        &quot;type&quot;: &quot;AzureBlobFSWriteSettings&quot;
                    },
                    &quot;formatSettings&quot;: {
                        &quot;type&quot;: &quot;JsonWriteSettings&quot;,
                        &quot;filePattern&quot;: &quot;arrayOfObjects&quot;
                    }
                },
                &quot;enableStaging&quot;: false
            },
            &quot;inputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;AzureSqlTable1&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                }
            ],
            &quot;outputs&quot;: [
                {
                    &quot;referenceName&quot;: &quot;Target_JSON&quot;,
                    &quot;type&quot;: &quot;DatasetReference&quot;
                }
            ]
        }
    ],
    &quot;variables&quot;: {
        &quot;res_str&quot;: {
            &quot;type&quot;: &quot;String&quot;
        },
        &quot;split_arr&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;keys_array&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;final_res_show&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        },
        &quot;json_arr&quot;: {
            &quot;type&quot;: &quot;Array&quot;
        }
    },
    &quot;annotations&quot;: []
}
}
</code></pre>
<p><strong>Result file:</strong></p>
<p><img src=""https://i.imgur.com/BHxec9G.png"" alt=""enter image description here"" /></p>
"
"75394945","In Pentaho data integration how to apply row_number() over (partition by category order by storename) rownumbers logic to the rows","<p>I just want to assign row numbers for similar kind of rows [enter image description here]</p>
<p><a href=""https://i.stack.imgur.com/NyLpT.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I am trying to assign row number for similar kind of rows in pentaho data integration</p>
","<pentaho><pentaho-spoon><pentaho-data-integration>","2023-02-09 06:23:54","61","0","1","75397524","<p>You can use the <code>Add value fields changing sequence</code> step to assign the row number of each group and don't forget to add the <code>Sort rows</code> step before assigning the value</p>
<p><a href=""https://i.stack.imgur.com/CP4bC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CP4bC.png"" alt=""demo"" /></a></p>
"
"75365649","Remove Device without Factory Reset/Removing Data?","<p>I've setup an enterprise, policy, and have provisioned my device. However, if this device is removed from the enterprise, the device is factory reset.</p>
<p>Is it possible to skip the factory reset step? I am using AMA mostly to batch install apps and adjust a few settings, so a factory reset would wipe all of this data that I want to keep on the device.</p>
<p>If not, is there an alternative to batch installing apps? Is using AMA overkill for my use case?</p>
<p>I found this flag in the Remove command, but turns out it just ignores the command instead of telling it to &quot;skip factory reset&quot;</p>
<pre><code>        var request = androidManagementService.Enterprises.Devices.Delete(device.Name);
        request.WipeDataFlags = EnterprisesResource.DevicesResource.DeleteRequest.WipeDataFlagsEnum.WIPEDATAFLAGUNSPECIFIED;
        var result = request.Execute();
</code></pre>
","<.net><mdm><android-management-api>","2023-02-06 19:16:28","75","0","1","75411414","<p>No APIs are available in AMAPI that will enable users/admins to skip factory reset when the device is removed from the enterprise, as this is the intended behavior.</p>
<p>When an admin removes an Android device from the associated enterprise, all work data and apps associated with the account are removed from the device. If the device has a work profile, then work-managed apps are removed from the device, too. Your Google Workspace data is still available when you sign in with your computer, a web browser, or another authorized mobile device. If you don't have a work profile or the legacy Device Administrator mode and you remove your work account, when you add it back to your device, all apps on your device are removed, including any personal apps.</p>
<p><a href=""https://developers.google.com/android/management/reference/rest/v1/enterprises.devices/delete?apix_params=%7B%22name%22%3A%22enterprises%2FLC03oveayg%2Fdevices%2F31d01a4e7e2b8650%22%2C%22wipeDataFlags%22%3A%5B%22WIPE_DATA_FLAG_UNSPECIFIED%22%5D%2C%22wipeReasonMessage%22%3A%22Testing%20Coffee%22%7D#wipedataflag"" rel=""nofollow noreferrer"">WipeDataFlag</a> can control the data-wiping behavior when a device is deleted (to some extent), <strong>but it will not be able to skip the factory reset entirely.</strong></p>
<p>The PRESERVE_RESET_PROTECTION_DATA flag tells the Android Management API to preserve the factory reset protection data on the device. This means that the device will still be locked and require the user to enter their Google account password to unlock it, even after it has been reset. This flag is useful if you want to ensure that the device is not reset without your permission. It can also be used to prevent unauthorized users from accessing the device's data.</p>
<p>To use this flag, you must include it in the WipeDataFlag parameter of the enterprises.devices.delete method. For example:</p>
<pre><code>enterprises.devices.delete(deviceId, null, null, WipeDataFlag.FLAG_WIPE_ALL_DATA | WipeDataFlag.PRESERVE_RESET_PROTECTION_DATA);
</code></pre>
<p>Batch installing apps is a valid use-case for Android Management API. &quot;is this overkill?&quot; would depend a bit on your specific use case. Still, we don't have enough info on what your individual use case is to be able to provide additional context.</p>
"
"75362403","Pentaho data integration merge with inner join has 5000+ matches and left join has 0 matches","<p>I want to merge an excel with a database query to add some fields in a transformations.</p>
<p>The merge with INNER seems to work well and has +5000 matches, but I need to do a LEFT JOIN to get the unmatches rows as well, and in this case the matched rows is 0.</p>
<p>Why is not matching any rows when LEFT JOIN is used? Any ideas?</p>
<p><strong>Transformation</strong></p>
<p><a href=""https://i.stack.imgur.com/pmL9U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pmL9U.png"" alt=""enter image description here"" /></a></p>
<p><strong>Merge</strong></p>
<p><a href=""https://i.stack.imgur.com/wDO7Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wDO7Q.png"" alt=""enter image description here"" /></a></p>
<p><strong>Sort rows (left side)</strong></p>
<p><a href=""https://i.stack.imgur.com/ZmzwZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZmzwZ.png"" alt=""enter image description here"" /></a></p>
<p><strong>Sort rows (right side)</strong></p>
<p><a href=""https://i.stack.imgur.com/GKYUq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GKYUq.png"" alt=""enter image description here"" /></a></p>
","<sorting><merge><pentaho><pentaho-data-integration>","2023-02-06 14:09:50","31","0","1","75362948","<p>From your screenshot, it looks fine but I'm unsure why the <code>Merge join</code> step didn't work for you maybe you can try another way to do left join by using the <code>Stream lookup</code> step instead, for me it works both ways. You can find the difference between the two steps from <a href=""https://helicaltech.com/what-is-a-lookup-how-is-it-different-from-the-merge-join-component-in-etl/"" rel=""nofollow noreferrer"">here</a></p>
<p><a href=""https://i.stack.imgur.com/rwVc7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rwVc7.png"" alt=""Merge join"" /></a></p>
<p><a href=""https://i.stack.imgur.com/rbGZn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rbGZn.png"" alt=""Stream lookup"" /></a></p>
"
"75360632","Spring cloud data flow S3 source to listen to a folder in S3 bucket","<p>I'm trying to create a stream which needs to listen to a folder in S3 bucket.</p>
<p>I could create a stream listening to the root of the bucket using following properties and could trigger the process.</p>
<pre><code>app.s3-in.s3.supplier.remote-dir=mybucket
app.s3-in.cloud.aws.credentials.accessKey=XXXXXXXXXX
app.s3-in.cloud.aws.credentials.secretKey=YYYYYYYYYYYYYYYYY
app.s3-in.cloud.aws.region.static=ZZZZZZZZ
app.s3-in.cloud.aws.stack.auto=false
</code></pre>
<p>But unable to listen to a folder in the bucket.
I tried with following options:</p>
<pre><code>app.s3-in.s3.supplier.remote-dir=mybucket/myfolder
app.s3-in.s3.supplier.remote-dir=mybucket/myfolder/
app.s3-in.s3.supplier.remote-dir=/mybucket/myfolder
app.s3-in.s3.supplier.remote-dir=/mybucket/myfolder/
</code></pre>
<p>I tried setting app.s3-in.path-style-access=true as well.</p>
","<spring-cloud-stream><spring-cloud-dataflow>","2023-02-06 11:20:14","109","0","1","75461957","<p>Try</p>
<pre><code>app.s3-in.s3.supplier.remote-dir=mybucket
app.s3-in.s3.supplier.filename-pattern=myfolder/**
</code></pre>
<p>(or some variation of the file pattern).</p>
"
"75356262","Mapping data flow allows duplicate records when using UPSERT","<p>Using Synapse pipelines and mapping data flow to process multiple daily files residing in ADLS which represent incremental inserts and updates for any given primary key column.  Each daily physical file has ONLY one instance for any given primary key value.  Keys/rows are unique within a daily file, but the same key value can exist in multiple files for each day where attributes related to that key column changed over time.  All rows flow to the Upsert condition as shown in screen shot.</p>
<p><a href=""https://i.stack.imgur.com/OtRZD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OtRZD.png"" alt=""enter image description here"" /></a></p>
<p>Sink is a Synapse table where primary keys can only be specified with non-enforced primary key syntax which can be seen below.</p>
<p><a href=""https://i.stack.imgur.com/nPHOI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nPHOI.png"" alt=""enter image description here"" /></a></p>
<p>Best practice with mapping data flows is avoid placing mapping data flow within a foreach activity to process each file individually as this spins up a new cluster for each file which takes forever and gets expensive. Instead, I have configured the mapping data flow source to use wildcard path to process all files at once with a sort by file name to ensure they are ordered correctly within a single data flow (avoiding the foreach activity for each file).</p>
<p><a href=""https://i.stack.imgur.com/mkFZ8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mkFZ8.png"" alt=""enter image description here"" /></a></p>
<p>Under this configuration, a single data flow looking at multiple daily files can definitely expect the same key column to exist on multiple rows.  When the empty target table is first loaded from all the daily files, we get multiple rows showing up for any single key column value instead of a single INSERT for the first one and updates for the remaining ones it sees (essentially never doing any UPDATES).</p>
<p>The only way I avoid duplicate rows by the key column is to process each file individually and execute a mapping data flow for each file within a for each activity.  Does anyone have any approach that would avoid duplicates while processing all files within a single mapping data flow without a foreach activity for each file?</p>
","<azure-data-factory><azure-synapse><azure-mapping-data-flow>","2023-02-05 23:35:00","152","0","1","75359219","<blockquote>
<p>Does anyone have any approach that would avoid duplicates while processing all files within a single mapping data flow without a foreach activity for each file?</p>
</blockquote>
<p>AFAIK, there is no other way than using ForEach loop to process file one by one.</p>
<p>When we use wildcard, it takes all the matching file in the one go. like below same values from different file.</p>
<p><img src=""https://i.imgur.com/xqawsRm.png"" alt=""enter image description here"" /></p>
<p>using alter rows condition will help you to upsert rows if you have only on single file as you are using multiple files this will create duplicate records like this similar question <a href=""https://stackoverflow.com/a/60862159"">Answer</a> by Leon Yue.</p>
<p>As scenario explained you have same values in multiple files, and you want to avoid that to being getting duplicated. to avoid this, you have to iterate over each of the file and then perform dataflow operations on that file to avoid duplicates getting upsert.</p>
"
"75276172","How to extend standard spring cloud data flow components and add our custom logic","<p>I'm looking to develop a custom processor that leverages the functionality of s3-sink and then add some custom logic to it.</p>
<p>I could do it without using standard components. i.e. using S3 APIs to write files along with custom logic. I would like to know if there is any way I can leverage the functionality of standard component and extend it to add our custom logic.</p>
","<spring-cloud-dataflow>","2023-01-29 15:40:42","42","0","1","75280166","<p>If you want to invoke some logic before or after the S3Sink normal logic then you can use <a href=""https://dataflow.spring.io/docs/feature-guides/streams/function-composition/#function-composition"" rel=""nofollow noreferrer"">function composition</a> and create a custom Sink application that imports the <code>S3SinkConfiguration</code> and defines a <code>java.util.Function</code> that is applied after the normal S3Sink logic.
You will need to set the <code>spring.cloud.function.definition</code> property to <code>s3Consumer|myCustomLogicFunctionName</code>.</p>
<p>However, if you want to tweak what the S3Sink logic actually is (not before or after it executes), you will have to look into what it offers for extensibility/configurability. The S3Sink app looks like this:</p>
<pre class=""lang-java prettyprint-override""><code>@SpringBootApplication
@Import({ org.springframework.cloud.fn.consumer.s3.AwsS3ConsumerConfiguration.class })
public class S3SinkKafkaApplication {

    public static void main(String[] args) {
        SpringApplication.run(S3SinkKafkaApplication.class, args);
    }
}
</code></pre>
<p>As you can see its just a SpringBoot app that imports a single configuration class. All the logic/magic is in that configuration. It in turn leverages <a href=""https://github.com/spring-projects/spring-integration-aws"" rel=""nofollow noreferrer"">Spring Integration AWS</a> <code>S3MessageHandler</code>.</p>
<p>What does the custom logic need to do? Maybe the underlying S3MessageHandler supports that via configuration/extension.</p>
"
"75200405","spring cloud data flow application properties / environment variables","<p>Have a docker container running a single spring cloud data flow application. Up until version 2.7.2 of data flow, I was able to pass all the database url/username/password properties to the application by defining them as environment variables in the container.</p>
<p>The application has application.yml defined to fetch the properties from variables, like this:</p>
<pre><code>appname:
  datasource:
    url: ${APP_DB_URL:jdbc:sqlserver://127.0.0.1:1433}
    password: ${APP_DB_PASSWORD:D3faultP4ss!}
    username: ${APP_DB_USER:sa}
</code></pre>
<p>As said previously these variables was able to be defined by just having them as env variables in the container, now this method doesn't seem to work anymore starting from data flow 2.8 upwards.</p>
<p>Setup is configured to automatically register the application to the spring cloud data flow after startup, with following logic</p>
<p>1st delete all old tasks</p>
<pre><code>curl &quot;http://localhost:9393/tasks/definitions/${APP_NAME}?cleanup=false&quot; -o /dev/null -s -w &quot;%{http_code}&quot; -X DELETE
</code></pre>
<p>2nd delete all old applications</p>
<pre><code>curl &quot;http://localhost:9393/apps&quot; -o /dev/null -s -w &quot;%{http_code}&quot; -X DELETE
</code></pre>
<p>3rd register application</p>
<pre><code>curl &quot;http://localhost:9393/apps/task/${APP_NAME}/${APP_VERSION}&quot; -o /dev/null -s -w &quot;%{http_code}&quot; -X POST -d &quot;uri=file%3A%2F%2Fapp%2F${APP_JAR}&amp;force=true&quot;
</code></pre>
<p>4th register task</p>
<pre><code>curl &quot;http://localhost:9393/tasks/definitions&quot; -o /dev/null -s -w &quot;%{http_code}&quot; -X POST -d &quot;name=${APP_NAME}&amp;definition=${APP_NAME}&quot;
</code></pre>
<p>APP_NAME, APP_VERSION and APP_JAR are env variables put into the container in the build phase.</p>
<p>The on schedule, daily, the job is triggered with the following call:</p>
<pre><code>curl &quot;http://localhost:9393/tasks/executions&quot; -o /dev/null -s -w &quot;%{http_code}&quot; -X POST -d &quot;name=${APP_NAME}&amp;properties=deployer.${APP_NAME}.local.workingDirectoriesRoot%3D%2Fapp%2Flogs&amp;arguments=--date%3D$(date --date='yesterday' +\%Y-\%m-\%d)+--range%3D${range}+--dataSource%3DApplication+SourceProperty&quot;
</code></pre>
<p>As said, until 2.7.2 version this worked, now when trying to update to 2.8 or newer the application tries to connect to the default database url defined in the application.yml. How can I override these using env variables?
And obviously if there's a better way to do the auto registration of the app, all tips are appreciated.</p>
","<spring><spring-cloud-dataflow>","2023-01-22 12:29:12","119","0","1","75208357","<p>Spring Cloud Data Flow 2.7.x used Spring Boot 2.3
Spring Cloud Data Flow 2.8.x used Spring Boot 2.4</p>
<p>There are changes to property names and the data source initialisation will override your configuration unless you disable autoconfiguration of datasources.</p>
<p>We suggest you move to latest Spring Cloud Data Flow and update your applications to Spring Boot 2.7.x
The you provide properties like:
<code>app.&lt;appname&gt;.spring.datasource.url</code> etc.</p>
"
"75192309","How to truncate the data to first 3 letter in data flow?","<p>I want to truncate data if unit=code.</p>
<p>Input:</p>
<p>Country, unit</p>
<p>India, code</p>
<p>Bangladesh, money</p>
<p>China, code</p>
<p>Output:</p>
<p>Country, unit</p>
<p>Ind, code</p>
<p>Bangladesh, money</p>
<p>Chi, code</p>
<p>What I tried?</p>
<p>I used case expression in dataflow but not able to truncate data to 3 letter code</p>
","<azure><azure-data-factory>","2023-01-21 09:25:00","63","0","1","75192601","<ul>
<li>You can use left() function in dataflow to get the first three characters of data.</li>
<li>I repro'd this with sample input.</li>
</ul>
<p><strong>Source data:</strong></p>
<p><img src=""https://i.imgur.com/L8XfpIT.png"" alt=""enter image description here"" /></p>
<ul>
<li>Derived column transformation is taken and expression for country column is given as <code>case(unit=='code',left(Country,3) , Country)</code></li>
</ul>
<p><strong>Derived column settings:</strong></p>
<p><img src=""https://i.imgur.com/KH6PSc2.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/9nUNTWB.png"" alt=""enter image description here"" /></p>
"
"75188128","How to add a row in a CSV file in pentaho data integration","<p>I need to add a row data in a CSV file using Pentaho Data Integration.</p>
<p>I've tried with this transformation</p>
<p><a href=""https://i.stack.imgur.com/cbJ6g.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cbJ6g.png"" alt=""enter image description here"" /></a></p>
<p>This is my CSV file input configuration</p>
<p><a href=""https://i.stack.imgur.com/1IX4I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1IX4I.png"" alt=""enter image description here"" /></a></p>
<p>and this is the CSV file output configuration (with the &quot;append&quot; check activated ...)</p>
<p><a href=""https://i.stack.imgur.com/V2Srd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V2Srd.png"" alt=""enter image description here"" /></a>
My constant definition</p>
<p><a href=""https://i.stack.imgur.com/3gtbZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3gtbZ.png"" alt=""enter image description here"" /></a></p>
<p>and this is my CSV file sample</p>
<p><a href=""https://i.stack.imgur.com/LR5s4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LR5s4.png"" alt=""enter image description here"" /></a></p>
<p>I'd like to have this</p>
<p><a href=""https://i.stack.imgur.com/pY5VW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pY5VW.png"" alt=""enter image description here"" /></a></p>
<p>Any suggestion will be appreciated!</p>
","<csv><pentaho-data-integration>","2023-01-20 18:36:22","121","0","1","75205461","<p>You can use the <code>Data grid</code> step to create your constant data and the <code>Append streams</code> step to merge two streams into one in your desired order (data type in two streams must be matched and the same order) and then you can write the data to a CSV file. If you don't need a header present in the CSV file you can uncheck the &quot;Header&quot; option in the content tab</p>
<p><a href=""https://i.stack.imgur.com/Eyilb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Eyilb.png"" alt=""transformation"" /></a></p>
"
"75188112","Managed Devices with Work Profile: How to be qualified to read privileged device identifiers","<p>Being an EMM organization, we have an application that provides a device owner and a user's work profile enrollment of their devices.</p>
<p>As an MDM application, we must manage the device information by accessing privileged identifiers. Since Android 11, we can access these identifiers in the Device Owner enrollments but not in the work profile enrollments. How do we qualify to read privileged device identifiers in the user's work profile enrollment?</p>
","<android><mdm><emm><android-work-profile>","2023-01-20 18:34:20","21","0","1","75236958","<p>I want to share the response that I received in the Android Discussion Forum:</p>
<blockquote>
<p>Since work profiles set up on a personally owned device will no longer
be granted access to the device's hardware identifiers (IMEI, MEID,
serial number) in <a href=""https://emm.androidenterprise.dev/s/article/Android-12-Impact-Device-Identifiers"" rel=""nofollow noreferrer"">Android 12</a>, <strong>there is currently no way for you
to have a special privilege to access these identifiers</strong>. We suggest
you leverage the <a href=""https://developer.android.com/reference/android/app/admin/DevicePolicyManager#getEnrollmentSpecificId()"" rel=""nofollow noreferrer"">enrollment-specific ID</a>. The enrollment-specific
ID remains stable across factory resets when re-enrolled to the same
organization managed by the same device policy application.    Here is
the <a href=""https://developer.android.com/training/articles/user-data-ids#best-practices-android-identifiers"" rel=""nofollow noreferrer""><strong>link</strong></a> for best practices for working with Android
identifiers.</p>
</blockquote>
"
"75179463","How to write expression in expression builder in data flow of ADF","<p>I need an expression in the expression builder to transform the customer Name as below
Take first character of word in the name followed by * . Customer name may contain 1 or more words
Name can be Tim or Tim John or Tim John Zac or Tim John Mike Zac</p>
","<regex><azure><azure-data-factory>","2023-01-20 00:00:10","216","1","1","75179995","<p>I have reproduced above and got below results using derived column.</p>
<p>I have used the same data that you have given in a single column and used the below dataflow expression in derived column.</p>
<pre><code>dropLeft(toString(reduce(map(split(Name, ' '),regexReplace(#item, concat('[^',left(#item,1),']'), '*')), '', #acc +  '  '  + #item, #result)), 2)
</code></pre>
<p>Here, some general regular expressions were given errors for me in dataflow, that's why used the above approach.</p>
<p>First, I have used <code>split()</code> by <code>space</code> to get an array of strings. Then used regular expression on every item of array like above.</p>
<p>As we do not have <code>join</code> in dataflow expression, I have used the code from this <a href=""https://stackoverflow.com/a/75089327"">SO answer</a> by <strong>@Jarred Jobe</strong> to convert array to a string seperated by spaces.</p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/2duHR6f.png"" alt=""enter image description here"" /></p>
<p><strong>NOTE:</strong></p>
<p>Make sure you give two spaces in <code>toString()</code> of above code to get the required result. If we give only one space it will give the results like below.</p>
<p><img src=""https://i.imgur.com/7nIg4tT.png"" alt=""enter image description here"" /></p>
<p><strong>Update:</strong></p>
<blockquote>
<p>Thank you so much for sharing this. I have tried your solution but I
got few names wrong .Also I want to replace the rest of the characters
with just 5 '<em>' irrespective of how many characters the name has. Also
name : Mia hellah came as M*</em> h****h instead of M***** h*****. Another
one SAM &amp; JOHN TIBEH should be S***** &amp;***** J***** T*****. I tried to
update your expression but I couldn't get it right.</p>
</blockquote>
<p>If you want to do like above, you can directly use <code>concat</code> function dataflow expression.</p>
<pre><code>dropLeft(toString(reduce(map(split(Name, ' '),concat(left(#item,1), '*****')), '', #acc +  '  '  + #item, #result)), 2)
</code></pre>
<p><strong>Results:</strong></p>
<p><img src=""https://i.imgur.com/hfvEq6n.png"" alt=""enter image description here"" /></p>
"
"75103052","How to stop rendering react component every data file while mapping","<p>So I have a js file where I store all of my data, and I will map out that js file to render the data I want to render. To stop myself from rendering all the data, I have made it so that it only conditionally renders every time title = element.id, therefore it should only render out the data that has the id specific id of the title. However, although this works, it ends up rendering a bunch of empty divs, and so this pushes my actual data down.
<a href=""https://i.stack.imgur.com/7u4Nf.jpg"" rel=""nofollow noreferrer"">Image of my data being pushed down</a>
As you can see the empty divs have pushed my actual data down a lot inside of my modal.</p>
<p>Does anybody know how to stop this from happening? (How to get rid of the space above by stopping the rendering of the blanks divs maybe?)</p>
<p>My modal file looks like this</p>
<pre><code>const Modal = ({ handleClose, id, title, subtitle, description, techStack, image, github, devpost }) =&gt; {
    let isId = false;
    return (
        &lt;Backdrop onClick={handleClose}&gt;
            &lt;motion.div 
                onClick={(e) =&gt; e.stopPropagation()}
                className='modal-ics'
                variants={dropIn}
                initial='hidden'
                animate='visible'
                exit='exit'
                
            &gt;
                &lt;div className='w-full flex flex-col items-center'&gt;
                    &lt;ModalButton onClick={handleClose} label='Close'&gt;&lt;/ModalButton&gt;
                    &lt;div className='flex flex-col w-full h-full justify-between items-start'&gt;
                        hello
       
                        {ICSData.map((element) =&gt; {
                            if (id == element.id) {
                                isId = true;
                            }

                            else {
                                isId = false;
                            }

                            return (
                                &lt;div&gt;
           
                                    { isId &amp;&amp;
                                        &lt;div&gt;
                                            &lt;div className='modal-text-separator'&gt;
                                                &lt;h1 className='modal-title'&gt;{element.title}&lt;/h1&gt;
                                                &lt;h3 className='modal-date'&gt;{element.subtitle1}&lt;/h3&gt;
                                                &lt;p className='modal-description mt-2'&gt;{element.description1}&lt;/p&gt;
                                            &lt;/div&gt;         
                                            { element.subtitle2 &amp;&amp; 
                                                &lt;div className='modal-text-separator'&gt;
                                                    &lt;h3 className='modal-date'&gt;{element.subtitle2}&lt;/h3&gt;
                                                    &lt;p className='modal-description mt-2'&gt;{element.description2}&lt;/p&gt;
                                                &lt;/div&gt;        
                                            }
                                            { element.subtitle3 &amp;&amp; 
                                                &lt;div className='modal-text-separator'&gt;
                                                    &lt;h3 className='modal-date'&gt;{element.subtitle3}&lt;/h3&gt;
                                                    &lt;p className='modal-description mt-2'&gt;{element.description3}&lt;/p&gt;
                                                &lt;/div&gt;        
                                            }
                                            { element.subtitle4 &amp;&amp; 
                                                &lt;div className='modal-text-separator'&gt;
                                                    &lt;h3 className='modal-date'&gt;{element.subtitle4}&lt;/h3&gt;
                                                    &lt;p className='modal-description mt-2'&gt;{element.description4}&lt;/p&gt;
                                                &lt;/div&gt;        
                                            }
                                            { element.subtitle5 &amp;&amp; 
                                                &lt;div className='modal-text-separator'&gt;
                                                    &lt;h3 className='modal-date'&gt;{element.subtitle5}&lt;/h3&gt;
                                                    &lt;p className='modal-description mt-2'&gt;{element.description5}&lt;/p&gt;
                                                &lt;/div&gt;        
                                            }
                                        &lt;/div&gt;
                                      
                                    }
                                &lt;/div&gt;                           
                            );
                        })}             
                    &lt;/div&gt;                  
                &lt;/div&gt;
            &lt;/motion.div&gt;
        &lt;/Backdrop&gt;
    )
}
export default Modal;
</code></pre>
<p>My data file looks like this</p>
<pre><code>import React from 'react'

const ICSData = [
  {
    id: &quot;classes&quot;,
    title: &quot;Classes&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  },
  {
    id: &quot;documentation-ds&quot;,
    title: &quot;Documentation&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  },
  {
    id: &quot;file-reading-and-writing&quot;,
    title: &quot;File Reading and Writing&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  },
  {
    id: &quot;object-concepts&quot;,
    title: &quot;Object Concepts&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  },
  {
    id: &quot;uml-diagrams&quot;,
    title: &quot;UML Diagrams&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  },
  {
    id: &quot;documentation-algorithms&quot;,
    title: &quot;Documentation&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  }
]
export default ICSData;
</code></pre>
","<javascript><reactjs><mapping><data-mapping>","2023-01-12 22:41:59","35","2","2","75103107","<p>You could just limit it by only calling the map function up until a certain ID</p>
<pre><code>{ICSData.filter((item, idx) =&gt; idx &lt; 5).map((element) =&gt; {...})}
</code></pre>
<p>And then you can also replace the 5 with a variable that controls that number if you want to make a load more button.</p>
"
"75103052","How to stop rendering react component every data file while mapping","<p>So I have a js file where I store all of my data, and I will map out that js file to render the data I want to render. To stop myself from rendering all the data, I have made it so that it only conditionally renders every time title = element.id, therefore it should only render out the data that has the id specific id of the title. However, although this works, it ends up rendering a bunch of empty divs, and so this pushes my actual data down.
<a href=""https://i.stack.imgur.com/7u4Nf.jpg"" rel=""nofollow noreferrer"">Image of my data being pushed down</a>
As you can see the empty divs have pushed my actual data down a lot inside of my modal.</p>
<p>Does anybody know how to stop this from happening? (How to get rid of the space above by stopping the rendering of the blanks divs maybe?)</p>
<p>My modal file looks like this</p>
<pre><code>const Modal = ({ handleClose, id, title, subtitle, description, techStack, image, github, devpost }) =&gt; {
    let isId = false;
    return (
        &lt;Backdrop onClick={handleClose}&gt;
            &lt;motion.div 
                onClick={(e) =&gt; e.stopPropagation()}
                className='modal-ics'
                variants={dropIn}
                initial='hidden'
                animate='visible'
                exit='exit'
                
            &gt;
                &lt;div className='w-full flex flex-col items-center'&gt;
                    &lt;ModalButton onClick={handleClose} label='Close'&gt;&lt;/ModalButton&gt;
                    &lt;div className='flex flex-col w-full h-full justify-between items-start'&gt;
                        hello
       
                        {ICSData.map((element) =&gt; {
                            if (id == element.id) {
                                isId = true;
                            }

                            else {
                                isId = false;
                            }

                            return (
                                &lt;div&gt;
           
                                    { isId &amp;&amp;
                                        &lt;div&gt;
                                            &lt;div className='modal-text-separator'&gt;
                                                &lt;h1 className='modal-title'&gt;{element.title}&lt;/h1&gt;
                                                &lt;h3 className='modal-date'&gt;{element.subtitle1}&lt;/h3&gt;
                                                &lt;p className='modal-description mt-2'&gt;{element.description1}&lt;/p&gt;
                                            &lt;/div&gt;         
                                            { element.subtitle2 &amp;&amp; 
                                                &lt;div className='modal-text-separator'&gt;
                                                    &lt;h3 className='modal-date'&gt;{element.subtitle2}&lt;/h3&gt;
                                                    &lt;p className='modal-description mt-2'&gt;{element.description2}&lt;/p&gt;
                                                &lt;/div&gt;        
                                            }
                                            { element.subtitle3 &amp;&amp; 
                                                &lt;div className='modal-text-separator'&gt;
                                                    &lt;h3 className='modal-date'&gt;{element.subtitle3}&lt;/h3&gt;
                                                    &lt;p className='modal-description mt-2'&gt;{element.description3}&lt;/p&gt;
                                                &lt;/div&gt;        
                                            }
                                            { element.subtitle4 &amp;&amp; 
                                                &lt;div className='modal-text-separator'&gt;
                                                    &lt;h3 className='modal-date'&gt;{element.subtitle4}&lt;/h3&gt;
                                                    &lt;p className='modal-description mt-2'&gt;{element.description4}&lt;/p&gt;
                                                &lt;/div&gt;        
                                            }
                                            { element.subtitle5 &amp;&amp; 
                                                &lt;div className='modal-text-separator'&gt;
                                                    &lt;h3 className='modal-date'&gt;{element.subtitle5}&lt;/h3&gt;
                                                    &lt;p className='modal-description mt-2'&gt;{element.description5}&lt;/p&gt;
                                                &lt;/div&gt;        
                                            }
                                        &lt;/div&gt;
                                      
                                    }
                                &lt;/div&gt;                           
                            );
                        })}             
                    &lt;/div&gt;                  
                &lt;/div&gt;
            &lt;/motion.div&gt;
        &lt;/Backdrop&gt;
    )
}
export default Modal;
</code></pre>
<p>My data file looks like this</p>
<pre><code>import React from 'react'

const ICSData = [
  {
    id: &quot;classes&quot;,
    title: &quot;Classes&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  },
  {
    id: &quot;documentation-ds&quot;,
    title: &quot;Documentation&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  },
  {
    id: &quot;file-reading-and-writing&quot;,
    title: &quot;File Reading and Writing&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  },
  {
    id: &quot;object-concepts&quot;,
    title: &quot;Object Concepts&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  },
  {
    id: &quot;uml-diagrams&quot;,
    title: &quot;UML Diagrams&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  },
  {
    id: &quot;documentation-algorithms&quot;,
    title: &quot;Documentation&quot;,
    image: &quot;InspiritAI.png&quot; ,
    subtitle1: &quot;very cool subtitle&quot;,
    description1:
        &quot;this is a very cool description&quot;,
    subtitle2: &quot;very cool subtitle&quot;,
    description2:
    &quot;this is a very cool description&quot;,
    subtitle3: &quot;very cool subtitle&quot;,
    description3:
        &quot;this is a very cool description&quot;,
    subtitle4: &quot;very cool subtitle&quot;,
    description4:
    &quot;this is a very cool description&quot;,
    subtitle5: &quot;very cool subtitle&quot;,
    description5:
        &quot;this is a very cool description&quot;,
  }
]
export default ICSData;
</code></pre>
","<javascript><reactjs><mapping><data-mapping>","2023-01-12 22:41:59","35","2","2","75103367","<p>The reason why you end up having multiple empty divs where there shouldn't be lies in your return statement of the mapping:</p>
<pre class=""lang-js prettyprint-override""><code>                 {ICSData.map((element) =&gt; {
                            if (id == element.id) {
                                isId = true;
                            }

                            else {
                                isId = false;
                            }

                            return (
                                &lt;div&gt;
                                    { isId &amp;&amp;
                                        &lt;div&gt;
                                            &lt;div className='modal-text-separator'&gt;
                                     ...
                                     }
                                &lt;/div&gt;
                            )
...
</code></pre>
<p>Here, you're still creating a div whether there should be or not. The easiest fix would be to return an empty element if your id doesn't match, like this:</p>
<pre class=""lang-js prettyprint-override""><code>{ICSData.map(element =&gt; {
    if (id == element.id) {
        return (
          &lt;YOUR_CODE /&gt;
        )
    } else {
        return &lt;&gt;&lt;/&gt;
    }
}
</code></pre>
<p>However, looping through all your data to show one element probably isn't the best idea. A few alternatives:</p>
<ul>
<li>change your data so that instead of being an array, it's an object of objects, with the key being an entry's ID and the values being the rest of your data (assuming all your IDs are unique)</li>
<li>use the find function (once again assuming your IDs are unique)</li>
</ul>
<p>I'll just go over the second solution since it doesn't involve changing your data and is just more straightforward. However, it is slower than the first solution as the <code>.find</code> function would be O(n), compared to the first solution would be O(1). This likely doesn't matter if you have little data, but is something to keep in mind if the number of items in your <code>ICSData</code> array grows.</p>
<p>You can implement the second solution using one line:</p>
<pre><code>const data = ICSData.find(element =&gt; id === element.id)
// Do whatever you want with this var in your component
</code></pre>
"
"75069068","How can I understand the behavior of pipe, with varying data flow?","<p>My problem is a bit hard to explain properly as I do not understand fully the behavior behind it.
I have been working on pipe and pipelines in C, and I noticed some behavior that is a bit mysterious to me.</p>
<p>Let's take a few example: Let's try to pipe yes with head. (yes  | head). Even though I coded the behavior in a custom program, I don't understand how the pipe knows when to stop piping ? It seems two underlying phenomenons are causing this (maybe), the SIGPIPE and/or the internal size a pipe can take. How does the pipe stop piping, is it when it's full ? But the size of a pipe is way superior to 10 &quot;yes\n&quot; no ? And SIGPIPE only works when the end read/write is closed no ?</p>
<p>Also let's take another example, for example cat and ls: cat | ls or even cat | cat | ls.
It seems the stdin of the pipe is waiting for input, but how does it know when to stop, i.e. after one input ? What are the mechanism that permits this behavior?</p>
<p>Also can anyone provide me with others examples of these very specific behavior if there are any in pipes and pipelines so I can get an good overview of theses mechanism ?</p>
<p>In my own implementation, I managed to replicate that behavior using waitpid. However how does the child process itself know when to stop ? Is it command specific ?</p>
","<c><linux><unix><pipe><pipeline>","2023-01-10 11:09:22","138","0","3","75072862","<p>The <code>write</code> operation will block when the pipe buffer is full, the <code>read</code> operation will block when the buffer is empty.</p>
<p>When the write end of the pipe is closed, the reading process will get an EOF indication after reading all data from the buffer. Many programs will terminate in this case.</p>
<p>When the read end of the pipe is closed, the writing process will get a SIGPIPE. This will also terminate most programs.</p>
<p>When you run <code>cat | ls</code>, STDOUT of <code>cat</code> is connected to STDIN of <code>ls</code>, but <code>ls</code> does not read from STDIN. On the system where I checked this, <code>ls</code> simply ignores STDIN and the file descriptor will be closed when <code>ls</code> terminates.</p>
<p>You will see the output of <code>ls</code>, and <code>cat</code> will be waiting for input.</p>
<p><code>cat</code> will not write anything to STDOUT before it has read enough data from STDIN, so it will not notice that the other end of the pipe has been closed.</p>
<p><code>cat</code> will terminate when it detects EOF on STDIN which can be done by pressing CTRL+D or by redirecting STDIN from <code>/dev/null</code>, or when it gets SIGPIPE after trying to write to the pipe which will happen when you (type something and) press ENTER.</p>
<p>You can see the behavior with <code>strace</code>.</p>
<p><code>cat</code> terminates after EOF on input which is shown as <code>read(0, ...)</code> returning 0.</p>
<pre><code>strace cat &lt; /dev/null | ls
</code></pre>
<p><code>cat</code> killed by SIGPIPE.</p>
<pre><code>strace cat &lt; /dev/zero | ls
</code></pre>
"
"75069068","How can I understand the behavior of pipe, with varying data flow?","<p>My problem is a bit hard to explain properly as I do not understand fully the behavior behind it.
I have been working on pipe and pipelines in C, and I noticed some behavior that is a bit mysterious to me.</p>
<p>Let's take a few example: Let's try to pipe yes with head. (yes  | head). Even though I coded the behavior in a custom program, I don't understand how the pipe knows when to stop piping ? It seems two underlying phenomenons are causing this (maybe), the SIGPIPE and/or the internal size a pipe can take. How does the pipe stop piping, is it when it's full ? But the size of a pipe is way superior to 10 &quot;yes\n&quot; no ? And SIGPIPE only works when the end read/write is closed no ?</p>
<p>Also let's take another example, for example cat and ls: cat | ls or even cat | cat | ls.
It seems the stdin of the pipe is waiting for input, but how does it know when to stop, i.e. after one input ? What are the mechanism that permits this behavior?</p>
<p>Also can anyone provide me with others examples of these very specific behavior if there are any in pipes and pipelines so I can get an good overview of theses mechanism ?</p>
<p>In my own implementation, I managed to replicate that behavior using waitpid. However how does the child process itself know when to stop ? Is it command specific ?</p>
","<c><linux><unix><pipe><pipeline>","2023-01-10 11:09:22","138","0","3","75077462","<blockquote>
<p>How does the pipe stop piping</p>
</blockquote>
<p>The pipe stops piping when either end is closed.</p>
<p>If the input(write) end of the pipe is closed, then any data in the pipe is held until it is read from the output end.  Once the buffer is emptied, anyone subsequently reading from the output end will get an EOF.</p>
<p>If the output(read) end of the pipe is closed, any data in the pipe will be discarded.  Anyone subsequently writing to the input end will get a SIGPIPE/EPIPE.  Note that a process merely holding open the input but not actively writing to it will <em>not</em> be signalled.</p>
<p>So when you type <code>cat | ls</code> you get a <code>cat</code> program with stdout connected to the input of the pipe and <code>ls</code> with stdin connected to the output.  <code>ls</code> runs and outputs some stuff (to its stdout, which is still the terminal) and never reads from stdin.  Once done it exits and closes the output of the pipe.  Meanwhile <code>cat</code> is waiting for input from its stdin (the terminal).  When it gets it (you type a line), it writes it to stdout, gets a SIGPIPE/EPIPE and exits (discarding the data as there's noone to write it to.)  This closes the input of the pipe, so the pipe goes away now that both ends have been closed.</p>
<p>Now lets look at what happens with <code>cat | cat | ls</code>.  You now have two pipes and two cat programs.  As before <code>ls</code> runs and exits, closing the output of the second pipe.  Now you type a line and the first cat reads it and copies it to the first pipe (still fully open) where the second cat reads it and copies it to the second pipe (which has its output closed), so it (the second cat) gets a SIGPIPE/EPIPE and exits (which closes the output of the first pipe).  At this point the first cat is still waiting for input, so if you type a second line, it copies that to the now closed first pipe and gets a SIGPIPE/EPIPE and exits</p>
"
"75069068","How can I understand the behavior of pipe, with varying data flow?","<p>My problem is a bit hard to explain properly as I do not understand fully the behavior behind it.
I have been working on pipe and pipelines in C, and I noticed some behavior that is a bit mysterious to me.</p>
<p>Let's take a few example: Let's try to pipe yes with head. (yes  | head). Even though I coded the behavior in a custom program, I don't understand how the pipe knows when to stop piping ? It seems two underlying phenomenons are causing this (maybe), the SIGPIPE and/or the internal size a pipe can take. How does the pipe stop piping, is it when it's full ? But the size of a pipe is way superior to 10 &quot;yes\n&quot; no ? And SIGPIPE only works when the end read/write is closed no ?</p>
<p>Also let's take another example, for example cat and ls: cat | ls or even cat | cat | ls.
It seems the stdin of the pipe is waiting for input, but how does it know when to stop, i.e. after one input ? What are the mechanism that permits this behavior?</p>
<p>Also can anyone provide me with others examples of these very specific behavior if there are any in pipes and pipelines so I can get an good overview of theses mechanism ?</p>
<p>In my own implementation, I managed to replicate that behavior using waitpid. However how does the child process itself know when to stop ? Is it command specific ?</p>
","<c><linux><unix><pipe><pipeline>","2023-01-10 11:09:22","138","0","3","75092984","<blockquote>
<p>How does the pipe stop piping, is it when it's full ?</p>
</blockquote>
<p>A pipe has several states:</p>
<ol>
<li>if you obtain the pipe through a call to <code>pipe(2)</code> (an unnamed pipe) both file descriptors are already open, so this doesn't apply to it (you start in point 2. below).  When you open a named pipe, your <code>open(2)</code> call (depending if you have open with <code>O_READ</code>, <code>O_WRITE</code>, or <code>O_RDWR</code>.  The pipe has two sides, the writer and the reader side.  When you open it, you attach to the sides, depending on how do you open it.  Well, up to here, the pipe blocks any <code>open(2)</code> call, until both sides have at least one process tied to them.  So, if you open a pipe and <code>read(2)</code> from it, then your open will be blocked, until other process has opened it to read.</li>
<li>once both extremes have it open, the readers (the process issuing a <code>read(2)</code> call) block when the pipe is empty, and the writers (the processes issuing a <code>write(2)</code> call) block whenever the write call cannot be satisfied due to fillin completely the pipe.  Old implementations of pipes used the filesystem to hold the data, and the data was stored only in the direct addressed disk blocks.  This meant (as there are 10 such blocks in an inode) that you normally had space in the pipe to hold 10 blocks, after that, the writers are blocked.  Later, pipes were implemented using the socket infrastructure in BSD systems, which allowed you to control the buffer size with <code>ioctl(2)</code> calls.  Today, IMHO, pipes use a common implementation, that is separate from sockets also.</li>
<li>When the processes close the pipe continues to work as said in point 2. above, until the number of readers/writers collapses to zero.  At that point, the pipe starts giving End Of File condition to all readers (this means <code>read(2)</code> syscall will return 0 bytes, without blocking) and error (cannot write to pipe) to writers.  In addition, the kernel sends a signal (which normally aborts the writer processes) <code>SIGPIPE</code>  to every process that has the pipe open for writing.  If you have not ignored that signal or you have not installed a signal handler for it, your process will die.  In this state, it's impossible to reopen the pipe again, until all processes have closed it.</li>
</ol>
<p>A common error is when you <code>pipe()</code> or you open a pipe with <code>O_RDWR</code>, and the other process closes its file descriptor, and you don't get anything indicating about the other's close call..... this is due to the thing that both sides of the pipe are still open (by the same process) so it will not receive anything because it can still write to the pipe.</p>
<p>Any other kind of misbehaviour could be explained if you had posted any code, but you didn't, so IMHO, thi answer is still incomplete, but the number of different scenarios is difficult to enumerate, so I'll be pendant of any update to your question with some faulty (or needed of explanation) code.</p>
"
"75048433","How to update credentials of Dataset (based on data flow) using service principal","<p>I am facing an issue while patching the credentials of a data set.</p>
<p>Our application is using the service principal to upload PowerBI reports to the respective PowerBI workspace. Most of the report is using Web Api sources, so we are patching the data source credentials using the below code, and it is working as expected.</p>
<pre><code>//Create UpdateDatasourceRequest to update datasource credentials                           
var credentials = new AnonymousCredentials();
var credentialDetails = new CredentialDetails(credentials, PrivacyLevel.None, EncryptedConnection.NotEncrypted);
UpdateDatasourceRequest req = new UpdateDatasourceRequest(credentialDetails);

//Execute Patch command to update datasource credentials
await client.Gateways.UpdateDatasourceAsync((Guid)gatewayId, (Guid)datasourceId, req, cancellationToken);
</code></pre>
<p>We recently created a Power BI report with data flow as a data source (please note that the data flow is not created by the service principal), and when we tried to refresh the dataset after publishing the report to workspace, it threw an error: &quot;Some of the data sources have missing credentials.&quot;</p>
<p>The credentials configuration is currently displayed in Power BI as follows:</p>
<p><a href=""https://i.stack.imgur.com/00T6O.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/00T6O.png"" alt=""enter image description here"" /></a></p>
<p>After going through the following tutorial, I used the code below to set the credentials.</p>
<p><a href=""https://www.stackoverflow.com/"">https://learn.microsoft.com/en-us/power-bi/developer/embedded/configure-credentials?tabs=sdk3</a> </p>
<pre><code>var gateway = pbiClient.Gateways.GetGatewayById(datasource.GatewayId);
var credentialsEncryptor = new AsymmetricKeyEncryptor(gateway.publicKey);
var credentialDetails = new CredentialDetails(
        credentials,
        PrivacyLevel.Private,
        EncryptedConnection.Encrypted,
        credentialsEncryptor);
</code></pre>
<p>However, because all of the data is in the cloud (Azure), obtaining the public key from the gateway is impossible.</p>
<p>I also tried the following code without encrypting the details, but it did not work. I received a Bad Request error and am pasting the exact error also.</p>
<pre><code>var delta = new UpdateDatasourceRequest
{
   CredentialDetails = new CredentialDetails
    {
      CredentialType = &quot;OAuth2&quot;,
      Credentials = &quot;{\&quot;credentialData\&quot;:[{\&quot;name\&quot;:\&quot;accessToken\&quot;, \&quot;value\&quot;:\&quot;&quot;+ result.AccessToken + &quot;\&quot;}]}&quot;,
      EncryptedConnection = &quot;Encrypted&quot;,
      EncryptionAlgorithm = &quot;None&quot;,
      PrivacyLevel = &quot;None&quot;
    }
};

await client.Gateways.UpdateDatasourceAsync((Guid)gatewayId, (Guid)datasourceId, delta, cancellationToken);
</code></pre>
<blockquote>
<p>{&quot;error&quot;:{&quot;code&quot;:&quot;DM_GWPipeline_Gateway_InvalidConnectionCredentials&quot;,&quot;pbi.error&quot;:{&quot;code&quot;:&quot;DM_GWPipeline_Gateway_InvalidConnectionCredentials&quot;,&quot;parameters&quot;:{},&quot;details&quot;:[{&quot;code&quot;:&quot;DM_ErrorDetailNameCode_UnderlyingErrorCode&quot;,&quot;detail&quot;:{&quot;type&quot;:1,&quot;value&quot;:&quot;-2147467259&quot;}},{&quot;code&quot;:&quot;DM_ErrorDetailNameCode_UnderlyingErrorMessage&quot;,&quot;detail&quot;:{&quot;type&quot;:1,&quot;value&quot;:&quot;The credentials provided for the PowerBI source are invalid. (Source at PowerBI.)&quot;}},{&quot;code&quot;:&quot;DM_ErrorDetailNameCode_UnderlyingHResult&quot;,&quot;detail&quot;:{&quot;type&quot;:1,&quot;value&quot;:&quot;-2147467259&quot;}},{&quot;code&quot;:&quot;Microsoft.Data.Mashup.CredentialError.DataSourceKind&quot;,&quot;detail&quot;:{&quot;type&quot;:1,&quot;value&quot;:&quot;PowerBI&quot;}},{&quot;code&quot;:&quot;Microsoft.Data.Mashup.CredentialError.DataSourceOriginKind&quot;,&quot;detail&quot;:{&quot;type&quot;:1,&quot;value&quot;:&quot;Web&quot;}},{&quot;code&quot;:&quot;Microsoft.Data.Mashup.CredentialError.DataSourceOriginPath&quot;,&quot;detail&quot;:{&quot;type&quot;:1,&quot;value&quot;:&quot;https://api.powerbi.com/powerbi/globalservice/v201606/clusterdetails&quot;}},{&quot;code&quot;:&quot;Microsoft.Data.Mashup.CredentialError.DataSourcePath&quot;,&quot;detail&quot;:{&quot;type&quot;:1,&quot;value&quot;:&quot;PowerBI&quot;}},{&quot;code&quot;:&quot;Microsoft.Data.Mashup.CredentialError.Reason&quot;,&quot;detail&quot;:{&quot;type&quot;:1,&quot;value&quot;:&quot;AccessForbidden&quot;}},{&quot;code&quot;:&quot;Microsoft.Data.Mashup.MashupSecurityException.DataSources&quot;,&quot;detail&quot;:{&quot;type&quot;:1,&quot;value&quot;:&quot;[{&quot;kind&quot;:&quot;PowerBI&quot;,&quot;path&quot;:&quot;PowerBI&quot;}]&quot;}},{&quot;code&quot;:&quot;Microsoft.Data.Mashup.MashupSecurityException.Reason&quot;,&quot;detail&quot;:{&quot;type&quot;:1,&quot;value&quot;:&quot;AccessForbidden&quot;}}],&quot;exceptionCulprit&quot;:1}}}</p>
</blockquote>
<p> </p>
<p>I'm missing something; any suggestions you have would be greatly appreciated.</p>
<p>Thanks in advance. </p>
","<powerbi><powerbi-embedded><powerbi-datasource><powerbi-api>","2023-01-08 14:13:50","627","1","1","75049160","<p>You can skip that bit for cloud data sources:</p>
<blockquote>
<p>If you're using cloud data sources, don't follow the next steps in
this section. Call Update Datasource to set the credentials by using
the gateway ID and data source ID that you obtained in step 1.</p>
</blockquote>
<p><a href=""https://learn.microsoft.com/en-us/power-bi/developer/embedded/configure-credentials?tabs=sdk3"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/power-bi/developer/embedded/configure-credentials?tabs=sdk3</a></p>
"
"75041423","reduce function not working in derived column in adf mapping data flow","<p>I am trying to create the derived column based on the condition that met the value and trying to do the summation of multiple matching column values dynamically. So I am using reduce function in ADF derived column mapping data flow. But the column is not getting created even the transformation is correct.</p>
<p>Columns from source</p>
<p><a href=""https://i.stack.imgur.com/rVjri.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rVjri.png"" alt=""enter image description here"" /></a></p>
<p>Derived column logic</p>
<p><a href=""https://i.stack.imgur.com/JFWGt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JFWGt.png"" alt=""enter image description here"" /></a></p>
<p>Derived column data preview without the new columns as per logic</p>
<p><a href=""https://i.stack.imgur.com/OU58H.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OU58H.png"" alt=""enter image description here"" /></a></p>
<p>I could see only the fields from source but not the derived column fields. If I use only the array($$) I could see the fields getting created.</p>
<p>Derived column data preview with logic only array($$)</p>
<p><a href=""https://i.stack.imgur.com/0F6Sb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0F6Sb.png"" alt=""enter image description here"" /></a></p>
<p>How to get the derived column with the summation of all the fields matching the condition?</p>
<p>We are getting data of 48 weeks forecast and the data to be prepared on monthly basis.</p>
<p>eg: Input data</p>
<p><a href=""https://i.stack.imgur.com/Osj2Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Osj2Q.png"" alt=""enter image description here"" /></a></p>
<p>Output data:</p>
<pre><code>JAN
----
506  -- This is for first record i.e. (94 + 105 + 109 + 103 + 95)
</code></pre>
","<azure-data-factory>","2023-01-07 15:13:47","98","0","1","75043406","<p>The problem is that the <code>array($$)</code> in the reduce function has only one element, so that the reduce function can not accumulate the content of the matching columns correctly.</p>
<p>You can solve this by using two derived columns and a data flow parameter as follows:</p>
<ol>
<li><p><strong>Create derived columns with pattern matching for each month-week</strong> you did it before, but put the reference $$ into the value field, instead of the reduce(...) function. <br><br />
This will create derived columns like jan0, jan1, etc. containing the copy of the original values. For example <strong>Week 0 (1 Jan - 7 Jan) =&gt; 0jan</strong> with value 95.
This step gives you a predefined set of column names for each week, which you can use to summarize the values with specific column names.
<br></p>
</li>
<li><p><strong>Define Data Flow parameters for each month</strong> containing the month-week column names in a string array, like this:<br><br />
<code>ColNamesJan=['0jan' ,'1jan', etc.] ColNamesFeb=['0feb' ,'1feb', etc.] and so on.</code><br><br />
You will use these column names in a reduce function to summarize the month-week columns to monthly column in the next step.</p>
</li>
<li><p><strong>Create a derived column for each month</strong>, which will contain the monthly totals, and use the following reduce function to sum the weekly values: <br><br />
<code>reduce(array(byNames($ColNamesJan)), 0, #acc + toInteger(toString(#item)),#result)</code><br />
<em>Replace the parameter name accordingly.</em></p>
</li>
</ol>
<p>I was able to summarize the columns dynamically with the above solution.<br />
Please let me know if you need more information (e.g. screenshots) to reproduce the solution.</p>
<p><strong>Update</strong> -- Here are the screenshots from my test environment.<br><br />
Data source (data preview):
<a href=""https://i.stack.imgur.com/B9tPI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B9tPI.png"" alt=""Data source"" /></a><br><br />
Derived columns with pattern matching (settings)<br />
<a href=""https://i.stack.imgur.com/oDSwN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oDSwN.png"" alt=""derived columns with pattern matching settings"" /></a><br><br />
Derived columns with pattern matching (data preview)<br />
<a href=""https://i.stack.imgur.com/KA1U8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KA1U8.png"" alt=""derived columns with pattern matching preview"" /></a><br />
Data flow parameter:<br />
<a href=""https://i.stack.imgur.com/mhVyM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mhVyM.png"" alt=""data flow parameter"" /></a><br><br />
Derived column for monthly sum (settings):<br />
<a href=""https://i.stack.imgur.com/OawNC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OawNC.png"" alt=""derived column for monthly sum settings"" /></a><br><br />
Derived column for monthly sum (data preview):<br />
<a href=""https://i.stack.imgur.com/SPFtA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SPFtA.png"" alt=""derived column for monthly sum data preview"" /></a></p>
"
"75036756","How to Load files with the same name in data flow Azure data factory","<p>I use data flow in Azure data factory And I set as source dataset files with the same name. The files have named “name_date1.csv” end “name_date2.csv”. I set path “name_*.csv”. I want that data flow load in sink db only data of  “name_date1”. How is it possible?</p>
","<azure-data-factory>","2023-01-06 22:41:24","79","0","1","75054320","<p>I have reproduced the above and able to get the desired file to sink using <code>Column to store file name</code> option in source options.</p>
<p>These are my source files in storage.</p>
<p><img src=""https://i.imgur.com/rzha8b2.png"" alt=""enter image description here"" /></p>
<p>I have given name_*.csv in wild card of source as same as you to read multiple files.</p>
<p><img src=""https://i.imgur.com/1K8HAr7.png"" alt=""enter image description here"" /></p>
<p>In source options, go to <code>Column to store file name</code> and give a name and this will store the file name of every row in new column.</p>
<p><img src=""https://i.imgur.com/LIwEoai.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.imgur.com/ajKE3WH.png"" alt=""enter image description here"" /></p>
<p>Then use <strong>filter transformation</strong> to get the row only from a particular file.</p>
<p><code>notEquals(instr(filename,'name_date1'),0)</code></p>
<p><img src=""https://i.imgur.com/3b57tMk.png"" alt=""enter image description here"" /></p>
<p>After this give your sink and you can get the rows from your desired file only.</p>
"
"75031644","Load files with the same name in data flow Azure data factory","<p>I use data flow in Azure data factory And I set as source dataset files with the same name. The files have named “name_date1” end “name_date2”. I want that data flow  load in sink db only data of file  “name_date1”.
How is it possible?</p>
","<azure-data-factory>","2023-01-06 13:36:32","120","0","1","75044787","<p>If you have multiple files being read in your ADF data flow source transformation, you can use the property called &quot;Column to store file name&quot; under Source Options. Provide a name to store the filename and it will become part of your metadata. You can then use a Filter transformation to include only source files with a specific filename by matching on that column that has the file name.</p>
"
"75031045","Problems with data flow in redux-toolkit, react","<p>In my productSlice, <code>createProduct</code> loads the product into the database, but often remains in the pending state.</p>
<p>It is similar with <code>deleteProduct</code>, it deletes, but remains in the pending state.</p>
<p>The <code>getProduct</code> renders always the data of the previous product.</p>
<p>After setting logger middleWare into my store, I have now more information about my crud - createProduct sometimes is running without problems. But then I have in action: <code>product/create/fulfilled</code> and directly after that:</p>
<blockquote>
<p>error state.product.push is not a function</p>
</blockquote>
<p>So it not jumps into next state.</p>
<p>The same happens with deleteProduct - action: <code>product/delete/fulfilled</code> and directly after that:</p>
<blockquote>
<p>error state.product.filter is not a function</p>
</blockquote>
<p>getProduct - prevState, action and next state have the correct data, but the text values are obviously rendered before getProduct starts</p>
<p>redux-slice:</p>
<pre><code>import {createSlice, createAsyncThunk} from '@reduxjs/toolkit'
import { RootState } from '../../app/store'
import productsService from './productsService'
import {UpdateProductData} from '../../pages/ProductEdit'
export interface Product{
    _id?:string,
    id?:string,
    image:string,
    title:string,
    producer:string,
    categories:string[],
    desc:string,
    price:string,
    currency:string,
    colors:string[],
    sizes:string[],
    inStock:boolean,
    createdAt:Date,
    updatedAt?:Date,
    accessToken?:string,
}
export interface InitialState{
    product:Product[],
    allProducts:Product[],
    isLoading:boolean,
    isSuccess:boolean,
    isError:boolean,
    message:string,
}
const initialState: InitialState ={
    product:[],
    allProducts:[],
    isLoading:false,
    isSuccess:false,
    isError:false,
    message:&quot;&quot;,
}
type AsyncThunkConfig = {
    state:RootState
}
export const createProduct = createAsyncThunk&lt;Product, FormData, AsyncThunkConfig&gt;('/product/create', async (productData, thunkAPI) =&gt; {
    try{
        const token:string = thunkAPI.getState().auth.user!.accessToken;
        return await productsService.createProduct(productData, token);
    } catch(error:any){
        const message =
        (error.response &amp;&amp;
          error.response.data &amp;&amp;
          error.response.data.message) ||
        error.message ||
        error.toString()
      return thunkAPI.rejectWithValue(message as string)
    }
})
export const updateProduct = createAsyncThunk&lt;Product[], UpdateProductData, AsyncThunkConfig&gt;('/product/update', async (updateProductData, thunkAPI)=&gt;{
    try{
        const token:string = thunkAPI.getState().auth.user!.accessToken;
        return await productsService.updateProduct(updateProductData, token);
    } catch(error:any){
        const message =
        (error.response &amp;&amp;
          error.response.data &amp;&amp;
          error.response.data.message) ||
        error.message ||
        error.toString()
      return thunkAPI.rejectWithValue(message as string)
    }
})

export const deleteProduct = createAsyncThunk&lt;Product, string, AsyncThunkConfig&gt;('product/delete', async (Id, thunkAPI)=&gt;{
    try{
        const token = thunkAPI.getState().auth.user!.accessToken;
        return await productsService.deleteProduct(Id, token);
    } catch(error:any){
        const message =
        (error.response &amp;&amp;
          error.response.data &amp;&amp;
          error.response.data.message) ||
        error.message ||
        error.toString()
      return thunkAPI.rejectWithValue(message as string)
    }
})
export const getProduct = createAsyncThunk&lt;Product[], string, AsyncThunkConfig&gt;('product/find', async (Id, thunkAPI)=&gt;{
    try{
        return await productsService.getProduct(Id);
    } catch(error:any){
        const message =
        (error.response &amp;&amp;
          error.response.data &amp;&amp;
          error.response.data.message) ||
        error.message ||
        error.toString()
      return thunkAPI.rejectWithValue(message as string)
    }
})
export const getAllProducts = createAsyncThunk&lt;Product[], void, AsyncThunkConfig&gt;('/product/findAll', async (_, thunkAPI)=&gt;{
    try{
        return await productsService.getAllProducts();
    } catch(error:any){
        const message =
        (error.response &amp;&amp;
          error.response.data &amp;&amp;
          error.response.data.message) ||
        error.message ||
        error.toString()
      return thunkAPI.rejectWithValue(message as string)
    }
})

export const productsSlice = createSlice({
    name:&quot;products&quot;,
    initialState,
    reducers:{
        reset:(state)=&gt;initialState,
    },
    extraReducers(builder) {
      builder
      .addCase(createProduct.pending, (state)=&gt;{
        state.isLoading = true; 
      })
      .addCase(createProduct.fulfilled, (state, action)=&gt;{
        state.isLoading = false;
        state.isSuccess = true;
        state.product.push(action.payload);
      })
      .addCase(createProduct.rejected, (state,action:any)=&gt;{
        state.isLoading = false;
        state.isError = true;
        state.message = action.payload;
      })
      .addCase(updateProduct.pending, (state)=&gt;{
        state.isLoading = true;
      })
      .addCase(updateProduct.fulfilled, (state, action)=&gt;{
        state.isLoading = false;
        state.isSuccess = true;
        state.product = action.payload;
      })
      .addCase(updateProduct.rejected, (state, action:any)=&gt;{
        state.isLoading = false;
        state.isError = true;
        state.message = action.payload;
      })
      .addCase(deleteProduct.pending, (state)=&gt;{
        state.isLoading = true;
      })
      .addCase(deleteProduct.fulfilled, (state, action)=&gt;{
        state.isLoading = false;
        state.isSuccess = true;
        state.product = state.product.filter((item)=&gt;item._id !== action.payload.id);
      })
      .addCase(deleteProduct.rejected, (state, action:any)=&gt;{
        state.isLoading = false;
        state.isError = true;
        state.message = action.payload;
      })
      .addCase(getProduct.pending, (state)=&gt;{
        state.isLoading = true;
      })
      .addCase(getProduct.fulfilled, (state, action)=&gt;{
        state.isLoading = false;
        state.isSuccess = true;
        state.product = action.payload;
      })
      .addCase(getProduct.rejected, (state, action:any)=&gt;{
        state.isLoading = false;
        state.isError = true;
        state.message = action.payload;
      })
      .addCase(getAllProducts.pending, (state)=&gt;{
        state.isLoading = true;
      })
      .addCase(getAllProducts.fulfilled, (state, action)=&gt;{
        state.isLoading = false;
        state.isSuccess = true;
        state.allProducts = action.payload;
      })
      .addCase(getAllProducts.rejected, (state, action:any)=&gt;{
        state.isLoading = false;
        state.isError = true;
        state.message = action.payload;
      })
    }
})

export const {reset} = productsSlice.actions;
export default productsSlice.reducer;
</code></pre>
<p>redux-service:</p>
<pre><code>import axios from 'axios';
import { UpdateProductData } from '../../pages/ProductEdit';
const API_URL = 'http://localhost:5001/api/products/';

const createProduct = async (productData:FormData, token:string)=&gt;{
    const config = {
        headers:{
            'Content-Type':&quot;multipart/form-data&quot;,
            token: `Bearer ${token}`,
        }
    }
    const response =  await axios.post(API_URL, productData, config);
    console.log(response.data);//immediatly correct data
    return response.data;
}
const updateProduct = async (updateProductData:UpdateProductData, token:string)=&gt;{
    const config ={
        headers:{
            'Content-Type':&quot;multipart/form-data&quot;,
            token:`Bearer ${token}`
        }
    }
    const response = await axios.put(API_URL+updateProductData.id, updateProductData.productData, config);
    console.log(response.data);
    return response.data;
}
const deleteProduct = async (id:string, token:string)=&gt;{
    const config = {
        headers:{
            token:`Bearer ${token}`
        }
    }
    const response = await axios.delete(API_URL + id, config);
    console.log(response.data);
    return response.data;
}
const getProduct = async (id:string)=&gt;{
    const getUrl = `find/${id}`;
    const response = await axios.get(API_URL+getUrl);
    console.log(response.data)
    return response.data;
}
const getAllProducts = async ()=&gt;{
    const response = await axios.get(API_URL+ 'find');
    console.log(response.data);
    return response.data;
}
const productsService = {
    createProduct,
    updateProduct,
    deleteProduct,
    getProduct,
    getAllProducts
}
export default productsService
</code></pre>
<p>store:</p>
<pre><code>import { configureStore, ThunkAction, Action} from '@reduxjs/toolkit';
import authReducer from '../features/authSlice';
import userReducer from '../features/user/userSlice'
import productReducer from '../features/products/productsSlice';
import descriptionItemReducer from '../features/descriptionItems/descriptionItemSlice';
import sliderItemsReducer from '../features/sliderItems/sliderItemSlice';
import storage from 'redux-persist/lib/storage'
import {persistReducer} from 'redux-persist'

const persistConfig = {
  key: 'root',
  version: 1,
  storage,
}
const persistedUserReducer = persistReducer(persistConfig,userReducer) 
const persistedProductReducer = persistReducer(persistConfig, productReducer)
const persistedSliderReducer = persistReducer(persistConfig, sliderItemsReducer)
export const store = configureStore({
  reducer: {
    auth:authReducer,
    user:persistedUserReducer,
    products:persistedProductReducer,
    descriptionItem:descriptionItemReducer,
    sliderItems:persistedSliderReducer,
  },
});
export type AppDispatch = typeof store.dispatch;
export type RootState = ReturnType&lt;typeof store.getState&gt;;
export type AppThunk&lt;ReturnType = void&gt; = ThunkAction&lt;
  ReturnType,
  RootState,
  unknown,
  Action&lt;string&gt;
&gt;;
</code></pre>
<p>I add the code from the backend:</p>
<pre><code>import {Router, Request, Response} from 'express';
const productsRouter = Router();
import {verifyTokenAndAdmin} from '../middleware/jwtVerify';
import Products from '../models/products';
import upload from '../utils/multer';
const cloudinary = require('../utils/cloudinary');
import path from 'path'
productsRouter.post('/', upload.single('image'), verifyTokenAndAdmin, async (req:Request, res:Response)=&gt;{
    console.log(req.file);
    console.log(req.body);
    let fileUrl = req.file!.path.replace(/\\/g, &quot;/&quot;);
    console.log(fileUrl);
    try{
        const uploadResult = await cloudinary.uploader.upload(fileUrl, {
            upload_preset: &quot;webshop_ts_mern&quot;,
            resource_type: &quot;auto&quot;,
        })
        
        const newProducts = new Products({
            cloudinary_id: uploadResult.public_id,
            title: req.body.title,
            producer: req.body.producer,
            categories: JSON.parse(req.body.categories).split(' '),
            desc: req.body.desc,
            price: req.body.price,
            currency:req.body.currency,
            colors:JSON.parse(req.body.colors).split(' '),
            sizes: JSON.parse(req.body.sizes).split(' '),
            inStock: req.body.inStock,
            image: uploadResult.secure_url,

        })
        console.log(newProducts);
        const savedproducts = await newProducts.save();
        res.status(200).json(savedproducts);
    } catch(error){
        res.status(403)
        console.log(error);
        throw new Error(&quot;Action failed&quot;);
    }
});
//update
productsRouter.put('/:id',upload.single(&quot;image&quot;), verifyTokenAndAdmin, async (req:Request, res:Response)=&gt;{
    console.log(req.file);
    console.log(req.body)

    try{
        let updatedProducts = await Products.findById(req.params.id);
        if(req.file){
        await cloudinary.uploader.destroy(updatedProducts?.cloudinary_id);
        }
       let result;
        if(req.file){
            let fileUrl = req.file!.path.replace(/\\/g, &quot;/&quot;);
        result = await cloudinary.uploader.upload(fileUrl, {
            upload_preset: &quot;webshop_ts_mern&quot;,
            resource_type: &quot;auto&quot;,
        })
        }
        const updatedData = {
            title: req.body.title || updatedProducts!.title,
            producer: req.body.producer || updatedProducts!.producer,
            categories: JSON.parse(req.body.categories) || updatedProducts!.categories,
            desc: req.body.desc || updatedProducts!.desc,
            price: req.body.price || updatedProducts!.price,
            currency: req.body.currency || updatedProducts!.currency,
            colors: JSON.parse(req.body.colors) || updatedProducts!.colors,
            sizes: JSON.parse(req.body.sizes) || updatedProducts!.sizes,
            inStock: req.body.inStock || updatedProducts!.inStock,
            cloudinary_id: result ? result.public_id : updatedProducts!.cloudinary_id,
            image: result ? result.secure_url : updatedProducts!.image,
        }
        console.log(updatedData);
        updatedProducts = await Products.findByIdAndUpdate(req.params.id, updatedData, {
            new:true,
        })
        res.status(200).json(updatedProducts);
    } catch(error){
        res.status(404)
        console.log(error);
        throw new Error('Not found')
    }
});
//delete
productsRouter.delete('/:id', verifyTokenAndAdmin, async (req:Request, res:Response)=&gt;{
    try{
        let deleteProducts = await Products.findById(req.params.id);
        await cloudinary.uploader.destroy(deleteProducts!.cloudinary_id);
         await deleteProducts!.remove();
        res.status(200).json(&quot;Produkt wurde gelöscht&quot;);
    } catch(error){
        res.status(404)
        throw new Error(&quot;Nicht gefunden&quot;)
    }
});
//get
productsRouter.get('/find/:id', async (req:Request, res:Response)=&gt;{
    try{
        const products = await Products.findById(req.params.id);
        res.status(200).json(products)
    } catch(error){
        res.status(404)
        throw new Error(&quot;Nicht gefunden&quot;);
    }
});
//get All
productsRouter.get('/find/', async (req:Request, res:Response)=&gt;{
        try{
        const allProducts = await Products.find()
        res.status(200).json(allProducts);
    } catch(error){
        res.status(404)
        throw new Error(&quot;Not found&quot;);
    }
})

export default productsRouter;
</code></pre>
","<javascript><node.js><reactjs><typescript><redux>","2023-01-06 12:38:25","199","0","1","75039205","<p>In your update and delete controllers you have written optional operator incorrectly, you have written <code>!.</code> instead it should be <code>?.</code>. I have updated the code for you.</p>
<pre><code>//update
productsRouter.put('/:id',upload.single(&quot;image&quot;), verifyTokenAndAdmin, async (req:Request, res:Response)=&gt;{
  try{
      let updatedProducts = await Products.findById(req.params.id);
      if(req.file){
      await cloudinary.uploader.destroy(updatedProducts?.cloudinary_id);
      }
     let result;
      if(req.file){
          let fileUrl = req.file?.path.replace(/\\/g, &quot;/&quot;);
      result = await cloudinary.uploader.upload(fileUrl, {
          upload_preset: &quot;webshop_ts_mern&quot;,
          resource_type: &quot;auto&quot;,
      })
      }
      const updatedData = {
          title: req.body.title || updatedProducts?.title,
          producer: req.body.producer || updatedProducts?.producer,
          categories: JSON.parse(req.body.categories) || updatedProducts?.categories,
          desc: req.body.desc || updatedProducts?.desc,
          price: req.body.price || updatedProducts?.price,
          currency: req.body.currency || updatedProducts?.currency,
          colors: JSON.parse(req.body.colors) || updatedProducts?.colors,
          sizes: JSON.parse(req.body.sizes) || updatedProducts?.sizes,
          inStock: req.body.inStock || updatedProducts?.inStock,
          cloudinary_id: result ? result.public_id : updatedProducts?.cloudinary_id,
          image: result ? result.secure_url : updatedProducts?.image,
      }
      console.log(updatedData);
      updatedProducts = await Products.findByIdAndUpdate(req.params.id, updatedData, {
          new:true,
      })
      res.status(200).json(updatedProducts);
  } catch(error){
      res.status(404)
      console.log(error);
      throw new Error('Not found')
  }
});
//delete
productsRouter.delete('/:id', verifyTokenAndAdmin, async (req:Request, res:Response)=&gt;{
  try{
      let deleteProducts = await Products.findById(req.params.id);
      await cloudinary.uploader.destroy(deleteProducts?.cloudinary_id);
       await deleteProducts?.remove();
      res.status(200).json(&quot;Produkt wurde gelöscht&quot;);
  } catch(error){
      res.status(404)
      throw new Error(&quot;Nicht gefunden&quot;)
  }
});
</code></pre>
"
"75003754","How to add leading zeros in ADF data flow from the expression builder","<p>How to add leading zeros in ADF data flow from the expression builder</p>
<p>For example – have column with numeric value as “000001” but it is coming as 1 only in SQL DB , if I put in  entire value in single quotes it is coming but I need dynamic way of implementation with out hard coding.</p>
","<sql-server><azure-data-factory>","2023-01-04 09:58:04","278","0","1","75005585","<p>I agree with <strong>@Larnu</strong>'s comments that even if we give <code>00001</code> to an int type column it will give as <code>1</code> only.</p>
<p>So, we have to give those in single quotes (<code>'00001'</code>) to use like that or import the incoming data as string instead of int.</p>
<p>As you are using ADF dataflow, if you want to use the <code>00001</code>, you can generate those using <strong>derived column transformation</strong> from SQL source. But this depends on your requirement like how your leading 0's varies. So, use according to it.</p>
<p><strong>Sample demo:</strong></p>
<pre><code>concat('0000', toString(id))
</code></pre>
<p><img src=""https://i.imgur.com/JFUhdii.png"" alt=""enter image description here"" /></p>
<p><strong>Result:</strong></p>
<p><img src=""https://i.imgur.com/x0OnWNn.png"" alt=""enter image description here"" /></p>
<p>Use that column as per your requirement, after that you can convert it back to the same input by <code>toInteger(id)</code>.</p>
"
"74981694","data factor alter row iif condition error: data flow expression should return boolean","<p>I have provided below if condition in update if block. I am getting error: data flow expression should return Boolean.
iif(Column_8 == 'AutomÃ¡tica','Automatic','Manual')</p>
<p>I tried updating rows present in a column based on a condition.</p>
","<azure-data-factory>","2023-01-02 11:11:18","83","1","1","74990029","<ul>
<li><p>You have to specify only condition in the alter row update if i.e., there has to be a Boolean value for these fields. The expression that you have used returns a string value and hence the error.</p>
</li>
<li><p>I have taken the following data in my table:</p>
</li>
</ul>
<p><img src=""https://i.imgur.com/nRaaURD.png"" alt=""enter image description here"" /></p>
<ul>
<li>If the data you want to update is your dataflow source, then use derived column transformation with the same <code>iff</code> condition.</li>
</ul>
<pre><code>iif(col1=='AutomÃ¡tica','Automatic','Manual')
</code></pre>
<p><img src=""https://i.imgur.com/mz7aZBZ.png"" alt=""enter image description here"" /></p>
<ul>
<li>If this table is your sink and you want to update the column values based on the condition, then try using pre-SQL scripts/ post-SQL scripts.</li>
<li><code>Update if</code> will update the entire sink row based on the source row data (after checking the condition given in alter row). It would not update from any foreign value (as you have tried). So, using the following post SQL scripts to update the values as per requirement.</li>
</ul>
<pre><code>Update demo set col1='Manual' where col1!= 'AutomÃ¡tica';
Update demo set col1='Automatic' where col1='AutomÃ¡tica';
</code></pre>
<p><img src=""https://i.imgur.com/RmqhSTN.png"" alt=""enter image description here"" /></p>
<ul>
<li>The final data in table would be as shown below (Id with value 4 and 5 are inserted first and then post SQL script will be applied).</li>
</ul>
<p><img src=""https://i.imgur.com/Y2jOuRr.png"" alt=""enter image description here"" /></p>
"
"74922655","Handling errors/exceptions in spring cloud data flow streams","<p>One of the components in our stream is throwing heap out of memory error, when input file is larger than a certain limit. While we are working to fix this issue, I would like to know, if such errors can be caught so that we can log or send notification accordingly.</p>
<p>I have seen couple of examples for error handling using imperative approach. I would like to know how error handling at application level can be achieved through an error channel using functional approach and log the error info. Please also provide links if any to resources on this.</p>
<p>Thank you.</p>
","<spring-cloud-stream><spring-cloud-dataflow>","2022-12-26 17:36:44","93","0","1","74932890","<p>The error handling available is provided by Spring Cloud Stream as described in the <a href=""https://docs.spring.io/spring-cloud-stream/docs/current/reference/html/spring-cloud-stream.html#spring-cloud-stream-overview-error-handling"" rel=""nofollow noreferrer"">Error Handling</a> section of the reference guide.</p>
<p>There is not a mechanism built-in to Dataflow to detect and react to errors in an executing stream.</p>
"
"74897070","How to use timestamp type parameters set in pipeline with timestamp type in data flow","<p>I can't send the question due to some mysterious error, so I'll share a screenshot of the question.</p>
<p>Can anyone help me solve this?</p>
<p><a href=""https://i.stack.imgur.com/dc58O.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dc58O.png"" alt=""ss of the error"" /></a></p>
","<azure><azure-data-factory>","2022-12-23 07:17:18","116","0","1","74918537","<p>I have reproduced the above and got same error when the Expression checkbox in checked.</p>
<p><img src=""https://i.imgur.com/U9NfF0V.png"" alt=""enter image description here"" /></p>
<p><strong>Remove the Expression checkbox check</strong> in dataflow pipeline assignment and pass it as a string. Now it won't give the error.</p>
<p><img src=""https://i.imgur.com/4Li5MHJ.png"" alt=""enter image description here"" /></p>
<p>It will take the Dataflow parameter like this.</p>
<p><img src=""https://i.imgur.com/MbdMvr2.png"" alt=""enter image description here"" /></p>
<p>Also, along with the Date time string pass the format in <code>toTimestamp()</code> function to avoid null values.</p>
<p><strong>This is my sample input data:</strong></p>
<p><img src=""https://i.imgur.com/SXfCuT6.png"" alt=""enter image description here"" /></p>
<p><strong>sample filter condition:</strong></p>
<pre><code>toTimestamp(start_date,'yyyy-MM-dd\'T\'HH:mm:ss')
</code></pre>
<p><img src=""https://i.imgur.com/ynXG3Cz.png"" alt=""enter image description here"" /></p>
<p><strong>Filtered Result:</strong></p>
<p><img src=""https://i.imgur.com/K7MPZMi.png"" alt=""enter image description here"" /></p>
"
"74885230","How to represent data flow when the same data is exchanged between entity and process?","<p>I have an ecommerce where users can ask questions on the product and the users who bought it can answer the questions and also vote on the questions like amazon , which means that the system will technically send all answers and votes to the user and the user will send answers and votes also.</p>
<p>So the same data is exchanged between the system and the customer, how can I represent it in a context diagram? I added an arrow head in both ends , is this correct?</p>
<p><a href=""https://i.stack.imgur.com/ekw8P.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ekw8P.png"" alt=""enter image description here"" /></a></p>
","<modeling><diagram><dataflow-diagram><dfd>","2022-12-22 07:20:19","32","1","1","75230540","<p>Dataflow diagrams frequently use unidirectional flows between processes or between processes and entities, but bidirectional flows with an arrow on both sides are a common practice that is perfectly valid.</p>
<p>The authors who popularised the two mainstream variants of DFD modelling <a href=""https://online.visual-paradigm.com/knowledge/software-design/gane-sarson-dfd-tutorial/"" rel=""nofollow noreferrer"">Gane &amp; Sarson</a> or <a href=""https://online.visual-paradigm.com/knowledge/software-design/dfd-tutorial-yourdon-notation/"" rel=""nofollow noreferrer"">Yourdon &amp; DeMarco</a>), both refer to it in their foundational books: Yourdon uses it in several examples, Gane and Sarson even recommend it for keeping diagrams simple.</p>
"
"74733156","How use output of data flow in the copy data activty in azure data factory","<p>I have a excel file which I transform in a azure data flow in adf. I have added some column and transformed some values. As next step I want to copy the new data into a cosmos db. How can I achieve this? It's not clear how do I get the result of the data flow into the copyData activity. I have a sink in the data flow which will store the transformed data into a csv. As I understand the adf will create multiple files for performance reason. Or is there a way to make the changes &quot;on the fly&quot; and work with the transformed file further</p>
<p>Thanks</p>
","<azure><azure-data-factory>","2022-12-08 16:07:27","471","0","1","74739706","<ul>
<li>If your sink is <code>Cosmos DB for No SQL</code>, then there is a direct sink connecter available in azure dataflows. After applying your transformations, you can create a dataset and directly move the data.</li>
</ul>
<p><img src=""https://i.imgur.com/RBnLWly.png"" alt=""enter image description here"" /></p>
<ul>
<li>If your sink is not for No SQL, then as you have done, write your data as csv files to your storage account. And if you choose to write the data to a single file, you can choose the <code>Output to single file</code> option in sink settings and give a filename.</li>
</ul>
<p><img src=""https://i.imgur.com/AtimP8m.png"" alt=""enter image description here"" /></p>
<ul>
<li>You can directly select this file to copy to your sink. But if you already have data written as multiple files in a folder, you can use wild card path option as shown below:</li>
</ul>
<p><img src=""https://i.imgur.com/pcwCuC1.png"" alt=""enter image description here"" /></p>
"
"74686006","How do you filter data flow in azure synapse?","<p>I have created a dataflow using row_number and I want to extract only the values where row_number = 1, but I can't get it to work. Please tell me how to do this.</p>
<p><a href=""https://i.stack.imgur.com/gOW4n.png"" rel=""nofollow noreferrer"">image</a></p>
<p>#azure #synapse #dataflow</p>
","<azure><azure-synapse>","2022-12-05 09:34:13","127","0","1","74697705","<blockquote>
<p>I want to extract only the values where row_number = 1</p>
</blockquote>
<p><strong>To filter rows based on column value we have two ways:</strong></p>
<ol>
<li>Filter</li>
<li>Conditional split</li>
</ol>
<p><strong>My Sample data after windows transformation</strong></p>
<p><img src=""https://i.imgur.com/fbDxF17.png"" alt=""enter image description here"" /></p>
<blockquote>
<p><strong>Filter transformation</strong></p>
</blockquote>
<p>In <strong>Filter settings</strong> give <code>Filter on</code> as <strong>column name with particular value.</strong>
Here I provided <code>RowNumber == 1</code>  so it will only filter rows where RowNumber =1.</p>
<p><img src=""https://i.imgur.com/zRhNs1A.png"" alt=""enter image description here"" /></p>
<p><strong>Output of Filter transformation</strong></p>
<p><img src=""https://i.imgur.com/24571L7.png"" alt=""enter image description here"" /></p>
<blockquote>
<p><strong>Conditional split</strong></p>
</blockquote>
<p>In <strong>Conditional split settings</strong> give <code>Split condition</code> as <strong>column name with particular value.</strong>
Here I provided <code>RowNumber == 1</code>  so it will only filter rows where RowNumber =1.
<img src=""https://i.imgur.com/NR36VXk.png"" alt=""enter image description here"" /></p>
<p><strong>Output of Conditional split transformation</strong></p>
<p><img src=""https://i.imgur.com/B3tYjxF.png"" alt=""enter image description here"" /></p>
"
"74685196","How to synchronize python and c sharp program running in windows?","<p>I am currently working in a project where I need to sychronise data between python and c sharp.I need to label data from c sharp using python machine learning program. To label the data, I am using timestamp from both the application and based on the common timestamp, I am labelling the data.</p>
<p>Python program is running every 0.5 to 1.5 sec and C sharp program is running 10 times every 1 sec. Since the two process are running differently, I know there is some time lag. So labelling the data using the timestamp is not much accurate. I want to analyse the time lag properly. For this I am looking for options of real time synchronization between the two programs. I have looked into sockets but I think there is a better way using IPC. I donot know much about this.</p>
<p>I am thinking to create a shared variable between python and c#. Since python is slower, I will update that variable using python and read that variable from c# program. so same variable instance on both the program would tell us that they are synchronized perfectly. So I can look the value of this variable instead of timestamp for labelling the data. I am thinking that this might solve the issue. Please let me know what would be the best optimal solution to minimize the time difference lag between the two program.</p>
<p>since these are complex projects, I cannot implement them in a single program. I need to find a way to synchronize these two programs.</p>
<p>Any suggestions would be appreciated. Thank you.</p>
<p>I tried working with socket programming but they were not that good and a bit complex. So I am now thinking about IPC but still not sure which is the best way.</p>
","<python><c#><synchronization><data-synchronization>","2022-12-05 08:22:23","35","0","1","75196651","<p>First of all, I implemented a socket in C# program so that I get data from the socket. Then I implemented multiprocessing in python. One process will request to the socket and another process will work for ML model. I was able to achieve the synchronization using the multiprocessing module. I used multiprocessing.Event() to wait for the event from another process. You can also look into shared variables in python using multiprocessing.Value, multiprocessing.Array, multiprocessing.Event.</p>
"
"74683285","How to Unescape a character in snowflake during data ingestion from CSV to Snowflake table","<p>I have a process that automatically ingests CSV data into Snowflake Tables from an external stage. CSV File that has the delimiter as pipe (vertical bar or | )</p>
<p>But one cell has data that contains the Escape character itself (which is backslash - &quot;&quot;) and immediately followed by the Column Delimiter (which is |)</p>
<p>Now the ingestion process is assuming that the delimiter (&quot;|&quot;) is part of the data itself because it is immediately followed by the &quot;&quot; and due to this, column values are shifting left causing erratic data in the table.</p>
<p>The data in CSV looks like this</p>
<p>Column_1|Column_2|Column_3 <br />
ABC .........     |DEF..........\ |EFG</p>
<p>When this gets ingested into the table it looks like this</p>
<p>Column_1    Column_2    Column_3 <br />
ABC..........           DEF......|EFG   NULL</p>
<p>Ideally, it should have got ingested as</p>
<p>Column_1    Column_2    Column_3 <br />
ABC..........           DEF.........\         EFG</p>
<p>Note: I can not alter the data as it has to flow from upstream systems and I need to ingest the data as is (even though it looks like a genuine data issue)</p>
<p>Could someone help me here as to which property of the File Format should I use in order for  ingesting this data as is?</p>
<p>Thank you in advance!</p>
","<csv><snowflake-cloud-data-platform><file-format><data-integration>","2022-12-05 04:14:04","66","0","1","74689951","<p>Setting the <code>ESCAPE_UNENCLOSED_FIELD = NONE</code> for the file_format configuration might help to achieve the desired result. The entire file format should like:</p>
<pre><code>  create or replace file format my_csv_format
  type = csv
  field_delimiter = '|'
  skip_header = 1
  null_if = ('NULL', 'null')
  ESCAPE_UNENCLOSED_FIELD = NONE
  empty_field_as_null = true
  ERROR_ON_COLUMN_COUNT_MISMATCH = false
  compression = auto;
</code></pre>
"
"74629505","How do I pull the last modified file with data flow in azure data factory?","<p>I have files that are uploaded into an onprem folder daily, from there I have a pipeline pulling it to a blob storage container (input), from there I have another pipeline from blob (input) to blob (output), here is were the dataflow is, between those two blobs. Finally, I have output linked to sql. However, I want the blob to blob pipeline to pull only the file that was uploaded that day and run through the dataflow. The way I have it setup, every time the pipeline runs, it doubles my files. I've attached images below</p>
<p>[![Blob to Blob Pipeline][1]][1]</p>
<p>Please let me know if there is anything else that would make this more clear
[1]: <a href=""https://i.stack.imgur.com/24Uky.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/24Uky.png</a></p>
","<azure><etl><azure-data-factory>","2022-11-30 14:34:45","322","0","2","74632422","<p>I was able to solve this by selecting &quot;Delete source files&quot; in dataflow. This way the the first pipeline pulls the new daily report into the input, and when the second pipeline (with the dataflow) pulls the file from input to output, it deletes the file in input, hence not allowing it to duplicate <a href=""https://i.stack.imgur.com/FUvje.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FUvje.png"" alt=""Dataflow"" /></a></p>
"
"74629505","How do I pull the last modified file with data flow in azure data factory?","<p>I have files that are uploaded into an onprem folder daily, from there I have a pipeline pulling it to a blob storage container (input), from there I have another pipeline from blob (input) to blob (output), here is were the dataflow is, between those two blobs. Finally, I have output linked to sql. However, I want the blob to blob pipeline to pull only the file that was uploaded that day and run through the dataflow. The way I have it setup, every time the pipeline runs, it doubles my files. I've attached images below</p>
<p>[![Blob to Blob Pipeline][1]][1]</p>
<p>Please let me know if there is anything else that would make this more clear
[1]: <a href=""https://i.stack.imgur.com/24Uky.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/24Uky.png</a></p>
","<azure><etl><azure-data-factory>","2022-11-30 14:34:45","322","0","2","74638009","<blockquote>
<p><strong>I want the blob to blob pipeline to pull only the file that was uploaded that day and run through the dataflow.</strong></p>
</blockquote>
<p>To achieve above scenario, you can use <code>Filter by last Modified date</code> by passing the dynamic content as below:</p>
<ul>
<li><code>@startOfDay(utcnow())</code> : It will take start of the day for the current timestamp.</li>
<li><code>@utcnow()</code> : It will take current timestamp.</li>
</ul>
<p><img src=""https://i.imgur.com/RbPV2HK.png"" alt=""enter image description here"" /></p>
<p>Input and Output of Get metadata activity: (Its filtering file for that day only)</p>
<p><img src=""https://i.imgur.com/XOPND59.png"" alt=""enter image description here"" /></p>
<p>If the files are multiple for particular day, then you have to use for each activity and pass the output of Get metadata activity to foreach activity as</p>
<pre><code>@activity('Get Metadata1').output.childItems
</code></pre>
<p><img src=""https://i.imgur.com/XzpJeDo.png"" alt=""enter image description here"" /></p>
<p>Then add Dataflow activity in Foreach and create source dataset with filename parameter</p>
<p><img src=""https://i.imgur.com/pimGawm.png"" alt=""enter image description here"" /></p>
<p>Give filename parameter which is created as dynamic value in filename
<img src=""https://i.imgur.com/vJZwasg.png"" alt=""enter image description here"" /></p>
<p>And then pass source parameter filename as <code>@item().name</code>
<img src=""https://i.imgur.com/27M1OLs.png"" alt=""enter image description here"" /></p>
<p>It will run dataflow for each file get metadata is returning.</p>
"
"74595924","SSIS - data flow bug when running from a parent package","<p>I'm facing a very weird error with SSIS and can't seem to find the reason.</p>
<p>I have around 20 packages which run well when I run them manually one by one. however, when I run them through a parent package (master.dtsx) which calls each of those 20 packages individually one after another, I am getting all sorts of issues.</p>
<p>Main issue is this: I get all green checkmarks on the package itself:
<a href=""https://i.stack.imgur.com/5SR2Z.png"" rel=""nofollow noreferrer"">FULL SSIS SCREENSHOT</a></p>
<p>but the DATA FLOW is always wrong, see below:
<a href=""https://i.stack.imgur.com/0Inzi.png"" rel=""nofollow noreferrer"">DATAFLOW SCREENSHOT</a></p>
<p>And some of the packages simply fail without any error but they are all ok if I run them manually straight after.</p>
<p>I have tried to run the MASTER package through the sql agent job, but that gives me a completely different error:</p>
<pre><code>Load_crmcci__Master:Error: The requested OLE DB provider MSOLEDBSQL is not registered. If the 64-bit driver is not installed, run the package in 32-bit mode. Error code: 0x00000000.
An OLE DB record is available.  Source: &quot;Microsoft OLE DB Service Components&quot;  Hresult: 0x80040154  Description: &quot;Class not registered&quot;.
</code></pre>
<p>Excuse me if this sounds a dumb question but I have not been able to fix this issue for several days now on my own. so thanks a lot in advance.</p>
","<sql-server><visual-studio><ssis>","2022-11-28 04:17:18","27","0","1","74607383","<p>my issue was that the proper provider (DRIVER : MSOLEDBSQL) was not installed on the sql server. I've installed and ran the project again, and it completed all properly.</p>
"
"74553943","Benefits of using spring data flow over spring batching","<p>I am working on evaluating the technologies we are using for processing the data of a large file and writing it to the database.</p>
<p>I need to know what are the advantages and disadvantages of using the  using <strong>spring data flow</strong> over <strong>spring batch</strong> in terms of handling large files very fast without increasing memory and without breaking them to batches and picking up pause process</p>
<p>I need to know which technology is better</p>
","<spring><spring-batch><spring-cloud-dataflow>","2022-11-23 23:16:05","133","0","1","74556064","<p>Spring Batch and Spring Cloud Dataflow are not at the same level of abstraction to be compared to each other.</p>
<p>Spring Batch is a batch processing framework that provides features to create and run batch jobs. Spring Cloud Dataflow on the other hand is a tool that allows you to run streaming and ephemeral apps, including, but not limited to Spring Batch jobs. What SCDF brings to the table compared to using only Spring Batch is the ability to:</p>
<ul>
<li>Run, stop, restart batch jobs from a Web UI, the CLI and an API</li>
<li>Inspect job logs from the Web UI</li>
<li>Inspect job metrics and tracing from the Web UI</li>
<li>Compose batch jobs into composed tasks to create advanced workflows</li>
<li>Upgrade/Downgrade batch applications in a seamless, production grade way</li>
<li>And many <a href=""https://dataflow.spring.io/docs/feature-guides/batch/"" rel=""nofollow noreferrer"">other features</a></li>
</ul>
<p>So it is not an &quot;either or&quot; question or &quot;which is better&quot;, those are complementary tools that you can use in combination to deploy a modern batch infrastructure.</p>
"
"74545888","React Parent and Child component data flow","<p>At loss when it comes to a reusable component I am making in React. The purpose of the component is to get API data and send it to parent where the data in mapped in cards. The component should also be able to receive a function call from the parent that is used to 'load more' data and props which to set whether I search old / new data.</p>
<p>I haven't been able to figure the best way to achieve this or whether I should do it that I through props.children I would display the cards inside the child, hence no need to send data upwards.</p>
<p>Parent:</p>
<pre><code>function Page() {

    const [eventData, setEventData] = useState([]);

    const EventCardUpcoming = () =&gt; {
        const filterRef = useRef(null);

        const handleClick = () =&gt; {
            filterRef.current.callLoad();
        };

        const setData = () =&gt; {
            setEventData()
        }

        return (
            &lt;InfiniteScroll
                pageStart={0}
                hasMore={true || false}
                loadMore={handleClick}
            &gt;
                &lt;button onClick={handleClick}&gt;Load more&lt;/button&gt;
                &lt;Row className=&quot;py-4&quot;&gt;
                    &lt;div&gt;
                        &lt;EventFiltering
                            OrganizerId={&quot;1&quot;}
                            ref={filterRef}
                            sendData={setData()}
                        /&gt;
                    &lt;/div&gt;
                    {eventData ? 
                    &lt;div&gt;
                        {eventData.map((v, i) =&gt; (
                            &lt;Col key={i} xs={12} sm={12} md={6} lg={4} xl={3}&gt;
                                &lt;div className=&quot;d-flex justify-content-center&quot;&gt;
                                    &lt;&gt;
                                        &lt;EventCard eventData={v} /&gt;
                                    &lt;/&gt;
                                &lt;/div&gt;
                            &lt;/Col&gt;
                        ))}
                    &lt;/div&gt;
                    : null}
                &lt;/Row&gt;
            &lt;/InfiniteScroll&gt;
        );
    };
    return (
        &lt;Container&gt;
            &lt;EventCardUpcoming /&gt;
        &lt;/Container&gt;
    );
}

export default Page;

</code></pre>
<p>Filtering child component:</p>
<pre><code>const Filtering = forwardRef((props, ref, { sendData }) =&gt; {

    const staticOrganizer = props.OrganizerId; //API configurable option
    
    const paramLimit = 4;
    const [eventData, setEventData] = useState([]);

    let params = {
        org: staticOrganizer,
        limit: paramLimit,
    };

    useImperativeHandle(ref, () =&gt; ({
        callLoad() {
            handleLoadMore(); //load more data from function called in parent
        },
    }));

    useEffect(() =&gt; {
        sendData(data); //send data to parent when more is fecthed
    }, [eventData]);

    const handleLoadMore = () =&gt; {
        APIGet(&quot;getData&quot;, params).then((res) =&gt; {
            if (res.data.body.length &gt; 0) {
                setEventData((eventData) =&gt; eventData.concat(res.data.body));
            }
        });
    };

        return (
            &lt;Container&gt;
                &lt;div&gt;
                    &lt;Dropdowns /&gt;
                &lt;/div&gt;
            &lt;/Container&gt;
        );
    }
);

export default Filtering;
</code></pre>
","<reactjs><parent-child>","2022-11-23 11:12:52","29","0","1","74545945","<p>Lose the parenthesis</p>
<pre><code>sendData={setData}
</code></pre>
<p>update setMethod to accept data as param</p>
<pre><code>  const setData = (data) =&gt; {
      setEventData(data)
  }
</code></pre>
"
"74542142","Android 12 - Launch MDM app after setup using PROVISIONING_SUCCESSFUL","<p>We recently migrated our <strong>Device Policy App(MDM)</strong> to support <strong>android 12</strong> and would like to open our MDM dpc app after initial setup wizard is complete. This was working fine till android 11 until android 12 updates came out. We are handling <strong>GET_PROVISIONING_MODE</strong> and <strong>ADMIN_POLICY_COMPLIANCE</strong> but <strong>PROVISIONING_SUCCESSFUL</strong> is never called.</p>
<p>This is our code in Manifest for <strong>PROVISIONING_SUCCESSFUL</strong> :</p>
<pre><code>&lt;activity
        android:name=&quot;.pages.ProvisioningSuccessActivity&quot;
        android:exported=&quot;true&quot;
        android:theme=&quot;@android:style/Theme.NoDisplay&quot;
        android:permission=&quot;android.permission.BIND_DEVICE_ADMIN&quot;&gt;
        &lt;intent-filter&gt;
            &lt;action android:name=&quot;android.app.action.PROVISIONING_SUCCESSFUL&quot;/&gt;
            &lt;category android:name=&quot;android.intent.category.DEFAULT&quot;/&gt;
        &lt;/intent-filter&gt;
&lt;/activity&gt;
</code></pre>
<p>We are calling <code>setResult(RESULT_FIRST_USER, intent);</code> on receive of <strong>ADMIN_POLICY_COMPLIANCE</strong>  intent.</p>
<p>Can anyone please help us with this?</p>
","<android><mdm><device-owner><android-device-owner>","2022-11-23 05:26:27","247","1","1","74837885","<p>Starting with Android 12 the <code>PROVISIONING_SUCCESSFUL</code> intent is <strong>only</strong> sent when</p>
<ul>
<li>Provisioning does not happen within the initial setup wizard (e.g. when the end-user installs and starts the DPC app which then triggers provisioning through <code>ACTION_PROVISION_MANAGED_PROFILE</code>) or</li>
<li>No activity for the <code>ADMIN_POLICY_COMPLIANCE</code> activity action can be resolved</li>
</ul>
<p>See <a href=""https://cs.android.com/android/platform/superproject/+/android-12.0.0_r1:packages/apps/ManagedProvisioning/src/com/android/managedprovisioning/finalization/SendDpcBroadcastService.java;l=49;drc=11a31192f765c8086f769bfbc11365e1f58569b1;bpv=0;bpt=1"" rel=""nofollow noreferrer"">here</a> for the corresponding source code.</p>
<p>So in your case you have to replace the intent filter for <code>PROVISIONING_SUCCESSFUL</code> of your ProvisioningSuccessActivity with an intent filter for <code>ADMIN_POLICY_COMPLIANCE</code>.</p>
<p><strong>Background Information</strong></p>
<p>The <code>ADMIN_POLICY_COMPLIANCE</code> intent has already been introduced in Android 10, together with <code>GET_PROVISIONING_MODE</code>. Since then, it is the recommended intent for DPCs, to extend the initial setup wizard with a compliance screen and enforce initial policy settings:</p>
<blockquote>
<p>DPCs <strong>must</strong> use this new Intent instead of listening for the <code>ACTION_PROFILE_PROVISIONING_COMPLETE</code> broadcast.
<a href=""https://developer.android.com/work/versions/android-10#improved_provisioning_tools_for_work_profiles:%7E:text=Your%20DPC%20must%20use%20this%20new%20Intent%20instead%20of%20listening%20for%20the%20ACTION_PROFILE_PROVISIONING_COMPLETE%20broadcast."" rel=""nofollow noreferrer"">(source)</a>.</p>
</blockquote>
<p>Until Android 12 this change didn't affect existing DPC implementations, that only supported fully managed provisioning. DPCs that also wanted to make use of the new working profiles, already had to switch to the new intents with Android 10.</p>
<p>With the deprecation of <a href=""https://developer.android.com/reference/android/app/admin/DevicePolicyManager#ACTION_PROVISION_MANAGED_DEVICE"" rel=""nofollow noreferrer"">ACTION_PROVISION_MANAGED_DEVICE</a>, fully managed provisioning can now only be triggered by the setup wizard. DPC implementations that support only fully managed provisioning have to switch now also to the new intents.</p>
<p>The <code>ACTION_PROFILE_PROVISIONING_COMPLETE</code> broadcast and the <code>PROVISIONING_SUCCESSFUL</code> intent are now only sent, when provisioning is triggered outside of the setup wizard. E.g. when a user installs and starts a  DPC app, which triggers provisioning of a working profile through <a href=""https://developer.android.com/reference/android/app/admin/DevicePolicyManager#ACTION_PROVISION_MANAGED_PROFILE"" rel=""nofollow noreferrer"">ACTION_PROVISION_MANAGED_PROFILE</a>.</p>
"
"74466093","How to create a key-value map with pipeline expression (not data flow expression) in Azure Data Factory (Synapse Studio)","<p>I have two arrays that have respectively contain keys and values:
<strong>array 1</strong> <code>[&quot;key1&quot;, &quot;key2&quot;, &quot;key3&quot;]</code> and <strong>array 2</strong> <code>[&quot;value1&quot;, &quot;value2&quot;, &quot;value3&quot;]</code></p>
<p>With ADF data flow expression, I can construct a key-value map with these two arrays using <code>keyValues</code> function:</p>
<pre><code>keyValues([&quot;key1&quot;, &quot;key2&quot;, &quot;key3&quot;],[&quot;value1&quot;, &quot;value2&quot;, &quot;value3&quot;])
</code></pre>
<p>And this will return a result like this:</p>
<pre><code>[&quot;key1&quot; -&gt; &quot;value1&quot;, &quot;key2&quot; -&gt; &quot;value2&quot;, &quot;key3&quot; -&gt; &quot;value3&quot;]
</code></pre>
<p>I want to construct the same key-value map within a pipeline, not data flow, but the <code>keyValues</code> function is not available in pipeline expression. How can I construct a key-value map with pipeline expression on Azure Data Factory / Synapse Studio?</p>
","<azure><azure-synapse><azure-data-factory>","2022-11-16 19:24:06","624","1","1","74470959","<p>There is no direct way to create a key value pair using pipeline expression builder as in dataflows. The following is one to build the required key value pairs in ADF pipelines:</p>
<ul>
<li>I have taken 2 parameters with the following values:</li>
</ul>
<p><img src=""https://i.imgur.com/ZjecqJY.png"" alt=""enter image description here"" /></p>
<ul>
<li>Let's say <code>req</code> is the variable in which we would like to store our final key value pair. I have initialized it with the value <code>{}</code></li>
</ul>
<p><img src=""https://i.imgur.com/dYWJRKx.png"" alt=""enter image description here"" /></p>
<ul>
<li>In <code>for each</code>, I have used items value as <code>@range(0,length(pipeline().parameters.keys))</code> to generate index.</li>
</ul>
<p><img src=""https://i.imgur.com/kjPjTwQ.png"" alt=""enter image description here"" /></p>
<ul>
<li>Now, I have taken a variable called <code>temp</code> to apply union on the current value of <code>req</code> and the dynamically built key-value pair for current iteration.</li>
</ul>
<pre><code>@string(union(json(variables('req')),json(concat('{&quot;',pipeline().parameters.keys[item()],'&quot;:&quot;',pipeline().parameters.values[item()],'&quot;}'))))
</code></pre>
<p><img src=""https://i.imgur.com/KxPqDvR.png"" alt=""enter image description here"" /></p>
<ul>
<li>Finally, I am updating the value of <code>req</code> variable for each iteration with the current <code>temp</code> value i.e., <code>@variables('temp')</code>:</li>
</ul>
<p><img src=""https://i.imgur.com/J8IFXRs.png"" alt=""enter image description here"" /></p>
<ul>
<li>After running the pipeline, it would generate the following output in req variable:
<img src=""https://i.imgur.com/3Zp7UjF.png"" alt=""enter image description here"" /></li>
</ul>
<p><strong>NOTE:</strong> Object type variables are not supported in ADF pipelines. Whenever you want to use this object (stored as string), you can convert it to an object type using <code>@json()</code> function</p>
"
"74462482","Deploying Microsoft Intune","<p>I'm here going to deploy (Intune) in our company however, the previous users use to Mass360 as the MDM. Is there any way we can detect when users will receive emails outside of Microsoft Intune? We are planning to block them until they enroll in Intune.</p>
<p>Thank you all in advance. </p>
<p>I've tried researching on this topic, nothing pops up.</p>
","<mdm>","2022-11-16 14:44:41","44","0","1","75673374","<p>Getting emails outside of Intune is a vague thing. I think you are referring to receiving emails on a not managed device. If you want to block that, you would do it with a AAD conditional access policy, e.g. only allow all cloud resources on a joined / compliant device</p>
"
"74415137","How to merge multiple csv files into a single parquet file in azure data flow?","<p>I would like to merge multiple CSV files in AzureDataFlow and output them to a single Parquet file, but I am having trouble with the data source file.</p>
<p>There are two types of data source files.
One has a header and the other does not.
These files have the same column names and I would like to merge them, but I don't know how.
I want to merge the files with headers by aligning the headers to the files with headers.</p>
<p><strong>In other words,I want to combine files with headers with files without headers.</strong></p>
<p>I have already tried one method.
That is to skip the first line of a file that has a header, merge it with the header removed, and then add the header in the mapping.
However, that method requires me to manually change the mapping every time the header of the data source is changed.
This is not a smart approach.
Any answers would be appreciated.
Thank you.</p>
","<azure><azure-data-factory>","2022-11-12 17:37:31","313","0","1","74418211","<p>If you simply want to combine the 2 files, use Union with the By Position method: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-union"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-union</a>.</p>
<p>If the columns do not line-up positionally, you can rearrange the column ordering with a Select transformation.</p>
"
"74394976","Doing data mapping for each value in a df","<p>I have a data set like this</p>
<pre><code>df1&lt;-data.frame(ID=c(1,2,3,4),colA=c(101,102,103,104),colB=c(201,202,203,204))

df2&lt;-data.frame(var_id=c(101,102,103,104,201,202,203,204),var_value=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;,&quot;F&quot;,&quot;G&quot;,&quot;H&quot;))
</code></pre>
<p>I want to map any value in df1 that is in df2$var_id with the corresponding string in df2$var_value.</p>
<p>Desired output</p>
<pre><code>df1&lt;-data.frame(ID=c(1,2,3,4),colA=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;),colB=c(&quot;E&quot;,&quot;F&quot;,&quot;G&quot;,&quot;H&quot;))
</code></pre>
<p>I have tried write a function, and then do lapply, but it only display one var_value</p>
","<r><data-mapping>","2022-11-10 20:28:01","42","0","3","74395068","<pre><code>cols = c(&quot;colA&quot;, &quot;colB&quot;)
df1[cols] &lt;- lapply(df1[cols], \(x) df2$var_value[match(x, df2$var_id)])
df1
#   ID colA colB
# 1  1    A    E
# 2  2    B    F
# 3  3    C    G
# 4  4    D    H
</code></pre>
"
"74394976","Doing data mapping for each value in a df","<p>I have a data set like this</p>
<pre><code>df1&lt;-data.frame(ID=c(1,2,3,4),colA=c(101,102,103,104),colB=c(201,202,203,204))

df2&lt;-data.frame(var_id=c(101,102,103,104,201,202,203,204),var_value=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;,&quot;F&quot;,&quot;G&quot;,&quot;H&quot;))
</code></pre>
<p>I want to map any value in df1 that is in df2$var_id with the corresponding string in df2$var_value.</p>
<p>Desired output</p>
<pre><code>df1&lt;-data.frame(ID=c(1,2,3,4),colA=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;),colB=c(&quot;E&quot;,&quot;F&quot;,&quot;G&quot;,&quot;H&quot;))
</code></pre>
<p>I have tried write a function, and then do lapply, but it only display one var_value</p>
","<r><data-mapping>","2022-11-10 20:28:01","42","0","3","74395267","<p>You can join twice.</p>
<pre><code>library(dplyr)

df1 %&gt;%
  left_join(df2, by = c(&quot;colA&quot; = &quot;var_id&quot;)) %&gt;%
  left_join(df2, by = c(&quot;colB&quot; = &quot;var_id&quot;)) %&gt;%
  select(ID, colA = var_value.x, colB = var_value.y)

#   ID colA colB
# 1  1    A    E
# 2  2    B    F
# 3  3    C    G
# 4  4    D    H
</code></pre>
"
"74394976","Doing data mapping for each value in a df","<p>I have a data set like this</p>
<pre><code>df1&lt;-data.frame(ID=c(1,2,3,4),colA=c(101,102,103,104),colB=c(201,202,203,204))

df2&lt;-data.frame(var_id=c(101,102,103,104,201,202,203,204),var_value=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;,&quot;F&quot;,&quot;G&quot;,&quot;H&quot;))
</code></pre>
<p>I want to map any value in df1 that is in df2$var_id with the corresponding string in df2$var_value.</p>
<p>Desired output</p>
<pre><code>df1&lt;-data.frame(ID=c(1,2,3,4),colA=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;),colB=c(&quot;E&quot;,&quot;F&quot;,&quot;G&quot;,&quot;H&quot;))
</code></pre>
<p>I have tried write a function, and then do lapply, but it only display one var_value</p>
","<r><data-mapping>","2022-11-10 20:28:01","42","0","3","74396224","<p>with the <code>tidyverse</code> you can apply a function <code>across</code> several columns:</p>
<pre class=""lang-r prettyprint-override""><code>library(tidyverse)


df1 |&gt;
  mutate(across(colA:colB, \(x) map_chr(x, \(y) with(df2,var_value[y == var_id] ))))
#&gt;   ID colA colB
#&gt; 1  1    A    E
#&gt; 2  2    B    F
#&gt; 3  3    C    G
#&gt; 4  4    D    H

#or 

df1 |&gt;
  mutate(across(colA:colB, \(x) with(df2, var_value[match(x, var_id)])))
#&gt;   ID colA colB
#&gt; 1  1    A    E
#&gt; 2  2    B    F
#&gt; 3  3    C    G
#&gt; 4  4    D    H
</code></pre>
"
"74300308","How streams API data flows in NodeJS?","<p>I am still not clear on how the data flows between stream in pipe. Most examples are using file or built in generator as starting point, then finished in another file or output. But most of them didn't show anything about how to make our own pipeline.</p>
<p>So, I need to generate data I received from another data from an external machine, then transform it into something, and retransmit the data via UDP socket (no <code>stream-udp</code> package please). I know how to get the data, and I know how to send data via datagram, but I am confused on how you can stream them as the data received and sent.</p>
<p>Let's say. I have a something like this:</p>
<pre><code>import { pipeline, Readable, Transform, Writable } from &quot;stream&quot;;

const reader = new Readable();
const transform = new Transform();
const writable = new Writable();

// How to flow the data from reader to transform to writer?

transform.on('data', (chunk) =&gt; {
    console.log('Transforming ' + chunk);
    const newchunk = chunk + 'transformed ';
    // how to send the data from here to writable?
    transform.push(newchunk);
})

writable.on('???', (chunk) =&gt; {
    // write the chunk into something
    console.log(chunk);
}

pipeline(
    reader,
    transform,
    writable,
    (err) =&gt; {
        console.log('Done')
        if (err) console.error(err);
    }
)

reader.push('this is new data');
</code></pre>
<p>So, can someone please help me on how the things flow from the reader sending a text to transform, then transform, then received at writable?</p>
<p>A simple example should suffice, like <code>reader.push('this is data')</code> and ends with <code>writer.on('???', (chunk) =&gt; console.log(chunk))</code></p>
<p>Thank you.</p>
","<node.js><node-streams><nodejs-stream>","2022-11-03 08:54:08","53","2","1","74300523","<p>one of the problems with streams Api is there are a lot of whys to implement and the example are not consistent</p>
<p>here is a straightforward example :</p>
<p>CharStream  - is a Readable Stream:
generate a character stream from a-z then when it reaches z send null and finish</p>
<p>Uppercasify - a Transformer takes each letter and change it to uppercase</p>
<p>StringWritable - aggregate all letters to one string and on finish console log it</p>
<pre><code>import { Transform, Readable, Writable } from 'readable-stream'

class CharStream extends Readable {
    constructor(options = {}) {
        super(options)
        this.charCode = 97;
    }
    _read() {
        this.push(`${String.fromCharCode(this.charCode++)}\n`);
        if (this.charCode &gt; 'z'.charCodeAt(0)) this.push(null);
    };
}

class Uppercasify extends Transform {
    constructor(options = {}) {
        super(options)
        this.counter = 0
    }

    _transform(chunk, encoding, done) {
        done(null, chunk.toString().toUpperCase())
    }
}

class StringWritable extends Writable {
    constructor(options = {}) {
        super(options)
        this.currentString = &quot;&quot;;
    }
    _write(chunk, encoding, done) {
        this.currentString += chunk.toString().replace(&quot;\n&quot;, &quot;&quot;)
        done()
    }
}



const cs = new CharStream()
const uts = new Uppercasify()
const out = new StringWritable()
const stream = cs.pipe(uts).pipe(out)
stream.on('finish', () =&gt; console.log(out.currentString))
</code></pre>
"
"74295369","how do I filter out errant integer data in pentaho data integration","<p>I have a fixed position input.txt file like this:</p>
<blockquote>
<p>4033667    70040118401401<br />
4033671    70040/8401901    &lt; not int because of &quot;/&quot;<br />
4033669    70040118401301<br />
4033673    70060118401101</p>
</blockquote>
<p>I'm using a text file input step to pull the data in, and I'd like to load the data into a database as int's and have errant data go to a log file.</p>
<p>I've tried to using the filter step and the data validator step, but I can't seem to get either to work.  I've even tried using the text input field to bring it in as a string and then converting it to an int w/ the Select/Rename values Step, and changing the data-type in meta-data section.</p>
<p>a typical error I keep running into is &quot;String : couldn't convert String to Integer&quot;</p>
<p>Any suggestions?</p>
<p>Thanks!</p>
","<pentaho><pentaho-spoon><pentaho-data-integration>","2022-11-02 20:37:08","37","0","2","74295859","<p>So I ended up using...
Text file input &gt; Filter Rows (regex \d+) &gt; select values (to cast string to int) &gt; table output</p>
<p>...and the error log comes off of the false result of the regex filter.</p>
"
"74295369","how do I filter out errant integer data in pentaho data integration","<p>I have a fixed position input.txt file like this:</p>
<blockquote>
<p>4033667    70040118401401<br />
4033671    70040/8401901    &lt; not int because of &quot;/&quot;<br />
4033669    70040118401301<br />
4033673    70060118401101</p>
</blockquote>
<p>I'm using a text file input step to pull the data in, and I'd like to load the data into a database as int's and have errant data go to a log file.</p>
<p>I've tried to using the filter step and the data validator step, but I can't seem to get either to work.  I've even tried using the text input field to bring it in as a string and then converting it to an int w/ the Select/Rename values Step, and changing the data-type in meta-data section.</p>
<p>a typical error I keep running into is &quot;String : couldn't convert String to Integer&quot;</p>
<p>Any suggestions?</p>
<p>Thanks!</p>
","<pentaho><pentaho-spoon><pentaho-data-integration>","2022-11-02 20:37:08","37","0","2","74320963","<p>I understand you problem.
Let do it simple.
<a href=""https://i.stack.imgur.com/gznRB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gznRB.png"" alt=""enter image description here"" /></a></p>
"
"74151965","ZooKeeper zxid is used up due to data synchronization between the active and standby HBase clusters","<p>During data synchronization between the active and standby HBase clusters, the active cluster frequently performs the setdata operation on ZooKeeper. Most znodes are in /hbase/replication/xxx format. As a result, the ZooKeeper zxid is exhausted and the ZooKeeper service is abnormal. Can the setdata operation be reduced?</p>
","<hbase>","2022-10-21 10:01:44","35","0","1","74176026","<p>The HLog size is being increased to reduce the number of registrations with the ZooKeeper during active/standby synchronization.</p>
"
"74129563","How to create separate data mapping file with function names in Swift","<p>I have a macOS app that I'm creating in Swift and I have integrated an  external HID device that has a number of controls on it.
The HID part is done where I am receiving all of the hid commands from the device and I am trying to create a mapping file where I can maintain the HID key mappings in a separate swift file.
All I want in that file is the data and what I want to do is this;</p>
<ul>
<li>raw hid data is received from HID device (In ViewController)</li>
<li>Lookup the function name assigned to this hid data (In separate file)</li>
<li>Run the function that is mapped to that key. (Function located in the main ViewController)</li>
</ul>
<p>So far I have the external swift file setup with all of the mapping and that all works fine but my issue is when I try to call the looked up function in the ViewController, it says the function can't be found in the scope.</p>
<p>Initially I thought I would use a delegate but the external file isn't a viewcontroller, just a separate swift file so I don't know if I can do that?.</p>
<p>I've tried searching but everything I've found is calling a function from another ViewController which I'm not. It's very possible I'm not using the best approach and my goal is to just keep all of the mapping in a separate file as there is a lot and it woudl be easier to maintain.</p>
<p>Any suggestions are appreciated.</p>
","<swift>","2022-10-19 17:26:54","51","-1","2","74129590","<p>You can simply create a UIViewController as the external file and add it as a property to the main ViewController.</p>
<p>In the external file add this.</p>
<pre><code>@IBOutlet var uiViewController: UIViewController!
</code></pre>
<p>In the ViewController add this.</p>
<pre><code>var externalFileViewController: UIViewController!

override func viewDidLoad() {
    super.viewDidLoad()

    externalFileViewController = externalFileViewController?.loadView()

    // If we have an object then load it
    if let viewController = externalFileViewController {
        viewController.view.frame = view.frame
        viewController.view.autoresizingMask = [.flexibleHeight, .flexibleWidth]
        view.addSubview(viewController.view)
        uiViewController = viewController

    }
}
</code></pre>
<p>Now in the viewController look up the functions to be called from the external file and call them using the function name.</p>
<p>The functions are defined in the external file using @IBAction.</p>
<p>Let me know if you have any questions.</p>
"
"74129563","How to create separate data mapping file with function names in Swift","<p>I have a macOS app that I'm creating in Swift and I have integrated an  external HID device that has a number of controls on it.
The HID part is done where I am receiving all of the hid commands from the device and I am trying to create a mapping file where I can maintain the HID key mappings in a separate swift file.
All I want in that file is the data and what I want to do is this;</p>
<ul>
<li>raw hid data is received from HID device (In ViewController)</li>
<li>Lookup the function name assigned to this hid data (In separate file)</li>
<li>Run the function that is mapped to that key. (Function located in the main ViewController)</li>
</ul>
<p>So far I have the external swift file setup with all of the mapping and that all works fine but my issue is when I try to call the looked up function in the ViewController, it says the function can't be found in the scope.</p>
<p>Initially I thought I would use a delegate but the external file isn't a viewcontroller, just a separate swift file so I don't know if I can do that?.</p>
<p>I've tried searching but everything I've found is calling a function from another ViewController which I'm not. It's very possible I'm not using the best approach and my goal is to just keep all of the mapping in a separate file as there is a lot and it woudl be easier to maintain.</p>
<p>Any suggestions are appreciated.</p>
","<swift>","2022-10-19 17:26:54","51","-1","2","74141965","<p>This is one way to achieve this. It can get tedious. You can totally skip writing out a separate protocol for the delegate, but this is cleaner design.</p>
<pre><code>protocol HIDMessageDelegate: AnyObject {
  // example messages
  func message1()
  func message2()
  func message3()
}

class HIDMessageParser {
  static weak var delegate: HIDMessageDelegate?

  static func parseHIDMessage() {
    var condition = 0
    // this is where your switch statement will go and you'll parse things and call the relevant delegate method
    switch (condition) {
    default:
      delegate?.message1()
    }
  }
}

class MyViewController: UIViewController {
  override func viewDidLoad() {
    super.viewDidLoad()
    HIDMessageParser.delegate = self
  }
}

extension MyViewController: HIDMessageDelegate {
  func message1() {

  }

  func message2() {

  }

  func message3() {

  }
}
</code></pre>
"
"74025125","How to use multiple cache sink in a single data flow and use outputs of both the sinks in a pipeline?","<p>I am having two cache sinks in a dataflow, when selecting write to activity output for both the sinks throws validation error: Multiple Cache sinks found with output set to 'true'. Output for only one sink is supported.
Is there any workaround / Solution to use output of both the sinks in a single pipeline?</p>
","<azure><azure-data-factory>","2022-10-11 08:25:29","222","1","1","74031693","<p>Only 1 cached sink can send output from a data flow. Would it be possible to combine or union multiple data streams into a single cache sink in your data flow?</p>
"
"73972352","SAS SQL Pass-Through Facility does not work as expected for Postgres database","<p>I am working with SCD Type 2 transformation in SAS Data integration Studio (4.905) and using Postgres (12) as database.</p>
<p>I am facing the following error when I try to execute a query via passthrough:</p>
<p><a href=""https://i.stack.imgur.com/39dWe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/39dWe.png"" alt=""enter image description here"" /></a></p>
<p>When using passthrough in Postgres, SCD Type 2 doesn't enclose the table name in quotes (which would keep the name uppercase, since postgres converts all unquoted data to lowercase) and so doesn't find it as you can see.</p>
<p>My questions are:</p>
<p>Is there a way to make SCD2 transformation declare the table’s name, used via passthrough, in quotes?</p>
<p>Is there a way to make the SCD2 transformation create intermediate tables ‘name in lower case so that the reference is not lost when doing passthrough?</p>
<p>Is there a global option in DI that allow us to modify/edit temporary table names?</p>
<p>Source and target tables are postgresql tables, with name and columns name in lowercase:
<a href=""https://i.stack.imgur.com/LVexx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LVexx.png"" alt=""enter image description here"" /></a></p>
<p>Please, if anyone has faced this problem before or knows what is missing, please, let I know.</p>
","<postgresql><sas><data-integration><pass-through>","2022-10-06 10:30:46","182","1","1","73985501","<p>To solve this issue, we have to select the following highlighted (source and target) table options. <br/> It results in quotes around source/target table names:</p>
<p><a href=""https://i.stack.imgur.com/vTMmc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vTMmc.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/52xf3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/52xf3.png"" alt=""enter image description here"" /></a></p>
<p>Then, SCD2 transformation automatically put quotes in tables y columns names as you can see:</p>
<p><a href=""https://i.stack.imgur.com/9Jrj5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9Jrj5.png"" alt=""enter image description here"" /></a></p>
"
"73966857","oneTBB data flow graph, how to write a node with single input, but output multiple times","<p>I want to use oneTBB to write such a node, that can receive a value, and then generate a vector of int. But instead of sending the vector as a single output, I want to send each vector element as a single message, so they can be processed by individual successors. After finishing sending all elements, then start to receive next input message. I don't think multifunction_node that publishes std::tuple is design for this purpose. Anyone could give me some suggestions? thank you!</p>
","<c++><tbb><tbb-flow-graph>","2022-10-05 22:19:39","137","0","1","73971098","<p>Multifunction_node is still useful here, because this is the type of the node that allows sending zero or more messages to each of its successors. The successors input type is specified as a single tuple element of the multifunction node's output. Thus, if accepting an <code>int</code> and sending of multiple  <code>ints</code> to single successor is desired then use <code>multifunction_node&lt;int, std::tuple&lt;int&gt;&gt;</code>. Since it is important that new message does not arrive while the current one is being processed, the node should have <code>serial</code> concurrency.</p>
<p>So, the result node declaration might look like the following in your code:</p>
<pre class=""lang-cpp prettyprint-override""><code>using mf_node_type = tbb::flow::multifunction_node&lt;int, std::tuple&lt;int&gt;&gt;;
mf_node_type mf(
    graph_reference, tbb::flow::serial,
    [output_vector_size](int message, mf_node_type::output_ports_type&amp; p) {
        for (int i = 0; i &lt; output_vector_size; ++i) {
          int message_to_successor = i;
          std::get&lt;0&gt;(p).try_put(message_to_successor);
        }
    });
        
</code></pre>
"
"73905557","How to use pentaho data integration for data migration","<p>i am new for pentaho,</p>
<p>i have create input mysql and have validator and i want to migrate from mySql to postgreSql</p>
<p>my question is how to drag from input to validators and to output ?</p>
<p><a href=""https://i.stack.imgur.com/N6Gt8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/N6Gt8.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/CrXCa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CrXCa.png"" alt=""enter image description here"" /></a></p>
","<pentaho>","2022-09-30 07:48:10","80","0","1","73908275","<p>If you hover over the origin step, pops up a line with options, one of them is to create a new hop to another step.</p>
<p>You also have the option of adding another step by double clicking the new step class in the design panel, if you have a step selected in the canvas, the new step is added with the hop created from the selected step to the new one.</p>
"
"73902336","Intune Win32 App Install Parameters Array","<p>I have some powershell who has the following:
<code>Param([Parameter(Mandatory = $true)] $TargetPaths=@())</code>
I bundle this .ps1 into a .intunewin with content manager and upload to intune.</p>
<p>I now wanted to call my powershell with an install command similar to <code>powershell -executionpolicy bypass -command .\powershell.ps1 -TargetPaths @('1','2','3'...)</code>
This works fine on my local machine... but does not pass through the intune installer, my log files are never created and after careful examination it is clear the script is never run.</p>
<p>Can anyone advise me on moving forward to get this to run? If I execute the script locally with the same parameters it works fine. I even re-jiggered it to work through a cmd execution instead of powershell (including the use of &quot;&quot;&quot; for a single double quote) and couldn't get this working.</p>
","<installation><mdm><intune>","2022-09-29 22:40:41","296","1","2","74010548","<p>Unfortunately, I cannot provide you with the final solution.</p>
<p>But you may want to try the <code>-file</code> parameter of powershell.exe. It partially works for us. This means that the script is executed correctly, but Intune always shows a wrong installation status in the first moment. So we have not yet solved this case conclusively either.</p>
<p><code>powershell -executionpolicy bypass -file &quot;.\install.ps1&quot; -i -name &quot;myApp&quot; -variant &quot;2022&quot;</code></p>
"
"73902336","Intune Win32 App Install Parameters Array","<p>I have some powershell who has the following:
<code>Param([Parameter(Mandatory = $true)] $TargetPaths=@())</code>
I bundle this .ps1 into a .intunewin with content manager and upload to intune.</p>
<p>I now wanted to call my powershell with an install command similar to <code>powershell -executionpolicy bypass -command .\powershell.ps1 -TargetPaths @('1','2','3'...)</code>
This works fine on my local machine... but does not pass through the intune installer, my log files are never created and after careful examination it is clear the script is never run.</p>
<p>Can anyone advise me on moving forward to get this to run? If I execute the script locally with the same parameters it works fine. I even re-jiggered it to work through a cmd execution instead of powershell (including the use of &quot;&quot;&quot; for a single double quote) and couldn't get this working.</p>
","<installation><mdm><intune>","2022-09-29 22:40:41","296","1","2","74101178","<p>While not really an answer to the question, I was able to work around the problem of passing an array by instead flattening the entire thing into a string on parameter pass and later breaking it into an array inside my code. In order to do this I removed all internal quotation marks and then assumed their location later. Not really the most elegant but the script now executes.</p>
"
"73875704","React Application not rendering components when using the NavBar","<blockquote>
<p>EDIT: I am using React 18.2.0 and React router dom 5.3.3, and I recently replaced React.Strictmode with Fragment this evening in my index.jsx file.</p>
</blockquote>
<p>As title says, the application is working well except for the NavBar. When you click the sections it does not render the component onto the page, but the href changes in the browser itself. Here is my NavBar component code:</p>
<pre><code>import { Navbar, Nav, Container } from 'react-bootstrap';
import { withRouter } from 'react-router';
import { NavLink } from 'react-router-dom';
import styled, { ThemeContext } from 'styled-components';
import endpoints from '../constants/endpoints';
import ThemeToggler from './ThemeToggler';

const styles = {
  logoStyle: {
    width: 50,
    height: 40,
  },
};

const ExternalNavLink = styled.a`
  color: ${(props) =&gt; props.theme.navbarTheme.linkColor};
  &amp;:hover {
    color: ${(props) =&gt; props.theme.navbarTheme.linkHoverColor};
  }
  &amp;::after {
    background-color: ${(props) =&gt; props.theme.accentColor};
  }
`;

const InternalNavLink = styled(NavLink)`
  color: ${(props) =&gt; props.theme.navbarTheme.linkColor};
  &amp;:hover {
    color: ${(props) =&gt; props.theme.navbarTheme.linkHoverColor};
  }
  &amp;::after {
    background-color: ${(props) =&gt; props.theme.accentColor};
  }
  &amp;.navbar__link--active {
    color: ${(props) =&gt; props.theme.navbarTheme.linkActiveColor};
  }
`;

const NavBar = () =&gt; {
  const theme = useContext(ThemeContext);
  const [data, setData] = useState(null);
  const [expanded, setExpanded] = useState(false);

  useEffect(() =&gt; {
    fetch(endpoints.navbar, {
      method: 'GET',
    })
      .then((res) =&gt; res.json())
      .then((res) =&gt; setData(res))
      .catch((err) =&gt; err);
  }, []);

  return (
    &lt;Navbar
      fixed=&quot;top&quot;
      expand=&quot;md&quot;
      bg=&quot;dark&quot;
      variant=&quot;dark&quot;
      className=&quot;navbar-custom&quot;
      expanded={expanded}
    &gt;
      &lt;Container&gt;
        {data?.logo &amp;&amp; (
          &lt;Navbar.Brand href=&quot;/&quot;&gt;
            &lt;img
              src={data?.logo?.source}
              className=&quot;d-inline-block align-top&quot;
              alt=&quot;main logo&quot;
              style={
                data?.logo?.height &amp;&amp; data?.logo?.width
                  ? { height: data?.logo?.height, width: data?.logo?.width }
                  : styles.logoStyle
              }
            /&gt;
          &lt;/Navbar.Brand&gt;
        )}
        &lt;Navbar.Toggle
          aria-controls=&quot;responsive-navbar-nav&quot;
          onClick={() =&gt; setExpanded(!expanded)}
        /&gt;
        &lt;Navbar.Collapse id=&quot;responsive-navbar-nav&quot;&gt;
          &lt;Nav className=&quot;me-auto&quot; /&gt;
          &lt;Nav&gt;
            {data
              &amp;&amp; data.sections?.map((section, index) =&gt; (section?.type === 'link' ? (
                &lt;ExternalNavLink
                  key={section.title}
                  href={section.href}
                  target=&quot;_blank&quot;
                  rel=&quot;noopener noreferrer&quot;
                  onClick={() =&gt; setExpanded(false)}
                  className=&quot;navbar__link&quot;
                  theme={theme}
                &gt;
                  {section.title}
                &lt;/ExternalNavLink&gt;
              ) : (
                &lt;InternalNavLink
                  key={section.title}
                  onClick={() =&gt; setExpanded(false)}
                  exact={index === 0}
                  activeClassName=&quot;navbar__link--active&quot;
                  className=&quot;navbar__link&quot;
                  to={section.href}
                  theme={theme}
                &gt;
                  {section.title}
                &lt;/InternalNavLink&gt;
              )))}
          &lt;/Nav&gt;
          &lt;ThemeToggler
            onClick={() =&gt; setExpanded(false)}
          /&gt;
        &lt;/Navbar.Collapse&gt;
      &lt;/Container&gt;
    &lt;/Navbar&gt;
  );
};

const NavBarWithRouter = withRouter(NavBar);
export default NavBarWithRouter;
</code></pre>
<p>App.jsx:</p>
<pre><code>import React, { useEffect, useState, useContext } from 'react';
import { Navbar, Nav, Container } from 'react-bootstrap';
import { withRouter } from 'react-router';
import { NavLink } from 'react-router-dom';
import styled, { ThemeContext } from 'styled-components';
import endpoints from '../constants/endpoints';
import ThemeToggler from './ThemeToggler';

const styles = {
  logoStyle: {
    width: 50,
    height: 40,
  },
};

const ExternalNavLink = styled.a`
  color: ${(props) =&gt; props.theme.navbarTheme.linkColor};
  &amp;:hover {
    color: ${(props) =&gt; props.theme.navbarTheme.linkHoverColor};
  }
  &amp;::after {
    background-color: ${(props) =&gt; props.theme.accentColor};
  }
`;

const InternalNavLink = styled(NavLink)`
  color: ${(props) =&gt; props.theme.navbarTheme.linkColor};
  &amp;:hover {
    color: ${(props) =&gt; props.theme.navbarTheme.linkHoverColor};
  }
  &amp;::after {
    background-color: ${(props) =&gt; props.theme.accentColor};
  }
  &amp;.navbar__link--active {
    color: ${(props) =&gt; props.theme.navbarTheme.linkActiveColor};
  }
`;

const NavBar = () =&gt; {
  const theme = useContext(ThemeContext);
  const [data, setData] = useState(null);
  const [expanded, setExpanded] = useState(false);

  useEffect(() =&gt; {
    fetch(endpoints.navbar, {
      method: 'GET',
    })
      .then((res) =&gt; res.json())
      .then((res) =&gt; setData(res))
      .catch((err) =&gt; err);
  }, []);

  return (
    &lt;Navbar
      fixed=&quot;top&quot;
      expand=&quot;md&quot;
      bg=&quot;dark&quot;
      variant=&quot;dark&quot;
      className=&quot;navbar-custom&quot;
      expanded={expanded}
    &gt;
      &lt;Container&gt;
        {data?.logo &amp;&amp; (
          &lt;Navbar.Brand href=&quot;/&quot;&gt;
            &lt;img
              src={data?.logo?.source}
              className=&quot;d-inline-block align-top&quot;
              alt=&quot;main logo&quot;
              style={
                data?.logo?.height &amp;&amp; data?.logo?.width
                  ? { height: data?.logo?.height, width: data?.logo?.width }
                  : styles.logoStyle
              }
            /&gt;
          &lt;/Navbar.Brand&gt;
        )}
        &lt;Navbar.Toggle
          aria-controls=&quot;responsive-navbar-nav&quot;
          onClick={() =&gt; setExpanded(!expanded)}
        /&gt;
        &lt;Navbar.Collapse id=&quot;responsive-navbar-nav&quot;&gt;
          &lt;Nav className=&quot;me-auto&quot; /&gt;
          &lt;Nav&gt;
            {data
              &amp;&amp; data.sections?.map((section, index) =&gt; (section?.type === 'link' ? (
                &lt;ExternalNavLink
                  key={section.title}
                  href={section.href}
                  target=&quot;_blank&quot;
                  rel=&quot;noopener noreferrer&quot;
                  onClick={() =&gt; setExpanded(false)}
                  className=&quot;navbar__link&quot;
                  theme={theme}
                &gt;
                  {section.title}
                &lt;/ExternalNavLink&gt;
              ) : (
                &lt;InternalNavLink
                  key={section.title}
                  onClick={() =&gt; setExpanded(false)}
                  exact={index === 0}
                  activeClassName=&quot;navbar__link--active&quot;
                  className=&quot;navbar__link&quot;
                  to={section.href}
                  theme={theme}
                &gt;
                  {section.title}
                &lt;/InternalNavLink&gt;
              )))}
          &lt;/Nav&gt;
          &lt;ThemeToggler
            onClick={() =&gt; setExpanded(false)}
          /&gt;
        &lt;/Navbar.Collapse&gt;
      &lt;/Container&gt;
    &lt;/Navbar&gt;
  );
};

const NavBarWithRouter = withRouter
(NavBar);
export default NavBarWithRouter;
</code></pre>
<p>MainApp.jsx:</p>
<pre><code>import { BrowserRouter as Router, Route, Switch } from 'react-router-dom';
import FallbackSpinner from './components/FallbackSpinner';
import NavBarWithRouter from './components/NavBar';
import Home from './components/Home';
import endpoints from './constants/endpoints';

function MainApp() {
  const [data, setData] = useState(null);

  useEffect(() =&gt; {
    fetch(endpoints.routes, {
      method: 'GET',
    })
      .then((res) =&gt; res.json())
      .then((res) =&gt; setData(res))
      .catch((err) =&gt; err);
  }, []);

  return (
    &lt;div className=&quot;MainApp&quot;&gt;
      &lt;NavBarWithRouter /&gt;
      &lt;Router className=&quot;main&quot;&gt;
        &lt;Switch&gt;
          &lt;Suspense fallback={&lt;FallbackSpinner /&gt;}&gt;
            &lt;Route exact path=&quot;/&quot; component={Home} /&gt;
            {data
              &amp;&amp; data.sections.map((route) =&gt; {
                const SectionComponent = React.lazy(() =&gt; import('./components/' + route.component));
                return (
                  &lt;Route
                    key={route.headerTitle}
                    path={route.path}
                    component={() =&gt; (
                      &lt;SectionComponent header={route.headerTitle} /&gt;
                    )}
                  /&gt;
                );
              })}
          &lt;/Suspense&gt;
        &lt;/Switch&gt;
      &lt;/Router&gt;
    &lt;/div&gt;
  );
}

export default MainApp;
</code></pre>
<p>I appreciate any and all help on this, I've been pulling my hair out over it!</p>
","<reactjs><state><react-router-dom><data-mapping><react-bootstrap-nav>","2022-09-28 02:53:31","117","1","1","73875929","<p>Actually, besides recommending upgrading to <code>react-router-dom@5.3.3</code> or better, the other issue I see is the <code>NavBar</code> being rendered <em><strong>outside</strong></em> the <code>Router</code>. Move it <em>into</em> the <code>Router</code> and remove the <code>withRouter</code> HOC. The <code>withRouter</code> HOC only injects the route props of the routing context <em>above</em> it in the ReactTree and there is none.</p>
<p>NavBar</p>
<p>Instead of</p>
<pre><code>const NavBarWithRouter = withRouter(NavBar);
export default NavBarWithRouter;
</code></pre>
<p>use</p>
<pre><code>export default NavBar;
</code></pre>
<p>...</p>
<pre><code>import NavBar from './components/NavBar';
...

function MainApp() {
  ...

  return (
    &lt;div className=&quot;MainApp&quot;&gt;
      &lt;Router className=&quot;main&quot;&gt;
        &lt;NavBar /&gt; // &lt;-- inside router
        &lt;Switch&gt;
          &lt;Suspense fallback={&lt;FallbackSpinner /&gt;}&gt;
            &lt;Route exact path=&quot;/&quot; component={Home} /&gt;
            {data
              &amp;&amp; data.sections.map((route) =&gt; {
                const SectionComponent = React.lazy(() =&gt; import('./components/' + route.component));
                return (
                  &lt;Route
                    key={route.headerTitle}
                    path={route.path}
                    component={() =&gt; (
                      &lt;SectionComponent header={route.headerTitle} /&gt;
                    )}
                  /&gt;
                );
              })}
          &lt;/Suspense&gt;
        &lt;/Switch&gt;
      &lt;/Router&gt;
    &lt;/div&gt;
  );
}
</code></pre>
"
"73838203","Scrapy Product datas in a json script / tags cannot be targeted","<p>I can parse the json datas in a script, but I am not able to target specific tags.</p>
<p>If it would be a &quot;normal&quot; script type like &quot;application/ld+json&quot;, it would be pretty easy to collect what I need. But I cannot address the script name, cause there is no name 😅
So I used the XPATH selector to the script via dev tools.</p>
<p>So, I read the json datas in a Scrapy shell from a <a href=""https://www.electronic4you.de/makita-dmp180z-akku-kompressor-189153.html"" rel=""nofollow noreferrer"">product link</a></p>
<pre><code>scrapy shell 'https://www.electronic4you.de/makita-dmp180z-akku-kompressor-189153.html' ...
...
&gt;&gt;&gt; response.xpath('//*[@id=&quot;root-wrapper&quot;]/div/script[5]/text()').get()
</code></pre>
<p>And I get all datas in the script, of course. The result shows us:</p>
<blockquote>
<p>['new E4uTrack(&quot;view_item&quot;, {&quot;currency&quot;:&quot;EUR&quot;,&quot;value&quot;:&quot;49&quot;,&quot;page_type&quot;:&quot;product&quot;,&quot;title&quot;:&quot;Makita DMP180Z Akku-Kompressor&quot;,&quot;items&quot;:[{&quot;item_name&quot;:&quot;Makita DMP180Z Akku-Kompressor&quot;,&quot;item_id&quot;:&quot;189153&quot;,&quot;id&quot;:&quot;189153&quot;,&quot;item_mpn&quot;:&quot;DMP180Z&quot;,&quot;item_gtin&quot;:&quot;0088381898263&quot;,&quot;item_brand&quot;:&quot;MAKITA&quot;,&quot;google_business_vertical&quot;:&quot;retail&quot;,&quot;price&quot;:&quot;49&quot;,&quot;currency&quot;:&quot;EUR&quot;,&quot;regular_price&quot;:109.9501,&quot;screensize&quot;:&quot;&quot;,&quot;vogels_fsf&quot;:false}]})']</p>
</blockquote>
<p>Normally I would targeting a value - e.g. the value from &quot;item_name&quot; with:</p>
<pre><code>scrapy shell 'https://www.electronic4you.de/makita-dmp180z-akku-kompressor-189153.html' ...
...
&gt;&gt;&gt; script_tag = response.xpath('//*[@id=&quot;root-wrapper&quot;]/div/script[5]/text()').get()
&gt;&gt;&gt; import json
&gt;&gt;&gt; json.loads(script_tag)[&quot;items&quot;][&quot;item_name&quot;]
</code></pre>
<p>But......</p>
<p>The output shows us:</p>
<blockquote>
<p>File &quot;&quot;,
line 1
json.loads(script_tag)[&quot;item_name&quot;]<br />
IndentationError: unexpected indent</p>
</blockquote>
<p>I have two questions.</p>
<p>Do I address the script correctly with the xpath selector? And how I can target from this script only tags I need?</p>
","<json><xpath><scrapy>","2022-09-24 14:58:24","52","1","2","73838786","<p>You need to use regular expression to clear XPath result:</p>
<pre><code>json_raw = response.xpath('//script[contains(., &quot;E4uTrack(\&quot;view_item\&quot;&quot;)]/text()').re_first(r'E4uTrack\(&quot;view_item&quot;,(.+?)\)$')
data = json.loads(json_raw)
</code></pre>
"
"73838203","Scrapy Product datas in a json script / tags cannot be targeted","<p>I can parse the json datas in a script, but I am not able to target specific tags.</p>
<p>If it would be a &quot;normal&quot; script type like &quot;application/ld+json&quot;, it would be pretty easy to collect what I need. But I cannot address the script name, cause there is no name 😅
So I used the XPATH selector to the script via dev tools.</p>
<p>So, I read the json datas in a Scrapy shell from a <a href=""https://www.electronic4you.de/makita-dmp180z-akku-kompressor-189153.html"" rel=""nofollow noreferrer"">product link</a></p>
<pre><code>scrapy shell 'https://www.electronic4you.de/makita-dmp180z-akku-kompressor-189153.html' ...
...
&gt;&gt;&gt; response.xpath('//*[@id=&quot;root-wrapper&quot;]/div/script[5]/text()').get()
</code></pre>
<p>And I get all datas in the script, of course. The result shows us:</p>
<blockquote>
<p>['new E4uTrack(&quot;view_item&quot;, {&quot;currency&quot;:&quot;EUR&quot;,&quot;value&quot;:&quot;49&quot;,&quot;page_type&quot;:&quot;product&quot;,&quot;title&quot;:&quot;Makita DMP180Z Akku-Kompressor&quot;,&quot;items&quot;:[{&quot;item_name&quot;:&quot;Makita DMP180Z Akku-Kompressor&quot;,&quot;item_id&quot;:&quot;189153&quot;,&quot;id&quot;:&quot;189153&quot;,&quot;item_mpn&quot;:&quot;DMP180Z&quot;,&quot;item_gtin&quot;:&quot;0088381898263&quot;,&quot;item_brand&quot;:&quot;MAKITA&quot;,&quot;google_business_vertical&quot;:&quot;retail&quot;,&quot;price&quot;:&quot;49&quot;,&quot;currency&quot;:&quot;EUR&quot;,&quot;regular_price&quot;:109.9501,&quot;screensize&quot;:&quot;&quot;,&quot;vogels_fsf&quot;:false}]})']</p>
</blockquote>
<p>Normally I would targeting a value - e.g. the value from &quot;item_name&quot; with:</p>
<pre><code>scrapy shell 'https://www.electronic4you.de/makita-dmp180z-akku-kompressor-189153.html' ...
...
&gt;&gt;&gt; script_tag = response.xpath('//*[@id=&quot;root-wrapper&quot;]/div/script[5]/text()').get()
&gt;&gt;&gt; import json
&gt;&gt;&gt; json.loads(script_tag)[&quot;items&quot;][&quot;item_name&quot;]
</code></pre>
<p>But......</p>
<p>The output shows us:</p>
<blockquote>
<p>File &quot;&quot;,
line 1
json.loads(script_tag)[&quot;item_name&quot;]<br />
IndentationError: unexpected indent</p>
</blockquote>
<p>I have two questions.</p>
<p>Do I address the script correctly with the xpath selector? And how I can target from this script only tags I need?</p>
","<json><xpath><scrapy>","2022-09-24 14:58:24","52","1","2","73838936","<p>You have to iterate over the ResultSet and pull the desired data</p>
<pre><code>import scrapy
import json
class TestSpider(scrapy.Spider):
    name = 'test'
    start_urls = ['https://www.electronic4you.de/makita-dmp180z-akku-kompressor-189153.html']

    def parse(self, response):

        script_tag =  response.xpath('(//*[@id=&quot;root-wrapper&quot;]/div/script)[5]/text()').re_first(r'E4uTrack\(&quot;view_item&quot;,(.+?)\)$')
        json_data= json.loads(script_tag)

        for item in json_data[&quot;items&quot;]:
            yield {
                'Name':item[&quot;item_name&quot;]
                }
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>{'Name': 'Makita DMP180Z Akku-Kompressor'}
</code></pre>
"
"73836278","How to clone a data flow in Synapse?","<p>How to clone a data flow in Synapse?</p>
<p>I see in Azure Data Factory there is a possibility to clone a data flow by going to Data Flows folder and right-click on a data flow. But in Synapse I don't find a way to see the list of the data flows...</p>
","<azure-data-factory><azure-synapse>","2022-09-24 09:53:46","229","0","1","73851840","<p>Posting it as answer for other community members.</p>
<p>In Synapse, you can find the list of Dataflows in the <strong>Develop</strong> pane.</p>
<p>You can create dataflows as well here. Go to <strong>Develop -&gt; + -&gt; Dataflow</strong>.</p>
<p><img src=""https://i.imgur.com/dFy3NRe.png"" alt=""enter image description here"" /></p>
<p><strong>You can create the clone of the Dataflow using the Data flow list here</strong>. It will contain all the dataflows including the dataflows that were created using the Data flow activity in the pipeline.</p>
<p><img src=""https://i.imgur.com/muCQFUn.png"" alt=""enter image description here"" /></p>
<p>The same will reflect in the Dataflow activity of the Pipeline.</p>
<p><img src=""https://i.imgur.com/d6KsMs6.png"" alt=""enter image description here"" /></p>
"
"73820633","Issue migrating custom positional flat file schema from Seeburger to BizTalk","<p>I am trying to migrate some positional flat file schemas from Seeburger to BizTalk and it seems I am stuck with a problem I do not know how to handle with BizTalk.</p>
<p>I'll try to reduce the complexity to the basic issue I am having.</p>
<p>Let's say that in Seeburger the following message is processed.</p>
<p><code>Field1----Field2-------Field3----Field4--[CR][LF]</code></p>
<p>I added padding character &quot;-&quot; here to be visible, and CR LF segment terminator</p>
<p>The seeburger schema has defined the following structure for the above message</p>
<p>Field1 = Min/Max length = 10 characters</p>
<p>Field2 = Min/Max length = 13 characters</p>
<p>Field3 = Min/Max length = 10 characters</p>
<p>Field4 = Min/Max length = 8 characters</p>
<p>All fields are optional so it may be possible to get a message like this only</p>
<p><code>Field1----Field2-------Field3----[CR][LF]</code></p>
<p>Until here everything works fine. The problem is that the actual messages received by Seeburger are looking like this for example:</p>
<p><code>Field1----Field2-------Field3----Field4[CR][LF]</code></p>
<p>or</p>
<p><code>Field1----Field2-------Field3[CR][LF]</code></p>
<p>Please notice that the padding characters are missing from the last element from the record. Seeburger knows to handle this but in BizTalk I am not able to validate a message like this. ( without last element length &lt; length defined in schema )</p>
<p>Biztalk expects that the length of last field from the record to be the defined length in the schema which makes sense. I did tried a lot of things but I was not able to find a solution for this.</p>
<p>Was somebody else stuck on this or do you have any suggestions?</p>
","<xml><csv><xsd><biztalk><data-integration>","2022-09-22 21:01:45","45","0","1","73860964","<ol>
<li><p>You need to set the Early Termination flag to True on the schema root
<a href=""https://i.stack.imgur.com/FlJGA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FlJGA.png"" alt=""Early Termination flag image"" /></a></p>
</li>
<li><p>Set the Early Terminate Optional Fields to Yes
<a href=""https://i.stack.imgur.com/FjeAt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FjeAt.png"" alt=""Early Terminate Optional Fields"" /></a></p>
</li>
<li><p>Make sure all the fields that are optional have the Min Occurs set to 0</p>
</li>
</ol>
<p>For an input file</p>
<pre><code>Field1----Field2-------Field3----Field4--[CR][LF]
Field1----Field2-------Field3----Field4[CR][LF]
Field1----Field2-------Field3[CR][LF]
Field1----Field2-------[CR][LF]
Field1----Field2[CR][LF]
Field1----[CR][LF]
Field1[CR][LF]

</code></pre>
<p>The schema</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-16&quot;?&gt;
&lt;xs:schema xmlns=&quot;http://Scratch2.Schema.SO73820633&quot; xmlns:b=&quot;http://schemas.microsoft.com/BizTalk/2003&quot; targetNamespace=&quot;http://Scratch2.Schema.SO73820633&quot; xmlns:xs=&quot;http://www.w3.org/2001/XMLSchema&quot;&gt;
  &lt;xs:annotation&gt;
    &lt;xs:appinfo&gt;
      &lt;schemaEditorExtension:schemaInfo namespaceAlias=&quot;b&quot; extensionClass=&quot;Microsoft.BizTalk.FlatFileExtension.FlatFileExtension&quot; standardName=&quot;Flat File&quot; xmlns:schemaEditorExtension=&quot;http://schemas.microsoft.com/BizTalk/2003/SchemaEditorExtensions&quot; /&gt;
      &lt;b:schemaInfo standard=&quot;Flat File&quot; codepage=&quot;65001&quot; default_pad_char=&quot; &quot; pad_char_type=&quot;char&quot; count_positions_by_byte=&quot;false&quot; parser_optimization=&quot;speed&quot; lookahead_depth=&quot;3&quot; suppress_empty_nodes=&quot;false&quot; generate_empty_nodes=&quot;true&quot; allow_early_termination=&quot;true&quot; early_terminate_optional_fields=&quot;true&quot; allow_message_breakup_of_infix_root=&quot;false&quot; compile_parse_tables=&quot;false&quot; root_reference=&quot;Root&quot; /&gt;
    &lt;/xs:appinfo&gt;
  &lt;/xs:annotation&gt;
  &lt;xs:element name=&quot;Root&quot;&gt;
    &lt;xs:annotation&gt;
      &lt;xs:appinfo&gt;
        &lt;b:recordInfo structure=&quot;delimited&quot; sequence_number=&quot;1&quot; preserve_delimiter_for_empty_data=&quot;true&quot; suppress_trailing_delimiters=&quot;false&quot; child_delimiter_type=&quot;hex&quot; child_delimiter=&quot;0x0D 0x0A&quot; child_order=&quot;postfix&quot; /&gt;
      &lt;/xs:appinfo&gt;
    &lt;/xs:annotation&gt;
    &lt;xs:complexType&gt;
      &lt;xs:sequence&gt;
        &lt;xs:annotation&gt;
          &lt;xs:appinfo&gt;
            &lt;groupInfo sequence_number=&quot;0&quot; xmlns=&quot;http://schemas.microsoft.com/BizTalk/2003&quot; /&gt;
          &lt;/xs:appinfo&gt;
        &lt;/xs:annotation&gt;
        &lt;xs:element maxOccurs=&quot;unbounded&quot; name=&quot;Record&quot;&gt;
          &lt;xs:annotation&gt;
            &lt;xs:appinfo&gt;
              &lt;recordInfo sequence_number=&quot;1&quot; structure=&quot;positional&quot; preserve_delimiter_for_empty_data=&quot;true&quot; suppress_trailing_delimiters=&quot;false&quot; xmlns=&quot;http://schemas.microsoft.com/BizTalk/2003&quot; /&gt;
            &lt;/xs:appinfo&gt;
          &lt;/xs:annotation&gt;
          &lt;xs:complexType&gt;
            &lt;xs:sequence&gt;
              &lt;xs:annotation&gt;
                &lt;xs:appinfo&gt;
                  &lt;groupInfo sequence_number=&quot;0&quot; xmlns=&quot;http://schemas.microsoft.com/BizTalk/2003&quot; /&gt;
                &lt;/xs:appinfo&gt;
              &lt;/xs:annotation&gt;
              &lt;xs:element minOccurs=&quot;0&quot; name=&quot;Root_Child1&quot; type=&quot;xs:string&quot;&gt;
                &lt;xs:annotation&gt;
                  &lt;xs:appinfo&gt;
                    &lt;b:fieldInfo justification=&quot;left&quot; pos_offset=&quot;0&quot; pos_length=&quot;10&quot; sequence_number=&quot;1&quot; /&gt;
                  &lt;/xs:appinfo&gt;
                &lt;/xs:annotation&gt;
              &lt;/xs:element&gt;
              &lt;xs:element minOccurs=&quot;0&quot; name=&quot;Root_Child2&quot; type=&quot;xs:string&quot;&gt;
                &lt;xs:annotation&gt;
                  &lt;xs:appinfo&gt;
                    &lt;b:fieldInfo justification=&quot;left&quot; pos_offset=&quot;0&quot; pos_length=&quot;13&quot; sequence_number=&quot;2&quot; /&gt;
                  &lt;/xs:appinfo&gt;
                &lt;/xs:annotation&gt;
              &lt;/xs:element&gt;
              &lt;xs:element minOccurs=&quot;0&quot; name=&quot;Root_Child3&quot; type=&quot;xs:string&quot;&gt;
                &lt;xs:annotation&gt;
                  &lt;xs:appinfo&gt;
                    &lt;b:fieldInfo justification=&quot;left&quot; pos_offset=&quot;0&quot; pos_length=&quot;10&quot; sequence_number=&quot;3&quot; /&gt;
                  &lt;/xs:appinfo&gt;
                &lt;/xs:annotation&gt;
              &lt;/xs:element&gt;
              &lt;xs:element minOccurs=&quot;0&quot; name=&quot;Root_Child4&quot; type=&quot;xs:string&quot;&gt;
                &lt;xs:annotation&gt;
                  &lt;xs:appinfo&gt;
                    &lt;b:fieldInfo justification=&quot;left&quot; pos_offset=&quot;0&quot; pos_length=&quot;8&quot; sequence_number=&quot;4&quot; /&gt;
                  &lt;/xs:appinfo&gt;
                &lt;/xs:annotation&gt;
              &lt;/xs:element&gt;
            &lt;/xs:sequence&gt;
          &lt;/xs:complexType&gt;
        &lt;/xs:element&gt;
      &lt;/xs:sequence&gt;
    &lt;/xs:complexType&gt;
  &lt;/xs:element&gt;
&lt;/xs:schema&gt;
</code></pre>
<p>Will parse it to below (XML tidied with Notepad++ for readability)</p>
<pre><code>&lt;Root xmlns=&quot;http://Scratch2.Schema.SO73820633&quot;&gt;
    &lt;Record xmlns=&quot;&quot;&gt;
        &lt;Root_Child1&gt;Field1----&lt;/Root_Child1&gt;
        &lt;Root_Child2&gt;Field2-------&lt;/Root_Child2&gt;
        &lt;Root_Child3&gt;Field3----&lt;/Root_Child3&gt;
        &lt;Root_Child4&gt;Field4--&lt;/Root_Child4&gt;
    &lt;/Record&gt;
    &lt;Record xmlns=&quot;&quot;&gt;
        &lt;Root_Child1&gt;Field1----&lt;/Root_Child1&gt;
        &lt;Root_Child2&gt;Field2-------&lt;/Root_Child2&gt;
        &lt;Root_Child3&gt;Field3----&lt;/Root_Child3&gt;
        &lt;Root_Child4&gt;Field4&lt;/Root_Child4&gt;
    &lt;/Record&gt;
    &lt;Record xmlns=&quot;&quot;&gt;
        &lt;Root_Child1&gt;Field1----&lt;/Root_Child1&gt;
        &lt;Root_Child2&gt;Field2-------&lt;/Root_Child2&gt;
        &lt;Root_Child3&gt;Field3&lt;/Root_Child3&gt;
    &lt;/Record&gt;
    &lt;Record xmlns=&quot;&quot;&gt;
        &lt;Root_Child1&gt;Field1----&lt;/Root_Child1&gt;
        &lt;Root_Child2&gt;Field2-------&lt;/Root_Child2&gt;
    &lt;/Record&gt;
    &lt;Record xmlns=&quot;&quot;&gt;
        &lt;Root_Child1&gt;Field1----&lt;/Root_Child1&gt;
        &lt;Root_Child2&gt;Field2&lt;/Root_Child2&gt;
    &lt;/Record&gt;
    &lt;Record xmlns=&quot;&quot;&gt;
        &lt;Root_Child1&gt;Field1----&lt;/Root_Child1&gt;
    &lt;/Record&gt;
    &lt;Record xmlns=&quot;&quot;&gt;
        &lt;Root_Child1&gt;Field1&lt;/Root_Child1&gt;
    &lt;/Record&gt;
&lt;/Root&gt;
</code></pre>
"
"73819579","Kafka: How to list all the topics that have data flowing in them","<p>How can I list all the topics that have data flowing in them?</p>
<p>I have a Kafka cluster (Confluent 7.2.1) with zookeeper. And I have over 120 topics where some of them are not active. I can list all the topics using <code>kafka-topics --list --bootstrap-server...</code>. This will return the list of all the topics.
I am looking to list the topics that currently have data in it. I have data on my topics for up to 7 days.
Thank you</p>
","<apache-kafka>","2022-09-22 19:05:31","74","1","1","73821990","<p>Out of the box, there is no easy way to do this.</p>
<p>You would need to loop over every topic, then pass name to <a href=""https://jaceklaskowski.gitbooks.io/apache-kafka/content/kafka-tools-GetOffsetShell.html"" rel=""nofollow noreferrer"">GetOffsetShell</a> command with <code>-1</code> and <code>-2</code> flags to get start and end offsets, then compare them; when they match, the topic is <strong>currently</strong> empty, but that doesn't mean that a new producer didn't spin up within the few seconds you run the tool.</p>
<p>Alternatively, there may be metrics tools like Prometheus exporters that can query the disk-usage of the broker log directories and report empty folders</p>
"
"73669955","Can we use for loop to create apache beam data flow pipeline dynamically?","<p>Can we use for loop to create apache beam data flow pipeline dynamically?
My fear is how for loop will behave in distributed environment when i am using it with data flow runner. I am sure this will work fine with direct runner</p>
<p>for example can I create pipelines dynamically like this:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>with beam.Pipeline(options=pipeline_options) as pipeline:
        for p in cdata['tablelist']:
            i_file_path = p['sourcefile']
            schemauri = p['schemauri']
            schema=getschema(schemauri)
            dest_table_id = p['targettable']

            (   pipeline | ""Read From Input Datafile"" + dest_table_id &gt;&gt; beam.io.ReadFromText(i_file_path)
                         | ""Convert to Dict"" + dest_table_id &gt;&gt; beam.Map(lambda r: data_ingestion.parse_method(r))
                         | ""Write to BigQuery Table"" + dest_table_id &gt;&gt; beam.io.WriteToBigQuery('{0}:{1}'.format(project_name, dest_table_id),
                                                                            schema=schema, write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)
            )</code></pre>
</div>
</div>
</p>
","<python><google-cloud-dataflow><apache-beam>","2022-09-10 07:11:34","187","-1","2","73682867","<p>I think it's not possible.</p>
<p>You have many other solutions to do that.</p>
<p>If you have an orchestrator like <code>Cloud Composer/Airflow</code> or <code>Cloud Workflows</code>, you can put this logic inside this orchestrator, instantiate and launch a <code>Dataflow</code> job per element in the loop :</p>
<p><strong>Solution 1, example with Airflow :</strong></p>
<pre class=""lang-py prettyprint-override""><code>for p in cdata['tablelist']:
      i_file_path = p['sourcefile']
      schemauri = p['schemauri']
      dest_table_id = p['targettable']

      options = {
          'i_file_path': i_file_path,
          'dest_table_id': dest_table_id,
          'schemauri' : schemauri,
          ...
      }

      dataflow_task = DataflowCreatePythonJobOperator(
          py_file=beam_main_file_path,
          task_id=f'task_{dest_table_id}',
          dataflow_default_options=your_default_options,
          options=options,
          gcp_conn_id=&quot;google_cloud_default&quot;
      )
      
      # You can execute your Dataflow jobs in parallel
      dataflow_task &gt;&gt; DummyOperator(task_id='END', dag=dag)
</code></pre>
<p><strong>Solution 2, with a shell script :</strong></p>
<pre class=""lang-bash prettyprint-override""><code>for module_path in ${tablelist}; do
   # Options
   i_file_path = ...
   schemauri = ...
   dest_table_id = ...

   #Python command to execute the Dataflow job
   python -m your_module.main \
        --runner=DataflowRunner \
        --staging_location=gs://your_staging_location/ \
        --temp_location=gs://your_temp_location/ \
        --region=europe-west1 \
        --setup_file=./setup.py \
        --i_file_path=$i_file_path \
        --schemauri=$schemauri \
        --dest_table_id=$dest_table_id
</code></pre>
<p>In this case <code>Dataflow</code> jobs are executed in sequential.</p>
<p>If you have too many files and <code>Dataflow</code> jobs to launch, you can think about another solution.
With a <code>shell script</code> or a <code>cloud function</code> you can get all the needed files and rename them as expected (with metadata on filename), move them in a separated object in <code>GCS</code>.</p>
<p>Then in a single <code>Dataflow</code> job :</p>
<ul>
<li>read all the previous files via a <code>pattern</code></li>
<li>Parse the metadata from filename like <code>schemauri</code> and <code>dest_table_id</code></li>
<li>apply the map operation in the job on the current element</li>
<li>write the result to <code>Bigquery</code></li>
</ul>
<p>If you don't have a huge amount of files, the two first solutions are simplers.</p>
"
"73669955","Can we use for loop to create apache beam data flow pipeline dynamically?","<p>Can we use for loop to create apache beam data flow pipeline dynamically?
My fear is how for loop will behave in distributed environment when i am using it with data flow runner. I am sure this will work fine with direct runner</p>
<p>for example can I create pipelines dynamically like this:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>with beam.Pipeline(options=pipeline_options) as pipeline:
        for p in cdata['tablelist']:
            i_file_path = p['sourcefile']
            schemauri = p['schemauri']
            schema=getschema(schemauri)
            dest_table_id = p['targettable']

            (   pipeline | ""Read From Input Datafile"" + dest_table_id &gt;&gt; beam.io.ReadFromText(i_file_path)
                         | ""Convert to Dict"" + dest_table_id &gt;&gt; beam.Map(lambda r: data_ingestion.parse_method(r))
                         | ""Write to BigQuery Table"" + dest_table_id &gt;&gt; beam.io.WriteToBigQuery('{0}:{1}'.format(project_name, dest_table_id),
                                                                            schema=schema, write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)
            )</code></pre>
</div>
</div>
</p>
","<python><google-cloud-dataflow><apache-beam>","2022-09-10 07:11:34","187","-1","2","73693028","<p>Yes, this is totally legal, and lots of pipelines (especially ML ones) are constructed this way. Your looped pipeline construction above should work just fine on all runners.</p>
<p>You can think of a Beam pipeline has having two phases: <strong>construction</strong> and <strong>execution</strong>. The first phase, construction, happens entirely in the main program and can have arbitrary loops, control statements, etc. Behind the scenes, this builds up a DAG of deferred operations (such as reads, maps, etc.) to perform. If you have a loop, each iteration will simply append more operations to this graph. The only think you can't do in this phase is inspect the data (i.e. the contents of a PCollection) itself.</p>
<p>The second phase, execution, starts when <code>pipeline.run()</code> is invoked. (For Python, this is implicitly invoked on exiting the <code>with</code> block). At this point the pipeline graph (as constructed above), its dependencies, pipeline options, etc. are passed to a <a href=""https://beam.apache.org/documentation/glossary/#runner"" rel=""nofollow noreferrer"">Runner</a> which will actually execute the fully-specified graph, ideally in parallel.</p>
<p>This is covered a bit in the <a href=""https://beam.apache.org/documentation/programming-guide/#overview"" rel=""nofollow noreferrer"">programming guide</a>, though I agree it could be more clear.</p>
"
"73662488","Shopware 6 Plugin Api get Product data with selected field to reduce response payload","<p>We created a plugin and now we want to get selected fields only on the response, to minimize the response payload.</p>
<p>We are aware that criteria &quot;includes&quot; will help based on the URL
<a href=""https://developer.shopware.com/docs/guides/integrations-api/general-concepts/search-criteria"" rel=""nofollow noreferrer"">https://developer.shopware.com/docs/guides/integrations-api/general-concepts/search-criteria</a></p>
<p>In the Array of <code>$criteria</code> we can see what &quot;includes&quot; is added like filter but does not give the selected fields</p>
<p>Please help what I am doing wrong?</p>
<p>EXample</p>
<pre><code>use Shopware\Core\Framework\DataAbstractionLayer\Search\Criteria;
$Criteria = new Criteria();
$Criteria-&gt;addFilter(new EqualsFilter('id', $productId));
$Criteria-&gt;setIncludes(array('product-alias' =&gt; array('name')));
$productRepo = $this-&gt;productRepository-&gt;search($Criteria, $context);
</code></pre>
<p>I have tried</p>
<pre><code>$Criteria-&gt;setIncludes(array('product' =&gt; array('name')));
$Criteria-&gt;setIncludes(array('name'));
</code></pre>
","<api><plugins><entity><criteria><shopware6>","2022-09-09 12:54:08","320","0","1","73672659","<p>The <code>includes</code> collection is only used when serializing the objects for the response json, as you can find in the <a href=""https://github.com/shopware/platform/blob/027efc5bd32b6e91475784e874f6157fc8a76fe5/src/Core/Framework/Api/Serializer/JsonApiEncoder.php#L47"" rel=""nofollow noreferrer""><code>JsonApiEncoder</code></a>. It's not in effect when fetching mapped objects internally, as it could lead to conflicts with only partially mapped objects. You could of course just use the <code>JsonApiEncoder</code> internally to encode a collection of entities to receive a response object including a subset of the data.</p>
"
"73640801","Cannot see Azure Integration feature in MaaS360","<p>I am trying to setup MaaS360 device compliance through Azure AD Conditional Access and having an issue with Azure Integration menu in MaaS360.
Basically one of the steps requires to setup &quot;Device compliance status sync for Android and iOS&quot; which requires the Azure tenant ID and Client ID established.</p>
<p>I am not able to see this checkbox when I go to the Setup-&gt;Azure Integration menu in MaaS360.</p>
<p>I only have 2 checkboxes that I allowed to configure:</p>
<ul>
<li>User Authentication</li>
<li>User Visibility</li>
</ul>
<p>I have been provided full admin roles on my account and I am not sure why else I cannot see this menu.</p>
<p>Here is the IBM article that I am following and if you see step 7 it shows the menu option.
<a href=""https://www.ibm.com/docs/en/maas360?topic=iaam-integrating-maas360-microsoft-enforce-device-compliance-through-azure-ad-conditional-access"" rel=""nofollow noreferrer"">https://www.ibm.com/docs/en/maas360?topic=iaam-integrating-maas360-microsoft-enforce-device-compliance-through-azure-ad-conditional-access</a></p>
<p>Any help is appreciated.</p>
<p>Thanks</p>
","<mobile><azure-active-directory><mdm><maas>","2022-09-07 19:50:00","97","0","1","73708327","<p>I was able to solve this, needed to enable this by opening a case with IBM to enable Azure conditional access.</p>
"
"73633302","Remove emojis from fields in ADF using data flow","<p>I am doing MongoDB to SQL conversion using Azure Data factory. My source has some fields which include emojis like 😉 in the value and also some characters like Ø.
So, I want to only remove emojis and not the other special characters.</p>
<p>I am using dataflow in ADF, for conversion.</p>
","<azure><unicode><emoji><azure-data-factory>","2022-09-07 09:44:41","63","0","1","73635871","<p>You can use REPLACE function</p>
<pre><code>   {
    &quot;name&quot;: &quot;pipeline7&quot;,
    &quot;properties&quot;: {
        &quot;activities&quot;: [
            {
                &quot;name&quot;: &quot;Set variable1&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;Test&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@pipeline().parameters.Test&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            },
            {
                &quot;name&quot;: &quot;Set variable2&quot;,
                &quot;type&quot;: &quot;SetVariable&quot;,
                &quot;dependsOn&quot;: [
                    {
                        &quot;activity&quot;: &quot;Set variable1&quot;,
                        &quot;dependencyConditions&quot;: [
                            &quot;Succeeded&quot;
                        ]
                    }
                ],
                &quot;userProperties&quot;: [],
                &quot;typeProperties&quot;: {
                    &quot;variableName&quot;: &quot;Rep&quot;,
                    &quot;value&quot;: {
                        &quot;value&quot;: &quot;@replace(pipeline().parameters.Test,'😉','')&quot;,
                        &quot;type&quot;: &quot;Expression&quot;
                    }
                }
            }
        ],
        &quot;parameters&quot;: {
            &quot;Test&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;defaultValue&quot;: &quot;😉&quot;
            }
        },
        &quot;variables&quot;: {
            &quot;Test&quot;: {
                &quot;type&quot;: &quot;String&quot;
            },
            &quot;Rep&quot;: {
                &quot;type&quot;: &quot;String&quot;
            }
        },
        &quot;annotations&quot;: []
    }
}
</code></pre>
<p>Note: The above JSON is for simple expression and not dataflow but the replace functionality remains the same.</p>
<p>you can leverage the same in dataflows</p>
"
"73631341","Azure datafactory: My REST api call works in copy data but not in data flow, why?","<p>I am trying to set up a data pipeline in ADF. I can't seem to figure out why an API call works as a source in the &quot;copy data&quot; block in the pipeline, but when I try to use it as a source in the data flow it throws an error.</p>
<p>This is the request in the copy data block: <a href=""https://i.stack.imgur.com/9wzSZ.png%27"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/9wzSZ.png'</a></p>
<p>This is the request in the data flow tab: : <a href=""https://i.stack.imgur.com/Onpuk.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/Onpuk.png</a></p>
<p>And this is the error I get: <a href=""https://i.stack.imgur.com/f9zdb.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/f9zdb.png</a></p>
<p>The request body should be the same. Still I get a good response when I preview the data in the copy block: <a href=""https://i.stack.imgur.com/cKZsb.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/cKZsb.png</a></p>
<p>Is there some setting that is different that I am missing?</p>
<p>EDIT: the url for the request is <a href=""https://www.daggegevens.knmi.nl/klimatologie/uurgegevens"" rel=""nofollow noreferrer"">https://www.daggegevens.knmi.nl/klimatologie/uurgegevens</a> with the request body <code>start=2022010101&amp;end=2022090724&amp;stns=330&amp;vars=DD:FH:FF:FX&amp;fmt=json</code></p>
","<azure><azure-data-factory>","2022-09-07 07:04:43","351","0","1","73824505","<p>I have reproduced same error in Dataflow.</p>
<p><img src=""https://i.imgur.com/NkkP934.png"" alt=""error"" /></p>
<ul>
<li>I tried to give Request body in query parameters as in below image</li>
</ul>
<p><img src=""https://i.imgur.com/knPSMUs.png"" alt=""Query params"" /></p>
<ul>
<li>Response of this API had header and body in each line item.
<img src=""https://i.imgur.com/r8Zi3YJ.png"" alt=""response"" /></li>
<li>Thus, <strong>select transformation</strong>  is used to select only the body response and copied it to the sink
<img src=""https://i.imgur.com/78QSjsw.png"" alt=""select transformation"" /></li>
</ul>
<p>Outputs that I received from both copy activity and data flow activity are same.</p>
<p><strong>copy activity output</strong>
<img src=""https://i.imgur.com/jpK1X3B.png"" alt=""copy activity output"" /></p>
<p><strong>data flow activity output</strong><img src=""https://i.imgur.com/vmwMIzA.png"" alt=""data flow activity output"" /></p>
"
"73591865","Deploy Spring cloud data flow on Google compute engine with data flow scheduling feature, and without using K8s or Cloud Foundry","<p>As they mention in the <a href=""https://dataflow.spring.io/docs/installation/"" rel=""nofollow noreferrer"">documentation</a>, there is only 3 ways how you can install spring cloud data flow:</p>
<ol>
<li>local (manually or using docker compose)</li>
<li>Cloud foundry</li>
<li>K8s</li>
</ol>
<p>And local platform is intented to be used only for development, so for production you will be forced to go with the second or third options.</p>
<p>Also if you want to use the scheduling feature, it's not available for local platform. SCDF scheduling feature is supported on Cloud Foundry and Kubernetes using the CF and K8s schedulers.</p>
<p>The problem is that I have a Spring Batch application hosted on Google compute engine, and I want to host also SDCF with scheduling feature on compute engine, because in the company I worked with, they want to host all their projects on Google cloud platform.</p>
<p>I have searched if I can use Google cloud platform as a provider for cloud foundry and use compute engine, but didn't find a solution.</p>
<p>So my question is, is there any way I can host Spring cloud data flow on Google compute engine with scheduling feature, and without using K8s ?</p>
","<google-cloud-platform><spring-batch><cloud-foundry><spring-cloud-dataflow>","2022-09-03 11:24:52","49","0","1","73767770","<p>An option may be to run SCDF in GKE. Using the helm chart assembled from bitnami, you can easily integrate with Cloud SQL and GAR. We run this setup with great success.</p>
"
"73577072","Is it possible to maintain folder structure and sink files in a mapping data flow?","<p>I need to process multiple files at once in a mapping data flow, but need to maintain the source folder structure when syncing.
In other words, the structure</p>
<pre><code>year=yyyy/month=mm/day=dd/files
</code></pre>
<p>must be maintained on the sink side as well.
Is there a way to configure the sink settings to accomplish this?
Please check the attached folder structure of the source data and the design of the mapping data flow.
Any help would be appreciated.
Thank you.</p>
","<azure><azure-pipelines><azure-data-factory><azure-synapse>","2022-09-02 01:55:34","160","1","1","73714412","<p>Yes, you can make the Sink path dynamic in the <code>Dataset</code> itself and then use that Dataset in the Sink in Data flow Mapping.</p>
<p>Under <code>Connection</code> tab in <strong>Dataset</strong>, for <code>File path</code> option use the below dynamic expression:</p>
<p><code>@concat(formatDateTime(utcNow(),'yyyy'),'/',formatDateTime(utcNow(),'MM'),'/',formatDateTime(utcNow(),'dd'),'/')</code></p>
<p><a href=""https://i.stack.imgur.com/Vb2Nz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Vb2Nz.png"" alt=""enter image description here"" /></a></p>
<p>Use the same Dataset in Data flow sink and run the pipeline.</p>
<p>Path will be created with the current <code>YYYY/MM/dd</code> format and file will be dumped there. Refer below image.</p>
<p><a href=""https://i.stack.imgur.com/cebUU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cebUU.png"" alt=""enter image description here"" /></a></p>
"
"73509020","Google cloud data flow exmaple","<p>I was trying to insert multiple csv files from cloud storage to big query using the below link, but getting an error &quot; <em><strong>attributeerror: 'filecoder' object has no attribute 'to_type_hint'</strong></em>&quot; . Can someone please help me on this</p>
<p><a href=""https://github.com/GoogleCloudPlatform/professional-services/blob/main/examples/dataflow-python-examples/batch-examples/cookbook-examples/pipelines/data_ingestion_configurable.py"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/professional-services/blob/main/examples/dataflow-python-examples/batch-examples/cookbook-examples/pipelines/data_ingestion_configurable.py</a></p>
","<python><google-cloud-platform><google-bigquery><apache-beam><dataflow>","2022-08-27 07:04:28","62","0","1","73534653","<p>It looks like FileCoder is incorrectly not inheriting from <code>beam.coders.Coder</code>; I suspect fixing this will make the issue go away.</p>
<p>It would actually be preferable to use a DoFn rather than a Coder here anyway, e.g.</p>
<pre><code>class CsvLineDecoder(beam.DoFn):
    &quot;&quot;&quot;Encode and decode CSV data coming from the files.&quot;&quot;&quot;

    def __init__(self, columns):
        self._columns = columns
        self._num_columns = len(columns)
        self._delimiter = &quot;,&quot;

    def process(self, value):
        st = io.StringIO(value)
        cr = csv.DictWriter(st,
                            self._columns,
                            delimiter=self._delimiter,
                            quotechar='&quot;',
                            quoting=csv.QUOTE_MINIMAL)
        return next(cr)
</code></pre>
<p>which would then be used as</p>
<pre><code>(p
 | 'Read From Text - ' + input_file &gt;&gt; beam.io.ReadFromText(gs_path, skip_header_lines=1)
 | beam.ParDo(CsvLineDecoder(list(fields.keys())))
 ...)

</code></pre>
"
"73452058","SAP BW data integration with Snowflake","<p>Is there anyway to get the data and connect directly to Snowflake without any third party or open source software?</p>
<p>Our current setup is getting the data from SAP BW into DATAMART and then it is used by PowerBI.</p>
<p>We have a client request to do assessment for moving the data from SAP BW to Snowflake directly cause after research I found that Snowflake doesn't allow a direct connection with SAP or OData data sources.</p>
<p>Is there any recommendation or concerns in going with this approach?</p>
<p>Thank you.</p>
","<ssis><snowflake-cloud-data-platform><sap-bw>","2022-08-22 23:57:38","170","0","1","73476730","<p>The only data ingestion capabilities Snowflake has is to bulk load from files held on a cloud platform (S3, Azure Blob, etc.).</p>
<p>If you can't, or don't want to, get your data into one of these file stores then you'd need to use a commercial ETL tool or &quot;roll your own&quot; solution using e.g. Python</p>
"
"73418169","Android launch application cross profile","<p>I have an Android app in the Personal Profile App A. I know there is an app installed in Work Profile, not my app, App B.</p>
<p>Is it possible to launch App B from App A even if there is user approval or some minimal user intervention needed?</p>
","<android><mdm>","2022-08-19 14:15:48","200","0","2","73418557","<p>Yes, It's possible to launch App B in work profile from App A in personal profile.</p>
<ul>
<li>App A in personal profile can launch a custom share intent which can be handled by the App B in work profile</li>
<li>By default , most intents do not cross from one profile to the another. The profile IT admin imposes restrictions on which intents are allowed to cross to the other profiles.</li>
<li>The application <a href=""https://github.com/android/enterprise-samples/tree/main/BasicManagedProfile#readme"" rel=""nofollow noreferrer"">Android BasicManagedProfile Sample</a> can be setup as an IT profile admin to understand and control intent forwarding between profiles.</li>
<li>In general intents are handled on the same profile where those are fired from . Make sure there's no handler for the intent fired from App A on the personal profile AND the admin restrictions on the work profile allow intents to cross the profile boundaries.</li>
</ul>
<p>Further reading <a href=""https://developer.android.com/work/managed-profiles#prevent_failed_intents"" rel=""nofollow noreferrer"">Prevent failed intents</a> , <a href=""https://developer.android.com/work/managed-profiles"" rel=""nofollow noreferrer"">Work profiles</a> on developers.android.com</p>
"
"73418169","Android launch application cross profile","<p>I have an Android app in the Personal Profile App A. I know there is an app installed in Work Profile, not my app, App B.</p>
<p>Is it possible to launch App B from App A even if there is user approval or some minimal user intervention needed?</p>
","<android><mdm>","2022-08-19 14:15:48","200","0","2","74358147","<p>Profile Owner in this case MDM probably as it is creating work profile can configure CrossProfileIntentFilter that would allow cross profile intent resolution and once you start intent it will give option to user to select either main application or work profile application</p>
"
"73339266","CR/LF as line terminator in Synapse data flow using delimited text as a sink","<p>I am using a data flow in Synapse, the sink is Delimited text.  I have to provide the output to a system that expects CR/LF (\r\n) as the row terminator.</p>
<p><strong>Default (\r,\n, or \r\n)</strong></p>
<p>returns \n (LF) only as the row terminator in all of my tests.  Has anyone had this requirement and found a work around?</p>
","<azure-data-factory><azure-synapse>","2022-08-12 20:04:54","199","1","3","73397179","<p>In <code>DataFlow mapping</code> for separating rows in Delimited text format there are certain default values:</p>
<ul>
<li><strong>To Read from file: &quot;\r\n&quot;, &quot;\r,&quot; or &quot;\n&quot;</strong></li>
<li><strong>To Write in file: &quot;\n&quot;</strong></li>
</ul>
<p>To workaround try by manually adding in dataflow script and running the pipeline.</p>
<pre class=""lang-json prettyprint-override""><code>rowDelimiter: '\r\n'
</code></pre>
<p><img src=""https://i.imgur.com/LMCua0G.png"" alt=""enter image description here"" /></p>
"
"73339266","CR/LF as line terminator in Synapse data flow using delimited text as a sink","<p>I am using a data flow in Synapse, the sink is Delimited text.  I have to provide the output to a system that expects CR/LF (\r\n) as the row terminator.</p>
<p><strong>Default (\r,\n, or \r\n)</strong></p>
<p>returns \n (LF) only as the row terminator in all of my tests.  Has anyone had this requirement and found a work around?</p>
","<azure-data-factory><azure-synapse>","2022-08-12 20:04:54","199","1","3","73507777","<p>[pratiklad] (<a href=""https://stackoverflow.com/users/18043699/pratiklad"">https://stackoverflow.com/users/18043699/pratiklad</a>) has it right.  As odd as it sounds CR/LF is not supported in data flow in sink.  Seems the best I can do, if I am using a data flow, is to out to my storage account, then use copy activity to open the file and re-write it adding the CR/LF (\r\n).</p>
"
"73339266","CR/LF as line terminator in Synapse data flow using delimited text as a sink","<p>I am using a data flow in Synapse, the sink is Delimited text.  I have to provide the output to a system that expects CR/LF (\r\n) as the row terminator.</p>
<p><strong>Default (\r,\n, or \r\n)</strong></p>
<p>returns \n (LF) only as the row terminator in all of my tests.  Has anyone had this requirement and found a work around?</p>
","<azure-data-factory><azure-synapse>","2022-08-12 20:04:54","199","1","3","74224105","<p>You can concat a '\r' on the last column of your dataset and then it will read \r\n on the text or csv files.</p>
"
"73324552","No data flows from data grid to Observable Collection after hitting a button","<p>I am using Editable Datagrid in WPF, when ever i edit the grid like creating new records and hitting a button say for example create button, Then data from my data grid flows properly to the Observable Collection in code behind .cs file. But when i load data to my grid with an excel upload and clicked the create button, then there is no data available in the Observable Collection in code behind. Its weird. Need some help. Find my code samples below</p>
<p>Data grid in XAML:</p>
<pre><code>    &lt;DataGrid x:Name=&quot;bulkGroupCreationDataGrid&quot; ItemsSource=&quot;{Binding BulkGroupCreationModel}&quot; ContextMenuOpening=&quot;TheGrid_ContextMenuOpening&quot; 
                                  Height=&quot;300&quot; CanUserAddRows=&quot;True&quot; CanUserDeleteRows=&quot;True&quot; CanUserResizeColumns=&quot;True&quot; IsManipulationEnabled=&quot;True&quot; AutoGenerateColumns=&quot;False&quot; HorizontalScrollBarVisibility=&quot;Auto&quot; VerticalScrollBarVisibility=&quot;Auto&quot; SelectionChanged=&quot;bulkGroupCreationDataGrid_SelectionChanged_1&quot;&gt;
    
                            &lt;DataGrid.Columns&gt;
                                &lt;DataGridTextColumn Width=&quot;100&quot; Header=&quot;Group&quot; Binding=&quot;{Binding Group, ValidatesOnExceptions=True}&quot; /&gt;
                                &lt;DataGridComboBoxColumn Header=&quot;Scope&quot; Width=&quot;100&quot; x:Name=&quot;Scope&quot; 
                        SelectedValueBinding=&quot;{Binding Scope, Mode=TwoWay}&quot;  
                        DisplayMemberPath=&quot;{Binding Scope}&quot;/&gt;
    
                                &lt;DataGridComboBoxColumn Header=&quot;Type&quot; Width=&quot;100&quot; x:Name=&quot;GroupType&quot; 
                        SelectedValueBinding=&quot;{Binding GroupType, Mode=TwoWay}&quot;  
                        DisplayMemberPath=&quot;{Binding GroupType}&quot;/&gt;
                                &lt;DataGridTextColumn Width=&quot;100&quot; Header=&quot;Description&quot; Binding=&quot;{Binding Description, ValidatesOnExceptions=True}&quot; /&gt;
                                &lt;DataGridTextColumn Width=&quot;70&quot; Header=&quot;OU&quot; Binding=&quot;{Binding OU, ValidatesOnExceptions=True}&quot; /&gt;
                                &lt;DataGridTextColumn Width=&quot;70&quot; Header=&quot;Sub-OU&quot; Binding=&quot;{Binding SubOU, ValidatesOnExceptions=True}&quot; /&gt;
                            &lt;/DataGrid.Columns&gt;
    
    
                            &lt;DataGrid.CommandBindings&gt;
                                &lt;CommandBinding Command=&quot;{x:Static ApplicationCommands.Paste}&quot; 
                            CanExecute=&quot;CanPaste&quot; Executed=&quot;Paste&quot;/&gt;
                                &lt;CommandBinding Command=&quot;{x:Static ApplicationCommands.New}&quot; 
                            CanExecute=&quot;CanAddNew&quot; Executed=&quot;AddNew&quot;/&gt;
                            &lt;/DataGrid.CommandBindings&gt;
                            &lt;DataGrid.ContextMenu&gt;
                                &lt;ContextMenu&gt;
                                    &lt;MenuItem Command=&quot;{x:Static ApplicationCommands.Paste}&quot; Header=&quot;Paste&quot;/&gt;
                                    &lt;MenuItem Command=&quot;{x:Static ApplicationCommands.New}&quot; Header=&quot;New row&quot;/&gt;
                                &lt;/ContextMenu&gt;
                            &lt;/DataGrid.ContextMenu&gt;
    
                        &lt;/DataGrid&gt;
 &lt;Button x:Name=&quot;Create&quot; 
                            Content=&quot;Create&quot; 
                            Click=&quot;Create_Click&quot;  Foreground=&quot;White&quot; FontWeight=&quot;Bold&quot; Width=&quot;100&quot; Height=&quot;25&quot; Background=&quot;#5cb85c&quot; RenderTransformOrigin=&quot;0.67,0.219&quot; /&gt;
</code></pre>
<p>Code Behind:</p>
<pre><code>public partial class BulkGroupCreationUserControl : UserControl
    {
        BulkGroupCreationClass bulkGroupCreationObj = new BulkGroupCreationClass();

        private ObservableCollection&lt;BulkGroupCreationModel&gt; _bulkGroupCreationCollection;


        public ObservableCollection&lt;BulkGroupCreationModel&gt; BulkGroupCreationCollection
        {

            get { return _bulkGroupCreationCollection ?? (_bulkGroupCreationCollection = new ObservableCollection&lt;BulkGroupCreationModel&gt;()); }

            set { _bulkGroupCreationCollection = value; }
        }
//constructor
        public BulkGroupCreationUserControl()
        {
            InitializeComponent();
            bulkGroupCreationDataGrid.ItemsSource = BulkGroupCreationCollection;
        }
</code></pre>
<p>buttonClick Code</p>
<pre><code> private async void Create_Click(object sender, System.Windows.RoutedEventArgs e)
        {
//Just checking whether i receive data here, I receive it when data manually typed in grid, but not when uploaded from excel
BulkGroupCreationCollection.Count;

}
</code></pre>
<p>Excel to data grid data upload</p>
<pre><code> private void Upload_Click(object sender, RoutedEventArgs e)
        {
            //Paste here
            OpenFileDialog openfile = new OpenFileDialog();
            openfile.DefaultExt = &quot;.xlsx&quot;;
            openfile.Filter = &quot;(.xlsx)|*.xlsx&quot;;
            //openfile.ShowDialog();

            var browsefile = openfile.ShowDialog();

            if (browsefile == true)
            {
                txtFileUpload.Text = openfile.FileName;

                Microsoft.Office.Interop.Excel.Application excelApp = new Microsoft.Office.Interop.Excel.Application();
                //Static File From Base Path...........
                //Microsoft.Office.Interop.Excel.Workbook excelBook = excelApp.Workbooks.Open(AppDomain.CurrentDomain.BaseDirectory + &quot;TestExcel.xlsx&quot;, 0, true, 5, &quot;&quot;, &quot;&quot;, true, Microsoft.Office.Interop.Excel.XlPlatform.xlWindows, &quot;\t&quot;, false, false, 0, true, 1, 0);
                //Dynamic File Using Uploader...........
                Microsoft.Office.Interop.Excel.Workbook excelBook = excelApp.Workbooks.Open(txtFileUpload.Text.ToString(), 0, true, 5, &quot;&quot;, &quot;&quot;, true, Microsoft.Office.Interop.Excel.XlPlatform.xlWindows, &quot;\t&quot;, false, false, 0, true, 1, 0);
                Microsoft.Office.Interop.Excel.Worksheet excelSheet = (Microsoft.Office.Interop.Excel.Worksheet)excelBook.Worksheets.get_Item(1); ;
                Microsoft.Office.Interop.Excel.Range excelRange = excelSheet.UsedRange;

                string strCellData = &quot;&quot;;
                double douCellData;
                int rowCnt = 0;
                int colCnt = 0;

                System.Data.DataTable dt = new System.Data.DataTable();
                for (colCnt = 1; colCnt &lt;= excelRange.Columns.Count; colCnt++)
                {
                    string strColumn = &quot;&quot;;
                    strColumn = (string)(excelRange.Cells[1, colCnt] as Microsoft.Office.Interop.Excel.Range).Value2;
                    dt.Columns.Add(strColumn, typeof(string));
                }

                for (rowCnt = 2; rowCnt &lt;= excelRange.Rows.Count; rowCnt++)
                {
                    string strData = &quot;&quot;;
                    for (colCnt = 1; colCnt &lt;= excelRange.Columns.Count; colCnt++)
                    {
                        try
                        {
                            strCellData = (string)(excelRange.Cells[rowCnt, colCnt] as Microsoft.Office.Interop.Excel.Range).Value2;
                            strData += strCellData + &quot;|&quot;;
                        }
                        catch (Exception ex)
                        {
                            douCellData = (excelRange.Cells[rowCnt, colCnt] as Microsoft.Office.Interop.Excel.Range).Value2;
                            strData += douCellData.ToString() + &quot;|&quot;;
                        }
                    }
                    strData = strData.Remove(strData.Length - 1, 1);
                    dt.Rows.Add(strData.Split('|'));
                }
           
                bulkGroupCreationDataGrid.ItemsSource = dt.DefaultView;
               
                excelBook.Close(true, null, null);  
                excelApp.Quit();
            }
        }
    }
</code></pre>
","<c#><.net><wpf><vb.net><datagrid>","2022-08-11 17:10:56","53","-1","1","73324684","<p>Look at this line in your <em>Upload_Click</em> method:</p>
<pre><code>bulkGroupCreationDataGrid.ItemsSource = dt.DefaultView;
</code></pre>
<p>What are you doing here? You are setting the DataGrid's ItemsSource to <em>dt.DefaultView</em>. In other words, when this line is being executed the DataGrid's ItemsSource is not the ObservableCollection anymore, it's <em>dt.DefaultView</em>. Additionally, the <em>Upload_Click</em> method reads the data from the Excel sheet into the DataTable, not into the ObservableCollection. (As a side note, even if your were to set your ItemsSource binding to be in TwoWay mode in an attempt to pass the new ItemsSource value back to the bound (View)Model property, it would quite likely not work without some changes in the respective (View)Model and possibly elsewhere in your code because a DataTable.DefaultView is not convertible to an ObservableCollection.)</p>
<p>Consequently, when <em>Upload_Click</em> is executed, the ObservableCollection provided by the (View)Model used as DataContext remains untouched (because of the Excel data <em>not</em> going into the ObservableCollection, and the DataGrid <em>not</em> using the ObservableCollection as its ItemsSource anymore).</p>
<p>Instead of swapping the ItemsSource of the DataGrid, i suggest you choose either DataTable <strong>or</strong> ObservableCollection as the type of your DataGrid's ItemsSource and stick with it. Do not manipulate the DataGrid's ItemsSource property directly. If you need to load (or change) content of the DataGrid programmatically (for example as part of the <em>Upload_Click</em> method), alter the DataTable / ObservableCollection instance that is already bound to the DataGrid and is part of the (View)Model that serves as the DataContext for the DataGrid's ItemsSource binding.</p>
<p>If you really need to change the bound DataTable/ObservableCollection to a different DataTable/ObservableCollection instance, change the value of the <em>BulkGroupCreationModel</em> property in the respective (View)Model that serves as DataContext, instead of manipulating the DataGrid's ItemsSource directly in the code-behind.</p>
"
"73321981","How can work the app restriction for deploy android app with a MDM config","<p>I need to implement an android config by a MDM (like Intunes). So I find the app_restictions. To do this, I did this in <code>AndroidManifest.xml</code>:</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;meta-data android:name=&quot;android.content.APP_RESTRICTIONS&quot;
            android:resource=&quot;@xml/app_restrictions&quot; /&gt;
</code></pre>
<p>And this in <code>app_restrictions.xml</code>:</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;restrictions xmlns:android=&quot;http://schemas.android.com/apk/res/android&quot;&gt;

    &lt;restriction
        android:key=&quot;domain&quot;
        android:title=&quot;Server domain&quot;
        android:restrictionType=&quot;string&quot;/&gt;

&lt;/restrictions&gt;
</code></pre>
<p>So, when I put this piece of code I have a bad result:</p>
<pre class=""lang-kotlin prettyprint-override""><code>val myRestrictionsMgr: RestrictionsManager = this.getSystemService(Context.RESTRICTIONS_SERVICE) as RestrictionsManager
val appRestrictions: Bundle = myRestrictionsMgr.applicationRestrictions

Log.e(TAG, appRestrictions.hasFileDescriptors().toString()) // &quot;False&quot;
val domain: String? =
    if (appRestrictions.containsKey(&quot;domain&quot;)) {
        appRestrictions.getString(&quot;domain&quot;)
    } else {
        &quot;NOP&quot;
    }
Log.e(TAG, domain.toString()) // &quot;NOP&quot;
</code></pre>
<p>And to finish, when I try to deploy my app by a MDM, the configuration doesn't not appear.</p>
","<android><android-studio><kotlin><mdm>","2022-08-11 13:58:33","93","2","1","73538797","<p>In my case, I can see app restrictions <strong>only</strong> when I deploy it through an MDM through a private <em>(or public)</em> Play Store. When we do this, we have access to the application restriction configuration.</p>
<p>To solve this problem, I decided to duplicate the restriction of my  application in my metadata <em>(for dev env only)</em>. In my service <em>(which get the app restrictions)</em>, I check if I am in <strong>prod</strong> or <strong>dev</strong> env <em>(by a MetaData)</em>, if I'm:</p>
<ul>
<li>in <strong>dev</strong>: I take MetaData</li>
<li>in <strong>prod</strong>: I take AppRestriction</li>
</ul>
<p>Here is my new function <em>(simplify)</em>:</p>
<pre class=""lang-kotlin prettyprint-override""><code>private fun genericFunction(dataKey: String): String?
{
    var value: String? = null

    if (this.isDevEnv) {        // If dev env, take meta data

        value = this.metaData.getString(dataKey)

    } else {                    // If not dev env, take restrictions data
        
        value = this.appRestrictions.getString(dataKey)
         
    }

    return value
}
</code></pre>
<p>-- <strong>OR</strong> if you prefer the compact way --</p>
<pre class=""lang-kotlin prettyprint-override""><code>private fun genericFunction(dataKey: String): String?
{
    return this.isDevEnv ? this.metaData.getString(dataKey) : this.appRestrictions.getString(dataKey)
}
</code></pre>
"
"73279833","Stripe - expand Product data with Prices","<p>Basically I'm just trying to expand the product call in stripe with prices, so I don't have to make another API for each product that's retrieved.</p>
<pre><code>Product.list(ProductListParams.builder()
                    .setActive(true)
                    .addExpand(&quot;data.prices&quot;)
                    .build());
</code></pre>
<p>I've tried both data.prices and data.price but the Product Collection response -&gt; defaultPrice is still just returning the prices id with a null expandable object.</p>
<p>I've also tried data.defaultPrice, but the call fails completely in this case.</p>
<p>Thanks all.</p>
","<java><stripe-payments>","2022-08-08 14:50:54","694","0","1","73283146","<p>Unfortunately you can't list all of a product's prices from the list products call. To get a full list of prices by product, you will need to make a List Prices call for each product ID that you want the prices for.[1]</p>
<p>The reason that you are getting an error is that Stripe's Product objects do not have a <code>prices</code> field to expand. This call will work if you specify an expandable field that products do have such as <code>default_price</code>.</p>
<p>[1] <a href=""https://site-admin.stripe.com/docs/api/prices/list#list_prices-product"" rel=""nofollow noreferrer"">https://site-admin.stripe.com/docs/api/prices/list#list_prices-product</a></p>
<p>[2] <a href=""https://site-admin.stripe.com/docs/api/products/object"" rel=""nofollow noreferrer"">https://site-admin.stripe.com/docs/api/products/object</a></p>
"
"73217998","Azure data factory - mapping data flows regex implementation to format a number","<p>I am creating a mapping data flow where I have a phone number column which can contain values like
(555) 555-1234 or
(555)555-1234 or
555555-1234</p>
<p>I want to extract numbers from this value. How can that be done. I have tried the below function with different variations but nothing is working.</p>
<p>regexExtract(&quot;(555) 555-1234&quot;,'\d+)')</p>
<p>regexExtract(&quot;(555) 555-1234&quot;,'(\d\d\d\d\d\d\d\d\d\d)')</p>
","<regex><azure-data-factory><regexp-replace><azure-mapping-data-flow>","2022-08-03 07:42:31","616","1","1","73230608","<p>Because you have multiple phone formats, you need to remove parentheses and spaces and dashes so you need multiple statements of regexExtract which will make your solution complicated.</p>
<p>instead, i suggest that you use <strong>regexReplace</strong>, mainly keeping only digits.
i tried it in ADF and it worked, for the sake of the demo, i added a derived column <code>phoneNumber</code> with a value:  <code> (555) 555-1234</code>
in the derived column activity i added a new column '<code>validPhoneNumber</code>' with a regexReplace value like so:</p>
<pre><code>regexReplace(phoneNumber,'[^0-9]', '')
</code></pre>
<p><a href=""https://i.stack.imgur.com/7rerj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7rerj.png"" alt=""enter image description here"" /></a></p>
<p><strong>Output</strong>:</p>
<p><a href=""https://i.stack.imgur.com/o4gq1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o4gq1.png"" alt=""enter image description here"" /></a></p>
<p>You can read about it here: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expressions-usage#regexReplace"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expressions-usage#regexReplace</a></p>
"
"73213158","Microsoft SSIS Designer ver 16.0 not able to connect to MySQL when running a data flow","<p>I try to develop SSIS ETL with MySQL as both the OLTP source and OLAP target. My DEV environment has Visual Studio 2019 + SSIS/SSDT ver 16.0 and MySQL 32bit ODBC driver 5.3 (64bit won't work for the SSIS version, and driver 8.x doesn't seem to have 32bit). I create &quot;user DSN&quot; to my ETL source and target -- &quot;system DSN&quot; won't work for SSIS.</p>
<p>I was able to create ODBC connection managers.</p>
<p>I was able to drag-n-drop ODBC source and target on a data flow.</p>
<p>For the ODBC source, I was able to use Table Name to view the table columns but got error when preview the data. So I use SQL with the MySQL syntax. Then I was able to preview the data.</p>
<p>For the ODBC target, I was able to see the columns and map the column.</p>
<p>But when I &quot;Debug --&gt; Start&quot; or run the data flow without debugging in the VS SSIS designer, I got:</p>
<p>Error: 0xC0014009 at Sure BI: There was an error trying to establish an Open Database Connectivity (ODBC) connection with the database server.
Error: 0xC020801C at Claim Party, Get catastrophe [2]: SSIS Error Code DTS_E_CANNOTACQUIRECONNECTIONFROMCONNECTIONMANAGER.  The AcquireConnection method call to the connection manager &quot;SCE_QA_32BIT_USR&quot; failed with error code 0xC0014009.  There may be error messages posted before this with more information on why the AcquireConnection method call failed.</p>
<p>What I did wrong?</p>
","<mysql><ssis><odbc>","2022-08-02 20:02:51","229","0","1","73238935","<p>One of my colleagues helped find the root cause and a solution.</p>
<p>Root Cause: Visual Studio 2019 is 32bit IDE and therefore everything that runs in the debugger should be 32bit. However, VS by default tries to make ODBC connection by 64bit drivers.</p>
<p>Solution: Choose menu option Debug --&gt; Debug Properties.
<a href=""https://i.stack.imgur.com/2H01a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2H01a.png"" alt=""enter image description here"" /></a></p>
<p>Set Debugging --&gt; Run64bitRuntime to False
<a href=""https://i.stack.imgur.com/qDgbh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qDgbh.png"" alt=""enter image description here"" /></a></p>
"
"73195619","Mapping column in Pandas DataFrame","<p><a href=""https://i.stack.imgur.com/fkV8q.png"" rel=""nofollow noreferrer"">1</a>I have created 3 data frames from a JSON file.
I'm trying to create a column on my working data frame by mapping the key column to the other 2 data frames, the method I use works, but it throws up a warning &quot;<em>A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead</em>.
How do I go around this?</p>
<p>Error image</p>
<p><img src=""https://i.stack.imgur.com/fkV8q.png"" alt=""1"" /></p>
","<python-3.x><pandas><dataframe><data-mapping>","2022-08-01 14:53:58","48","0","1","73195739","<p>Alternatively, you can use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer"">pandas merge</a> function to join the dataframes.</p>
<p>if you are just bothered by warning messages and if you don't like to see them, you can suppress them by the following code</p>
<pre><code>import warnings
warnings.filterwarnings('ignore')
</code></pre>
<p>Good luck!</p>
"
"73141779","How do you filter for a string (not) containing a substring in an Azure Data Factory data flow expression?","<p>As someone with a background in Alteryx, it has been a slow process to get up to speed with the expressions and syntax within Azure Data Factory data flows. I am trying to filter out rows containing the following string in a similar manner to this Alteryx filter code below:</p>
<pre><code>!Contains([Subtype], &quot;News&quot;)
</code></pre>
<p>After scrolling through all the string expressions in Azure Data Factory, I am struggling to find anything similar to the logic above. Thanks in advance for any help you can provide me on this front!</p>
","<azure><azure-data-factory>","2022-07-27 16:49:21","822","0","1","73142114","<p>You can use <strong><strong>Filter</strong> transformation</strong> in ADF Data flow and give the condition
for any column like below:
<br><br>
<strong>My Sample Data:</strong>
<br>
<img src=""https://i.imgur.com/9pN7hcT.png"" alt=""enter image description here"" />
<br>
<br>
Here I am filtering out the rows the which contains a string of <code>&quot;Rakesh&quot;</code> in the <code>Name</code> column with the Data flow expression <code>instr(Name,&quot;Rakesh&quot;)==0</code>.
<strong>instr()</strong> returns number of common letters. Our condition satisfies if its result is 0.</p>
<p><strong>Filter Transformation:</strong>
.<br>
<img src=""https://i.imgur.com/ZbMuPCp.png"" alt=""enter image description here"" />
<br>
<strong>Output in Data preview of filter:</strong></p>
<p>You can see the remaining rows only in the result.
<br>
<img src=""https://i.imgur.com/aicuohr.png"" alt=""enter image description here"" /></p>
"
"73124632","Oracle DB trigger-based synchronization error in history store table type 0?","<p>We are doing a <em>trigger-based</em> synch with DB Convert studio tool from Oracle as a source. This method creates triggers on each table and a <code>history_store</code> table.</p>
<p>But, the software that uses the source oracle DB gives the following error and refuses to work:</p>
<blockquote>
<p>Unknown type (table: history_store, column: pk_date_dest, type: 0)</p>
</blockquote>
<p>The <code>pk_date_dest</code> column is initialized as <code>&quot;pk_date_dest&quot; NVARCHAR2(400) NOT NULL</code>. What might be the reason for this <em>type 0</em>?</p>
","<oracle><types><triggers><data-synchronization>","2022-07-26 13:51:02","40","0","1","73138360","<p>I changed manually the <code>NVARCHAR2</code> data type of the columns to <code>VARCHAR2</code> and the error stopped appearing. Quite strange to be honest, as there are <code>NVARCHAR2</code> data types in other tables that are native to the database, but... at least that fixed it.</p>
<pre><code>ALTER TABLE tablename MODIFY (&quot;pk_date_dest&quot; VARCHAR2(400));
</code></pre>
"
"73055956","Lowercasing complex object field names in azure data factory data flow","<p>I'm trying to lowercase the field names in a row entry in azure data flow. Inside a complex object I've got something like</p>
<pre><code>{
 &quot;field&quot;: &quot;sample&quot;,
 &quot;functions&quot;: [
    {
      &quot;Name&quot;: &quot;asdf&quot;,
      &quot;Value&quot;: &quot;sdfsd&quot;
    },
    {
      &quot;Name&quot;: &quot;dfs&quot;,
      &quot;Value&quot;: &quot;zxcv&quot;
    }
  ]
}
</code></pre>
<p>and basically what I want is for &quot;Name&quot; and &quot;Value to be &quot;name&quot; and &quot;value&quot;. However can't seem to use any expressions that will work for the nested fields of a complex object in the expression builder.</p>
<p>I've tried using a something like a select with a rule-based mapping that is the rule being 1 == 1 and lower($$), but with $$ it seems to only work for root columns of the complex object and not the nested fields inside.</p>
","<azure><azure-data-factory>","2022-07-20 17:33:54","125","1","1","73061303","<p>As suggested by <strong>@Mark Kromer MSFT</strong>, for changing case of columns inside complex type select the <strong>functions</strong> in the Hierarchy level.</p>
<p>Please check the below for your reference:</p>
<p><img src=""https://i.imgur.com/a49SBQ1.png"" alt=""enter image description here"" /></p>
<p>Here, I have used both.
You can see the difference in results.</p>
<p><img src=""https://i.imgur.com/ny12sui.png"" alt=""enter image description here"" /></p>
"
"73034986","How to create a database, table and Insert data into it and use it as a source in another data flow in SSIS?","<p>I have a need to create a SQL database and a table and Insert data into the table from another SQL database . And also to use this newly created database as a oledb source in another dataflow in the same SSIS package. The table and database name are fixed.</p>
<p>I tried using script task to create database and tables. But when I  have to insert data , I am not able to give database name in the connection manager as the database is created only in runtime.</p>
<p>I have tried setting ValidExternalMetaData to false, but that doesnt seems to help as well.</p>
<p>Any idea or suggestions on how to accomplish this will be of great help. Thanks</p>
","<sql><ssis><dynamic-sql><script-task>","2022-07-19 10:07:08","120","0","2","73036032","<p>use a variable to hold the new table name create and populate the using the variable then use the variable name in the source object.</p>
"
"73034986","How to create a database, table and Insert data into it and use it as a source in another data flow in SSIS?","<p>I have a need to create a SQL database and a table and Insert data into the table from another SQL database . And also to use this newly created database as a oledb source in another dataflow in the same SSIS package. The table and database name are fixed.</p>
<p>I tried using script task to create database and tables. But when I  have to insert data , I am not able to give database name in the connection manager as the database is created only in runtime.</p>
<p>I have tried setting ValidExternalMetaData to false, but that doesnt seems to help as well.</p>
<p>Any idea or suggestions on how to accomplish this will be of great help. Thanks</p>
","<sql><ssis><dynamic-sql><script-task>","2022-07-19 10:07:08","120","0","2","73043476","<p>I think you just need two things to make this work:</p>
<ol>
<li>While developing the package, the database and table will need to exist.</li>
<li>Set DelayValidation to true on the connection manager and dataflow tasks in order to avoid failures with connection tests before they are created.</li>
</ol>
"
"73014105","How to verify if a string contains at least two numbers in Talend?","<p>Does anybody know how to verify if a string contains at least two numbers in Talend using the Tmap component?</p>
<p>Example: The string &quot;46 W Street, New York, NY 10033&quot; is true since I have &quot;46&quot; and &quot;10033&quot;</p>
","<java><talend><tmap><data-integration>","2022-07-17 17:53:24","335","0","2","73014682","<p>I think i found how to do what you want, you can go to the java api link :<a href=""https://docs.oracle.com/javase/7/docs/api/java/lang/String.html"" rel=""nofollow noreferrer"">https://docs.oracle.com/javase/7/docs/api/java/lang/String.html</a>
here you can find the contains methode wich return a boolean if there are the characters.</p>
"
"73014105","How to verify if a string contains at least two numbers in Talend?","<p>Does anybody know how to verify if a string contains at least two numbers in Talend using the Tmap component?</p>
<p>Example: The string &quot;46 W Street, New York, NY 10033&quot; is true since I have &quot;46&quot; and &quot;10033&quot;</p>
","<java><talend><tmap><data-integration>","2022-07-17 17:53:24","335","0","2","73019932","<p>In the var section : - Declare a variable called <code>checkNumber</code> that have type a <code>boolean</code></p>
<p>That have as value</p>
<pre><code>row1.YourString.matches(&quot;.*\\d.*&quot;) ? true : false
</code></pre>
"
"72982927","spring cloud data flow - s3 source not working with minio","<p>I am running a similar docker-compose of what you can find in the spring docs for running Spring Cloud Dada Flow in your local machine.</p>
<pre><code>version: '3.8'

volumes:
  zookeeper_data:
    driver: local
  kafka_data:
    driver: local
  elastic_data:
    driver: local
  minio_data1:
    driver: local
  minio_data2:
    driver: local

networks:
  sdf:
    driver: &quot;bridge&quot;

services:

  minio:
    image: quay.io/minio/minio:latest
    container_name: minio
    command: server --console-address &quot;:9001&quot; http://minio/data{1...2}
    ports:
      - '9000:9000'
      - '9001:9001'
    expose:
      - &quot;9000&quot;
      - &quot;9001&quot;
    # environment:
    #   MINIO_ROOT_USER: minioadmin       093DrIkcXK8J3SC1
    #   MINIO_ROOT_PASSWORD: minioadmin   CfjqeNxAtDLnUK8Fbhka8RwzfZTNlrf5
    hostname: minio
    volumes:
      - minio_data1:/data1
      - minio_data2:/data2
    networks:
      - sdf


  zookeeper:
    image: bitnami/zookeeper:3
    container_name: zookeeper
    ports:
      - '2181:2181'
    volumes:
      - 'zookeeper_data:/bitnami'
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    networks:
      - sdf


  kafka:
    image: bitnami/kafka:2
    container_name: kafka
    ports:
      - '9092:9092'
      - '29092:29092'
    volumes:
      - 'kafka_data:/bitnami'
    environment:
      - KAFKA_CREATE_TOPICS=&quot;requests:1:1,responses:1:1,notifications:1:1&quot;
      #- KAFKA_AUTO_CREATE_TOPICS_ENABLE=false
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:29092,PLAINTEXT_HOST://:9092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
    depends_on:
      - zookeeper
    networks:
      - sdf


  dataflow-server:
    user: root
    image: springcloud/spring-cloud-dataflow-server:${DATAFLOW_VERSION:-2.9.1}${BP_JVM_VERSION:-}
    container_name: dataflow-server
    ports:
      - &quot;9393:9393&quot;
    environment:
      - SPRING_CLOUD_DATAFLOW_APPLICATIONPROPERTIES_STREAM_SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKERS=PLAINTEXT://kafka:29092
      - SPRING_CLOUD_DATAFLOW_APPLICATIONPROPERTIES_STREAM_SPRING_CLOUD_STREAM_KAFKA_STREAMS_BINDER_BROKERS=PLAINTEXT://kafka:29092
      - SPRING_CLOUD_DATAFLOW_APPLICATIONPROPERTIES_STREAM_SPRING_CLOUD_STREAM_KAFKA_BINDER_ZKNODES=zookeeper:2181
      - SPRING_CLOUD_DATAFLOW_APPLICATIONPROPERTIES_STREAM_SPRING_CLOUD_STREAM_KAFKA_STREAMS_BINDER_ZKNODES=zookeeper:2181

      - SPRING_CLOUD_DATAFLOW_APPLICATIONPROPERTIES_STREAM_SPRING_KAFKA_STREAMS_PROPERTIES_METRICS_RECORDING_LEVEL=DEBUG
      # Set CLOSECONTEXTENABLED=true to ensure that the CRT launcher is closed.
      - SPRING_CLOUD_DATAFLOW_APPLICATIONPROPERTIES_TASK_SPRING_CLOUD_TASK_CLOSECONTEXTENABLED=true

      - SPRING_CLOUD_SKIPPER_CLIENT_SERVER_URI=${SKIPPER_URI:-http://skipper-server:7577}/api

      - SPRING_DATASOURCE_URL=jdbc:mysql://mysql:3306/dataflow
      - SPRING_DATASOURCE_USERNAME=root
      - SPRING_DATASOURCE_PASSWORD=rootpw
      - SPRING_DATASOURCE_DRIVER_CLASS_NAME=org.mariadb.jdbc.Driver
      # (Optionally) authenticate the default Docker Hub access for the App Metadata access.
      # - SPRING_CLOUD_DATAFLOW_CONTAINER_REGISTRY_CONFIGURATIONS_DEFAULT_USER=${METADATA_DEFAULT_DOCKERHUB_USER}
      # - SPRING_CLOUD_DATAFLOW_CONTAINER_REGISTRY_CONFIGURATIONS_DEFAULT_SECRET=${METADATA_DEFAULT_DOCKERHUB_PASSWORD}

      # - SPRING_CLOUD_DATAFLOW_CONTAINER_REGISTRYCONFIGURATIONS_DEFAULT_USER=${METADATA_DEFAULT_DOCKERHUB_USER}
      # - SPRING_CLOUD_DATAFLOW_CONTAINER_REGISTRYCONFIGURATIONS_DEFAULT_SECRET=${METADATA_DEFAULT_DOCKERHUB_PASSWORD}
    depends_on:
      - kafka
      - skipper-server
    entrypoint: &gt;
      bin/sh -c &quot;
         apt-get update &amp;&amp; apt-get install --no-install-recommends -y wget &amp;&amp;
         wget --no-check-certificate -P /tmp/ https://raw.githubusercontent.com/vishnubob/wait-for-it/master/wait-for-it.sh &amp;&amp;
         chmod a+x /tmp/wait-for-it.sh &amp;&amp;
         /tmp/wait-for-it.sh mysql:3306 -- /cnb/process/web&quot;
    restart: always
    volumes:
      - ${HOST_MOUNT_PATH:-.}:${DOCKER_MOUNT_PATH:-/home/cnb/scdf}
    networks:
      - sdf


  app-import-stream:
    image: springcloud/baseimage:1.0.0
    container_name: dataflow-app-import-stream
    depends_on:
      - dataflow-server
    command: &gt;
      /bin/sh -c &quot;
        ./wait-for-it.sh -t 360 dataflow-server:9393;
        wget -qO- '${DATAFLOW_URI:-http://dataflow-server:9393}/apps' --no-check-certificate --post-data='uri=${STREAM_APPS_URI:-https://dataflow.spring.io/kafka-maven-latest&amp;force=true}';
        wget -qO- '${DATAFLOW_URI:-http://dataflow-server:9393}/apps/sink/ver-log/3.0.1' --no-check-certificate --post-data='uri=maven://org.springframework.cloud.stream.app:log-sink-kafka:3.0.1';
        wget -qO- '${DATAFLOW_URI:-http://dataflow-server:9393}/apps/sink/ver-log/2.1.5.RELEASE' --no-check-certificate --post-data='uri=maven://org.springframework.cloud.stream.app:log-sink-kafka:2.1.5.RELEASE';
        wget -qO- '${DATAFLOW_URI:-http://dataflow-server:9393}/apps/sink/dataflow-tasklauncher/${DATAFLOW_VERSION:-2.9.1}' --no-check-certificate --post-data='uri=maven://org.springframework.cloud:spring-cloud-dataflow-tasklauncher-sink-kafka:${DATAFLOW_VERSION:-2.9.1}';
        echo 'Maven Stream apps imported'&quot;
    networks:
      - sdf


  skipper-server:
    user: root
    image: springcloud/spring-cloud-skipper-server:${SKIPPER_VERSION:-2.8.1}${BP_JVM_VERSION:-}
    container_name: skipper-server
    ports:
      - &quot;7577:7577&quot;
      - ${APPS_PORT_RANGE:-20000-20195:20000-20195}
    environment:
      - SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_LOCAL_ACCOUNTS_DEFAULT_PORTRANGE_LOW=20000
      - SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_LOCAL_ACCOUNTS_DEFAULT_PORTRANGE_HIGH=20190
      - SPRING_DATASOURCE_URL=jdbc:mysql://mysql:3306/dataflow
      - SPRING_DATASOURCE_USERNAME=root
      - SPRING_DATASOURCE_PASSWORD=rootpw
      - SPRING_DATASOURCE_DRIVER_CLASS_NAME=org.mariadb.jdbc.Driver
      - LOGGING_LEVEL_ORG_SPRINGFRAMEWORK_CLOUD_SKIPPER_SERVER_DEPLOYER=ERROR
    entrypoint: &gt;
      bin/sh -c &quot;
         apt-get update &amp;&amp; apt-get install --no-install-recommends -y wget &amp;&amp;
         wget --no-check-certificate -P /tmp/ https://raw.githubusercontent.com/vishnubob/wait-for-it/master/wait-for-it.sh &amp;&amp;
         chmod a+x /tmp/wait-for-it.sh &amp;&amp;
         /tmp/wait-for-it.sh mysql:3306 -- /cnb/process/web&quot;
    restart: always
    volumes:
      - ${HOST_MOUNT_PATH:-.}:${DOCKER_MOUNT_PATH:-/home/cnb/scdf}
    networks:
      - sdf


  mysql:
    image: mysql:5.7.25
    container_name: mysql
    environment:
      MYSQL_DATABASE: dataflow
      MYSQL_USER: root
      MYSQL_ROOT_PASSWORD: rootpw
    expose:
      - 3306
    networks:
      - sdf

</code></pre>
<p>I was able to add Elasticsearch to this docker-compose and successfully connect to it (using elasticsearch sink), but here I am just keeping what concerns to the issue.</p>
<p>This is how I am configuring the s3 source</p>
<pre><code>app.s3.cloud.aws.credentials.accessKey=093DrIkcXK8J3SC1
app.s3.cloud.aws.credentials.secretKey=CfjqeNxAtDLnUK8Fbhka8RwzfZTNlrf5
app.s3.cloud.aws.region.static=us-west-1
app.s3.cloud.aws.stack.auto=false
app.s3.common.endpoint-url=http://minio:9000
app.s3.supplier.remote-dir=/kafka-connect/topics
app.s3.logging.level.org.apache.http=DEBUG
</code></pre>
<p><a href=""https://i.stack.imgur.com/Cbcsn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Cbcsn.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/3f04J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3f04J.png"" alt=""enter image description here"" /></a></p>
<p>This is the error that I am getting</p>
<pre><code>
2022-07-14 14:11:32.625 DEBUG [s3-source,,] 470 --- [oundedElastic-4] o.a.h.client.protocol.RequestAddCookies  : CookieSpec selected: default
2022-07-14 14:11:32.625 DEBUG [s3-source,,] 470 --- [oundedElastic-4] o.a.h.client.protocol.RequestAuthCache   : Auth cache not set in the context
2022-07-14 14:11:32.625 DEBUG [s3-source,,] 470 --- [oundedElastic-4] h.i.c.PoolingHttpClientConnectionManager : Connection request: [route: {}-&gt;http://kafka-connect.minio:9000][total available: 0; route allocated: 0 of 50; total allocated: 0 of 50]
2022-07-14 14:11:32.626 DEBUG [s3-source,,] 470 --- [oundedElastic-4] h.i.c.PoolingHttpClientConnectionManager : Connection leased: [id: 44][route: {}-&gt;http://kafka-connect.minio:9000][total available: 0; route allocated: 1 of 50; total allocated: 1 of 50]
2022-07-14 14:11:32.626 DEBUG [s3-source,,] 470 --- [oundedElastic-4] o.a.http.impl.execchain.MainClientExec   : Opening connection {}-&gt;http://kafka-connect.minio:9000
2022-07-14 14:11:32.626 DEBUG [s3-source,,] 470 --- [oundedElastic-4] h.i.c.DefaultManagedHttpClientConnection : http-outgoing-44: Shutdown connection
2022-07-14 14:11:32.626 DEBUG [s3-source,,] 470 --- [oundedElastic-4] o.a.http.impl.execchain.MainClientExec   : Connection discarded
2022-07-14 14:11:32.626 DEBUG [s3-source,,] 470 --- [oundedElastic-4] h.i.c.PoolingHttpClientConnectionManager : Connection released: [id: 44][route: {}-&gt;http://kafka-connect.minio:9000][total available: 0; route allocated: 0 of 50; total allocated: 0 of 50]
2022-07-14 14:11:32.678 DEBUG [s3-source,,] 470 --- [oundedElastic-4] o.a.h.client.protocol.RequestAddCookies  : CookieSpec selected: default
2022-07-14 14:11:32.679 DEBUG [s3-source,,] 470 --- [oundedElastic-4] o.a.h.client.protocol.RequestAuthCache   : Auth cache not set in the context
2022-07-14 14:11:32.679 DEBUG [s3-source,,] 470 --- [oundedElastic-4] h.i.c.PoolingHttpClientConnectionManager : Connection request: [route: {}-&gt;http://kafka-connect.minio:9000][total available: 0; route allocated: 0 of 50; total allocated: 0 of 50]
2022-07-14 14:11:32.679 DEBUG [s3-source,,] 470 --- [oundedElastic-4] h.i.c.PoolingHttpClientConnectionManager : Connection leased: [id: 45][route: {}-&gt;http://kafka-connect.minio:9000][total available: 0; route allocated: 1 of 50; total allocated: 1 of 50]
2022-07-14 14:11:32.679 DEBUG [s3-source,,] 470 --- [oundedElastic-4] o.a.http.impl.execchain.MainClientExec   : Opening connection {}-&gt;http://kafka-connect.minio:9000
2022-07-14 14:11:32.679 DEBUG [s3-source,,] 470 --- [oundedElastic-4] h.i.c.DefaultManagedHttpClientConnection : http-outgoing-45: Shutdown connection
2022-07-14 14:11:32.679 DEBUG [s3-source,,] 470 --- [oundedElastic-4] o.a.http.impl.execchain.MainClientExec   : Connection discarded
2022-07-14 14:11:32.679 DEBUG [s3-source,,] 470 --- [oundedElastic-4] h.i.c.PoolingHttpClientConnectionManager : Connection released: [id: 45][route: {}-&gt;http://kafka-connect.minio:9000][total available: 0; route allocated: 0 of 50; total allocated: 0 of 50]
2022-07-14 14:11:32.704 DEBUG [s3-source,,] 470 --- [oundedElastic-4] o.a.h.client.protocol.RequestAddCookies  : CookieSpec selected: default
2022-07-14 14:11:32.704 DEBUG [s3-source,,] 470 --- [oundedElastic-4] o.a.h.client.protocol.RequestAuthCache   : Auth cache not set in the context
2022-07-14 14:11:32.704 DEBUG [s3-source,,] 470 --- [oundedElastic-4] h.i.c.PoolingHttpClientConnectionManager : Connection request: [route: {}-&gt;http://kafka-connect.minio:9000][total available: 0; route allocated: 0 of 50; total allocated: 0 of 50]
2022-07-14 14:11:32.704 DEBUG [s3-source,,] 470 --- [oundedElastic-4] h.i.c.PoolingHttpClientConnectionManager : Connection leased: [id: 46][route: {}-&gt;http://kafka-connect.minio:9000][total available: 0; route allocated: 1 of 50; total allocated: 1 of 50]
2022-07-14 14:11:32.705 DEBUG [s3-source,,] 470 --- [oundedElastic-4] o.a.http.impl.execchain.MainClientExec   : Opening connection {}-&gt;http://kafka-connect.minio:9000
2022-07-14 14:11:32.705 DEBUG [s3-source,,] 470 --- [oundedElastic-4] h.i.c.DefaultManagedHttpClientConnection : http-outgoing-46: Shutdown connection
2022-07-14 14:11:32.705 DEBUG [s3-source,,] 470 --- [oundedElastic-4] o.a.http.impl.execchain.MainClientExec   : Connection discarded
2022-07-14 14:11:32.705 DEBUG [s3-source,,] 470 --- [oundedElastic-4] h.i.c.PoolingHttpClientConnectionManager : Connection released: [id: 46][route: {}-&gt;http://kafka-connect.minio:9000][total available: 0; route allocated: 0 of 50; total allocated: 0 of 50]
2022-07-14 14:11:33.021 DEBUG [s3-source,,] 470 --- [oundedElastic-4] o.a.h.client.protocol.RequestAddCookies  : CookieSpec selected: default
2022-07-14 14:11:33.021 DEBUG [s3-source,,] 470 --- [oundedElastic-4] o.a.h.client.protocol.RequestAuthCache   : Auth cache not set in the context
2022-07-14 14:11:33.021 DEBUG [s3-source,,] 470 --- [oundedElastic-4] h.i.c.PoolingHttpClientConnectionManager : Connection request: [route: {}-&gt;http://kafka-connect.minio:9000][total available: 0; route allocated: 0 of 50; total allocated: 0 of 50]
2022-07-14 14:11:33.021 DEBUG [s3-source,,] 470 --- [oundedElastic-4] h.i.c.PoolingHttpClientConnectionManager : Connection leased: [id: 47][route: {}-&gt;http://kafka-connect.minio:9000][total available: 0; route allocated: 1 of 50; total allocated: 1 of 50]
2022-07-14 14:11:33.021 DEBUG [s3-source,,] 470 --- [oundedElastic-4] o.a.http.impl.execchain.MainClientExec   : Opening connection {}-&gt;http://kafka-connect.minio:9000
2022-07-14 14:11:33.021 DEBUG [s3-source,,] 470 --- [oundedElastic-4] h.i.c.DefaultManagedHttpClientConnection : http-outgoing-47: Shutdown connection
2022-07-14 14:11:33.021 DEBUG [s3-source,,] 470 --- [oundedElastic-4] o.a.http.impl.execchain.MainClientExec   : Connection discarded
2022-07-14 14:11:33.022 DEBUG [s3-source,,] 470 --- [oundedElastic-4] h.i.c.PoolingHttpClientConnectionManager : Connection released: [id: 47][route: {}-&gt;http://kafka-connect.minio:9000][total available: 0; route allocated: 0 of 50; total allocated: 0 of 50]
2022-07-14 14:11:33.026 ERROR [s3-source,,] 470 --- [oundedElastic-4] o.s.i.util.IntegrationReactiveUtils      : Error from Flux for : org.springframework.integration.aws.inbound.S3InboundFileSynchronizingMessageSource@3d20e22e

org.springframework.messaging.MessagingException: Problem occurred while synchronizing '/kafka-connect/topics' to local directory; nested exception is org.springframework.messaging.MessagingException: Failed to execute on session; nested exception is com.amazonaws.SdkClientException: Unable to execute HTTP request: kafka-connect.minio
    at org.springframework.integration.file.remote.synchronizer.AbstractInboundFileSynchronizer.synchronizeToLocalDirectory(AbstractInboundFileSynchronizer.java:348) ~[spring-integration-file-5.5.12.jar!/:5.5.12]
    at org.springframework.integration.file.remote.synchronizer.AbstractInboundFileSynchronizingMessageSource.doReceive(AbstractInboundFileSynchronizingMessageSource.java:267) ~[spring-integration-file-5.5.12.jar!/:5.5.12]
    at org.springframework.integration.file.remote.synchronizer.AbstractInboundFileSynchronizingMessageSource.doReceive(AbstractInboundFileSynchronizingMessageSource.java:69) ~[spring-integration-file-5.5.12.jar!/:5.5.12]
    at org.springframework.integration.endpoint.AbstractFetchLimitingMessageSource.doReceive(AbstractFetchLimitingMessageSource.java:47) ~[spring-integration-core-5.5.12.jar!/:5.5.12]
    at org.springframework.integration.endpoint.AbstractMessageSource.receive(AbstractMessageSource.java:142) ~[spring-integration-core-5.5.12.jar!/:5.5.12]
    at org.springframework.integration.util.IntegrationReactiveUtils.lambda$messageSourceToFlux$0(IntegrationReactiveUtils.java:83) ~[spring-integration-core-5.5.12.jar!/:5.5.12]
    at reactor.core.publisher.MonoCreate$DefaultMonoSink.onRequest(MonoCreate.java:221) ~[reactor-core-3.4.18.jar!/:3.4.18]
    at org.springframework.integration.util.IntegrationReactiveUtils.lambda$messageSourceToFlux$1(IntegrationReactiveUtils.java:83) ~[spring-integration-core-5.5.12.jar!/:5.5.12]
    at reactor.core.publisher.MonoCreate.subscribe(MonoCreate.java:58) ~[reactor-core-3.4.18.jar!/:3.4.18]
    at reactor.core.publisher.Mono.subscribe(Mono.java:4400) ~[reactor-core-3.4.18.jar!/:3.4.18]
    at reactor.core.publisher.MonoSubscribeOn$SubscribeOnSubscriber.run(MonoSubscribeOn.java:126) ~[reactor-core-3.4.18.jar!/:3.4.18]
    at org.springframework.cloud.sleuth.instrument.reactor.ReactorSleuth.lambda$null$6(ReactorSleuth.java:324) ~[spring-cloud-sleuth-instrumentation-3.1.3.jar!/:3.1.3]
    at reactor.core.scheduler.WorkerTask.call(WorkerTask.java:84) ~[reactor-core-3.4.18.jar!/:3.4.18]
    at reactor.core.scheduler.WorkerTask.call(WorkerTask.java:37) ~[reactor-core-3.4.18.jar!/:3.4.18]
    at java.base/java.util.concurrent.FutureTask.run(Unknown Source) ~[na:na]
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source) ~[na:na]
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[na:na]
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[na:na]
    at java.base/java.lang.Thread.run(Unknown Source) ~[na:na]
Caused by: org.springframework.messaging.MessagingException: Failed to execute on session; nested exception is com.amazonaws.SdkClientException: Unable to execute HTTP request: kafka-connect.minio
    at org.springframework.integration.file.remote.RemoteFileTemplate.execute(RemoteFileTemplate.java:461) ~[spring-integration-file-5.5.12.jar!/:5.5.12]
    at org.springframework.integration.file.remote.synchronizer.AbstractInboundFileSynchronizer.synchronizeToLocalDirectory(AbstractInboundFileSynchronizer.java:341) ~[spring-integration-file-5.5.12.jar!/:5.5.12]
    ... 18 common frames omitted
Caused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: kafka-connect.minio
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1207) ~[aws-java-sdk-core-1.11.792.jar!/:na]
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1153) ~[aws-java-sdk-core-1.11.792.jar!/:na]
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802) ~[aws-java-sdk-core-1.11.792.jar!/:na]
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770) ~[aws-java-sdk-core-1.11.792.jar!/:na]
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744) ~[aws-java-sdk-core-1.11.792.jar!/:na]
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704) ~[aws-java-sdk-core-1.11.792.jar!/:na]
    at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686) ~[aws-java-sdk-core-1.11.792.jar!/:na]
    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550) ~[aws-java-sdk-core-1.11.792.jar!/:na]
    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530) ~[aws-java-sdk-core-1.11.792.jar!/:na]
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5062) ~[aws-java-sdk-s3-1.11.792.jar!/:na]
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5008) ~[aws-java-sdk-s3-1.11.792.jar!/:na]
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5002) ~[aws-java-sdk-s3-1.11.792.jar!/:na]
    at com.amazonaws.services.s3.AmazonS3Client.listObjects(AmazonS3Client.java:898) ~[aws-java-sdk-s3-1.11.792.jar!/:na]
    at org.springframework.integration.aws.support.S3Session.list(S3Session.java:91) ~[spring-integration-aws-2.3.4.RELEASE.jar!/:na]
    at org.springframework.integration.aws.support.S3Session.list(S3Session.java:52) ~[spring-integration-aws-2.3.4.RELEASE.jar!/:na]
    at org.springframework.integration.file.remote.synchronizer.AbstractInboundFileSynchronizer.transferFilesFromRemoteToLocal(AbstractInboundFileSynchronizer.java:356) ~[spring-integration-file-5.5.12.jar!/:5.5.12]
    at org.springframework.integration.file.remote.synchronizer.AbstractInboundFileSynchronizer.lambda$synchronizeToLocalDirectory$0(AbstractInboundFileSynchronizer.java:342) ~[spring-integration-file-5.5.12.jar!/:5.5.12]
    at org.springframework.integration.file.remote.RemoteFileTemplate.execute(RemoteFileTemplate.java:452) ~[spring-integration-file-5.5.12.jar!/:5.5.12]
    ... 19 common frames omitted
Caused by: java.net.UnknownHostException: kafka-connect.minio
    at java.base/java.net.InetAddress$CachedAddresses.get(Unknown Source) ~[na:na]
    at java.base/java.net.InetAddress.getAllByName0(Unknown Source) ~[na:na]
    at java.base/java.net.InetAddress.getAllByName(Unknown Source) ~[na:na]
    at java.base/java.net.InetAddress.getAllByName(Unknown Source) ~[na:na]
    at com.amazonaws.SystemDefaultDnsResolver.resolve(SystemDefaultDnsResolver.java:27) ~[aws-java-sdk-core-1.11.792.jar!/:na]
    at com.amazonaws.http.DelegatingDnsResolver.resolve(DelegatingDnsResolver.java:38) ~[aws-java-sdk-core-1.11.792.jar!/:na]
    at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:112) ~[httpclient-4.5.13.jar!/:4.5.13]
    at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376) ~[httpclient-4.5.13.jar!/:4.5.13]
    at jdk.internal.reflect.GeneratedMethodAccessor73.invoke(Unknown Source) ~[na:na]
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[na:na]
    at java.base/java.lang.reflect.Method.invoke(Unknown Source) ~[na:na]
    at com.amazonaws.http.conn.ClientConnectionManagerFactory$Handler.invoke(ClientConnectionManagerFactory.java:76) ~[aws-java-sdk-core-1.11.792.jar!/:na]
    at com.amazonaws.http.conn.$Proxy145.connect(Unknown Source) ~[na:na]
    at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393) ~[httpclient-4.5.13.jar!/:4.5.13]
    at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236) ~[httpclient-4.5.13.jar!/:4.5.13]
    at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186) ~[httpclient-4.5.13.jar!/:4.5.13]
    at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185) ~[httpclient-4.5.13.jar!/:4.5.13]
    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83) ~[httpclient-4.5.13.jar!/:4.5.13]
    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56) ~[httpclient-4.5.13.jar!/:4.5.13]
    at com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72) ~[aws-java-sdk-core-1.11.792.jar!/:na]
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1330) ~[aws-java-sdk-core-1.11.792.jar!/:na]
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145) ~[aws-java-sdk-core-1.11.792.jar!/:na]
    ... 35 common frames omitted

</code></pre>
<p>Debugging the http connections I can see that is trying to connect to: <strong><a href=""http://kafka-connect.minio:9000"" rel=""nofollow noreferrer"">http://kafka-connect.minio:9000</a></strong></p>
<pre><code>Connection request: [route: {}-&gt;http://kafka-connect.minio:9000][total available: 0; route allocated: 0 of 50; total allocated: 0 of 50]
</code></pre>
<p>This is resulting in an Unknown host as clearly the host <strong>kafka-connect.minio</strong> doesn't exist. But I was able to curl <strong>http://minio:9000</strong> successfully (from inside the skipper-server which is the container that runs the s3 connector).</p>
<p>So here is my question actually:</p>
<ul>
<li>Why is it trying to connect to <strong>http://{BUCKET_NAME}.minio:9000</strong> instead of <strong>http://minio:9000</strong> and then go to the bucket?</li>
</ul>
<p>I am able to connect to this same Minio, to the same bucket with Kafka Connect (s3-source), and I am doing that with a similar approach, meaning, kafka-connect is a separate container in the same docker-compose and it can connect to the minio container and fetch the files. You can see how this is done here <a href=""https://stackoverflow.com/questions/72872816/kafka-connect-s3-source-not-working-with-minio"">kafka connect s3 source not working with Minio</a></p>
","<amazon-s3><spring-cloud-stream><spring-cloud-dataflow><minio>","2022-07-14 15:19:59","247","1","1","73061550","<p>Adding this property fixes the problem</p>
<pre><code>app.s3.path-style-access=true
</code></pre>
<p>By default is false, and for the kafka-connect case, by default is true</p>
"
"72976856","How to transform a list of numbers into seperate elements talend tmap","<p>I have a list of codes for each state as an input stored in a table.</p>
<p><a href=""https://i.stack.imgur.com/uvZsz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uvZsz.png"" alt=""enter image description here"" /></a></p>
<p>What I want as output is this , using tmap transformations</p>
<p><a href=""https://i.stack.imgur.com/kZZ8U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kZZ8U.png"" alt=""enter image description here"" /></a></p>
<p>This is the job I made but it doesn't seem to work correctly like I want. The output should have 1000 rows.
<a href=""https://i.stack.imgur.com/t0pKg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t0pKg.png"" alt=""enter image description here"" /></a>
Does anybody know how to solve this?</p>
","<database><talend><tmap><data-integration>","2022-07-14 07:37:09","76","0","1","72986021","<p>One way to do this is to convert your list to a string of comma-separated values (using tConvertType for instance) and then use tNormalize to split this string into individual rows.</p>
"
"72955422","What syntax should be used for reading the final row in an Array on the Mapping tab on the Copy Data activity in Azure Data Factory / Synapse?","<p>I'm using the copy data activity in Azure Data Factory to copy data from an API to our data lake for alerting &amp; reporting purposes. The API response is comprised of multiple complex nested JSON arrays with key-value pairs. The API is updated on a quarter-hourly basis and data is only held for 2 days before falling off the stack. The API adopts an oldest-to-newest record structure and so the newest addition to the array would be the final item in the array as opposed to the first.</p>
<p>My requirement is to copy only the most recent record from the API as opposed to the collection - so the 192th reading or item 191 of the array (with the array starting at 0.)
Due to the nature of the solution, there are times when the API isn't being updated as the sensors that collect and send over the data to the server may not be reachable.</p>
<p>The current solution is triggered every 15 minutes and tries a copy data activity of item 191, then 190, then 189 and so on. After 6 attempts it fails and so the record is missed.</p>
<p><a href=""https://i.stack.imgur.com/Kzder.png"" rel=""nofollow noreferrer"">current pipeline structure</a></p>
<p>I have used the mapping tab to specify the items in the array as follows (copy attempt 1 example):</p>
<p><code>$['meta']['params']['sensors'][*]['name']</code>  <br/>
<code>$['meta']['sensorReadings'][*]['readings'][191]['dateTime']</code>  <br/>
<code>$['meta']['sensorReadings'][*]['readings'][191]['value']</code>  <br/></p>
<p>Instead of explicitly referencing the array number, I was wondering if it is possible to reference the last item of the array in the above code?
I understand we can use 0 for the first record however I don't understand how to reference the final item. I've tried the following using the 'last' function but am unsure of how to place it:</p>
<p><code>$['meta']['sensorReadings'][*]['readings'][last]['dateTime']</code>  <br/>
<code>$['meta']['sensorReadings'][*]['readings']['last']['dateTime']</code>  <br/>
<code>last['meta']['sensorReadings'][*]['readings']['dateTime']</code>  <br/>
<code>$['meta']['sensorReadings'][*]['readings']last['dateTime']</code>  <br/></p>
<p><a href=""https://i.stack.imgur.com/LQdgE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LQdgE.png"" alt=""Example of my API response"" /></a></p>
<p>Any help or advice on a better way to proceed would be greatly appreciated.</p>
","<arrays><azure><azure-data-factory><pipeline><data-mapping>","2022-07-12 16:19:05","124","1","1","72969650","<p>Can you call your API with a Web activity?  If so, this pulls the API result into the data pipeline and then apply ADF functions like <code>last</code> to it.</p>
<p>A simple example calling the <a href=""https://www.api.gov.uk/gds/bank-holidays/#bank-holidays"" rel=""nofollow noreferrer"">UK Gov Bank Holidays API</a>:</p>
<p><a href=""https://i.stack.imgur.com/gwes6l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gwes6l.png"" alt=""enter image description here"" /></a></p>
<p>This returns a resultset that looks like this:</p>
<pre><code>{
    &quot;england-and-wales&quot;: {
        &quot;division&quot;: &quot;england-and-wales&quot;,
        &quot;events&quot;: [
            {
                &quot;title&quot;: &quot;New Year’s Day&quot;,
                &quot;date&quot;: &quot;2017-01-02&quot;,
                &quot;notes&quot;: &quot;Substitute day&quot;,
                &quot;bunting&quot;: true
            },
            {
                &quot;title&quot;: &quot;Good Friday&quot;,
                &quot;date&quot;: &quot;2017-04-14&quot;,
                &quot;notes&quot;: &quot;&quot;,
                &quot;bunting&quot;: false
            },
            {
                &quot;title&quot;: &quot;Easter Monday&quot;,
                &quot;date&quot;: &quot;2017-04-17&quot;,
                &quot;notes&quot;: &quot;&quot;,
                &quot;bunting&quot;: true
            },
            ... etc
</code></pre>
<p>You can now apply the last function to is, e.g. using a <code>Set Variable</code> activity:</p>
<pre><code>@string(last(activity('Web1').output['england-and-wales'].events))
</code></pre>
<p>Which yields the last bank holiday of 2023:</p>
<pre><code>{
    &quot;name&quot;: &quot;varWorking&quot;,
    &quot;value&quot;: &quot;{\&quot;title\&quot;:\&quot;Boxing Day\&quot;,\&quot;date\&quot;:\&quot;2023-12-26\&quot;,\&quot;notes\&quot;:\&quot;\&quot;,\&quot;bunting\&quot;:true}&quot;
}
</code></pre>
<p>Or</p>
<pre><code>@string(last(activity('Web1').output['england-and-wales'].events).date)
</code></pre>
"
"72953212","Is it possible to use data mapping with a custom field?","<p>I was wondering if I can link a custom field to a cms block/slot (Layout)? If not, can I write a custom plugin that allows this functionality?</p>
","<shopware><shopware6>","2022-07-12 13:35:10","263","0","1","72953615","<p>Yes. I had similar functionality. You have to override administration component to add your field to the list and your logic to load the data.</p>
"
"72949666","Is there a react-native library with Azure AD broker authentication for MDM devices","<p>I am building a cross platform application with React Native for MDM managed devices. The authentication to the Azure AD should be SSO (automatically based on the work profile / intune information) and not through a classic WebView.</p>
<p>I heard that this could be done by Broker Authentication Support (e.g. for Microsoft Authenticator) but I couldn't find any library supporting that. I've tried following libraries but most of them are not maintained anymore or they are following the WebView approach: <code>react-native-ms-adal</code>, <code>react-native-azure-adal</code>, <code>react-native-azure-ad</code>.</p>
<p>Has anyone build such an use case or does know a working library?</p>
","<react-native><azure-active-directory><mdm><azure-authentication><intune>","2022-07-12 09:03:03","325","0","1","73002626","<p>•   To enable broker authentication support for react native applications on Azure AD MDM managed devices, you will have to ensure that the following <strong>method is enabled for broker component support in ADAL for Android 3.0.0</strong>: -</p>
<pre><code> Microsoft.ADAL.AuthenticationSettings.setUseBroker(true);
</code></pre>
<p><strong>Developer needs to register special ‘redirectUri’ for broker usage</strong>. ‘RedirectUri’ is in the format of <em><strong>‘msauth://packagename/Base64UrlencodedSignature’</strong></em>. Thus, since <code>ADALis deprecated and MSAL is in place for it, you will have to use MSAL for Android 3.0+ library</code>. For that purpose, you will have to install dependencies and declare it on MSAL as below: -</p>
<pre><code> dependencies {
  implementation 'com.microsoft.identity.client:msal:3.0.+'
  }
</code></pre>
<p>Thus, in this way, you can configure Azure AD broker authentication in MDM devices. For more information, kindly refer to the below documentation link: -</p>
<p><strong><a href=""https://github.com/AzureAD/microsoft-authentication-library-for-android"" rel=""nofollow noreferrer"">https://github.com/AzureAD/microsoft-authentication-library-for-android</a></strong></p>
<p><strong><a href=""https://github.com/northwesternmutual/azure-activedirectory-library-for-react-native/blob/master/README.md#android-quirks"" rel=""nofollow noreferrer"">https://github.com/northwesternmutual/azure-activedirectory-library-for-react-native/blob/master/README.md#android-quirks</a></strong></p>
"
"72920439","Transform columns on Talend using TMAP","<p>So I have this table as an input:</p>
<p><a href=""https://i.stack.imgur.com/5uD9s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5uD9s.png"" alt=""enter image description here"" /></a></p>
<p>The &quot;numero&quot; column is either of &quot;Type&quot; 7 or 2.
What I want as an output is this</p>
<p><a href=""https://i.stack.imgur.com/SuEgG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SuEgG.png"" alt=""enter image description here"" /></a></p>
<p>My tmap does not seem to work correctly. Can anybody suggest me a way to solve this?</p>
<p><a href=""https://i.stack.imgur.com/nVXRN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nVXRN.png"" alt=""enter image description here"" /></a></p>
","<mapping><talend><tmap><data-integration>","2022-07-09 09:58:06","72","0","1","72970909","<p>If row1.Type is an integer, the condition in the expression that defines your variables should use the == operator and not the .equals(&quot;&quot;) method. For Var.Portable, <code>row1.Type == 2 ? row1.Numero : null</code></p>
"
"72889320","Vue 3: Edit modal data flow","<p>I'm new to Vue and I created a list of items and each item has an edit-button. When I click the button, I open a modal window and pass the item's reactive data object to the modal (:data=&quot;itemData&quot;) to fill a form for editing the data.</p>
<p>When editing the data in the form, I don't want the original items in the list to change. Therefor I made a copy of the data prop (not sure if I did this correctly). It seems to work. When I save the changes, the list updates accordingly.</p>
<p>The problem is, this only works once. After editing and saving e.g. the 1st item in the list and then try to edit the 2nd item, nothing happens. I expected the edit form to update and show the 2nd item's data. I'm obviously doing something wrong, but I can't figure out what it is and how to  do this properly. Please help.</p>
<p>Here's a very basic dummy for demonstration:</p>
<p><strong>App.vue:</strong></p>
<pre><code>&lt;script setup&gt;
  import { reactive } from 'vue';
  import EditModal from './components/EditModal.vue'
  import ListItem from './components/ListItem.vue'

  const state = reactive({
    items: [
      { id: 1, title: 'Item 1' },
      { id: 2, title: 'Item 2' }
    ],
    currentItem: null,
    showModal: false
  });

  function editItem(itemData) {
    state.showModal = true;
    state.currentItem = itemData;
  }

  function updateItem(itemData) {
    state.items = state.items.map(item =&gt; item.id === itemData.id ? itemData : item);
  }
&lt;/script&gt;

&lt;template&gt;
  &lt;div&gt;
    &lt;EditModal
      v-if=&quot;state.currentItem&quot;
      :show=&quot;state.showModal&quot;
      :data=&quot;state.currentItem&quot;
      @save=&quot;itemData =&gt; updateItem(itemData)&quot;
    /&gt;

    &lt;ListItem
      v-for=&quot;itemData in state.items&quot;
      :data=&quot;itemData&quot;
      @edit=&quot;itemData =&gt; editItem(itemData)&quot;
    /&gt;
  &lt;/div&gt;
&lt;/template&gt;
</code></pre>
<p><strong>ListItem.vue:</strong></p>
<pre><code>&lt;script setup&gt;
  defineProps(['data']);
&lt;/script&gt;

&lt;template&gt;
  &lt;div&gt;
    {{ data.title }}
    &lt;button @click=&quot;$emit('edit', data)&quot;&gt;Edit&lt;/button&gt;
  &lt;/div&gt;
&lt;/template&gt;
</code></pre>
<p><strong>EditModal.vue:</strong></p>
<pre><code>&lt;script setup&gt;
  import { reactive, toRaw } from 'vue';

  const props = defineProps(['show', 'data']);
  const data = Object.assign({}, toRaw(props.data));
&lt;/script&gt;

&lt;template&gt;
  &lt;div v-show=&quot;show&quot;&gt;
    &lt;div class=&quot;modal&quot;&gt;
      &lt;div class=&quot;modal-inner&quot;&gt;
        &lt;input v-model=&quot;data.title&quot;&gt;
        &lt;button @click=&quot;$emit('save', data)&quot;&gt;Save&lt;/button&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/template&gt;
</code></pre>
","<javascript><vuejs3>","2022-07-06 20:09:09","1052","2","2","72896231","<p>Okay, I think I solved it. If someone has a better solution, please let me know.</p>
<p><strong>EditModal:</strong></p>
<pre><code>&lt;script setup&gt;
  import { toRaw, reactive, watch } from 'vue';

  const props = defineProps(['show', 'data']);
  let data = Object.assign({}, toRaw(props.data));
  let state = reactive({ data: data });

  watch(() =&gt; props.data, (selection, prevSelection) =&gt; {
    data = Object.assign({}, toRaw(selection));
    state.data = data;
  });
&lt;/script&gt;

&lt;template&gt;
  &lt;div v-show=&quot;show&quot;&gt;
    &lt;div class=&quot;modal&quot;&gt;
      &lt;div class=&quot;modal-inner&quot;&gt;
        &lt;input v-model=&quot;state.data.title&quot;&gt;
        &lt;button @click=&quot;$emit('save', state.data)&quot;&gt;Save&lt;/button&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/template&gt;
</code></pre>
"
"72889320","Vue 3: Edit modal data flow","<p>I'm new to Vue and I created a list of items and each item has an edit-button. When I click the button, I open a modal window and pass the item's reactive data object to the modal (:data=&quot;itemData&quot;) to fill a form for editing the data.</p>
<p>When editing the data in the form, I don't want the original items in the list to change. Therefor I made a copy of the data prop (not sure if I did this correctly). It seems to work. When I save the changes, the list updates accordingly.</p>
<p>The problem is, this only works once. After editing and saving e.g. the 1st item in the list and then try to edit the 2nd item, nothing happens. I expected the edit form to update and show the 2nd item's data. I'm obviously doing something wrong, but I can't figure out what it is and how to  do this properly. Please help.</p>
<p>Here's a very basic dummy for demonstration:</p>
<p><strong>App.vue:</strong></p>
<pre><code>&lt;script setup&gt;
  import { reactive } from 'vue';
  import EditModal from './components/EditModal.vue'
  import ListItem from './components/ListItem.vue'

  const state = reactive({
    items: [
      { id: 1, title: 'Item 1' },
      { id: 2, title: 'Item 2' }
    ],
    currentItem: null,
    showModal: false
  });

  function editItem(itemData) {
    state.showModal = true;
    state.currentItem = itemData;
  }

  function updateItem(itemData) {
    state.items = state.items.map(item =&gt; item.id === itemData.id ? itemData : item);
  }
&lt;/script&gt;

&lt;template&gt;
  &lt;div&gt;
    &lt;EditModal
      v-if=&quot;state.currentItem&quot;
      :show=&quot;state.showModal&quot;
      :data=&quot;state.currentItem&quot;
      @save=&quot;itemData =&gt; updateItem(itemData)&quot;
    /&gt;

    &lt;ListItem
      v-for=&quot;itemData in state.items&quot;
      :data=&quot;itemData&quot;
      @edit=&quot;itemData =&gt; editItem(itemData)&quot;
    /&gt;
  &lt;/div&gt;
&lt;/template&gt;
</code></pre>
<p><strong>ListItem.vue:</strong></p>
<pre><code>&lt;script setup&gt;
  defineProps(['data']);
&lt;/script&gt;

&lt;template&gt;
  &lt;div&gt;
    {{ data.title }}
    &lt;button @click=&quot;$emit('edit', data)&quot;&gt;Edit&lt;/button&gt;
  &lt;/div&gt;
&lt;/template&gt;
</code></pre>
<p><strong>EditModal.vue:</strong></p>
<pre><code>&lt;script setup&gt;
  import { reactive, toRaw } from 'vue';

  const props = defineProps(['show', 'data']);
  const data = Object.assign({}, toRaw(props.data));
&lt;/script&gt;

&lt;template&gt;
  &lt;div v-show=&quot;show&quot;&gt;
    &lt;div class=&quot;modal&quot;&gt;
      &lt;div class=&quot;modal-inner&quot;&gt;
        &lt;input v-model=&quot;data.title&quot;&gt;
        &lt;button @click=&quot;$emit('save', data)&quot;&gt;Save&lt;/button&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/template&gt;
</code></pre>
","<javascript><vuejs3>","2022-07-06 20:09:09","1052","2","2","72896393","<p>For better solution (my opinion)</p>
<p>There is a getter and setter in 'computed' you could try to use it instead of using 'watch'
<a href=""https://vuejs.org/guide/essentials/computed.html#writable-computed"" rel=""nofollow noreferrer"">https://vuejs.org/guide/essentials/computed.html#writable-computed</a></p>
<pre><code>&lt;script setup&gt;
  import { reactive, computed } from 'vue';

  const props = defineProps(['show', 'data']);
  const state = reactive({ data: null });

  const inputData = computed({
    get() { return props.data; },
    set(newVal) { state.data = newVal },
  });
&lt;/script&gt;

&lt;template&gt;
  &lt;div v-show=&quot;show&quot;&gt;
    &lt;div class=&quot;modal&quot;&gt;
      &lt;div class=&quot;modal-inner&quot;&gt;
        &lt;input v-model=&quot;inputData&quot;&gt;
        &lt;button @click=&quot;$emit('save', state.data)&quot;&gt;Save&lt;/button&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/template&gt;
</code></pre>
"
"72887057","Azure mapping data flows - insert rows into two sink sequentially","<p>I am implementing a data flow in which I need to insert rows into two different sink and both of the sink are Azure SQL database tables.</p>
<p>The first sink i.e. the first Azure SQL database table has an auto generated ID column. After inserting values in the first table, we need to insert the values in the second sink which is an audit table.</p>
<p>Now the problem is that the identity column values generated in the first sink needs to be inserted in the second sink also. How can I get these auto incremented id column values to be inserted in the audit table.</p>
","<azure-sql-database><etl><azure-data-factory><auto-increment><azure-mapping-data-flow>","2022-07-06 16:34:29","361","0","1","73218872","<p>By default, data is written to multiple sinks in a nondeterministic order. The execution engine writes data in parallel as the transformation logic is completed, and the sink ordering might vary each run. To specify an exact sink ordering, enable Custom sink ordering on the General tab of the data flow. When enabled, sinks are written sequentially in increasing order.</p>
<p>Refer below image for sink order.</p>
<p><a href=""https://i.stack.imgur.com/6wETV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6wETV.png"" alt=""enter image description here"" /></a></p>
"
"72799546","How to integrate Microsoft AX 2009(on-premise) to Microsoft Dataverse and dump to Azure datalake?","<p>Can anyone give me any solution how to integrate it?</p>
<p>I have seen that we don't have any connector for ax 2009.</p>
<p>Any possible way we can connect to Microsot Ax 2009 as I have to migrate the data one time from there to dataverse then to datalake.</p>
","<azure><dynamics-ax-2009><azure-data-lake-gen2><data-integration><dataverse>","2022-06-29 10:07:14","172","0","1","72801414","<p>AX 2009 is not supported as a connector.</p>
<p>You can connect to on-premises data over the <a href=""https://learn.microsoft.com/en-us/connectors/connector-reference/connector-reference-powerapps-connectors"" rel=""nofollow noreferrer"">connectors</a> that use data gateway.</p>
<p>You can connect to on-premise databases using a <a href=""https://learn.microsoft.com/en-us/powerapps/maker/canvas-apps/gateway-management"" rel=""nofollow noreferrer"">gateway connector</a>:  It can work for most CRUD operations.</p>
<p>You can also create your own custom connector: <a href=""https://learn.microsoft.com/en-us/powerapps/maker/canvas-apps/register-custom-api"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/powerapps/maker/canvas-apps/register-custom-api</a></p>
<p><strong>Microsoft Dataverse to Azure Datalake -</strong></p>
<p>To copy data from Dataverse to Data Lake Storage Gen2 follow this <a href=""https://learn.microsoft.com/en-us/power-apps/maker/data-platform/export-to-data-lake-data-adf#set-the-data-lake-storage-gen2-storage-account-as-a-sink"" rel=""nofollow noreferrer"">link</a></p>
<p>Reference - <a href=""https://powerusers.microsoft.com/t5/Building-Power-Apps/power-apps-connecting-with-ax-2009/td-p/694811"" rel=""nofollow noreferrer"">https://powerusers.microsoft.com/t5/Building-Power-Apps/power-apps-connecting-with-ax-2009/td-p/694811</a></p>
"
"72791420","data factory data flows - path does not resolve to any file(s)","<p>trying to set up a data flow to de-duplicate data but there seems to be a simple error that I cannot fix, the data set is set up like so and the preview works correctly - updated file path and imported schema <a href=""https://i.stack.imgur.com/LVyIi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LVyIi.png"" alt=""enter image description here"" /></a></p>
<p>the data flow is set up like this
<a href=""https://i.stack.imgur.com/7nrbY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7nrbY.png"" alt=""enter image description here"" /></a></p>
<p>however I get this error when trying Data preview - updated -&gt; got the columns from importing the schema in the dataset</p>
<p><a href=""https://i.stack.imgur.com/pfhUe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pfhUe.png"" alt=""enter image description here"" /></a></p>
<p>I have tried editing wildcard paths in source options and so far that has not worked
i tried</p>
<pre><code>daily/*csv and *.csv
</code></pre>
<p>the structure of the blob account looks like so:
<a href=""https://i.stack.imgur.com/hugEA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hugEA.png"" alt=""enter image description here"" /></a></p>
<p>source options from the answer given:
<a href=""https://i.stack.imgur.com/wtWPs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wtWPs.png"" alt=""enter image description here"" /></a></p>
<p>still the error:  Path does not resolve to any file(s). Please make sure the file/folder exists and is not hidden. At the same time, ensure special character is not included in file/folder name, for example, name start with _</p>
<p>each directory is from an azure export and creates its own monthly folder - this works on data factory but each csv has month to date cost so I was trying to use data flows to only take new data instead of all of the data with duplicates
<a href=""https://i.stack.imgur.com/XbTUJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XbTUJ.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2022-06-28 18:22:22","876","1","1","72797158","<p>You can use the wildcard path below to get the files of the required type.</p>
<p><em><strong>Input folder path:</strong></em></p>
<p><a href=""https://i.stack.imgur.com/yNopV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yNopV.png"" alt=""enter image description here"" /></a></p>
<p><em><strong>Azure data flow:</strong></em></p>
<ol>
<li>Source dataset</li>
</ol>
<p><a href=""https://i.stack.imgur.com/OtrQu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OtrQu.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>Source transformation: In source options provide the wildcard path to get the files of the required extension type. I have also included columns to store filenames to verify the data from all the files.</li>
</ol>
<p>Wildcard paths : <code>daily/*.csv</code></p>
<p><a href=""https://i.stack.imgur.com/RZNzi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RZNzi.png"" alt=""enter image description here"" /></a></p>
<ol start=""3"">
<li>Import projections and preview data.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/SHDvg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SHDvg.png"" alt=""enter image description here"" /></a></p>
"
"72773732","Is ObjectBox pragmatic for multi-vendor saas application in terms of data sync and storage","<p>I am exploring the options to start working on a hotel management app that many hotel owners can download, create an account and use on their front desk. Offline support of ObjectBox makes it an attractive option. But I have this question:</p>
<blockquote>
<p>Lets say a Hotel A lists their rooms in my app, and uses the app to book reservations, track inventory, and generate reports. Let's say Hotel B and C do the same on using their own accounts.</p>
</blockquote>
<p>Does data syncing mean that all the data from all accounts (from all the hotels) is automatically synced to when internet is available, does that not mean that all connected devices will download data (by way of syncing) from other users? How does OB decide how much of the data is to be synced?</p>
","<synchronization><data-synchronization><objectbox>","2022-06-27 14:25:15","40","0","1","72786249","<p>When using ObjectBox Sync all data is currently shared. Sync will make sure each client has all of the data the server has (and changes of a client are distributed to all other clients).</p>
<p>To handle your use case would require to run a separate Sync server instance for each customer.</p>
<p>Note: there is still <a href=""https://sync.objectbox.io/sync-client#authentication-options"" rel=""nofollow noreferrer"">authentication</a> (via shared secret or Google auth).</p>
<p>For more technical details see the <a href=""https://sync.objectbox.io/"" rel=""nofollow noreferrer"">docs</a>. There is also <a href=""https://objectbox.io/sync/"" rel=""nofollow noreferrer"">a website</a> and you can email us to talk about details.</p>
"
"72717167","Android Management API Kiosk Mode No Notification Sound","<p>Working with Android Management API that has a custom launcher app set to Kiosk mode. In this way it overrides the old launcher.</p>
<p>However, it does not receive notifications from for example whatsapp and Gmail because of the Kiosk mode in Android Management API.</p>
<p>Is there a way to bypass this?</p>
","<android><android-intent><mdm><kiosk-mode><android-management-api>","2022-06-22 14:14:03","147","0","1","72772089","<p>This is a known issue on android 8.1 and below with  no fix available on those versions. This should not be an issue on later versions.. To enable notifications you should add <code>&quot;statusBar&quot;: &quot;NOTIFICATIONS_AND_SYSTEM_INFO_ENABLED&quot;</code> in your (<code>KioskCustomization</code>)[https://developers.google.com/android/management/reference/rest/v1/enterprises.policies#kioskcustomization] policy.</p>
"
"72715798","Is it possible to switch lockedTaskMode for device enrolled with Android Management API programmatically?","<p>I develop an android kiosk application. I want my app to start automatically when device boots and run in &quot;locked task&quot; mode most of the time. But I also need to be able to switch off &quot;locked task&quot; mode programmatically from the app for maintenance/debugging.</p>
<p>Now I'm planing to enroll tablets with Android Management API (QRCode) to</p>
<ol>
<li>autostart app when device boot and run it in &quot;locked task&quot; mode without &quot;Pin dialog&quot; notification</li>
<li>simplify initial device setup</li>
<li>simplify app auto-update</li>
</ol>
<p>Does anyone have an idea how to configure device with Android Management API to be able to to switch on/off &quot;locked task&quot; mode programmatically?</p>
<p>I can't just white list apps because I'm not sure which apps I'll need.</p>
<p>I tried to set install type to &quot;KIOSK&quot; for my app. DPC locks task for my app in this case but I can't stop this mode. App just restarts when I call <code>stopLockTask()</code>.</p>
<p>And it looks like when I set install type just to <code>REQUIRED_FOR_SETUP</code> I can't start locked task mode because my app is not whitelisted for that.</p>
<p>I also tried to play with <code>kioskCustomLauncherEnabled</code> option. But if I set this to &quot;true&quot; it becomes impossible to setup my app as a launcher to auto start with device.</p>
","<mdm><android-management-api><emm>","2022-06-22 12:46:23","126","0","1","72772215","<p>You can switch off of lock task mode by using a different install type in the policy (e.g. change install type from <code>KIOSK</code> to <code>FORCE_INSTALLED</code>).
You may also want to implement a maintenance window to ensure that device updates are done when the device is not in use.</p>
<p>You can check the official documentation for more information about dedicated devices: <a href=""https://developers.google.com/android/management/policies/dedicated-devices?hl=en#kiosk-mode"" rel=""nofollow noreferrer"">https://developers.google.com/android/management/policies/dedicated-devices?hl=en#kiosk-mode</a></p>
"
"72706659","Azure cosmos db error in data factory - data flow sink -> job failed due to reason: Conversion from StructType","<p>I'm building a simple <code>data flow</code> in azure data factory to get some specific data from a content hub location. This information is in json format.</p>
<p>Transformations:</p>
<ol>
<li>Source: <code>rest API</code> get method to retrieve the data from the URL</li>
<li>Transformation 1: <code>flatten</code> to put into rows an item list that  contains all the articles.</li>
<li>Transformation 2: <code>select</code> to chose specific attributes from each items in the list.</li>
<li>Transformation 3: <code>alter row</code> to <code>upsert</code> data if condition <code>true()</code></li>
<li>Sink: using cosmosdb dataset to load the selected data into a collection.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/xBOyh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xBOyh.png"" alt="""" /></a></p>
<p>The problem is with the last item <code>elements</code> since this is a <code>StructType</code> <code>{}</code>:</p>
<pre class=""lang-json prettyprint-override""><code>elements: {
  headline: {
    title: &quot;Title&quot;,
    dataType: &quot;string&quot;,
    name: &quot;headline&quot;,
    variations: { },
    multiValue: false,
    :type: &quot;string&quot;,
  },
  alternativeHeadline: {
    title: &quot;Subtitle&quot;,
    dataType: &quot;string&quot;,
    name: &quot;alternativeHeadline&quot;,
    variations: { },
    multiValue: false,
    :type: &quot;string&quot;,
  },
  author: {
    title: &quot;Author&quot;,
    dataType: &quot;string&quot;,
    name: &quot;author&quot;,
    variations: { },
    multiValue: false,
    :type: &quot;string&quot;,
  },
...
}
</code></pre>
<p>When I run the job I'm getting this error:</p>
<pre><code>{&quot;StatusCode&quot;:&quot;DFExecutorUserError&quot;,&quot;Message&quot;:&quot;Job failed due to reason: Conversion from StructType(StructField(headline,StructType(StructField(:type,StringType,true), StructField(dataType,StringType,true), StructField(multiValue,BooleanType,true), StructField(name,StringType,true), StructField(title,StringType,true), StructField(value,StringType,true), StructField(variations,StructType(StructField(mobile,StructType(StructField(:type,StringType,true), StructField(dataType,StringType,true), StructField(multiValue,BooleanType,true), StructField(name,StringType,true), StructField(title,StringType,true), StructField(value,StringType,true)),true), StructField(spanish,StructType(StructField(:type,StringType,true), StructField(dataType,StringType,true), StructField(multiValue,BooleanType,true), StructField(name,StringType,true), StructField(title,StringType,true), StructField(value,StringType,true)),true)),true)),true), StructField(icon,StructType(StructField(:type,StringType,true), StructField(dataType,StringType,true), StructField(multiValue,BooleanType,true),&quot;,&quot;Details&quot;:&quot;&quot;}
</code></pre>
<p>It seems one ore more data struct types are wrong from the source vs the target but I would like this to be dynamic. Since it is some metadata, the attribute can or can't be there.</p>
<p>I tried using a <code>copy activity</code> directly in a pipeline using the same dataset and it worked fine the problem is I'll need to transform the data later and the copy activity is limited in this aspect. Any thoughts?</p>
","<azure><azure-cosmosdb><azure-data-factory>","2022-06-21 20:17:46","418","0","1","72718520","<p>I found the root of the problem. In the dataset, I imported the schema, once I cleared it, the struct data type verification stopped in the data-flow. The load ran successfully.</p>
<p><a href=""https://i.stack.imgur.com/PAa0O.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PAa0O.png"" alt="""" /></a></p>
<p><a href=""https://i.stack.imgur.com/kdput.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kdput.png"" alt="""" /></a></p>
"
"72689574","How to set ""lineSep"" for Azure Data Factory mapping data flow with UTF16LE input and document per line?","<p>I am trying to read a UTF16-LE JSON file with a &quot;document per line&quot;.  I During the read file I get the following exception.  I don't see a &quot;lineSep&quot; option in the portal.  How does one set this?</p>
<p>Operation on target dataflow1 failed:</p>
<pre><code>{&quot;StatusCode&quot;:&quot;DFExecutorUserError&quot;,&quot;Message&quot;:&quot;Job failed due to reason: at Source 'source1': java.lang.IllegalArgumentException: requirement failed: The lineSep option must be specified for the UTF-16LE encoding&quot;,&quot;Details&quot;:&quot;java.lang.IllegalArgumentException: requirement failed: The lineSep option must be specified for the UTF-16LE encoding\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.sql.catalyst.json.JSONOptionsInRead.checkedEncoding(JSONOptions.scala:153)\n\tat org.apache.spark.sql.catalyst.json.JSONOptions$$anonfun$34.apply(JSONOptions.scala:109)\n\tat org.apache.spark.sql.catalyst.json.JSONOptions$$anonfun$34.apply(JSONOptions.scala:109)\n\tat scala.Option.map(Option.scala:146)\n\tat org.apache.spark.sql.catalyst.json.JSONOptions.&lt;init&gt;(JSONOptions.scala:109)\n\tat org.apache.spark.sql.catalyst.json.JSONOptionsInRead.&lt;init&gt;(JSONOptions.scala:133)\n\tat org.apache.spark.sql.catalyst.json.JSONOptionsInRead.&lt;init&gt;(JSONOptions.scala:139)\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.inferSchema(JsonFileFormat.scala:55)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:193)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.&quot;}
</code></pre>
<p>Thanks!<br />
Ed</p>
","<azure-data-factory>","2022-06-20 15:35:52","117","1","1","72695556","<p><code>UTF-16LE</code> JSON file encoding format is currently not supported in the Azure data factory.</p>
<p>You can go through this Microsoft <a href=""https://learn.microsoft.com/en-us/azure/data-factory/format-json#dataset-properties"" rel=""nofollow noreferrer"">document</a> for all the supported encoding types in JSON dataset in the Azure data factory.</p>
"
"72632065","Data Mapping In Typescript","<p>I have two static data sets. I would like to implement them dynamically with an API but I can not format them to the expected type.</p>
<p>This is the first data set:</p>
<pre><code>export let lineChartSeries = [
    {
        name: 'Cumulative',
        series: [
            {
                name: '2022-06-01',
                value: 50
            },
            {
                name: '2022-05-01',
                value: 80
            },
            {
                name: '2022-04-01',
                value: 85
            },
            {
                name: '2022-03-01',
                value: 90
            },
            {
                name: '2022-02-01',
                value: 100
            }
        ]
    }
];
</code></pre>
<p>This is the second data set :</p>
<pre><code>export let barChart: any = [
    {
        name: '2022-06-01',
        value: 50000
    },
    {
        name: '2022-05-01',
        value: 30000
    },
    {
        name: '2022-04-01',
        value: 10000
    },
    {
        name: '2022-03-01',
        value: 5000
    },
    {
        name: '2022-02-01',
        value: 500
    }
];
</code></pre>
<p>And this is my code for mapping the data.</p>
<pre><code>this.apiDashboard.api(&quot;test&quot;).toPromise().then((data: any) =&gt; {
            this.multi = [];
            this.single = [];
            data.list.forEach(element =&gt; {

                this.multi = data.list.map(datum =&gt; ({ name: &quot;Cumulative&quot;, series: [{ 
                name: datum.period, value: datum.cumulative }] }));
                this.single = data.list.map(datum =&gt; ({
                    name: datum.period, value: datum.amount
                }));
            });
        }
</code></pre>
<p>This is the console log for results. The first array is the expected result and the second array is mine data format type. I have to put the period and the value inside of the series[]. How could I deal with this data mapping?<br />
<a href=""https://i.stack.imgur.com/llw1b.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/llw1b.png"" alt=""Console"" /></a></p>
","<angular><typescript><data-mapping>","2022-06-15 13:11:16","578","0","1","72749692","<p>I found a reasonable way.</p>
<p>To map data like this: Create an array, map the data on this array and attend it to our data set.</p>
<p>Code:</p>
<pre><code>   let cumulative = [];

    this.apiDashboard.api(&quot;month-transacted&quot;).toPromise().then((data: any) =&gt; {
        this.dataSource = [];
        this.multi = [];
        this.single = [];
        data.list.forEach(element =&gt; {

            this.dataSource.push(element);

            cumulative = data.list.map(datum =&gt; ({ name: datum.period, value: datum.cumulative }));
            this.single = data.list.map(datum =&gt; ({
                name: datum.period, value: datum.amount
            }));
        });

        this.multi = [
            { name: &quot;Cumulative&quot;, series: cumulative }
        ];
    });
}
</code></pre>
"
"72627505","Visual Studio 2019 completely freezes when going down to data flow level","<p>I've checked &quot;Skip validation when opening a package&quot; so when opening a package it doesn't freeze, but when going down to data flow level, it starts validating the connection and freezes. I would chalk it up to it being a big database, but on other machines, it works without a hitch. Do you guys have any advice?</p>
","<ssis><visual-studio-2019><freeze>","2022-06-15 07:36:24","581","1","3","72628872","<p>Check your individual connections, you might need to set delay validation to true on the connections too.</p>
<p><a href=""https://i.stack.imgur.com/iotim.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iotim.png"" alt=""Update Connections"" /></a></p>
"
"72627505","Visual Studio 2019 completely freezes when going down to data flow level","<p>I've checked &quot;Skip validation when opening a package&quot; so when opening a package it doesn't freeze, but when going down to data flow level, it starts validating the connection and freezes. I would chalk it up to it being a big database, but on other machines, it works without a hitch. Do you guys have any advice?</p>
","<ssis><visual-studio-2019><freeze>","2022-06-15 07:36:24","581","1","3","72958900","<p>I too was having this issue when going into a data flow. I am on VS 2019 16.11 and SSIS 3.16 (not Preview 4.0). I changed the DelayValidation to True on the Data Flow and the project, and it resolved my issue.</p>
<p><a href=""https://i.stack.imgur.com/yBmFJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yBmFJ.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/SWyYj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SWyYj.png"" alt=""enter image description here"" /></a></p>
"
"72627505","Visual Studio 2019 completely freezes when going down to data flow level","<p>I've checked &quot;Skip validation when opening a package&quot; so when opening a package it doesn't freeze, but when going down to data flow level, it starts validating the connection and freezes. I would chalk it up to it being a big database, but on other machines, it works without a hitch. Do you guys have any advice?</p>
","<ssis><visual-studio-2019><freeze>","2022-06-15 07:36:24","581","1","3","74974146","<p>I Updated My Visual Studio 2019 to latest version and Problem Solved!</p>
"
"72596041","SSIS destination component failing due to current version of the data flow","<p>I have a working SSIS package. It works from vs, meaning it builds and executes as expected producing results in the CRM.</p>
<p>After I get the generated .ispac file and deploy it in MSSQL server, I fail on executing.</p>
<p>I get an error that looks like this:</p>
<blockquote>
<p>Test Package:Error: Microsoft.SqlServer.Dts.Pipeline.ComponentVersionMismatchException: The version of Create Records is not compatible with this version of the DataFlow.  [[The version or pipeline version or both for the specified component is higher than the current version.  This package was probably created on a new version of DTS or the component than is installed on the current PC.]]
at Microsoft.SqlServer.Dts.Pipeline.ManagedComponentHost.HostCheckAndPerformUpgrade(IDTSManagedComponentWrapper100 wrapper, Int32 lPipelineVersion).</p>
</blockquote>
<p>The aforementioned 'Create Record' is the 'Dynamics CRM Destination' component.</p>
<p>There are many who have such issue, I searched the web, but what all suggest is that we change the target version of the MSSQL server from VS, but I tried that and it doesn't work.</p>
<p><strong>Additional information:</strong></p>
<p>System: Windows 10 pro</p>
<p>VS: 2015</p>
<p>MSSQL: 13.0.6300.2(so it's sql server 2016)</p>
<p>Kingsway version: v21.2(21.2.0.31501)</p>
<p>The 'TargetServerVersion' is set to: 'SQL Server 2016'</p>
<p><strong>What have I tried ?</strong></p>
<p>I have tried to deploy straight from VS, but no luck there.</p>
<p>I ran the same(separately built) SSIS on my local machine where I have vs2019 and sql server 2019, it works fine in here.</p>
<p>I tried to build the project in vs2019 on the main machine and I'm still getting the same error.</p>
<p><strong>So, do you have any idea what's happening, can you give me some advice ?</strong></p>
","<ssis><dynamics-crm><dynamics-crm-2011><microsoft-dynamics><kingswaysoft>","2022-06-12 21:34:16","172","2","1","72604487","<p>Thank you for the question about our integration toolkit. To use any of our toolkits for deployment and scheduled execution, the same version should be installed in possibly three places:</p>
<ol>
<li>On your development machine (this is generally covered by our free Developer license)</li>
<li>On your SSIS Integration server</li>
<li>On the machine which is used to deploy your SSIS packages to your Integration server (if different from your development machine)</li>
</ol>
<p>Based on the error message, it is either that you didn't install our Dynamics 365 toolkit or otherwise the installed version on the server is lower than what you have installed on your development workstation.</p>
"
"72588558","Prevent WaitForSingleObject from hanging on a named semaphore when other process is terminated?","<p>When using a globally named mutex to synchronize across two processes, and one of the two processes are killed (say in Task Manager, or due to a fault), the other process returns from <code>WaitForSingleObject()</code> with the appropriate error code and can continue.</p>
<p>When using a globally name semaphore, it does not release the waiting process if the other process is killed / terminated.  <code>WaitForSingleObject()</code> will wait until it times out (which may be <code>INFINITE</code> or hours).</p>
<p>How do I stop <code>WaitForSingleObject()</code> from waiting when the other process is killed or terminated?</p>
<p>In this case, there is a single count on the semaphore used to control read/write requests of a shared buffer.  The Requester signals the Provider to provide certain data, the Provider updates the buffer and signals back to the Requester that it can now read the buffer.</p>
","<winapi><synchronization><semaphore><data-synchronization>","2022-06-11 23:34:01","65","0","2","72588928","<p>I suggest that you switch to using WaitForMultipleObjects and wait for the handle of the process that might get terminated (or thread if you want to do this within a single process) in addition to your semaphore handle. That way you can continue to use INFINITE timeouts. Just have to check the return value to see which object was signalled.</p>
<p>Also, I would consider a process terminating while holding a semaphore somewhat of a bug, particularly a semaphore used for actual inter-process communication.</p>
"
"72588558","Prevent WaitForSingleObject from hanging on a named semaphore when other process is terminated?","<p>When using a globally named mutex to synchronize across two processes, and one of the two processes are killed (say in Task Manager, or due to a fault), the other process returns from <code>WaitForSingleObject()</code> with the appropriate error code and can continue.</p>
<p>When using a globally name semaphore, it does not release the waiting process if the other process is killed / terminated.  <code>WaitForSingleObject()</code> will wait until it times out (which may be <code>INFINITE</code> or hours).</p>
<p>How do I stop <code>WaitForSingleObject()</code> from waiting when the other process is killed or terminated?</p>
<p>In this case, there is a single count on the semaphore used to control read/write requests of a shared buffer.  The Requester signals the Provider to provide certain data, the Provider updates the buffer and signals back to the Requester that it can now read the buffer.</p>
","<winapi><synchronization><semaphore><data-synchronization>","2022-06-11 23:34:01","65","0","2","72589494","<p>Adding to the accepted answer.</p>
<p>I added logic if the <code>waitms</code> was going to be longer than some value <code>maxwaitms</code> then the requester/provider exchange the providers process id (<code>GetCurrentProcessId()</code>) before the long process.  The requester opens a handle (<code>OpenHandle()</code>) to the provider process and waits on both the semaphore and the process handle to know when writing is done (or process terminated).</p>
"
"72578688","How to execute a SQL Server stored procedure after a data flow task in SSIS","<p>I am new to SSIS. I am trying to create an ETL pipeline to automate the updating and deleting process for a database.</p>
<p>I have created a data flow task which reads the Excel file and sends the data to respective staging tables in SQL Server.</p>
<p><img src=""https://i.stack.imgur.com/VFrvP.png"" alt=""Data Flow"" /></p>
<p>For the data to be updated in the main database, it has to go through some transformation in the staging tables. I have created a stored procedure that will enforce these changes.</p>
<p>I want the stored procedure to get called right after data is loaded through the data flow task to the staging tables rather than me going to SSMS to manually execute the stored procedure.</p>
<p><img src=""https://i.stack.imgur.com/hUhPG.png"" alt=""Stored procedure"" /></p>
<p>I have tried adding the &quot;Execute SQL Task&quot; on the control flow tab but not getting any results.</p>
<p>I would like to further add many more transformations in this whole process in future steps. Any ideas on how to make this whole process more convenient would also be appreciated.</p>
","<sql-server><ssis>","2022-06-10 18:47:57","78","-1","1","72579588","<p>[Data Flow Task] -&gt; [Execute SQL Task]</p>
<p>Configure the <code>Execute SQL Task</code> with a Direct Input value of</p>
<pre><code>EXECUTE dbo.MasterQuery;
</code></pre>
<p>Based on the image of your stored procedure, it would appear you have a logic error in there.</p>
<pre><code>IF EXISTS(SELECT 1 FROM dbo.OutlookDataStg WHERE [Flag] = 'Outlook')
BEGIN
    UPDATE dbo.OutlookDataStg
    SET [Data Type] = 'Outlook'
    WHERE [Flag] = 'Actual'

-- Cut off at this point
END
</code></pre>
<p>The logic provided is</p>
<blockquote>
<p>If there is at least one row in the table dbo.OutlookDataStg where the value flag is Outlook, then update the same table but set the Data type to Outlook for any rows with a flag of Actual.</p>
</blockquote>
<p>Unless you have some unusual condition, it would see you've mixed your Flag and Data Type values</p>
"
"72558498","Is it sensible to use symfony as native app api?","<p>Good day,</p>
<p>I have a question for the experienced developers:</p>
<p>At the moment I work a lot with the PHP framework Symfony.</p>
<p>Out of interest, I would now like to delve into the topic of native app development
using React.
As part of a practice project, I want to transfer/sync data between a SQL DB on a server and the app.
My question is, is it a good way to write a symfony application for this,
which only acts as an API for the database?</p>
<ul>
<li>Does this make sense from a performance and effort point of view?</li>
<li>What alternative ways are there?</li>
<li>Which ways of storing data on servers are used most frequently in the productive environment?</li>
</ul>
<p>I am happy about suggestions, links and informations in every direction.</p>
<p>Thanks and Greetings</p>
","<reactjs><database><api><symfony><data-synchronization>","2022-06-09 10:10:26","74","-1","1","72560181","<p>My answers (based on my experiences and my collegues answers) :</p>
<ul>
<li>As you can see on the documentation <a href=""https://symfony.com/doc/current/the-fast-track/en/26-api.html"" rel=""nofollow noreferrer"">here</a> it's very easy to made your own api with Symfony tkants to API Platform. For the performance it's very acceptable especially if you use Symfony (&gt;= 5.4) because a lot of cleaning has been done in the kernel and PHP 8 for its performance improvements especially at compile time (JIT compiletor). More info <a href=""https://developers.ibexa.co/blog/benchmarks-php-7.4-8.0-jit-opcache-preloading-symfony"" rel=""nofollow noreferrer"">here</a> if you need it.</li>
<li>Alternatively you can create your API rest with NodeJS but it doesn't bring much especially if your application is already made in PHP. Adding a layer can sometimes make things heavier instead of lighter.</li>
<li>It's depend of your need, the size, the number of users ... You have to determine the target to choose the better solution. If you have it already I can help you.</li>
</ul>
"
"72540156","Android Management API Disable Camera Except For Custom App","<p>I am exploring about using Android Management API to restrict the use of Camera for my company device. So what I need is disable all camera access except for 1 application that my company made to take picture using camera.
So what I want to ask is it possible to restrict all camera access using android management api except for 1 application that my company made?</p>
","<android><mdm><android-management-api>","2022-06-08 04:44:28","225","2","1","72575593","<p>There are some workarounds depending on the use case:</p>
<p>By setting the device to <a href=""https://developers.google.com/android/work/overview#company-owned-devices-for-knowledge-workers"" rel=""nofollow noreferrer"">fully manage mode</a>, you can <a href=""https://developers.google.com/android/work/requirements#4.20.-disable-cameras_1"" rel=""nofollow noreferrer"">disable camera</a> access on the entire device using the policy <code>&quot;cameraDisabled&quot;: true</code>. In this case, there is no way you can allow exceptions for specific custom app since the policy is device-wide and cannot be overridden with  <code>&quot;permissionGrants&quot;: [ {object ([PermissionGrant])} ]</code>.</p>
<p>With that being said there are a few things we can do depending on the use case:</p>
<ul>
<li>Making the camera app the default app that responds to <code>android.media.action.IMAGE_CAPTURE</code> intent using <a href=""https://developers.google.com/android/management/reference/rest/v1/enterprises.policies#persistentpreferredactivity"" rel=""nofollow noreferrer"">PersistentPreferredActivity</a></li>
<li>Setting up the device in kiosk mode and expose the camera app to the user.</li>
</ul>
"
"72488285","new record not visible in activity previews in mapping data flow","<p>I have a mapping data flow with an Azure sql database source.
For testing i have only 10 records in the source.</p>
<p>If i insert a new record to the database table, then view preview of the source data in the mapping data flow, and click the refresh icon, the new record is not visible.
I have to turn the debug cluster off and on again.</p>
<p>Sampling is disabled so it should be reading all the data.</p>
<p>Is there a reason ADF mapping data flow source might not be seeing a new record immediately?</p>
","<azure-data-factory><azure-mapping-data-flow>","2022-06-03 10:29:15","294","0","1","72489542","<p>The reason is ADF caches the preview data and the change in source data won't reflect in data-preview until and unless we change something in our dataflow which will trigger the actual computation again.</p>
"
"72480995","How to find the source and the destination in each data flow task for every package in the solution?","<p>I have hundreds of SSIS packages that are not documented.  I'd like to come up with the code/program to list all data flow tasks in every package in the given solution.  The list should include:
Package Name, Data Flow Task Name, Source Connection or Source DB, Source Table/File, Destination Connection or DB, Destination Table.</p>
<p>Is it doable?</p>
","<ssis><dataflow>","2022-06-02 18:47:50","426","0","1","72481945","<p>You can absolutely write this using the correct version of the SQL Server binaries and your favorite .NET language to do so.</p>
<p>I did a lot with SSIS in my life and I would absolutely not want to write that as there's such a variety of ways a package could be drafted. Things you'll discover is that every container might contain a Data Flow and there can be data flows in places they don't belong, like an Event Handler. And there may be COM to deal with.</p>
<p>There were paid for tools on the market at one point: BI Documenter (I think that was the name) was one. The open source project BIDS Helper which has since been rebranded had some documentation functionality in it.</p>
<p>But, by far, hands down the absolute best way to handle it and the only approach I would advocate is that you use Biml for a project like this. Biml, is the Business Intelligence Markup Language, it's a domain specific language that reduces SSIS (and tables and cubes) to the smallest unit. And then allows you to query over it.</p>
<p>Step 1. Download <a href=""https://varigence.com/bimlexpress"" rel=""nofollow noreferrer"">BimlExpress</a> It's a free add-in for Visual Studio. You will use this to reverse engineer all your SSIS packages into the corresponding Biml</p>
<p>Step 2. Right click on the project and select Add new Biml File</p>
<p>Step 3. Write the Biml</p>
<p>I used the following query in a recent project because we needed to extract source queries (Postgres) and then the target tables in SQL Server. The source query bit assumes there are Expressions on the data flow so not included.</p>
<pre><code>&lt;#
string dataflowName = &quot;&quot;;
string sourceComponentName = &quot;&quot;;
string targetComponentName = &quot;&quot;;

// Loop through all the packages
foreach (AstPackageNode p in this.RootNode.Packages)
{

    // Find all the Data Flows on the canvas itself
    // Your TODO: inspect the containers via  p.Tasks.OfType&lt;AstContainerTaskNode&gt;() looking for dataflows
    foreach (AstDataflowTaskNode t in p.Tasks.OfType&lt;AstDataflowTaskNode&gt;())
    {
        // TODO: Lots of gotchas here, 
        // What if there are multiple sources
        // what if the source doesn't support &quot;names&quot; for the table/query
        // What if the package was built via Import/Export wizard which has 
        //  several independent source to destinations all within the same DFT
        // etc
        // Same applies to destinations

        dataflowName = t.Name;
    
            // This is an example of how to iterate through all the OLE DB Destination components identifying various bits I needed to know
            foreach (AstOleDbDestinationNode tn in t.Transformations.OfType&lt;AstOleDbDestinationNode&gt;())
            {
                // What connection is being used
                WriteLine(tn.Connection.Name);
                // Extract table best we can
                System.Xml.Linq.XElement xe = System.Xml.Linq.XElement.Parse(tn.GetBiml());
                foreach (System.Xml.Linq.XElement node in xe.Elements().Where(xnode =&gt; xnode.Name == &quot;ExternalTableOutput&quot;))
                {
                    tableName = node.Attributes(&quot;Table&quot;).FirstOrDefault().Value.ToString().Split('.')[1];
                    // Table will end up looking like either [srcdb].[foo] OR &quot;srcdb&quot;.&quot;foo&quot;
                    tableName = tableName.Substring(1, tableName.Length-2);
                }
                // What if we need columns?
                // &lt;Columns&gt;&lt;Column SourceColumn=&quot;id&quot; TargetColumn=&quot;Id&quot; /&gt;
                foreach (System.Xml.Linq.XElement node in xe.Elements().Where(xnode =&gt; xnode.Name == &quot;Columns&quot;))
                {
                    foreach (System.Xml.Linq.XElement inode in node.Elements().Where(xnode =&gt; xnode.Name == &quot;Column&quot;))
                    {
                        WriteLine(inode.ToString());
                        WriteLine(&quot;&quot;);
                    }
                    WriteLine(&quot;&quot;);
                }
            }

#&gt;
</code></pre>
<p>For the way the BimlExpress works, you need to either have all the packages open (so_69215306.biml as my image shows) and the various WriteLine(s) will show in the preview panel.</p>
<p><a href=""https://i.stack.imgur.com/b8rqF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/b8rqF.png"" alt=""enter image description here"" /></a></p>
<p>When you want to attempt to do this across your whole ecosystem of packages, then you'll want to multi-select in the Miscellaneous section of Visual Studio and then right click on your inspector script (for my screenshot, that would be BimlScript6.biml) and Check Biml for Error or Generate SSIS package.</p>
<p>Both will have the same effect - assuming you're just using Biml for the object model capability. Instead of WriteLines which are helpful for debugging, I assume you'll write to a <em>something</em> json/csv/txt/database/whatever</p>
<p>Step 4. Fall in love with all the power of Biml and buy BimlStudio</p>
<p>Example of converting a project to Biml</p>
<ul>
<li>Convert to Biml</li>
<li>Select the project and [Import] (this identifies what you <em>could</em> bring into the Biml project definition)</li>
<li>Select the assets you want to [Add to Project] (likely everything)</li>
<li>Right click on project and choose
<a href=""https://i.stack.imgur.com/WvhpQ.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WvhpQ.gif"" alt=""enter image description here"" /></a></li>
</ul>
"
"72460221","ADF mapping data flow only inserting, never updating","<p>I have an ADF data flow that will only insert. It never updates rows.
Below is a screenshot of the flow, and the Alter Row task that sets the insert/Update policies.</p>
<p><a href=""https://i.stack.imgur.com/yY1Qc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yY1Qc.png"" alt=""enter image description here"" /></a>
data flow</p>
<p><a href=""https://i.stack.imgur.com/Bev4j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Bev4j.png"" alt=""enter image description here"" /></a>
alter row task</p>
<p>There is a source table and a destination table.
There is a source table for new data.
A lookup is done against the key of the destination table.
Two columns are then generated, a hash of the source data &amp; hash of the destination data.
In the alter row task, the policy's are as follows:</p>
<ul>
<li>Insert: if the lookup found no matching id.</li>
<li>Update: if lookup found a matching id and the checksums do not match (i.e. user exists but data is different between the source and existing record).</li>
<li>Otherwise it should do nothing.</li>
</ul>
<p>The Sink allows insert and updates:</p>
<p><a href=""https://i.stack.imgur.com/9OTnn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9OTnn.png"" alt=""enter image description here"" /></a></p>
<p>Even so, on first run it inserts all records but on second run it inserts all the records again, even if they exist.</p>
<p>I think I am misunderstanding the process and so appreciate any expertise or advise.</p>
","<azure-data-factory><azure-mapping-data-flow>","2022-06-01 10:24:14","768","0","1","72831686","<p>Thank you Joel Cochran for your valuable inputs, repro’d the scenario, and posting it as an answer to help other community members.</p>
<p>If you are using the upsert method in the sink, add alter row transformation with upsert if and write the expression for the upsert condition.</p>
<p>If you are using insert and update as your update method in the sink then in alter row transformation use both inserts if and update if conditions to insert and update data accordingly into the sink based on alter row conditions.</p>
<p><a href=""https://i.stack.imgur.com/e6sFB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e6sFB.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/BdfCz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BdfCz.png"" alt=""enter image description here"" /></a></p>
"
"72451408","pentaho data integration: How to dynamically split excel file into multiple sheets based on a columns value?","<p>I have an excel file that I want to break up the data by locations and add them to a single excel file as multiple sheets based on their location. I was able to do this by creating multiple filters, one for each location:
<a href=""https://i.stack.imgur.com/i55Jl.png"" rel=""nofollow noreferrer"">PDI Sample Image</a></p>
<p>But what I really need is to dynamically split the data into sheets by location since I will not always know what locations will be included in the next file.
the data looks like this<br>
Location | name <br>
BJCC      | Tom<br>
BJCC      | Bill<br>
BJCC      | Ted<br>
CDCC      | Hank<br>
CDCC      | Perl<br>
CS11A      | Everett<br>
CSD2      | Paul<br></p>
<p>I need to results to appear for each excel sheet by location.
I'm a total novice so any suggestions would be helpful. Thanks in advance.</p>
","<excel><pentaho><pentaho-data-integration><pdi>","2022-05-31 16:41:34","186","0","1","72461086","<p>You could use Metadata Injection. So you build a transformation similar to the one on your screenshot, but only with the BJCC filter, and you use another transformation to read the diferent locations from your excel file, injecting the metadata of those locations into the transformation with the BJCC filter, changing the value of the filter and the name of the sheet to write using metadata injection: <a href=""https://help.hitachivantara.com/Documentation/Pentaho/9.3/Products/ETL_metadata_injection"" rel=""nofollow noreferrer"">https://help.hitachivantara.com/Documentation/Pentaho/9.3/Products/ETL_metadata_injection</a></p>
"
"72438793","Cannot select sink for Oracle linked service from data flow","<p>I have a linked service that connects to an on-prem Oracle database through a self hosted integration runtime.</p>
<p>I am able to access this from a data factory pipeline - the dataset that uses the linked service is called pc_payinitil.</p>
<p><a href=""https://i.stack.imgur.com/pHhXZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pHhXZ.png"" alt=""enter image description here"" /></a></p>
<p>However, the pc_payinitial dataset is not available to select from the sink tab of a sink shape of a data flow:</p>
<p><a href=""https://i.stack.imgur.com/2RSg4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2RSg4.png"" alt=""enter image description here"" /></a></p>
<p>Am I trying to do something that's not possible - or did I just go wrong somewhere?</p>
","<azure-data-factory>","2022-05-30 18:42:56","207","0","1","72439468","<p>Currently, the <code>Oracle</code> dataset is not supported in mapping data flow in the Azure data factory. It is only supported in copy activity source/sink and lookup activity.</p>
<p>There are only limited datasets that are supported in mapping data flow as of now. You can go through this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-overview#supported-data-stores"" rel=""nofollow noreferrer"">MS document</a> for more information on supported data stores in the Azure data factory.</p>
"
"72386194","How to keep updating recyclerview in the next activity by the data flow sent by BLE device in current activity?","<p>I am completely new to Android and just learned Object-oriented programming. My project requires me to build something on open-source code. I am really struggling with this special case. Two fragments are under activity_main, one of them is TerminalFragment. If TerminalFragment is active under activity_main, data from a BLE device will keep flowing into activity_main. Is it possible to keep passing the data to the next activity without clicking a button (in this case, menuItem(R.id.plot))? I added a recyclerview on activity_main2 and want it display the data which is flowing in activity_main.</p>
<p>In my testing with my current method, the recycler view won't update itself, it just captures the data in the recyclerview of activity_main with TerminalFragment once the user clicked the menuItem (id,plot). What kind of thing I need to add in my method? Should I create another Fragment instead of activity_main2? As I looked this up in the internet, it seems not possible to work that way between a fragment and next activity. Much Appreciated.</p>
<p>onOptionsItemSelected of TerminalFragment</p>
<pre><code>if (id == R.id.plot){
            Intent intent = new Intent(getActivity(), MainActivity2.class);
            intent.putExtra(&quot;output&quot;,output); //output
            startActivity(intent);
            return true;
        }
</code></pre>
<p>receive() of TerminalFragment</p>
<pre><code>private void receive(byte[] data) {
        if(hexEnabled) {
            receiveText.append(&quot;Hello&quot; + TextUtil.toHexString(data) + '\n');
        } else {
            String msg = new String(data);
            if(newline.equals(TextUtil.newline_crlf) &amp;&amp; msg.length() &gt; 0) {
                // don't show CR as ^M if directly before LF
                msg = msg.replace(TextUtil.newline_crlf, TextUtil.newline_lf);
                // special handling if CR and LF come in separate fragments
                if (pendingNewline &amp;&amp; msg.charAt(0) == '\n') {
                    Editable edt = receiveText.getEditableText();
                    if (edt != null &amp;&amp; edt.length() &gt; 1)
                        edt.replace(edt.length() - 2, edt.length(), &quot;&quot;);
                }
                pendingNewline = msg.charAt(msg.length() - 1) == '\r';
            }
            receiveText.append(TextUtil.toCaretString(msg, newline.length() != 0)); //print out data
            output = receiveText.getText().toString(); // CharSequence to String
        }
    }
</code></pre>
<p>OnCreate of mainActivity2, *receiveData() is to add data to the recyclerview</p>
<pre><code>@Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main2);
        Toolbar toolbar = findViewById(R.id.toolbar2);
        setSupportActionBar(toolbar);
        recyclerView = findViewById(R.id.dataflow);
        dataList = new ArrayList&lt;&gt;();
        String data;
        Bundle extras = getIntent().getExtras();
        if (extras != null)
        {
            data = extras.getString(&quot;output&quot;);
            receiveData(data);
        }
        setAdapter();
    }
</code></pre>
","<java><android><android-intent><android-activity><android-recyclerview>","2022-05-26 02:57:34","44","0","1","72405276","<p>i think your problems is unknown how to get activity_main data onto activity_main2 dynamically , You can user &quot; EventBus &quot; to solve data updates dynamically , whatever you use another fragment or activity_main2 .</p>
"
"72348785","How to build a correct data flow using Do-notation?","<p>I just started to learn functional programming world by using fp-ts lib. At this moment I can understand basic concept of proposed function by this lib, but I can't understand how to glue them all together in the single data flow.</p>
<p>I would like to share a user story that I want to implement and use it as an example for this question. It sounds like this:</p>
<ul>
<li>User should be able to book an appointment of selected specialist</li>
</ul>
<p>I know it doesn't make sense to you at this moment, but let me show you how it looks in the code to be on the same page.</p>
<p>Note: This is pseudo code to make it more readable</p>
<pre class=""lang-js prettyprint-override""><code>const inputData = {
  userId: 1,
  specialistId: 2,
  selectedServicesIds: ['a', 'b', 'c'],
  startTime: 'today at 12:00'
}

const user = await fetchUserById(inputData.userId)

if (user === null) {
  throw 'User not found'
}

const specialist = await fetchSpecialistById(inputData.specialistId)

if (user === null) {
  throw 'Specialist not found'
}

const workingDay = await fetchWorkingDay(inputData.specialistId, inputData.startTime)

if (workingDay === null) {
  throw 'WorkingDay not found'
}

const selectedServices = await fetchSelectedServices(inputData.specialistId, inputData.selectedServicesIds)

if (selectedServices.length &lt; inputData.selectedServices) {
  throw 'Some selected services are not belong to the selected specialist'
}

const selectedServicesDuration = calculateDuration(selectedServices)
const appointmentEndTime = addMinutes(inputData.startTime, selectedServicesDuration)

const existingAppointments = await fetchAppointmentsOfSpeciallist(inputData.specialistId)

const isAppointmentOverlapExistingAppointments = isOverlaps(existingAppointments, inputData.startTime, appointmentEndTime)

if (isAppointmentOverlapExistingAppointments) {
  throw 'Appointment overlap existing appointments'
}

return new Appointment(inputData.userId, inputData.specialistId, ...)
</code></pre>
<p>As you can see this is typical imperative code:</p>
<ol>
<li>take input data</li>
<li>fetch data from db</li>
<li>apply validation</li>
<li>return result</li>
</ol>
<p>Now what I was able to achieve using fp-ts and <code>Do</code>-notation</p>
<pre class=""lang-js prettyprint-override""><code>  pipe(
    RTE.Do,
    RTE.apS('user', fetchUserById(args.input.clientId)),
    RTE.apSW('specialist', fetchSpecialistById(args.input.specialistId)),
    RTE.apSW('workingDay', fetchWorkingDay(args.input.specialistId, args.input.startDateTime)),
    RTE.apSW('assignedServices', getAssignedServicesOfSpecialist(args.input.specialistId, args.input.servicesIds))
    RTE.map({ user, specialist, workingDay, assignedServices } =&gt; {
       // Do I need to write all logic here? 
    })
</code></pre>
<p>As you can see, there are few parallel requests to fetch related data, but don't know what to do next. If I just put the imperative logic from the previous example inside <code>RTE.map</code> function it will look like I wrapped imperative code with some fp-ts functions.</p>
<p>Could you please give me an advice of how to split this into different function and how to glue them all together?</p>
","<fp-ts>","2022-05-23 12:49:08","595","0","1","72600492","<p>The key observation when refactoring code to use <code>fp-ts</code> is to rely on <code>Either</code> or <code>TaskEither</code> to convey information about errors. Then, via composition, you get to build more complex workflows by using simpler workflows with precise error handling since the information about possible errors is stored in the TypeScript types.</p>
<p>In your case, the snippet of <code>fp-ts</code> code is one valid way to write that logic. Each individual function you use in that pipeline (<code>fetchUserById</code>, <code>fetchSpecialistById</code>, <code>fetchWorkingDay</code>, <code>getAssignedServicesOfSpecialist</code>) should return <code>ReaderTaskEither&lt;R, E, A&gt;</code> (or just <code>TaskEither&lt;E, A&gt;</code>, since I do not see the <code>R</code> being used in your code). Each of them should do its own error handling. For example, <code>fetchUserById</code> could return a type similar to <code>TaskEither&lt;{ type: 'user-not-found' }, User&gt;</code>, where the first generic argument determines the possible error that this function can return.</p>
<p>If each individual function processes the data it fetches and returns either an error or that data, then in the final <code>map</code> you can do the last data-processing based on all the fetched information (which must be valid at this point, since <code>TaskEither</code> takes care of error propagation, so <code>map</code> will not be called if there was at least one error).</p>
<h2>Possible enhancements</h2>
<h3>Using <code>TaskEither</code> instead of <code>ReaderTaskEither</code></h3>
<p>Your <code>fp-ts</code> snippet uses <code>RTE</code>, which I assume is <code>ReaderTaskEither</code>. It does not user the <code>Reader</code> part of that type, though. It always refers to <code>args</code>, which I assume is a variable in the parent scope. Thus, you can simplify this code and use the <code>TaskEither</code> type instead.</p>
<h3>Handling errors that can arise in the last <code>map</code></h3>
<p>If the final data-combination done in <code>map</code> can result in some error, you may want to use <a href=""https://gcanti.github.io/fp-ts/modules/TaskEither.ts.html#chaineitherkw"" rel=""nofollow noreferrer""><code>taskEither.chainEitherK</code></a> to return the result:</p>
<pre class=""lang-ts prettyprint-override""><code>TE.chainEitherK(({ user, specialist, workingDay, assignedServices }) =&gt; {
  if (someCondition(user, specialist)) {
    return E.left({ type: 'some-error' });
  }

  // ...

  return E.right(/* ... */);
});
</code></pre>
<h3>Reporting all errors instead of the first one</h3>
<p>By default <code>TaskEither</code> will only propagate information about a single error. This happens when you use <code>ap</code> or <code>chain</code> - only the first error will be propagated in the final result.</p>
<p>If you have multiple parallel network calls that can fail, you may want to return all the errors instead of only the first one.</p>
<p>If your errors have the same type, you can use either <a href=""https://gcanti.github.io/fp-ts/modules/Apply.ts.html#sequencet"" rel=""nofollow noreferrer""><code>apply.sequenceT</code></a> or <a href=""https://gcanti.github.io/fp-ts/modules/Array.ts.html#sequence"" rel=""nofollow noreferrer""><code>array.sequence</code></a> to combine the results into a single <code>Either&lt;E[], A[]&gt;</code>. I like <code>apply.sequenceT</code> more because the result's <code>Right</code> case is not an array (of unknown length), but a tuple with known length and values.</p>
<p>As the <code>Apply</code>/<code>Applicative</code> parameter of either of these functions you could use <a href=""https://gcanti.github.io/fp-ts/modules/TaskEither.ts.html#getapplicativetaskvalidation"" rel=""nofollow noreferrer""><code>taskEither.getApplicativeTaskValidation</code></a>, which combines errors into an array.</p>
<p><code>getApplicativeTaskValidation</code> needs to be provided a <code>Semigroup</code> for the array elements. The easiest case is when all errors have the same type - you can use <a href=""https://gcanti.github.io/fp-ts/modules/Array.ts.html#getsemigroup"" rel=""nofollow noreferrer""><code>array.getSemigroup&lt;MyErrorType&gt;()</code></a> to get <code>Semigroup&lt;MyErrorType[]&gt;</code>.</p>
<p>However, usually my error types are different. I assume this may be the case here too. Thus, I developed this utility function as an alternative approach that uses an union of error types for the <code>either.Left</code>:</p>
<pre class=""lang-ts prettyprint-override""><code>const tupleError = &lt;E, A&gt;(
  t: taskEither.TaskEither&lt;E, A&gt;
): taskEither.TaskEither&lt;[E], A&gt; =&gt;
  pipe(
    t,
    taskEither.mapLeft((e) =&gt; [e])
  );

const partitionErrors = &lt;
  T extends nonEmptyArray.NonEmptyArray&lt;either.Either&lt;any, any&gt;&gt;
&gt;(
  results: T
) =&gt; {
  type ExtractLeft&lt;T&gt; = T extends either.Left&lt;infer E&gt; ? E : never;
  type WorkflowError = ExtractLeft&lt;typeof results[number]&gt;[number];
  const validation = either.getApplicativeValidation(
    array.getSemigroup&lt;WorkflowError&gt;()
  );
  return apply.sequenceT(validation)(...results);
};

pipe(
  apply.sequenceT(task.ApplyPar)(
    tupleError(fetchUserById(args.input.clientId)),
    tupleError(fetchSpecialistById(args.input.specialistId)),
    tupleError(
      fetchWorkingDay(args.input.specialistId, args.input.startDateTime)
    ),
    tupleError(
      getAssignedServicesOfSpecialist(
        args.input.specialistId,
        args.input.servicesId
      )
    )
  ),
  task.map(partitionErrors),
  taskEither.match(
    (errors) =&gt; {
      // TODO: handle errors
    },
    ([user, specialist, workingDay, assignedServices]) =&gt; {
      // TODO:
    }
  )
);

// Stub types
interface User {}
interface Specialist {}
interface WorkingDay {}
interface SelectedServices {}

// Your functions should return TaskEither. The first generic argument specifies the possible error type. The second argument is for the &quot;right&quot; value.
declare function fetchUserById(
  clientId: unknown
): taskEither.TaskEither&lt;{ type: &quot;user-not-found-error&quot; }, User&gt;;

declare function fetchSpecialistById(
  specialistId: unknown
): taskEither.TaskEither&lt;{ type: &quot;specialist-not-found&quot; }, Specialist&gt;;

declare function fetchWorkingDay(
  specialistId: unknown,
  startDateTime: unknown
): taskEither.TaskEither&lt;{ type: &quot;working-day-not-found&quot; }, WorkingDay&gt;;

declare function getAssignedServicesOfSpecialist(
  specialistId: unknown,
  servicesIds: unknown[]
): taskEither.TaskEither&lt;
  { type: &quot;selected-services-do-not-belong-to-specialist&quot; },
  SelectedServices
&gt;;
</code></pre>
<p>In fact, I wrote <a href=""https://www.gregroz.me/article/collecting-multiple-errors-fp-ts"" rel=""nofollow noreferrer"">a blog post about this pattern</a>.</p>
"
"72319088","C++ prioritize data synchronization between threads","<p>I have a scenario, where I have a shared data model between several threads. Some threads are going to write to that data model cyclically and other threads are reading from that data model cyclically. But it is guaranteed that writer threads are only writing and reader threads are only reading.</p>
<p><a href=""https://i.stack.imgur.com/fJ717.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fJ717.png"" alt=""enter image description here"" /></a></p>
<p>Now the scenario is, that reading data shall have higher priority than writing data due to real time constraints on the reader side. So it is not acceptable that e.g. a writer is locking the data for a too long time. But a lock with a guaranteed locking time would be acceptable (e.g. it would be acceptable for the reader to wait max 1 ms until the data is synchronized and available).</p>
<p>So I'm wondering how this is achievable, because the &quot;traditional&quot; locking mechanisms (e.g. <code>std::lock</code>) wouldn't give those real time guarantees.</p>
","<c++><multithreading>","2022-05-20 12:39:44","171","3","4","72319352","<p>Well, you've got readers, and you've got writers, and you need a lock, so.... how about <a href=""https://en.wikipedia.org/wiki/Readers%E2%80%93writer_lock"" rel=""nofollow noreferrer"">a readers/writer lock</a>?</p>
<p>The reason I mention that up-front is because (a) you might not be aware of it, but more importantly (b) there's no standard RW lock in C++ (EDIT: my mistake, one was added in C++14), so your thinking about this is perhaps being done in the context of std::mutex. Once you've decided to go with a RW lock, you can benefit from other people's thinking about those locks.</p>
<p>In particular, there's a number of different options for prioritizing threads contending over RW locks. With one option, a thread acquiring a write lock waits until all current reader threads drop the lock, but readers who start waiting after the writer don't get the lock until the writer's done with it.</p>
<p>With that strategy, as long as the writer thread releases and reacquires the lock after each transaction, and as long as the writer completes each transaction within your 1 ms target, readers don't starve.</p>
<p>And if your writer <em>can't</em> promise that, then there is zero alternative but to redesign the writer: either doing more processing <em>before</em> acquiring the lock, or splitting a transaction into multiple pieces where it's safe to drop the lock between each.</p>
<p>If, on the other hand, your writer's transactions take <em>much less</em> than 1 ms, then you might <em>consider</em> skipping the release/reacquire between each one if less than 1 ms has elapsed (purely to reduce the processing overhead of doing so).... but I wouldn't advise it. Adding complexity and special cases and (shudder) <em>wall clock time</em> to your implementation is rarely the most practical way to maximize performance, and rapidly increases the risk of bugs. A simple multithreading system is a reliable multithreading system.</p>
"
"72319088","C++ prioritize data synchronization between threads","<p>I have a scenario, where I have a shared data model between several threads. Some threads are going to write to that data model cyclically and other threads are reading from that data model cyclically. But it is guaranteed that writer threads are only writing and reader threads are only reading.</p>
<p><a href=""https://i.stack.imgur.com/fJ717.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fJ717.png"" alt=""enter image description here"" /></a></p>
<p>Now the scenario is, that reading data shall have higher priority than writing data due to real time constraints on the reader side. So it is not acceptable that e.g. a writer is locking the data for a too long time. But a lock with a guaranteed locking time would be acceptable (e.g. it would be acceptable for the reader to wait max 1 ms until the data is synchronized and available).</p>
<p>So I'm wondering how this is achievable, because the &quot;traditional&quot; locking mechanisms (e.g. <code>std::lock</code>) wouldn't give those real time guarantees.</p>
","<c++><multithreading>","2022-05-20 12:39:44","171","3","4","72322472","<p>Normally in such a scenario you use a <a href=""https://en.wikipedia.org/wiki/Readers%E2%80%93writer_lock"" rel=""nofollow noreferrer"">reader-writer-lock</a>. This allows either a read by all readers in parallel or a write by a single writer.</p>
<p>But that does nothing to stop a writer from holding the lock for minutes if it so desires. Forcing the writer out of the lock is probably also not a good idea. The object is probably in some inconsistent state mid changed.</p>
<p>There is another synchronization method called <a href=""https://en.wikipedia.org/wiki/Read-copy-update"" rel=""nofollow noreferrer"">read-copy-update</a> that might help. This allows writers to modify element without being blocked by readers. The drawback is that you might get some readers still reading the old data and others reading the new data for some time.</p>
<p>It also might be problematic with multiple writers if they try to change the same member. The slower writer might have computed all the needed updates only to notice some other thread changes the object. It then has to start over wasting all the time it already spend.</p>
<p>Note: copying the element can be done in constant time, certainly under 1ms. So you can guarantee readers are never blocked for long. By releasing the write lock first you guarantee readers to read between any 2 writes, assuming the RW lock is designed with the same principle.</p>
<hr />
<p>So I would suggest another solution I call write-intent-locking:</p>
<p>You start with a RW lock but add a lock to handle write-intent. Any writer can acquire the write-intent lock at any time, but only one of them, it's exclusive. Once a write holds the write-intent lock it copies the element
and starts modifying the copy. It can take as long as it wants to do that as it's not blocking any readers. It does block other writers though.</p>
<p>When all the modifications are done the writer acquires the write lock and then quickly copies, moves or replaces the element with the prepared copy. It then releases the write and write-intent lock, unblocking both the readers and writers that want to access the same element.</p>
"
"72319088","C++ prioritize data synchronization between threads","<p>I have a scenario, where I have a shared data model between several threads. Some threads are going to write to that data model cyclically and other threads are reading from that data model cyclically. But it is guaranteed that writer threads are only writing and reader threads are only reading.</p>
<p><a href=""https://i.stack.imgur.com/fJ717.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fJ717.png"" alt=""enter image description here"" /></a></p>
<p>Now the scenario is, that reading data shall have higher priority than writing data due to real time constraints on the reader side. So it is not acceptable that e.g. a writer is locking the data for a too long time. But a lock with a guaranteed locking time would be acceptable (e.g. it would be acceptable for the reader to wait max 1 ms until the data is synchronized and available).</p>
<p>So I'm wondering how this is achievable, because the &quot;traditional&quot; locking mechanisms (e.g. <code>std::lock</code>) wouldn't give those real time guarantees.</p>
","<c++><multithreading>","2022-05-20 12:39:44","171","3","4","72322687","<p>The way I would approach this is to have two identical copies of the dataset; call them copy A and copy B.</p>
<p>Readers always read from copy B, being careful to lock a reader/writer lock in read-only mode before accessing it.</p>
<p>When a writer-thread wants to update the dataset, it locks copy A (using a regular mutex) and updates it.  The writer-thread can take as long as it likes to do this, because no readers are using copy A.</p>
<p>When the writer-thread is done updating copy A, it locks the reader/writer lock (in exclusive/writer-lock mode) and swaps dataset A with dataset B.  (This swap should be done by exchanging pointers, and is therefore O(1) fast).</p>
<p>The writer-thread then unlocks the reader/writer-lock (so that any waiting reader-threads can now access the updated data-set), and then updates the other data-set the same way it updated the first data-set.  This can also take as long as the writer-thread likes, since no reader-threads are waiting on this dataset anymore.</p>
<p>Finally the writer-thread unlocks the regular mutex, and we're done.</p>
"
"72319088","C++ prioritize data synchronization between threads","<p>I have a scenario, where I have a shared data model between several threads. Some threads are going to write to that data model cyclically and other threads are reading from that data model cyclically. But it is guaranteed that writer threads are only writing and reader threads are only reading.</p>
<p><a href=""https://i.stack.imgur.com/fJ717.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fJ717.png"" alt=""enter image description here"" /></a></p>
<p>Now the scenario is, that reading data shall have higher priority than writing data due to real time constraints on the reader side. So it is not acceptable that e.g. a writer is locking the data for a too long time. But a lock with a guaranteed locking time would be acceptable (e.g. it would be acceptable for the reader to wait max 1 ms until the data is synchronized and available).</p>
<p>So I'm wondering how this is achievable, because the &quot;traditional&quot; locking mechanisms (e.g. <code>std::lock</code>) wouldn't give those real time guarantees.</p>
","<c++><multithreading>","2022-05-20 12:39:44","171","3","4","72324685","<p>If model allows writing to be interrupted, then it also allows buffering. Use a fifo queue and start reading only when there are 50 elements written already. Use (smart)pointers to swap data in fifo queue. Swapping 8 bytes of pointer takes nanoseconds. Since there is buffering, writing will be on a different element than readers are working with so there wont be lock contention as long as producer can keep the pace with producers.</p>
<hr />
<p>Why doesn't the reader produce its own consumer data? If you can have n producers and n consumers, each consumer can produce its own data too, without any producer. But this will have different multithread scaling. Maybe your algorithm is not applicable here but if it is, it would be more like independent multi-processing instead of multi-threading.</p>
<hr />
<p>Writer work can be converted to multiple smaller jobs? Progress within writer can be reported to an atomic counter. When a reader has a waiting budget, it checks atomic value and if it looks slow, it can use same atomic value to instantly push it to 100% progress and writer sees it and early-quits lock.</p>
"
"72311709","How to configure iOS MDM payload DNSSettings to stay active after device restart?","<p>We are using SimpleMDM to manage iPhones and iPads, and push a Custom Configuration Profile with <a href=""https://developer.apple.com/documentation/devicemanagement/dnssettings"" rel=""nofollow noreferrer"">DNSSettings MDM payload</a></p>
<p>This DNSSettings payload configures device to use a custom DNS server via HTTPS protocol.</p>
<p>These changes work until the device is restarted. After the restart we can see the profile with DNSSettings payload on the device in Settings &gt; General &gt; VPN, DNS, &amp; Device Management. However, the device is no longer configured to use that custom DNS server, until we un-assign and re-assign the profile on the device.</p>
<p>How can we ensure that a config profile with DNSSettings MDM payload is active on the device even after device restart?</p>
","<ios><configuration><dns><mdm><dns-over-https>","2022-05-19 22:27:11","308","0","1","72705292","<p>I was trying to get this to work as well, then I found buried in Apple's documentation that when the DNS Settings profile is installed via an MDM, it only works when connected to managed wifi networks.</p>
<p><a href=""https://developer.apple.com/documentation/devicemanagement/dnssettings"" rel=""nofollow noreferrer"">https://developer.apple.com/documentation/devicemanagement/dnssettings</a></p>
"
"72306022","are multiline sql statements possible in visual expression builder in data flows?","<p>The source in my dataflow is a sql server, and i am obtaining the data using a sql query.
The Visual expression builder throws errors and says the expression is incorrect if i try to do this over multiline (see below images).</p>
<p>Am i doing something incorrectly, or does the IDE still not allow multi line statements?</p>
<p><a href=""https://i.stack.imgur.com/YNCax.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YNCax.png"" alt=""enter image description here"" /></a></p>
<p>Single line, no errors</p>
<p><a href=""https://i.stack.imgur.com/vhEkB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vhEkB.png"" alt=""enter image description here"" /></a></p>
<p>multi line - errors on each line break</p>
","<azure-data-factory>","2022-05-19 14:04:51","491","1","1","72347045","<p>Thank you @ray &amp; @Benedikt for your valuable input, posting it as an answer to help other community members.</p>
<p>If you use a direct query without including parameters, you can write your multi-line SQL query in the source query builder.</p>
<p><a href=""https://i.stack.imgur.com/m38wS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m38wS.png"" alt=""enter image description here"" /></a></p>
<p>Use visual expression builder if you want to pass the query dynamically to include expression elements.</p>
<p>Here, use Concat() function to build a multi-line query in the expression builder.</p>
<p><a href=""https://i.stack.imgur.com/RaLvX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RaLvX.png"" alt=""enter image description here"" /></a></p>
"
"72298345","Xamarin IOS/Android DOTMIM SYNC compatible with Azure Sql","<p>I have an Xamarin cross platform application.  I am currently storing my data in a Sqlite Database.  However we want to have bi-directional syncing with an Azure Sql Database.  After much reading I decided to give Dotmim Sync a try.  The initial sync worked, however when adding a column to a table and attempting to migrate the data (following the tutorial), I got an error stating that the tracking table was missing.  I redid everything again and realized that the entity tracking table was never created and I am not sure why.  However Sql created a tracking table but it was not the entity tracking table that the error stated was missing.</p>
<p>I am curious if anyone with Xamarin has been able to successfully create bi-directional syncing with Sqlite and Azure Sql using Dotmim Sync.  I have yet to find anything else that will work.  Other than hand jamming it in this tutorial:  <a href=""https://www.xamarinhelp.com/mobile-database-bi-directional-synchronization-rest-api/"" rel=""nofollow noreferrer"">https://www.xamarinhelp.com/mobile-database-bi-directional-synchronization-rest-api/</a></p>
<p>I am not against that, just seems like a lot of room for error.  I am hoping someone out there has had success with what I am trying to do.</p>
","<database><sqlite><xamarin.forms><azure-sql-database><data-synchronization>","2022-05-19 03:36:16","215","0","1","72950103","<p>Hello I'm using Dotmim sync to synchronize a Sql Server database with a Sqlite database hotsted in a xamarin forms application through an API.
I think that I had the same problem as yours.
The problem is, as far as I understood, that the tracking tables are created only on the first sync.
If you change the schema of those tables you will need to call the Deprovision method.
This method will re-create the stored procedures and the triggers with the new database schema.
I'll leave you the link to the docs:
<a href=""https://dotmimsync.readthedocs.io/Provision.html#provision-deprovision"" rel=""nofollow noreferrer"">https://dotmimsync.readthedocs.io/Provision.html#provision-deprovision</a>.</p>
"
"72263748","How to patch spring cloud data flow starter application (add oracle driver to jdbc-source app)","<p>Im trying to add an oracle jdbc driver to to the <a href=""https://docs.spring.io/spring-cloud-stream-app-starters/docs/current-SNAPSHOT/reference/htmlsingle/#spring-cloud-stream-modules-jdbc-source"" rel=""nofollow noreferrer"">ojdbc-source</a> from SCDF. Regarding to the <a href=""https://docs.spring.io/spring-cloud-stream-app-starters/docs/Einstein.RELEASE/reference/htmlsingle/#_patching_pre_built_applications"" rel=""nofollow noreferrer"">official guide</a> it should be an easy task but at runtime i always get an</p>
<blockquote>
<p>NoSuchMethodError: 'void org.springframework.integration.jdbc.JdbcPollingChannelAdapter.setMaxRowsPerPoll(int)'</p>
</blockquote>
<p>This is my Main-Class:</p>
<pre><code>package com.example.sourcejdbcora;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.Import;

@SpringBootApplication
@Import(org.springframework.cloud.stream.app.jdbc.source.JdbcSourceConfiguration.class)
public class SourceJdbcOraApplication {

    public static void main(String[] args) {
        SpringApplication.run(SourceJdbcOraApplication.class, args);
    }

}
</code></pre>
<p>and my pom.xml look like this:</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
    xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.6.7&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;groupId&gt;com.example&lt;/groupId&gt;
    &lt;artifactId&gt;source-jdbc-ora&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;source-jdbc-ora&lt;/name&gt;
    &lt;description&gt;ojdb-source wit horacle driver&lt;/description&gt;
    &lt;properties&gt;
        &lt;java.version&gt;11&lt;/java.version&gt;
        &lt;spring-cloud.version&gt;2021.0.2&lt;/spring-cloud.version&gt;
    &lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-starter-task&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud.stream.app&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-starter-stream-source-jdbc&lt;/artifactId&gt;
            &lt;version&gt;2.1.7.RELEASE&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.oracle.database.jdbc&lt;/groupId&gt;
            &lt;artifactId&gt;ojdbc10&lt;/artifactId&gt;
            &lt;version&gt;19.14.0.0&lt;/version&gt;
        &lt;/dependency&gt;

    &lt;/dependencies&gt;
    &lt;dependencyManagement&gt;
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
                &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
                &lt;version&gt;${spring-cloud.version}&lt;/version&gt;
                &lt;type&gt;pom&lt;/type&gt;
                &lt;scope&gt;import&lt;/scope&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/dependencyManagement&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;
</code></pre>
<p>What am i doing wrong?</p>
","<spring-cloud-stream><spring-cloud-dataflow>","2022-05-16 18:09:44","215","0","1","72277171","<p>You are using the old (now deprecated) app starters repository. The new apps repository is here: <a href=""https://github.com/spring-cloud/stream-applications"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/stream-applications</a>.</p>
<p>You need to patch the JDBC Source with the proper Oracle driver.</p>
<p>Here is a somewhat related answer to a different app: <a href=""https://stackoverflow.com/a/67132875/2070861"">https://stackoverflow.com/a/67132875/2070861</a></p>
<p>You need to clone the repository and add the necessary dependencies. Then, rebuild the JDBC Source app.</p>
<p>You need to add the dependencies in the base supplier artifact: <a href=""https://github.com/spring-cloud/stream-applications/blob/main/functions/supplier/jdbc-supplier/pom.xml"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/stream-applications/blob/main/functions/supplier/jdbc-supplier/pom.xml</a></p>
<p>Then build the following:</p>
<pre><code>./mvnw clean install -pl :jdbc-supplier
./mvnw clean install -pl :jdbc-source
</code></pre>
<p>That should generate the proper uber jars.</p>
"
"72169428","Android 12 Work profile device identification","<p>With the latest changes in Android 12, there is no way on work profile to obtain any kind of device identifier. In <a href=""https://emm.androidenterprise.dev/s/article/Android-12-Impact-Device-Identifiers"" rel=""nofollow noreferrer"">this post</a> specifies to use the Enrollment ID in work profile with Custom DPC:</p>
<pre><code>if(VERSION.SDK_INT &gt;= VERSION_CODES.S) {

   // getEnrollmentSpecificId will return an empty string (&quot;&quot;) 

   // when organization id is not set

   devicePolicyManager.setOrganizationId(organizationId);

   devicePolicyManager.getEnrollmentSpecificId();

} else {

   // existing functionality to retrieve device identifier(s)

}
</code></pre>
<p>But without custom DPC in Android Management API we can´t call setOrganizationId as it requires Profile owner scope... Is any way to have the enterprise id, enrollment id, or some idenfifier on Android 12 in work profile mode?</p>
<p>Kind regards</p>
","<android><mdm><android-management-api>","2022-05-09 09:03:32","523","3","2","72285083","<p>This seems to be supported by default: <a href=""https://developer.android.com/reference/android/app/admin/DevicePolicyManager#setOrganizationId(java.lang.String)"" rel=""nofollow noreferrer"">DevicePolicyManager#setOrganizationId(java.lang.String)</a></p>
<blockquote>
<p>Sets the Enterprise ID for the work profile or managed device. This is a requirement for generating an enrollment-specific ID for the device, see <code>getEnrollmentSpecificId()</code>. It is recommended that the Enterprise ID is at least 6 characters long, and no more than 64 characters.</p>
</blockquote>
<p>There's still an <a href=""https://support.google.com/work/android/thread/122594729/get-unique-device-id-for-fully-managed-android-and-chrome-os-devices-from-managed-app?hl=en"" rel=""nofollow noreferrer"">alternate option</a> available:</p>
<ul>
<li>To grant permission <code>DELEGATION_CERT_INSTALL</code> and to use <code>Build.getSerial()</code>.</li>
</ul>
<p>Guess you'd be looking for a &quot;Work Policy Controller&quot;: <a href=""https://developer.android.com/work/dpc/build-dpc"" rel=""nofollow noreferrer"">https://developer.android.com/work/dpc/build-dpc</a></p>
<p>But there it reads:</p>
<blockquote>
<p>Note: This guide does not cover the situation where the work profile, under the enterprise's control, is the only profile on the device.<br/><br/>See <a href=""https://developers.google.com/android/work/overview#deployment_scenarios"" rel=""nofollow noreferrer"">Deployment scenarios</a> for more information.</p>
</blockquote>
<p>Since I don't know which scenario is given, that's probably the best I can offer. This part of the documentation at least explains the scenarios which are being considered and explains them in detail. As EMM provider, you might be able to directly reach out for their support. It still may help to determine the given scenario, to begin with - because it dictates what and how it can be done.</p>
"
"72169428","Android 12 Work profile device identification","<p>With the latest changes in Android 12, there is no way on work profile to obtain any kind of device identifier. In <a href=""https://emm.androidenterprise.dev/s/article/Android-12-Impact-Device-Identifiers"" rel=""nofollow noreferrer"">this post</a> specifies to use the Enrollment ID in work profile with Custom DPC:</p>
<pre><code>if(VERSION.SDK_INT &gt;= VERSION_CODES.S) {

   // getEnrollmentSpecificId will return an empty string (&quot;&quot;) 

   // when organization id is not set

   devicePolicyManager.setOrganizationId(organizationId);

   devicePolicyManager.getEnrollmentSpecificId();

} else {

   // existing functionality to retrieve device identifier(s)

}
</code></pre>
<p>But without custom DPC in Android Management API we can´t call setOrganizationId as it requires Profile owner scope... Is any way to have the enterprise id, enrollment id, or some idenfifier on Android 12 in work profile mode?</p>
<p>Kind regards</p>
","<android><mdm><android-management-api>","2022-05-09 09:03:32","523","3","2","75302148","<p>For Android Management API, I suggest reviewing <code>HardwareInfo.serialNumber</code>. This API returns information about device <a href=""https://developers.google.com/android/management/reference/rest/v1/enterprises.devices#hardwareinfo"" rel=""nofollow noreferrer"">hardware</a>, specifically the device serial number or ESID (enrollment-specific ID) on newly enrolled Android 12 Work Profile devices.</p>
<p><code>HardwareInfo.enterpriseSpecificId</code> will also report ESID (enrollment-specific ID) that uniquely identifies a personally-owned device in a particular organization. When enrolled with the same organization, this ID persists across setups, and even factory resets on the same physical device. Note that this ID is available on personally-owned devices with a work profile on Android 12 and above devices.</p>
"
"72146773","In an Azure data flow, can joins and lookups only be done between datasets of the same type?","<p>It's my understanding that lookups and joins in Azure data flows can only be performed between identical/similar formats. For example, between dedicated SQL pool and Azure SQL. In other words, I cannot pair up a dedicated SQL pool and parquet. Am I correct? Thanks much!</p>
","<azure><azure-sql-database><azure-synapse>","2022-05-06 19:57:34","61","0","1","72172868","<p><code>Joins</code> and <code>lookup</code> will only work if the data types of 2 datasets columns that are being matched are both compatible.</p>
<p>You can compare any 2 datasets if the column types are compatible with each other.</p>
<p><strong>Example:</strong></p>
<p>String type column can only be joined to string type column. If both columns are not of the same type, it gives an error.</p>
<p><a href=""https://i.stack.imgur.com/lVrPb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lVrPb.png"" alt=""enter image description here"" /></a></p>
"
"72139579","Create Function class in My Routine in Talend","<p><strong>Anyone Can help me to fix my Java Function to use it in my Routine.</strong></p>
<p>The Function will <strong>remove exponent</strong> and put it in a <strong>Float format</strong>.</p>
<p><strong>This is the function :</strong></p>
<pre><code>float amount = 3.53435E12;
java.text.DecimalFormat df = new java.text.DecimalFormat(&quot;# .# &quot;);
String s=df.format(amount );
System.out.println(&quot;amount = &quot; + s);
</code></pre>
<p>it will <strong>convert the number</strong> format from this <strong>3.53435E12</strong> to this <strong>3 534 350 000 000</strong></p>
<p>the result will be like this ---&gt; <strong>3 534 350 000 000</strong></p>
<p>Now i want to <strong>create a function</strong> in my Routine in order <strong>to use it in Tmap with my column.</strong></p>
<p>Thanks.</p>
","<java><function><talend><data-integration>","2022-05-06 09:55:47","208","0","1","72140144","<p>In your routine named for Example  <strong>MyRoutine</strong></p>
<pre><code> public static double convertNumber(double message) {
    
    java.text.DecimalFormat df = new java.text.DecimalFormat(&quot;# .# &quot;);
    String s=df.format(message );
    System.out.println(&quot;amount = &quot; + s);
    return (Double.parseDouble(s))  ;

}
</code></pre>
<p>The in tMap simply call</p>
<pre><code>MyRoutine.convertNumber(row1.amount)
</code></pre>
"
"72109911","How to transform a text file with tab separated fields to a pipe separated fields in pentaho?","<p>I have a situation where I want to transform a text file which has tab spaced fields like in the 'space-separated.png' below.</p>
<p>I want to transform this file by replacing tabs with pipes(|) like the 'pipe-separated.png' file below.</p>
<p>How can I do this in pentaho?</p>
<p><a href=""https://i.stack.imgur.com/jgHlD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jgHlD.png"" alt=""enter image description here"" /></a></p>
<p>space-separated.png</p>
<p><a href=""https://i.stack.imgur.com/VUFiy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VUFiy.png"" alt=""enter image description here"" /></a></p>
<p>pipe-separated.png</p>
","<pentaho><pentaho-spoon><pentaho-data-integration><data-integration>","2022-05-04 08:25:32","123","0","2","72126448","<p>It can be achieved by a transformation with two steps.</p>
<ol>
<li>Text file input (specify TAB as the separator in the content tab)</li>
<li>Text file output (specify | as the separator in the content tab)</li>
</ol>
<p>Remember to click on 'Get Fields' option in both the steps. Not clicking on 'Get Fields' is what took me time.</p>
"
"72109911","How to transform a text file with tab separated fields to a pipe separated fields in pentaho?","<p>I have a situation where I want to transform a text file which has tab spaced fields like in the 'space-separated.png' below.</p>
<p>I want to transform this file by replacing tabs with pipes(|) like the 'pipe-separated.png' file below.</p>
<p>How can I do this in pentaho?</p>
<p><a href=""https://i.stack.imgur.com/jgHlD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jgHlD.png"" alt=""enter image description here"" /></a></p>
<p>space-separated.png</p>
<p><a href=""https://i.stack.imgur.com/VUFiy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VUFiy.png"" alt=""enter image description here"" /></a></p>
<p>pipe-separated.png</p>
","<pentaho><pentaho-spoon><pentaho-data-integration><data-integration>","2022-05-04 08:25:32","123","0","2","72131418","<p>If you don't want for any reason load as TEXT FILE OUTPUT step, you also can read the file text, without delimeter, so entire data will be in a row, use REPLACE IN STRING step, REGEX YES and search \t, replace for |. thats all.</p>
<p>all data in a field:</p>
<p><a href=""https://i.stack.imgur.com/bUzCK.png"" rel=""nofollow noreferrer"">data view</a></p>
<p>Replace in string step:</p>
<p><a href=""https://i.stack.imgur.com/9O3rS.png"" rel=""nofollow noreferrer"">Configuration</a></p>
<p>Preview result:</p>
<p><a href=""https://i.stack.imgur.com/MNCHj.png"" rel=""nofollow noreferrer"">result with pipe</a></p>
"
"72048628","How to ensure data synchronization with OpenMP?","<p>When I try to do the math expression from the following code the matrix values are not consistent, how can I fix that?</p>
<pre><code>#pragma omp parallel num_threads (NUM_THREADS)
    {
    #pragma omp for
        for(int i = 1; i &lt; qtdPassos; i++)
        {   
            #pragma omp critical
            matriz[i][0] = matriz[i-1][0]; /
            for (int j = 1; j &lt; qtdElementos-1; j++)
            {
                matriz[i][j] = (matriz[i-1][j-1] + (2 * matriz[i-1][j]) + matriz[i-1][j+1]) / 4; // Xi(t+1) = [Xi-1 ^ (t) + 2 * Xi ^ (t)+ Xi+1 ^ (t)] / 4
            }
            matriz[i][qtdElementos-1] = matriz[i-1][qtdElementos-1]; 
        }
    }
</code></pre>
<p><a href=""https://i.stack.imgur.com/UwrzA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UwrzA.png"" alt=""Matrix execution"" /></a></p>
","<c><parallel-processing><openmp>","2022-04-28 18:27:29","69","0","1","72049114","<p>The problem comes from a <strong>race condition</strong> which is due to a <strong>loop carried dependency</strong>. The encompassing loop cannot be parallelised (nor the inner loop) since loop iterations <code>matriz</code> read/write the current and previous row. The same applies for the column.</p>
<p>Note that OpenMP does not check if the loop can be parallelized (in fact, it theoretically cannot in general). It is <em>your responsibility</em> to check that. Additionally, note that using a critical section for the whole iteration serializes the execution defeating the purpose of a parallel loop (in fact, it will be slower due to the overhead of the critical section). Note also that <code>#pragma omp critical</code> only applies on the next statement. Protecting the line <code>matriz[i][0] = matriz[i-1][0];</code> is not enough to avoid the race condition.</p>
<p>I do not think this current code can be (efficiently) parallelised. That being said, if your goal is to implement a 1D/2D stencil, then you can use a double buffering technique (ie. write in a 2D array that is different from the input array). A similar logic can be applied for 1D stencil repeated multiple times (which is apparently what you want to do). Note that the results will be different in that case. For the 1D stencil case, this double buffering strategy can fix the dependency issue and enable you to parallelize the inner-loop. For the 2D stencil case, the two nested loops can be parallelized.</p>
"
"72045776","MDM commands metadata","<p>I'm new in MDM servers at all.
I'm working with <a href=""https://github.com/micromdm/nanomdm"" rel=""nofollow noreferrer"">nanomdm</a> server. Can I add my own metadata to the commands or payload, which send to the device? For example, UDID,  that will be readed later by my mobile application and how can my mobile application do that? Or can I get UDID, which used in nanomdm, by my mobile application in another way?</p>
<p>Like that:</p>
<pre class=""lang-xml prettyprint-override""><code>...
&lt;key&gt;PayloadContent&lt;/key&gt;
  &lt;array&gt;
      &lt;dict&gt;
          &lt;key&gt;UDID&lt;/key&gt;
        &lt;string&gt;MY_UDID&lt;/string&gt;
      &lt;/dict&gt;
    &lt;dict&gt;
      &lt;key&gt;PayloadDisplayName&lt;/key&gt;
      &lt;string&gt;Profile Removal&lt;/string&gt;
      &lt;key&gt;PayloadIdentifier&lt;/key&gt;
...
</code></pre>
<pre class=""lang-swift prettyprint-override""><code>udid = get_from_profile()
</code></pre>
","<ios><swift><iphone><mdm>","2022-04-28 14:45:24","86","0","1","72049697","<p>One your iOS device has been registered with your MDM, you can provide configuration to an iOS app using the Settings command.</p>
<p>The app must first be &quot;managed&quot; by the MDM. This can be accomplished using the &quot;InstallApplication&quot; command. Once you allow the app to be managed, using the Settings command with a payload containing your devices UUID.</p>
"
"72033307","How does Firebase and MongoDB Atlas Synchronise Time?","<p>Does anyone know how the following services take their time references from. In other words: <strong>with what source do they sync their time reference?</strong></p>
<ul>
<li>Firebase</li>
<li>MongoDB Atlas</li>
</ul>
<p>Found out that AWS services sync their time with a service called Amazon Time Sync.</p>
<blockquote>
<p>Amazon Time Sync is used by EC2 instances and other AWS services. It
uses a fleet of redundant satellite-connected and atomic clocks in
each Region to deliver time derived from these highly accurate
reference clocks. This service is provided at no additional charge.</p>
</blockquote>
<p>Likewise I need information about Firebase and MongoDB Atlas specifically. Any help/source is appreciated.</p>
","<firebase><time><mongodb-atlas><data-synchronization>","2022-04-27 17:57:04","76","-1","1","72041249","<p>What I found by myself.</p>
<p><strong>AWS Services</strong> - AWS services sync time with Amazon Time Sync. It uses a fleet of redundant satellite-connected and atomic clocks in each Region to deliver time derived from these highly accurate reference clocks.</p>
<p><strong>Google Services</strong> - Google services including Firebase use Google Public NTP. This is a free, global time service that you can use to synchronize to Google's atomic clocks.</p>
<p><strong>MongoDB Atlas</strong> - MongoDB Atlas has been enhanced by a move to a global logical clock. Implemented as a hybrid logical clock and protected by encryption.</p>
<p>Since all the services use highly accurate time services, we can assume that they represent the exact time so that time is synchronized across all the services in one application.</p>
<p><em><strong>References</strong></em></p>
<ol>
<li><a href=""https://aws.amazon.com/about-aw"" rel=""nofollow noreferrer"">https://aws.amazon.com/about-aw</a></li>
<li><a href=""https://developers.google.com/time/faq"" rel=""nofollow noreferrer"">https://developers.google.com/time/faq</a></li>
<li><a href=""https://www.mongodb.com/blog/post/transactions-background-part-4-the-global-logical-clock"" rel=""nofollow noreferrer"">https://www.mongodb.com/blog/post/transactions-background-part-4-the-global-logical-clock</a></li>
</ol>
"
"71949154","How to remove a Automated Device Enrollment profile from macOS","<p>First, show the profiles I have.</p>
<pre><code>sudo profiles show -type enrollment
</code></pre>
<p>Second，remove all the profiles</p>
<pre><code>sudo profiles remove -all 
</code></pre>
<p>An error occurs:</p>
<pre><code>profiles: There are profiles installed that marked non-removable
</code></pre>
","<macos><profile><mdm>","2022-04-21 04:59:51","8118","2","2","71949155","<p>I found an answer here.
<a href=""https://graffino.com/til/UmkCdmEx7v-remove-a-non-removable-mdm-profile-from-macos-without-a-complete-wipe"" rel=""nofollow noreferrer"">https://graffino.com/til/UmkCdmEx7v-remove-a-non-removable-mdm-profile-from-macos-without-a-complete-wipe</a></p>
<ol>
<li>Boot the Mac into Recovery Mode (hold down command+R during startup).</li>
<li>Go to the Utilities menu and open Terminal and type: csrutil disable. This will disable SIP (System Integrity Protection).</li>
<li>Reboot into the OS.</li>
<li>Open the integrated terminal and type:</li>
</ol>
<pre><code>cd /var/db/ConfigurationProfiles
rm -rf *
mkdir Settings
touch Settings/.profilesAreInstalled
</code></pre>
<ol start=""5"">
<li>Reboot.</li>
<li>Boot the Mac into Recovery Mode (hold down command+R during startup).</li>
<li>Go to the Utilities menu and open Terminal and type: csrutil enable. This will re-enable SIP.</li>
<li>Reboot into the OS.</li>
</ol>
"
"71949154","How to remove a Automated Device Enrollment profile from macOS","<p>First, show the profiles I have.</p>
<pre><code>sudo profiles show -type enrollment
</code></pre>
<p>Second，remove all the profiles</p>
<pre><code>sudo profiles remove -all 
</code></pre>
<p>An error occurs:</p>
<pre><code>profiles: There are profiles installed that marked non-removable
</code></pre>
","<macos><profile><mdm>","2022-04-21 04:59:51","8118","2","2","74436978","<p>Suppression de profil MDM de supervision</p>
<p> Supervision d’appareils Apple</p>
<p>1️⃣ Turn of system integrity</p>
<p>Shut down computer.
Boot up computer while holding (command + R)
Press ‘utilities’.
Type (csrutil disable)
Restart</p>
<p>2️⃣ Give terminal root files access</p>
<p>Open ‘Privacy and Security’ in ‘System Preferences’.
Press ‘Privacy’
Press ‘Complete Disk Access’
Unlock with the lock button at the bottom left
Add terminal.</p>
<p>3️⃣ Terminal Commands</p>
<p>Type: ‘sudo jamf -removeFramework’ into terminal, press enter.
Type: ‘sudo -i’ into terminal, press enter and enter your password, press enter.
Type: ‘cd /var/db/‘ into terminal, press enter.
Type: ‘mv ConfigurationProfiles ConfigurationProfilesOLD’ into terminal, press enter.
Type: ‘logout’ into terminal, press enter.</p>
<p>4️⃣ Final steps</p>
<p>Restart computer.</p>
"
"71827939","Columns gets removed while loading an excel file to cosmos dB in ADF using data flow","<p>I am new to ADF and cosmos db. I am trying to load an excel which has 34 columns from a blob to a cosmos db container using a mapping data flow . While debugging the data flow I could preview all the columns in the source and its data available in both source as well as sink. But after running the pipeline(using trigger/debug) when I check the items/documents in the cosmos db container certain columns are missing. When I do the same data load using copy activity all the columns are getting inserted and this seems to be weird. I need to change the data types of few columns before loading into the database and that's the reason I am trying to load the data using data flow activity. I have been stuck at this for so long and wasn't able to proceed further. Any help would be appreciated.</p>
","<excel><azure-blob-storage><azure-cosmosdb><azure-data-factory>","2022-04-11 12:34:55","97","0","1","71841667","<p>I had repro’d and was able to load all (34) columns into cosmos db successfully using data flow.</p>
<ol>
<li>Source projection</li>
</ol>
<p><a href=""https://i.stack.imgur.com/cmuey.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cmuey.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>Source data preview: Here C1 column is of string type.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/I56iF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/I56iF.png"" alt=""enter image description here"" /></a></p>
<ol start=""3"">
<li>Changed the datatype of the C1 column from string to an integer.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/hnreQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hnreQ.png"" alt=""enter image description here"" /></a></p>
<ol start=""4"">
<li>Sink transformation: Make sure you have included all source columns in sink mappings.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/A3mTW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A3mTW.png"" alt=""enter image description here"" /></a></p>
<ol start=""5"">
<li>Cosmosdb Output: It includes all 34 columns from the source and C1 as integer type.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/Q1tbx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q1tbx.png"" alt=""enter image description here"" /></a></p>
"
"71753194","ADF copy activity and data flow behaving differently when writing data to multi lookup field in Dynamics 365","<p>I am trying to import data from a CSV file into a Dynamics 365 Account table. As I need to do some transformations I am using a dataflow rather than a basic copy activity.</p>
<p>I was having difficulties getting it to work using a dataflow for writing to a multi lookup field so I tried using a copy activity to see if that worked using the exact same source,sink and mappings. I was able to import the
data successfully with the copy activity. I'm confused as to why the data flow does not work using the same source,sink and mappings. Below are screenshots of the various elements I set up and configured. Would appreciate any suggestions to get the dataflow working.</p>
<p>I'm using a cut down version of what will ultimately be my source CSV file. This is just so I can concentrate on getting the writing to the lookup field working.</p>
<h1>Source CSV file</h1>
<p><a href=""https://i.stack.imgur.com/8XH7y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8XH7y.png"" alt=""enter image description here"" /></a></p>
<h1>Copy Activity Source</h1>
<p><a href=""https://i.stack.imgur.com/2CTWR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2CTWR.png"" alt=""enter image description here"" /></a></p>
<h1>Copy Activity Sink</h1>
<p><a href=""https://i.stack.imgur.com/soUvB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/soUvB.png"" alt=""enter image description here"" /></a></p>
<h1>Dynamics 365 Sink</h1>
<p><a href=""https://i.stack.imgur.com/lv2qD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lv2qD.png"" alt=""enter image description here"" /></a></p>
<h1>Dataflow Source</h1>
<p><a href=""https://i.stack.imgur.com/q0ZH4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q0ZH4.png"" alt=""enter image description here"" /></a></p>
<h1>Dataflow Sink</h1>
<p><a href=""https://i.stack.imgur.com/OZHNq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OZHNq.png"" alt=""enter image description here"" /></a></p>
<h1>Copy Activity Mapping</h1>
<p><a href=""https://i.stack.imgur.com/R3ZCI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/R3ZCI.png"" alt=""enter image description here"" /></a></p>
<h1>Dataflow Mapping</h1>
<p><a href=""https://i.stack.imgur.com/ZFDaa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZFDaa.png"" alt=""enter image description here"" /></a></p>
<h1>Copy Activity Success</h1>
<p><a href=""https://i.stack.imgur.com/hT6mE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hT6mE.png"" alt=""enter image description here"" /></a></p>
<h1>Dataflow Failure</h1>
<p><a href=""https://i.stack.imgur.com/s2BMv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s2BMv.png"" alt=""enter image description here"" /></a></p>
<h1>Dataflow Error</h1>
<p><a href=""https://i.stack.imgur.com/UA8ox.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UA8ox.png"" alt=""enter image description here"" /></a></p>
<p>Details
{&quot;StatusCode&quot;:&quot;DFExecutorUserError&quot;,&quot;Message&quot;:&quot;Job failed due to reason: DF-REST_001 - Rest - Error response received from the server (url:https://##############v9.0/accounts,request body: Some({&quot;accountid&quot;:&quot;8b0257ea-de19-4aaa-9945-############&quot;,&quot;name&quot;:&quot;A User&quot;,&quot;ownerid&quot;:&quot;7d64133b-daa8-eb11-9442-############&quot;,&quot;ownerid@EntityReference&quot;:&quot;systemuser&quot;}), request method: POST, status code: 400), response body: Some({&quot;error&quot;:{&quot;code&quot;:&quot;0x0&quot;,&quot;message&quot;:&quot;An error occurred while validating input parameters: Microsoft.OData.ODataException: A 'PrimitiveValue' node with non-null value was found when trying to read the value of the property 'ownerid'; however, a 'StartArray' node, a 'StartObject' node, or a 'PrimitiveValue' node with null value was expected.\r\n at Microsoft.OData.JsonLight.ODataJsonLightPropertyAndValueDeserializer.ValidateExpandedNestedResourceInfoPropertyValue(IJsonReader jsonReader, Nullable1 isCollection, String propertyName, IEdmTypeReference typeReference)\r\n at Microsoft.OData.JsonLight.ODataJsonLightResourceDeserializ&quot;,&quot;Details&quot;:&quot;com.microsoft.dataflow.Issues: DF-REST_001 - Rest - Error response received from the server (url:<a href=""https://dev-gc.crm11.dynamics.com/api/data/v9.0/accounts,request"" rel=""nofollow noreferrer"">https://dev-gc.crm11.dynamics.com/api/data/v9.0/accounts,request</a> body: Some({&quot;accountid&quot;:&quot;8b0257ea-de19-4aaa-9945-############&quot;,&quot;name&quot;:&quot;A User&quot;,&quot;ownerid&quot;:&quot;7d64133b-daa8-eb11-9442-############&quot;,&quot;ownerid@EntityReference&quot;:&quot;systemuser&quot;}), request method: POST, status code: 400), response body: Some({&quot;error&quot;:{&quot;code&quot;:&quot;0x0&quot;,&quot;message&quot;:&quot;An error occurred while validating input parameters: Microsoft.OData.ODataException: A 'PrimitiveValue' node with non-null value was found when trying to read the value of the property 'ownerid'; however, a 'StartArray' node, a 'StartObject' node, or a 'PrimitiveValue' node with null value was expected.\r\n at Microsoft.OData.JsonLight.ODataJsonLightPropertyAndValueDeserializer.ValidateExpandedNestedResourceInfoPropertyValue(IJsonReader jsonReader, Nullable1 isCollection, String propertyName, IEdmTypeReference typeReference)\r\n at Microsoft.OData.JsonLight.ODataJsonLightResourceDeser&quot;}</p>
","<azure><dynamics-365><azure-data-factory>","2022-04-05 14:04:39","741","2","1","72658302","<p>I am running into a same wall,
but a temporary solution here is to sink the dataflow output to a Csv/or similar file into ADLS and then use a Copy activity to extract those files and Upsert it into the Dynamics.</p>
<p>Other references: <a href=""https://vishalgrade.com/2020/10/01/how-to-populate-multi-lookup-attribute-in-ce-using-azure-data-factory/"" rel=""nofollow noreferrer"">https://vishalgrade.com/2020/10/01/how-to-populate-multi-lookup-attribute-in-ce-using-azure-data-factory/</a></p>
"
"71750287","Orika data mapping error after installing cmsoccaddon on custom commercewebservices on Hybris 2005","<p>We are using the custom commercewebservice extended from ycommercewebservice. We are not using the CMS API as of now but we have we need CMS APIs for spartacus.</p>
<p>I have install <strong>cmsoccaddon</strong> on our customer webservice extension. Addon install successfully but when we try to access https://localhost:9002/rest/v2/{base_site}/cms/pages it will throws the error:</p>
<p><code>ma.glasnost.orika.MappingException: While attempting the following mapping: sourceType = de.hybris.platform.cmsfacades.data.PageContentSlotData sourceProperty = slotShared(boolean) destinationType = java.util.ArrayList&lt;ContentSlotWsDTO&gt; destinationProperty = 8(ContentSlotWsDTO) Error occurred: ma.glasnost.orika.MappingException: Encountered mapping of primitive to object (or vise-versa); sourceType=boolean, destinationType=ContentSlotWsDTO</code></p>
<p>Fetch component API is working fine with the cmsaddon (https://localhost:9002/rest/v2/{base_site}/cms/components?fields=DEFAULT&amp;componentIds={componentId})</p>
<p>We Just update hybris version to 2005</p>
","<sap-commerce-cloud><spartacus-storefront>","2022-04-05 10:35:45","176","0","1","71970530","<p>Since you migrated from 1905 to 2005, the endpoints prefix has been changed from <code>/rest/v2/</code> to <code>/occ/v2</code>.</p>
<p>More information can be found in the following:</p>
<p>help.sap (2005) <a href=""https://help.sap.com/docs/SAP_COMMERCE/e5d7cec9064f453b84235dc582b886da/d46d19516961438f8939718e87ed787b.html?version=2005"" rel=""nofollow noreferrer"">https://help.sap.com/docs/SAP_COMMERCE/e5d7cec9064f453b84235dc582b886da/d46d19516961438f8939718e87ed787b.html?version=2005</a>
<a href=""https://help.sap.com/docs/SAP_COMMERCE/9d346683b0084da2938be8a285c0c27a/9136088a207c4fc1bb5ec2b53c462b23.html?version=2005"" rel=""nofollow noreferrer"">https://help.sap.com/docs/SAP_COMMERCE/9d346683b0084da2938be8a285c0c27a/9136088a207c4fc1bb5ec2b53c462b23.html?version=2005</a></p>
"
"71747151","How to connect Power BI with the website database?","<p>I have a Python -based website hosted on pythonanywhere.com. The website asks for feedback from the user and stores input in sqlite3.</p>
<p>How do I access the website's database in Microsoft Power BI (preferably real-time or periodic otherwise) to display the results?</p>
","<database><sqlite><powerbi><data-integration>","2022-04-05 06:28:28","858","0","1","73932825","<p>This is possible. You need the ODBC driver and gateway on your machine. The same user needs to be logged on your machine to make it work. See.
check <a href=""https://community.powerbi.com/t5/Power-Query/connecting-a-SQLite-database/td-p/224205"" rel=""nofollow noreferrer"">https://community.powerbi.com/t5/Power-Query/connecting-a-SQLite-database/td-p/224205</a></p>
"
"71620131","How to Import xPath fields of XML to Pimcore DataDirector dataport raw data fields?","<p>Pimcore has powerfull plugin DataDirector for import automation.
It proposes manual configuration of fields to import and their mapping to Pimcore fields <a href=""https://www.youtube.com/watch?v=nyhKJTzTq-4&amp;list=PL4-QRNfdsdKIfzQIP-c9hRruXf0r48fjt"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=nyhKJTzTq-4&amp;list=PL4-QRNfdsdKIfzQIP-c9hRruXf0r48fjt</a></p>
<p>It works fine if you have 10-50 fields.
How to import that configuration from some csv file whan you have 700+ fields?</p>
","<import><pimcore><data-mapping><datadirector>","2022-03-25 16:21:15","42","0","1","71620132","<p>There is no ability to import using interface of commandline DataDirector API.</p>
<p>I tried to request such functionality creation from vendor -- it costs.</p>
<p>I tried to edit it in MySQL but it is strictly coupled to other data, see: SELECT sourceconfig FROM <code>plugin_pim_dataport</code>;</p>
<p><strong>Solution:</strong></p>
<p>Each dataport can be exported and imported to JSON. This is our chance.</p>
<ol>
<li><p>create sample dataport using your source XML</p>
</li>
<li><p>export it to sample.json</p>
</li>
<li><p>unserialize $json_a['plugin_pim_dataport']['sourceconfig'] and $json_a['plugin_pim_rawitemField'] containers</p>
<pre><code>     $string = file_get_contents(
         &quot;sample.json&quot;
     );
     $json_a = json_decode($string, true);
     $sourceConfig = $json_a['plugin_pim_dataport']['sourceconfig'];
     $sourceConfig = unserialize($sourceConfig, ['allowed_classes' =&gt; false]);

     $fieldsConfig = $json_a['plugin_pim_rawitemField']; 
</code></pre>
</li>
<li><p>add new fields to them from any source e.g. CSV</p>
<pre><code>     while (($data = fgetcsv($handle, 1000, &quot;,&quot;)) !== false) {
         $row++;
         $id = $max + 1 + $row;
         $data = $this-&gt;addField($id, $data[0], $data[1]);
         $sourceConfig['fields']['field_' . ($id + 1)] = $data['sourceConfig'];
         $fieldsConfig[$id] = $data['fieldsConfig'];
     }

 public function addField(
     $id,
     $name = 'ObjectBrick/Field',
     $xpath = '//*[.//MainContainer = &quot;systeme&quot;]//*[.//Description = &quot;SomeAttribute&quot;]/anyPath/SampleContainer'
 ) {
     $res = [];

     $res['fieldsConfig'] = [
         &quot;dataportId&quot; =&gt; &quot;1&quot;,
         &quot;fieldNo&quot;    =&gt; $id + 1,
         &quot;name&quot;       =&gt; $name,
         &quot;priority&quot;   =&gt; $id
     ];
     $res['sourceConfig'] = [
         'xpath'       =&gt; $xpath,
         'multiValues' =&gt; false,
     ];

     return $res;
 }
</code></pre>
</li>
<li><p>serialize, save to JSON, import to your dataport.</p>
</li>
</ol>
"
"71603333","How to make device owner from setup wizard in AOSP programmatically","<p>I am looking to the setup device owner from custom setup wizard app programmatically
Currently, I am testing with google sample testDpc app.</p>
<p>I am able to ask for provisioning mode by using the calling ACTION_GET_PROVISIONING_MODE intent, but I don't know what to do after receiving there result</p>
<pre><code>Process proc = Runtime.getRuntime().exec(&quot;dpm set-device-owner com.afwsamples.testdpc/.DeviceAdminReceiver&quot;);
</code></pre>
<p>If I try to run the above command then I am getting the below error</p>
<pre><code>Exception occurred while executing 'set-device-owner': 
java.lang.SecurityException: Calling identity is not authorized
</code></pre>
<p>I want to setup device as orgnization owned work profile from setup wizard <em>(COPE profile)</em></p>
","<android><android-source><mdm><device-owner>","2022-03-24 13:24:19","1167","0","2","71612416","<p>Use DevicePolicyManager's hide method: setActiveAdmin and setDeviceOwner</p>
<p><a href=""https://cs.android.com/android/platform/superproject/+/master:packages/apps/ManagedProvisioning/src/com/android/managedprovisioning/task/SetDeviceOwnerPolicyTask.java"" rel=""nofollow noreferrer"">SetDeviceOwnerPolicyTask.run</a> is an example.</p>
"
"71603333","How to make device owner from setup wizard in AOSP programmatically","<p>I am looking to the setup device owner from custom setup wizard app programmatically
Currently, I am testing with google sample testDpc app.</p>
<p>I am able to ask for provisioning mode by using the calling ACTION_GET_PROVISIONING_MODE intent, but I don't know what to do after receiving there result</p>
<pre><code>Process proc = Runtime.getRuntime().exec(&quot;dpm set-device-owner com.afwsamples.testdpc/.DeviceAdminReceiver&quot;);
</code></pre>
<p>If I try to run the above command then I am getting the below error</p>
<pre><code>Exception occurred while executing 'set-device-owner': 
java.lang.SecurityException: Calling identity is not authorized
</code></pre>
<p>I want to setup device as orgnization owned work profile from setup wizard <em>(COPE profile)</em></p>
","<android><android-source><mdm><device-owner>","2022-03-24 13:24:19","1167","0","2","72330509","<p>To execute the dpm command successfully from within the application, two conditions should be fulfilled:</p>
<ul>
<li>Your application must have <code>android.permission.MANAGE_DEVICE_ADMINS</code> and <code>android.permission.MANAGE_PROFILE_AND_DEVICE_OWNERS</code> permissions;</li>
<li>The Android setup wizard should not be completed.</li>
</ul>
<p>To get the <code>android.permission.MANAGE_PROFILE_AND_DEVICE_OWNERS</code> permission, your application must be a system app, that is, you need to set the shared user as <code>android.uid.system</code> in <code>AndroidManifest.xml</code> and sign the app by <a href=""https://stackoverflow.com/questions/51723768/how-to-sign-android-app-with-platform-keys-using-gradle"">platform keys</a>.</p>
<p>The second condition could be ignored if your app, instead of running the <code>dpm set-device-owner</code> command, directly updates the system configuration XML files (since it's signed by platform keys, it is able to update them!).</p>
<p>Two system files should be updated. They must have the following content:</p>
<ol>
<li><p><code>/data/system/device_owner_2.xml</code></p>
<pre><code>&lt;?xml version='1.0' encoding='utf-8' standalone='yes' ?&gt;
&lt;root&gt;
    &lt;device-owner 
        package=&quot;your.package.name&quot; 
        name=&quot;&quot;
        component=&quot;your.package.name/your.package.name.AdminReceiverClassName&quot; 
        userRestrictionsMigrated=&quot;true&quot; 
        canAccessDeviceIds=&quot;true&quot; /&gt;
    &lt;device-owner-context userId=&quot;0&quot; /&gt;
&lt;/root&gt;
</code></pre>
</li>
<li><p><code>/data/system/device_policies.xml</code></p>
<pre><code>&lt;?xml version='1.0' encoding='utf-8' standalone='yes' ?&gt;
&lt;policies setup-complete=&quot;true&quot; provisioning-state=&quot;3&quot;&gt;
    &lt;admin name=&quot;your.package.name/your.package.name.AdminReceiverClassName&quot;&gt;
        &lt;policies flags=&quot;17&quot; /&gt;
        &lt;strong-auth-unlock-timeout value=&quot;0&quot; /&gt;
        &lt;user-restrictions no_add_managed_profile=&quot;true&quot; /&gt;
        &lt;default-enabled-user-restrictions&gt;
            &lt;restriction value=&quot;no_add_managed_profile&quot; /&gt;
        &lt;/default-enabled-user-restrictions&gt;
        &lt;cross-profile-calendar-packages /&gt;
    &lt;/admin&gt;
    &lt;password-validity value=&quot;true&quot; /&gt;
    &lt;lock-task-features value=&quot;16&quot; /&gt;
&lt;/policies&gt;
</code></pre>
</li>
</ol>
<p>To apply these changes in system configuration files, the device needs to be rebooted.</p>
<p>As a working sample, take a look at the source code of <a href=""https://h-mdm.com"" rel=""nofollow noreferrer"">Headwind MDM</a> which is the open source MDM solution able to be integrated into the AOSP (for example, LineageOS). Disclaimer: I am the owner of Headwind MDM.</p>
<p>The related code can be found in the following <a href=""https://github.com/h-mdm/hmdm-android/blob/master/app/src/main/java/com/hmdm/launcher/util/SystemUtils.java"" rel=""nofollow noreferrer"">file</a>:</p>
<pre><code>app/src/main/java/com/hmdm/launcher/util/SystemUtils.java
</code></pre>
"
"71595275","In ADF, I am not able to define the ""Response body definition"" in REST call (ExternalCall) in Mapping data flow","<p><a href=""https://i.stack.imgur.com/t3byO.png"" rel=""nofollow noreferrer"">pipeline</a></p>
<p><a href=""https://i.stack.imgur.com/t3byO.png"" rel=""nofollow noreferrer"">MY CURRENT PIPELINE</a></p>
<p>and I am trying to make an external call to my API which is an azure functions and the response of the API looks like</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>[
    {
        ""customerTaxCountryCode"": ""CODE1"",
        ""customerTaxLocalName"": ""TAXLOCAL1"",
        ""customerTaxNumber"": ""TAX00023"",
        ""customerTaxCountry"": ""USA"",
        ""customerTaxTypeCode"": ""TAXNO1""
    },
    {
        ""customerTaxCountryCode"": ""CODE2"",
        ""customerTaxLocalName"": ""TAXLOCAL2"",
        ""customerTaxNumber"": ""TAX9544"",
        ""customerTaxCountry"": ""USA"",
        ""customerTaxTypeCode"": ""TAXNO2""
    }
]</code></pre>
</div>
</div>
</p>
<p>I tried importing the schema by clicking Import Projection, but it is giving the irrelevant response</p>
<p><a href=""https://i.stack.imgur.com/pjezh.png"" rel=""nofollow noreferrer"">error response</a></p>
<p>I tried writing the complex structure as ,</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>(customerTaxTypeCode as string, customerTaxCountryCode as string, customerTaxLocalName as string, customerTaxNumber as string, customerTaxCountry as string )[]</code></pre>
</div>
</div>

<a href=""https://i.stack.imgur.com/sD1oY.png"" rel=""nofollow noreferrer"">not getting mapped</a></p>
<p>still, the data is not getting mapped.</p>
","<azure><azure-data-factory>","2022-03-23 23:23:58","265","1","1","71615468","<p>This isn't the right approach.</p>
<p>To read the data from API, you need to use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/control-flow-web-activity"" rel=""nofollow noreferrer""><code>Web activity</code></a> to fetch the data.</p>
<p>Use <code>@activity('Web1').output.Response</code> expression at second web activity to save the output as a blob to the container.</p>
<p><a href=""https://i.stack.imgur.com/sqnH4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sqnH4.png"" alt=""enter image description here"" /></a></p>
<p>Once data stored in blob storage, you can create a linked service for it and read the data in <code>data flow</code> activity and then you can transform it easily.</p>
"
"71588638","Microsoft Endpoint Manager Error deploying Preference File to Supervised MacBook","<p>When deploying a preference file (&quot;.plist&quot;) through MS Endpoint Manager (&quot;Intune&quot;) to a supervised MacBook Pro I get a an error:</p>
<ul>
<li>Setting name: ConfigurationXmlPcl</li>
<li>Error codes: -2016341103 and 0x87d11391</li>
</ul>
<p>This is the preference file I am using:</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;!DOCTYPE plist PUBLIC &quot;-//Apple//DTD PLIST 1.0//EN&quot; 
&quot;http://www.apple.com/DTDs/PropertyList-1.0.dtd&quot;&gt;
&lt;plist version=&quot;1.0&quot;&gt;
    &lt;dict&gt;
        &lt;key&gt;EnableMediaRouter&lt;/key&gt;
        &lt;false /&gt;
        &lt;key&gt;HomepageLocation&lt;/key&gt;
        &lt;string&gt;https://somesubdomain.sharepoint.com&lt;/string&gt;
        &lt;key&gt;NewTabPageSearchBox&lt;/key&gt;
        &lt;string&gt;redirect&lt;/string&gt;
        &lt;key&gt;PasswordManagerEnabled&lt;/key&gt;
        &lt;false /&gt;
        &lt;key&gt;PasswordProtectionChangePasswordURL&lt;/key&gt;
        &lt;string&gt;https://passwordreset.somedomain.com&lt;/string&gt;
    &lt;/dict&gt;
&lt;/plist&gt;
</code></pre>
<p>Tried to search for the setting and the error codes, but could not find anything related to Intune.</p>
<p>Any push in the right direction is much appreciated.</p>
","<macos><mdm><intune>","2022-03-23 14:08:07","1674","0","2","71782525","<p>I was having the same issue and error codes except trying to push Zoom configurations.</p>
<p>I was able to resolve this by stripping the all additional tags (, , etc) and leaving only values.  For example my edited .plist looked like this:</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;key&gt;ZAutoSSOLogin&lt;/key&gt;
&lt;false/&gt;
&lt;key&gt;ZSSOHost&lt;/key&gt;
&lt;string&gt;******.zoom.us&lt;/string&gt;
</code></pre>
<p>Also I had the 'Preference domain name'/App ID wrong and had to correct it (had entered us.zoom.voice and the actual App ID for Zoom for Mac is us.zoom.xos.)</p>
<p>Hope this helps.</p>
"
"71588638","Microsoft Endpoint Manager Error deploying Preference File to Supervised MacBook","<p>When deploying a preference file (&quot;.plist&quot;) through MS Endpoint Manager (&quot;Intune&quot;) to a supervised MacBook Pro I get a an error:</p>
<ul>
<li>Setting name: ConfigurationXmlPcl</li>
<li>Error codes: -2016341103 and 0x87d11391</li>
</ul>
<p>This is the preference file I am using:</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;!DOCTYPE plist PUBLIC &quot;-//Apple//DTD PLIST 1.0//EN&quot; 
&quot;http://www.apple.com/DTDs/PropertyList-1.0.dtd&quot;&gt;
&lt;plist version=&quot;1.0&quot;&gt;
    &lt;dict&gt;
        &lt;key&gt;EnableMediaRouter&lt;/key&gt;
        &lt;false /&gt;
        &lt;key&gt;HomepageLocation&lt;/key&gt;
        &lt;string&gt;https://somesubdomain.sharepoint.com&lt;/string&gt;
        &lt;key&gt;NewTabPageSearchBox&lt;/key&gt;
        &lt;string&gt;redirect&lt;/string&gt;
        &lt;key&gt;PasswordManagerEnabled&lt;/key&gt;
        &lt;false /&gt;
        &lt;key&gt;PasswordProtectionChangePasswordURL&lt;/key&gt;
        &lt;string&gt;https://passwordreset.somedomain.com&lt;/string&gt;
    &lt;/dict&gt;
&lt;/plist&gt;
</code></pre>
<p>Tried to search for the setting and the error codes, but could not find anything related to Intune.</p>
<p>Any push in the right direction is much appreciated.</p>
","<macos><mdm><intune>","2022-03-23 14:08:07","1674","0","2","73251834","<p>How have you managed to define into your preference file the &quot;PackageRecommand&quot; settings with all the others.</p>
<p>I know that MS says to no include any dict bracket but i do not see any other ways to get all settings, and I followed the Zoom article.</p>
<p>For example, my preference file is looking as below</p>
<pre><code>&lt;dict&gt;
    &lt;key&gt;nogoogle&lt;/key&gt;
    &lt;true/&gt;
    &lt;key&gt;nofacebook&lt;/key&gt;
    &lt;true/&gt;
    &lt;key&gt;NoSSO&lt;/key&gt;
    &lt;true/&gt;
    &lt;key&gt;KeepSignedIn&lt;/key&gt;
    &lt;true/&gt;
    &lt;key&gt;zAutoUpdate&lt;/key&gt;
    &lt;true/&gt;
    &lt;key&gt;EnableSilentAutoUpdate&lt;/key&gt;
    &lt;true/&gt;
    &lt;key&gt;AlwaysCheckLatestVersion&lt;/key&gt;
    &lt;true/&gt;
    &lt;key&gt;disableDaemonInstall&lt;/key&gt;
    &lt;false/&gt;
    &lt;key&gt;Login_Domain&lt;/key&gt;
        &lt;string&gt;CorrectDomainName&lt;/string&gt;
    &lt;key&gt;EnableMirrorEffect&lt;/key&gt;
        &lt;true/&gt;     
    &lt;key&gt;PackageRecommend&lt;/key&gt;
        &lt;dict&gt;
            &lt;key&gt;ZDisableVideo&lt;/key&gt;
            &lt;false/&gt;
            &lt;key&gt;DisableComputerAudio&lt;/key&gt;
            &lt;false/&gt;
            &lt;key&gt;MuteWhenLockScreen&lt;/key&gt;
            &lt;true/&gt;         
            &lt;key&gt;ZAutoJoinVoip&lt;/key&gt;
            &lt;false/&gt;
            &lt;key&gt;AudioAutoAdjust&lt;/key&gt;
            &lt;true/&gt;
            &lt;key&gt;ZDualMonitorOn&lt;/key&gt;
            &lt;false/&gt;
            &lt;key&gt;ZAutoFullScreenWhenViewShare&lt;/key&gt;
            &lt;false/&gt;
            &lt;key&gt;ZAutoFitWhenViewShare&lt;/key&gt;
            &lt;true/&gt;
            &lt;key&gt;zDisableAnnotation&lt;/key&gt;
            &lt;false/&gt;
            &lt;key&gt;EnableDoNotDisturbInSharing&lt;/key&gt;
            &lt;true/&gt;
            &lt;key&gt;DisableWhiteboard&lt;/key&gt;
            &lt;false/&gt;            
            &lt;key&gt;EnableShareVideo&lt;/key&gt;
            &lt;true/&gt;         
            &lt;key&gt;EnableShareAudio&lt;/key&gt;
            &lt;true/&gt;         
            &lt;key&gt;FullScreenWhenJoin&lt;/key&gt;
            &lt;false/&gt;            
            &lt;key&gt;AutoHideToolbar&lt;/key&gt;
            &lt;false/&gt;            
            &lt;key&gt;ZUse720PByDefault&lt;/key&gt;
            &lt;false/&gt;            
            &lt;key&gt;zRemoteControllAllApp&lt;/key&gt;
            &lt;true/&gt;
            &lt;key&gt;ConfirmWhenLeave&lt;/key&gt;
            &lt;true/&gt;
            &lt;key&gt;DisableScreenShare&lt;/key&gt;
            &lt;true/&gt;
            &lt;key&gt;SetSuppressBackgroundNoiseLevel&lt;/key&gt;
                &lt;string&gt;0&lt;/string&gt;
        &lt;/dict&gt;
&lt;/dict&gt;
</code></pre>
"
"71585142","Spring cloud data flow UI not showing stream definition for deployed streams","<p>In spring-cloud-data-flow UI, after clicking on deployed stream name, stream definition is not visible on UI. It keeps loading but never get loaded. However, whenever somebody clicks on undeployed stream name, definition gets populated on UI.</p>
<p>I searched on google for the error mentioned in the logs but didn't get any related info. It will be great if someone can guide on this issue.</p>
<p>Spring cloud data flow version : 2.0.1</p>
<p>Logs are as below :</p>
<p><code>2022-03-23 20:23:36.357 ERROR 7380 --- [http-nio-9393-exec-3] o.s.c.d.s.c.RestControllerAdvice         : Caught exception while handling a request</code></p>
<p><code>org.springframework.web.client.HttpClientErrorException$NotFound: 404</code>
<code>at org.springframework.web.client.HttpClientErrorException.create(HttpClientErrorException.java:85)</code>
<code>at org.springframework.web.client.DefaultResponseErrorHandler.handleError(DefaultResponseErrorHandler.java:122)</code>
<code>at org.springframework.web.client.DefaultResponseErrorHandler.handleError(DefaultResponseErrorHandler.java:102)</code>
<code>at org.springframework.cloud.skipper.client.SkipperClientResponseErrorHandler.handleError(SkipperClientResponseErrorHandler.java:78)</code>
<code>at org.springframework.web.client.ResponseErrorHandler.handleError(ResponseErrorHandler.java:63)</code>
<code>at org.springframework.web.client.RestTemplate.handleResponse(RestTemplate.java:778)</code>
<code>at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:736)</code>
<code>at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:691)</code>
<code>at org.springframework.web.client.RestTemplate.exchange(RestTemplate.java:618)</code>
<code>at org.springframework.cloud.skipper.client.DefaultSkipperClient.getLog(DefaultSkipperClient.java:169)</code>
<code>at org.springframework.cloud.dataflow.server.stream.SkipperStreamDeployer.getLog(SkipperStreamDeployer.java:498)</code>
<code>at org.springframework.cloud.dataflow.server.controller.StreamLogsController.getLog(StreamLogsController.java:50)</code>
<code>at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</code>
<code>at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</code>
<code>at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</code>
<code>at java.lang.reflect.Method.invoke(Method.java:498)</code>
<code>at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:190)</code>
<code>at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:138)</code>
<code>at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)</code>
<code>at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:892)</code>
<code>at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:797)</code>
<code>at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)</code>
<code>at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1039)</code>
<code>at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:942)</code>
<code>at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1005)</code>
<code>at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:897)</code>
<code>at javax.servlet.http.HttpServlet.service(HttpServlet.java:634)</code>
<code>at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:882)</code>
<code>at javax.servlet.http.HttpServlet.service(HttpServlet.java:741)</code>
<code>at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231)</code>
<code>at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)</code>
<code>at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:53)</code>
<code>at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)</code>
<code>at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)</code>
<code>at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:103)</code>
<code>at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)</code>
<code>at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)</code>
<code>at org.springframework.boot.actuate.web.trace.servlet.HttpTraceFilter.doFilterInternal(HttpTraceFilter.java:88)</code>
<code>at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:109)</code>
<code>at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)</code>
<code>at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)</code>
<code>at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99)</code>
<code>at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:109)</code>
<code>at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)</code>
<code>at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)</code>
<code>at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:92)</code>
<code>at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:109)</code>
<code>at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)</code>
<code>at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)</code>
<code>at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:93)</code>
<code>at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:109)</code>
<code>at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)</code>
<code>at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)</code>
<code>at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.filterAndRecordMetrics(WebMvcMetricsFilter.java:114)</code>
<code>at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:104)</code>
<code>at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:109)</code>
<code>at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)</code>
<code>at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)</code>
<code>at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:200)</code>
<code>at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:109)</code>
<code>at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)</code>
<code>at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)</code>
<code>at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:202)</code>
<code>at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96)</code>
<code>at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:490)</code>
<code>at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:139)</code>
<code>at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:92)</code>
<code>at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)</code>
<code>at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:343)</code>
<code>at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:408)</code>
<code>at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66)</code>
<code>at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:853)</code>
<code>at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1587)</code>
<code>at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49)</code>
<code>at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</code>
<code>at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</code>
<code>at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)</code>
<code>at java.lang.Thread.run(Thread.java:748)</code></p>
","<spring-cloud-stream><spring-cloud-dataflow>","2022-03-23 09:56:11","120","0","1","71588220","<p>Is the included stacktrace generated when clicking on the deployed or undeployed stream? Does this happen always or is it intermittent? Your best bet may be to open an issue in <a href=""https://spring.io/projects/spring-cloud-dataflow/issues"" rel=""nofollow noreferrer"">https://spring.io/projects/spring-cloud-dataflow/issues</a>.</p>
<p>Do the skipper logs have any details in them about that error?</p>
"
"71490176","Combine columns form multiple csv files using data flow in azure data factory","<p>First of, I know there are similar solutions exists but this problem is somewhat different.</p>
<p>I have a process that produces multiple csv files based on user input 'n' (Where n &gt; 1 and n &lt;100). Means user can generate any number of files.</p>
<p>These files have same columns:</p>
<pre><code>file1 -&gt; Col1 Col2 Col3 Col4 Col5 output
file2 -&gt; Col1 Col2 Col3 Col4 Col5 output
file3 -&gt; Col1 Col2 Col3 Col4 Col5 output
</code></pre>
<p>These files are stored in azure blob with some datapath.</p>
<p>I want to read all the files and produce a result file like this:</p>
<pre><code>Col1 Col2 Col3 Col4 Col5 output1 output2 output3
</code></pre>
<p>Is there any way of doing this dynamically. I.e without creating multiple sources in data flow and joining them because the files generated depends on the user and I cannot hardcode it.</p>
","<azure><azure-data-factory>","2022-03-15 23:32:29","484","0","1","71500265","<p>There are multiple steps to be followed in this solution process
First we need to add the filePath as a column
Next rank the data based in the filePath
Implement pivot operation on the table.
The implementation is based on three major steps.</p>
<ol>
<li>Source of the dataset.(list of csv files)</li>
<li>Rank – Ranking rows on column</li>
<li>Pivoting – Pivots row values into columns and groups columns and finally aggregate the data</li>
</ol>
"
"71483791","Ios Custom apps and Mdm","<p>I have a custom app on the app store.  I will assign it to a clients apple business manager account.  Does that client have the ability to be able to send configuration changes (for instance if they want to change a url that the app is connecting to) through their existing mdm to my custom app? Would there be any other setup I would have to do in my custom app besides reading the IOS userdefaults that the mdm server sets? Thanks in advance.</p>
","<ios><mdm><apple-business-manager>","2022-03-15 14:17:18","70","0","1","71487286","<p>If the client's MDM supports <a href=""https://www.appconfig.org/ios/"" rel=""nofollow noreferrer"">AppConfig</a> then it can supply an XML file that contains specific settings. The contents of this XML file are merged with <code>UserDefaults</code> for your app on the device and your app can access them at runtime.</p>
<p>You need to create the logic in your app so that it uses these values when present and standard values if they are not.</p>
"
"71470709","Springcloud data flow stream with http-request post","<p>I'm trying to deploy a springcloud data flow stream like this:</p>
<pre><code>time | http-request --http-method-expression='POST' --url-expression='http://mydomain.fake/myservice?myparams=myvalues'  | log
</code></pre>
<p>The stream deploy without errors, but when I check the http-request log I can see a exception. Obviously, the stream not works</p>
<blockquote>
<p>Property or field 'POST' cannot be found on object of
type 'org.springframework.messaging.support.GenericMessage'</p>
</blockquote>
<p>I checked the <a href=""https://github.com/spring-cloud/stream-applications/blob/main/functions/function/http-request-function/src/main/java/org/springframework/cloud/fn/http/request/HttpRequestFunctionProperties.java"" rel=""nofollow noreferrer"">HttpRequestFunctionProperties</a> class from github, but I didn't get any clue about my fault.</p>
<p>Any help please?</p>
<p>Update 1:</p>
<p>I get it. It works with <code>--http-method-expression=new String('POST')</code> BUT only when the stream is <strong>updated</strong> . I cant <strong>deploy</strong> the stream directly with this param ...</p>
<p>Thanks !!!</p>
","<spring-cloud-stream><spring-cloud-dataflow>","2022-03-14 15:59:21","132","0","1","71483772","<p>The error indicates that the SpEL parser is interpreting <code>POST</code> as a field on the Message (the default evaluation context). The expression <code>'POST'</code> is not being parsed correctly as a literal string. This is most likely due to some escaping problem. Try what Gary suggests.</p>
"
"71465577","SpelParseException in Springcloud data flow","<p>I'm trying to deploy a springcloud data flow stream like this:</p>
<pre><code>time | http-request --url-expression='http://mydomain.fake:1234/myservice?myparams=myvalues' | log
</code></pre>
<p>Versions used:</p>
<ul>
<li>dataflow=2.9.2</li>
<li>skipper=2.8.2</li>
<li>http-request= <a href=""https://maven://org.springframework.cloud.stream.app:http-request-processor-kafka:3.2.0"" rel=""nofollow noreferrer"">3.2.0</a></li>
</ul>
<p>But I have some troubles with the <code>url-expresion</code> param format. The stream partialy deploy, with exceptions in the <code>http-request</code> step.</p>
<pre><code>&gt;   Property: http.request.url-expression
&gt;     Value: http://mydomain.fake:1234/myservice?myparams=myvalues
&gt;     Origin: System Environment Property &quot;SPRING_APPLICATION_JSON&quot;
&gt;     Reason: failed to convert java.lang.String to org.springframework.expression.Expression (caused by
&gt; org.springframework.expression.spel.SpelParseException: EL1041E: After
&gt; parsing a valid expression, there is still more data in the
&gt; expression: 'colon(:)')
</code></pre>
<p>I'm sure that I'm using a bad format in the stream params, but I can't find the good way to write it.I tried it with <strong>simple quote</strong>,  <strong>double quote</strong> and <strong>no quote</strong>, and the exception is always the same. Any help please ?</p>
<p>Thanks !</p>
","<spring-cloud-stream><spring-cloud-dataflow>","2022-03-14 09:36:32","63","0","1","71468374","<p>It's solved ( I think so ) .
Its possible that the expression takes the colon like a ternary operator ( some like <a href=""https://stackoverflow.com/questions/25166951/escaping-colon-in-spel-expression"">here</a> )</p>
<p>I replace the expression</p>
<pre><code>--url-expression='http://mydomain.fake:1234/myservice?myparams=myvalues'
</code></pre>
<p>to</p>
<pre><code>--url-expression='http://mydomain.fake'+':1234/myservice?myparams=myvalues'
</code></pre>
<p>and it works. Thanks !</p>
"
"71427718","Springcloud data flow 2.9.3 delete the TriggerProperties","<p>REcently upgraded my springcloud data flow version and I cant configure the trigger sources.</p>
<p>2.1.0 version
<a href=""https://i.stack.imgur.com/hWpAi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hWpAi.png"" alt=""enter image description here"" /></a></p>
<p>I cant configure it now... any help please ?</p>
<p>Thanks!!</p>
","<stream><spring-cloud-dataflow>","2022-03-10 16:39:37","29","0","1","71430128","<p>The application properties were removed in favor of common spring integration properties. We will look into adding these to the property dialog. Meanwhile,  you should be able to manually type them into the stream definition or set as deployment properties. <a href=""https://github.com/spring-projects/spring-boot/blob/main/spring-boot-project/spring-boot-autoconfigure/src/main/java/org/springframework/boot/autoconfigure/integration/IntegrationProperties.java#L325"" rel=""nofollow noreferrer"">https://github.com/spring-projects/spring-boot/blob/main/spring-boot-project/spring-boot-autoconfigure/src/main/java/org/springframework/boot/autoconfigure/integration/IntegrationProperties.java#L325</a>  e.g., <code>--spring.integration.poller.fixedDelay=2000</code></p>
"
"71326961","Connection with On-premises PostgreSQL DB from Azure Data Factory specifically in Mapping data flows","<p>By using Pay As You Go in Azure, Will I be able to connect to PostgreSQL DB in mapping data flow which is implemented inside azure data factory.</p>
<p>I want to connect to an On-premises postgreSQL DB at source and then transform the data in mapping data flow, once done then again send the transform data back into On-premises PostgreSQL DB. If you see here, the options for PostgreSQL are not active. So does that mean that I need Upgrade to PAY AS YOU GO Subscription.</p>
<p><a href=""https://i.stack.imgur.com/Gpqmp.png"" rel=""nofollow noreferrer"">Image shows that PostgreSQL cannot be leveraged in ADF</a></p>
","<azure><azure-data-factory>","2022-03-02 17:39:42","222","-1","1","71373434","<p>Hey <a href=""https://stackoverflow.com/users/17286801/pushkar-wani"" title=""1 reputation"">Pushkar Wani</a> this seems to be an issue with your Subscription type. Tested the scenario in a paid subscription and I am able to visualize the Databases (not grayed out).</p>
<p><a href=""https://i.stack.imgur.com/x6rae.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x6rae.png"" alt=""enter image description here"" /></a></p>
<p>So,</p>
<ol>
<li>you can try <a href=""https://learn.microsoft.com/en-us/azure/cost-management-billing/manage/upgrade-azure-subscription"" rel=""nofollow noreferrer"">upgrading</a> your subscription to paid version or</li>
<li>As rightly suggested by <a href=""https://stackoverflow.com/users/272109/david-makogon"" title=""66,655 reputation"">David Makogon</a>, reach out to support teams via an <a href=""https://learn.microsoft.com/en-us/azure/azure-portal/supportability/how-to-create-azure-support-request"" rel=""nofollow noreferrer"">Azure Support ticket</a></li>
</ol>
"
"71255482","How to handle seasonal items in NetSuite","<p>My organization sells outdoor equipment (tents, sleeping bags, etc.) and is hitting a roadblock with how to handle seasonal items in NetSuite. We are in the process of standing up Centric as a PLM system for our Product team, which has a product hierarchy starting with the season (for example - you start with 2022, select your product category, then select your product). In NetSuite, all of our products are currently set up as non-matrix inventory items, and we have fields appended with &quot;Next Season&quot; or &quot;Season After Next&quot; to attempt to have all the relevant data on the item record. We are running into a multitude of issues as our business continues to grow, and are working to redefine our item structure.</p>
<p>The tricky bits - our Sales department would like to have products that are &quot;the same&quot; year to year share a SKU so that our retailers can track them as the same inventory. However, these products are not the same year to year from a manufacturing or costing standpoint (we may shift factories, or have material components pricing change which changes our landed cost). From my research, there is no good way around this. We investigated matrix items as a possible solution, but they don't quite fit the need.</p>
<p>Has anyone come across a reasonable solution for something like this? We need a way to either have seasonal (essentially time-series) item data on one item record, or to have multiple item records (one for each season) that share a SKU and can be rolled into reports and inventory calculations as if they were one item.</p>
<p>Any help or ideas here would be greatly appreciated! Cheers!</p>
","<netsuite><erp><master-data-management>","2022-02-24 16:51:22","168","0","2","71270526","<p>An approach that could work is to use Assembly Items.</p>
<p>The things that can make this work:</p>
<ul>
<li>you can receive assemblies into inventory as though they were inventory items.</li>
<li>you can have an assembly that has only one component</li>
<li>you can create and sell multiple revisions of an assembly simultaneously.</li>
</ul>
<p>So you define an assembly with sku X
It has a single sku A from manufacturer B Even initially you might give it a revision of Spring2022</p>
<p>sku A would be a inventory item but you might never have any in stock.</p>
<p>The next bit here is vague because it depends on your PO management process and what automations you have there but you either:</p>
<ul>
<li>order sku A; receive MN of them and have a script that autobuilds N sku X rev or</li>
<li>N sku X rev Spring2022 from vendor B and receive those into inventory.</li>
</ul>
<p>next season sku X gets a new revision, Summer2022, where sku A is retired and new component sku B is added. You can still fulfill rev Spring 2022 until you are out and then start fufilling rev Summer 2022.</p>
<p>Your costs roll up from you components; your skus stay stable. You have an easy path to clearing old inventory.</p>
<p>In order to track the quantities you have of each revision you would probably need to also set these up as lot numbered assemblies. You could tie the revision to a lot number and automations could help minimize the number of clicks needed to coordinate.</p>
<p>One recommendation is to do whatever you do in a sandbox account. There can be a lot of details to these changes and you might find your self tearing this all apart a couple of times until you are tracking costs the way you want to.</p>
"
"71255482","How to handle seasonal items in NetSuite","<p>My organization sells outdoor equipment (tents, sleeping bags, etc.) and is hitting a roadblock with how to handle seasonal items in NetSuite. We are in the process of standing up Centric as a PLM system for our Product team, which has a product hierarchy starting with the season (for example - you start with 2022, select your product category, then select your product). In NetSuite, all of our products are currently set up as non-matrix inventory items, and we have fields appended with &quot;Next Season&quot; or &quot;Season After Next&quot; to attempt to have all the relevant data on the item record. We are running into a multitude of issues as our business continues to grow, and are working to redefine our item structure.</p>
<p>The tricky bits - our Sales department would like to have products that are &quot;the same&quot; year to year share a SKU so that our retailers can track them as the same inventory. However, these products are not the same year to year from a manufacturing or costing standpoint (we may shift factories, or have material components pricing change which changes our landed cost). From my research, there is no good way around this. We investigated matrix items as a possible solution, but they don't quite fit the need.</p>
<p>Has anyone come across a reasonable solution for something like this? We need a way to either have seasonal (essentially time-series) item data on one item record, or to have multiple item records (one for each season) that share a SKU and can be rolled into reports and inventory calculations as if they were one item.</p>
<p>Any help or ideas here would be greatly appreciated! Cheers!</p>
","<netsuite><erp><master-data-management>","2022-02-24 16:51:22","168","0","2","71300728","<p>Another approach that could work would be to set your items up to be sourced from multiple vendors. The vendor's purchasing details are then available as sublists on the item and you can place POs for each using each vendor's name for the item. Your main sku still stays stable and you could track per vendor stock levels using lot numbers similar to the assembly idea.</p>
<p>This is a simpler approach but with assembly revisions you have the revision drop-down that could be used to automatically set the batch number. Skipping having to open the Inventory detail dialog would be more work to automate if that becomes important.</p>
<p>To turn this on:</p>
<ul>
<li>go to Setup -&gt; Company -&gt; Enable Features then</li>
<li>select Items and Inventory tab</li>
<li>check Multiple Vendors</li>
</ul>
"
"71247805","Perform data mapping in GCP","<p>I have data coming from multiple hotels. These hotels are not using the same naming convention for storing the order information. I have a predefined dataset created in the bigquery(called hotel_order). I
want to map the data coming from different hotels to the single dataset in GCP, so it is easier for me to do comparisons in the bigquery.</p>
<p>If the column name(from hotel1) matches the bigquery dataset columnname, then the bigquery should load the data in the column, if the columnnames (from hotel orders data and dataset in bigquery) don't match, then column in the bigquery should have the null value. How do I do implement this in GCP? Problem of mapping in the GCP?</p>
","<google-cloud-platform><google-bigquery>","2022-02-24 06:07:39","79","0","1","71249854","<p>If you want to join tables together, and show a null value when a match doesn't exist, then you can do so using 'left join'.</p>
<p><strong>Rough example</strong>
<code>from hotel.orders as main left join hotel_number_one as Hotel_One on main.order_information = Hotel_One.order_information</code></p>
<p>Difficult to give a more detailed answer without more details or a working example using dbfiddle.</p>
"
"71230734","Embed Spring data flow server and skipper server with my existing spring boot based micro service","<p>I am new to SCDF. I would like to embed SCDF server and skipper server to a single existing micro services rather than having dedicate executable jars.
Is it possible and do we have sample example?</p>
","<spring-cloud-dataflow>","2022-02-23 02:38:07","184","0","1","71240493","<p>Technically you can run Dataflow embedded in another app but you can not run both Dataflow and Skipper together in the same app.</p>
<p>⚠️ <strong>Disclaimer:</strong> This mode is not recommended nor officially supported. However, I will share how to theoretically embed the Dataflow server in an existing app.</p>
<ol>
<li><p>Add the SCDF sever starter dependency. In maven it would be the following:</p>
<pre><code>&lt;dependency&gt;
  &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
  &lt;artifactId&gt;spring-cloud-starter-dataflow-server&lt;/artifactId&gt;
  &lt;version&gt;2.9.2&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
  &lt;groupId&gt;com.h2database&lt;/groupId&gt;
  &lt;artifactId&gt;h2&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>
<p>I added H2 in order to avoid dealing w/ the database. You will also need to be sure you have a schema available (something the install normally handles for you).</p>
</li>
<li><p>Add the <code>@EnabledDataFlowServer</code> to your SBA (note the auto-config exclusions - taken from <a href=""https://github.com/spring-cloud/spring-cloud-dataflow/blob/b154b318fdda8b7d22bd67a019c48333f0921223/spring-cloud-dataflow-server/src/main/java/org/springframework/cloud/dataflow/server/single/DataFlowServerApplication.java#L38"" rel=""nofollow noreferrer"">here</a>).</p>
</li>
</ol>
<p>At this point the app should start successfully.</p>
<p>If you continue the exercise and attempt to add Skipper server, you will see how it breaks.</p>
<ol start=""3"">
<li>Add in the Skipper server starter dependency:
<pre><code>&lt;dependency&gt;
   &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
   &lt;artifactId&gt;spring-cloud-starter-skipper-server&lt;/artifactId&gt;
   &lt;version&gt;2.8.2&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
</li>
<li>Add the <code>@EnableSkipperServer</code> to your SBA.</li>
</ol>
<p>On startup you will see the bean name clashes. If you attempt to allow bean overriding you will see the next wave of bean mismatches. As pointed out above, the overall system is not architected to run this way.</p>
<p>Summary: You can run Dataflow embedded in another app but you can not run both Dataflow and Skipper together in the same app.</p>
"
"71223194","How to validate 100's of columns in Azure DataFactory data flow","<p>I have a data flow which needs to validate 200+ columns in Azure Data Factory. My source file is Excel and I am using 'Assert' to validate the columns. When I gave 5 columns to validate in assert, I am able to preview data in debug mode. But when I used all the 200+ columns validations, it just shows 'Fetching Data' and then timeouts. Can someone please help how to achieve this quick.
I have published the flow and tried to execute it on 16 cores Integration runtime. Still it is of no use. I terminated the flow after waiting for 30 minutes. The size of my data set is quite small around 20 rows.</p>
","<azure><azure-data-factory>","2022-02-22 14:35:12","369","1","1","71231566","<p>I found this official MS doc documenting the similar scenario <a href=""https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance-troubleshooting#timeout-or-slow-performance-when-parsing-large-excel-file"" rel=""nofollow noreferrer"">here</a>, see if this helps.</p>
<p>In excel source dataset, use range (for example, A1:G100) + firstRowAsHeader=false, and then it can load data from all Excel files even though the column name and count is different.</p>
<blockquote>
<h3>Timeout or slow performance when parsing large Excel file</h3>
<ul>
<li><p><strong>Symptoms</strong>:</p>
<ul>
<li><p>When you create Excel dataset and import schema from connection/store, preview data, list, or refresh worksheets, you may
hit timeout error if the excel file is large in size.</p>
</li>
<li><p>When you use copy activity to copy data from large Excel file (&gt;= 100 MB) into other data store, you may experience slow performance
or OOM issue.</p>
</li>
</ul>
</li>
<li><p><strong>Cause</strong>:</p>
<ul>
<li><p>For operations like importing schema, previewing data, and listing worksheets on excel dataset, the timeout is 100 s and static.
For large Excel file, these operations may not finish within the
timeout value.</p>
</li>
<li><p>The copy activity reads the whole Excel file into memory then locate the specified worksheet and cells to read data. This behavior
is due to the underlying SDK the service uses.</p>
</li>
</ul>
</li>
<li><p><strong>Resolution</strong>:</p>
<ul>
<li><p>For importing schema, you can generate a smaller sample file, which is a subset of original file, and choose &quot;import schema from
sample file&quot; instead of &quot;import schema from connection/store&quot;.</p>
</li>
<li><p>For listing worksheet, in the worksheet dropdown, you can click &quot;Edit&quot; and input the sheet name/index instead.</p>
</li>
<li><p>To copy large excel file (&gt;100 MB) into other store, you can use Data Flow Excel source which sport streaming read and perform
better.</p>
</li>
</ul>
</li>
</ul>
</blockquote>
"
"71217036","Mapping Synapse data flow with parameterized dynamic source need importing projection dynamically","<p><a href=""https://i.stack.imgur.com/CFvwV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CFvwV.png"" alt=""enter image description here"" /></a></p>
<p>I am trying to build a cloud data warehouse where I have staged the on-prem tables as parquet files in data lake.</p>
<p>I implemented the metadata driven incremental load.</p>
<p>In the above data flow I am trying to implement merge query passing the table name as parameter so that the data flow dynamically locate respective parquet files for full data and incremental data and then go through some ETL steps to implement merge query.</p>
<p>The merge query is working fine. But I found that projection is not correct. As the source files are dynamic, I also want to &quot;import projection&quot; dynamically during the runtime. So that the same data flow can be used to implement merge query for any table.</p>
<p>In the picture, you see it is showing 104 columns (which is a static projection that it imported at the development time). Actually for this table it should be 38 columns.</p>
<p>Can I dynamically (i.e run-time) assign the projection? If so how?
Or anyone has any suggestion regarding this?
Thanking</p>
<p>Muntasir Joarder</p>
","<azure-data-factory><data-warehouse><azure-synapse>","2022-02-22 06:53:05","357","1","1","71252417","<p>Enable Schema drift in your source transformation when the metadata is often changed. This removes or adds columns in the run time.</p>
<p>The source projection displays what has been imported at the run time but it changes based on the source schema at run time.</p>
<p>Refer to this <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-schema-drift#main"" rel=""nofollow noreferrer"">document</a> for more details with examples.</p>
"
"71189895","Spring cloud data flow kubernetes","<p>I have a basic question about deploying Spring tasks/batch jobs on SCDF Kubernetes. Now if I deploy the SCDF on Kubernetes and then schedule a batch job, which Kubernetes cluster is the batch job deployed on? where is the pod created? the same cluster where the SCDF server is running?</p>
","<spring-cloud-dataflow>","2022-02-19 22:54:01","202","0","1","71190283","<p>By default the apps are deployed on the same cluster and namespace as the SCDF server but this is configurable. You can configure any number of target “platforms”. Each platform is essentially a set of deployment properties keyed to a logical name. You can set the platform name as a parameter on which each task is launched. This is described <a href=""https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#configuration-kubernetes-tasks"" rel=""nofollow noreferrer"">here</a></p>
"
"71136638","Pentaho PDI - data integration CSV s3 enclosure bug","<p>For some reason, in Pentaho PDI --- the s3 CSV input --- I'm inputting csv files of course -- they are delimted by commas and some fields contain quotation enclosures &quot;&quot; ... however there are commas within the quotations &quot;test, two, three&quot;. Pentaho correctly ignores these during the Preview, but seems to not register such an enclosure at run time, separating out stuff that shouldn't be separated.</p>
<p>Anyone familiar with this?</p>
<p>I'm trying to think of a work-around.</p>
<p>really, the &quot;false&quot; commas all have spaces after them, but that doesn't provide much help. The reverse would be useful as I can specify a commma-space as a deliminator.</p>
","<csv><amazon-s3><pentaho><pentaho-data-integration>","2022-02-16 05:00:49","201","0","1","71149886","<p>Sounds like that is a bug in the step. I think your best option is to download the file, replace all occurrences of comma-space with something else (like |) and then try again.</p>
<p>You could also try reporting the bug and see if any action is taken - <a href=""http://jira.pentaho.com/browse/PDI"" rel=""nofollow noreferrer"">http://jira.pentaho.com/browse/PDI</a></p>
"
"71093522","How to push data from a on-premises database to tableau crm","<p>We have an on-premises oracle database installed on a server. We have to create some Charts/Dashboards with Tableau CRM on those data on-premises. Note that, tableau CRM is not Tableau Online, it is a Tableau version for the Salesforce ecosystem.</p>
<p>Tableau CRM has APIs, so we can push data to it or can upload CSV programmatically to it.
So, what can be done are,</p>
<ol>
<li>Run a nodeJS app on the on-premise server, pull data from Oracle DB, and then push to Tableau CRM via the TCRM API.</li>
<li>Run a nodeJS app on the on-premise server, pull data from Oracle DB, create CSV, push the CSV via TCRM API</li>
</ol>
<p>I have tested with the 2nd option and it is working fine.</p>
<p>But, you all know, it is not efficient. Because I have to run a cronJob and schedule the process multiple times in a day. I have to query the full table all the time.</p>
<p>I am looking for a better approach. Any other tools/technology you know to have a smooth sync process?</p>
<p>Thanks</p>
","<database><tableau-api><data-synchronization><oracle-cdc>","2022-02-12 16:08:08","348","0","1","71397637","<p>The second method you described in the questions is a good solution. However, you can optimize it a bit.</p>
<blockquote>
<pre><code>I have to query the full table all the time.
</code></pre>
</blockquote>
<p>This is can be avoided. If you take a look at the <a href=""https://developer.salesforce.com/docs/atlas.en-us.bi_dev_guide_ext_data.meta/bi_dev_guide_ext_data/bi_ext_data_object_externaldata.htm"" rel=""nofollow noreferrer"">documentation</a> of SObject <code>InsightsExternalData</code> you can see that it has a field by name <code>Operation</code> which takes one of these values <code>Append, Delete, Overwrite, Upsert</code>
what you will have to do is when you push data to Tableau CRM you can use the <code>Append</code> operator and push the records that don't exist in TCRM. That way you only query the delta records from your database. This reduces the size of the CSV you will have to push and since the size is less it takes less time to get uploaded into TCRM.</p>
<p>However, to implement this solution you need two things on the database side.</p>
<ol>
<li>A unique identifier that uniquely identifies every record in the database</li>
<li>A DateTime field</li>
</ol>
<p>Once you have these two, you have to write a query that sorts all the records in ascending order of the DateTime field and take only the files that fall below the last UniqueId you pushed into TCRM. That way your result set only contains delta records that you don't have on TCRM. After that you can use the same pipeline you built to push data.</p>
"
"71090933","How to disable auto detect datatype in azure data flow","<p>I was working on a XML file in ADF and there is a field that is having 42 integer digits. I want it to be reflected as it is in azure data flow so that I could perform some transformations
There is a checkbox &quot;Detect Data Type&quot; in copy data activity which when unchecked provides the exact value.
but when using the dataflow, it is not giving any option to not to perform auto detect data type .
kindly suggest what need to be done</p>
<pre><code>    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
   &lt;start&gt;
       &lt;name&gt;ABC&lt;/name&gt;
       &lt;id&gt;123456789098765432123456789098765432123456&lt;/id&gt;
   &lt;/start&gt;
</code></pre>
","<xml><azure><azure-data-factory>","2022-02-12 10:13:55","268","0","1","71091380","<p><strong>Import projection</strong> from the <strong>Projection tab</strong> is used to Import schema and <strong>detect data types</strong> from the source. If you just don't click it or use this, your data columns will be of type string.</p>
<p>Here is an example:</p>
<p><strong>Source XML:</strong></p>
<p><a href=""https://i.stack.imgur.com/pF3za.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pF3za.png"" alt=""enter image description here"" /></a></p>
<p><strong>Import projections:</strong></p>
<blockquote>
<p>You can see the column <code>id</code> is inferred as type <code>Double</code></p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/DMuJ0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DMuJ0.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/skeoz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/skeoz.png"" alt=""enter image description here"" /></a></p>
<p><strong>While if you just don't use Import projection option, and <em>Reset Schema</em></strong></p>
<blockquote>
<p>you get the column <code>id</code> as type <code>String</code></p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/7dN0u.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7dN0u.gif"" alt=""enter image description here"" /></a></p>
<p>For further details, refer official MS doc on <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-source#projection"" rel=""nofollow noreferrer"">ADF Dataflow Projections</a></p>
"
"71085818","Access Intune configuration settings in a React Native application?","<p>My company is doing Mobile Device Management with Microsoft Intune. We've successfully deployed an internal iOS app (using the Apple Developer Enterprise Program).</p>
<p>With Intunes' configuration settings we're trying to make each user's individual email available to the mobile app.
<a href=""https://learn.microsoft.com/en-us/mem/intune/apps/app-configuration-policies-use-ios"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/mem/intune/apps/app-configuration-policies-use-ios</a></p>
<p>How do you normally access these types of settings in an app? I found this library but I'd need to eject from Expo which is not ideal for me:
<a href=""https://github.com/robinpowered/react-native-mdm"" rel=""nofollow noreferrer"">https://github.com/robinpowered/react-native-mdm</a></p>
","<javascript><react-native><expo><mdm><intune>","2022-02-11 19:49:10","501","3","2","71104490","<p>As the MDM has native dependencies, You'll not be able to make it with Expo. Expo projects are written only in JavaScript and don't support packages that contain Objective-C or Java (Native code/dependencies).</p>
<p>Expo provides an advanced SDK called <code>ExpoKit</code> for when you absolutely need to use custom native code. However, there are some cases where developers need native capabilities outside of what Expo offers directly. The most common situation is when a project requires a specific Native Module (like MDM) that is not supported by <code>React Native Core</code> or the <code>Expo SDK</code>. You'll have to detach the Expo project to create Xcode and Android Studio projects that contain ExpoKit. This step will generate <code>android</code> and <code>ios</code> project directories. Then you would add custom Objective-C or Java the same way as with any other Xcode or Android Studio project.</p>
<p>The Expo docs warn about some of the downsides of writing custom native code and discourage most of our developers from taking this route, as Expo's motive, almost everything you need to do is better accomplished in a cross-platform way with JS. Writing in JS enables you to best take advantage of code aster deployment and benefit from ongoing updates and support from Expo. You should only do this if you have a particular demand from native code which Expo won’t do a good job supporting, such as (for example) specialized CPU-intensive video processing that must happen locally on the device, Custom native libraries.</p>
<p>Here are only two options to support the MDM, either eject the project or create <code>react-native-cli</code> project and migrate your project into newly created one.</p>
"
"71085818","Access Intune configuration settings in a React Native application?","<p>My company is doing Mobile Device Management with Microsoft Intune. We've successfully deployed an internal iOS app (using the Apple Developer Enterprise Program).</p>
<p>With Intunes' configuration settings we're trying to make each user's individual email available to the mobile app.
<a href=""https://learn.microsoft.com/en-us/mem/intune/apps/app-configuration-policies-use-ios"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/mem/intune/apps/app-configuration-policies-use-ios</a></p>
<p>How do you normally access these types of settings in an app? I found this library but I'd need to eject from Expo which is not ideal for me:
<a href=""https://github.com/robinpowered/react-native-mdm"" rel=""nofollow noreferrer"">https://github.com/robinpowered/react-native-mdm</a></p>
","<javascript><react-native><expo><mdm><intune>","2022-02-11 19:49:10","501","3","2","71107697","<p>You can add expo support into <a href=""https://github.com/robinpowered/react-native-mdm"" rel=""nofollow noreferrer"">react-native-mdm</a> by fork <a href=""https://github.com/robinpowered/react-native-mdm"" rel=""nofollow noreferrer"">react-native-mdm</a> and use <a href=""https://docs.expo.dev/guides/config-plugins/"" rel=""nofollow noreferrer"">Config Plugins</a></p>
<p>here is the PR for adding expo support into the native package <a href=""https://github.com/Shobbak/react-native-compressor/pull/62"" rel=""nofollow noreferrer"">https://github.com/Shobbak/react-native-compressor/pull/62</a></p>
<p>After adding support you just have to do</p>
<h3>Managed Expo</h3>
<pre><code>yarn install react-native-mdm_from_your_fork
</code></pre>
<p>Add the <a href=""https://github.com/robinpowered/react-native-mdm"" rel=""nofollow noreferrer"">react-native-mdm</a> plugin to your Expo config (<code>app.json</code>, <code>app.config.json</code> or <code>app.config.js</code>):</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;name&quot;: &quot;my app&quot;,
  &quot;plugins&quot;: [&quot;react-native-mdm&quot;]
}
</code></pre>
<p>Finally, compile the mods:</p>
<pre><code>expo prebuild
</code></pre>
<p>To apply the changes, build a new binary with EAS:</p>
<pre><code>eas build
</code></pre>
"
"71076129","Could track mdm ios/android device/user profiles restrictions?","<p>I have a mobile app in ios and android platforms. Our some customers are distributing it in mdm(many even use airwatch). Airwatch has some restriction payloads like: &quot;Allow use of camera&quot;, &quot;Allow screen capture&quot;, &quot;Allow Biometric ID modification&quot; etc.(these are ios/android device/user profiles restrictions) I want to take action in my mobile apps according to these payloads. For example there is an upload option with camera. I want to that if &quot;Allow use of camera&quot; is set as false, I want to not showing camera option.</p>
<p>Could track these payloads form ios and android apps? If yes, how to make it?</p>
<p>Thanks in advance.</p>
","<android><ios><mdm><payload><airwatch>","2022-02-11 06:49:34","69","1","1","71086137","<p>Take a look at android.content.RestrictionsManager.
getApplicationRestrictions() will return a Bundle of restrictions.</p>
<p><a href=""https://developer.android.com/reference/android/content/RestrictionsManager"" rel=""nofollow noreferrer"">RestrictionsManager</a></p>
"
"71049232","Efficient way to integrate Informatica MDM DaaS email validation utility as Cleanse function","<p>We have DaaS email validation utility subscription provided from Informatica which we have added as Cleanse function in MDM HUB.</p>
<p>we are trying to bypass the DaaS utility validation if email from source is coming as blank or null. just to avoid unnecessary use of utility as it is chargeable based on number of hits consume.</p>
<p>We used &quot;Is String Empty&quot; function in mapping and navigated to some hardcoded value and only allowed valid email value to be passed in DaaS cleanse function. But we see the MDM Mapping is still consuming the DaaS cleanse function and our hits are always decrementing for Null or Blank email values also.  We are looking for some efficient way to smartly handle this use-case.</p>
<p>Thanks in advance</p>
","<informatica><master-data-management>","2022-02-09 12:00:46","51","1","1","75802148","<p>You have to use Is String Null as as well as Is String Empty function in the cleanse funtion in MDM. The reason because for MDM, empty and null are different .</p>
"
"71013756","Azure Data Factory data flow timezone conversion problem","<p>I was trying to convert a timezone from CST to UTC inside a data flow. A quick google gave me that CST is in UTC-6:00, so it should add 6 hours.</p>
<p>The following derived column function for some reason only added 5 hours:</p>
<pre><code>toUTC(localDatetime, 'CST')
</code></pre>
<p>Moreover, CurrentUTC('CST') Gave me the correct 6 hours difference, so I had to rewrite this function like this:</p>
<pre><code>LocalDatetime + hours(toInteger((currentUTC('CST') - currentUTC()) / 1000 / 60 / 60)) 
// Minus between timestamps gives difference in milliseconds
</code></pre>
<p>Is this intended behavior? I am worried that when the timezone &quot;shifts&quot; this code will break, so using toUTC would be the best solution, but for some reason it gives incorrect results now</p>
","<azure><etl><azure-data-factory>","2022-02-07 05:05:33","727","0","1","71032289","<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expressions-usage#toutc"" rel=""nofollow noreferrer"">toUTC()</a>  converts a datetime to UTC time zone based on the Daylight-Saving Time effectiveness.</p>
<p>When you are converting from CST to UCT, if your date falls when Daylight saving is effective, then it adds +5 to your date else adds +6 to your date.</p>
<p>Example:</p>
<p><a href=""https://i.stack.imgur.com/SDFQB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SDFQB.png"" alt=""enter image description here"" /></a></p>
<p>Derived column expression: <code>toUTC(toTimestamp(Date_UTC), 'CST')</code></p>
<p><a href=""https://i.stack.imgur.com/lWSGz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lWSGz.png"" alt=""enter image description here"" /></a></p>
"
"71005389","Does using setters still break one way data flow principle?","<p>I'm building a data table component that nests subcomponents about five levels deep. e.g. (simplified) <code>controller</code>, <code>table</code>, <code>thead</code>/ <code>tbody</code>, <code>tr</code>, <code>td</code>.</p>
<p>The top component (<code>controller</code>) contains all the definitions and passes them down.</p>
<p>However, the last component in the chain (<code>td</code>) is performing some calculations on the data and those calculations are then passed to <strong>other</strong> subcomponents in the render stack.</p>
<p>I really hate to break the one-way data flow principle here, but we're talking tens of thousands of calculations, and hundreds of events bubbling up four levels. Bubbling up manually, of course.</p>
<p>I can do it using event bus pattern or a shared state pattern and I think that would comply with all the recommended practices.</p>
<p>However, I have found that simply declaring a setter on the affected prop attributes and placing deep watches on the props involved (in the few strategic places) results in much faster code.</p>
<p>What is the recommended way of solving a data-flow problem like this?</p>
<p><strong>Edit: asked to provide an example. Hopefully this oversimplification will do:</strong></p>
<p>controller is responsible for declaring table columns. It also contains a property called maxWidth containing the measured maximum td width for the column.</p>
<p>As a consequence components <code>table</code>, <code>thead</code>/ <code>tbody</code>, and <code>tr</code> all contain prop called <code>columns</code> while <code>td</code> contains <code>column</code> as in one column from the <code>columns</code> array.</p>
<p>Say <code>td</code> measures rendered width and reports that back to the controller so that it can decide which columns to hide.</p>
<ul>
<li><p><strong>Approach 1: (props down, events up)</strong></p>
<p><code>td</code> emits event, <code>tr</code> captures it and re-emits, etc. until it is captured by <code>controller</code> which then performs the necessary calculations and thus modifies the <code>columns</code>, triggering a re-render of the entire stack.</p>
</li>
<li><p><strong>Approach 2: (vuex)</strong></p>
<p>The columns array is in storage with action defined for adjusting column measurements?</p>
</li>
<li><p><strong>Approach 3: (property setter)</strong></p>
<p>The columns array declares a setter / method for adjusting maxWidth value.</p>
<p>Note how a method would essentially be the same as in vuex approach while a setter makes the assignment syntax somewhat less obvious that we're using an actual method for changing the state.</p>
</li>
</ul>
<p>Also note how I myself fail to see the difference between vuex approach, column setter method or column property setter... Hence the question.</p>
<p>Edit 2: more clarification</p>
<p>If I'm using a setter, linter will immediately complain about <code>vue/no-mutating-props</code> whereas it won't do so when I'm using a method with basically the same code, except that it isn't a property setter, but a &quot;separate&quot; method.</p>
<p>Would that automatically mean that using a method is OK as it's semantically distinct from assigning a value?</p>
","<vue.js>","2022-02-06 08:24:50","92","0","1","71159320","<p>For reference: I have forgone setters since they seemed too obvious and linter complained about property modification.</p>
<p>Instead I have created setter functions, e.g. <code>setMaximumWidth</code> on the object containing the property.</p>
<p>This solution <em><strong>seems</strong></em> like it conforms to best practices, at least as far as I understand them, and doesn't really change the approach.</p>
<p>I'm happy, but will probably get bashed some time for not using something more obvious, whatever that may be.</p>
"
"70966962","Delete null rows in azure data factory data flow transformation","<p>I have created a Data Flow transformation in Azure Data Factory</p>
<p><strong>Source</strong> : Excel file uploaded in Azure Blob
<strong>Destination</strong> : JSON File created in Azure Blob</p>
<ol>
<li>I do not want to insert null rows from source to target</li>
<li>If a particular row does not have a specific value then I want to insert value from earlier row.</li>
</ol>
<p>I have tried to Filter transformation to filter out rows if all columns has null values using below expression , but it is not able to remove null rows</p>
<p>!(isNull(Column_1)) &amp;&amp; !(isNull(Column_2)) &amp;&amp; !(isNull(Column_3)) &amp;&amp; !(isNull(Column_4)) &amp;&amp; !(isNull(Column_5)) &amp;&amp; !(isNull(Column_6))</p>
","<azure-data-factory>","2022-02-03 06:50:52","4110","1","3","70982411","<p>Use the columns() function in your Filter to get the value from all columns per row. Put that inside an array() and then use contains() to search the array for the existence of values (i.e. not null). Notice you have to coalesce the array elements to string:</p>
<p>contains(array(toString(columns())),!isNull(#item))</p>
<p>Put that in your Filter expression, should work.</p>
"
"70966962","Delete null rows in azure data factory data flow transformation","<p>I have created a Data Flow transformation in Azure Data Factory</p>
<p><strong>Source</strong> : Excel file uploaded in Azure Blob
<strong>Destination</strong> : JSON File created in Azure Blob</p>
<ol>
<li>I do not want to insert null rows from source to target</li>
<li>If a particular row does not have a specific value then I want to insert value from earlier row.</li>
</ol>
<p>I have tried to Filter transformation to filter out rows if all columns has null values using below expression , but it is not able to remove null rows</p>
<p>!(isNull(Column_1)) &amp;&amp; !(isNull(Column_2)) &amp;&amp; !(isNull(Column_3)) &amp;&amp; !(isNull(Column_4)) &amp;&amp; !(isNull(Column_5)) &amp;&amp; !(isNull(Column_6))</p>
","<azure-data-factory>","2022-02-03 06:50:52","4110","1","3","71203150","<p>I used filter transform and provided below filter on condition</p>
<p>!(isNull(Prop_0))||!(isNull(Prop_1))||!(isNull(Prop_2))||!(isNull(Prop_3))||!(isNull(Prop_4))||!(isNull(Prop_5))||!(isNull(Prop_6))||!(isNull(Prop_7))||!(isNull(Prop_8))||!(isNull(Prop_9))||!(isNull(Prop_10))</p>
"
"70966962","Delete null rows in azure data factory data flow transformation","<p>I have created a Data Flow transformation in Azure Data Factory</p>
<p><strong>Source</strong> : Excel file uploaded in Azure Blob
<strong>Destination</strong> : JSON File created in Azure Blob</p>
<ol>
<li>I do not want to insert null rows from source to target</li>
<li>If a particular row does not have a specific value then I want to insert value from earlier row.</li>
</ol>
<p>I have tried to Filter transformation to filter out rows if all columns has null values using below expression , but it is not able to remove null rows</p>
<p>!(isNull(Column_1)) &amp;&amp; !(isNull(Column_2)) &amp;&amp; !(isNull(Column_3)) &amp;&amp; !(isNull(Column_4)) &amp;&amp; !(isNull(Column_5)) &amp;&amp; !(isNull(Column_6))</p>
","<azure-data-factory>","2022-02-03 06:50:52","4110","1","3","73067677","<p>Alternatively, you can use <code>equals()</code> to achieve what you want, like:</p>
<pre><code>!equals(toString(Foo),'') &amp;&amp; !equals(toString(Bar),'') 
</code></pre>
"
"70886891","REST source in Azure data flow retrieving an xml file results in corrupt record","<p>In Azure Data Factory, I've got a source in data flow that calls a rest API.  This is the call:  <a href=""https://stats.oecd.org/restsdmx/sdmx.ashx/GetData/MEI_CLI/CSCICP03.USA.M/OECD?startTime=2020-01&amp;format=compact_v2"" rel=""nofollow noreferrer"">https://stats.oecd.org/restsdmx/sdmx.ashx/GetData/MEI_CLI/CSCICP03.USA.M/OECD?startTime=2020-01&amp;format=compact_v2</a></p>
<p>When I do data preview, I get _corrupt_record.  If I get the result of the call through postman and save it in a data lake as an XML file and use that as a source, it works fine.  Any advice?</p>
<p>[setup<a href=""https://i.stack.imgur.com/hpWqV.png"" rel=""nofollow noreferrer"">][1]</a></p>
","<azure-data-factory>","2022-01-27 23:00:56","133","1","1","70895699","<p>You can try using Copy activity to make REST API call and store REST response as XML in datalake and from there use dataflow to pick that data to process further.</p>
"
"70880342","Error when executing data flow job with new service account : Current user cannot act as service account pa-tf-multi@skyuk-uk-pa-tf-dev","<pre><code>Caused by: 
com.google.api.client.googleapis.json.GoogleJsonResponseException: 
403 Forbidden
POST https://dataflow.googleapis.com/v1b3/projects/skyuk-uk-pa-tf-dev/locations/europe-west1/jobs
{
  &quot;code&quot; : 403,
  &quot;errors&quot; : [ {
&quot;domain&quot; : &quot;global&quot;,
&quot;message&quot; : &quot;(b04b0291fadae2a5): Current user cannot act as service account pa-tf-multi@skyuk-uk-pa-tf-dev.iam.gserviceaccount.com. Causes: (b04b0291fadae750): Current user cannot act as service account pa-tf-multi@skyuk-uk-pa-tf-dev.iam.gserviceaccount.com.&quot;,
&quot;reason&quot; : &quot;forbidden&quot;
} ],
&quot;message&quot; : &quot;(b04b0291fadae2a5): Current user cannot act as service account pa-tf-multi@skyuk-uk-pa-tf-dev.iam.gserviceaccount.com. Causes: (b04b0291fadae750): Current user cannot act as service account pa-tf-multi@skyuk-uk-pa-tf-dev.iam.gserviceaccount.com.&quot;,
&quot;status&quot; : &quot;PERMISSION_DENIED&quot;
}
    at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)
    at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)
    at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)
    at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)
    at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)
    at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)
    at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)
    at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)
</code></pre>
","<google-cloud-dataflow>","2022-01-27 14:31:26","398","0","1","70906568","<p>In order to run any Google Cloud Platform process using your own service account, the user starting the process needs to be a <strong>Service Account User</strong> for that service account.</p>
<p>Granting the Service Account User role to a user for a service account can be accomplished by either:</p>
<ol>
<li>Granting the user the Service Account User role at the GCP project level.  This can be done through the <a href=""https://console.cloud.google.com/iam-admin/iam"" rel=""nofollow noreferrer"">IAM settings found in the GCP console</a>.</li>
<li>Granting the user the Service Account User role at the individual service account level.  This too can be done through the console by managing the permissions on the <a href=""https://console.cloud.google.com/iam-admin/serviceaccounts"" rel=""nofollow noreferrer"">service account</a>.</li>
</ol>
<p>Further information can be found in the <a href=""https://cloud.google.com/iam/docs/service-accounts#user-role"" rel=""nofollow noreferrer"">Google Cloud Platform documentation</a>:</p>
<blockquote>
<p>You can grant the Service Account User role (roles/iam.serviceAccountUser) at the project level for all service accounts in the project, or at the service account level.</p>
<ul>
<li><p>Granting the Service Account User role to a user for a project gives the user access to all service accounts in the project, including service accounts that might be created in the future.</p>
</li>
<li><p>Granting the Service Account User role to a user for a specific service account gives a user access to only that service account.</p>
</li>
</ul>
</blockquote>
"
"70868012","Output Many CSV files, and Combing into one without performance impact with Transform data using mapping data flows via Azure Data Factory","<p>I followed the example below, and all is going well.</p>
<p><a href=""https://learn.microsoft.com/en-gb/azure/data-factory/tutorial-data-flow"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-gb/azure/data-factory/tutorial-data-flow</a></p>
<p>Below is about the output files and rows:</p>
<blockquote>
<p>If you followed this tutorial correctly, you should have written 83
rows and 2 columns into your sink folder.</p>
</blockquote>
<p>Below is the result from my example, which is correct having the same number of rows and columns.
<a href=""https://i.stack.imgur.com/70374.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/70374.png"" alt=""enter image description here"" /></a></p>
<p>Below is the output. Please note that the total number of files is 77, not 83, not 1.
<a href=""https://i.stack.imgur.com/L8Z5y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L8Z5y.png"" alt=""enter image description here"" /></a></p>
<p><strong>Question:</strong>: Is it correct to have so many csv files (77 items)?</p>
<p><strong>Question:</strong>: How to combine all files into one file without slowing down the process?</p>
<p>I can create one file by following the link below, which warns of slowing down the process.</p>
<p><a href=""https://stackoverflow.com/questions/69311896/how-to-remove-extra-files-when-sinking-csv-files-to-azure-data-lake-gen2-with-az"">How to remove extra files when sinking CSV files to Azure Data Lake Gen2 with Azure Data Factory data flow?</a></p>
","<azure><azure-data-factory>","2022-01-26 17:49:28","311","1","1","70871010","<p>The number of files generated from the process is dependent upon a number of factors. If you've set the default partitioning in the optimize tab on your sink, that will tell ADF to use Spark's current partitioning mode, which will be based on the number of cores available on the worker nodes. So the number of files will vary based upon how your data is distributed across the workers. You can manually set the number of partitions in the sink's optimize tab. Or, if you wish to name a single output file, you can do that, but it will result in Spark coalescing to a single partition, which is why you see that warning. You may find it takes a little longer to write that file because Spark has to coalesce existing partitions. But that is the nature of a big data distributed processing cluster.</p>
"
"70843320","How to format 2 string columns into date time value using Azure mapping data flows","<p>Can you please help  me with my problem ,</p>
<p>I have 2 columns Tran_date &amp; Tran time like below which are feeding from my source as a string value without special characters to separate month,date &amp;year similarly tran_time  for time . Now how can i transform them as a datetime value like shown in below example using Azure Data Flows.</p>
<p>Input
|Tran_date|Tran_date|
|---------|---------|
|01242022|064033|
|01222022|051003|</p>
<p>Output
|Der_datetime|
|------------|
|01-24-2022 06:40:33|
|01-22-2022 05:10:03|</p>
","<azure-data-factory>","2022-01-25 04:06:56","917","0","1","70847672","<p>You can use <code>derived column</code> transformation in the <strong>data flow</strong> to convert the string to date time.</p>
<ol>
<li>Using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#concat"" rel=""nofollow noreferrer"">concat()</a> function, combine the 2 columns  Tran_date &amp; Tran_time and then convert to datetime using toTimestamp().</li>
</ol>
<blockquote>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#totimestamp"" rel=""nofollow noreferrer"">toTimestamp()</a> function returns the default format as <code>yyyy-MM-dd hh:mm:ss:SSS</code>. If you want the DateTime in a different format, you can provide the output format later in sink transformation under mappings.</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/g8E1I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g8E1I.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>Using <code>select</code> transformation, you can delete unwanted columns which are passed to output/sink. (this is optional alternatively, you can edit the mappings in sink transformation to delete unwanted columns)</li>
</ol>
<p><a href=""https://i.stack.imgur.com/9uHLL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9uHLL.png"" alt=""enter image description here"" /></a></p>
<p><strong>Select transformation preview:</strong></p>
<p><a href=""https://i.stack.imgur.com/9hEKZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9hEKZ.png"" alt=""enter image description here"" /></a></p>
<ol start=""3"">
<li>In <code>sink</code> transformation, under <strong>mapping</strong>, you can provide your required DateTime <strong>output format</strong> as shown below.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/ezkEy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ezkEy.png"" alt=""enter image description here"" /></a></p>
<p><strong>Sink preview:</strong></p>
<p><a href=""https://i.stack.imgur.com/D06fF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D06fF.png"" alt=""enter image description here"" /></a></p>
"
"70687102","Firestore custom document data mapping","<p><a href=""https://i.stack.imgur.com/S6JK4.png"" rel=""nofollow noreferrer"">Firestore Model</a></p>
<p>I have the following data structure in Firestore. I am having a hard time mapping it to my ios app. I am using Swift. Could anyone provide the codable struct or something?</p>
<pre><code>struct CartModel: Codable {

    let cart: [CartItemModel]
   
    enum CodingKeys: String, CodingKey {
        case cart
    }    
}

extension CartModel{
    static var dummyCartData : CartModel {
        .init(cart: [CartItemModel.dummyCartData])
    }
}


struct CartItemModel: Codable {
    let brand: String
    let itemDescription: String
    let image: String
    let selection: [UserSelection]
    let title:String    
   
    enum CodingKeys: String, CodingKey {
        
        case brand
        case itemDescription = &quot;description&quot;
        case image
        case selection
        case title

    }
}


extension CartItemModel{
    static var dummyCartData : CartItemModel {

        .init(brand: &quot;gucci&quot;,
              itemDescription: &quot;adidas&quot;,
              image: &quot;yeezy&quot;,
              selection: [UserSelection.dummyData],
              title: &quot;123&quot;)
    }
}


struct UserSelection: Codable, Equatable{
    var size :String
    var count : Int

    enum CodingKeys: String, CodingKey {
        case size = &quot;size&quot;
        case count
    }

}

extension UserSelection{

    static var dummyData : UserSelection {

        .init(size: &quot;XS11&quot;, count: 20)
    }
}


    docRef.getDocument { (snapshot, error) in
        if error != nil {
            return
        }

        if let snapshot = snapshot, snapshot.exists {
            print(snapshot.data())
        let products = try! snapshot.data(as: CartModel.self)
            print(products)
       
        }
    }
</code></pre>
<p>Firestore returns</p>
<pre><code>Optional([&quot;cart&quot;: &lt;__NSArrayM 0x600002979380&gt;(
{
    brand = Nike;
    description = &quot;Find the Jordan Point Lane at . &quot;;
    image = &quot;https://static.nike.com/a/images/t_default/f105dd55-53d0-4a50-b43a-55bd9231c658/jordan-point-lane-mens-shoes-PPMHdC.png&quot;;
    selection =     {
        count = 1;
        size = &quot;&quot;;
    };
    title = &quot;Jordan Point Lane Men's Shoes&quot;;
}
)
])
</code></pre>
<p>Fatal error: 'try!' expression unexpectedly raised an error: Swift.DecodingError.typeMismatch(Swift.Array, Swift.DecodingError.Context(codingPath: [CodingKeys(stringValue: &quot;cart&quot;, intValue: nil),</p>
<p>_FirestoreKey(stringValue: &quot;Index 0&quot;, intValue: 0), CodingKeys(stringValue: &quot;selection&quot;, intValue: nil)], debugDescription: &quot;Not an array&quot;, underlyingError: nil))</p>
<p>Fatal error: 'try!' expression unexpectedly raised an error: Swift.DecodingError.typeMismatch(Swift.Array, Swift.DecodingError.Context(codingPath: [CodingKeys(stringValue: &quot;cart&quot;, intValue: nil), _FirestoreKey(stringValue: &quot;Index 0&quot;, intValue: 0), CodingKeys(stringValue: &quot;selection&quot;, intValue: nil)], debugDescription: &quot;Not an array&quot;, underlyingError: nil))</p>
","<ios><swift><iphone><google-cloud-firestore><mapping>","2022-01-12 19:08:30","42","0","1","70687213","<p>As the error says, <code>selection</code> is not an array in the Firestore document, but you've defined it as one in your model.</p>
<p>Change <code>let selection: [UserSelection]</code> to <code>let selection: UserSelection</code></p>
"
"70604819","MariaDB Replication stopped over New Year 2021/2022 - all seems fine, but no data flows","<p>I am replication from one MariaDb 10.6 on windows to a MariaDB on Ubuntu 20.04 LTS (Server version: 10.5.13-MariaDB-log MariaDB Server)
It has been working fine for a few weeks, but it has stopped working and the last data was received on new years eve.</p>
<p>On the Master I have the following logs:</p>
<pre><code>MariaDB [stdws]&gt; SHOW BINARY LOGS;
+----------+-----------+
| Log_name | File_size |
+----------+-----------+
| .000098  | 104857723 |
| .000099  | 104857700 |
| .000100  | 104857730 |
| .000101  | 104858096 |
| .000102  | 104857886 |
| .000103  | 104857729 |
| .000104  | 104858105 |
| .000105  | 104857803 |
| .000106  | 104857785 |
| .000107  | 104857906 |
| .000108  | 104857886 |
| .000109  | 104857790 |
| .000110  | 104857826 |
| .000111  | 104858473 |
| .000112  | 104857653 |
| .000113  |   9040227 |
+----------+-----------+
</code></pre>
<p>And the Master Status</p>
<pre><code>MariaDB [stdws]&gt; SHOW MASTER STATUS;
+---------+----------+--------------+------------------+
| File    | Position | Binlog_Do_DB | Binlog_Ignore_DB |
+---------+----------+--------------+------------------+
| .000113 |  9445790 | stdws        |                  |
+---------+----------+--------------+------------------+
</code></pre>
<p>On the client my status is as follows</p>
<pre><code>MariaDB [stdws]&gt; show replica status\G
*************************** 1. row ***************************
                Slave_IO_State: Waiting for master to send event
                   Master_Host: 172.29.3.10
                   Master_User: repl_client
                   Master_Port: 3306
                 Connect_Retry: 10
               Master_Log_File: .000106
           Read_Master_Log_Pos: 84917475
                Relay_Log_File: mysqld-relay-bin.000001
                 Relay_Log_Pos: 4
         Relay_Master_Log_File: .000106
              Slave_IO_Running: Yes
             Slave_SQL_Running: Yes
               Replicate_Do_DB:
           Replicate_Ignore_DB:
            Replicate_Do_Table:
        Replicate_Ignore_Table:
       Replicate_Wild_Do_Table:
   Replicate_Wild_Ignore_Table:
                    Last_Errno: 0
                    Last_Error:
                  Skip_Counter: 0
           Exec_Master_Log_Pos: 84917475
               Relay_Log_Space: 256
               Until_Condition: None
                Until_Log_File:
                 Until_Log_Pos: 0
            Master_SSL_Allowed: No
            Master_SSL_CA_File:
            Master_SSL_CA_Path:
               Master_SSL_Cert:
             Master_SSL_Cipher:
                Master_SSL_Key:
         Seconds_Behind_Master: 0
 Master_SSL_Verify_Server_Cert: No
                 Last_IO_Errno: 0
                 Last_IO_Error:
                Last_SQL_Errno: 0
                Last_SQL_Error:
   Replicate_Ignore_Server_Ids:
              Master_Server_Id: 1
                Master_SSL_Crl:
            Master_SSL_Crlpath:
                    Using_Gtid: No
                   Gtid_IO_Pos:
       Replicate_Do_Domain_Ids:
   Replicate_Ignore_Domain_Ids:
                 Parallel_Mode: optimistic
                     SQL_Delay: 0
           SQL_Remaining_Delay: NULL
       Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates
              Slave_DDL_Groups: 0
Slave_Non_Transactional_Groups: 0
    Slave_Transactional_Groups: 0
1 row in set (0.001 sec)
</code></pre>
<p>I can easily ping between the two machines, but I do not see data from 2022 in the replica.
There is 1 hour of time difference between the two machines. (Iceland vs Denmark).</p>
<pre><code>2021-12-20 10:44:44 3962 [Warning] Aborted connection 3962 to db: 'stdws' user: 'root' host: 'grafana.docker_default' (Got timeout reading communication packets)
2021-12-28 17:59:49 68303 [Warning] Aborted connection 68303 to db: 'stdws' user: 'root' host: 'grafana.docker_default' (Got timeout reading communication packets)
2021-12-31 12:16:39 5 [Note] Slave I/O thread: Failed reading log event, reconnecting to retry, log '.000106' at position 84917475
2021-12-31 12:16:57 5 [ERROR] Slave I/O: error reconnecting to master 'repl_client@172.29.3.10:3306' - retry-time: 10  maximum-retries: 86400  message: Can't connect to server on '172.29.3.10' (113 &quot;Host is unreachable&quot;), Internal MariaDB error code: 2003
2021-12-31 12:23:40 5 [Note] Slave: connected to master 'repl_client@172.29.3.10:3306',replication resumed in log '.000106' at position 84917475
</code></pre>
<p>This was the last message in the logs.</p>
","<mariadb><replication>","2022-01-06 09:12:36","174","0","1","70787292","<p>On the slave, run:</p>
<pre><code>STOP SLAVE;
START SLAVE;
</code></pre>
<p>if that doesn't wake it up, reinitialize the connection point:</p>
<pre><code>STOP SLAVE;
CHANGE MASTER TO MASTER_LOG_FILE='.000106', MASTER_LOG_POS=84917475;
START SLAVE;
</code></pre>
<p>If it still doesn't work, do a full reinitialization of the slave connection:</p>
<pre><code>STOP SLAVE;
RESET SLAVE;
CHANGE MASTER TO MASTER_LOG_FILE='.000106',
    MASTER_LOG_POS=84917475,
    MASTER_USER='repl_client',
    MASTER_PASSWORD='your_password',
    MASTER_HOST='172.29.3.10';
START SLAVE;
</code></pre>
"
"70546223","Azure mapping data flow - how to arrange transformations","<p>I created an <a href=""https://learn.microsoft.com/en-us/azure/synapse-analytics/concepts-data-flow-overview"" rel=""nofollow noreferrer"">Azure Mapping Data Flow</a> with <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-aggregate"" rel=""nofollow noreferrer"">Aggregate transformation</a> that shows in the first row of the diagram below. Then I added a new branch to the Employee source, and then new data source Department. But, as shown in figure 2 below, when I try to join branched data source Employee with the department data source, I was expecting the join transformation to appear in between branched data source Employee and the department. But instead, the join appears next to the first data source Employee on top, and the Aggregate transform along with the Sink data source move down to second row (as shown in figure 3 below). <strong>Question</strong>: Why this is happening and how can I move the Join transformation between the branched data source (Employee) and the Department?</p>
<p><strong>Employee Source with its Branch (below it) and Aggregate and Sink next to it</strong>:</p>
<p><a href=""https://i.stack.imgur.com/2EPRu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2EPRu.png"" alt=""enter image description here"" /></a></p>
<p><strong>Adding Join transformation between Branched Employee source and Department (hoping the Join will appear between Branched Employee source and Department)</strong>:</p>
<p><a href=""https://i.stack.imgur.com/sTirf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sTirf.png"" alt=""enter image description here"" /></a></p>
<p><strong>But, instead following happened</strong>: Why? Note: I can still join the Employee and Department by it will look ugly that left part of the join is on top, and the right part (Department) of the join is below the second row.</p>
<p><a href=""https://i.stack.imgur.com/aH0Xm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aH0Xm.png"" alt=""enter image description here"" /></a></p>
<p><strong>Expected Graph</strong> [taken from another document on Data Factory]: Something like this I wanted</p>
<p><a href=""https://i.stack.imgur.com/MqVOq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MqVOq.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-synapse><azure-data-factory><azure-mapping-data-flow>","2021-12-31 23:31:15","98","0","1","70560003","<p>Mapping Data Flows is not a free-flow diagram. It is a construction graph that will automatically adjust the node positions, connecting lines, and reference nodes for you to optimize space in the graph. Your diagrams above are expressing the exact same semantic to ADF. The only difference is that the UI is moving the Join node to the top stream. If you look at the script behind that data flow, you'll note that ADF sees the original Employee source and the new branch of Employee as the same object. If you'd like to influence the placement of the streams in your graph, open the script behind your flow and you can swap the lines something like this:</p>
<p>Employee aggregate(newfield = sum(1)) ~&gt; Aggregate1
Employee, Department join(surrogatekey == movie,
joinType:'inner',
broadcast: 'auto')~&gt; Join1</p>
<p>... change it to this with Join on top ...</p>
<p>Employee, Department join(surrogatekey == movie,
joinType:'inner',
broadcast: 'auto')~&gt; Join1
Employee aggregate(newfield = sum(1)) ~&gt; Aggregate1</p>
<p>That will swamp the order of the streams in your flow and should give you what you are looking for.</p>
"
"70388604","Azure data factory data flow : expression input is a columnname","<p>I am trying to build a data flow in a data factory , which it will take the variable into to aggregation expression sum , avg ....</p>
<p>however I found it will need to see the actual column name , otherwise it will have error like in the following</p>
<p><a href=""https://i.stack.imgur.com/xA8Yq.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xA8Yq.jpg"" alt=""enter image description here"" /></a></p>
<p>$columnname is a parameter of the dataflow ...</p>
<p>any idea for this ?</p>
<p>add new screen</p>
<p><a href=""https://i.stack.imgur.com/bwk1S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bwk1S.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2021-12-17 04:57:05","799","0","1","70391945","<p>The data flow parameter should be integer type to be used in aggregate functions like <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#sum"" rel=""nofollow noreferrer"">sum()</a> and <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#avg"" rel=""nofollow noreferrer"">avg()</a>. They only accept numeric values/columns.</p>
<p>As shown below, I have created 2 <strong>Data flow</strong> parameters, one <strong>integer</strong> type (column), and one <strong>string</strong> type (column1).</p>
<p><a href=""https://i.stack.imgur.com/kk6hm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kk6hm.png"" alt=""enter image description here"" /></a></p>
<p>When I used these parameters in <code>aggregate</code> transformation, for <strong>sum</strong> and <strong>avg</strong>, it only accepts an integer parameter and errors out when a string parameter is used.</p>
<p><a href=""https://i.stack.imgur.com/nKcvh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nKcvh.png"" alt=""enter image description here"" /></a></p>
<p><strong>Update:</strong></p>
<p>Convert the string variable to integer and use aggregate function SUM/AVG</p>
<p><code>sum(toInteger($columnname))</code></p>
<p><a href=""https://i.stack.imgur.com/daXkg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/daXkg.png"" alt=""enter image description here"" /></a></p>
"
"70377595","Sync sanity database to salesforce","<p>I have some sites made with sanity. So I want to sync my sanity database to salesforce. If I get a better flow or idea or links it will help</p>
","<salesforce><data-synchronization><sanity>","2021-12-16 10:40:38","75","0","1","70389261","<p>you need to create RestResource's in salesforce and then need to hit resource end point to create/update/delete data into salesforce.
It's pretty easy you can learn it from trailhead <a href=""https://trailhead.salesforce.com/en/content/learn/modules/apex_integration_services/apex_integration_webservices"" rel=""nofollow noreferrer"">https://trailhead.salesforce.com/en/content/learn/modules/apex_integration_services/apex_integration_webservices</a></p>
"
"70356681","Preventing invalid JOINs using dimension/fact tables that contain NULL Foreign Keys","<h2>Goal</h2>
<p>We're trying to produce fact and dimension tables that will be easy for <strong>anybody</strong> to use. Many modern BI systems promote exploration and experimentation and we want people of all skill levels to be successful.</p>
<h2>Problem</h2>
<p>Our data has tons of <code>NULL</code>able Foreign Key IDs. This causes unexpected behavior in almost every DB. In <a href=""https://docs.snowflake.com/en/sql-reference/constructs/join.html"" rel=""nofollow noreferrer"">Snowflake</a> (our data warehouse), <code>JOIN</code>ing on <code>NULL</code> results in a <code>CROSS JOIN</code>, which is very very bad.</p>
<h2>Potential Approaches</h2>
<p><strong>Require extra care in every <code>JOIN</code></strong><br />
This can be done two ways but they both are easy to mess up or miss...</p>
<ol>
<li>Use <code>COALESCE(nullable, '--SOMETHING_INVALID--')</code> in <code>JOIN</code>s</li>
<li>Check for <code>NULL</code> in the <code>ON</code> clause of <code>JOIN</code>s</li>
</ol>
<p><strong>Replace <code>NULL</code> ids with a default</strong><br />
If we default to something valid, this will skew reporting in unknown ways. If we default to something invalid, we'd be breaking referential integrity in a second and hard-to-trace way.</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT
  customer_id,
  -- Some default &quot;id&quot; that wont cause a collision
  COALESCE(location_id, '9999999999') AS location_id,
  ...
FROM
  crm.customers
</code></pre>
<p><strong>Add a row of all <code>NULL</code> values to every table</strong><br />
It's a bit heavy-handed but... the <code>JOIN</code>s will always work.</p>
<h2>The Question</h2>
<p>All of these have drawbacks.</p>
<ul>
<li>Is there something better/cleaner?</li>
<li>Does <a href=""https://www.getdbt.com/"" rel=""nofollow noreferrer"">dbt</a> have tools or utilities to help?</li>
</ul>
","<sql><snowflake-cloud-data-platform><data-modeling><dbt><data-integration>","2021-12-14 23:17:17","249","1","1","70356762","<p>I think a good approach would be to create views on top of each of the table.
In those views you can take care of the cleaning of the data such as dealing with NULL values, and then only use the views for JOINing and whatever other downstream reporting / processing you need to do.</p>
<p>Edit following response to my comment,
You could add a record into each of the dimensions (such as Location) for &quot;Unspecified&quot;  and/or &quot;Unknown&quot; with e.g. Location_ID being -1 or 999999999
Then you need to update the tables containing the NULL values (referring to Location in this example) to those values you want (-1 or 999999999).
Then you can set these values to be the default to deal with any new insertion/update on the tables using those keys.
And then you can enforce referential integrity.</p>
"
"70339962","Android activity.stopLockTask() causing SecurityException","<p>I am calling <code>stopLockTask</code> on an <code>Activity</code>, but after a few times of turning lock task mode on and off, the call will eventually cause a <code>Security Exception</code>:</p>
<p><code>SecurityException</code>: Invalid uid, expected 0.</p>
<p>I am not sure why this is happening. The activity is the same one that called <code>startLockTask</code>, is not <code>null</code> when called, but it still throws the error.</p>
<p>Code is pretty simple, so not sure there is much as for snippets that will help. The Fragment is calling <code>getActivity().startLockTask()</code>, <code>getActivity.stopLockTask()</code></p>
<p>Am I missing something obvious here?</p>
","<java><android><mdm>","2021-12-13 19:29:17","372","0","1","70427973","<p>This needs to be setup with <a href=""https://github.com/googlesamples/android-testdpc"" rel=""nofollow noreferrer"">DPC</a> first. <a href=""https://developer.android.com/work/dpc/dedicated-devices/lock-task-mode"" rel=""nofollow noreferrer"">Lock task mode</a> explains this step by step; as it seems, there are only certain <a href=""https://androidenterprisepartners.withgoogle.com/devices"" rel=""nofollow noreferrer"">devices</a> supported. The error message <code>SecurityException: Invalid uid, expected 0</code> generally means, that you're not running this as device owner or admin, which results in &quot;nope&quot;. Since the recent GSuite to Workplace migration, there's also two levels of end-device management plans available - which may eventually limit or enable end-device management possibilities (I don't know). It's more difficult than setting up a single PC for kiosk mode - because not having <code>ROOT_UID</code> <code>0</code> generally translates to: &quot;not owning the device&quot; (contrary to what the sales person may have told you). That's why the <code>SecurityException</code> is being thrown ...unless taking device ownership first.</p>
<p>This <a href=""https://github.com/googlesamples/android-testdpc/blob/master/app/src/main/AndroidManifest.xml"" rel=""nofollow noreferrer""><code>AndroidManifest.xml</code></a> shows the environment you'd need for lock-task mode.<br/>
<strong>But</strong> the current status of this is unsure, as no more custom DPC will be accepted.<br/>
So this may not be wrongful, but stale to outdated information; see part two.</p>
<hr />
<p>2021 Update.<br/>
Just seen that meanwhile the <a href=""https://developers.google.com/android/management"" rel=""nofollow noreferrer"">Android Management API
</a> is being suggested,<br/>were a device policy alike this might be similar to what you're trying to do ...</p>
<p>It's not the app in lock-task mode, but the app being installed in <code>KIOSK</code> mode:</p>
<pre><code>&quot;applications&quot;: [
  {
    &quot;packageName&quot;: &quot;com.example.app&quot;,
    &quot;installType&quot;: &quot;KIOSK&quot;,
    &quot;defaultPermissionPolicy&quot;: &quot;GRANT&quot;
  }
]
</code></pre>
<p>There's also a <a href=""https://github.com/googleapis/google-api-java-client-services/tree/main/clients/google-api-services-androidmanagement/v1"" rel=""nofollow noreferrer""><code>google-api-services-androidmanagement</code></a> library,<br/>which both may generally be the more future-proof alternative.</p>
"
"70310553","Can you make a Azure Data Factory data flow for updating data using a foreign key?","<p>I've tried this a few ways and seem to be blocked.</p>
<p>This is nothing more than a daily ETL process.  What I'm trying to do is to use ADF and pull in a csv as one of my datasets.  With that data I need to update docs in a CosmosDb container, which is the other dataset in this flow.  My data really simple.</p>
<ul>
<li>ForeignId string</li>
<li>Value1 int</li>
<li>Value2 int</li>
<li>Value3 int</li>
</ul>
<p>The Cosmos docs all have these data items and more.  ForeignId is unique in the container and is the partition key.  The docs are a composite dataset that actually have 3 other id fields that would be considered the PK in the system of origin.</p>
<p>When you try and use a data flow UPDATE with this data the validation complains that you have to  map &quot;Id&quot; to use UPDATE.  I have an Id in my document, but it only relates to my collection, not to old, external systems.  I have no choice but to use the ForeignId. I have it flowing using UPSERT but, even though I have the ForeignId mapped between the datasets, I get inserts instead of updates.</p>
<p>Is there something I'm missing or is ADF not set up to sync data based on anything other than the a data item named &quot;id&quot;?  Is there another option ADF aside from the straight-forward approach?  I've read that you can drop updates into the Lookup tasks but that seems like a hack.</p>
","<azure><azure-data-factory>","2021-12-10 21:27:58","139","0","1","70327703","<p>The row ID is needed by CosmosDB to know which row to update. It has nothing to do with ADF.</p>
<p>To make this work in ADF, add an Exists transformation in your data flow to see if the row already exists in your collection. Check using the foreign key column in your incoming source data against the existing collection.</p>
<p>If a row is found with that foreign key, then you can the corresponding ID to your metadata, allowing you to include it in your sink.</p>
"
"70275427","DDD and Microservices - data flow and structure","<p>I'm trying to create a simple blogging platform and at the same time learn more about DDD and Microservices, so I wanted to ask you about two advises in this context:</p>
<ol>
<li>One of the business rules I assumed in my project is that only users in roles <code>Publicist</code> and <code>Administrator</code> are able to create posts, but posts created by <code>Publicist</code> have to be approved by <code>Administrator</code> first before they go public. In my understanding this is a part of a <code>Posts.Domain</code>, so in <code>Post</code> aggregate (and at the same time entity) I encapsulated changing the post's status into methods like <code>SetPublishedStatusBy</code> that take <code>User</code> (requestor) data as parameters and evaluate above rule (+ create domain events). However now I'm having some doubts whether information about requestor is really a part of the <code>Posts.Domain</code>. Maybe requestor should be evaulated in a different place, like <code>Posts.API</code> or some other service and <code>SetPublishedStatus</code> would be then called without parameters after it is already done?</li>
<li>Let's stick to above context. Despite <code>Posts</code> microservice, I'm also developing independent <code>Users</code> microservice responsible for storing the users and providing some tools for <code>Administrator</code> to manage them. What would be then the proper data flow when a user wants to publish a new post? I'd imagine this in a following way:</li>
</ol>
<ul>
<li>Client sends <code>PublishPost</code> command with a post ID to the gateway</li>
<li>Gateway authenticates user from HTTP request (probably done via cookie with JWT)</li>
<li>Gateway sends a <code>PublishPost</code> command to <code>Posts</code> microservice</li>
<li><code>Posts</code> microservice calls <code>Users</code> microservice to get relevant user data from DB</li>
<li><code>Posts</code> microservice retreives post from DB by ID</li>
<li>All business rules are evaluated through the <code>Posts.Domain</code> and status is changed to <code>Public</code></li>
<li><code>Posts</code> microservice updates DB if everything goes fine and notifies Gateway that sends <code>Success</code> HTTP response</li>
</ul>
","<design-patterns><architecture><microservices><domain-driven-design><domain-model>","2021-12-08 12:59:51","234","0","1","70292320","<p>My thoughts ...</p>
<p>For DDD, you're best served by taking guidance from the Ubiquitous Language of the domain when discussed with a domain expert.</p>
<p>The term &quot;SetPublishedStatusBy&quot; probably wouldn't come up in that discussion.</p>
<p>I think the most likely outcome of that discussion would be:</p>
<ul>
<li>An Administrator and <strong>publish</strong> a <em>post</em>.</li>
<li>A Publicist can <strong>submit</strong> a <em>post</em> that an Administrator must <strong>approve</strong> before it is published.</li>
<li>An Administrator can <strong>approve</strong> a <em>submitted post</em> that has been Submitted by a Publicist, which will result in the <em>post</em> being published.</li>
<li>An Administrator can <strong>reject</strong> a <em>submitted post</em>.</li>
</ul>
<p>My Post aggregate would then end up looking something like:</p>
<pre><code>class Post
{
    void Submit()
    {
        this.Status = Submitted;
    }
    void Publish()
    {
        this.Status = Published;
    }
    void Approve()
    {
        if (this.Status != Submitted) throw &quot;This post is not pending approval.&quot;;
        this.Status = Published;
    }
    void Reject()
    {
        if (this.Status != Submitted) throw &quot;This post is not pending approval.&quot;;
        this.Status = Rejected;
    }
}
</code></pre>
<p>When creating the post, the UI would either be calling Publish or Submit in your API, depending on the context.  The API would then check that current user can perform the requested Publish or Submit.</p>
<p>Two other options:</p>
<ol>
<li>Introduce an Aggregate called PostRequest that Publicists have permission to create and only create a Post when that is approved by an Administrator.</li>
<li>If you want the rules to be more dynamic, i.e. a user just hits 'Publish', whether they are a Publicist or an Administrator and then the outcome is either a published post or a submitted post depending on the rules of the day, then you'd want an orchestration / saga / task layer in between your API and the Aggregate which can interact with User service to decide whether the the first call to the Posts service should be a &quot;Submit&quot; or a &quot;Publish&quot;.</li>
</ol>
"
"70266127","terraform : data flow pubsubtopics to bigquery","<p>I want to create pubsub topics to bigquery jobs in terraform. dataflow has this template. I did not find the terraform example. Would you provide any example terraform code ?</p>
<p><a href=""https://i.stack.imgur.com/lEg1a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lEg1a.png"" alt=""enter image description here"" /></a></p>
","<terraform><google-cloud-dataflow><terraform-provider-gcp>","2021-12-07 19:47:00","583","0","1","70282914","<p>This seems to be the example you are looking for: <a href=""https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/dataflow_job"" rel=""nofollow noreferrer"">https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/dataflow_job</a></p>
"
"70264408","azure data flow convert string json array to json array","<p>I'm using azure data flow, and I want to pass a json array inside an at() function like this :
<a href=""https://i.stack.imgur.com/fIZod.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fIZod.png"" alt=""enter image description here"" /></a></p>
<p>the error is :</p>
<blockquote>
<p>At function takes an array or a map for the first parameter.</p>
</blockquote>
<p>urls value is :</p>
<blockquote>
<p>[{&quot;url&quot;:&quot;http://url1.com&quot;},{&quot;url&quot;:&quot;http://url2.com&quot;},{&quot;url&quot;:&quot;http://url3.com&quot;},{&quot;url&quot;:&quot;http://url4.com&quot;}]</p>
</blockquote>
<p>why it consider urls value as a string ?</p>
","<azure-data-factory>","2021-12-07 17:21:35","1573","0","1","70374878","<p>You can convert to an array using the <code>Derived column</code> transformation.</p>
<ol>
<li>In a <code>Derived Column</code>, first <strong>replace</strong> square brackets with empty/blank and with <strong>split</strong> function, split the string based on the delimiter <strong>comma</strong> (,).</li>
</ol>
<p><a href=""https://i.stack.imgur.com/cQsrg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cQsrg.png"" alt=""enter image description here"" /></a></p>
<p><strong>Derived Column preview:</strong></p>
<p><a href=""https://i.stack.imgur.com/AcfnE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AcfnE.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>Now you can use this array in the <strong>at()</strong> function in your expression.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/xY9cV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xY9cV.png"" alt=""enter image description here"" /></a></p>
"
"70242824","Spring cloud data flow http source kafka application request body memory size issue","<p>I'm facing an issue with the HTTP source app in a spring cloud data flow stream.</p>
<pre><code>org.springframework.core.io.buffer.DataBufferLimitException: Exceeded limit on max bytes to buffer : 262144
    at org.springframework.core.io.buffer.LimitedDataBufferList.raiseLimitException(LimitedDataBufferList.java:98) ~[spring-core-5.3.10.jar:5.3.10]
    Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException:
Error has been observed at the following site(s):
    |_ checkpoint ? org.springframework.security.web.server.authorization.AuthorizationWebFilter [DefaultWebFilterChain]
    |_ checkpoint ? org.springframework.security.web.server.authorization.ExceptionTranslationWebFilter [DefaultWebFilterChain]
    |_ checkpoint ? org.springframework.security.web.server.authentication.logout.LogoutWebFilter [DefaultWebFilterChain]
    |_ checkpoint ? org.springframework.security.web.server.savedrequest.ServerRequestCacheWebFilter [DefaultWebFilterChain]
    |_ checkpoint ? org.springframework.security.web.server.context.SecurityContextServerWebExchangeWebFilter [DefaultWebFilterChain]
    |_ checkpoint ? org.springframework.security.web.server.context.ReactorContextWebFilter [DefaultWebFilterChain]
    |_ checkpoint ? org.springframework.security.web.server.header.HttpHeaderWriterWebFilter [DefaultWebFilterChain]
    |_ checkpoint ? org.springframework.security.config.web.server.ServerHttpSecurity$ServerWebExchangeReactorContextWebFilter [DefaultWebFilterChain]
    |_ checkpoint ? org.springframework.security.web.server.WebFilterChainProxy [DefaultWebFilterChain]
    |_ checkpoint ? org.springframework.cloud.sleuth.instrument.web.TraceWebFilter [DefaultWebFilterChain]
    |_ checkpoint ? org.springframework.boot.actuate.metrics.web.reactive.server.MetricsWebFilter [DefaultWebFilterChain]
    |_ checkpoint ? HTTP POST &quot;/&quot; [ExceptionHandlingWebHandler]
</code></pre>
<p>This happens when I try to post an HTTP request with huge body size to the HTTP endpoint.</p>
<p>I have tried setting the following property in the deployment which didn't help me:</p>
<pre><code>app.http.spring.codec.max-in-memory-size: 10MB
</code></pre>
<p>spring cloud starter HTTP Kafka source app version used is 3.1.1</p>
<p>Does anyone have a clue about how to fix this?</p>
<p>This can be reproduced minimally in a standalone manner like below</p>
<ol>
<li>Download jar from: <a href=""https://mvnrepository.com/artifact/org.springframework.cloud.stream.app/http-source-kafka/3.1.1"" rel=""nofollow noreferrer"">https://mvnrepository.com/artifact/org.springframework.cloud.stream.app/http-source-kafka/3.1.1</a></li>
<li>Run a Kafka instance locally on port 9092 and create a topic named &quot;output&quot;.</li>
<li>Run the jar with <code>java -jar http-source-kafka-3.1.1.jar</code></li>
<li>Make an HTTP post request to localhost:8080 with request body size greater than 700KB</li>
</ol>
","<java><spring><spring-cloud-stream><spring-cloud-dataflow>","2021-12-06 08:45:49","218","1","1","70279834","<p>Thank you for raising this!</p>
<p>It turns out that <code>HttpSupplierConfiguration</code> for that HTTP Source application doesn't take into account a <code>ServerCodecConfigurer</code> auto-configured by Spring Boot, including the mentioned <code>spring.codec.max-in-memory-size</code> property.</p>
<p>I have a fix like this: <a href=""https://github.com/spring-cloud/stream-applications/pull/204"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/stream-applications/pull/204</a>, which is going to be merged soon.</p>
<p>Unfortunately there is no easy workaround for the out-of-the-box application.
You may consider to implement your own source application with the custom HTTP Inbound Endpoint.</p>
"
"70236373","Azure data factory: How to get first row from a stream in data flow transformation","<p>How can I get the latest row from a stream in data flow transformation. Below is the sql equivalent query. I checked filter mapping transformation but I did find any relevant function in visual expression builder. I amnew to data factory and I am currently exploring the data flow canvas.</p>
<p><strong>SQL: Select Top 1 * from XYZ table order by timestamp desc;</strong></p>
","<azure><azure-data-factory>","2021-12-05 16:39:43","1067","0","1","70259205","<p>In Source transformation, under source options, you can select the Input as Query and write the SQL query to get the latest record.</p>
<p><a href=""https://i.stack.imgur.com/LOOIJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LOOIJ.png"" alt=""enter image description here"" /></a></p>
"
"70233579","azure data flow I cant pass json string to the sink","<p>I'm using azure data factory to transform data, I have a derived column to process images.</p>
<pre><code>iif(isNull(column1_images),
iif(isNull(column2_images),
iif(isNull(images),'N/A',toString(images))
 ,concat(toString(column12_images),' ','(',column2_type,')')),
 concat(toString(column1_images),' ','(',column1_type,')'))
</code></pre>
<p>when I click on refresh buton I can see the result :
<a href=""https://i.stack.imgur.com/lDXMZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lDXMZ.png"" alt=""enter image description here"" /></a></p>
<p>but when I pass this column to the sink I'M GETTING THIS ERROR :</p>
<blockquote>
<p>Conversion from StringType to ArrayType(StructType(StructField(url,StringType,true)),false) not defined</p>
</blockquote>
<p>can you tell me what is the problem please ?</p>
","<json><azure><azure-data-factory>","2021-12-05 10:55:22","269","0","1","70259833","<p>The error you are getting because there is no Schema designed for the Sink. Hence, you need to use Derived column expression and create a JSON schema to convert the data. Check <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column#building-schemas-using-the-expression-builder"" rel=""nofollow noreferrer"">Building schemas using the expression builder</a>.</p>
<p>You can also check this similar kind of <a href=""https://stackoverflow.com/questions/66687600/how-to-map-a-json-string-into-object-type-in-sink-transformation"">thread</a> for your reference.</p>
"
"70232230","Azure Datafactory: How to implement nested sql query in transformation data flow","<p>[![enter image description here][1]][1]
I have two streams <strong>customer</strong> and <strong>customercontact</strong>. I am new to azure data factory. I just want to know which activity in data flow transformation will achieve the below sql query result.</p>
<p>(SELECT *
FROM customercontact
WHERE customerid IN
(SELECT customerid
FROM customer)
ORDER BY timestamp DESC
LIMIT 1)</p>
<p>I can utilize Exist transformation for inner query but I am need some help on how I can fetch the first row after sorting customer contact data.So , basically I am looking for a way to add limit/Top/Offset clause in dataflow.</p>
","<azure><azure-data-factory>","2021-12-05 07:22:24","271","0","1","70261995","<p>You can achieve transformation for a given query in data flow with different transformation.</p>
<p>For sorting you can use Sort transformation. Here you can select Order Ascending or descending.</p>
<p><a href=""https://i.stack.imgur.com/J44ij.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/J44ij.png"" alt=""enter image description here"" /></a></p>
<p>For top few records you can use Rank transformation.</p>
<p><a href=""https://i.stack.imgur.com/5EdRN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5EdRN.png"" alt=""enter image description here"" /></a></p>
<p>For “IN” clause you can use Exists transformation.</p>
<p><a href=""https://i.stack.imgur.com/kx65R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kx65R.png"" alt=""enter image description here"" /></a></p>
<p>Refer - <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-rank"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-rank</a></p>
<p><a href=""https://i.stack.imgur.com/8L7Nj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8L7Nj.png"" alt=""enter image description here"" /></a></p>
<p>Here is my sample data in SQL as Source</p>
<p><a href=""https://i.stack.imgur.com/svX9j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/svX9j.png"" alt=""enter image description here"" /></a></p>
<p>I have used Rank transformation.</p>
<p><a href=""https://i.stack.imgur.com/J5Kij.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/J5Kij.png"" alt=""enter image description here"" /></a></p>
<p>After rank transformation one more column i.e. RankColumn got added.</p>
<p><a href=""https://i.stack.imgur.com/kv91F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kv91F.png"" alt=""enter image description here"" /></a></p>
<p>Now to select only top 1 record I have used Filter Row Modifier. I used <code>equals(RankColumn,1)</code> expression to select Top 1 record.</p>
<p><a href=""https://i.stack.imgur.com/76wL7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/76wL7.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/HpcXV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HpcXV.png"" alt=""enter image description here"" /></a></p>
<p>Now finally use Sink activity and run pipeline.</p>
"
"70181211","Azure data factory - Data Flow-how to get duration for a data flow","<p>In the <strong>output</strong> of <strong>Copy Data</strong>, there is a <strong>&quot;copyDuration&quot;</strong>.</p>
<p>Is there also a duration part in the output of the <strong>data flow</strong>, which is the amount time spent executing this whole data flow? How can I get the <strong>total seconds</strong> of this data flow activity?</p>
<p>Is this the right way? (@activity('activityName').Duration)</p>
<p>If it' s the right way, how can I transform this duration(hh:mm:ss) (like &quot;00:01:02&quot;) to total seconds(&quot;62&quot;)?</p>
<p>Thanks!!!!</p>
","<azure-data-factory><dataflow>","2021-12-01 08:41:06","266","0","1","70588300","<p>Yes <code>@activity('activityName').Duration</code> will give you duration details of your data flow execution.</p>
<p>Regarding, transforming duration to seconds. Kindly check below link.
<a href=""https://stackoverflow.com/questions/70185485/how-to-transform-hhmmss-to-total-seconds-in-azure-data-factory"">How to transform hh:mm:ss to total seconds in azure data factory</a></p>
"
"70109619","Talend Open Studio : timpala component missing","<p>I am using Talend Open Studio for data integration.
When I create a new connection to an impala database, it works, I can even find the tables, but I can't add this component in the current job, I get this error:</p>
<pre><code>Cannot apply drag and drop operation on this component currently.
</code></pre>
<p>If I import a project using an impala component, it says that this component is missing.
According to the documentation, I should install the timpala component, but I don't have this option.</p>
","<etl><talend><impala><data-integration>","2021-11-25 10:37:58","111","0","1","70157731","<p>If you use Talend open studio for data Integration, the impala components exist only in &quot;Talend open studio for big data&quot; and &quot;Talend data fabric&quot;.</p>
<p>You can see the component compatibility matrix here :
<a href=""https://www.talendforge.org/components/index.php"" rel=""nofollow noreferrer"">https://www.talendforge.org/components/index.php</a></p>
<p>And this is written in the documentation of impala component :
<a href=""https://help.talend.com/r/en-US/7.3/impala/timpalaconnection-standard-properties"" rel=""nofollow noreferrer"">https://help.talend.com/r/en-US/7.3/impala/timpalaconnection-standard-properties</a></p>
"
"70108694","Can you use a data flow sink as a source in the same data flow?","<p>I am trying to load the sales data to the database using Azure Synapse Analytics pipelines, and the strategy is as follows (scenario is made up):</p>
<ol>
<li>Load the students` data to the table Students</li>
<li>Load the students` classes information to the table StudentsClasses. In this data flow I need to join the data with the Students table (obviously, the new data about students must be loaded to Students at this join step)</li>
</ol>
<p>Can I have these two processes in the same data flow with Sink ordering? Or does the sink ordering not define source read ordering? (that is, the source reading and transformations are done in parallel, and only the write is according to the ordering?</p>
<p>Edit: This is an example data flow that I want to implement:
<a href=""https://i.stack.imgur.com/2nqOb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2nqOb.png"" alt=""enter image description here"" /></a></p>
<p>source3 and sink1 are the same table. What I want is to first populate sink1, then use it for source 2 to join with it. Can this be implemented using Sink ordering? Or source3 will be empty regardless of sink ordering?</p>
","<azure-data-factory><pipeline><azure-synapse>","2021-11-25 09:31:50","1504","0","1","70109536","<p>Yes, you can use multiple source and sinks in a single data flow and reference same source over join activity. And order sink write using Custom sink ordering property</p>
<p>I am using Inline dataset but you can use any type</p>
<p>Using <strong>inline dataset</strong> to store the result in <strong>sink1</strong>. In <strong>source3</strong>, use the same <strong>inline dataset</strong> to join with <strong>Source2</strong></p>
<p><a href=""https://i.stack.imgur.com/2oVPT.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2oVPT.gif"" alt=""enter image description here"" /></a></p>
<hr />
<p><a href=""https://i.stack.imgur.com/ayr4O.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ayr4O.png"" alt=""enter image description here"" /></a></p>
<p>Make sure you give the sink order correctly, if you have the wrong order or if it encounters no data while proceeding with transformation, it will publish with no errors however the pipeline run would fail.</p>
<p><a href=""https://i.stack.imgur.com/6TnT2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6TnT2.png"" alt=""enter image description here"" /></a></p>
<p>Refer MS DOC: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-sink#custom-sink-ordering"" rel=""nofollow noreferrer"">Sink ordering</a></p>
"
"70099078","Mapping non-numeric factor to choose higher value between two columns in R","<p>I have a dataframe with two column: PathGroupStage, ClinGroupStage. I want to create a new column, OutputStage, that chooses the higher stage.</p>
<p>Valid value of stage: I, IA, IB, II, IIA, IIB, III, IIIA, IIIB, IIIC ,IV, IVA, IVB, IVC, Unknown.</p>
<ul>
<li>If both stages have values, then use the highest, e.g., IIIB &gt; IIIA &gt; III</li>
<li>If one is missing and the other has value, the use the one with value</li>
<li>If both are missing or unknown, then .= unknown</li>
</ul>
<p>How would I derive the OutputStage variable comparing the non-numeric values from the two columns? I am thinking I need to factor levels but how would I compare the factors between different columns?</p>
<p>Here is the sample dataset:</p>
<pre><code>   PathGroupStage       ClinGroupStage
1              II                 &lt;NA&gt;
2               I                   IA
3             IVB                  IVB
4            IIIA Unknown/Not Reported
5               I                  III
6              II                 &lt;NA&gt;
7            IIIA                  IIB
8              II                   II
9            &lt;NA&gt;                 &lt;NA&gt;
10           IIIB Unknown/Not Reported

 df &lt;- structure(list(PathGroupStage = c(&quot;II&quot;, &quot;I&quot;, &quot;IVB&quot;, &quot;IIIA&quot;, &quot;I&quot;, 
    &quot;II&quot;, &quot;IIIA&quot;, &quot;II&quot;, NA, &quot;IIIB&quot;), ClinGroupStage = c(NA, &quot;IA&quot;, 
    &quot;IVB&quot;, &quot;Unknown/Not Reported&quot;, &quot;III&quot;, NA, &quot;IIB&quot;, &quot;II&quot;, NA, &quot;Unknown/Not Reported&quot;
    )), row.names = c(NA, 10L), class = &quot;data.frame&quot;)
</code></pre>
","<r><dplyr><levels><data-mapping>","2021-11-24 15:51:33","54","2","2","70099237","<p>One option could be:</p>
<pre><code>stages &lt;- c(&quot;Unknown/Not Reported&quot;, &quot;I&quot;, &quot;IA&quot;, &quot;IB&quot;, &quot;II&quot;, &quot;IIA&quot;, &quot;IIB&quot;, &quot;III&quot;, &quot;IIIA&quot;, &quot;IIIB&quot;, &quot;IIIC&quot; ,&quot;IV&quot;, &quot;IVA&quot;, &quot;IVB&quot;, &quot;IVC&quot;)

df %&gt;%
    mutate(across(everything(), ~ factor(., levels = stages, ordered = TRUE)),
           OutputStage = pmax(PathGroupStage, ClinGroupStage, na.rm = TRUE))

   PathGroupStage       ClinGroupStage OutputStage
1              II                 &lt;NA&gt;          II
2               I                   IA          IA
3             IVB                  IVB         IVB
4            IIIA Unknown/Not Reported        IIIA
5               I                  III         III
6              II                 &lt;NA&gt;          II
7            IIIA                  IIB        IIIA
8              II                   II          II
9            &lt;NA&gt;                 &lt;NA&gt;        &lt;NA&gt;
10           IIIB Unknown/Not Reported        IIIB
</code></pre>
"
"70099078","Mapping non-numeric factor to choose higher value between two columns in R","<p>I have a dataframe with two column: PathGroupStage, ClinGroupStage. I want to create a new column, OutputStage, that chooses the higher stage.</p>
<p>Valid value of stage: I, IA, IB, II, IIA, IIB, III, IIIA, IIIB, IIIC ,IV, IVA, IVB, IVC, Unknown.</p>
<ul>
<li>If both stages have values, then use the highest, e.g., IIIB &gt; IIIA &gt; III</li>
<li>If one is missing and the other has value, the use the one with value</li>
<li>If both are missing or unknown, then .= unknown</li>
</ul>
<p>How would I derive the OutputStage variable comparing the non-numeric values from the two columns? I am thinking I need to factor levels but how would I compare the factors between different columns?</p>
<p>Here is the sample dataset:</p>
<pre><code>   PathGroupStage       ClinGroupStage
1              II                 &lt;NA&gt;
2               I                   IA
3             IVB                  IVB
4            IIIA Unknown/Not Reported
5               I                  III
6              II                 &lt;NA&gt;
7            IIIA                  IIB
8              II                   II
9            &lt;NA&gt;                 &lt;NA&gt;
10           IIIB Unknown/Not Reported

 df &lt;- structure(list(PathGroupStage = c(&quot;II&quot;, &quot;I&quot;, &quot;IVB&quot;, &quot;IIIA&quot;, &quot;I&quot;, 
    &quot;II&quot;, &quot;IIIA&quot;, &quot;II&quot;, NA, &quot;IIIB&quot;), ClinGroupStage = c(NA, &quot;IA&quot;, 
    &quot;IVB&quot;, &quot;Unknown/Not Reported&quot;, &quot;III&quot;, NA, &quot;IIB&quot;, &quot;II&quot;, NA, &quot;Unknown/Not Reported&quot;
    )), row.names = c(NA, 10L), class = &quot;data.frame&quot;)
</code></pre>
","<r><dplyr><levels><data-mapping>","2021-11-24 15:51:33","54","2","2","70099392","<pre><code>df &lt;- structure(
    list(
        PathGroupStage = c(&quot;II&quot;, &quot;I&quot;, &quot;IVB&quot;, &quot;IIIA&quot;, &quot;I&quot;, &quot;II&quot;, &quot;IIIA&quot;, &quot;II&quot;, NA, &quot;IIIB&quot;),
        ClinGroupStage = c(NA, &quot;IA&quot;, &quot;IVB&quot;, &quot;Unknown/Not Reported&quot;, &quot;III&quot;, NA, &quot;IIB&quot;, &quot;II&quot;, NA, &quot;Unknown/Not Reported&quot;)
    ),
    row.names = c(NA, 10L), class = &quot;data.frame&quot;
) 

# The variables are not yet factors as far as R is concerned as you can 
# see from the tibble print method
df %&gt;% as_tibble()

stages &lt;- c(&quot;I&quot;, &quot;IA&quot;, &quot;IB&quot;, &quot;II&quot;, &quot;IIA&quot;, &quot;IIB&quot;, &quot;III&quot;, &quot;IIIA&quot;, &quot;IIIB&quot;, &quot;IIIC&quot; ,&quot;IV&quot;, &quot;IVA&quot;, &quot;IVB&quot;, &quot;IVC&quot;, &quot;Unknown/Not Reported&quot;)

df %&gt;%
    as_tibble() %&gt;%
    dplyr::mutate(
        # if we make them ordered factors then they now have values you can do a mathematical operation on
        PathGroupStage = factor(PathGroupStage, levels = stages, ordered = TRUE),
        ClinGroupStage = factor(ClinGroupStage, levels = stages, ordered = TRUE),
        # case when is like a more general if_else() with multiple conditions
        # of the form: logical test ~ result if true
        OutputStage = case_when(
            (is.na(ClinGroupStage) | ClinGroupStage == &quot;Unknown/Not Reported&quot;) &amp; 
            (is.na(PathGroupStage) | PathGroupStage == &quot;Unknown/Not Reported&quot;) ~ 
                factor(&quot;Unknown/Not Reported&quot;, levels = stages, ordered = TRUE),
            is.na(PathGroupStage) ~ ClinGroupStage,
            is.na(ClinGroupStage) ~ PathGroupStage,
            PathGroupStage &gt;= ClinGroupStage ~ PathGroupStage,
            ClinGroupStage &gt;= PathGroupStage ~ ClinGroupStage
        )
    )
</code></pre>
"
"70093824","Verilog basic gate data flow not working for NAND & NOR, but works for XNOR & XOR","<p>I have some basic code using data flow statements, but nor and nand functions are not working with this.</p>
<pre><code>module basic_gates_bitwise_df(
input A,
input B,
output andd,orr,nota,nandd,norr,xorr,xnorr
);
assign andd=A&amp;B;
assign orr=A|B;
assign nota=~A;
assign nandd=A~&amp;B;
assign norr=A~|B ;
assign xorr=A^B;
assign xnorr=A~^B;
endmodule
</code></pre>
<p>I got errors like this:</p>
<pre><code>ERROR:HDLCompiler:806 - &quot;F:\basic.v&quot; Line 37: Syntax error near &quot;~&amp;&quot;.
ERROR:HDLCompiler:806 - &quot;F:\basic.v&quot; Line 38: Syntax error near &quot;~|&quot;.
ERROR:HDLCompiler:598 - &quot;F:\basic.v&quot; Line 21: 
Module&lt;basic_gates_bitwise_df&gt; ignored due to previous errors.
</code></pre>
<p>What can I try to resolve this?</p>
","<verilog><fpga><digital><register-transfer-level><vlsi>","2021-11-24 09:43:28","324","1","1","70095056","<p>Sometimes you get a more helpful message with different simulators.  For example, with Synopsys VCS on edaplayground:</p>
<pre><code>Error-[SE] Syntax error
  Following verilog source has syntax error :
    Invalid use of unary operator '~&amp;'
   token is ';'
  assign nandd=A~&amp;B;
                    ^
1 error
</code></pre>
<p>To fix the errors, change:</p>
<pre><code>assign nandd=A~&amp;B;
assign norr=A~|B ;
</code></pre>
<p>to:</p>
<pre><code>assign nandd=~(A&amp;B);
assign norr=~(A|B);
</code></pre>
<p>Refer to IEEE Std 1800-2017, section 11.4.9 <em>Reduction operators</em> :</p>
<blockquote>
<p>The unary reduction operators shall perform a bitwise operation on a
single operand to produce a single-bit result.</p>
</blockquote>
<p>For a NAND, you should AND the 2 inputs, then negate the AND expression.</p>
<hr />
<p>There is no syntax error with <code>~^</code> because it is also a binary operator, as well as a unary operator.</p>
"
"70003994","Azure Data factory - data flow expression date and timestamp conversions","<p>Using derived column i am adding 3 columns -&gt; 2 columns for date and 1 for timestamp. for the date columns i am passing a string as parameter. for eg: 21-11-2021  and timstamp i am using currenttimestamp fucntion.</p>
<p>i wrote expressions in derived columns to convert them as date and timestamp datatype and also in a  format that target table needs which is dd-MM-yyyy and dd-MM-yyyy HH:mm:ss repectively</p>
<p>For date-&gt;</p>
<p>expression used:  toDate($initialdate, 'dd-MM-yyyy')</p>
<p>data preview output:  2021-01-21    --(not in the format i want)</p>
<p>After pipline Debug Run, value in target DB(Azure sql database) column:
2021-01-21T00:00:00   -- in table it shows like this I dont understand why</p>
<p>For Timstamp conversion:</p>
<p>Expression used:</p>
<p>toTimestamp(toString(currentTimestamp(), 'dd-MM-yyyy HH:mm:ss', 'Europe/Amsterdam'), 'dd-MM-yyyy HH:mm:ss')</p>
<p>Data preview output: 2021-11-17 19:37:04  -- not in the format i want</p>
<p>After pipline Debug Run, value in target DB(Azure sql database) column:
2021-11-17T19:37:04:932   -in table it shows like this I dont understand why</p>
<p>question 1: I am NOT getting values in the format the target requires ???and it should be only in DATE And Datetime2 dataype respectively so no string conversions</p>
<p>question 2: after debug run i dont know why  after insert the  table values look different from Data preview???</p>
<p>Kinldy let me know if i have written any wrong expressions??</p>
<p>--apologies i am not able post pictures---</p>
","<azure-data-factory><date-format>","2021-11-17 11:59:16","4420","0","1","70019690","<ol>
<li><p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#todate"" rel=""nofollow noreferrer"">toDate()</a> converts input date string to date with default format as yyyy-[M]M-[d]d. <strong>Accepted formats</strong> are :[ yyyy, yyyy-[M]M, yyyy-[M]M-[d]d, yyyy-[M]M-[d]dT* ].</p>
<p>Same goes with <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#totimestamp"" rel=""nofollow noreferrer"">toTimestamp()</a>, the default pattern is yyyy-[M]M-[d]d hh:mm:ss[.f...] when it is used.</p>
</li>
<li><p>In <code>Azure SQL Database</code> as well the default <strong>date</strong> and <strong>datetime2</strong> formats are in YYYY-MM-DD and YYYY-MM-DD HH:mm:ss as shown below.</p>
</li>
</ol>
<p><a href=""https://i.stack.imgur.com/WzRv5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WzRv5.png"" alt=""enter image description here"" /></a></p>
<ol start=""3"">
<li>But if your column datatypes are in string (varchar) format, then you can change the output format of <code>date</code> and <code>DateTime</code> in azure data flow mappings.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/Zrs22.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Zrs22.png"" alt=""enter image description here"" /></a></p>
<p>When loaded to Azure SQL database, it is shown as below:</p>
<p><a href=""https://i.stack.imgur.com/qBe76.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qBe76.png"" alt=""enter image description here"" /></a></p>
<p><strong>Note</strong>: This format results when datatype is varchar</p>
<ol start=""4"">
<li>If the datatype is the date in the Azure SQL database, you can convert them to the required format using date conversions as</li>
</ol>
<p><code>select id, col1, date1, convert(varchar(10),date1,105) as 'dd-MM-YYYY' from test1</code></p>
<p><a href=""https://i.stack.imgur.com/xqnt1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xqnt1.png"" alt=""enter image description here"" /></a></p>
<ol start=""5"">
<li>Azure SQL Database always follows the UTC time zone. Using “AT TIME ZONE” convert it another non-UTC time zone.</li>
</ol>
<p><code>select getdate() as a, getdate() AT TIME ZONE 'UTC' AT TIME ZONE 'Central Standard Time' as b</code></p>
<p><a href=""https://i.stack.imgur.com/TPeaO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TPeaO.png"" alt=""enter image description here"" /></a></p>
<p>You can also refer to sys.time_zone_info view to check current UTC offset information.</p>
<p><code>select * from sys.time_zone_info</code></p>
"
"69985128","App is blocked when opening from lock screen with face id","<p>I have an app that forbids other apps from using the camera while my app is running, But when I turn it off and back on, my device has face id so my app is blocked just like when another app uses it. use camera. Here is my camera check code:</p>
<pre><code>fun checkCameraAvailable(listener: ((isAvailable: Boolean) -&gt; Unit)) {

    val manager = getSystemService(CAMERA_SERVICE) as CameraManager
    val handler = Handler(Looper.getMainLooper())

    manager.registerAvailabilityCallback(object : AvailabilityCallback() {
        override fun onCameraAvailable(cameraId: String) {

            listener.invoke(true)
            super.onCameraAvailable(cameraId)
        }

        override fun onCameraUnavailable(cameraId: String) {

            listener.invoke(false)
            super.onCameraUnavailable(cameraId)
        }
    }, handler)
}
</code></pre>
","<android><kotlin><android-camera><mdm><lockscreen>","2021-11-16 07:17:52","87","0","1","69985668","<p>There is little information given in your original question but one of the valid issues could be that you are not stopping the camera preview properly when your app goes to the background after the device is locked.</p>
<p>You should <a href=""https://developer.android.com/training/camera/cameradirect#TaskReleaseCamera"" rel=""nofollow noreferrer"">stop the preview and release the camera</a> as soon as you don't need it anymore.</p>
<p>I'll quote the docs mentioned at that link:</p>
<blockquote>
<p>Once your application is done using the camera, it's time to clean up. In particular, you must release the Camera object, or you risk crashing other applications, including new instances of your own application.</p>
<p>When should you stop the preview and release the camera? Well, having your preview surface destroyed is a pretty good hint that it’s time to stop the preview and release the camera, as shown in these methods from the Preview class.</p>
</blockquote>
<pre><code>override fun surfaceDestroyed(holder: SurfaceHolder) {
    // Surface will be destroyed when we return, so stop the preview.
    // Call stopPreview() to stop updating the preview surface.
    mCamera?.stopPreview()
}

/**
 * When this function returns, mCamera will be null.
 */
private fun stopPreviewAndFreeCamera() {
    mCamera?.apply {
        // Call stopPreview() to stop updating the preview surface.
        stopPreview()

        // Important: Call release() to release the camera for use by other
        // applications. Applications should release the camera immediately
        // during onPause() and re-open() it during onResume()).
        release()

        mCamera = null
    }
}
</code></pre>
<p>If you are using CameraX it should be bound to the lifecycle of the Activity/Fragment that is using it and thus released automatically.</p>
<p><a href=""https://github.com/android/camera-samples/blob/main/CameraXBasic/app/src/main/java/com/android/example/cameraxbasic/fragments/CameraFragment.kt#L334"" rel=""nofollow noreferrer"">See an example here</a>.</p>
"
"69952568","How can i update values stored in map to database after every 1 hour while synchronizing incoming requests on route which updates map continously","<p>I am facing a problem to update the values stored in map to database because the map is updating continously due to incoming requests on the route and at the same time I want to update values in database periodically with the help of map. How to synchronize both the operations?</p>
<pre><code>const url_count = new Map();

async function update_count(){
 
    // update value in mongodb 
    
}
app.get('/teeny/:code', async (req, res) =&gt; {
    try {
        const url = await Link_URL.findOne({
            _id : req.params.code
        })
        if (url) {
          if(url_count.has(req.params.code)){
            const val = url_count.get(req.params.code);
            url_count.set(req.params.code,val+1);
            update_count();
          }
          else{
              url_count.set(req.params.code,1);
          }  
            return res.redirect(url.URL);
        } else {
            return res.status(404).json('No URL Found')
        }
    }
    catch (err) {
        console.error(err)
        res.status(500).json('Server Error')
    }
})
cron.schedule('* * 1 * *', update_count);
</code></pre>
","<javascript><node.js><mongodb><data-synchronization>","2021-11-13 08:09:37","125","0","1","69952683","<p>You could use <a href=""http://bunkat.github.io/later/"" rel=""nofollow noreferrer"">Later</a> for a simple solution... here you would be collecting all those values and then insert them at a specified time that you specified in later..</p>
<p>For something heavier, you could go for <a href=""https://github.com/OptimalBits/bull"" rel=""nofollow noreferrer"">Bull</a> or for <a href=""https://github.com/arobson/rabbot"" rel=""nofollow noreferrer"">Rabbot</a></p>
<p>If you want to kill a lion, go for <a href=""https://www.npmjs.com/package/node-celery"" rel=""nofollow noreferrer"">Node-Celery</a>... but make sure to have python installed.</p>
<p>Basically, all you need is a queuing system to collect[in a queue] all those tasks that you want to save later...</p>
"
"69861293","Pentaho data integration loop count variable","<p>I want a simple loop function to count the number of loop like below in java programming:</p>
<pre class=""lang-java prettyprint-override""><code>for (int i = 0; i &lt;3; i++) {
    count = count+1;
}
System.out.println(count);
</code></pre>
<p>I am doing it using Pentaho data integration. so I have 1 job contain 3 transformations in it, where first transformation set the number of loop (above example 3), then second transformation click &quot;Execute every input row&quot; for looping, and set variable inside the transformation using Javascript with <code>getVariable()</code> and <code>setVariable()</code> function. the last transformation just get variable and write log to show the count.
The problem is every loop in the transformation 2 will get variable as 0. so it end up <code>result = 1</code>, what I expect is <code>3</code>.</p>
<p>added the project files here: <a href=""https://drive.google.com/drive/folders/1f8KTVZ4OseZkMqOl4sX2tlo4ebDyfGES?usp=sharing"" rel=""nofollow noreferrer"">file</a></p>
","<loops><variables><counter><pentaho><kettle>","2021-11-06 03:43:57","544","0","2","69887950","<p>We'll need more details to help you, don't you have a simple sample of what you are trying to accomplish?
You can pass variables to a transformation from the job, so I don't think you'll need the getVariable() and setVariable() methods, you can just use the configuration properties of the transformation to execute:</p>
<p><a href=""https://i.stack.imgur.com/eeYJJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eeYJJ.png"" alt=""Passing a value to a variable in a job"" /></a></p>
<p>I prefer using parameters (next tab) better than arguments/variables, but that's my preference.</p>
"
"69861293","Pentaho data integration loop count variable","<p>I want a simple loop function to count the number of loop like below in java programming:</p>
<pre class=""lang-java prettyprint-override""><code>for (int i = 0; i &lt;3; i++) {
    count = count+1;
}
System.out.println(count);
</code></pre>
<p>I am doing it using Pentaho data integration. so I have 1 job contain 3 transformations in it, where first transformation set the number of loop (above example 3), then second transformation click &quot;Execute every input row&quot; for looping, and set variable inside the transformation using Javascript with <code>getVariable()</code> and <code>setVariable()</code> function. the last transformation just get variable and write log to show the count.
The problem is every loop in the transformation 2 will get variable as 0. so it end up <code>result = 1</code>, what I expect is <code>3</code>.</p>
<p>added the project files here: <a href=""https://drive.google.com/drive/folders/1f8KTVZ4OseZkMqOl4sX2tlo4ebDyfGES?usp=sharing"" rel=""nofollow noreferrer"">file</a></p>
","<loops><variables><counter><pentaho><kettle>","2021-11-06 03:43:57","544","0","2","69918254","<p>The problem is that, in t2 transformation, you are getting the variable and setting a new value for the same variable at the same time, which does not work in the same transformation. When you close the Set variable step you get this warning:</p>
<p><a href=""https://i.stack.imgur.com/GgNDn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GgNDn.png"" alt=""Set variable step warning"" /></a></p>
<p>To avoid it you need to use two variables, one you set before executing the loop, and another set each time you execute the loop or after executing the loop with the last value.</p>
<p>I have modified your job to make it work, in t1 transformation, I have added a new field (rownum_seq) created with the Add sequence step, to know how much to add to variable cnt in each execution of the loop. I could have used your id field, but in case you don't have a similar field in your real world job, that's the step you need to achieve something similar. I have modified the variable name to make more clear what I'm doing, in t1 I set the value of variable <strong>var_cnt_before</strong>.</p>
<p>In t2 transformation, I read var_cnt_before, and set the value of <strong>var_cnt_after</strong> as the sum of var_cnt_before + rownum_seq, this means I'm changing the value of var_cnt_after each time t2 is executed.</p>
<p>In t3 transformation, I read var_cnt_after, which has the value of the last execution of t2.</p>
<p>You could also calculate var_cnt_after in t1 and not modify it in t2, using the Group by step to get the max value of rownum_seq, so you don't need to modify that variable each time you execute t2, depending on what you need to achieve you might need to use it or change in t2 or you just need the final value so you calculate it in t1.</p>
<p>This is the <a href=""https://drive.google.com/file/d/1fSwEcIJpaMfC7dYto_O7xi8x9icUBy7e/view?usp=sharing"" rel=""nofollow noreferrer"">link</a> to the modified job and transformations.</p>
"
"69850927","How to Select or Filter out values with endsWith() in Mapping data flow","<p>I would like to filter out all values that does not end with &quot;:Q_TT&quot;.</p>
<p>I have tried with the following activities</p>
<p><a href=""https://i.stack.imgur.com/aN1om.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aN1om.png"" alt=""enter image description here"" /></a></p>
<p>My bronze data has a column named &quot;pnt_name&quot;. One of the rows in the table ends with &quot;:Q_TT&quot; so I would expect that the Exists activity would pass that row through.</p>
<p>Custom expression in Exists1</p>
<pre><code>endsWith(':Q_TT', pnt_name) 
</code></pre>
<p>In the future I would like the SourceData dataset to hold the filter values.
Thanks very much</p>
","<azure-data-factory>","2021-11-05 09:17:28","408","0","1","69851305","<p>You should be using <code>filter</code> activity instead of <code>exists</code> activity for your case.</p>
<p>The pipeline(where you can see the 2 test cases of <code>A</code> and <code>B:Q_TT</code>:
<a href=""https://i.stack.imgur.com/2T3cd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2T3cd.png"" alt=""enter image description here"" /></a></p>
<p>Here is the preview of the filter activity, you should use an expression of <code>endsWith(pnt_name, &quot;:Q_TT&quot;)</code> too. You can see <code>A</code> is removed and <code>B:Q_TT</code> is kept.
<a href=""https://i.stack.imgur.com/UaUlv.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UaUlv.jpg"" alt=""enter image description here"" /></a></p>
"
"69823373","How to force error or stoppage in SSIS data flow during the debugging process?","<p>The problem is: i make left join for two tables and then i need to load whole data to another table but only if every row from first table has match in second one, so, to cut it short, there is no NULLs in one exact column.
If there is at least one null i want to fail my data flow so it'll not load any data to final table and then send an email with the error by executing sql task.</p>
<p><a href=""https://i.stack.imgur.com/Msc3z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Msc3z.png"" alt=""enter image description here"" /></a></p>
<p>After many tries i can only make errors if there is nulls but this error are not fatal. How can i raise fatal error not using smth stupid like data conversion which can't be done? I was trying to make breakpoint after some variable is changed but was defeated by ssis(</p>
","<sql><ssis><ssis-2012>","2021-11-03 10:37:06","306","0","1","69832297","<p>If I understand correctly, the Data Flow loads data to Table1. The Execute SQL Task uses Table1 to populate Table2.</p>
<p>The business rule is that the Execute SQL Task should <em>only</em> fire if a column from the previous data load had no NULLs.</p>
<p>The lazy way to handle this, is to put the logic in the query itself. Something like the following and yes, there are ways to optimize this</p>
<pre><code>INSERT INTO dbo.Table2 SELECT * FROM dbo.Table1 WHERE NOT EXISTS (SELECT * FROM dbo.Table1 WHERE MyColumn IS NOT NULL)
</code></pre>
<p>To make this happen only in SSIS,</p>
<ul>
<li>Add a Variable to the Package called NullRowCount and initialize it to zero.</li>
<li>In the Data flow, add a Multicast between the Join and the Destination. Route one path to the destination</li>
<li>In the Data flow, connect a Conditional Split to a new path from the Multicast. Configure the Conditional Split to have have an Output Name of &quot;No Data&quot; and use an expression like IsNull([MyColumn]). That's a boolean, yes/no.</li>
<li>In the Data flow, add a Row Count transformation to the Conditional Split and attach it to the &quot;No Data&quot; pipe (the Default pipe will contain rows that have values in MyColumn). Use @[User::NullRowCount] in the Row Count transformation.</li>
<li>Finally, double click the precedent constraint you have between the Data Flow and the Execute SQL Task. Make it back to an On Success constraint and then change the evaluation option from <code>Constraint</code> to <code>Constraint and Expression</code>. Here, we'll use an expression of <code>@[User::NullRowCount] == 0</code></li>
</ul>
<p>In plain English, we are going to have the data flow count how many rows in our set have a NULL in <code>MyColumn</code>. The Precedent Constraint will allow/disallow the Execute SQL Task to run and the criteria we specify are that the data flow had to run successfully and the count of rows with NULL in it is zero.</p>
<p>If say you wanted to have an action when the count is non-zero (send email or other alert), then you would add another Task and configure it with Expression and Constraint but now use an expression of <code>@[User::NullRowCount] &gt; 0</code></p>
<p>Based on the comment</p>
<blockquote>
<p>may be i can stop it (force an error) inside the data flow before loading data in data source? because this sql text sends an email, so i want all etl process to be done in one data flow</p>
</blockquote>
<p>No, not really. Assuming you changed out the Row Count in the above with a Script Task that explicitly raises an error or a Derived Column Task that forces a divide by zero - either of those would interrupt a data flow, <em>but</em> you don't know whether it was the first row of the data flow that caused the exception or the one billionth. In the later case, the data has already flowed into the destination (unless you have the commit size of 0 which can lead to other issues) and you have partially loaded data.</p>
<p>Ultimately, you need to preprocess your data to identify if there's data that does not conform to expectation. I would make the above changes - count if you have any bad data but instead of landing data into a table, land it into a <a href=""https://learn.microsoft.com/en-us/sql/integration-services/data-flow/raw-file-destination?view=sql-server-ver15"" rel=""nofollow noreferrer"">Raw File</a>. A raw file is a compact binary of the data so yes, you'll pay disk IO penalty but it will save you reprocessing the data if it's valid.</p>
<p>Then you add a new Data Flow Task that only works when you have a zero null count, using the precedent constraint approach described above. This new data flow is just Raw File Source to &quot;original destination.&quot; Now you'll get a clean separation of data landing in your table only if pristine and not have to worry about partial loads.</p>
"
"69763710","How to convert UST to EST in Azure data factory mapping data flows?","<p>1.How to convert UST to EST in mapping data flows in azure data factory</p>
","<azure-data-factory>","2021-10-29 04:39:22","247","0","1","69763962","<p>Simply use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#fromutc"" rel=""nofollow noreferrer"">fromUTC()</a> and supplement <code>America/New_York</code> as the second parameter if your input column is already a timestamp type.</p>
<p><a href=""https://i.stack.imgur.com/Z1I9E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Z1I9E.png"" alt=""enter image description here"" /></a></p>
<p>For eastern time reference, please check out <a href=""https://howtodoinjava.com/java/date-time/convert-date-time-to-est-est5edt/"" rel=""nofollow noreferrer"">this article</a> about java timezone.</p>
"
"69707615","Merging attributes in pentaho (kettle)","<p>I have 2 tables, both have 2 primary keys (anys_mes_dia and aircraftreg) and each table has other attributes. I want to join both tables by the 2 PK.
The thing is, for some [any_mes_dia,aircraftreg] I have all the attributes of both tables but for others I only have the attribute of one table.
How can I join this tables so as to get [anys_mes_dia,aircraftreg,dy,add,cn] and only nulls in the attribute that a specitic row doesn't have.</p>
<p>Here an image of what I have (some rows only have aircraftreg_1, any_mes_dia1 and CN).</p>
<p><a href=""https://i.stack.imgur.com/qRFOt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qRFOt.png"" alt=""enter image description here"" /></a></p>
","<attributes><pentaho><kettle><pentaho-data-integration><data-integration>","2021-10-25 11:54:01","101","0","1","69720837","<p>In the Merge join step you have the option to define the type of join, in this case you could use the LEFT/RIGHT OUTER join (depending on which table is leading) to get the results you want.</p>
"
"69688229","Convert JSON objects to array using Azure data flow","<p>I'm using Azure Data Flow, and I'm using Union to combine two sources, so this union contains JSON documents. Is there a way to convert this JSON document to array of documents?</p>
<p>Union contains:</p>
<pre><code>{&quot;key&quot;:1,&quot;value&quot;:&quot;test8&quot;}
{&quot;key&quot;:2,&quot;value&quot;:&quot;test6&quot;}
{&quot;key&quot;:3,&quot;value&quot;:&quot;test3&quot;}
</code></pre>
<p>What I'm looking for is a way to get like this format:</p>
<pre><code>[
 {
   &quot;key&quot;: 1,
   &quot;value&quot;: &quot;test8&quot;
 },
{
 &quot;key&quot;: 2,
 &quot;value&quot;: &quot;test6&quot;
},
{
 &quot;key&quot;: 3,
 &quot;value&quot;: &quot;test3&quot;
}
]
</code></pre>
<p>Thanks for you help</p>
","<azure><azure-data-factory>","2021-10-23 12:37:27","1632","0","1","69737670","<p>You could use <code>Aggregate</code> transformation and use <strong>collect</strong> expression to combine all the JSON document and pass it to sink with JSON dataset. But this will not output the result exactly what you are looking for and gives <strong>aggregated</strong> <strong>column</strong> name in the output as shown below.</p>
<p><strong>Aggregate</strong>:</p>
<p>Column1: <code>collect(@(key=key,value=value))</code></p>
<p><a href=""https://i.stack.imgur.com/4D3pi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4D3pi.png"" alt=""enter image description here"" /></a></p>
<p><strong>Data flow Output</strong>:</p>
<p><a href=""https://i.stack.imgur.com/o0zXx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o0zXx.png"" alt=""enter image description here"" /></a></p>
<p>As an <strong>alternative</strong>, you can copy the union JSON documents to the storage and use a <code>copy data</code> activity to get convert the JSON document to an array of documents.</p>
<p><a href=""https://i.stack.imgur.com/RDXjA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RDXjA.png"" alt=""enter image description here"" /></a></p>
<p><strong>Output</strong>:</p>
<p><a href=""https://i.stack.imgur.com/sOPJE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sOPJE.png"" alt=""enter image description here"" /></a></p>
"
"69663648","SQL Server Connection Read Timeout","<p>I am trying to load from a SQL Server 2012 to Azure SQL Server using Talend. Job will read all the data from the source DB and load into destination. All the jobs are doing truncate and load. Lot of tables have records more than 7 millions. 5 tables have nearly 25 million records. I have one master job which will trigger all the jobs in parallel. I am facing a <em><strong>com.microsoft.sqlserver.jdbc.SQLServerException: Connection timed out (Read failed)</strong></em> issue.</p>
<p>When anyone of the job/table throws this error, all the jobs that are running parallelly also throws the same error. What is the reason for this issue and how to fix it?</p>
<p>Thanks,</p>
","<sql-server><azure-sql-database><etl><talend><data-integration>","2021-10-21 14:18:07","502","0","1","69707650","<p>&quot;When you connect to a SQL Server 2012 instance running on Windows Server 2012, you get time-out error messages.&quot;</p>
<p>It's possible that the test server didn't get the same Windows updates as the production server. That might explain why production code and configuration are identical.</p>
<p>This is the <a href=""https://social.msdn.microsoft.com/Forums/sqlserver/en-US/afb47f41-5196-4e30-8213-d72cddb79641/sqlexception-read-timed-out-clientconnectionidf9c1a8c4326a4945a166e0c906bfe4e5?forum=sqlgetstarted"" rel=""nofollow noreferrer"">Original Post</a>, this issue might be caused by something unrelated to Java JDBC, such as SSMS or applications that do not use JDBC.</p>
<blockquote>
<ol>
<li>Verify that you can ping the SQL Server-installed server.</li>
<li>Verify that the SQL Server instance is up and operating.</li>
<li>Double-check that the connection codes you used are valid.</li>
</ol>
</blockquote>
"
"69638752","Azure Data Factory data flow file sink","<p>I am using a .csv file to import data into an Azure SQL database. After the data import is complete I am now moving the source file from the Source container to myArchive container. I am now trying to save the filename as <em><strong>SaleData_yyyyMMdd_HHmm.csv</strong></em>, but,  I have the folder with this name getting created and the file is broken down into multiple part files (part-00000-, part-00001-,...). Could you please guide me on how to specify the filename with current data &amp; timestamp.</p>
<p>File System: myArchive</p>
<p>Folder Path: <code>concat('SalesDepartment/Warehouse1/','SaleData_',toString(currentTimestamp(),'yyyyMMdd_HHmm'),'.csv')</code></p>
<p><a href=""https://i.stack.imgur.com/Bucdn.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Bucdn.jpg"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2021-10-19 23:44:43","3357","2","1","69640789","<p>Folder path can be mentioned directly in the sink dataset. (Note, my source and sink both are delimited type)</p>
<p>For filename,
Under sink data set, create a parameter to pass file name and use it in the file name portion of dataset.</p>
<p><a href=""https://i.stack.imgur.com/qhJKW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qhJKW.png"" alt=""SinkSetting"" /></a></p>
<p>Use the below expression in copy activity sink's parameter value</p>
<pre><code>@concat('SaleData_',formatDateTime(utcnow(),'yyyyMMdd_HHmm'),'.csv')
</code></pre>
<p><a href=""https://i.stack.imgur.com/20yWy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/20yWy.png"" alt=""SinkParamValue"" /></a></p>
<p>Remember, this just copies your source in a different name. We need to add a delete activity to delete the original source file.</p>
<p>If you are using a dataflow,</p>
<p>make sure you are choosing single partition in the optimize tab of Sink instead of Use current Partitioning.</p>
<p><a href=""https://i.stack.imgur.com/IBquQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IBquQ.png"" alt=""Partitioningoption"" /></a></p>
<p>Then, go to Settings, choose Output to SIngle file. Under filename, mention the expression with timestamp.</p>
<pre><code>concat('SaleData_',toString(currentUTC('yyyyMMdd_HHmm')),'.csv')
</code></pre>
<p><a href=""https://i.stack.imgur.com/PnL2F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PnL2F.png"" alt=""Setting"" /></a></p>
"
"69620752","Duplicates getting inserted in data flow transformation - ADF pipeline","<p>I am new to ADF. I have a sql query that I need to convert to ADF pipeline.
I have table1 in linkedserver A and table2 in Linkedserver B. I need to import data from Table1 to table2.
I have below sql query</p>
<pre><code>select A.Id from linkedserver1.databasename.dbo.table1 A left join  linkedserver2.databasename.dbo.table2 B on a.id = b.id
where b.id is null
</code></pre>
<p>I need to convert this to pipeline
I don't want to import record that is already existing in table B. below pipline is inserting duplicate records. I tried using left join .
<a href=""https://i.stack.imgur.com/QQpIi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QQpIi.png"" alt=""enter image description here"" /></a></p>
","<sql><azure-data-factory>","2021-10-18 18:18:44","358","0","1","69623908","<p>You just need 1 more filter activity after your left join and before sink</p>
<p>You can use <code>isNull(...)</code> in the <code>Filter on</code> expression.
<a href=""https://i.stack.imgur.com/ZhWxT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZhWxT.png"" alt=""enter image description here"" /></a></p>
"
"69606534","Offline first app and data sharing between users","<p>I want to develop cross platform app which has three components (client app, server app, database) if possible. I especially want to use it offline.</p>
<p>App is similar to TODO app. I want to make a TODO app to try at the beginning.</p>
<ol>
<li><p>Users use the app without any internet connection. They are make CRUD operations as offline in first use. The user can use it offline indefinitely.</p>
</li>
<li><p>If the user wants to register to app, all of user's data sync to online database and use offline or online. Data can be synchronized at certain times every day.</p>
</li>
<li><p>Users share some data with other users and use offline/online.</p>
</li>
</ol>
<p>How can I handle these situations. Are there any examples/documents/suggestions :)</p>
<p>Thank you.</p>
<p>I researched many platforms.</p>
<p>Maybe pouchdb/couchdb can handle first 2 situations but I think the third is complicated with couchdb per user, and I don't want to connect directly to database, I want to use application server as middleware.</p>
<p>I saw dexiejs, Mendix, logux, Mango realm, ......</p>
<p>I guess what I've seen doesn't quite meet the demands.</p>
<p>I will concentrate on PWA.</p>
","<web-applications><couchdb><offline><pouchdb><data-synchronization>","2021-10-17 16:44:54","603","0","1","69669993","<p>With CouchDB you can let users access databases and documents that belong to a different user right down to the document level.</p>
<p>You need to look at how &quot;users&quot; and &quot;roles&quot; work, and you need to look into &quot;design documents&quot; for setting what users can access and do with documents they have access to in specific databases.</p>
<p>The PouchDB.com website has some great documentation and example code to help you get started with this. Check out their &quot;Plug-ins&quot; page and specifically &quot;pouchdb.authentication.js&quot; and the &quot;CouchDB authentication recipes&quot; <a href=""https://github.com/pouchdb-community/pouchdb-authentication/blob/master/docs/recipes.md"" rel=""nofollow noreferrer"">here</a></p>
"
"69555039","ADF - get random sample data from data flows","<p>Is it possible to get random sample data from data flows?</p>
<p><a href=""https://i.stack.imgur.com/ySa8R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ySa8R.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2021-10-13 11:51:23","290","0","1","69558125","<p>Sampling property provided in source setting in ADF is used to limit the records from the dataset. You can choose this when you have a large dataset. However, while you use debug to preview, row limit set in the <code>debug setting</code> will hold <em>preference</em> or is considered over Sampling row limit set.</p>
<p>I tried to verify and yes, it appears this is not the sampling as in context of getting random rows. There is no out of the box availability to get random rows from source in ADF. You would have to explicitly design a flow depending on the type of source data.</p>
<p>You can share a <a href=""https://feedback.azure.com/d365community/forum/1219ec2d-6c26-ec11-b6e6-000d3a4f032c"" rel=""nofollow noreferrer"">feedback</a></p>
"
"69545917","MDM bridge WMI: How to allow camera MDM_Policy_Config01_Camera02","<p>I would like to enable and disable windows devices as camera using WMI in C++. For example I'm able to access to table MDM_Policy_Result01_Camera02 and get the property AllowCamera quite easily where on the documentation is specify &quot;Access type: Read/write&quot;.
So in my opinion I should be able to modify it.</p>
<p>But seems like WQL doesn't work with UPDATE.</p>
<p>This is my code to access to table MDM_Policy_Result01_Camera02 :</p>
<pre><code>#include &lt;iostream&gt;
#define _WIN32_DCOM
#include &lt;windows.h&gt;
#include &lt;Wbemidl.h&gt;
#include &lt;comdef.h&gt;

# pragma comment(lib, &quot;wbemuuid.lib&quot;)

bool initializeCom(){
    // Step 1: --------------------------------------------------
    // Initialize COM. ------------------------------------------

    HRESULT hres =  CoInitializeEx(0, COINIT_MULTITHREADED); 
    if (FAILED(hres))
    {
    std::cout &lt;&lt; &quot;Failed to initialize COM library. Error code = 0x&quot; 
        &lt;&lt; std::hex &lt;&lt; hres &lt;&lt; std::endl;
    return false;                  // Program has failed.
    }

    // Step 2: --------------------------------------------------
    // Set general COM security levels --------------------------

    hres =  CoInitializeSecurity(
        nullptr, 
        -1,                          // COM authentication
        nullptr,                        // Authentication services
        nullptr,                        // Reserved
        RPC_C_AUTHN_LEVEL_DEFAULT,   // Default authentication 
        RPC_C_IMP_LEVEL_IMPERSONATE, // Default Impersonation  
    nullptr,                        // Authentication info
        EOAC_NONE,                   // Additional capabilities 
        nullptr                         // Reserved
        );

    if (FAILED(hres))
    {
        std::cout &lt;&lt; &quot;Failed to initialize security. Error code = 0x&quot; 
            &lt;&lt; std::hex &lt;&lt; hres &lt;&lt; std::endl;
        CoUninitialize();
        return false;                    // Program has failed.
    }
    return true;
}

bool setUpWBEM(IWbemLocator*&amp; wbemLocator, IWbemServices*&amp; wbemServices){
    // Step 3: ---------------------------------------------------
    // Obtain the initial locator to WMI -------------------------
    HRESULT hres = CoCreateInstance(
        CLSID_WbemLocator,             
        0, 
        CLSCTX_INPROC_SERVER, 
        IID_IWbemLocator, (LPVOID *) &amp;wbemLocator);

    if (FAILED(hres))
    {
        std::cout &lt;&lt; &quot;Failed to create IWbemLocator object.&quot;
            &lt;&lt; &quot; Err code = 0x&quot;
            &lt;&lt; std::hex &lt;&lt; hres &lt;&lt; std::endl;
        CoUninitialize();
        return false;                 // Program has failed.
    }

    // Step 4: -----------------------------------------------------
    // Connect to WMI through the IWbemLocator::ConnectServer method

    // Connect to the root\cimv2 namespace with
    // the current user and obtain pointer wbemServices
    // to make IWbemServices calls.

    hres = wbemLocator-&gt;ConnectServer(
         _bstr_t(L&quot;Root\\CIMv2\\MDM\\DMMap&quot;), // Object path of WMI namespace
         nullptr,                    // User name. NULL = current user
         nullptr,                    // User password. NULL = current
         0,                       // Locale. NULL indicates current
         0,                    // Security flags.
         0,                       // Authority (for example, Kerberos)
         0,                       // Context object 
        &amp;wbemServices            // pointer to IWbemServices proxy
         );

    if (FAILED(hres))
    {
        std::cout &lt;&lt; &quot;Could not connect. Error code = 0x&quot; &lt;&lt; std::hex &lt;&lt; hres &lt;&lt; std::endl;
        wbemLocator-&gt;Release();     
        CoUninitialize();
        return false;                // Program has failed.
    }

    std::cout &lt;&lt; &quot;Connected to ROOT\\CIMV2 WMI namespace&quot; &lt;&lt; std::endl;


    // Step 5: --------------------------------------------------
    // Set security levels on the proxy -------------------------

    hres = CoSetProxyBlanket(
       wbemServices,                // Indicates the proxy to set
       RPC_C_AUTHN_WINNT,           // RPC_C_AUTHN_xxx
       RPC_C_AUTHZ_NONE,            // RPC_C_AUTHZ_xxx
       nullptr,                        // Server principal name 
       RPC_C_AUTHN_LEVEL_CALL,      // RPC_C_AUTHN_LEVEL_xxx 
       RPC_C_IMP_LEVEL_IMPERSONATE, // RPC_C_IMP_LEVEL_xxx
       nullptr,                        // client identity
       EOAC_NONE                    // proxy capabilities 
    );

    if (FAILED(hres))
    {
        std::cout &lt;&lt; &quot;Could not set proxy blanket. Error code = 0x&quot; 
            &lt;&lt; std::hex &lt;&lt; hres &lt;&lt; std::endl;
        wbemServices-&gt;Release();
        wbemLocator-&gt;Release();     
        CoUninitialize();
        return false;               // Program has failed.
    }

    return true;
}

int main() {

    std::cout &lt;&lt; &quot;HelloWorld&quot; &lt;&lt; std::endl;
    IWbemLocator* wbemLocator{nullptr};
    IWbemServices* wbemServices{nullptr};

    try{
        if(!initializeCom())
            throw &quot;initializeCom failed&quot;;

        if(!setUpWBEM(wbemLocator,wbemServices))
            throw &quot;setUpWBEM failed&quot;;

        // Step 6: --------------------------------------------------
        // Use the IWbemServices pointer to make requests of WMI ----

        BSTR bstr_wql = SysAllocString(L&quot;WQL&quot; );
        BSTR bstr_sql = SysAllocString(L&quot;SELECT AllowCamera FROM MDM_Policy_Result01_Camera02&quot; ); 

        // For example, get the name of the operating system
        IEnumWbemClassObject* pEnumerator{nullptr};
        HRESULT hres = wbemServices-&gt;ExecQuery(
            bstr_wql, 
            bstr_sql,
            WBEM_FLAG_FORWARD_ONLY | WBEM_FLAG_RETURN_IMMEDIATELY, 
            nullptr,
            &amp;pEnumerator);
    
        if (FAILED(hres))
        {
            std::cout &lt;&lt; &quot;Query for operating system name failed.&quot;
                &lt;&lt; &quot; Error code = 0x&quot; 
                &lt;&lt; std::hex &lt;&lt; hres &lt;&lt; std::endl;
            wbemServices-&gt;Release();
            wbemLocator-&gt;Release();
            CoUninitialize();
            throw &quot;ExecQuery failed&quot;;;               // Program has failed.
        }

        // Step 7: -------------------------------------------------
        // Get the data from the query in step 6 -------------------

        IWbemClassObject *pclsObj{nullptr};
        ULONG uReturn = 0;

        while (pEnumerator)
        {
            HRESULT hr = pEnumerator-&gt;Next(WBEM_INFINITE, 1, 
                &amp;pclsObj, &amp;uReturn);

            if(0 == uReturn)
            {
                break;
            }

            VARIANT vtProp;

            // Get the value of the Name property
            hr = pclsObj-&gt;Get(L&quot;AllowCamera&quot;, 0, &amp;vtProp, 0, 0);

            if(FAILED(hr))
                std::cout &lt;&lt; &quot;Failed to get name &quot; &lt;&lt; std::endl;

            std::cout &lt;&lt; &quot;Camera allow : &quot; &lt;&lt; vtProp.intVal &lt;&lt; std::endl;
            VariantClear(&amp;vtProp);
        
            pclsObj-&gt;Release();
        }

        // Cleanup
        // ========
    
        wbemServices-&gt;Release();
        wbemLocator-&gt;Release();
        pEnumerator-&gt;Release();
        CoUninitialize();

    } catch(const std::string&amp; error){
        std::cout &lt;&lt; error &lt;&lt; std::endl;
    }
    return 0;
}
</code></pre>
<p>Changing <code>SELECT AllowCamera FROM MDM_Policy_Result01_Camera02</code> with <code>UPDATE MDM_Policy_Result01_Camera02 SET AllowCamera=0</code> doesn't work ...</p>
<p>If you have any idea, let me know !</p>
","<c++><windows><winapi><wmi><mdm>","2021-10-12 19:17:57","129","0","1","69617077","<p>Finally came out the MDM brigde WSI C++ example :
<a href=""https://learn.microsoft.com/fr-fr/windows/win32/wmisdk/example--calling-a-provider-method"" rel=""nofollow noreferrer"">https://learn.microsoft.com/fr-fr/windows/win32/wmisdk/example--calling-a-provider-method</a></p>
"
"69491442","present data flow on excel histogram","<p>I am trying to show the flow of passengers from different stations. The data table looks like:</p>
<pre><code>station_from | quantity | station_to
A 10 B
A 10 C
B 5 A
A 10 B
</code></pre>
<p>So I would like to get such histogram:</p>
<p><a href=""https://i.stack.imgur.com/Em2Bk.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Em2Bk.jpg"" alt=""enter image description here"" /></a></p>
<p>Could you write me any tips to get it in excel?</p>
","<excel>","2021-10-08 06:28:46","39","0","1","69493117","<p>You need to get your data into two columns like this so you can select E1:G4 and draw a column chart with two series:</p>
<p><a href=""https://i.stack.imgur.com/RbgqD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RbgqD.png"" alt=""enter image description here"" /></a></p>
<p>There are a number of ways of getting a list of all the station names without duplicates in  column E, but once you have done this you can just use a simple formula to get the total in for each station in F2 :</p>
<pre><code>=SUMIFS(B$2:B$5,C$2:C$5,E2)
</code></pre>
<p>and the total out in G2</p>
<pre><code>=SUMIFS(B$2:B$5,A$2:A$5,E2)
</code></pre>
<p>and copied down.</p>
"
"69476272","Unchange Data, hashing data","<p>I want to save some data in a file like .cvs. One of the condition is, that the data can't change after write in the file.
If I want to read the data, it should be save that the data were not change in the past and I want to get an alert, if the data changed in the past by other.</p>
<p>For example a line in my .csv looks like this:</p>
<pre><code>34, 234, test, monday, peter
</code></pre>
<p>How can I check, that line changed by somebody?<br />
How can I lock, that nobody changed one of my lines?</p>
<p>I hope you know what I mean.
If use google-search in context of this topic, I will often meet the topic of &quot;hashing my data&quot;.</p>
<p>How does it work?</p>
","<hash><data-integrity><data-integration><data-security><universal-hashing>","2021-10-07 06:18:10","32","0","1","69574920","<p>Hashing is the concept of taking a lot of data and reducing to a single (much smaller) string or number. So, there are various algorithms that would take the whole file and come up with a hash. For example, here is how you might do that in <a href=""https://ilikekillnerds.com/2020/04/how-to-get-the-hash-of-a-file-in-node-js/"" rel=""nofollow noreferrer"">Node</a>.</p>
<p>The most important aspect of the algorithm is that if the contents change in any way, then the hash will also change. This is how git works, for example.</p>
<p>So, what you could do it hash that file and save the resulting string somewhere and then compare the results later. You may have seen this is installer downloads over the years. Sometimes they come with a hash that you could check yourself to see if it's been tampered with.</p>
"
"69448808","Azure Data Factory data flow expression unexpectedly scrambled","<p>I have an usual derive Columns activity in my ADF data flow, which is simply concatenation of some fields and should be error-free.</p>
<p>The expression in expression builder:
<a href=""https://i.stack.imgur.com/5AMDW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5AMDW.png"" alt=""enter image description here"" /></a></p>
<p>First unexpected behaviour I observed is that, there is no data preview available in the expression builder (while other fields in same derive column activity have):
<a href=""https://i.stack.imgur.com/zxCdU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zxCdU.png"" alt=""enter image description here"" /></a>
When I press the &quot;Save and finish&quot; button, the display in the Derived Column's settings show error in the expression:
<a href=""https://i.stack.imgur.com/yFPwR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yFPwR.png"" alt=""enter image description here"" /></a>
I tried to investigate and reopen the expression builder, the expression is unexpectedly scrambled:
<a href=""https://i.stack.imgur.com/3DQ8v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3DQ8v.png"" alt=""enter image description here"" /></a></p>
<p>Is there any explanation for the behaviour? And do we have some fix / workaround for me to keep the original expression?</p>
<p>Many thanks in advance</p>
","<azure-data-factory>","2021-10-05 10:26:13","604","0","1","69481965","<p>Use <strong>curl</strong> brackets in the expression to include the columns.</p>
<pre><code>toString({ContactType@ID}) + '_' + toString({ContactType@Name}) + '_' + toString({ContactType@ModifiedDate})
</code></pre>
<p><a href=""https://i.stack.imgur.com/Cllao.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Cllao.png"" alt=""enter image description here"" /></a></p>
<p>You can also use the <strong>CONCAT</strong> function to combine multiple string values to get a value.</p>
<pre><code>concat(toString({ContactType@ID}) , '_' , toString({ContactType@Name}) , '_' , toString({ContactType@ModifiedDate}))
</code></pre>
<p><a href=""https://i.stack.imgur.com/jUp4k.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jUp4k.png"" alt=""enter image description here"" /></a></p>
"
"69448216","Azure Data Factory ForEach is seemingly not running data flow in parallel","<p>In Azure Data Factory I am using a Lookup activity to get a list of files to download, then pass it to a ForEach where a dataflow is processing each file</p>
<p>I do not have 'Sequential' mode turned on, I would assume that the data flows should be running in parallel. However, their runtimes are not the same but actually have almost constant time between them (like, first data flow ran 4 mins, second 6, third 8 and so on). It seems as if the second data flow is waiting for the first one to finish and then uses its cluster to process the file.</p>
<p>Is that intended behavior? I have TTL on the cluster set but that did not help too much. If it is, then what is a workaround? I am currently working on creating a list of files first and using that instead of a ForEach but I am not sure if I am going to see an increase in efficiency</p>
","<azure-data-factory>","2021-10-05 09:44:33","1241","0","1","69475001","<p>I have not been able to solve the issue with the Parallel data flows not executing in parallel, however, I have managed to change the solution that would increase performance.</p>
<p>What was before: A lookup activity that would get a list of files to process, passed on to a ForEach loop with a data flow activity.</p>
<p>What I am testing now: A Data flow activity that would get a list of files, and save them in a text file in ADLS, Then another data flow activity that was previously in a ForEach loop, but changed its source to use &quot;List of Files&quot; and point to that list</p>
<p>The result was an increase in efficiency (Using the same cluster, 40 files would take around 80 mins using ForEach and only 2-3 mins using List of Files), however, debugging is not easy now that everything is in 1 data flow</p>
<p>You can overwrite a list of files file, or use dynamic expressions and name the file as the pipelineId or something else</p>
"
"69442066","data factory - data flow delete row in parquet - sink","<p>I have the upsert if working for the .parquet files by selecting the upsert if setting to true()For example when the source is update ,then the parquet sink is updated accordingly.But if a row is deleted in source it is still shown in sink parquet even when check the Allow Delete.Any suggestions?Thank you</p>
","<azure><azure-data-factory>","2021-10-04 20:27:51","727","0","1","69465184","<p>Just like <code>Upsert if</code> is set to <code>true()</code>, you have to set <code>Delete if</code> to <code>true()</code>.</p>
<p>I have tried similar use case and I successfully deleted row in Sink which was also deleted in Source.</p>
<p><strong>Step1:</strong> I have used CSV file as Source with 10 records.
<a href=""https://i.stack.imgur.com/hKIxC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hKIxC.png"" alt=""enter image description here"" /></a></p>
<p><strong>Step2:</strong> Created data flow with AlterRow activity.</p>
<p>Added 2 conditions</p>
<pre><code>Upsert if true()

Delete if true()
</code></pre>
<p><a href=""https://i.stack.imgur.com/9yjfr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9yjfr.png"" alt=""enter image description here"" /></a></p>
<p><strong>Step3:</strong> Here I have stored data in parquet file. All 10 records inserted.</p>
<p><a href=""https://i.stack.imgur.com/fzDox.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fzDox.png"" alt=""enter image description here"" /></a></p>
<p><strong>Step4:</strong> Now I have deleted first row from my source file.</p>
<p><a href=""https://i.stack.imgur.com/REC38.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/REC38.png"" alt=""enter image description here"" /></a></p>
<p><strong>Step5:</strong> Now there are only 9 records in Sink as shown below.</p>
<p><a href=""https://i.stack.imgur.com/63D0t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/63D0t.png"" alt=""enter image description here"" /></a></p>
"
"69435639","How can I implement size based batching instead of time based for BoundedSource in apache beam/data flow?","<p><strong>Intention</strong>: I have a bounded data set (Rest API exposed, 10k records) to be written to BigQuery with some additional steps. As my data set is bounded I've implemented <a href=""https://beam.apache.org/releases/javadoc/2.4.0/org/apache/beam/sdk/io/BoundedSource.html"" rel=""nofollow noreferrer"">BoundedSource</a> interface to read records in my apache beam pipeline.</p>
<p><strong>Problem</strong>: all 10k records are read in one shot (one shot for write to BigQiery as well). But I want to query small part (for example 200 rows), process, save to BigQuery and then query next 200 rows.</p>
<p>I've found that I can use <a href=""https://beam.apache.org/documentation/programming-guide/#windowing-bounded-collections"" rel=""nofollow noreferrer"">windowing with bounded PCollections</a>, but windows are created on time basis (every 10 sec for example) and I want it to be on record counter basis (every 200 records)</p>
<p><strong>Question</strong>: How can I implement the mentioned splitting to batches/windows with 200 records size? Am I missing something?</p>
<p>The question is similar to <a href=""https://stackoverflow.com/questions/53845681/fetching-records-based-on-batch-size-in-apache-beam"">this</a> but it wasn't answered</p>
","<java><apache-beam><dataflow>","2021-10-04 11:58:57","360","0","1","69442042","<p>Given a PCollection of rows, you can use <a href=""https://beam.apache.org/releases/javadoc/2.30.0/org/apache/beam/sdk/transforms/GroupIntoBatches.html"" rel=""nofollow noreferrer"">GroupIntoBatches</a> to batch these up into a PCollection of sets of rows of a given size.</p>
<p>As for reading your input in an incremental way, you can use the split method of BoundedSource to shard your read into several pieces which will then be read independently (possibly on separate workers). For a bounded pipeline, this will still happen in its entirety (all 10k records read) before anything is written, but need not happen on a single worker.  You could also insert a <a href=""https://beam.apache.org/releases/javadoc/2.32.0/org/apache/beam/sdk/transforms/Reshuffle.html"" rel=""nofollow noreferrer"">Reshuffle</a> to decouple the parallelism between your read and your write.</p>
"
"69407476","Pentaho data integration - Replace in String (deleting everthing after a simbol)","<p>I´m new on Pentaho and in one of my fields, I have to delete everything after the first &quot; simbol, just like the example:</p>
<p>CTe35210915180327000160570050000752951251590035&quot; versao=&quot;3.00&quot;</p>
<p>I´m trying to use the replace in string using RegEx, but I´m not sure how... anyone can help me?</p>
<p>Best regards,
Ana</p>
","<replace><pdi><step>","2021-10-01 14:22:03","496","0","1","69422452","<p>You can use step &quot;Modified JavaScript value&quot; and using one simple line [Yourstring.match(/[^&quot;]*/i)[0]] you can do this.</p>
<p>You can look this <a href=""https://drive.google.com/drive/folders/1Z5CWxgUUtA3ITwhkgclQme4HX4MBYkG9?usp=sharing"" rel=""nofollow noreferrer"">Transformation</a>.</p>
"
"69383266","Issue with PolledProcessor on Spring cloud data flow","<p>I'm implememting a spring cloud data flow processor using PolledProcessor. I followed the example here <a href=""https://spring.io/blog/2018/02/27/spring-cloud-stream-2-0-polled-consumers"" rel=""nofollow noreferrer"">https://spring.io/blog/2018/02/27/spring-cloud-stream-2-0-polled-consumers</a>. Below is my code. I deployed a stream with a source piping to this processor (source | polled-processor) to scdf, and have the source published some messages. I confirm that the processor polls message from the scdf rabbitmq every second, but the <code>result</code> is always <code>false</code>. I went to the scdf rabbitmq console, I see those messages are all in the queue. So the processor is not polling the message although it keeps polling in the code. I also see there is no consumer for the queue. Looks like scdf did not bind this processor to the queue. Any idea why?</p>
<pre><code>public interface PolledProcessor {
    @Input
    PollableMessageSource source();

    @Output
    MessageChannel dest();
}

@SpringBootApplication
@EnableBinding(PolledProcessor.class)
public class Application {

    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }

    @Bean
    public ApplicationRunner runner(PollableMessageSource source, MessageChannel dest) {
        return args -&gt; {
            while (true) {
                boolean result = source.poll(dest::send);
                Thread.sleep(1000);
            }
        };
    }
}

</code></pre>
<p>here is the status of the queue between the source and the processor</p>
<p><a href=""https://i.stack.imgur.com/D2sPs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D2sPs.png"" alt=""enter image description here"" /></a></p>
","<spring-cloud-stream><spring-cloud-dataflow>","2021-09-29 20:56:15","200","0","1","69392967","<p>I've tested a Spring Cloud Stream app with no problems:</p>
<pre class=""lang-java prettyprint-override""><code>@SpringBootApplication
@EnableBinding(Polled.class)
public class So69383266Application {

    public static void main(String[] args) {
        SpringApplication.run(So69383266Application.class, args);
    }

    @Bean
    public ApplicationRunner runner(PollableMessageSource source) {
        return args -&gt; {
            while (true) {
                boolean result = source.poll(System.out::println);
                System.out.println(result);
                Thread.sleep(1000);
            }
        };
    }

}

interface Polled {

    @Input
    PollableMessageSource source();

}
</code></pre>
<pre><code>false
GenericMessage [payload=byte[6], headers={...
true
false
</code></pre>
<p>I suggest you set a breakpoint in <code>AmqpMessageSource.doReceive()</code> to see what's going on.</p>
<p><strong>EDIT</strong></p>
<p>Here's how to check that the source is consuming from the correct queue:</p>
<pre class=""lang-java prettyprint-override""><code>@Bean
public ApplicationRunner runner(PollableMessageSource source) {
    return args -&gt; {
        while (true) {
            DirectFieldAccessor dfa = new DirectFieldAccessor(source);
            log.info(dfa.getPropertyValue(&quot;source.h.advised.targetSource.target.queue&quot;).toString());
            boolean result = source.poll(System.out::println);
            System.out.println(result);
            Thread.sleep(1000);
        }
    };
}
</code></pre>
"
"69366825","Optimizing a XQuery-heavy SQL query used in SSIS data flow task","<p>This query is used to import around 600,000 rows into a target table in a data warehouse every night. The target table is truncated before each import.</p>
<pre><code>SELECT -- Around 70 fields from MainTable which contains around 600,000 rows
-- Around 150 fields from around 50 various tables, some quite big
-- Around 35 fields from XQuery derived table queries such as dt_EXTERNAL_CODE1
FROM MainTable
LEFT JOIN -- Around 50 tables
LEFT JOIN
(
SELECT df.ParentID,
ISNULL(df2.XMLValue.value('(Item/*[local-name()=&quot;CustomData&quot;]/root/A/Number)[1]', 'float'),0) AS a, 
ISNULL(df2.XMLValue.value('(Item/*[local-name()=&quot;CustomData&quot;]/root/B/Number)[1]', 'float'),0) AS b,
ISNULL(df2.XMLValue.value('(Item/*[local-name()=&quot;CustomData&quot;]/root/C/Number)[1]', 'float'),0) AS c,
ISNULL(df2.XMLValue.value('(Item/*[local-name()=&quot;CustomData&quot;]/root/D/Number)[1]', 'float'),0) AS d,
ISNULL(df2.XMLValue.value('(Item/*[local-name()=&quot;CustomData&quot;]/root/E/Number)[1]', 'float'),0) AS e,
ISNULL(df2.XMLValue.value('(Item/*[local-name()=&quot;CustomData&quot;]/root/F/Number)[1]', 'float'),0) AS f
FROM DynamicField df
INNER JOIN DynamicField df1 ON df.DynamicFieldID = df1.DynamicFieldID
INNER JOIN DynamicField df2 ON df1.DynamicFieldID = df2.ParentID
WHERE df2.XMLValue.value('(Item/*[local-name()=&quot;ExternalCode&quot;])[1]', 'nvarchar(50)') IN('EXTERNAL_CODE1')
) dt_EXTERNAL_CODE1 ON MainTable.DynamicFieldID = dt_EXTERNAL_CODE1.ParentID
LEFT JOIN -- 6 more like the derived table query above, but with some other external code
</code></pre>
<p>The SSIS import job takes about 10 hours to complete. Any suggestions on how to optimize this query? The joins cannot be inner joins.</p>
","<sql><sql-server><ssis><query-optimization><xquery>","2021-09-28 18:28:18","111","0","1","69367752","<p>I just want to mention, this advice is without considering possible indexes that could be added to your XML in the database. Depending on the other six queries that you didn't show and many other factors, indexing the XML data might be a good thing to do as well. The advice I gave here is really just general X-query advice that would apply to almost any X-query expressions.</p>
<p>Also worth noting that storing and using XML to filter like this in a SQL database along with relational data is a BAD idea, especially when you plan on doing large scale ETL solutions using that data. As you've already experienced, it is going to be a hassle. If you are still at the stage where this could be changed, I would STRONGLY advise that you do so.</p>
<p>That aside, here a couple of suggestions:</p>
<p>First, the filter expression <code>WHERE df2.XMLValue.value('(Item/*[local-name()=&quot;ExternalCode&quot;])[1]', 'nvarchar(50)') IN('EXTERNAL_CODE1')</code> should be rewritten to use the <code>exist</code> operator (Microsoft doc <a href=""https://learn.microsoft.com/en-us/sql/t-sql/xml/exist-method-xml-data-type?view=sql-server-ver15"" rel=""nofollow noreferrer"">here</a>). Per Microsoft (<a href=""https://learn.microsoft.com/en-us/sql/t-sql/xml/value-method-xml-data-type?view=sql-server-ver15#d-using-the-exist-method-instead-of-the-value-method"" rel=""nofollow noreferrer"">here</a>):</p>
<blockquote>
<p>For performance reasons, instead of using the value() method in a predicate to compare with a relational value, use exist() with sql:column()</p>
</blockquote>
<p>Second, I would move the newly created <code>exist</code> expression to the join clause instead of the <code>WHERE</code> clause. When I view this query, the optimizer might be applying your filter to the entire <code>DynamicField df2</code> table prior to actually performing the join. Depending on the cardinality of these joins, that could be really detrimental to performance. I'd imagine you want this filter to only be performed for rows returned from your expression <code>FROM DynamicField df INNER JOIN DynamicField df1 ON df.DynamicFieldID = df1.DynamicFieldID</code>. The main point here is <strong>Reducing the number of records that are going to use any XML filtering is going to seriously help performance</strong>.</p>
<p>Third, each call to <code>value()</code> is going to instantiate a new XML reader that is required to traverse the path <code>(Item/*[local-name()=&quot;CustomData&quot;]/root/D/Number)</code>. Reducing the amount of work that each instance of a XML reader has to do to retrieve the value required for your <code>SELECT</code> will go a long way for performance. If you have a repetitive path you are traversing (like your example) you would likely be better off using an additional <code>OUTER APPLY</code> operator to a <code>query</code> call to retrieve the XML element <code>root</code> as a separate node, then using that new node in your <code>value</code> statements in the final <code>SELECT</code>. Something like this:</p>
<pre><code>SELECT 
   df.ParentID
    ,ISNULL(root.RootXmlFrag.value('(root/A/Number)[1]', 'float'),0) AS a
    ...... 
FROM 
    DynamicField df
    INNER JOIN DynamicField df1 ON df.DynamicFieldID = df1.DynamicFieldID
    INNER JOIN DynamicField df2 ON df1.DynamicFieldID = df2.ParentID
    OUTER APPLY df2.XMLValue.query('(Item/*[local-name()=&quot;CustomData&quot;]/root)[1]') AS root(RootXmlFrag)
</code></pre>
<p>The actual final <code>query</code> path expression might vary, but working with the idea that you don't want to have to traverse a complicated path like <code>(Item/*[local-name()=&quot;CustomData&quot;]/root/D/Number)</code> for each <code>value</code> expression will certainly help performance in the end.</p>
"
"69364138","How to mock rabbitmq to test the standalone Spring cloud data flow processor?","<p>I have a SCDF processor using PolledProcessor to poll from the input rabbitmq every second. Is there a way to mock the input rabbitmq so that I can verify the polling behavior?</p>
","<spring-cloud-stream><spring-cloud-dataflow>","2021-09-28 15:00:44","286","0","1","69367157","<p>You don't need to mock RabbitMQ. Use the <code>TestChannelBinder</code> from SpringCloudStream and register a <code>MessageSource&lt;byte[]&gt;</code> to simulate content received from a <code>PollableSource</code>. You need to include the following dependency:</p>
<pre><code>&lt;dependency&gt;
        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
        &lt;artifactId&gt;spring-cloud-stream&lt;/artifactId&gt;
        &lt;type&gt;test-jar&lt;/type&gt;
        &lt;classifier&gt;test-binder&lt;/classifier&gt;
        &lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;
</code></pre>
<p>As an example, see <a href=""https://github.com/spring-cloud/spring-cloud-stream/blob/main/spring-cloud-stream/src/test/java/org/springframework/cloud/stream/binder/PollableConsumerTests.java"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-stream/blob/main/spring-cloud-stream/src/test/java/org/springframework/cloud/stream/binder/PollableConsumerTests.java</a></p>
"
"69319982","Optimizing rendering of data received from getStaticProps (data occasionally not visible on initial render)","<p>I have a function I run on my NextJS application's homepage that takes in the props received from getStaticProps. The only problem is that on iOS and Mac, using the Safari/DuckDuckGo browser, the page will occasionally (about every 5/6 cacheless reloads in incognito) load -all- content....but none of it is visible.</p>
<p>You can still copy and paste the text, you can push down on the images and THEN see them, but the only things visible are the header background color and HTML/Body background color. HOWEVER, it works 100% of the time in Chrome, on all operating systems.</p>
<p>If you think that the problem lies w/i the length and/or methodology of this function, can you please provide me with guidance on how to condense it, or use better practices?</p>
<p>And if you think the problem lies elsewhere, I'd love to know where to start looking. The console doesn't show any problems whatsoever.</p>
<p>Here's the component that is being rendered:</p>
<pre><code>import React, { useState, useEffect } from &quot;react&quot;;
import organizeMenu from &quot;../models/orgMenu&quot;;
import {
  Box,
  Heading,
  SimpleGrid,
  Divider,
  Center,
  Container,
} from &quot;@chakra-ui/react&quot;;
import ItemCard from &quot;./molecules/ItemCard&quot;;
import { useMenuStore } from &quot;../state/store&quot;;
import SearchBar from './search/SearchBar';

const HomeContainer = ({props}) =&gt; {
 

  const { setStateModifierLists } = useMenuStore();
  const modifierLists = props.data.objects.filter(
    (object) =&gt; object.type === &quot;MODIFIER_LIST&quot;
  );

  useEffect(() =&gt; {
    setStateModifierLists(modifierLists);
  }, []);

  const itemList = props.data.objects.filter(
    (object) =&gt; object.type === &quot;ITEM&quot;
  );

  const categories = props.data.objects.filter(
    (object) =&gt; object.type === &quot;CATEGORY&quot;
  );

  const loadThis = organizeMenu(props);

  const bfast = loadThis.bfast;
  const entrees = loadThis.entrees;
  const drinks = loadThis.drinks;
  console.log(`bfast`, bfast);

  const [loaded, setLoaded] = useState(false);

  const handleLoad = (e) =&gt; {
    console.log(&quot;loaded&quot;);
    setLoaded(true);
  };
    return (
        
             &lt;Box w=&quot;100%&quot;&gt;
      &lt;Container&gt;
        &lt;SearchBar categories={categories} itemList={itemList} /&gt;
      &lt;/Container&gt;

      
      &lt;Heading ml={3}&gt;Breakfast&lt;/Heading&gt;
      &lt;Divider /&gt;
      &lt;Center&gt;
        &lt;SimpleGrid
          m=&quot;0 auto&quot;
          alignItems=&quot;center&quot;
          spacing={6}
          p=&quot;2&quot;
          columns={[1, null, 2, null, 3]}
        &gt;
          {bfast.map((b) =&gt; (
            &lt;ItemCard modifierLists={modifierLists} key={b.id} item={b} /&gt;
          ))}
        &lt;/SimpleGrid&gt;
      &lt;/Center&gt;
      &lt;Heading ml={3}&gt;Entrees&lt;/Heading&gt;
      &lt;Divider /&gt;
      &lt;Center&gt;
        &lt;SimpleGrid
          m=&quot;0 auto&quot;
          alignItems=&quot;baseline&quot;
          onLoad={handleLoad}
          spacing={6}
          p=&quot;2&quot;
          columns={[1, null, 2, null, 3]}
        &gt;
          {entrees.map((e) =&gt; (
            &lt;&gt;
              &lt;ItemCard modifierLists={modifierLists} key={e.id} item={e} /&gt;
            &lt;/&gt;
          ))}
        &lt;/SimpleGrid&gt;
      &lt;/Center&gt;
      &lt;Heading ml={3}&gt;Drinks&lt;/Heading&gt;
      &lt;Divider /&gt;
      &lt;Center&gt;
        &lt;SimpleGrid
          m=&quot;0 auto&quot;
          alignItems=&quot;stretch&quot;
          onLoad={handleLoad}
          spacing={6}
          p=&quot;2&quot;
          columns={[1, null, 2, null, 3]}
        &gt;
          {drinks.map((d) =&gt; (
            &lt;ItemCard modifierLists={modifierLists} key={d.id} item={d} /&gt;
          ))}
        &lt;/SimpleGrid&gt;
      &lt;/Center&gt;
    &lt;/Box&gt;
        
    )
}

export default HomeContainer

</code></pre>
<p>This is the function I wrote to organize the data for rendering:</p>
<pre><code>export default function organizeMenu(props) {
  
    // Segment menu items
    let menuItems = [];
  
    menuItems = props.data.objects.filter((object) =&gt; object.type === &quot;ITEM&quot;);

    //Segment menu item images
    let itemImages = [];
    itemImages = props.data.objects.filter((object) =&gt; object.type === &quot;IMAGE&quot;);

    //Segment Categories
    let categories = [];
    categories = props.data.objects.filter(
      (object) =&gt; object.type === &quot;CATEGORY&quot;
    );

    //Segment Modifier Lists
    let modifierLists = [];
    modifierLists = props.data.objects.filter(
      (object) =&gt; object.type === &quot;MODIFIER_LIST&quot;
    );
    
  
    // Merge data to provide better mapping and ordering process
  
    //Looping through menuItems and itemImages to combine fields into menuItems for ease of mapping data to components
  
    for (let x = 0; x &lt; menuItems.length; x++) {
      menuItems[x].modifiers = [];
      menuItems[x].imageData = {
        url: &quot;https://via.placeholder.com/250&quot;,
      };
      for (let y = 0; y &lt; itemImages.length; y++) {
        if (menuItems[x].imageId === itemImages[y].id) {
          // console.log(`Match: ${menuItems[x].imageId}`);
          menuItems[x].imageData = itemImages[y].imageData;
        } else {
          // console.log(&quot;No match&quot;);
        }
      }
    }
  

  
    // Next, we're going to tie the actual modifiers to the menuItem objects, rather than having to map them separately.
  
    for (let mm = 0; mm &lt; menuItems.length; mm++) {
      menuItems[mm].availableModifiers = [];
      if (menuItems[mm].itemData.modifierListInfo) {
        for (
          let xx = 0;
          xx &lt; menuItems[mm].itemData.modifierListInfo.length;
          xx++
        ) {
          if (menuItems[mm].itemData.modifierListInfo[xx].enabled === true) {
            // console.log(&quot;enabled&quot;);
            for (let zz = 0; zz &lt; modifierLists.length; zz++) {
              if (
                menuItems[mm].itemData.modifierListInfo[xx].modifierListId ===
                modifierLists[zz].id
              ) {

                for (
                  let xo = 0;
                  xo &lt; modifierLists[zz].modifierListData.modifiers.length;
                  xo++
                ) {
  
                  menuItems[mm].availableModifiers.push(
                    modifierLists[zz].modifierListData.modifiers[xo]
                  );
                }
  

              }
            }
          } else {
            // console.log(&quot;no mods&quot;);
          }
        }
      }
    }
  
//If modifier has a price, map the price according to the needs of square's api.
  
    for (let qu = 0; qu &lt; menuItems.length; qu++) {
      for (let xz = 0; xz &lt; menuItems[qu].availableModifiers.length; xz++) {
        if (menuItems[qu].availableModifiers[xz].modifierData.priceMoney) {
          menuItems[qu].availableModifiers[xz].basePriceMoney = {
            ...menuItems[qu].availableModifiers[xz].modifierData.priceMoney,
          };
        } else {
          menuItems[qu].availableModifiers[xz].modifierData.basePriceMoney = {
            amount: &quot;0&quot;,
            currency: &quot;USD&quot;,
          };
          menuItems[qu].availableModifiers[xz].basePriceMoney = {
            amount: &quot;0&quot;,
            currency: &quot;USD&quot;,
          };
        }
      }
    }
  
  
// Set primary variation (default)
  
    for (let h = 0; h &lt; menuItems.length; h++) {
      if (
        menuItems[h].itemData.variations[0] &amp;&amp;
        menuItems[h].itemData.variations[0].isDeleted === false
      ) {
        menuItems[h].primaryVariation = {
          ...menuItems[h].itemData.variations[0],
          isChosen: false,
        };
      }
    }
  

  
    // Merging &quot;CATEGORIES&quot; with menuItems
  
    for (let q = 0; q &lt; menuItems.length; q++) {
      menuItems[q].categoryName;
      for (let w = 0; w &lt; categories.length; w++) {
        if (menuItems[q].itemData.categoryId === categories[w].id) {
          menuItems[q].categoryName = categories[w].categoryData.name;
        }
      }
    }
  
    // Separating items into arrays based on category...
  
    let breakfastItems = [];
    breakfastItems = menuItems.filter(
      (object) =&gt; object.categoryName === &quot;Breakfast&quot;
    );

  
    let entreeItems = [];
    entreeItems = menuItems.filter((object) =&gt; object.categoryName === &quot;Entree&quot;);
  
    let drinkItems = [];
    drinkItems = menuItems.filter((object) =&gt; object.categoryName === &quot;Drinks&quot;);

// The object to be returned... Items are rendered from these objects.
    const catalog = {
      bfast: breakfastItems,
      entrees: entreeItems,
      drinks: drinkItems,
    };
  
 
    return catalog;
  }
</code></pre>
<p>When it works:
<a href=""https://i.stack.imgur.com/oG4z1.jpg"" rel=""nofollow noreferrer"">Image 1</a></p>
<p>When it doesn't:
<a href=""https://i.stack.imgur.com/nbGPm.png"" rel=""nofollow noreferrer"">Image 1</a>
<a href=""https://i.stack.imgur.com/f42hw.jpg"" rel=""nofollow noreferrer"">Image 2</a></p>
","<reactjs><function><next.js><rendering><data-mapping>","2021-09-24 19:02:51","92","0","1","69320717","<p>Your issue seems due an understanding of the rendering cycle and what causes components to render. Any of these changes will cause the component to render again:</p>
<ol>
<li>Update to props</li>
<li>Update to state</li>
<li><a href=""https://reactjs.org/docs/context.html"" rel=""nofollow noreferrer"">Context</a> value changes</li>
</ol>
<p>Your <code>const loadThis = organizeMenu(props);</code> is being run synchronously <strong>every time</strong> when one of the above changes (could be triggered by parent component too) and is wasteful and causing UI components to render unnecessarily.</p>
<pre><code>  const loadThis = organizeMenu(props);

  const bfast = loadThis.bfast;
  const entrees = loadThis.entrees;
  const drinks = loadThis.drinks;
  console.log(`bfast`, bfast);
</code></pre>
<p>Move these into <code>state</code>, <code>useEffect</code>, or <strong>ideally</strong> run the <code>organizeMenu</code> functionality at build time using <code>getStaticProps</code> and pass the data to the component as props.</p>
<p>See the simplified example with comments below. Every time you click the button, <code>organizeMenu(props)</code> is run and will output to the console and result in the list rendering again.</p>
<pre><code>export default function IndexPage({ categories, menu }) {
  const [selected, setSelected] = useState(0);
  const [menuList, setMenuList] = useState();

  const organizeMenu = (data) =&gt; {
    console.info('running organizeMenu');

    return data.map((category) =&gt; {
      return {
        id: category,
        name: category.toString()
      };
    });
  };

  // bad, this will be run every time there is a state change i.e. when the button is clicked and setSelected
  const foo = organizeMenu(categories);

  const handleOnClick = () =&gt; {
    setSelected(selected + 1);
  };

  useEffect(() =&gt; {
    console.info('running useEffect');
    setMenuList(
      categories.map((category) =&gt; {
        return {
          id: category,
          name: category.toString()
        };
      })
    );
  }, [categories]);

  return (
    &lt;div&gt;
      Hello World. {selected}
      &lt;br /&gt;
      {menu &amp;&amp; menu.map((menuItem) =&gt; &lt;li key={menuItem.id}&gt;{menuItem.name}&lt;/li&gt;)}
      &lt;br /&gt;
      {menuList &amp;&amp; menuList.map((menuItem) =&gt; &lt;li key={menuItem.id}&gt;{menuItem.name}&lt;/li&gt;)}
      &lt;hr /&gt;
      &lt;button onClick={handleOnClick}&gt;Make it so&lt;/button&gt;
    &lt;/div&gt;
  );
}

export async function getStaticProps(context) {
  const categories = [1, 2, 3];

  // prep menu here so run at build time
  const menu = categories.map((category) =&gt; {
    return {
      id: category,
      name: category.toString()
    };
  });

  return {
    props: {
      categories,
      menu
    }
  };
}
</code></pre>
<p><a href=""https://codesandbox.io/s/so-69319982-c9b24?file=/pages/index.js"" rel=""nofollow noreferrer"">CodesandBox Demo</a></p>
"
"69311896","How to remove extra files when sinking CSV files to Azure Data Lake Gen2 with Azure Data Factory data flow?","<p>I have done data flow tutorial. Sink currently created 4 files to Azure Data Lake Gen2.
I suppose this is related to HDFS file system.</p>
<p>Is it possible to save without success, committed, started files?</p>
<p>What is best practice? Should they be removed after saving to data lake gen2?
Are then needed in further data processing?</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-data-flow"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/tutorial-data-flow</a></p>
","<azure-data-factory>","2021-09-24 08:31:18","473","1","1","69342538","<p>There are a couple of options available.</p>
<ol>
<li><p>You can mention the output filename in Sink transformation settings.</p>
<ul>
<li><p>Select <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-sink#sink-settings"" rel=""nofollow noreferrer""><strong>Output to single file</strong></a> from the dropdown of file name option and give the output file name.</p>
</li>
<li><p>You could also parameterize the output file name as required. Refer to this <a href=""https://stackoverflow.com/questions/61123171/how-to-set-the-file-name-based-on-a-parameter-in-the-sink"">SO</a> thread.</p>
<p><a href=""https://i.stack.imgur.com/q6RIZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q6RIZ.png"" alt=""enter image description here"" /></a></p>
</li>
</ul>
</li>
<li><p>You can add <a href=""https://learn.microsoft.com/en-us/azure/data-factory/delete-activity"" rel=""nofollow noreferrer"">delete activity</a> after the data flow activity in the pipeline and delete the files from the folder.</p>
</li>
</ol>
<p><a href=""https://i.stack.imgur.com/1lYvv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1lYvv.png"" alt=""enter image description here"" /></a></p>
"
"69152012","Java: data synchronization issue","<p>My Java program makes a request to client's API and client's API will return a key-value pair on <strong>irregular</strong> time intervals (i.e. not every 5 seconds / 10 seconds, sometimes 1 second or 5 seconds).</p>
<p>And I have inserted my own code which is a <code>HashMap</code>, into the client API code to store all the key-value pairs.</p>
<p>My goal is run the below code marked with &quot;!!!&quot; as soon as the Hashmap conditions are matched.
I am not an expert in Java data synchronization. Note that <code>Thread.Sleep(3000)</code> would not work, as the key-value pairs are updated on irregular time intervals. Moreover, the value of the same key will also change over time.
I tried the run the program below and it immediately ran through the block of code marked with &quot;!!!&quot;, which is not what I want to achieve.</p>
<p>What is the most efficient solution to tackle this problem ?</p>
<pre><code>Class Testing{
    public static void main(String[] args){
        // connect to API...
        ClientAPI clientAPI = new ClientAPI();
        clientAPI.connect();

        // make a request to retrieve an update of a HashMap
        clientAPI.requestHashMapUpdate();

        // !!! execute the following block of code only if hashmap contains a specific key value pair, i.e. key1, 1 
        // if hashmap does not contain key or the key-value pair do not match , then wait until the key-value pair are matched and run the code
        if(clientAPI.getMyHashMap().containsKey(&quot;key1&quot;)){
            if(clientAPI.getMyHashMap().get(&quot;key1&quot;) == 1){
                    System.out.println(&quot;Print Me only if key and value are matched !!!!!!!!&quot;);
            }
        }

    }
}


Class ClientAPI{
    private HashMap&lt;String,Integer&gt; myHashMap;
    
    // Constructor
    public clientAPI(){
            myHashMap = new HashMap&lt;String,Integer&gt;();
    }

    // callback function of API's requestHashMapUpdate method
    public void updateHashMapKeyValuePair(String key, int value){
        System.out.println(&quot;Key:&quot; + key + &quot;, Value: &quot; + value);
        // code i wrote to update hashmap with key value pair returned from API
        myHashMap.put(key,value);
    }

    // my code to retrieve the latest snapshot of myHashMap
    public HashMap&lt;String,Integer&gt; getMyHashMap(){
        return myHashMap;
    }

}
</code></pre>
","<java><volatile><data-synchronization>","2021-09-12 13:48:05","74","0","2","69152421","<p>Override your client API (if it is not closed to extends):</p>
<pre><code>class MyClientAPI extends ClientAPI {
    ...
    @Override
    public void updateHashMapKeyValuePair(String key, int value) {
        super.updateHashMapKeyValuePair(key, value);
        if(&quot;key1&quot;.equals(key) &amp;&amp; 1 == value)
            System.out.println(&quot;Print Me only if key and value are matched !!!!!!!!&quot;);
    }
    ...
}
</code></pre>
<p>and simply check each new value that comes in.</p>
<p>To use it, only change</p>
<pre><code>ClientAPI clientAPI = new MyClientAPI();
</code></pre>
<p>Another way to solve this is to provide your <code>ClientAPI</code> class with the ability to register listeners</p>
<pre><code>interface UpdateHashMapKeyValuePairListener {
    void digestUpdate(String key, int value);
}

class ClientAPI {
    ...
    private List&lt;UpdateHashMapKeyValuePairListener&gt; listeners = new ArrayList();
    ...
    public void registerUpdateListener(UpdateHashMapKeyValuePairListener u) {
        listeners.add(u);
    }
    ...
    public void updateHashMapKeyValuePair(final String key, final int value) {
        listeners.forEach(u -&gt; u.digestUpdate(key, value));
        ...
    }
    ...
}
</code></pre>
<p>then, anyone who wants to know when a new value enters, just implement this interface, e.g.</p>
<pre><code>...
clientAPI.registerUpdateListener((key, value) -&gt; {
    if(&quot;key1&quot;.equals(key) &amp;&amp; 1 == value)
        System.out.println(&quot;Print Me only if key and value are matched !!!!!!!!&quot;);
});
clientAPI.connect();
...
</code></pre>
"
"69152012","Java: data synchronization issue","<p>My Java program makes a request to client's API and client's API will return a key-value pair on <strong>irregular</strong> time intervals (i.e. not every 5 seconds / 10 seconds, sometimes 1 second or 5 seconds).</p>
<p>And I have inserted my own code which is a <code>HashMap</code>, into the client API code to store all the key-value pairs.</p>
<p>My goal is run the below code marked with &quot;!!!&quot; as soon as the Hashmap conditions are matched.
I am not an expert in Java data synchronization. Note that <code>Thread.Sleep(3000)</code> would not work, as the key-value pairs are updated on irregular time intervals. Moreover, the value of the same key will also change over time.
I tried the run the program below and it immediately ran through the block of code marked with &quot;!!!&quot;, which is not what I want to achieve.</p>
<p>What is the most efficient solution to tackle this problem ?</p>
<pre><code>Class Testing{
    public static void main(String[] args){
        // connect to API...
        ClientAPI clientAPI = new ClientAPI();
        clientAPI.connect();

        // make a request to retrieve an update of a HashMap
        clientAPI.requestHashMapUpdate();

        // !!! execute the following block of code only if hashmap contains a specific key value pair, i.e. key1, 1 
        // if hashmap does not contain key or the key-value pair do not match , then wait until the key-value pair are matched and run the code
        if(clientAPI.getMyHashMap().containsKey(&quot;key1&quot;)){
            if(clientAPI.getMyHashMap().get(&quot;key1&quot;) == 1){
                    System.out.println(&quot;Print Me only if key and value are matched !!!!!!!!&quot;);
            }
        }

    }
}


Class ClientAPI{
    private HashMap&lt;String,Integer&gt; myHashMap;
    
    // Constructor
    public clientAPI(){
            myHashMap = new HashMap&lt;String,Integer&gt;();
    }

    // callback function of API's requestHashMapUpdate method
    public void updateHashMapKeyValuePair(String key, int value){
        System.out.println(&quot;Key:&quot; + key + &quot;, Value: &quot; + value);
        // code i wrote to update hashmap with key value pair returned from API
        myHashMap.put(key,value);
    }

    // my code to retrieve the latest snapshot of myHashMap
    public HashMap&lt;String,Integer&gt; getMyHashMap(){
        return myHashMap;
    }

}
</code></pre>
","<java><volatile><data-synchronization>","2021-09-12 13:48:05","74","0","2","69152875","<p>Check the contents of the HashMap every time it is updated. To achieve this, you can register a listener in <code>Testing</code> class to be triggered every time the <code>HashMap</code> is updated in <code>ClientAPI</code> class.</p>
<p>First define the functional interface for the listener:</p>
<pre><code> @FunctionalInterface
  public interface OnUpdateMapListener {
      void onUpdateMap(Map&lt;String, Integer&gt; map);
  }
</code></pre>
<p>then add the listener in the <code>ClientAPI</code></p>
<pre><code> public class ClientAPI {
        private HashMap&lt;String, Integer&gt; myHashMap;
        private OnUpdateMapListener onUpdateMapListener;
</code></pre>
<p>Define <code>onMapUpdate</code> method to pass the listener's body in <code>ClientAPI</code>:</p>
<pre><code>public void onMapUpdate(OnUpdateMapListener onUpdateMapListener) {
        this.onUpdateMapListener = onUpdateMapListener;
}
</code></pre>
<p>and trigger the listener upon HashMap update in <code>updateHashMapKeyValuePair</code></p>
<pre><code>public void updateHashMapKeyValuePair(String key, int value) {
  System.out.println(&quot;Key:&quot; + key + &quot;, Value: &quot; + value);
  // code i wrote to update hashmap with key value pair returned from API
   myHashMap.put(key, value);
   onUpdateMapListener.onUpdateMap(myHashMap);
 }
</code></pre>
<p>In the main method register the listener and check the map contents. This will be triggered every time the <code>ClientAPI</code> receives new Map contents in <code>updateHashMapKeyValuePair</code> method:</p>
<pre><code>clientAPI.onMapUpdate(stringIntegerHashMap -&gt; {
      // !!! the following code is executed every time the hashMap is updated
       if (clientAPI.getMyHashMap().containsKey(&quot;key1&quot;)) {
           if (clientAPI.getMyHashMap().get(&quot;key1&quot;) == 1) {
             System.out.println(&quot;Print Me only if key and value are matched !!!!!!!!&quot;);
           }
      }
});
</code></pre>
<p><code>ClientAPI</code>  class:</p>
<pre><code>public class ClientAPI {
    private HashMap&lt;String, Integer&gt; myHashMap;

    private OnUpdateMapListener onUpdateMapListener;

    // Constructor
    public ClientAPI() {
        myHashMap = new HashMap&lt;String, Integer&gt;();
    }

    // callback function of API's requestHashMapUpdate method
    public void updateHashMapKeyValuePair(String key, int value) {
        System.out.println(&quot;Key:&quot; + key + &quot;, Value: &quot; + value);
        // code i wrote to update hashmap with key value pair returned from API
        myHashMap.put(key, value);
        onUpdateMapListener.onUpdateMap(myHashMap);
    }

    // my code to retrieve the latest snapshot of myHashMap
    public HashMap&lt;String, Integer&gt; getMyHashMap() {
        return myHashMap;
    }

    public void requestHashMapUpdate() {
      //.....
    }

    public void onMapUpdate(OnUpdateMapListener onUpdateMapListener) {
        this.onUpdateMapListener = onUpdateMapListener;
    }
}
</code></pre>
<p><code>Testing</code> class:</p>
<pre><code> public static void main(String[] args) {
      // connect to API...
        ClientAPI clientAPI = new ClientAPI();
        clientAPI.connect();

        // make a request to retrieve an update of a HashMap
       clientAPI.requestHashMapUpdate();

        clientAPI.onMapUpdate(stringIntegerHashMap -&gt; {
            // !!! the following code is executed every time the hashMap is updated
            if (clientAPI.getMyHashMap().containsKey(&quot;key1&quot;)) {
                if (clientAPI.getMyHashMap().get(&quot;key1&quot;) == 1) {
                    System.out.println(&quot;Print Me only if key and value are matched !!!!!!!!&quot;);
                }
            }
        });

        clientAPI.updateHashMapKeyValuePair(&quot;key1&quot;, 1);
        clientAPI.updateHashMapKeyValuePair(&quot;key1&quot;, 2);
    }
</code></pre>
<p>Results:</p>
<pre><code>Key:key1, Value: 1
Print Me only if key and value are matched !!!!!!!!
Key:key1, Value: 2
</code></pre>
"
"69113231","How to implement SSO in Xamarin Forms App?","<p>I am trying to implement SSO feature in my app (Xamarin.Forms App - both of ios and android). Login flow is: when user opens login page, she/he will see a button as login with sso. And if user chooses this button, app will open a webview for sso and when sso authentication is completed (it works mdm solution such as airwatch etc.), returns a SAML token.</p>
<p>My problem is how can I implement this, how can I capture saml token from webview? Actually there is a way for this in xamarin because of I see a video in xamarin evolve conferance:
<a href=""https://www.youtube.com/watch?v=AAAQJgBDK0w&amp;t=1163s"" rel=""noreferrer"">https://www.youtube.com/watch?v=AAAQJgBDK0w&amp;t=1163s</a></p>
<p>When AuthTpye was set as &quot;SAML&quot; (in video 15:51 second) and when open the app login with airwatch sso on webview (in video 16:45-17:14 seconds), user can login with sso. But I only could find this video. I couldn't find any other source for this implementation.</p>
<p>Furthermore I saw some Stack Overflow questions for this, and their answers say use Xamarin.Auth nuget package. But Xamarin.Auth uses oauth2.0 protocol. I need use saml protocol for sso.</p>
<p>How can I do this?</p>
","<xamarin.forms><xamarin.android><xamarin.ios><single-sign-on><mdm>","2021-09-09 06:27:21","962","5","1","70161640","<p>Xamarin.Essentials has a class called &quot;WebAuthenticator&quot; that will help you a lot.</p>
<p>Documentation can be found here: <a href=""https://learn.microsoft.com/en-us/xamarin/essentials/web-authenticator?tabs=android"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/xamarin/essentials/web-authenticator?tabs=android</a></p>
<p>It makes the process very simple. You just send the login request:</p>
<pre><code>var authResult = await WebAuthenticator.AuthenticateAsync(loginUrl, responseUrl);
</code></pre>
<p>The &quot;authResult&quot; variable would hold your token and claims.</p>
<p>You will also need to implement an activity or method (depending on platform) to handle the custom URL you passed in as &quot;responseUrl&quot; in the code above and relay the response back to the Xamarin.Essentials library. I strongly encourage you to read the docs linked above. They are very helpful.</p>
"
"69068973","How to represent a duplicate data store in a data flow diagram (DFD) using Yourdon DeMarco notation?","<p>While searching for the answer, I found that in the Gane &amp; Sarson notation, duplicate data stores are indicated by adding an extra line. Another website suggested adding an asterisk next to the data store name. However, I was unable to find any answers specific to the Yourdon DeMarco notation.</p>
<p>Is there a generally accepted way to represent duplicates in the Yourdon DeMarco notation? Or should I adopt one of the methods stated above?</p>
","<dataflow-diagram>","2021-09-06 03:56:58","317","0","1","69081705","<p>I asked this question in StackExchange and got an <a href=""https://softwareengineering.stackexchange.com/questions/431677/how-to-represent-a-duplicate-data-store-in-a-data-flow-diagram-dfd-in-the-your"">answer</a>.</p>
<p>To summarise the answer, the Yourdon Demarco notation avoids duplication where possible because it makes it difficult to visually follow the flow, and this is the practice that should be adopted if possible.</p>
<p>However, in cases where duplication cannot be avoided, there are 2 ways to represent it:</p>
<ol>
<li><p>Make no graphical difference between the duplicate stores. This was the approach used in the Yourdon &amp; Demarco book.</p>
</li>
<li><p>Use an asterisk exponent behind the store name, and add a legend for the asterisk. This representation can be used if attention needs to be drawn to the duplication.</p>
</li>
</ol>
<p>(Edit: added more details)</p>
"
"69060321","In a Data Flow Diagram, can 2 entities use the same process (with the same incoming data flow)?","<p>For a school project, I need to make a DFD for an educational game, where both students and teachers need to be able to view the game's leaderboard.</p>
<p>Currently in my diagram, &quot;Student&quot; and &quot;Teacher&quot; are seperate external entities, &quot;View leaderboard&quot; is a process, and the leaderboard data is stored in a data store called &quot;Leaderboard&quot;.</p>
<p>To represent this, can I use the same process for both entities:</p>
<p><img src=""https://i.stack.imgur.com/ewS2e.jpg"" alt="""" /></p>
<p>or must I create separate processes for each entity like this:</p>
<p><img src=""https://i.stack.imgur.com/uifa3.jpg"" alt="""" /></p>
","<dataflow-diagram>","2021-09-05 03:11:53","310","-2","1","69081683","<p>I asked this question on StackExchange and got an <a href=""https://softwareengineering.stackexchange.com/questions/431675/in-a-data-flow-diagram-can-2-entities-use-the-same-process-with-the-same-incom"">answer</a>.</p>
<p>To summarise the answer, in the Yourdon &amp; Demarco DFD notation, there are 2 ways I can represent the situation above:</p>
<ol>
<li><p>I could use a single process, like depicted in my first picture. In order to indicate that only one entity receives data at a time, I can put a <code>+</code> symbol in a rounded circle between the two outgoing flows, which would mean “or”.</p>
</li>
<li><p>I could create a new external entity that would be a combination of the Student and Teacher entities. That way, I would only have to use a single process.</p>
</li>
</ol>
"
"69030775","Azure Data Factory data flow writing to sink also creates an empty blob file","<p>I'm new with Azure Data Factory and their data flows. I've created a data flow that compares existing and new data based on an md5hash and writes the inserts/updates to a parquet file in ADLS. However, when writing to the sink with folder path: containername/target with filename option Pattern and the pattern being concat($TargetFileName, toString(currentUTC())), it created a folder 'target' (as I would expect) but also an empty blob named 'target', which I don't understand and can't seem to figure out why it also creates a file.
Any suggestions on how to stop it from creating the target file?</p>
<p><a href=""https://i.stack.imgur.com/N4YDz.png"" rel=""nofollow noreferrer"">sink</a></p>
<p><a href=""https://i.stack.imgur.com/5Mxr3.png"" rel=""nofollow noreferrer"">sink settings</a></p>
","<azure><azure-data-factory><sink>","2021-09-02 12:59:00","758","0","1","69034104","<p>If you are using Azure Blob Store as the target and creating a new folder, ADF must place an empty file in that new folder in order to create that folder. This is an Azure Blob Store requirement.</p>
"
"69016693","JSON data mapping in Java with Jackson","<p>I got stuck in mapping a json into my data structures &quot;the easy way&quot; using just Jackson decorators and I was wondering if there is a way to do this ...</p>
<p>The json that I try to read has the following structure:</p>
<pre><code>{
   &quot;animals&quot;: [
       {&quot;data_info&quot;:{&quot;ns&quot;:&quot;dog&quot;}, &quot;sound&quot;:&quot;bowwow&quot;, &quot;bites&quot;:True},
       {&quot;data_info&quot;:{&quot;ns&quot;:&quot;dog&quot;}, &quot;sound&quot;:&quot;woofWoof&quot;, &quot;bites&quot;:False},
       {&quot;data_info&quot;:{&quot;ns&quot;:&quot;cat&quot;}, &quot;sound&quot;:&quot;meeeOwww&quot;, &quot;age&quot;:5}
    ],
    &quot;data_info&quot;:{&quot;ns&quot;:&quot;animal&quot;}
}
</code></pre>
<p>So basically every data entity has a &quot;data_info&quot; object (mapped in my code from below to  DataTypeInfo) that has a property &quot;ns&quot; object (mapped in my code TypeInfo) which contains the object type. So this means that the discriminator for object types is always under data_info.ns</p>
<p>Here are my data entities:</p>
<pre><code>public class Animals extends DataTypeInfo {
    @JsonProperty(&quot;animals&quot;)
    List&lt;Mamals&gt; animals;
}

@JsonTypeInfo( use = JsonTypeInfo.Id.NAME, include = JsonTypeInfo.As.PROPERTY, property = &quot;data_info.ns&quot;)
@JsonSubTypes({
        @JsonSubTypes.Type(value = Cat.class, name = &quot;cat&quot;),
        @JsonSubTypes.Type(value = Dog.class, name = &quot;dog&quot;),
})

public abstract class Mamals extends DataTypeInfo {    
}

public class Cat extends Mammals {
    @JsonProperty(&quot;sound&quot;)
    private String sound;
    @JsonProperty(&quot;age&quot;)
    private in age;
}

public class Dog extends Mammals {    
    @JsonProperty(&quot;sound&quot;)
    private String sound;
    @JsonProperty(&quot;bites&quot;)
    boolean bites    
}

public class DataTypeInfo {
    @JsonProperty(&quot;data_info&quot;)
    TypeInfo typeInfo;
}

public class TypeInfo {
    @JsonProperty(&quot;ns&quot;)
    String nameSpace;
}
</code></pre>
<p>The error in my code is in the discriminator from the Mammals class: property = &quot;data_info.ns&quot; since this is intended to work with properties but I try to use a sub property ...
Is there a way to correctly declare the discriminator of the Mammal abstract class so that the correct Dog or Cat objects are instantiated ?</p>
","<java><json><jackson><jackson-databind>","2021-09-01 15:28:44","103","0","1","69103964","<p>The solution that I ended up with was to use a custom builder (JsonCreator) in the abstract class (for the example from above Mammals).</p>
<p>My updated Mammals class looks like this:</p>
<pre><code>    @JsonCreator
public static Mammals create(Map&lt;String,Object&gt; jsonMap) throws JsonProcessingException {
    Mammals result;
    Map type_info = (Map&lt;String,String&gt;) jsonMap.get(&quot;data_info&quot;);
    String type = (String) type_info.get(&quot;ns&quot;);

    ObjectMapper mapper = new ObjectMapper();
    String json = mapper.writeValueAsString(jsonMap);
    if (type.equals(&quot;cat&quot;)) {
        result = mapper.readValue(json, Cat.class);
    } else if (type.equals(&quot;dog&quot;)) {
        result = mapper.readValue(json, Dog.class);
    } else {
        throw new RuntimeException(&quot;Unknown entity type&quot;);
    }
    return result;
}
</code></pre>
<p>Since my root class (Animals) contains a list of Mammals, for every element of this list this creator is executed to build the proper instance of Mammals (Cat or Dog in my example).</p>
"
"69002323","Can an iOS app be installed on zillions of devices without paying the App Store yearly subscription?","<p>I have finished an iOS app for the company I work. I have a paid app store account for my own particular apps.</p>
<p>The company wants to distribute this app to zillions of their customers without paying anything to Apple.</p>
<p>Their idea: we will distribute it by using MDM.</p>
<p>Will this work?</p>
<p>Will an app that is simply installed on their test device work if they copy the binary image of that device using MDM to other devices? I don't think so.</p>
<p>As far as I know you first have to create a signed IPA for distribution, using a paid account. They think they can simply install the app on the device using Xcode and they copy the device's binary image to other devices.</p>
<p>What do you guys think?</p>
","<ios><app-store><mdm>","2021-08-31 16:31:50","46","0","1","69003012","<p>I don't think using MDM will work. Check out this page:</p>
<p><a href=""https://support.apple.com/guide/mdm/mdm-overview-mdmbf9e668/web"" rel=""nofollow noreferrer"">https://support.apple.com/guide/mdm/mdm-overview-mdmbf9e668/web</a></p>
<blockquote>
<p>You can then wirelessly distribute, manage, and configure apps and books purchased through Apple School Manager or Apple Business Manager.</p>
</blockquote>
<p>AFAIK you still need to enroll in the Apple Developer Program (where the $99 per year comes from) to publish to those channels.</p>
"
"68966694","Mapping data flow SQL query and Parameters failing","<p>In my mapping dataflow I have simplified this down to dimdate just for the test</p>
<p><strong>My parameters are</strong>
<a href=""https://i.stack.imgur.com/6PrAE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6PrAE.png"" alt=""enter image description here"" /></a></p>
<p>The source even tells you exactly how to enter the select query if you are using parameters which is what I'm trying to achieve</p>
<p><a href=""https://i.stack.imgur.com/pMleJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pMleJ.png"" alt=""enter image description here"" /></a></p>
<p>Then I import but get two different errors
for parameterizing a table`</p>
<pre><code>SELECT * FROM {$df_TableName}
</code></pre>
<p>I get</p>
<blockquote>
<p>This error from a select * or invidiual columns</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/of8FS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/of8FS.png"" alt=""enter image description here"" /></a></p>
<p>I've tried just the WHERE clause (what I actually need) as a parameter but keep getting datatype mismatch errors</p>
<p>I then started testing multiple ways and it only allows the schema to be parameterised from my queries below</p>
<p>all of these other options seem to fail no matter what I do</p>
<pre><code>SELECT * FROM [{$df_Schema}].[{$df_TableName}] Where [Period] = {$df_incomingPeriod}

SELECT * FROM [dbo].[DimDate] Where [Period] = {$df_incomingPeriod}

SELECT * FROM [dbo].[{$df_TableName}] Where [Period] = {$df_incomingPeriod}

SELECT * FROM [dbo].[DimDate] Where [Period] = 2106
</code></pre>
<p><a href=""https://i.stack.imgur.com/VOCP7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VOCP7.png"" alt=""enter image description here"" /></a></p>
<p>I know there's an issue with the Integer datatype but don't know how to pass this to the query within the  parameter without changing its type as the sql engine cannot run [period] as a string</p>
","<apache-spark><apache-spark-sql><azure-data-factory>","2021-08-28 17:26:42","637","1","2","68979257","<p>Use <strong>CONCAT</strong> function in expression builder to build the Query in Dataflow.</p>
<pre><code>concat(&lt;this&gt; : string, &lt;that&gt; : string, ...) =&gt; string
</code></pre>
<blockquote>
<p>Note: Concatenates a variable number of strings together. All the variables should be in form of strings.</p>
</blockquote>
<p><strong>Example 1</strong>:</p>
<p><a href=""https://i.stack.imgur.com/FXVR2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FXVR2.png"" alt=""enter image description here"" /></a></p>
<pre><code>concat(toString(&quot;select * from &quot;), toString($df_tablename))
</code></pre>
<p><a href=""https://i.stack.imgur.com/nZiaz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nZiaz.png"" alt=""enter image description here"" /></a></p>
<p><strong>Example 2</strong>:</p>
<pre><code>concat(toString(&quot;select * from &quot;), toString($df_tablename), ' ', toString(&quot; where incomingperiod = &quot;), toString($df_incomingPeriod))
</code></pre>
<p><a href=""https://i.stack.imgur.com/PrfkI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PrfkI.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/R1m9J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/R1m9J.png"" alt=""enter image description here"" /></a></p>
"
"68966694","Mapping data flow SQL query and Parameters failing","<p>In my mapping dataflow I have simplified this down to dimdate just for the test</p>
<p><strong>My parameters are</strong>
<a href=""https://i.stack.imgur.com/6PrAE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6PrAE.png"" alt=""enter image description here"" /></a></p>
<p>The source even tells you exactly how to enter the select query if you are using parameters which is what I'm trying to achieve</p>
<p><a href=""https://i.stack.imgur.com/pMleJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pMleJ.png"" alt=""enter image description here"" /></a></p>
<p>Then I import but get two different errors
for parameterizing a table`</p>
<pre><code>SELECT * FROM {$df_TableName}
</code></pre>
<p>I get</p>
<blockquote>
<p>This error from a select * or invidiual columns</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/of8FS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/of8FS.png"" alt=""enter image description here"" /></a></p>
<p>I've tried just the WHERE clause (what I actually need) as a parameter but keep getting datatype mismatch errors</p>
<p>I then started testing multiple ways and it only allows the schema to be parameterised from my queries below</p>
<p>all of these other options seem to fail no matter what I do</p>
<pre><code>SELECT * FROM [{$df_Schema}].[{$df_TableName}] Where [Period] = {$df_incomingPeriod}

SELECT * FROM [dbo].[DimDate] Where [Period] = {$df_incomingPeriod}

SELECT * FROM [dbo].[{$df_TableName}] Where [Period] = {$df_incomingPeriod}

SELECT * FROM [dbo].[DimDate] Where [Period] = 2106
</code></pre>
<p><a href=""https://i.stack.imgur.com/VOCP7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VOCP7.png"" alt=""enter image description here"" /></a></p>
<p>I know there's an issue with the Integer datatype but don't know how to pass this to the query within the  parameter without changing its type as the sql engine cannot run [period] as a string</p>
","<apache-spark><apache-spark-sql><azure-data-factory>","2021-08-28 17:26:42","637","1","2","71224024","<p>Awesome, it worked like magic for me. I was struggling with parameterizing tables= names which I was passing through Array list.
Created a data flow parameter and gave this value:
@item().TABLE_NAME</p>
"
"68867208","azure data flow converting boolean to string","<p>I am trying to split large json files into smaller chunks using azure data flow. It splits the file but it changes column type boolean  to string in output files. This same data flow will be used for different json files with different schemas therefore can't have  any fixed schema mapping defined. I have to use auto mapping option. Please suggest how  could I solve this issue of automatic datatype conversion? or Any other approach to split the file in the azure data factory?<a href=""https://i.stack.imgur.com/bAxSm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bAxSm.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory><azure-data-lake-gen2>","2021-08-20 19:25:08","1718","2","2","68891984","<p>Here, with my dataset I have tried to have a source as a json file and Sink as a json. If you have a fixed Schema and import it then the data flow works fine and could return a <code>boolean</code> value after running the pipeline.</p>
<p>But as you stated to have  &quot;same data flow will be used for different json files with different schemas therefore can't have any fixed schema mapping defined&quot;. Hence, you must have a <code>Derived Column</code> to explicitly covert all to <code>boolean</code> values.</p>
<p><a href=""https://i.stack.imgur.com/0JEig.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0JEig.jpg"" alt=""enter image description here"" /></a></p>
<p><strong>Import Schema :</strong></p>
<p><a href=""https://i.stack.imgur.com/dYHpX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dYHpX.jpg"" alt=""enter image description here"" /></a></p>
<p>**In the sink you could inspect :
<a href=""https://i.stack.imgur.com/In0K3.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/In0K3.jpg"" alt=""enter image description here"" /></a></p>
<p><strong>Data Preview :</strong></p>
<p><a href=""https://i.stack.imgur.com/Q8IKL.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q8IKL.jpg"" alt=""enter image description here"" /></a></p>
"
"68867208","azure data flow converting boolean to string","<p>I am trying to split large json files into smaller chunks using azure data flow. It splits the file but it changes column type boolean  to string in output files. This same data flow will be used for different json files with different schemas therefore can't have  any fixed schema mapping defined. I have to use auto mapping option. Please suggest how  could I solve this issue of automatic datatype conversion? or Any other approach to split the file in the azure data factory?<a href=""https://i.stack.imgur.com/bAxSm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bAxSm.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory><azure-data-lake-gen2>","2021-08-20 19:25:08","1718","2","2","68899471","<p>In your ADF Data Flow Source transformation, click on the Projection Tab and click &quot;Define default format&quot;. Set explicit values for Boolean True/False so that ADF can use that hint for proper data type inference for your data.</p>
<p><a href=""https://i.stack.imgur.com/yvvi2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yvvi2.png"" alt=""enter image description here"" /></a></p>
"
"68866105","Creating an array of columns from an array of column names in data flow","<p>How can I create an array of columns from an array of column names in dataflow?
The following creates an array of sorted columns with and exception of the last column:</p>
<pre><code>sort(slice(columnNames(), 1, size(columnNames()) - 1), compare(#item1, #item2))
</code></pre>
<p>I want to get an array of the columns for this array of column names. I tried this:</p>
<pre><code>toString(byNames(sort(slice(columnNames(), 1, size(columnNames()) - 1), compare(#item1, #item2))))
</code></pre>
<p>But I keep getting the error:</p>
<pre><code>Column name function 'byNames' does not accept column or argument parameters 
</code></pre>
<p>Please can anyone help me with a workaround for this?</p>
","<azure-data-factory>","2021-08-20 17:34:27","1506","0","1","68948420","<p><strong>Update--</strong></p>
<p>It seems using <strong>ColumnNames()</strong> in any way (directly or assigning it to a parameter) seems to be leading to error. As at runtime on Spark it is fed to the <strong>byNames()</strong> function. Due to unavailability of a way to re-introduce as parameter or assign variable in Data Flow directly, see below which works for me.</p>
<p>Have empty string array type parameter in DataFLow</p>
<p><a href=""https://i.stack.imgur.com/Dt7Ab.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dt7Ab.png"" alt=""enter image description here"" /></a></p>
<p>Use sha2 function as usual in derived column with parameter  <code>sha2(256,byNames($cols))</code></p>
<p><a href=""https://i.stack.imgur.com/pKgcf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pKgcf.png"" alt=""enter image description here"" /></a></p>
<p>Create pipeline, there use getMetadata to get Structure from which you can get column names.</p>
<p><a href=""https://i.stack.imgur.com/S5F1c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S5F1c.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/o0U3b.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o0U3b.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/LCvr3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LCvr3.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/YNxHK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YNxHK.png"" alt=""enter image description here"" /></a></p>
<p>For each column, inside ForEach activity append to a variable.</p>
<p><a href=""https://i.stack.imgur.com/CuyNC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CuyNC.png"" alt=""enter image description here"" /></a></p>
<p>Next, connect to DataFLow and pass the variable containing Column names.</p>
<p><a href=""https://i.stack.imgur.com/r8b4U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/r8b4U.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/aPwM8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aPwM8.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/1USW6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1USW6.png"" alt=""enter image description here"" /></a></p>
<p>The documentation for the byNames function states '<strong>Computed inputs are not supported but you can use parameter substitutions'</strong>.  This explains why you should use a parameter as input to create the array used in the byNames function:</p>
<p>Example: Where <code>$cols</code> parameter hold the list of columns.</p>
<pre><code>sha2(256,byNames(split($cols,',')))
</code></pre>
<p>You can use computed columns names as input by creating the array prior to using in function. Instead of creating the expression in-line in the function call, set the column values in a parameter prior and then use it in your function directly afterwards.</p>
<p>For a parameter <code>$cols</code> of type array:</p>
<pre><code>$cols = sort(slice(columnNames(), 1, size(columnNames()) - 1), compare(#item1, #item2))

toString(byNames(sort(slice($cols, compare(#item1, #item2))))
</code></pre>
<p>Refer: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#:%7E:text=functions%20%28toDate,%20toString,%20...%29.-,Column%20names%20known%20at%20design%20time%20should%20be%20addressed%20just%20by%20their%20name.%20Computed%20inputs%20are%20not%20supported%20but%20you%20can%20use%20parameter%20substitutions.,-toString%28byNames%28%5B%27parent"" rel=""nofollow noreferrer"">byNames</a></p>
"
"68769883","mule3 to mule 4 expression to dataweave 2.0","<p>I'm new to migrating the mule 3 apps to mule 4 I have done almost conversion but one expression stopped my flow and not able to achieve the logic for it if anyone has an idea regarding the expression to transform please help me</p>
<p><strong>Expression:</strong></p>
<pre><code>if(flowVars.maindata.keySet().contains(payload.idCaseNumber))
{
flowVars.temporary=[];
flowVars.maindata.get(payload.idCaseNumber).add(map);
}
else
{
flowVars.temporary.add(previousdata);
vars.maindata.put(payload.idCaseNumber,temporary);
}
</code></pre>
<p>I have tried up to my knowledge on the above code but still I'm getting problem</p>
<pre><code>flowVars.maindata.get(payload.idCaseNumber).add(map);
</code></pre>
","<mule><dataweave><payload><mule4><data-mapping>","2021-08-13 09:27:04","354","0","1","68771970","<p>In Mule 3 the expression language is MEL. In Mule 4 it is DataWeave 2.0. You can't just translate directly. MEL is an imperative scripting language, similar to a subset of Java and it is easy to call Java methods. DataWeave 2.0 is a functional language. Furthermore Mule 4 operations (example: a , , etc) can only return one value, which can be assigned to the payload or to one variable.</p>
<p>For your snippet I'll assume that maindata is a map. You can use two set-variable to assign each variable:</p>
<pre><code>&lt;set-variable variableName=&quot;temporary&quot; value=&quot;#[ if( namesOf(vars.maindata) contains payload.idCaseNumber ) [] else vars.temporary ++ **previousdata** ]&quot; /&gt;

</code></pre>
<p>I don't know exactly what do you use for previousdata.</p>
<p>To update the variable maindata it is probably a good match for the <a href=""https://docs.mulesoft.com/mule-runtime/4.3/dw-operators#update-operator"" rel=""nofollow noreferrer"">update operator</a>, in a separate  or Transform operation, with the same condition than for vars.temporary.</p>
<p>Update:
I'll assume vars.maindata is a map, which DataWeave will consider an object, and each element is a list. As an example of doing an <a href=""https://docs.mulesoft.com/mule-runtime/4.3/dw-operators#upserting"" rel=""nofollow noreferrer"">'upsert'</a> operation with a <a href=""https://docs.mulesoft.com/mule-runtime/4.3/dw-operators#dynamic-selector"" rel=""nofollow noreferrer"">dynamic selector</a>:</p>
<pre><code>%dw 2.0
output application/java
var temporary=[5]
var maindata={ a:[1,2,3,4] }
var myKey=&quot;a&quot;
---
maindata  update {
        case data at .&quot;$(myKey)&quot;! -&gt;  if (data != null) data ++ temporary else temporary
}
</code></pre>
<p>You could replace in above script the DataWeave var <code>temporary</code> with the expression from my example above, and the other DataWeave variables with the Mule variables (<code>vars.name</code>) or payload. If you change in above example myKey to have value &quot;b&quot; you will see that key being added.</p>
"
"68769415","ADF Json File Column count and Flattening all columns in file without data flow","<p>How to count the number of all column in JSON file using Azure Data Factory activity ?
How to Flatten complete objects(All columns) in JSON file using Azure Data Factory activity?</p>
","<azure><azure-adf>","2021-08-13 08:52:16","194","0","1","68770268","<p>Upload Json file in blob storage, create dataset after creating linked service to connect the json file, then create data flow with source as the json dataset, in data flow you will find the FLATTEN
step to flatten complex object in JSON</p>
"
"68643554","docker-compose healthcheck for pentaho data integration (pdi)","<p>I am building my custom pdi image using docker. I could build image and ran it without any issues. Now I need to add <strong>healthcheck</strong> for my pdi container. <strong>Can anyone suggest me a healthcheck command?</strong></p>
<p>I tried to use,</p>
<pre><code>healthcheck:
      test: /home/data-integration/kitchen.sh -file=&quot;/home/jobs/my.kjb&quot; -level=Basic &amp;&amp; echo $? || exit 1 
</code></pre>
<p>but gives an error as,
<code>ERROR: Invalid interpolation format for &quot;healthcheck&quot; option in service &quot;pentaho&quot;: &quot;/home/data-integration/kitchen.sh -file=&quot;/home/jobs/my.kjb&quot; -level=Basic &amp;&amp; echo $? || exit 1&quot;</code></p>
<p>If I use healthcheck command as below it become <strong>unhealthy</strong> even there are no any errors.</p>
<pre><code>healthcheck:
      test: /home/data-integration/kitchen.sh -file=&quot;/home/jobs/my.kjb&quot; -level=Basic || exit 1 
</code></pre>
<p>if I find it from docker inspect containerID,
<a href=""https://i.stack.imgur.com/PkXJ2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PkXJ2.png"" alt=""enter image description here"" /></a></p>
<ul>
<li>I do not use carte or anything or any UIs. I just unzip the pdi zip file and want to run my pdi job to a given schedule. my <strong>entrypoint.sh</strong> file is as below,</li>
</ul>
<pre><code>#!/bin/sh
## entrypoint.sh
/home/data-integration/kitchen.sh -file=&quot;/home/jobs/my.kjb&quot; -level=Basic
tail -f /dev/null
</code></pre>
<p>When I manually run job file and check echo $? it gives 0 as output if job is success. how to use it correctly in to docker-compose healthcheck?</p>
","<docker><docker-compose><exit-code><pentaho-data-integration>","2021-08-03 22:46:24","740","1","2","68644270","<p>Being a new user, I cannot comment yet, so I hope this answer gives you something to think about.</p>
<h2>Food for thought</h2>
<p>Per the Docker documentation on healthchecks, the format is as stated:
<a href=""https://docs.docker.com/engine/reference/builder/#healthcheck"" rel=""nofollow noreferrer"">https://docs.docker.com/engine/reference/builder/#healthcheck</a></p>
<p>I'm not familiar with your application specifically, but if there is any startup required, then setting a delay to give the container time to initialize may be helpful.</p>
<p>I also see that you're using the same command in your entrypoint script that you are using to healthcheck.</p>
<p>Healthchecks should typically not be the same thing as the running process, and instead should be used to ensure the running process is working correctly. The docs highlight this, as does <a href=""https://blog.sixeyed.com/docker-healthchecks-why-not-to-use-curl-or-iwr/"" rel=""nofollow noreferrer"">this blogpost</a>, detailing how to check that a web app is alive by pinging the server.</p>
<p>Another note is that if your entrypoint tails dev null, you will not get the logs of the running process through <code>docker logs</code>. If you want to schedule a task to run often in a container, I recommend wrapping your command in a <code>while</code> loop which calls the command, or using an external orchestrator like <a href=""https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/"" rel=""nofollow noreferrer"">Kubernetes Cron Jobs</a> (Edit: Or even a crontab on the host that calls <code>docker run</code>)</p>
<h2>Fix</h2>
<p>Finally, if you're set on simply fixing the immediate issue of formatting, you need to escape the <code>$</code> character in the healthcheck, like so:</p>
<pre><code>      test: /home/data-integration/kitchen.sh -file=&quot;/home/jobs/my.kjb&quot; -level=Basic &amp;&amp; echo $$? || exit 1
</code></pre>
<p>Other issues akin to this are:
<a href=""https://stackoverflow.com/questions/40619582/how-can-i-escape-a-dollar-sign-in-a-docker-compose-file"">How can I escape a $ dollar sign in a docker compose file?</a></p>
<p>And it links to the docs on variable escaping here:
<a href=""https://docs.docker.com/compose/compose-file/compose-file-v3/#variable-substitution"" rel=""nofollow noreferrer"">https://docs.docker.com/compose/compose-file/compose-file-v3/#variable-substitution</a></p>
"
"68643554","docker-compose healthcheck for pentaho data integration (pdi)","<p>I am building my custom pdi image using docker. I could build image and ran it without any issues. Now I need to add <strong>healthcheck</strong> for my pdi container. <strong>Can anyone suggest me a healthcheck command?</strong></p>
<p>I tried to use,</p>
<pre><code>healthcheck:
      test: /home/data-integration/kitchen.sh -file=&quot;/home/jobs/my.kjb&quot; -level=Basic &amp;&amp; echo $? || exit 1 
</code></pre>
<p>but gives an error as,
<code>ERROR: Invalid interpolation format for &quot;healthcheck&quot; option in service &quot;pentaho&quot;: &quot;/home/data-integration/kitchen.sh -file=&quot;/home/jobs/my.kjb&quot; -level=Basic &amp;&amp; echo $? || exit 1&quot;</code></p>
<p>If I use healthcheck command as below it become <strong>unhealthy</strong> even there are no any errors.</p>
<pre><code>healthcheck:
      test: /home/data-integration/kitchen.sh -file=&quot;/home/jobs/my.kjb&quot; -level=Basic || exit 1 
</code></pre>
<p>if I find it from docker inspect containerID,
<a href=""https://i.stack.imgur.com/PkXJ2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PkXJ2.png"" alt=""enter image description here"" /></a></p>
<ul>
<li>I do not use carte or anything or any UIs. I just unzip the pdi zip file and want to run my pdi job to a given schedule. my <strong>entrypoint.sh</strong> file is as below,</li>
</ul>
<pre><code>#!/bin/sh
## entrypoint.sh
/home/data-integration/kitchen.sh -file=&quot;/home/jobs/my.kjb&quot; -level=Basic
tail -f /dev/null
</code></pre>
<p>When I manually run job file and check echo $? it gives 0 as output if job is success. how to use it correctly in to docker-compose healthcheck?</p>
","<docker><docker-compose><exit-code><pentaho-data-integration>","2021-08-03 22:46:24","740","1","2","68659848","<p>Found the healthcheck for pdi container and I will post here since this can help for others.</p>
<p>Basically, When a <strong>job executed without any errors it return 0 as exit code</strong>. But when there is an error it usually returns 1 as exit code as I found.</p>
<p>So, checked exit code status for the needed pdi job execution command and used it for healthcheck as below sample,</p>
<p>create <strong>healthcheck.sh</strong> file and copy it to your container,(in here, I copied it to /home/scripts/ path inside my container.)</p>
<pre><code>#!/bin/sh
set -e

## execute job 
/home/data-integration/kitchen.sh -file=&quot;/home/jobs/my.kjb&quot; -level=Basic
 
## Job executed without errors? Use exit status of the job command ##
if [ $? -eq 0 ]
then
  echo &quot;Success&quot;
  exit 0
else
  echo &quot;Failure&quot; &gt;&amp;2
  exit 1
fi
</code></pre>
<p>Then run <strong>healthcheck.sh</strong> file in <strong>docker-compose.yml</strong> (used 2.3 docker-compose.yml version)</p>
<pre><code>healthcheck:
      test: ./home/scripts/healthcheck.sh
      interval: 55s
      timeout: 50s
      retries: 3
      start_period: 9m
</code></pre>
<p><strong>Note:</strong></p>
<ul>
<li>Make sure to <strong>run chmod +x</strong> for healthcheck.sh file after copy it to the container using <strong>Dockerfile</strong>, before run the <strong>docker-compose.yml</strong> file <strong>OR change healthcheck command</strong> as,</li>
</ul>
<pre><code>healthcheck:
      test: chmod +x /home/scripts/healthcheck.sh &amp;&amp; ./home/scripts/healthcheck.sh
      interval: 55s
      timeout: 50s
      retries: 3
      start_period: 9m
</code></pre>
<ul>
<li><p>For my docker image, I unzipped <strong>pdi-ce-9.1.0.0-324.zip</strong> file and executed the job file repeatedly using a entrypoint.sh file to do my ETL process as a schedule. Used <strong>java:8-jre-alpine</strong> image to unzip it.</p>
</li>
<li><p><strong>New entrypoint.sh</strong> edited according to <a href=""https://stackoverflow.com/users/16588529/thequeenisdead"">@TheQueenIsDead</a> suggested with infinite while loop to run pdi job repeatedly,</p>
</li>
</ul>
<pre><code>#!/bin/sh
## entrypoint.sh
while :
do
    /home/data-integration/kitchen.sh -file=&quot;/home/jobs/my.kjb&quot; -level=Basic
    sleep 120
done
</code></pre>
<p><strong>References:</strong></p>
<ul>
<li><a href=""http://diethardsteiner.blogspot.com/2013/03/pentaho-kettle-pdi-get-pan-and-kitchen.html"" rel=""nofollow noreferrer"">http://diethardsteiner.blogspot.com/2013/03/pentaho-kettle-pdi-get-pan-and-kitchen.html</a></li>
<li><a href=""https://www.cyberciti.biz/faq/bash-get-exit-code-of-command/"" rel=""nofollow noreferrer"">https://www.cyberciti.biz/faq/bash-get-exit-code-of-command/</a></li>
</ul>
"
"68621008","Can you use use read/write on data flow chart?","<p>If I have a front-end application that reads from the API of a backend, then I would have a data flow chart like this, because the data flows from the backend to the frontend:</p>
<p><a href=""https://i.stack.imgur.com/n2eo3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/n2eo3.png"" alt=""enter image description here"" /></a></p>
<p>However, when I would like to emphasize that this is a read only API, and that the front-end cannot write to the backend, I feel like have to turnaround the arrow, saying &quot;The Frontend reads from Backend&quot;:</p>
<p><a href=""https://i.stack.imgur.com/TkinD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TkinD.png"" alt=""enter image description here"" /></a></p>
<p>But this would conflict with the data-flow arrow, because the arrow indicates the direction of the flow:</p>
<p><a href=""https://i.stack.imgur.com/bO1Gj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bO1Gj.png"" alt=""enter image description here"" /></a></p>
<p>How do you represent a read/write relation to a data-flow diagram?</p>
","<dataflow-diagram><dfd>","2021-08-02 11:43:41","365","1","1","68746527","<p>Data-flow diagrams use arrows to show the direction of the flow of data. Whether you use <a href=""https://online.visual-paradigm.com/knowledge/software-design/gane-sarson-dfd-tutorial/"" rel=""nofollow noreferrer"">Gane &amp; Sarson</a> or <a href=""https://online.visual-paradigm.com/knowledge/software-design/dfd-tutorial-yourdon-notation/"" rel=""nofollow noreferrer"">Yourdon &amp; DeMarco</a>), a unidirectional arrow means an unidirectional flow:</p>
<ul>
<li><p>Your first diagram means that the data flows from the back-end to the front-end and only in that direction. There is no need to tell that it's read-only, because the flow is unidirectional.</p>
</li>
<li><p>If you wanted the front-end to read and write data from/to the back-end, you'd need a second arrow in the opposite direction.  A common practice is to use a bidirectional arrow (Yourdon uses it in several examples, Gane and Sarson even recommend it for keeping diagrams simple).</p>
</li>
<li><p>If you want to have it read-only,  but indicate the data elements used to find the relevant data,  you may use Gane &amp; Sarson's notation of &quot;search argument specification&quot; (which is meant to apply to data stores and not processes):</p>
<p><a href=""https://i.stack.imgur.com/yyGUB.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yyGUB.jpg"" alt=""enter image description here"" /></a>
<em>Source: Structured System Analysis, Chris Gane &amp; Trish Sarson</em></p>
</li>
</ul>
<p>Your second diagram would be misleading: not only would it show the flow in the wrong direction, but in addition, the DFD convention is to label the arrows with the kind of data that flows along:</p>
<blockquote>
<p>Arrows should be named to indicate the meaning of the data that moves along the flow that is, a noun.<br>
Data flows with a verb name signify a process that has been omitted. Data flow in and out of a process must be altered in some way.<br> -
Source : <a href=""https://online.visual-paradigm.com/knowledge/software-design/gane-sarson-dfd-tutorial/"" rel=""nofollow noreferrer"">Visual Paradigm tutorial</a></p>
</blockquote>
<p>This convention explain also why double-sided arrows are quite rare in practice, as soon as you label the flow: the flow back would have a slightly different name  (e.g. updated data, corrected data, validated data).</p>
<p>The only ambiguity in your diagram is who initiates the flow: is it the front-end that reads or the back-end that writes? But this is normal: it's not the purpose of this kind of diagrams to go in such details. You could add an arrow &quot;request data&quot; to show that the front-end initiates the communication,  but this would quickly overload your diagram and the added precision would come at the expense of the readability.</p>
"
"68574949","How to map the data from a nested json object in flutter","<p>Here I get this JSON object from the API and I need to add it to a list and return so that I can get it from the snapshot to display the data.But i get the snapshot.data as null.Please help me to solve this issue.</p>
<pre><code>...
{
    &quot;Data&quot;: [
        {
            &quot;product_name&quot;: &quot;MACC Tea Master Blend 40 Bags&quot;,
            &quot;img_url&quot;: &quot;1605262901.jpg&quot;,
            &quot;order_no&quot;: &quot;1625809545122&quot;,
            &quot;category&quot;: [
                {
                    &quot;category_name&quot;: &quot;01 Box (40 Bags)&quot;,
                    &quot;order_no&quot;: &quot;1625809545122&quot;,
                    &quot;qty&quot;: &quot;1&quot;,
                    &quot;line_total&quot;: &quot;1.79&quot;
                }
            ]
        }
    ],
    &quot;ID&quot;: &quot;200&quot;
}
...
</code></pre>
<p>This is the code on how i tried so far.</p>
<pre><code>...
Future&lt;List&lt;OrderDetails&gt;&gt; fetchMyOrderDetails(order_no) async {
  var body = jsonEncode({&quot;order_no&quot;: order_no});
  print(&quot;order_no : &quot; + order_no);
  http.Response response = await http.post(
      Uri.encodeFull(api + &quot;get_order_details_by_orderno.php&quot;), //url
      headers: {&quot;Accept&quot;: &quot;application/json&quot;},
      body: body);
  if (response.statusCode == 200) {
    Map&lt;String, dynamic&gt; map = json.decode(response.body);
    // var map = json.decode(response.body);
    print(&quot;response.body : &quot; + &quot;${response.body}&quot;);
    print(&quot;map : &quot; + &quot;${map['Data']}&quot;);

    List&lt;OrderDetails&gt; orderDetailsList;
    orderDetailsList = (json.decode(response.body)['Data'] as List)
        .map((i) =&gt; OrderDetails.fromJson(i))
        .toList();

    return orderDetailsList;
  } else {
    // print(&quot;Failed to load categories&quot;);
    throw Exception('Failed to load the Orders');
  }
}

class OrderDetails {
  final String product_name;
  final String img_url;
  final String order_no;
  final List&lt;Category&gt; category;

  OrderDetails({
    this.product_name,
    this.img_url,
    this.order_no,
    this.category,
  });

  factory OrderDetails.fromJson(Map&lt;String, dynamic&gt; json) {
    return OrderDetails(
      product_name: json['product_name'] as String,
      img_url: json['img_url'] as String,
      order_no: json['order_no'] as String,
      category: json['category'] as List,
    );
  }
}

class Category {
  final String category_name;
  final String qty;
  final String line_total;

  Category({this.category_name, this.qty, this.line_total});

  factory Category.fromJson(Map&lt;String, dynamic&gt; json) {
    return Category(
      category_name: json['category_name'] as String,
      qty: json['qty'] as String,
      line_total: json['line_total'] as String,
    );
  }
}
...
</code></pre>
<p>From the below code i try to access the data but the snapshot.data get null and the page is loading.</p>
<pre><code>...
    child: FutureBuilder&lt;List&lt;OrderDetails&gt;&gt;(
        future: fetchMyOrderDetails(order_no),
        builder: (BuildContext context, AsyncSnapshot snapshot) {
          print(&quot;snapshot data : &quot; + &quot;${snapshot.data}&quot;);
          if (snapshot.data == null) {
            return Container(
              child: Center(
                child: CircularProgressIndicator(),
              ),
            );
          } else {
            return Center(
              child: Text(snapshot.data.product_name),
            );
          }
        },
      ),
...
</code></pre>
","<flutter><dart><null><snapshot><data-mapping>","2021-07-29 11:18:32","1178","0","1","68575994","<p>Please update <code>OrderDetails</code> class.</p>
<p><code>json['category'] as List</code> is <code>List&lt;dynamic&gt;</code> , not <code>List&lt;Category&gt;</code></p>
<pre><code>factory OrderDetails.fromJson(Map&lt;String, dynamic&gt; json) {
    return OrderDetails(
      product_name: json['product_name'] as String,
      img_url: json['img_url'] as String,
      order_no: json['order_no'] as String,
      category: (json['category'] == null) 
                    ? null 
                    : (json['category'] as List).map(e =&gt; Category.fromJson(e)).toList(),
    );
}


Future&lt;List&lt;OrderDetails&gt;&gt; fetchMyOrderDetails(order_no) async {
  var body = jsonEncode({&quot;order_no&quot;: order_no});
  print(&quot;order_no : &quot; + order_no);
  http.Response response = await http.post(
      Uri.encodeFull(api + &quot;get_order_details_by_orderno.php&quot;), //url
      headers: {&quot;Accept&quot;: &quot;application/json&quot;},
      body: body);
  if (response.statusCode == 200) {
    Map&lt;String, dynamic&gt; map = json.decode(response.body);        
    print(&quot;response.body : &quot; + &quot;${response.body}&quot;);
    print(&quot;map : &quot; + &quot;${map['Data']}&quot;);

    List&lt;OrderDetails&gt; orderDetailsList;
    orderDetailsList = (map['Data'] as List)
        .map((i) =&gt; OrderDetails.fromJson(i))
        .toList();

    return orderDetailsList;
  } else {
    // print(&quot;Failed to load categories&quot;);
    throw Exception('Failed to load the Orders');
  }
}
</code></pre>
"
"68555255","Trying to build a logging mechanism in pentaho data integration","<p>I have the data as below.</p>
<p><a href=""https://i.stack.imgur.com/YN3Eb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YN3Eb.png"" alt=""enter image description here"" /></a></p>
<p>I need the output as below.</p>
<p><a href=""https://i.stack.imgur.com/DoQBc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DoQBc.png"" alt=""enter image description here"" /></a></p>
<p>I tried to build the transformation as below.</p>
<p><a href=""https://i.stack.imgur.com/FtvtR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FtvtR.png"" alt=""enter image description here"" /></a></p>
<p>Here if I run the transformation I am getting output as below.</p>
<p><a href=""https://i.stack.imgur.com/QzSZl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QzSZl.png"" alt=""enter image description here"" /></a></p>
<p>Can someone suggest the components in PDI to get the proper output?</p>
","<pentaho><pentaho-spoon><pentaho-data-integration><pdi><data-integration>","2021-07-28 06:13:04","67","0","1","68587784","<p>Probably, you'll need two &quot;<em>Memory Group by</em>&quot; steps after SV and SV2 steps, and then a Merge Join by MPN_code.</p>
<p>What I don't know is if you are achieving what you want. Keep in mind that in a transformation all steps are initialized at the same time, so depending on how you calculate the Starttime and Endtime, you might get the same time for all your rows, because it's calculated as the time the first row reach the stream of the step.</p>
"
"68547866","Can we transfer file from SFTP to server through spring cloud data flow?","<p>My requirement is to get a file from SFTP and make it available client to download. I am forcefully asked to do that using spring cloud data flow.</p>
<p>In the documentation, I saw that there is an SFTP to the JDBC File Ingest tutorial (<a href=""https://dataflow.spring.io/docs/recipes/batch/sftp-to-jdbc/"" rel=""nofollow noreferrer"">https://dataflow.spring.io/docs/recipes/batch/sftp-to-jdbc/</a>).</p>
<p>So my question is can we transfer a file through spring cloud data flow rather than reading the file and inserting it into the databae?</p>
<p>Thanks,
Dasun.</p>
","<spring-cloud-dataflow><spring-cloud-dataflow-ui>","2021-07-27 15:38:16","275","-1","1","68549843","<p>Yes you can. It’s similar to the sftp to jdbc example which downloads the file to a  shared file system from which the batch job reads it. You can create a simple pipeline like sftp | s3 or sftp l file or sftp l sftp, depending on your specific use case.</p>
"
"68530470","How to exit Kiosk Mode in Android Management API (MDM)?","<p>I am facing a nasty scenario with the Android Management API (MDM) where the app is in the KIOSK MODE without option to leave the app, except if the policy is updated to FORCED_INSTALLED.</p>
<p>My question is if there is a way to leave the KIOSK MODE if WIFI drops or WIFI settings changes at some point? Is there any work around to leave the app to reconnect to the network or so?</p>
<p>Kind regards</p>
","<android><kotlin><mobile><mdm><android-management-api>","2021-07-26 13:08:38","212","1","2","70844893","<p>To move your app out of KIOSK mode you need to have change the <a href=""https://developers.google.com/android/management/reference/rest/v1/enterprises.policies#installtype"" rel=""nofollow noreferrer"">installType</a> to other values.</p>
<p>you can give value :</p>
<blockquote>
<p>INSTALL_TYPE_UNSPECIFIED : Unspecified. Defaults to AVAILABLE.</p>
</blockquote>
<p>However, there's a workaround which worked for me, you can always call an intent from the app which is in KIOSK mode to manage your WiFi configurations from the app.</p>
<pre><code>startActivity(new Intent(Settings.ACTION_WIFI_SETTINGS));
</code></pre>
<p>Note : This code is for java, you can look for kotlin <a href=""https://developer.android.com/reference/kotlin/android/provider/Settings#action_wifi_settings"" rel=""nofollow noreferrer"">here</a></p>
"
"68530470","How to exit Kiosk Mode in Android Management API (MDM)?","<p>I am facing a nasty scenario with the Android Management API (MDM) where the app is in the KIOSK MODE without option to leave the app, except if the policy is updated to FORCED_INSTALLED.</p>
<p>My question is if there is a way to leave the KIOSK MODE if WIFI drops or WIFI settings changes at some point? Is there any work around to leave the app to reconnect to the network or so?</p>
<p>Kind regards</p>
","<android><kotlin><mobile><mdm><android-management-api>","2021-07-26 13:08:38","212","1","2","70920386","<p>If you get disconnected to a saved WIFI configuration it should automatically reconnect, However, if you want to connect to a new WIFI hotspot or change WIFI settings you can do this by using kiosk custom launcher and adding <code>com.android.settings</code> under the application policy.</p>
<p>Sample policy:</p>
<pre><code>&quot;kioskCustomLauncherEnabled&quot;: true,
&quot;applications&quot;: [
  {
    &quot;packageName&quot;: &quot;com.facebook.katana&quot;,
    &quot;installType&quot;: &quot;FORCE_INSTALLED&quot;,
    &quot;defaultPermissionPolicy&quot;: &quot;GRANT&quot;
  },
  {
    &quot;packageName&quot;: &quot;com.android.settings&quot;,
    &quot;installType&quot;: &quot;FORCE_INSTALLED&quot;,
    &quot;defaultPermissionPolicy&quot;: &quot;GRANT&quot;
  }
]
}
</code></pre>
<p>Please refer to this <a href=""https://developers.google.com/android/management/reference/rest/v1/enterprises.policies#kioskcustomization"" rel=""nofollow noreferrer"">link</a> for more information.</p>
"
"68501896","group the record by columns and wanted to pick only the max of created date record using mapping data flows azure data factory","<p>HI there in Mapping data flows and azure data factory. I tried to create data flow in that we used aggregate transformation to group the records, now there is a column called [createddate] now we want to pick the max of created record and out put should show all the columns.</p>
<p>any advice please help me</p>
","<azure-data-factory>","2021-07-23 15:50:18","506","0","1","68503089","<p>The Aggregate transformation will only output the columns participating in the aggregation (group by and aggregate function columns). To include all of the columns in the Aggregate output, add a new column pattern to your aggregate (see pic below ... change column1, column2, ... to the names of the columns you are already using in your agg)</p>
<p><a href=""https://i.stack.imgur.com/fyfrO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fyfrO.png"" alt=""enter image description here"" /></a></p>
"
"68404511","Zero Bytes Files are getting created by ADF data flow","<p>There is a Conditional Split in my ADF data flow. Success puts the rows to a SQL database and failure conditions collect all the incorrect records and puts them into a sink which is of type CSV (Delimited text).</p>
<p><a href=""https://i.stack.imgur.com/hLTtZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hLTtZ.png"" alt=""enter image description here"" /></a></p>
<p>In case of success condition, there is an empty CSV file of 0 bytes is getting created in the sink.</p>
<p>How can I stop this?</p>
<p><a href=""https://i.stack.imgur.com/YfmOS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YfmOS.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory>","2021-07-16 06:38:49","784","0","1","68480913","<p>If you don't wish to write output to an external source, you can use <strong>cache sink</strong>. It writes data into the Spark cache instead of a data store. In mapping data flows, you can reference this data within the same flow many times using a cache lookup. If you want to store this later to a data store, just reference data as part of an expression.</p>
<p>To write to a cache sink, add a sink transformation and select Cache as the sink type. Unlike other sink types, you don't need to select a dataset or linked service because you aren't writing to an external store.</p>
<p><img src=""https://i.stack.imgur.com/dwJnk.png"" alt=""enter image description here"" /></p>
<blockquote>
<p><strong>Note:</strong> A cache sink must be in a completely independent data stream from any transformation referencing it via a cache lookup. A cache
sink also must the first sink written.</p>
<p>When utilizing cached lookups, make sure that your sink ordering has
the cached sinks set to 1, the lowest (or first) in ordering.</p>
</blockquote>
<p><img src=""https://i.stack.imgur.com/CHpHP.png"" alt=""enter image description here"" /></p>
<p>Reference this data within the same flow using a cache lookup, as part of an expression to store this to a data store.</p>
<p><img src=""https://i.stack.imgur.com/opFWS.png"" alt=""enter image description here"" /></p>
<p><img src=""https://i.stack.imgur.com/66mY8.png"" alt=""enter image description here"" /></p>
<p>Alternately use cache lookup against source to select and write it to CSV or log in a different stream or sink.</p>
<p><img src=""https://i.stack.imgur.com/RTEe3.png"" alt=""enter image description here"" /></p>
<p>Refer: <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-sink#cache-sink"" rel=""nofollow noreferrer"">CacheSink</a>, <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-expression-builder#cached-lookup"" rel=""nofollow noreferrer"">CachedLookup</a></p>
<p>If you still want to delete empty Zero byte files, you can use ADF or programmatic way to delete at the end of execution (<strong>Delete Activity in Azure Data Factory</strong>)</p>
<p><a href=""https://learn.microsoft.com/en-us/azure/data-factory/delete-activity#examples-of-using-the-delete-activity"" rel=""nofollow noreferrer"">Examples of using the Delete activity</a></p>
"
"68375516","How to synchnorize files with mobile phone running LineageOS?","<p>I have a mobile phone running LineageOS 16.0. I would like to organize the automatic synchronization of particular directory on mobile phone with some remote directory (e.g. exposed via FTP/SMB/SSH) on the server. Ideally would be to have an option either to make one-way (synchronize only missing files) or two-way (source and target are identical after synchronization) modes.</p>
<p>In particular above can be achieved by using the <a href=""https://en.wikipedia.org/wiki/Rsync"" rel=""nofollow noreferrer""><code>rsync</code></a> utility. I see the following options:</p>
<h3 id=""mobile-phone-is-exposing-the-folder-via-e.g.ssh-6hwt"">Mobile phone is exposing the folder via e.g. SSH</h3>
<p>This would require LineageOS running SSH server. In this case <code>rsync</code> could be run on the server (periodically or on some event).</p>
<ul>
<li>Is SSHD server already installed on LineageOS (<a href=""https://binfalse.de/2018/09/06/native-ssh-server-on-linageos/"" rel=""nofollow noreferrer"">Native SSH server on LinageOS</a>)? If not, is it possible to install it from a package?</li>
<li>Will running in the background <code>sshd</code> daemon drain the mobile battery?</li>
<li>How difficult would be to add it to startup (<a href=""https://lisas.de/%7Ealex/?p=237"" rel=""nofollow noreferrer"">SSHD on LineageOS</a>, <a href=""https://h4des.org/blog/index.php?/archives/359-Android-LineageOS-16-Execute-Script-on-Start-Up.html"" rel=""nofollow noreferrer"">Execute Script on Start Up</a>)?</li>
</ul>
<h3 id=""mobile-phone-is-running-rsync-857p"">Mobile phone is running rsync</h3>
<ul>
<li>This requires <a href=""https://github.com/LineageOS/android_external_rsync"" rel=""nofollow noreferrer""><code>rsync</code> installation to LineageOS</a>. Maybe there is ready-to-use package or will it require the compilation?</li>
<li>I would need an icon (or some other easy way) to execute <code>rsync</code> on mobile. How to organize that?</li>
</ul>
<p>If you see other more handy/easy options, please describe how to setup/organize them.</p>
<p>P.S. Dropbox or any other cloud solutions is not an option (out of scope for this question).</p>
<p>Thanks in advance!</p>
","<android><rsync><data-synchronization><lineageos>","2021-07-14 09:31:04","125","0","1","68471640","<p>I use the FolderSync app for this (it supports connecting to an SSH server at scheduled intervals, although I personally use it with Nextcloud). Syncthing is also a good solution I've used in the past, although you'll need to install that on your server also (no cloud required).</p>
<p>Trying to run an rsync server on a phone doesn't sound appealing, but there are a couple of rsync client apps in the play store.</p>
"
"68355695","Is Core Data necessary for across device data synchronization with iCloud?","<p>With <code>NSPersistentCloudKitContainer</code> developers can easily enable data synchronization across devices for their App with Core Data stack.</p>
<p>How about for a new app that requires local data persistent (offline data storage on the client) and across device data synchronization (data available on iPhone, iPad, macOS with the same iCloud account sign-in), is Core Data still required? would CloudKit be used as a replacement for Core Data?</p>
<p>Is Core Data a stack can be skipped with CloutKit?</p>
","<ios><swift><core-data><icloud><cloudkit>","2021-07-13 02:15:43","344","-1","2","68355995","<p>It seems CloudKit is a transport mechanism and it is not meant to be used as local storage or persistent mechanism. If there is local offline storage need Core Data should still be used.</p>
"
"68355695","Is Core Data necessary for across device data synchronization with iCloud?","<p>With <code>NSPersistentCloudKitContainer</code> developers can easily enable data synchronization across devices for their App with Core Data stack.</p>
<p>How about for a new app that requires local data persistent (offline data storage on the client) and across device data synchronization (data available on iPhone, iPad, macOS with the same iCloud account sign-in), is Core Data still required? would CloudKit be used as a replacement for Core Data?</p>
<p>Is Core Data a stack can be skipped with CloutKit?</p>
","<ios><swift><core-data><icloud><cloudkit>","2021-07-13 02:15:43","344","-1","2","68356412","<p>CloudKit is transport only. Core Data is local persistence only. You can use them together, but you can also use them separately. If you want to use CloudKit but not Core Data, you would need to write your own code to handle local persistence. You might, for example, use plain SQLite, with a wrapper like <a href=""https://github.com/groue/GRDB.swift"" rel=""nofollow noreferrer"">GRDB</a> for local persistence. Or some other way. There are lots of ways to store local data on apps. However, <code>NSPersistentCloudKitContainer</code> is part of Core Data (that’s what makes it work with CloudKit), so you would not use that.</p>
"
"68284121","Azure data flow string to date conversion in formate","<p>My source is CSV contains enddate &amp; startdate column datype string. I need to change those date columns in destination table format.</p>
<p>source -</p>
<p><a href=""https://i.stack.imgur.com/q7S3T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q7S3T.png"" alt=""enter image description here"" /></a></p>
<p>Destination :</p>
<pre><code>SALES_ORGANIZATION  STARTDATE   ENDDATE EXTRACT_MNTS
AE93-AE-CS  2020-01-01  2020-12-31  4
AE93-AE-CS  2020-01-01  2020-12-31  4
AE93-AE-CS  2020-01-01  2020-12-31  4
AE93-AE-CS  2020-01-01  2020-12-31  4
</code></pre>
<p>I am using data flow :
<a href=""https://i.stack.imgur.com/XJkaV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XJkaV.png"" alt=""enter image description here"" /></a>
END
Enddate is not coming as excepted :
<a href=""https://i.stack.imgur.com/e6ld5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e6ld5.png"" alt=""enter image description here"" /></a></p>
<p>What is the way I already check todate(ENDDATE,'YYYY-MM-DD') this is giving NULL.</p>
","<azure><tsql><to-date><azure-data-factory>","2021-07-07 10:17:06","2947","1","1","68441562","<p>Use <code>toDate()</code> condition inside <code>iifNull()</code> condition and it will work fine. Also, make sure that <code>MM</code> in date format should be in upper case.</p>
<p>Eg: <code>iifNull(toDate(start, 'MM/dd/yyyy'), toDate(start, 'dd-MM-yyyy'))</code></p>
<p>Find the snippets below.</p>
<p><strong>Source data preview:</strong>
<a href=""https://i.stack.imgur.com/xjkl0.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xjkl0.jpg"" alt=""enter image description here"" /></a></p>
<p><strong>Derived column data preview:</strong>
<a href=""https://i.stack.imgur.com/XS9IB.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XS9IB.jpg"" alt=""enter image description here"" /></a></p>
"
"68258307","Data factory data flow expressions vs dynamic expressions","<p>I'm trying to find the equivalent to doing this in the Wildcard path dataflow expressions. I use this in my data factory dynamic queries</p>
<pre><code>@concat(  pipeline().parameters.source,
              '/dirName/year=',
              formatDateTime(utcnow(), 'yyyy'),
              '/month=',
              formatDateTime(utcnow(), 'MM'),
              '/day=',
              formatDateTime(utcnow(), 'dd')
          )
</code></pre>
<p>I thought it would be something like this</p>
<pre><code>concat( $source,'/dirName/year=',toTimestamp(currentUTC(), 'YYYY', UTC),
              '/month=',
              toTimestamp(currentUTC(), 'MM', UTC),
              '/day=',
             toTimestamp(currentUTC(), 'dd', UTC)
          )
</code></pre>
<p>What I'm trying to do is source from a blob path created with UTC date
<em>example</em>
blah/year=2021/month=07/day=05/.csv</p>
","<nsregularexpression><azure-data-factory>","2021-07-05 15:03:59","105","0","2","68275201","<p>Something easier as toString(year(currentUTC())) will work easier. Or a string interpolation without concat would also work</p>
<p>&quot;/dirName/year={year(currentUTC())}/month={month(currentUTC())}/day={day(currentUTC())}&quot;</p>
"
"68258307","Data factory data flow expressions vs dynamic expressions","<p>I'm trying to find the equivalent to doing this in the Wildcard path dataflow expressions. I use this in my data factory dynamic queries</p>
<pre><code>@concat(  pipeline().parameters.source,
              '/dirName/year=',
              formatDateTime(utcnow(), 'yyyy'),
              '/month=',
              formatDateTime(utcnow(), 'MM'),
              '/day=',
              formatDateTime(utcnow(), 'dd')
          )
</code></pre>
<p>I thought it would be something like this</p>
<pre><code>concat( $source,'/dirName/year=',toTimestamp(currentUTC(), 'YYYY', UTC),
              '/month=',
              toTimestamp(currentUTC(), 'MM', UTC),
              '/day=',
             toTimestamp(currentUTC(), 'dd', UTC)
          )
</code></pre>
<p>What I'm trying to do is source from a blob path created with UTC date
<em>example</em>
blah/year=2021/month=07/day=05/.csv</p>
","<nsregularexpression><azure-data-factory>","2021-07-05 15:03:59","105","0","2","74299004","<p><code>day</code> is not a function in Data flow expression builder. <br />
So, its better to go with:</p>
<pre><code>concat('dirName/year=', toString(currentUTC(), 'yyyy'), '/month=',toString(currentUTC(), 'MM'), '/day=',toString(currentUTC(), 'dd') )
</code></pre>
"
"68223621","How AzureAD knows that device is registered or not?","<p>We are trying to clear some things on conditional access using intune and azureAD. I have enrolled my device with intune and also pushed Microsoft team app using play store account in my device.
When i trying to login into microsoft teams using AzureAD, how AzureAD is getting device related information and using that information decision will be taken.
so basically we want to know data flow between azureAD, device and intune at the time of login.</p>
","<azure-ad-b2c><mdm><intune>","2021-07-02 10:29:54","55","0","1","68226391","<p>So to summarize in a statement, conditional access policies are ANDed i.e. it enforces policies in 2 steps and if all the conditions are not satisfied, the block takes precedence and the user/device needs to satisfy all the conditions set forth to get access to the app/online service to which the device/user has requested.Refer to the below article for more insights</p>
<p><a href=""https://www.vansurksum.com/2020/05/25/may-2020-update-of-the-conditional-access-demystified-whitepaper-workflow-cheat-sheet-implementation-workflow-and-documentation-spreadsheet/"" rel=""nofollow noreferrer"">https://www.vansurksum.com/2020/05/25/may-2020-update-of-the-conditional-access-demystified-whitepaper-workflow-cheat-sheet-implementation-workflow-and-documentation-spreadsheet/</a></p>
"
"68203166","Spring cloud data flow task Invalid TaskExecution ID","<p>I am just trying to create a hello world SCDF task. It is my understanding that out of the box the task should be able to read the needed data from the h2 database by just including h2 in the pom, but I am wondering if that is the correct assumption. Can anybody lend any suggestions to why I am getting the following error: 'taskLifecycleListener'; nested exception is java.lang.IllegalArgumentException: Invalid TaskExecution, ID...</p>
<p>Task</p>
<pre><code>package com.hello;

import org.springframework.boot.CommandLineRunner;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.task.configuration.EnableTask;
import org.springframework.context.annotation.Bean;

@SpringBootApplication
@EnableTask
public class HelloTask {

    @Bean
    public CommandLineRunner commandLineRunner() {
        return new HelloWorldCommandLineRunner();
    }

    public static void main(String[] args) {
        SpringApplication.run(HelloTask.class, args);
    }

    public static class HelloWorldCommandLineRunner implements CommandLineRunner {

        
        public void run(String... strings) throws Exception {
            System.out.println(&quot;Hello World!&quot;);
        }
    }
}
</code></pre>
<p>POM</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
    xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.5.2&lt;/version&gt;
        &lt;relativePath /&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;groupId&gt;com.hello&lt;/groupId&gt;
    &lt;artifactId&gt;privet&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;privet&lt;/name&gt;
    &lt;description&gt;Demo project for Spring Boot&lt;/description&gt;
    &lt;properties&gt;
        &lt;java.version&gt;11&lt;/java.version&gt;
        &lt;spring-cloud.version&gt;2020.0.3&lt;/spring-cloud.version&gt;
        &lt;start-class&gt;com.hello.HelloTask&lt;/start-class&gt;
    &lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-starter-task&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.h2database&lt;/groupId&gt;
            &lt;artifactId&gt;h2&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt;
            &lt;optional&gt;true&lt;/optional&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
    &lt;dependencyManagement&gt;
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
                &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
                &lt;version&gt;${spring-cloud.version}&lt;/version&gt;
                &lt;type&gt;pom&lt;/type&gt;
                &lt;scope&gt;import&lt;/scope&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/dependencyManagement&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.1.0&lt;/version&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;
</code></pre>
<p>I run spring cloud data flow skipper 2.8.1</p>
<p>I run spring cloud data flow server 2.8.1, obviously launches h2</p>
<pre><code>Start Embedded H2
2021-06-28 19:08:43.967  INFO 95256 --- [           main] o.s.c.d.s.config.web.WebConfiguration    : Starting H2 Server with URL: jdbc:h2:tcp://localhost:19092/mem:dataflow
</code></pre>
<p>I build the app, and in the UI - import the app, create a task, start the task and I get the dreaded....</p>
<pre><code>org.springframework.context.ApplicationContextException: Failed to start bean 'taskLifecycleListener'; nested exception is java.lang.IllegalArgumentException: Invalid TaskExecution, ID 7 not found
    at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:181) ~[spring-context-5.3.8.jar!/:5.3.8]
</code></pre>
<p>Any suggestions?</p>
","<spring-cloud-dataflow><spring-cloud-task>","2021-07-01 01:12:43","407","0","1","68369685","<p>Spring Cloud Data Flow and your Spring Cloud Task app need to access the same database. Your <code>pom.xml</code> suggests that your SCT app is using an embedded H2 database that only itself can access.</p>
<p>Your app needs the driver for the shared database on the classpath. Please also have a look at the official documentation: <a href=""https://docs.spring.io/spring-cloud-dataflow/docs/2.8.1/reference/htmlsingle/#spring-cloud-dataflow-register-task-apps"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-cloud-dataflow/docs/2.8.1/reference/htmlsingle/#spring-cloud-dataflow-register-task-apps</a></p>
<p>The task execution with its <code>id</code> is created by Data Flow before launching the app. When the app starts, it expects to find a task execution with the given id in the database that it's connected to.</p>
<p>If you start your app stand alone from the command line, it should work fine, as it then does not expect that the task execution is already created in its database.</p>
"
"68187754","Azure Data Factory data flow - drops null columns","<p>When using a data flow in azure data factory to move data, I've noticed that the data (at the sink) is missing columns that contains NULL values. When using the copy activity to copy the same data, the columns are present in the sink with their NULL values.</p>
<p>Record after a copy activity:
<a href=""https://i.stack.imgur.com/3KlR5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3KlR5.png"" alt=""Record after a copy activity"" /></a></p>
<p>Record after a data flow:
<a href=""https://i.stack.imgur.com/Csnu6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Csnu6.png"" alt=""Record after a data flow"" /></a></p>
<p>Source is parquet, sink is azure cosmos db. My goal is to avoid defining any schemas, as I simply want to copy all of the data &quot;as is&quot;. I've used the &quot;allow schema drift&quot; option on the source and sink.</p>
<p>I would just use the copy activity, but it doesn't appear to have the ability to define a maximum speed (RU consumption) like the data flow does, so the copy activity ends up consuming all of the cosmos db's RUs very quickly (as described <a href=""https://stackoverflow.com/questions/61954653/azure-data-factory-copy-activity-data-flow-consumes-all-rus-in-cosmosdb"">here</a>)</p>
<hr />
<p>EDIT:</p>
<p>sink data preview shows all columns
<a href=""https://i.stack.imgur.com/C1CnC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/C1CnC.png"" alt=""sink data tab"" /></a></p>
<p>sink inspect tab shows all columns
<a href=""https://i.stack.imgur.com/XV6rc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XV6rc.png"" alt=""sink inspect tab"" /></a></p>
","<azure-data-factory>","2021-06-30 02:34:54","1120","1","2","68229645","<p>Dataflows always skip writing JSON tags with NULLs. There is no workaround currently other than copy activity.</p>
"
"68187754","Azure Data Factory data flow - drops null columns","<p>When using a data flow in azure data factory to move data, I've noticed that the data (at the sink) is missing columns that contains NULL values. When using the copy activity to copy the same data, the columns are present in the sink with their NULL values.</p>
<p>Record after a copy activity:
<a href=""https://i.stack.imgur.com/3KlR5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3KlR5.png"" alt=""Record after a copy activity"" /></a></p>
<p>Record after a data flow:
<a href=""https://i.stack.imgur.com/Csnu6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Csnu6.png"" alt=""Record after a data flow"" /></a></p>
<p>Source is parquet, sink is azure cosmos db. My goal is to avoid defining any schemas, as I simply want to copy all of the data &quot;as is&quot;. I've used the &quot;allow schema drift&quot; option on the source and sink.</p>
<p>I would just use the copy activity, but it doesn't appear to have the ability to define a maximum speed (RU consumption) like the data flow does, so the copy activity ends up consuming all of the cosmos db's RUs very quickly (as described <a href=""https://stackoverflow.com/questions/61954653/azure-data-factory-copy-activity-data-flow-consumes-all-rus-in-cosmosdb"">here</a>)</p>
<hr />
<p>EDIT:</p>
<p>sink data preview shows all columns
<a href=""https://i.stack.imgur.com/C1CnC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/C1CnC.png"" alt=""sink data tab"" /></a></p>
<p>sink inspect tab shows all columns
<a href=""https://i.stack.imgur.com/XV6rc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XV6rc.png"" alt=""sink inspect tab"" /></a></p>
","<azure-data-factory>","2021-06-30 02:34:54","1120","1","2","74942866","<p><strong>This is really not a good design or behavior on Microsoft's part</strong>  because you can't Standardize in Cosmos weather to &quot;Keep&quot; or &quot;Remove&quot; null fields in your JSON.
Querying Cosmos
Where field1 = NULL is  completely different than where NOT IS_DEFINED (field1)  and will yield an entirely different result set.</p>
<p>And if your users don't know if the ADF developer used a Dataflow with a Sink vs a Copy Activity in a Pipeline then the may get erroneous results in a query. The only way to ensure you get all the data is to always use:
Where field1 = NULL or where NOT IS_DEFINED (field1)</p>
<p>Users should not have to depend on knowing what kind of ADF functionality was chosen for a specific JSON document in a Cosmos NoSQL collection to do a query.  Plus you can't standardize that you will &quot;Keep&quot; null across all Cosmos documents or you will &quot;Remove&quot; nulls across all Cosmos documents. Unless you force everyone to use Pipelines only or Dataflows only. Depending on the complexity using Pipeline only is not always possible. But using Dataflow only is also not always needed.</p>
"
"68177521","How to disable MDM kiosk mode programmatically","<p>I am facing an issue with Android Management API policy <strong>install type: KIOSK</strong>.</p>
<p>I have fully enrolled the device and fully working kiosk mode without a way to leave it, but unfortunately one of the libs (iZettle) required bluetooth connection but KIOSK modes hides the dialog immediately without leaving an option to connect a bluetooth device. Is there are way to disable <strong>KIOSK</strong> mode for a moment from the app in order to setup all the required equipment and turn it on again?</p>
<p>Kind regards</p>
","<android><kotlin><enterprise><mdm>","2021-06-29 11:05:20","284","0","1","72330664","<p>In kiosk mode, all packages except explicitly enabled ones are silently blocked by the OS. So, to make your app working as required without turning off kiosk mode, you need to find a blocked package and add it to the list of enabled apps.</p>
<p>You can achieve this by <a href=""https://qa.h-mdm.com/5840/how-to-get-diagnostic-info-logcat-from-the-android-device"" rel=""nofollow noreferrer"">collecting logcat</a> from the device.</p>
<ol>
<li><p>Install <a href=""https://developer.android.com/studio"" rel=""nofollow noreferrer"">Android Studio</a>.</p>
</li>
<li><p>Turn on the developer mode on the device.</p>
</li>
<li><p>Start collecting logcat.</p>
</li>
<li><p>Raise the issue (try to open Bluetooth in kiosk mode).</p>
</li>
<li><p>Search for messages like this:</p>
<pre><code>10-12 09:56:10.149  1408 12311 E ActivityTaskManager: Attempted Lock Task Mode violation mStartActivity=ActivityRecord{db0c8b6 u0 com.android.server.telecom/.PrivilegedCallActivity t160}
</code></pre>
</li>
</ol>
<p>This message means that you need to add the <code>com.android.server.telecom</code> package to the kiosk mode policy (in your case, the package id may be different!).</p>
"
"68143899","Decrease data flow in get firebase realtime database","<p>I have this structure in my bd realtime database in firebase google</p>
<p><img src=""https://i.stack.imgur.com/gp0FN.png"" alt=""enter image description here"" /></p>
<p>I would like to retrieve only the data from the rooms, without the data from the children, but when I add a &quot;listener&quot; on no value, the data returned includes the children. How to do this to reduce the amount of data transferred?</p>
<pre><code>roomRef.on('value',
    (snapshot) =&gt; {
                        console.log(snapshot.val());
                    });
</code></pre>
<p><a href=""https://i.stack.imgur.com/gp0FN.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<javascript><firebase><firebase-realtime-database>","2021-06-26 15:36:55","63","0","1","68143963","<p>The Firebase SDKs always retrieve full nodes. There is no way through the JavaScript SDK to get the keys without also getting the child data.</p>
<p>If you find yourself needing this, it typically means that you've <a href=""https://firebase.google.com/docs/database/web/structure-data#avoid_nesting_data"" rel=""nofollow noreferrer"">nested data  more than you should</a>.</p>
<p>For example, in your case I'd recommend putting the questions in a separate higher-level node, likely with the same key <code>-McyCb...</code>. That way you can initially load the other non-question data with one call, and then perform an extra call when you need to questions</p>
"
"68127250","Using SQLAdapter and SQLCommandBuilder without a Primary Key","<p>I'm exploring the use of <code>SqlCommandBuilder</code> alongside <code>Adapter.Update()</code> to synchronize a DataGridView with an SQL Database table.</p>
<p>I want to auto-generate SQL Update statements using <code>SqlCommandBuilder.GetUpdateCommand()</code>, however, it fails with
&quot;<code>Dynamic SQL generation for the UpdateCommand is not supported against a SelectCommand that does not return any key column information</code>&quot;. This makes sense, because my table <strong>doesn't have a primary key</strong>.</p>
<p>I cannot set the primary key on the source table, but I do have an identity column.
I'd like to specify to the command builder which column to use as the primary key. There is such a feature on the DataTable class, but it seems to have no effect on the <code>SqlCommandBuilder</code>.</p>
<p>I tried the following:</p>
<pre><code>// Add Primary Key to help command builder identify unique rows
Table.PrimaryKey = new DataColumn[] { Table.Columns[&quot;ComponentID&quot;] };
</code></pre>
<p>But it seems that this information does not propagate to the <code>SqlDataAdapter</code> and <code>SqlCommandBuilder</code> because I still get the error.</p>
<p>Here's the order I've tried:</p>
<pre><code>// get data
Adapter.Fill(Table);

// specify primary key column
Table.PrimaryKey = new DataColumn[] { Table.Columns[&quot;ComponentID&quot;] };

cmdBuilder = new SqlCommandBuilder(Adapter);

cmdBuilder.GetUpdateCommand() // &lt;-- Error here
</code></pre>
<p>Are there any solutions here at all, or do I have to specify the update and insert statements?</p>
","<c#><sql><.net><data-synchronization>","2021-06-25 07:39:17","531","2","1","68146181","<p>So, as pointed out by @PanagiotisKanavos, the <code>SqlCommandBuilder</code> does not support tables without primary keys, even if you set it in the <code>DataTable</code> object.</p>
<p>Therefore, I had no choice but write my own Command Builder.</p>
<p>To use it, you need to provide:</p>
<ul>
<li>The SqlConnection to use</li>
<li>The Database (if not provided in the connection)</li>
<li>The SQLAdapter, with the Select Command already set (there's a constructor for that)</li>
</ul>
<p>How you use it:</p>
<pre><code>string selectQuery = &quot;SELECT * FROM [dbCache].[dbo].[Component] ORDER BY [ComponentType] DESC&quot;;

// Initialize the SqlDataAdapter object by specifying a Select command 
// that retrieves data from the table.
Adapter = new SqlDataAdapter(selectQuery, Connection)
{
    FillLoadOption = LoadOption.PreserveChanges,
    MissingSchemaAction = MissingSchemaAction.AddWithKey
};

// build all sql commands
Adapter = SQLCommandBuilder.BuildAll(Adapter, Connection);
</code></pre>
<p>Next, the full class code:</p>
<pre><code>public static class SQLCommandBuilder
{
    public enum CommandType
    {
        Update = 0,
        Insert = 1,
        Delete = 2
    }

    /// &lt;summary&gt;
    /// Build and add the insert, update and delete commands to the given SqlAdapter
    /// &lt;/summary&gt;
    /// &lt;param name=&quot;adapter&quot;&gt;&lt;/param&gt;
    /// &lt;param name=&quot;connection&quot;&gt;&lt;/param&gt;
    /// &lt;param name=&quot;database&quot;&gt;&lt;/param&gt;
    /// &lt;param name=&quot;idColumns&quot;&gt;&lt;/param&gt;
    /// &lt;returns&gt;the modified adapter&lt;/returns&gt;
    public static SqlDataAdapter BuildAll(
        SqlDataAdapter adapter, SqlConnection connection, string database = null, string[] idColumns = null
        )
    {
        DataTable data = new DataTable();
        // fill datatable with select data
        adapter.Fill(data);

        if (database == null)
        {
            if (string.IsNullOrEmpty(connection.Database))
            {
                throw new ArgumentException(
                    &quot;Could not determine database from connection object. Please specify it manually&quot;
                    );
            }
            // get database from connection
            database = connection.Database;
        }
        // get table name
        string table = data.TableName;
        // get all column names
        string[] allColumns = data.Columns.Cast&lt;DataColumn&gt;()
            .Select(col =&gt; col.ColumnName).ToArray();
        
        // only get id columns if the user has not manually specified them
        if (idColumns == null)
        {
            // get id columns from the table. This includes any unique or auto-incrementing column
            idColumns = data.Columns.Cast&lt;DataColumn&gt;()
                .Where(col =&gt; col.AutoIncrement || col.Unique)
                .Select(col =&gt; col.ColumnName)
                .ToArray();

            // if no id columns found
            if (idColumns.Length == 0)
            {
                // throw an error
                throw new Exception(&quot;No ID columns found in the table!&quot;);
            }
        }
        else
        {
            // if the specfified columns don't exist
            if (idColumns.All(id =&gt; allColumns.Contains(id, StringComparer.CurrentCultureIgnoreCase)))
            {
                // throw an error
                throw new ArgumentException(&quot;Provided ID columns do not exist in the table!&quot;);
            }
        }
        

        // generate all commands
        adapter.InsertCommand =
            BuildCommand(CommandType.Insert, connection, database, table, allColumns, idColumns);
        adapter.UpdateCommand =
            BuildCommand(CommandType.Update, connection, database, table, allColumns, idColumns);
        adapter.DeleteCommand =
            BuildCommand(CommandType.Delete, connection, database, table, allColumns, idColumns);

        // return the modified adapter
        return adapter;
    }

    /// &lt;summary&gt;
    /// Build a command of the given type using the provided parameters
    /// &lt;/summary&gt;
    /// &lt;param name=&quot;cmdtype&quot;&gt;&lt;/param&gt;
    /// &lt;param name=&quot;connection&quot;&gt;&lt;/param&gt;
    /// &lt;param name=&quot;database&quot;&gt;&lt;/param&gt;
    /// &lt;param name=&quot;table&quot;&gt;&lt;/param&gt;
    /// &lt;param name=&quot;allColumns&quot;&gt;&lt;/param&gt;
    /// &lt;param name=&quot;idColumns&quot;&gt;&lt;/param&gt;
    /// &lt;returns&gt;&lt;/returns&gt;
    public static SqlCommand BuildCommand(
        CommandType cmdtype, SqlConnection connection, string database, string table, 
        string[] allColumns, string[] idColumns
        )
    {
        if (allColumns == null || allColumns.Length == 0)
        {
            throw new ArgumentNullException(&quot;allColumns&quot;, &quot;allColumns cannot be null or empty!&quot;);
        }
        if (idColumns == null || idColumns.Length == 0)
        {
            throw new ArgumentNullException(&quot;idColumns&quot;, &quot;idColumns cannot be null or empty!&quot;);
        }

        string strCommand = null;

        switch (cmdtype)
        {
            case CommandType.Insert:

                // get columns to set values for. Id columns not included because they should
                // be set by the table
                string[] insertCols = allColumns.Except(idColumns).ToArray();

                strCommand =
                    &quot;INSERT INTO [&quot; + database + &quot;].[dbo].[&quot; + table + &quot;]\n&quot; +
                    &quot;([&quot; + string.Join(&quot;], [&quot;, insertCols) + &quot;])\n&quot; +
                    &quot;VALUES (@&quot; + string.Join(&quot;, @&quot;, insertCols.Select(s =&gt; s.Replace(&quot; &quot;, &quot;&quot;))) + &quot;)&quot;;
                break;
            case CommandType.Update:
                // compare each id column to a paremeterized variable of the same name prefixed with &quot;old&quot;
                string[] idCompsOld = idColumns
                    .Select(col =&gt; &quot;[&quot; + col + &quot;] = @old&quot; + col.Replace(&quot; &quot;, &quot;&quot;))
                    .ToArray();

                // create a setting statement. Don't set id columns, as they should never be modifiable
                string[] setStatement = allColumns.Except(idColumns)
                    .Select(col =&gt; &quot;[&quot; + col + &quot;] = @&quot; + col.Replace(&quot; &quot;, &quot;&quot;))
                    .ToArray();

                strCommand =
                    &quot;UPDATE [&quot; + database + &quot;].[dbo].[&quot; + table + &quot;]\n&quot; +
                    &quot;SET &quot; + string.Join(&quot;, &quot;, setStatement) + &quot;\n&quot; +
                    &quot;WHERE &quot; + string.Join(&quot; AND &quot;, idCompsOld);
                break;
            case CommandType.Delete:
                // compare each id column to a paremeterized variable of the same name
                string[] idComps = idColumns
                    .Select(col =&gt; &quot;[&quot; + col + &quot;] = @&quot; + col.Replace(&quot; &quot;, &quot;&quot;))
                    .ToArray();
                strCommand =
                    &quot;DELETE FROM [&quot; + database + &quot;].[dbo].[&quot; + table + &quot;]\n&quot; +
                    &quot;WHERE &quot; + string.Join(&quot; AND &quot;, idComps);
                break;
        }

        SqlCommand command = new SqlCommand(strCommand, connection);

        // cycle through all columns
        for( int i = 0; i &lt; allColumns.Length; i++)
        {
            string col = allColumns[i];

            // create a parameter for that column
            SqlParameter para = new SqlParameter()
            {
                ParameterName = &quot;@&quot; + col.Replace(&quot; &quot;, &quot;&quot;),
                SourceColumn = col
            };
            // add the paramter to the command
            command.Parameters.Add(para);

            // in the special case of the update statement, extra parameters are needed for the
            // old values
            if (cmdtype == CommandType.Update)
            {
                // create a parameter for that column
                para = new SqlParameter()
                {
                    ParameterName = &quot;@old&quot; + col.Replace(&quot; &quot;, &quot;&quot;),
                    SourceColumn = col,
                    SourceVersion = DataRowVersion.Original
                };
                // add the paramter to the command
                command.Parameters.Add(para);
            }
        }

        return command;
    }
}
</code></pre>
<p>Using the following code to print the commands:</p>
<pre><code>// Display the Update, Insert, and Delete commands that were automatically generated
// by the SQLCommandBuilder.
Console.WriteLine(&quot;Update command : &quot;);
Console.WriteLine(Adapter.UpdateCommand.CommandText);
Console.WriteLine();

Console.WriteLine(&quot;Insert command : &quot;);
Console.WriteLine(Adapter.InsertCommand.CommandText);
Console.WriteLine();

Console.WriteLine(&quot;Delete command : &quot;);
Console.WriteLine(Adapter.DeleteCommand.CommandText);
Console.WriteLine();
</code></pre>
<p>I get:</p>
<pre><code>Update command : 
UPDATE [dbCache].[dbo].[Component]
SET [ComponentType] = @ComponentType, [Drawings] = @Drawings, [StatusNo] = @StatusNo
WHERE [ComponentlD] = @oldComponentlD 

Insert command :
INSERT INTO [dbCache].[dbo].[Component]
([ComponentType], [Drawings], [StatusNo])
VALUES (@ComponentType, @Drawings, @StatusNo) 

Delete command :
DELETE FROM [dbCache].[dbo].[Component]
WHERE [ComponentlD] = @ComponentlD 
</code></pre>
<p>I have tested the insert, update and delete statements and they seem to all work!</p>
"
"68093723","It may take a long time to start a data flow job","<p>When I start a data flow job, it sometimes waits for more than 30 minutes without being allocated an instance.</p>
<p>What is happen??</p>
","<google-cloud-platform><google-cloud-dataflow><apache-beam>","2021-06-23 04:57:12","1029","0","1","68099787","<p>Your Dataflow Job is getting slow because the time needed to start the VMs on Google Compute Engine grows with the number of VMs you start, and in general VM startup and shutdown performance can have high variance.</p>
<p>you can look at Cloud Logs for your job ID, and see if there's any logging going on, also you can check the Dataflow monitoring interface inside your Dataflow job.[1]</p>
<p>you can enable autoscaling[2] instead of specifying a large number of instances manually, it should gradually scale to the appropriate number of VMs at the appropriate moment in the job's lifetime.</p>
<p>Without autoscaling, you have to choose a fixed number of workers by specifying workers to execute your pipeline. As the input workload varies over time, this number can become either too high or too low. Provisioning too many workers results in unnecessary extra cost, and provisioning too few workers results in higher latency for processed data. By enabling autoscaling, resources are used only as they are needed.</p>
<p>The objective of autoscaling  is to minimize backlog while maximizing worker utilization and throughput, and quickly react to spikes in load.</p>
<p>[1] <a href=""https://cloud.google.com/dataflow/docs/guides/using-monitoring-intf"" rel=""nofollow noreferrer"">https://cloud.google.com/dataflow/docs/guides/using-monitoring-intf</a></p>
<p>[2] <a href=""https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#streaming-autoscaling"" rel=""nofollow noreferrer"">https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#streaming-autoscaling</a></p>
"
"68089581","SAP MDG see Change Documents of a given Business Partner?","<p>I would like to know if there's a way or some query to see the Change Documents out of a given Business Partner using SAP or SAP Webdynpro GUI. I've googled but tables such as CDHDR or CDPOS are not working for me. Any other thoughts?</p>
","<sap-erp><master-data-management>","2021-06-22 19:17:21","819","1","1","68095495","<p>You should check and enable (if they were disabled) the change document object BUPA_BUP in SCDO transaction, with the following values</p>
<p><a href=""https://i.stack.imgur.com/4WUXa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4WUXa.png"" alt=""enter image description here"" /></a></p>
<p>Check this mini-guide: <a href=""http://abapcadabra.com/index.php/cookies/541-change-documents"" rel=""nofollow noreferrer"">http://abapcadabra.com/index.php/cookies/541-change-documents</a></p>
"
"68089313","How to pass linked services parameters to data flow from pipeline?","<p>I have a source on Data Flow where I want to use a parameter to make a dynamic endpoint, I want to set the endpoint value from the data pipeline, I'm using a parameter created on the linked service but I didn't not found how is the appropriate way to set the value to that parameter from the pipeline.</p>
<p><a href=""https://i.stack.imgur.com/xmGzX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xmGzX.png"" alt=""enter image description here"" /></a></p>
<p>I tried to create a parameter on the dataflow and set the linked service parameter default value pointed to data flow parameter but it was not successful.</p>
","<azure><azure-data-factory>","2021-06-22 18:53:48","630","1","1","68092801","<p>Please try this:</p>
<ol>
<li>create a parameter in the linked service as you do.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/8yU9y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8yU9y.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>create a parameter in the dataset and pass it's value to the parameter in the linked service.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/onF4K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/onF4K.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/RDFjd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RDFjd.png"" alt=""enter image description here"" /></a></p>
<ol start=""3"">
<li>create a parameter in your pipeline and set the default value.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/dFhKd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dFhKd.png"" alt=""enter image description here"" /></a></p>
<ol start=""4"">
<li>pass the value from pipeline's parameter to your dataset in the setting of Data Flow.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/rMGK5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rMGK5.png"" alt=""enter image description here"" /></a></p>
"
"68059407","Can a Kusto table have multiple update policy?","<p>I have a Kusto table and want to ingest data from two azure blob locations. Data from both sources need some transformation, hence I defined two update policies. So now I have two source tables and one destination table. The first update policy was working fine, and then I went on defining the second update policy which also worked fine. But after that when I observe, data was not getting ingested from the first source(It is present in the temporary source table) to the target table. I wonder if defining the second update policy somehow disabled the first update policy? Is it possible to have two update policies on one destination table?</p>
","<bigdata><azure-data-explorer><kql><data-mapping>","2021-06-20 19:33:20","491","2","1","68059565","<blockquote>
<p>Is it possible to have two update policies on one destination table?</p>
</blockquote>
<p>Yes, it is possible.</p>
<p>You may want to verify that you have an array of 2 update policies defined on the target table, and that you didn't overwrite the first with the second using the <code>.alter</code> policy command.</p>
"
"68051596","Azure Data factory passing parameters to data flow","<p>I have a requirement to pass Year, Month, Day from pipeline to dataflow and use these 3 parameter values in dataflow to read files from corresponding folder in ADLS.</p>
<p>To start with I have created 3 parameters at data flow level and used these 3 parameters in &quot;Source&quot; &quot;Source Options&quot; under wildcard paths but it's giving me error message as &quot;not file/folder exists and is not hidden&quot;.</p>
<p>How can I pass the parameters from pipeline and use those values at dataflow level?</p>
","<azure-data-factory>","2021-06-19 23:58:02","2709","0","1","68061483","<p>Please make sure there's file/folder under wildcard paths. Then try this:</p>
<ol>
<li>Create three parameters named Year, Month and Day in the pipeline.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/8XLmU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8XLmU.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>Create the correspond parameters at data flow level.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/UD6SH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UD6SH.png"" alt=""enter image description here"" /></a></p>
<p>3.Pass the parameters form pipeline to data flow.</p>
<p><a href=""https://i.stack.imgur.com/LWTVS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LWTVS.png"" alt=""enter image description here"" /></a></p>
<ol start=""4"">
<li>Use this expression <code>concat('/',$Year,'/',$Month,'/',$Day,'/*')</code> in the Wildcard paths setting.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/ofC6t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ofC6t.png"" alt=""enter image description here"" /></a></p>
"
"68043640","How to set ingestion mapping for TSV or W3CLOGFILE format in Kusto database in Azure Data Explorer?","<p>I am relatively new to Kusto. I was wondering how do we specify ingestion mappings in Kusto for formats other than CSV, JSON, AVRO, ORC, or Parquet. I could see examples only for these data formats in the documentation. I want to create an ingestion mapping for TSV or W3CLOGFILE format, and ingest data through the built-in pipeline in ADX from azure blob storage.</p>
","<json><csv><azure-data-explorer><kql><data-mapping>","2021-06-19 04:34:04","211","0","1","68045120","<p>For TSV it is the same as CSV, see relevant <a href=""https://learn.microsoft.com/en-us/azure/data-explorer/kusto/management/mappings#csv-mapping"" rel=""nofollow noreferrer"">doc</a> comment:</p>
<blockquote>
<p>CSV mapping can be applied on all the delimiter-separated formats:
CSV, TSV, PSV, SCSV, and SOHsv.</p>
</blockquote>
<p>As for W3CLOGFILE, the mapping format is the same as for <a href=""https://learn.microsoft.com/en-us/azure/data-explorer/kusto/management/mappings#avro-mapping"" rel=""nofollow noreferrer"">AVRO mapping</a></p>
"
"68034930","Is there a way around data flow diagrams rules?","<p>Can data come out of a data store and then be processed and then go to an external entity?</p>
<p><img src=""https://i.stack.imgur.com/sKfcM.jpg"" alt=""the image shows that cold water going to reservoir and then to the heater through U-shaped tubes and then to sprinkler and into container which would process coffee beans and hot water and pour into jug "" /></p>
<p>My question is that is it ok that data (coffee and hot water) are coming out of a data store (container) and then are processed into coffee and then poured into an external entity which is the jig? Is this dfd obeying dfd rules?</p>
","<dataflow><dataflow-diagram>","2021-06-18 12:09:48","372","1","1","68035486","<ol>
<li><p>DFDs can never contain <code>user</code> as an entity, rather you should represent some functionalities through which the data flows eg. login, authentication etc</p>
</li>
<li><p>Your data here seems correct , you can just optimize the parallel flows using <code>joins</code> and <code>forks</code></p>
</li>
</ol>
"
"68023742","In Azure data factory, is it possible to control the filenames of numerous output files without using a data flow?","<p>For instance, I may use a copy activity in data factory to copy a 10 million record customer table into an Azure data lake, and use partition option of 'dynamic range' in the source options. My understanding is that this would result in data factory splitting the data into numerous files in the lake.</p>
<p>Using this method, how do I force a naming convention for the outputted files in the lake? e.g. so each of the filenames begin with 'cust_', meaning the files would be called cust_1, cust_2, cust_3, cust_4 etc.</p>
","<azure-data-factory>","2021-06-17 17:05:53","458","1","1","68028635","<p>My understanding is partition option of 'dynamic range' is used to split a file into multiple files and then perform asynchronous copying. This is a multi-threaded operation to increase the copy speed. It is <a href=""https://learn.microsoft.com/azure/data-factory/copy-activity-performance-features"" rel=""nofollow noreferrer"">Copy activity performance optimization features</a>. I think this is not the file splitting you want.</p>
<ol>
<li><p>Select 'None' at source setting.
<a href=""https://i.stack.imgur.com/m7MCM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m7MCM.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>We can set 'File extension', 'Max rows per file' and 'File name prefix' at sink setting.
<a href=""https://i.stack.imgur.com/kv3CG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kv3CG.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>In my side, ADF will automatically split into multiple files, each file contains 50 rows of records.
<a href=""https://i.stack.imgur.com/NKAJ2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NKAJ2.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>
"
"67961667","How data flows between worker nodes in Kubernetes?","<p>I have a single master cluster with 3 worker nodes. The master node has one network interface of 10Gb capacity and all worker nodes have a 40Gb interface. They are all connected via a switch.</p>
<p>I'd like to know if this might create a bottleneck if the data between nodes have to pass through the master node?</p>
<p>In general, I like to understand the communication flow between worker nodes. For instance, a pod in node1 sends data to a pod in node2, does the traffic go through the master node? I have seen the architecture diagram on the Kubernetes docs and it appears to be the case:</p>
<p><a href=""https://i.stack.imgur.com/RUasH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RUasH.png"" alt=""enter image description here"" /></a>
source: <a href=""https://kubernetes.io/docs/concepts/overview/components/"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/concepts/overview/components/</a></p>
<p>If this is the case, it is possible to define a control plane network separate from the data plane by possibly adding another interface to worker nodes?</p>
<p>Please note that this is a bare-metal on-prem installation with OSS Kubernetes v1.20.</p>
","<kubernetes>","2021-06-13 18:52:07","219","0","1","67961876","<blockquote>
<p>For instance, a pod in node1 sends data to a pod in node2, does the traffic go through the master node?</p>
</blockquote>
<p>No. Kubernetes is designed with a flat network model. If Pod on node A send a request to Pod on node B, the inter-node traffic is directly from node A to node B as they are on the same IP network.</p>
<p>See also <a href=""https://kubernetes.io/docs/concepts/cluster-administration/networking/#the-kubernetes-network-model"" rel=""nofollow noreferrer"">The Kubernetes network model</a></p>
"
"67856560","Azure data factory tempdb usage in data flow","<p>I am using data flow to copy large data to sql azure database.</p>
<p>I recieved an error</p>
<pre><code> Operation on target Data flow1 failed: {&quot;StatusCode&quot;:&quot;DFExecutorUserError&quot;,&quot;Message&quot;:&quot;Job 
 failed due to reason: at Sink 'xxxx': 
 shaded.msdataflow.com.microsoft.sqlserver.jdbc.SQLServerException: The database 'tempdb' has 
 reached its size quota. Partition or delete data, drop indexes, or consult the documentation 
 for possible resolutions.&quot;,&quot;Details&quot;:&quot;at Sink 'xxx': 
 shaded.msdataflow.com.microsoft.sqlserver.jdbc.SQLServerException: The database 'tempdb' has 
 reached its size quota. Partition or delete data, drop indexes, or consult the documentation 
 for possible resolutions.&quot;}
</code></pre>
<p>To resolve, i tried creating a different schema in the data flow setting and unchecked &quot;Use tempdb&quot; option and provided with schema name of schema i created.</p>
<p>I continue to get the same error with not much information availabe about solving thise.</p>
","<azure-sql-database><azure-data-factory>","2021-06-06 07:00:21","675","1","1","67865424","<p>By default, Azure SQL sink use the global temp DB to stage the data and the size of global temp DB depends on your service tier. You can either increase the size by increasing the service tier, or use a normal user table to stage the data instead of table from global temp DB by unchecking the &quot;Use tempdb&quot; option and publish the changes. Please note that using user table to stage data will impact the performance significantly.</p>
"
"67848019","Are iOS MDM profile config file meant to be public?","<p>Below is my sample MDM <em>profile.mobileconfig</em> file.</p>
<pre><code>&lt;plist version=&quot;1.0&quot;&gt;
&lt;dict&gt;
&lt;key&gt;PayloadContent&lt;/key&gt;
&lt;array&gt;
    &lt;dict&gt;
        &lt;key&gt;AccessRights&lt;/key&gt;
        &lt;integer&gt;8191&lt;/integer&gt;
        &lt;key&gt;CheckInURL&lt;/key&gt;
        &lt;string&gt; [my url] &lt;/string&gt;
        &lt;key&gt;CheckOutWhenRemoved&lt;/key&gt;
        &lt;true/&gt;
        &lt;key&gt;PayloadDescription&lt;/key&gt;
        &lt;string&gt;Configures Mobile Device Management&lt;/string&gt;
        &lt;key&gt;PayloadDisplayName&lt;/key&gt;
        &lt;string&gt;Mobile Device Management&lt;/string&gt;
        &lt;key&gt;PayloadIdentifier&lt;/key&gt;
        &lt;string&gt;com.mytests.mdm&lt;/string&gt;
        &lt;key&gt;PayloadOrganization&lt;/key&gt;
        &lt;string&gt; [my organisation] &lt;/string&gt;
        &lt;key&gt;PayloadType&lt;/key&gt;
        &lt;string&gt;com.apple.mdm&lt;/string&gt;
        &lt;key&gt;PayloadUUID&lt;/key&gt;
        &lt;string&gt; [my payload] &lt;/string&gt;
        &lt;key&gt;PayloadVersion&lt;/key&gt;
        &lt;integer&gt;1&lt;/integer&gt;
        &lt;key&gt;ServerURL&lt;/key&gt;
        &lt;string&gt; [my url] &lt;/string&gt;
        &lt;key&gt;SignMessage&lt;/key&gt;
        &lt;true/&gt;
        &lt;key&gt;Topic&lt;/key&gt;
        &lt;string&gt;com.novabase.mdm.pushcert&lt;/string&gt;
        &lt;key&gt;UseDevelopmentAPNS&lt;/key&gt;
        &lt;true/&gt;
    &lt;/dict&gt;
&lt;/array&gt;
&lt;key&gt;PayloadDescription&lt;/key&gt;
&lt;string&gt;Profile description.&lt;/string&gt;
&lt;key&gt;PayloadDisplayName&lt;/key&gt;
&lt;string&gt;MDM test&lt;/string&gt;
&lt;key&gt;PayloadIdentifier&lt;/key&gt;
&lt;string&gt;com.mytests.mdm&lt;/string&gt;
&lt;key&gt;PayloadOrganization&lt;/key&gt;
&lt;string&gt; [my organisation] &lt;/string&gt;
&lt;key&gt;PayloadRemovalDisallowed&lt;/key&gt;
&lt;false/&gt;
&lt;key&gt;PayloadType&lt;/key&gt;
&lt;string&gt;Configuration&lt;/string&gt;
&lt;key&gt;PayloadUUID&lt;/key&gt;
&lt;string&gt;11-22-33-44&lt;/string&gt;
&lt;key&gt;PayloadVersion&lt;/key&gt;
&lt;integer&gt;1&lt;/integer&gt;
&lt;key&gt;Password&lt;/key&gt;
&lt;string&gt;123456&lt;/string&gt;
&lt;key&gt;PayloadCertificateFileName&lt;/key&gt;
&lt;string&gt;mdmkey.p12&lt;/string&gt;
&lt;key&gt;PayloadContent&lt;/key&gt;
&lt;data&gt;MY SIGNING KEY HERE
&lt;/data&gt;
&lt;key&gt;PayloadDescription&lt;/key&gt;
&lt;string&gt;Adds a PKCS-12 certificate&lt;/string&gt;
&lt;key&gt;PayloadDisplayName&lt;/key&gt;
&lt;string&gt;mdmkey.p12&lt;/string&gt;
&lt;/dict&gt;
&lt;/plist&gt;
</code></pre>
<p>In the above file you could see my <strong>password</strong> and <strong>p12 signing key</strong>. By seeing this I'm curious whether this file is meant to be public ?</p>
<p>If this should be kept private,</p>
<ol>
<li>What are all the steps I should take to keep this file private and secure ?</li>
<li>what can a malicious hacker do if he gets access to the above file ? OR What are all the actions one can do if he gets access to this file ?</li>
<li>What are all the actions I should take to keep myself secure if my mdm profile file is accessed by a hacker ?</li>
</ol>
","<ios><ios8><mdm>","2021-06-05 09:08:49","73","0","1","68348836","<p>So technically iOS would (should?) refuse MDM profile install if it is not delivered over SSL.</p>
<p>Therefore, barring any MiTM attacks, corporate proxies etc. you have at least the SSL protection of your certificate and password.</p>
<p>However, others have been concerned about this as you are, and so iOS supports SCEP key delivery since pretty much iOS 5, where the device generates private keys and server signs the certificate so the server has only the public key.</p>
<p>So to answer your questions:</p>
<ol>
<li>Instead of sending certificate and password, use SCEP</li>
<li>If an attacker gets ahold of private keys (the whole certificate contains private keys) they can then decrypt configuration profiles that are installed via MDM commands from the server. The payloads may contain sensitive information (VPN/Email passwords, other certificates etc.)</li>
<li>Practically speaking i imagine it is hard to know whether a cert has been accessed by a hacker. You should monitor for irregular behaviour as a general risk mitagation strategy (e.g. multiple concurrent requests from geo dispersed IP addresses, command replays, missing certificate in response to CertificateList command)</li>
</ol>
"
"67795837","How to test Semi-Autonomous app mode in iOS simulator or on a dev device","<p>When using the call</p>
<p>UIAccessibilityRequestGuidedAccessSession</p>
<p>To put an app in and out of single app mode, as far as I know this only works on a device in MDM that is supervised. I am wondering if there is a convenient way to test this during development?</p>
<p>If possible I would love to do this in the simulator, however I do have a device that is in our MDM and is supervised. However I am unsure how to test it on the device, I can't specify the app in the MDM provider's list of apps since its not in the app store yet. What am I missing?</p>
<p>Note, we do not have an enterprise development account, we have less than 100 employees. I don't need to deploy the app through MDM, I can deploy it through the apple store, but don't want to go through the store for testing.</p>
","<ios><mdm>","2021-06-01 20:49:45","136","0","1","68684101","<p>I still haven't found any documentation or answers for this, but I did end up working through it.</p>
<p>Here is what I ended up doing.</p>
<p>Get the app on the app store, not just as a beta. I had to implement as much as I could to make the app functional enough to get published on the app store.</p>
<p>Deploy MDM policies with the new app. If you aren't on the app store, at least in the MDM tools I used I couldn't set the policy, hence the need to get it on the app store.</p>
<p>Now you can install app onto device with MDM policies (via testflight beta only) and test.</p>
<p>It would be nice if you could put a simulator in a mode where it accepts UIAccessibilityRequestGuidedAccessSession.</p>
"
"67775925","Transfer xml data to Oracle table by column or fields by using talend","<p>I am using Talend Studio with objects tFileInputDelimited row1(Main) to tOracleOutput what I want is to transfer the data in xml file to Oracle table.
I want to transfer the values of the last two columns (product_label and email_order) of my excel file to the product table which has this column structure (PRODUCT_ID,PRODUCT_CODE,PRODUCT_LABEL,EMAIL_COMAND
ORDER_ID).
Also, I want to process this condition if a row in my excel file contains an empty product code column then is not insert the column values product_label and email_command.</p>
<p><a href=""https://i.stack.imgur.com/YNG7S.png"" rel=""nofollow noreferrer"">XML File to load</a></p>
<p><a href=""https://i.stack.imgur.com/ZzLcM.png"" rel=""nofollow noreferrer"">Product table</a></p>
<p><a href=""https://i.stack.imgur.com/l8qBA.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>what is the proper settings in tFileInputDelimited , or do I need to use other tools?</p>
","<oracle><talend><data-integration>","2021-05-31 14:45:47","87","-1","1","67926200","<p><a href=""https://i.stack.imgur.com/ShMR9.png"" rel=""nofollow noreferrer"">Refer this image for your reference </a></p>
<p>Use tFileInputXMl file and filter the records by using tFilterRow and then connect with tOracleOutput</p>
"
"67710946","Need help in reducing the complexity or duplication in the function mentioned","<p>Hi can someone please help me in reducing the complexity of the below mentioned code as I am new to this I need it to reduce the amount of code and improve the code and to improve simplicity and reduce duplications in the overall coding any any help in this regard can be of great help and thanks in advance for your time and consideration in helping me in this regard.</p>
<pre class=""lang-py prettyprint-override""><code>class SynonymSerializer(serializers.ModelSerializer):
    class Meta:
        model = synonym
        fields = '__all__'

    
    def update(self, instance, validated_data):

        instance.insured_synonym = validated_data.get('insured_synonym', instance.insured_synonym)
        instance.obligor_borrower_counterparty_synonym = validated_data.get('obligor_borrower_counterparty_synonym',
                                                                            instance.obligor_borrower_counterparty_synonym)
        instance.guarantor_synonym = validated_data.get('guarantor_synonym', instance.guarantor_synonym)
        instance.credit_rating_synonym = validated_data.get('credit_rating_synonym', instance.credit_rating_synonym)
        instance.country_of_risk_synonym = validated_data.get('country_of_risk_synonym',
                                                              instance.country_of_risk_synonym)
        instance.tenor_synonym = validated_data.get('tenor_synonym', instance.tenor_synonym)
        instance.coverage_synonym = validated_data.get('coverage_synonym', instance.coverage_synonym)
        instance.insured_transaction_synonym = validated_data.get('insured_transaction_synonym',
                                                                  instance.insured_transaction_synonym)
        instance.any_third_parties_synonym = validated_data.get('any_third_parties_synonym',
                                                                instance.any_third_parties_synonym)
        instance.premium_rate_synonym = validated_data.get('premium_rate_synonym', instance.premium_rate_synonym)
        instance.margin_synonym = validated_data.get('margin_synonym', instance.margin_synonym)
        instance.utilization_expected_utilization_synonym = validated_data.get(
            'utilization_expected_utilization_synonym', instance.utilization_expected_utilization_synonym)
        instance.purpose_synonym = validated_data.get('purpose_synonym', instance.purpose_synonym)
        instance.retained_amount_synonym = validated_data.get('retained_amount_synonym',
                                                              instance.retained_amount_synonym)
        instance.insured_percentage_synonym = validated_data.get('insured_percentage_synonym',
                                                                 instance.insured_percentage_synonym)
        instance.payment_terms_synonym = validated_data.get('payment_terms_synonym', instance.payment_terms_synonym)
        instance.secured_security_synonym = validated_data.get('secured_security_synonym',
                                                               instance.secured_security_synonym)
        instance.waiting_period_synonym = validated_data.get('waiting_period_synonym', instance.waiting_period_synonym)
        instance.brokerage_synonym = validated_data.get('brokerage_synonym', instance.brokerage_synonym)
        instance.broker_synonym = validated_data.get('broker_synonym', instance.broker_synonym)
        instance.ipt_synonym = validated_data.get('ipt_synonym', instance.ipt_synonym)
        instance.enquiry_code_synonym = validated_data.get('enquiry_code_synonym', instance.enquiry_code_synonym)
        instance.law_synonym = validated_data.get('law_synonym', instance.law_synonym)
        instance.terms_of_repayment_synonym = validated_data.get('terms_of_repayment_synonym',
                                                                 instance.terms_of_repayment_synonym)
        instance.financials_limit_synonym = validated_data.get('financials_limit_synonym',
                                                               instance.financials_limit_synonym)

        fields_25_synonym_key = [
            'insured_synonym', 'obligor_borrower_counterparty_synonym', 'guarantor_synonym', 'credit_rating_synonym',
            'country_of_risk_synonym',
            'tenor_synonym', 'coverage_synonym', 'insured_transaction_synonym', 'any_third_parties_synonym',
            'premium_rate_synonym', 'margin_synonym',
            'utilization_expected_utilization_synonym', 'purpose_synonym', 'retained_amount_synonym',
            'insured_percentage_synonym', 'payment_terms_synonym', 'secured_security_synonym',
            'waiting_period_synonym', 'brokerage_synonym', 'broker_synonym', 'ipt_synonym', 'enquiry_code_synonym',
            'law_synonym', 'terms_of_repayment_synonym', 'financials_limit_synonym'
        ]

        dict_obj = temp_dict()

        # Logic to store data
        for index, key in enumerate(fields_25_synonym_key):
            dict_obj.add(key, validated_data.get(key))

        # path to where synonym excel sheet should be present

        path = &quot;C:\\Users\\batman\\mycode\\dataingest\\data\\&quot;

        # change directory to desired location , making sure we are at the correct location
        os.chdir(path)

        # code to delete existing excel sheet
        for file in os.listdir(path):
            print(&quot;file neme is :&quot;, file)

            # SynonymList.xlsx
            import pandas as pd
            if 'SynonymList.xlsx' in file:
                excel_path = path + file
                g = pd.read_excel(excel_path)
                excel_new_path = path + &quot;\\SynonymList_old.xlsx&quot;
                g.to_excel(excel_new_path, index=False)
                os.remove(file)
        # code to create new excel sheet on synonym table update

        try:

            tuple_list = []
            # create Workbook object
            wb = Workbook()
            # select demo.xlsx
            sheet = wb.active

            initial = dict_obj
            tuple_list.append((&quot;s_field&quot;, &quot;s_synonym&quot;))
            for key, value in initial.items():
                dict_values = initial[key]
                values_list = dict_values.split(&quot;,&quot;)
                for n in range(len(values_list)):
                    if key == &quot;insured_synonym&quot;:
                        tuple_list.append((&quot;insured&quot;, values_list[n].lower().strip()))
                    if key == &quot;obligor_borrower_counterparty_synonym&quot;:
                        tuple_list.append((&quot;obligor&quot;, values_list[n].lower().strip()))
                    if key == &quot;guarantor_synonym&quot;:
                        tuple_list.append((&quot;guarantor&quot;, values_list[n].lower().strip()))
                    if key == &quot;credit_rating_synonym&quot;:
                        tuple_list.append((&quot;credit rating&quot;, values_list[n].lower().strip()))
                    if key == &quot;country_of_risk_synonym&quot;:
                        tuple_list.append((&quot;obligor country&quot;, values_list[n].lower().strip()))
                    if key == &quot;tenor_synonym&quot;:
                        tuple_list.append((&quot;tenor&quot;, values_list[n].lower().strip()))
                    if key == &quot;coverage_synonym&quot;:
                        tuple_list.append((&quot;coverage&quot;, values_list[n].lower().strip()))
                    if key == &quot;insured_transaction_synonym&quot;:
                        tuple_list.append((&quot;insured transaction&quot;, values_list[n].lower().strip()))
                    if key == &quot;any_third_parties_synonym&quot;:
                        tuple_list.append((&quot;third parties&quot;, values_list[n].lower().strip()))
                    if key == &quot;premium_rate_synonym&quot;:
                        tuple_list.append((&quot;premium rate&quot;, values_list[n].lower().strip()))
                    if key == &quot;margin_synonym&quot;:
                        tuple_list.append((&quot;margin&quot;, values_list[n].lower().strip()))
                    if key == &quot;utilization_expected_utilization_synonym&quot;:
                        tuple_list.append((&quot;utilization&quot;, values_list[n].lower().strip()))
                    if key == &quot;purpose_synonym&quot;:
                        tuple_list.append((&quot;purpose&quot;, values_list[n].lower().strip()))
                    if key == &quot;retained_amount_synonym&quot;:
                        tuple_list.append((&quot;retained amount&quot;, values_list[n].lower().strip()))
                    if key == &quot;insured_percentage_synonym&quot;:
                        tuple_list.append((&quot;insured %&quot;, values_list[n].lower().strip()))
                    if key == &quot;payment_terms_synonym&quot;:
                        tuple_list.append((&quot;payment terms&quot;, values_list[n].lower().strip()))
                    if key == &quot;secured_security_synonym&quot;:
                        tuple_list.append((&quot;secured&quot;, values_list[n].lower().strip()))
                    if key == &quot;waiting_period_synonym&quot;:
                        tuple_list.append((&quot;waiting period&quot;, values_list[n].lower().strip()))
                    if key == &quot;brokerage_synonym&quot;:
                        tuple_list.append((&quot;brokerage&quot;, values_list[n].lower().strip()))
                    if key == &quot;broker_synonym&quot;:
                        tuple_list.append((&quot;broker&quot;, values_list[n].lower().strip()))
                    if key == &quot;ipt_synonym&quot;:
                        tuple_list.append((&quot;ipt&quot;, values_list[n].lower().strip()))
                    if key == &quot;enquiry_code_synonym&quot;:
                        tuple_list.append((&quot;enquiry code&quot;, values_list[n].lower().strip()))
                    if key == &quot;law_synonym&quot;:
                        tuple_list.append((&quot;law&quot;, values_list[n].lower().strip()))
                    if key == &quot;terms_of_repayment_synonym&quot;:
                        tuple_list.append((&quot;terms of repayment&quot;, values_list[n].lower().strip()))
                    if key == &quot;financials_limit_synonym&quot;:
                        tuple_list.append((&quot;limit&quot;, values_list[n].lower().strip()))
                    # if key == &quot;ef_created&quot;:
                    #     tuple_list.append((&quot;created at&quot;, values_list[n].lower().strip()))
                    # if key == &quot;ef_last_updated&quot;:
                    #     tuple_list.append((&quot;last update&quot;, values_list[n].lower().strip()))
            # append all rows
            for row in tuple_list:
                sheet.append(row)
            # save file
            wb.save(path + &quot;\\SynonymList.xlsx&quot;)
            print(&quot;File saved at..&quot;, path)
        except Exception as e:
            print(&quot;Not able to create synonym excel sheet&quot;, e)

        instance.save()
        return instance
</code></pre>
","<python><django><code-duplication><redundancy><data-integration>","2021-05-26 18:42:07","53","-3","1","67711355","<ul>
<li>Did not check all related lines of code but perhaps you can use <code>getattr</code> and <code>setattr</code> to reduce dulpication of information</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>properties = [
    'insured_synonym',
    'obligor_borrower_counterparty_synonym',
    'guarantor_synonym',
    'credit_rating_synonym',
    # and so on
]
for prop in properties:
    setattr(instance, prop, validated_data.get(prop, getattr(instance, prop)))
</code></pre>
<ul>
<li>You never use <code>index</code> so you do not need <code>enumerate</code></li>
</ul>
<pre class=""lang-py prettyprint-override""><code>for key in fields_25_synonym_key:
    dict_obj.add(key, validated_data.get(key))
</code></pre>
<ul>
<li>A lookup table to get rid of all those ifs</li>
<li>You can iterate over the items in a list directly so you do not need <code>n</code>, <code>range</code>, and <code>len</code></li>
</ul>
<pre class=""lang-py prettyprint-override""><code>labels = {
    &quot;insured_synonym&quot;:                          &quot;insured&quot;,
    &quot;obligor_borrower_counterparty_synonym&quot;:    &quot;obligor&quot;,
    &quot;guarantor_synonym&quot;:                        &quot;guarantor&quot;,
    &quot;credit_rating_synonym&quot;:                    &quot;credit rating&quot;,
    # and so on
}
for val in values_list:
    tuple_list.append((labels[key], val.lower().strip()))
</code></pre>
"
"67703829","Need help in reducing the complexity or duplication in the function","<p>Hi can someone please help me in reducing the complexity of the below mentioned code as I am new to this I need it to reduce the amount of code and improve the code and to improve simplicity and reduce duplications in the overall coding any any help in this regard can be of great help and thanks in advance for your time and consideration in helping me in this regard.</p>
<pre><code>def update(self, instance, validated_data):

    instance.email_id = validated_data.get('email_id', instance.email_id)
    instance.email_ml_recommendation = validated_data.get('email_ml_recommendation',
                                                          instance.email_ml_recommendation)
    instance.ef_insured_name = validated_data.get('ef_insured_name', instance.ef_insured_name)
    instance.ef_broker_name = validated_data.get('ef_broker_name', instance.ef_broker_name)
    instance.ef_obligor_name = validated_data.get('ef_obligor_name', instance.ef_obligor_name)
    instance.ef_guarantor_third_party = validated_data.get('ef_guarantor_third_party',
                                                           instance.ef_guarantor_third_party)
    instance.ef_coverage = validated_data.get('ef_coverage', instance.ef_coverage)
    instance.ef_financials = validated_data.get('ef_financials', instance.ef_financials)
    instance.ef_commercial_brokerage = validated_data.get('ef_commercial_brokerage',
                                                          instance.ef_commercial_brokerage)
    # fixing bug of pipeline

    instance.ef_underwriter_decision = validated_data.get('ef_underwriter_decision',
                                                          instance.ef_underwriter_decision)
    instance.ef_decision_nty_fields = validated_data.get('ef_decision_nty_fields', 
                                                   instance.ef_decision_nty_fields)
    instance.ef_feedback = validated_data.get('ef_feedback', instance.ef_feedback)

    instance.relation_id = validated_data.get('relation_id', instance.relation_id)
    instance.email_outlook_date = validated_data.get('email_outlook_date', 
                                                           instance.email_outlook_date)
    instance.ef_decision_nty_fields = validated_data.get('ef_decision_nty_fields', 
                                                           instance.ef_decision_nty_fields)
    instance.ef_pl_est_premium_income = validated_data.get('ef_pl_est_premium_income',
                                                           instance.ef_pl_est_premium_income)
    instance.ef_pl_prob_closing = validated_data.get('ef_pl_prob_closing', 
                                                                       instance.ef_pl_prob_closing)
    instance.ef_pl_time_line = validated_data.get('ef_pl_time_line', instance.ef_pl_time_line)
    instance.ef_pipeline = validated_data.get('ef_pipeline', instance.ef_pipeline)
    instance.el_insured_margin = validated_data.get('el_insured_margin', instance.el_insured_margin)
    instance.ef_last_updated = validated_data.get('ef_last_updated', instance.ef_last_updated)
    instance.relation_id = validated_data.get('relation_id', instance.relation_id)
    # CR3.2 Primium and basis point addition
    instance.broker_email_id = validated_data.get('broker_email_id', instance.broker_email_id)
    instance.premium = validated_data.get('premium', instance.premium)
    instance.basis_points = validated_data.get('basis_points', instance.basis_points)
    instance.basis_points_decision = validated_data.get('basis_points_decision', 
                                                           instance.basis_points_decision)

    embedded_json = validated_data.get('email_embedd', instance.email_embedd)
    instance.email_embedd = json.dumps(embedded_json)
</code></pre>
","<python><database><code-duplication><redundancy><data-integration>","2021-05-26 11:19:01","46","1","1","67704030","<p>If all items in your dictionary <code>validated_data</code> that have a corresponding attribute in <code>instance</code> have to be copied to that instance, then iterate those items and use <code>setattr</code> to set the corresponding attributes of your <code>instance</code> object.</p>
<p>You seem to have one special case where a value needs to be stringified as JSON. So you'll need specific code to deal with that scenario:</p>
<pre><code>def update(self, instance, validated_data):
    for key, value in validated_data.items():
        if hasattr(instance, key):
            if key == &quot;email_embedd&quot;:  # special case
                instance.email_embedd = json.dumps(value)
            else:
                setattr(instance, key, value) 
</code></pre>
<h2>A Logical Error...</h2>
<p>There is a problem in your code for the special case:</p>
<pre><code>embedded_json = validated_data.get('email_embedd', instance.email_embedd)
instance.email_embedd = json.dumps(embedded_json)
</code></pre>
<p>If this gets executed when <code>validated_data</code> does not have the key <code>email_embedd</code>, then <code>embedded_json</code> will default to <code>instance.email_embedd</code>. But that value <em>is already JSON encoded</em>! So if you now proceed with <code>json.dumps(embedded_json)</code> you'll end up with a JSON string that itself has been stringified again!</p>
<p>This problem will not occur with the code proposed above.</p>
"
"67697063","How to Import Google workspace data automatically to Big Query database?","<p>How to daily import Google workspace data automatically to Big Query database?</p>
<p>I'm new to Big Query and i can do it manually but i want to automate this process. Thanks.</p>
","<import><google-bigquery><google-cloud-sql><data-synchronization><database-link>","2021-05-26 00:16:03","68","0","1","67697406","<p>With BigQuery you can create <code>external tables</code>, that enable you to query data that is stored in your Google Drive (CSV, Avro, JSON, or Google Sheets documents).</p>
<p>You can find a nice how-to <a href=""https://cloud.google.com/bigquery/external-data-drive"" rel=""nofollow noreferrer"">here</a>.</p>
"
"67670252","Azure Data Factory - MQ data flow","<p>I have been tasked to create an Azure Data Factory pipeline that will process messages being generated from an MQ Farm and that are stored in Data Storage in .xml format and then ingest them in a SharePoint Table.</p>
<p>The question is how would your approach be in that scenario to slice the .xml files in smaller pieces? The .xml files are nesting a lot of records in one file (with a valid separator on each record) and I wish to discard some while process the valid ones.</p>
<p>P.S.: For receiving and storing the MQ Farm messages I am using a logic app before Azure Data Factory</p>
","<azure><azure-logic-apps><mq><azure-data-factory>","2021-05-24 10:15:00","528","0","1","67686353","<p>OK the solution was more obvious than previously thought... Solved from the logic app designer and saving to blob</p>
<p><a href=""https://i.stack.imgur.com/jRHPl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jRHPl.png"" alt=""enter image description here"" /></a></p>
"
"67621965","mapping data flow column pattern type =='decimal' not changing decimal columns","<p>I'm using a column pattern to catch nulls.  My logic is very simple.</p>
<p><strong>Matching condition</strong></p>
<pre><code>type=='decimal'
</code></pre>
<p><strong>Column name expression</strong></p>
<pre><code>$$
</code></pre>
<p><strong>Value expression</strong></p>
<pre><code>coalesce($$,toDecimal(0,38,18))
</code></pre>
<p>I can't get this to work it simply leaves NULL values in place.  I can however use the expression</p>
<pre><code>type!='string' &amp;&amp; type!='date'
</code></pre>
<p>This does change the value of the columns with NULL values to 0.  In my current case this is fine I have strings, dates and decimals but I can see how this is not ideal as I might have integers, doubles or other types and I would not want these to be converted to decimals and the list of &amp;&amp; statements becomes quite long.</p>
<p>Can anyone supply details of how I should specify a decimal type or a better workaround if not?  If there is a link to any good references would be helpful too, the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-column-pattern#pattern-matching-expression-values"" rel=""nofollow noreferrer"">official</a> documentation is not especially specific.</p>
<p>To confirm I am working with decimal values:
<a href=""https://i.stack.imgur.com/rMDJ8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rMDJ8.jpg"" alt=""showing decimals"" /></a></p>
","<azure-synapse><azure-data-factory>","2021-05-20 14:04:19","850","2","3","67627775","<p>Try this instead - Decimal has precision and scale</p>
<p>startsWith(type, 'decimal')</p>
"
"67621965","mapping data flow column pattern type =='decimal' not changing decimal columns","<p>I'm using a column pattern to catch nulls.  My logic is very simple.</p>
<p><strong>Matching condition</strong></p>
<pre><code>type=='decimal'
</code></pre>
<p><strong>Column name expression</strong></p>
<pre><code>$$
</code></pre>
<p><strong>Value expression</strong></p>
<pre><code>coalesce($$,toDecimal(0,38,18))
</code></pre>
<p>I can't get this to work it simply leaves NULL values in place.  I can however use the expression</p>
<pre><code>type!='string' &amp;&amp; type!='date'
</code></pre>
<p>This does change the value of the columns with NULL values to 0.  In my current case this is fine I have strings, dates and decimals but I can see how this is not ideal as I might have integers, doubles or other types and I would not want these to be converted to decimals and the list of &amp;&amp; statements becomes quite long.</p>
<p>Can anyone supply details of how I should specify a decimal type or a better workaround if not?  If there is a link to any good references would be helpful too, the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-column-pattern#pattern-matching-expression-values"" rel=""nofollow noreferrer"">official</a> documentation is not especially specific.</p>
<p>To confirm I am working with decimal values:
<a href=""https://i.stack.imgur.com/rMDJ8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rMDJ8.jpg"" alt=""showing decimals"" /></a></p>
","<azure-synapse><azure-data-factory>","2021-05-20 14:04:19","850","2","3","73376169","<p>You can also use <code>typeMatch(type, 'number')</code> if you want to match all number types.  Per the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expressions-usage#typematch"" rel=""nofollow noreferrer"">documentation</a>, other options are:</p>
<pre><code>typeMatch(type, 'integral') /* matches short, integer, long */
typeMatch(type, 'fractional') /* matches double, float, decimal */
typeMatch(type, 'datetime') /* matches date, timestamp */
</code></pre>
"
"67621965","mapping data flow column pattern type =='decimal' not changing decimal columns","<p>I'm using a column pattern to catch nulls.  My logic is very simple.</p>
<p><strong>Matching condition</strong></p>
<pre><code>type=='decimal'
</code></pre>
<p><strong>Column name expression</strong></p>
<pre><code>$$
</code></pre>
<p><strong>Value expression</strong></p>
<pre><code>coalesce($$,toDecimal(0,38,18))
</code></pre>
<p>I can't get this to work it simply leaves NULL values in place.  I can however use the expression</p>
<pre><code>type!='string' &amp;&amp; type!='date'
</code></pre>
<p>This does change the value of the columns with NULL values to 0.  In my current case this is fine I have strings, dates and decimals but I can see how this is not ideal as I might have integers, doubles or other types and I would not want these to be converted to decimals and the list of &amp;&amp; statements becomes quite long.</p>
<p>Can anyone supply details of how I should specify a decimal type or a better workaround if not?  If there is a link to any good references would be helpful too, the <a href=""https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-column-pattern#pattern-matching-expression-values"" rel=""nofollow noreferrer"">official</a> documentation is not especially specific.</p>
<p>To confirm I am working with decimal values:
<a href=""https://i.stack.imgur.com/rMDJ8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rMDJ8.jpg"" alt=""showing decimals"" /></a></p>
","<azure-synapse><azure-data-factory>","2021-05-20 14:04:19","850","2","3","75412809","<p>I know this is an old post but I stumbled in the same issue and I fixed it by just assigning the precision and scale like this <code>type=='decimal(38,20)'</code> just note that the arguments go like (scale, precision). This worked for me.</p>
"
"67597719","How to convert '02-3月 -21' to date 21-03-02 00:00:00 psql","<p>How to convert '02-3月 -21' to date 21-03-02 00:00:00? in postgresql or kettle</p>
","<postgresql><kettle><data-integration>","2021-05-19 05:59:31","90","-2","1","67598165","<p>I think a simple <code>replace</code> can do the trick here. Just replace the Chinese character <code>月</code> with nothing and format it.</p>
<pre><code>SELECT TO_TIMESTAMP(REPLACE ('02-3月-21', '月', ''),'DD-MM-YY');
// output: 2021-03-02 00:00:00+00


SELECT TO_CHAR(TO_TIMESTAMP(REPLACE ('02-3月-21', '月', ''),'DD-MM-YY'), 'YY-MM-DD HH24:MI:SS')
// output: 21-03-02 00:00:00
</code></pre>
"
"67584366","How much of Talend functionality is translated in SQL-Query and how much in Java?","<p>I am facing an internship and they asked me to learn how to use talend ETL.
I did it, not so difficult.
One of the extra-tasks that have been assigned to me is to verify how much of the operations I set on the design workspace is executed in java and what is done through the use of queries.
I've set up a simple Join using the TMap component and I monitored the SQLdatabase through the use of SQL Profiler. the result is that only the essential create/drop and the select/insert of the table is done via sql while every other thing like the actual join is made &quot;Java&quot; side.
As long as it is an simple operation like join, wouldn't it be convenient to execute it through a query without having to bother java to perform it?
For those who also know SAP, in terms of performance is there so much difference between Talend and SAP?</p>
","<etl><talend><data-integration><talend-mdm>","2021-05-18 10:23:15","154","1","1","67598987","<p>Only operations in tDB components (create,select,insert, etc) are actually done through SQL. All operations done in other talend components (tMap, tFilter, aggregate, etc) are done through java.
Indeed you'll have better performances doing operations SQL-side. You then have to find the right balance between an &quot;all-in-sql&quot; type of job and an &quot;all-java&quot; one. (it could be harder for a talend developer to debug operations if all the sql part is done through a unique query inside a single component...).</p>
<p>You could definitely have your joins inside a tDBInput component, and output the result in a single output flow.
You can also check ELT* components : they let you use SQL-engine instead of java-engine to perform all operations (join,aggregate,filter) while using a talend interface.</p>
"
"67574530","Filter one dataframe based on the last two digits of another dataframe's value under one column in R","<p>The table, Data_frame, has an ID column that contains over 1000 participants' information, such as &quot;Sample_LI.01&quot;</p>
<p>My_ColData also has an ID column that contains only 40 participants' different information, such as &quot;Sample_LI-01&quot;.</p>
<p>I want to use the ID column in My_ColData to filter the Data_frame table. However, you may have noticed the formats of ID are slightly different. I wonder if the best way to possibly filter based on the last two digits?</p>
<p>I have a code so far, look like</p>
<p><code>data_frame %&gt;% filter (ID %in% my_ColData$ID, if______)</code></p>
<p>Having no idea what to write about in this if condition. Or is there a better to realize my goal? Any suggestions would be appreciated.</p>
","<r><data-manipulation><master-data-management>","2021-05-17 17:36:49","105","1","2","67574548","<p>We could use <code>str_replace</code> to replace the <code>-</code> with <code>.</code> to match the 'ID' from 'data_frame' with the 'ID' from 'my_ColData'</p>
<pre><code>library(dplyr)
library(stringr)
data_frame %&gt;% 
       filter(ID %in% str_replace(my_ColData$ID, '-', '.') )
</code></pre>
"
"67574530","Filter one dataframe based on the last two digits of another dataframe's value under one column in R","<p>The table, Data_frame, has an ID column that contains over 1000 participants' information, such as &quot;Sample_LI.01&quot;</p>
<p>My_ColData also has an ID column that contains only 40 participants' different information, such as &quot;Sample_LI-01&quot;.</p>
<p>I want to use the ID column in My_ColData to filter the Data_frame table. However, you may have noticed the formats of ID are slightly different. I wonder if the best way to possibly filter based on the last two digits?</p>
<p>I have a code so far, look like</p>
<p><code>data_frame %&gt;% filter (ID %in% my_ColData$ID, if______)</code></p>
<p>Having no idea what to write about in this if condition. Or is there a better to realize my goal? Any suggestions would be appreciated.</p>
","<r><data-manipulation><master-data-management>","2021-05-17 17:36:49","105","1","2","67574735","<p>We could use <code>str_sub</code> to check for the last two digits</p>
<pre><code>library(dplyr)
library(stringr)
data_frame %&gt;% 
  filter(str_sub(ID, -2) %in% str_sub(my_colData$ID, -2))
</code></pre>
"
"67570311","How to store a value from source into a parameter and use it in data flow transformations?","<p>I have a source table which just has one row:</p>
<p><a href=""https://i.stack.imgur.com/MvmKq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MvmKq.png"" alt=""enter image description here"" /></a></p>
<p>So i stored the value from Values_per_Country into a parameter:
<a href=""https://i.stack.imgur.com/NVL84.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NVL84.png"" alt=""enter image description here"" /></a></p>
<p>I want to use this parameter into my SELECT transformation(schema modifier),
<a href=""https://i.stack.imgur.com/djm8t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/djm8t.png"" alt=""enter image description here"" /></a></p>
<p>but this error comes up:
<a href=""https://i.stack.imgur.com/hwCPL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hwCPL.png"" alt=""enter image description here"" /></a></p>
<p>Is there a way around this,so i can use values from the source tables?</p>
","<azure-data-factory>","2021-05-17 13:07:06","191","0","1","67582199","<p>You can create a Lookup activity to get the column values of source table. And then pass to the parameter in Data Flow. Finally, your expression <code>type == 'double' &amp;&amp; position &gt; 0 &amp;&amp; position &lt;= $parameter3</code> will work.</p>
<p>Screenshot:</p>
<p><a href=""https://i.stack.imgur.com/Hhx8F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Hhx8F.png"" alt=""enter image description here"" /></a></p>
<p>Expression in the below image: <code>@activity('Lookup1').output.firstRow['Values_per_Country']</code>
<a href=""https://i.stack.imgur.com/Am4wq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Am4wq.png"" alt=""enter image description here"" /></a></p>
"
"67518891","Performing a delete in an ADF mapping data flow - Updated with potential solution","<p>I am trying to do upsert and delete in a mapping data flow.
There is a dimension table, DimCustomer.
It is being populated with data from a file.</p>
<p>If a Sha2 hash does not match then upsert.
if CustomerID is missing from the rawSource data, then delete (see image below for settings)</p>
<p><a href=""https://i.stack.imgur.com/SU98f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SU98f.png"" alt=""enter image description here"" /></a></p>
<p>The upsert works, but the delete does not. Its likely because in the sink i have selected the customerID column as the key, but this means it can never delete a record if the entire record, including key is missing from source.</p>
<p>Is there a prescribed design pattern for this scenario?</p>
<p>The easiest solution i can think of is a 2nd dataflow, in which the only customerID's sent to the sink are ones where there is no matching customerID in the source (effectively a right outer join), but want to see if this is indeed the best way to do this.</p>
<p><strong>Update</strong>:
The best solution i can come up with for this is, to the above dataflow, add an additional column, the formula for which is coalesce(RawCustomerData@CustomerID,DimCustomer@CustomerID)</p>
<p>This ensures there is a CustID column that always has a value.
In the sink, i change the mapping so that this custID maps to the sink CustomerID.</p>
<p>The delete now works as expected. Still unsure if this is the best solution but it works and doesn't appear to cause a major performance issue.</p>
","<azure-data-factory><azure-synapse>","2021-05-13 12:02:08","1418","1","1","67528496","<p>Per my experience, I think that's the best solution, add a new column can solve the problem is much easier than other operations. The way which simplest and effective is the best solution. You don't need create another data flow actives to achieve it or re-design the Alter active logic.</p>
<p><strong>Your Solution:</strong></p>
<p>Add an additional column <code>CustID</code>, the formula for which is: <code>coalesce(RawCustomerData@CustomerID,DimCustomer@CustomerID)</code></p>
<p>This ensures there is a <code>CustID</code> column that always has a value. In the sink, you change the mapping so that this <code>custID</code> maps to the sink <code>CustomerID</code>.</p>
<p>The delete now works as expected.</p>
"
"67513470","iOS MDM : Can any MDM SDK send MDM command without Internet?","<p>iOS MDM : Can any MDM SDK send MDM command without Internet ?</p>
<p>In offline situation (while iPhone do not have internet connectivity - like Airplane Mode)  if need to generate mdm command to change the profile (change the restrictions), is it possible ?</p>
","<ios><mdm>","2021-05-13 03:20:06","88","0","1","68348865","<p>iOS does not support this as the mdm daemon only communicates over https.</p>
<p>Android supports partial configuration of settings via a local app (especially via the old Admin profile which is now deprecated).</p>
"
"67493283","Understanding PayPal smart button authorization data flow and when funding failure can occur","<p>I am using smart buttons with server-side API calls for the purpose of an authorization-capture type transaction. I already have orders being set up and passed back to the client for the PayPal popup to use.</p>
<p>I am trying to work out the handling of the user's authorization, as far as how that information eventually makes it to my server to be processed so that it can later be captured.</p>
<p>I currently use a webhook listening for the authorization notification with no further server calls being made from the smart button code, but was suggested in <a href=""https://stackoverflow.com/questions/67476475/paypal-orders-api-vs-payments-api-and-finding-better-documentation#67476823"">a different post</a> that webhooks were overcomplicated for the purpose and to switch to just using the <code>onApprove</code> callback to send the data to my server directly from the client, but without much explanation of why that was better.</p>
<p>Other than being faster than the ~20 or so seconds it takes to receive the webhook notification, it seems like piping this kind of sensitive data through the client leaves it vulnerable to user tampering. Am I missing something? Am I just supposed to take the AuthorizationId and use it to fetch the transaction info safely from my server or something?</p>
<p>On a separate, but related note, the sample code in <a href=""https://developer.paypal.com/docs/checkout/integration-features/funding-failure/"" rel=""nofollow noreferrer"">the documentation</a> regarding funding failures seems to indicate that a funding failure can only be determined at the time of capture. Isn't the point of authorization to have the funds put on hold so that I can be sure they exist before I try to capture?</p>
<p>Am I misreading the documentation or does authorization just succeed regardless of the person actually being able to afford it? How can I tell, as early as possible, that there is a funding failure when I don't capture immediately? And where else in the authorization flow can errors occur, PayPal side?</p>
","<paypal>","2021-05-11 19:35:45","47","0","1","67494067","<p>As explained in comments there, the payer gives their approval on the client, in the browser -- so the notification to the server should come from the client. This way the server can act on the approval and authorize (or if <code>&quot;intent&quot;:&quot;capture&quot;</code>, capture) the order and immediately reply to the client regarding whether it was successful or not.</p>
<p>That reply to the client should include (propagate) information about any errors, so that recoverable ones can be handled or non-recoverable ones can show a message to the user. Sample client-side error handling code is given here: <a href=""https://developer.paypal.com/docs/business/checkout/server-side-api-calls/#server-side-api-calls"" rel=""nofollow noreferrer"">https://developer.paypal.com/docs/business/checkout/server-side-api-calls/#server-side-api-calls</a></p>
"
"67370325","SSIS - Auto increment field is not inserted correctly with data flow task","<p>I am trying to copy data from one database to another using ssis. I created the dtsx package with the SQL Server Import and Export Wizard.
The table I am copying from has a column name &quot;Id&quot;, the other table has name &quot;ModuleCategoryId&quot;, which I mapped together.</p>
<p>ModuleCategoryId is the identity, and has an auto increment of 1.
In the source database, the Id's are not ordered, and go like this:</p>
<ul>
<li>32  Name1</li>
<li>14  Name2</li>
<li>7   Name3</li>
</ul>
<p>After executing the data flow, the destination DB looks like this:</p>
<ul>
<li>1 Name1</li>
<li>2 Name2</li>
<li>3 Name3</li>
</ul>
<p>I have enabled identity insert in the wizard during the, but this doesn't do anything.
<img src=""https://i.stack.imgur.com/4ooE7.png"" alt=""Column mapping"" /></p>
<p>The destination database was made with Entity Framework, code first.</p>
<p>If I explicitly turn off ValueGeneratedOnAdd, and remake the destination database, the data is being transferred correctly, but I was wondering if there's a way to transfer all the data without turning off the auto increment, and then turning it back on.</p>
<p>If I manually set Identity Insert on for that table, I can insert rows with whatever ModuleCategoryId I want, so it must be something with the dataflow.</p>
","<sql-server><ssis>","2021-05-03 14:04:35","594","1","1","67372413","<p>Table definitions are table definitions - regardless of the syntactic sugar ORM tools might overlay.</p>
<p>I created a source and destination table and populated the source to match your supplied data. I do define the identity property on the destination table as well. Whether that's what a <code>ValueGeneratedOnAdd</code> is implemented as in the API, I don't know but it almost has to be otherwise the Enable Identity Insert should fail (if the UI even allows it).</p>
<p>The IDENTITY property allows you to seed it with any initial value you want. For the taget table, I seed at the minimum value allowed for a signed integer so that if the identity insert doesn't work, the resulting values will look really &quot;wrong&quot;</p>
<pre><code>DROP TABLE IF EXISTS dbo.SO_67370325_Source;
DROP TABLE IF EXISTS dbo.SO_67370325_Destination;

CREATE TABLE dbo.SO_67370325_Source
(
    Id int IDENTITY(1,1) NOT NULL
,   Name varchar(50)
);
CREATE TABLE dbo.SO_67370325_Destination
(
    ModuleCategoryId int IDENTITY(-2147483648,1) NOT NULL
,   Name varchar(50)
);

CREATE TABLE dbo.SO_67370325_Destination_noident
(
    ModuleCategoryId int NOT NULL
,   Name varchar(50)
);

SET IDENTITY_INSERT dbo.SO_67370325_Source ON;
INSERT INTO DBO.SO_67370325_Source
(
    Id
,   Name
)
VALUES
    (32, 'Name1')
,   (14, 'Name2')
,   (7, 'Name3');
SET IDENTITY_INSERT dbo.SO_67370325_Source OFF;

INSERT INTO dbo.SO_67370325_Source
(
    Name
)
OUTPUT Inserted.*
VALUES
(
    'Inserted naturally' -- Name - varchar(50)
);
</code></pre>
<p>Beyond your 3 supplied values, I added a fourth and if you run the supplied query, you'll see the generated ID is likely 33. Source table is created with an identity seeded at 1 but the explicit identity inserts on the source table advance the seed value to 32. Assuming no other activity occurs, next value would be 33 since our increment is 1.</p>
<p>All that said, I have 3 scenarios established. In the Import Export wizard, I checked the Identity Insert and mapped Id to ModuleCategoryId and ran the package.</p>
<pre><code>ModuleCategoryId|Name
32|Name1
14|Name2
7|Name3
33|Inserted naturally
</code></pre>
<p>The data in the target table is identical to the source - as expected. At this point, the identity seed is sitting at 33 which I could verify with some DBCC check command I don't have handy.</p>
<p>The next case is taking the same package and unchecking the Identity Insert property. This becomes invalid as I'd get an error reporting</p>
<blockquote>
<p>Failure inserting into the read-only column &quot;ModuleCategoryId&quot;</p>
</blockquote>
<p>The only option is to unmap the Id to ModuleCategoryId. Assuming I loaded to the same table as before, I would see data something like this</p>
<pre><code>ModuleCategoryId|Name
34|Name1
35|Name2
36|Name3
37|Inserted naturally
</code></pre>
<p>If I had never put a record into this table, then I'd get results like</p>
<pre><code>ModuleCategoryId|Name
-2147483648|Name1
-2147483647|Name2
-2147483646|Name3
-2147483645|Inserted naturally
</code></pre>
<p>WITHOUT AN EXPLICIT ORDER BY ON MY SOURCE, THERE IS NO GUARANTEE OF RESULTS ORDERING. I fight this battle often. The SQL Engine has no obligation to return data in the primary key order or any other such order unless you <em>explicitly</em> ask for it. Had the following results been stored, it would be equally valid.</p>
<pre><code>ModuleCategoryId|Name
34|Inserted naturally
35|Name1
36|Name2
37|Name3
</code></pre>
<p>If you have a requirement for data to be inserted into the target table based on the ascending values of Id in the source table, in the Import/Export wizard, you need to go to the screen where it asks whether you want to pick tables or write a query and choose the second option of query. Then you will write <code>SELECT * FROM dbo.SO_67370325_Source ORDER BY Id;</code> or whatever your source table is named.</p>
<p>The final test, loading SO_67370325_Destination_noident, demonstrates a table with no identity property defined. If I do not map Id to ModuleCategoryId, the package will fail as the column is defined as NOT NULL. When I map the Id to ModuleCategoryId, I will see the same results as the first (7,14,32,33) BUT, every subsequent insert to the target table will have to provide their own Id which may or may not align with what your FluentAPI stuff does.</p>
<p>Similar question/answer <a href=""https://stackoverflow.com/questions/20948819/error-0xc0202049-data-flow-task-1-failure-inserting-into-the-read-only-column/20953512#20953512"">Error 0xc0202049: Data Flow Task 1: Failure inserting into the read-only column</a></p>
"
"67359864","Talend open studio for data integration","<p>In Talent open studio. i have  to add different source file to one output table..How can i fetch the last id in that output table and generate the very next id and continue insertion with different sources</p>
","<talend><insert-update>","2021-05-02 18:31:09","137","0","1","67367002","<p>Add a subjob prior to your current subjob.
in a tDBINPut component select your lastID through a query (select maxID , select top 1 , etc) .
Put the result in a variable (global variable or context variable), in a tJavaRow for example. (context.lastID=input_row.id)
Use this variable in a tMap to generate the next ID, through Numeric.sequence function.
In the output mapping of your tmap, you should add something like  <code>Numeric.sequence(context.lastID,1,1)</code></p>
<p>I think there are plenty of solutions to get the lastID and generate a sequence from there. you can also check advanced output parameters on your tDBOutput.</p>
"
"67227932","Flatten two arrays having corresponding values using mapping data flow in azure data factory","<p>I am new to data flows in adf. I have a set of json files in a folder which i would like to parse and flatten two arrays into a csv.
The json structure is as follows:
<a href=""https://i.stack.imgur.com/FkwkQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FkwkQ.png"" alt=""enter image description here"" /></a></p>
<p>I would like to flatten the timestamps and values arrays. Each timestamp in Timestamps array has the corresponding value in the Values array. i.e, value at Timestamps[i] is Values[i]. The output should have a row for each timestamp and corresponding value for a particular label.</p>
<p><a href=""https://i.stack.imgur.com/82Tmh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/82Tmh.png"" alt=""enter image description here"" /></a></p>
<p>Can anyone please help me in achieving this</p>
<p>Thank you!</p>
","<azure><azure-data-factory>","2021-04-23 10:03:05","1386","0","4","67234567","<p>In your data flow, add 2 Flatten transformations and point to the Timestamps and Values arrays in the unroll property in each transformation.</p>
"
"67227932","Flatten two arrays having corresponding values using mapping data flow in azure data factory","<p>I am new to data flows in adf. I have a set of json files in a folder which i would like to parse and flatten two arrays into a csv.
The json structure is as follows:
<a href=""https://i.stack.imgur.com/FkwkQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FkwkQ.png"" alt=""enter image description here"" /></a></p>
<p>I would like to flatten the timestamps and values arrays. Each timestamp in Timestamps array has the corresponding value in the Values array. i.e, value at Timestamps[i] is Values[i]. The output should have a row for each timestamp and corresponding value for a particular label.</p>
<p><a href=""https://i.stack.imgur.com/82Tmh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/82Tmh.png"" alt=""enter image description here"" /></a></p>
<p>Can anyone please help me in achieving this</p>
<p>Thank you!</p>
","<azure><azure-data-factory>","2021-04-23 10:03:05","1386","0","4","67238247","<p>To do the correlation between 2 arrays use mapIndex function and merge the arrays together. Then you will use a flatten to get it into relational form.</p>
<p>mapIndex(Timestamps, @(
Timestamp = #item, Value = Values[#index]))</p>
"
"67227932","Flatten two arrays having corresponding values using mapping data flow in azure data factory","<p>I am new to data flows in adf. I have a set of json files in a folder which i would like to parse and flatten two arrays into a csv.
The json structure is as follows:
<a href=""https://i.stack.imgur.com/FkwkQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FkwkQ.png"" alt=""enter image description here"" /></a></p>
<p>I would like to flatten the timestamps and values arrays. Each timestamp in Timestamps array has the corresponding value in the Values array. i.e, value at Timestamps[i] is Values[i]. The output should have a row for each timestamp and corresponding value for a particular label.</p>
<p><a href=""https://i.stack.imgur.com/82Tmh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/82Tmh.png"" alt=""enter image description here"" /></a></p>
<p>Can anyone please help me in achieving this</p>
<p>Thank you!</p>
","<azure><azure-data-factory>","2021-04-23 10:03:05","1386","0","4","71868606","<p>I know this is rather too late for an answer but I came across this when I was struggling to get the example working. Taking what Kiran wrote and expanding on it, I've written it up as a full end-to-end blog post which is here; it might save others time in future</p>
<p><a href=""https://medium.com/kinandcartacreated/consuming-informatica-api-data-with-data-factory-and-data-flows-a3220f6190d3"" rel=""nofollow noreferrer"">https://medium.com/kinandcartacreated/consuming-informatica-api-data-with-data-factory-and-data-flows-a3220f6190d3</a></p>
"
"67227932","Flatten two arrays having corresponding values using mapping data flow in azure data factory","<p>I am new to data flows in adf. I have a set of json files in a folder which i would like to parse and flatten two arrays into a csv.
The json structure is as follows:
<a href=""https://i.stack.imgur.com/FkwkQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FkwkQ.png"" alt=""enter image description here"" /></a></p>
<p>I would like to flatten the timestamps and values arrays. Each timestamp in Timestamps array has the corresponding value in the Values array. i.e, value at Timestamps[i] is Values[i]. The output should have a row for each timestamp and corresponding value for a particular label.</p>
<p><a href=""https://i.stack.imgur.com/82Tmh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/82Tmh.png"" alt=""enter image description here"" /></a></p>
<p>Can anyone please help me in achieving this</p>
<p>Thank you!</p>
","<azure><azure-data-factory>","2021-04-23 10:03:05","1386","0","4","72106399","<p>You could also do this with Azure Synapse Analytics, serverless SQL pools which supports <code>OPENJSON</code>:</p>
<pre><code>SELECT
    JSON_VALUE( m.value, '$.Id' ) Id,
    JSON_VALUE( m.value, '$.Label' ) Label,
    t.value AS ts,
    v.value AS v
FROM
    OPENROWSET(
        BULK 'https://somestorage.dfs.core.windows.net/datalake/raw/json/metrics.json',
        FORMAT = 'CSV',
        FIELDQUOTE = '0x0b',
        FIELDTERMINATOR ='0x0b',
        ROWTERMINATOR = '0x0b'
    )
    WITH (
        jsonContent varchar(MAX)
    ) AS [result]
    CROSS APPLY OPENJSON( jsonContent, '$.MetricDataResults' ) m
        CROSS APPLY OPENJSON( m.value, '$.Timestamps' ) t
        CROSS APPLY OPENJSON( m.value, '$.Values' ) v
WHERE t.[key] = v.[key];
</code></pre>
<p>My results:
<a href=""https://i.stack.imgur.com/722hW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/722hW.png"" alt=""enter image description here"" /></a></p>
<p>It's a valid alternative approach with a pay as you go service.</p>
"
"67218069","azure data factory - data flow source file error","<p>I have a data flow within a foreach loop. It loops through pipe delimited txt files in azure storage, configured the storage as block blob and used identity management as a way for my data factory resource to connect to storage account, i made sure it had all necessary access to the storage account. The pipeline and files got processed/ran fine until last week. All of a sudden this week, i started getting the below error. The Data flow runs for about 15 minutes and throws this error. I made sure the latest file is of the same old format. Anyone faced this issue?</p>
<p>{&quot;StatusCode&quot;:&quot;DFExecutorUserError&quot;,&quot;Message&quot;:&quot;Job failed due to reason: at Source 'sourcefiles': Status code: -1 error code: null error message: InvalidAbfsRestOperationExceptionjava.net.UnknownHostException: stroageaccountname.dfs.core.windows.net&quot;,&quot;Details&quot;:&quot;at Source 'sourcefiles': Status code: -1 error code: null error message: InvalidAbfsRestOperationExceptionjava.net.UnknownHostException: stroageaccountname.dfs.core.windows.net&quot;}</p>
","<azure><azure-data-factory>","2021-04-22 17:24:35","384","0","1","67233892","<p>I reached out to microsoft support and they asked me to change the Azureblob storage authentication method from managedidentity to Accountkey method and it worked.</p>
"
"67197018","How to create a generic data flow in Azure Synapse","<p>I am trying to build a generic data flow using parameters to specifiy the columns names.</p>
<p>I can use byName within &quot;Filter&quot;, &quot;Join&quot; and &quot;Derived Columns&quot;, but get the following error when using byName in &quot;Window&quot;:-
<a href=""https://i.stack.imgur.com/n27MI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/n27MI.png"" alt=""enter image description here"" /></a></p>
<p>For example byName('xxxx') works fine in the previous steps but fails in &quot;Window&quot;.</p>
<p>How can I work around this?</p>
","<azure><azure-data-factory><azure-synapse>","2021-04-21 13:35:22","101","0","1","67202175","<p>Instead of using byName() directly in the over clause, use a Derived Column prior to your Window transformation and set the name of the column using byName() there. Call the new column something like columnForMyOverClause. Then pick columnForMyOverClause in the Window transformation.</p>
"
"67193158","How can I filter on a list in Azure Synapse filter transformation mapping data flow","<p>In Azure Synapse Data flows I want to filter on a list of values.</p>
<p>I can use the following syntax:-</p>
<p><code>column1 == 'A' || column1 =='B'</code></p>
<p>but would like to use something like this:-</p>
<p><code>column1 IN ('A','B')</code></p>
<p>How can I do that?</p>
<p><a href=""https://i.stack.imgur.com/dhS2l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dhS2l.png"" alt=""enter image description here"" /></a></p>
","<azure><azure-data-factory><azure-synapse>","2021-04-21 09:38:21","1053","2","1","67193413","<p>This expression <code>column1 IN ('A','B')</code> can't work. Instead,you can build an array and then use this expression:</p>
<pre><code>in(['A','B'],column1)
</code></pre>
<p>Reference:<a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#in"" rel=""noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#in</a></p>
"
"67188957","How to control data flow for a combination of pagination, select filter and search with jQuery","<p>I'm in charge of maintaining an old project using jQuery and PHP. I have problem of making a combination of pagination, select filter and search on a list of div. Currently, my approach is using built-in hide() and show() functions in jQuery to render data for those 3 features. For example, when user paginate, I hide() all div in prev page. and show() divs in next page. However, I'm struggle to manage the data flow between 3 features. Like when I paginate away first and then select some filter. The data list for pagination is not correct anymore.</p>
","<javascript><jquery>","2021-04-21 03:30:32","90","0","1","67189176","<p>Is the data coming from the server side (php)?</p>
<p>What you need to try and achieve is a fixed / known set of items that you can filter, search and paginate through. In that order. Certainly with pagination coming last.</p>
<p>Obviously if you have a set of results based on some criteria (a blank search or blank filter) the amount of pages in the result-set are going to be different to what you would have if you have a non-blank search or filter. You need to take that in to account.</p>
<p>I would suggest that when search or filter change, you should be going back to page 1. Pagination is <em><strong>related</strong></em> to the results you have, whereas the search and filter <em><strong>determine</strong></em> the results you have. Does that make sense?</p>
<p>I would apply the search and/or filter first, to figure out how many pages I have, then reset the current page to 1.</p>
"
"67139518","Multiple data flow tasks vs one data flow task with many source-destinations","<p>I am building a simple SSIS package that truncates 10 tables and fills them up again. The source data is on one Server and the destination is a SQL Server. The data passes through another Server where the SSIS package get executed, since I do not have the permissions to execute SSIS packages from the destination SQL Server.</p>
<p>Question: should I create 10 separate data flow tasks or one data flow task containing 10 source/destinations?</p>
<p>My reasoning is that since the data passes though the server which executes the SSIS package then it might use all the memory on that server since all the data are being transformed simultaneously...</p>
","<sql-server><ssis>","2021-04-17 14:53:26","381","0","1","67140088","<p>Create 11 packages.</p>
<ul>
<li>10 implementing the truncate and reload pattern. Here's a <a href=""https://billfellows.blogspot.com/2015/01/biml-replicate-o-matic.html"" rel=""nofollow noreferrer"">replicate-o-matic Biml Pattern</a></li>
<li>1 implementing an orchestrator pattern</li>
</ul>
<p>If it's truly a truncate and reload, that's extract and load, no transformation required. The amount of data in the data buffer (in memory) will be approximately constant for the whole run but it will not allocate enough data to hold all the source data. It'll extract data in a batch size from the source, do whatever transformations are specified - none in your case, and then push it to the destination. If there's more data to be had, the same allocated memory is reused with the subsequent batches.</p>
<p>Your orchestrator pattern then allows you to structure the execution however makes sense. If your concern is minimizing memory, then you likely want the sequence execution.</p>
<ul>
<li><p>You could run all packages in sequence - this would result in steady memory usage but a longer total duration.</p>
</li>
<li><p>You could run the packages all in parallel - shorter total duration but a much higher burst of memory usage.</p>
</li>
<li><p>My personal preference is to use a combination of parallel and sequence. I have 3, 5, whatever foreach loop containers that will all run in parallel. They enumerate through the work to be done which I will have segmented beforehand (typically, there's a few big tables and they might be the only member of a processing stream and then lots of little tables could be in another). The purpose of this is to try and get the total processing across all the parallel operators to do a roughly equivalent workload.</p>
</li>
</ul>
"
"67106031","Azure Data Factory ""Name File as column data"" option in Sink transformation of data flow is creating blobs name for virtual folder","<p>I have create and ADF pipeline. Source and sink, both are storage account.
I want to create file based on date in column data, so I selected the option &quot;File Name as Column Data&quot;.
In this option, we are giving filename, with virtual folder path.</p>
<p>But when process is completed, blob(hot inferred) also created for virtual folder, which I don't need. I just need blob for files(which is also present). If I delete those virtual folder blobs, I cant put the file in incremental way in those folder.</p>
<p>What should I do?</p>
","<azure-data-factory><azure-storage-account>","2021-04-15 09:41:46","1188","0","1","67118146","<p>You should set the column format first in source <strong>projection</strong> before set &quot;File Name as Column Data&quot;.</p>
<p><strong>Here's my source file:</strong></p>
<p><a href=""https://i.stack.imgur.com/ILuNq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ILuNq.png"" alt=""enter image description here"" /></a></p>
<p><strong>Source <strong>projection</strong> settings:</strong> specify the column date type and format.</p>
<p><a href=""https://i.stack.imgur.com/B4wkV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B4wkV.png"" alt=""enter image description here"" /></a></p>
<p><strong>Sink settings:</strong></p>
<p><a href=""https://i.stack.imgur.com/Op0Nk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Op0Nk.png"" alt=""enter image description here"" /></a></p>
<p>Output:
<a href=""https://i.stack.imgur.com/w95Qa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w95Qa.png"" alt=""enter image description here"" /></a></p>
<p>Then we can get the output files without the virtual folder.</p>
<p>HTH.</p>
"
"67104874","Running a data flow job with n2-standard-2 machine type , will the data flow job anytime run with only 2 vCPU's or it can auto scale to quota limit","<p>I have a question regarding data flow job for the machine type. I understand data flow uses default machine type as n1-standard-1 for batch processing and we have quota issues in asia-south1 for n1 machine type. We have been asked to use N2 machine type instead. So while creating the data flow template or while even running the data flow job from cloud shell, I am specifying the machine type as</p>
<pre><code>python -m &lt;Python script&gt; \
--project &lt;project&gt; \
--region asia-south1 \
--runner DataflowRunner \
--staging_location &lt;staging location&gt; \
--temp_location &lt;temp location&gt; \
--subnetwork &lt;sub network&gt; \
--zone asia-south1-a \
--machine_type n2-standard-2 \
--save_main_session
</code></pre>
<p>My understanding is that machine type is what data flow uses when starting workers but in case of auto scaling and based on job, the numbers of vCPU's can be more based on project quota which is 32 CPU's now and data flow job will not limit it to just 2 vCPU's. In case my data flow job needs 4 vCPU's while running, it will get assigned irrespective of what I have mentioned while running or creating the data flow template as n2-standard-2.</p>
<p>Can some one please confirm my understanding?</p>
","<google-cloud-platform><google-cloud-dataflow>","2021-04-15 08:27:45","2325","0","1","67106095","<p>In the <a href=""https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options"" rel=""nofollow noreferrer"">dataflow pipeline option</a> you have 2 interesting parameters:</p>
<ul>
<li><code>num_workers</code> which is the minimal number of worker that you want</li>
<li><code>max_num_workers</code> which is the max number of worker that you want</li>
</ul>
<p>It your machine type is <code>n2-standard-2</code> and your n2 vCPU quotas is 32, you can't put more than 16 in the num_workers param (min or max). If you put 4 in the <code>max_num_workers</code> and even if your project quotas is 32 vCPU, the pipeline will be able to create only 4 n2-standard-2 VM, thus maximum 8 vCPU.</p>
<p>So, the project quota can limit you globally (for the Dataflow pipeline, and also others VM/GKE creation), and the <code>max_num_workers</code> limit only your current pipeline in number of workers (no direct relation with the number CPU, it depends on the machine type)</p>
"
"67039582","How to build a collaborative app using Google Drive as storage?","<p>I am writing an app with JSON as the file format to store the data. I want the users to store those JSON files in their own Google drive so that they can share the files to their friends as they want. And their friends can also open my app at the same time to collaborative real time.</p>
<p>Google Drive API allows an app to create and delete files in users' Google drive. But how can my app support collaboration like Google Docs or Google Sheets, so that 2 or more people can work on the same file and the change will be saved back to the Google drive and updated on the other side in the real time?</p>
","<google-drive-api><collaboration><data-synchronization><google-drive-shared-drive>","2021-04-10 21:50:24","354","1","1","67054942","<h3>Answer:</h3>
<p><em>You need to create a <a href=""https://developers.google.com/drive/api/v3/reference/permissions"" rel=""nofollow noreferrer"">Permission</a> for each user/group you wish to give access to the file.</em></p>
<h3>Example:</h3>
<p>Using the <a href=""https://developers.google.com/drive/api/v3/reference/permissions/create"" rel=""nofollow noreferrer"">Permissions.create</a> API method, you can create a permission and attach it to the file.</p>
<p>The following example shows how to give a user write permissions using the Google API JavaScript client:</p>
<pre class=""lang-js prettyprint-override""><code>// Copyright 2021 Google LLC.
// SPDX-License-Identifier: Apache-2.0

function execute() {
  return gapi.client.drive.permissions.create({
    &quot;fileId&quot;: &quot;-some-do&quot;,
    &quot;resource&quot;: {
      &quot;role&quot;: &quot;writer&quot;,
      &quot;type&quot;: &quot;user&quot;,
      &quot;emailAddress&quot;: &quot;someuser@domain.com&quot;
    }
  })
}
</code></pre>
<h3>References:</h3>
<ul>
<li><a href=""https://developers.google.com/drive/api/v3/reference/permissions"" rel=""nofollow noreferrer"">Permissions | Google Apps Script | Google Developers</a></li>
<li><a href=""https://developers.google.com/drive/api/v3/reference/permissions/create"" rel=""nofollow noreferrer"">Permissions: create | Google Apps Script | Google Developers</a></li>
</ul>
"
"67038727","Export data flow to csv file in Sftp server","<p>I want to make some logic for data from diffrents data sources  after that I want to generate csv file with the result of joins and save it in a Sftp server.</p>
<p>I made a data flow that contains my logic, joins and conditions,after that when I sink data to Sftp dataset an error is appearing
<a href=""https://i.stack.imgur.com/vwFk2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vwFk2.png"" alt=""enter image description here"" /></a></p>
<p>is there any solutions to make this ? making join from different table in different database and export a csv file to sftp server and in the same time using Azure data factory for this.
Thanks.</p>
","<azure><azure-data-factory>","2021-04-10 20:04:45","616","0","1","67066911","<p>Like the error said, SFTP is not supported in Data Flow. You can ref the connector <a href=""https://learn.microsoft.com/en-us/azure/data-factory/connector-overview"" rel=""nofollow noreferrer"">support overview</a>. We can't change this.</p>
<p>Some workarounds are that you could store the file into a temp storage firstly, like blob storage or table in SQL database, then copy the data into SFTP again.</p>
<p>I'm glad you will try to use temporary table in SQL and then copy data from one dataset to SFTP server.</p>
<p>HTH.</p>
"
"66997667","Docker multi postgres containers with one mount point","<p>I have two Postgres databases and I want to sync data between themes.</p>
<p>So far I have these two containers, exactly the same with different posts and different names.</p>
<pre><code>docker container run --name='p1' -d -p 5435:5432 -v /tmp/dbs/test/:/var/lib/postgresql/data postgres


docker container run --name='p2' -d -p 5436:5432 -v /tmp/dbs/test/:/var/lib/postgresql/data postgres  
</code></pre>
<p>The problem happens when something changed.</p>
<p>If I change something in p1 like insert a row, then I can't see it in p2.</p>
<p>But if I kill, and run containers again, then I can see the inserted data in both of themes.</p>
<p>Why this is happening?<br />
Is there a way to sync data between themes?</p>
","<postgresql><docker><docker-volume><data-synchronization>","2021-04-08 05:08:56","443","2","1","66998188","<p>Running two postmaster processes on the same files is a sure road to data corruption. Don't do that.</p>
<p>You cannot have multi-master replication with standard PostgreSQL, but you can have a read-only standby server.</p>
"
"66917316","How to use named destination for custom processor in spring cloud data flow?","<p>I am learning SCDF these days...I hava some questions about named destination.
I create a stream like &quot;:test-topic &gt; log&quot;. I can see the log sink consume data from topic &quot;test-topic&quot;.
But if I add a custom processor to SCDF.Then I create a stream like this:</p>
<p>:test-topic-source &gt; etl-data-transform &gt; :test-topic-sink</p>
<p>I think the &quot;etl-data-transform&quot; processor will consume data from topic &quot;test-topic-source&quot;(Kafka) and product data to &quot;test-topic-sink&quot;, but the log is &quot;Subscribed to topic(s): stringOperation-in-0&quot; and &quot;Using kafka topic for outbound: stringOperation-out-0&quot; (&quot;stringOperation&quot; is my custom function)</p>
<p>Why SCDF do not use the topic &quot;test-topic-source&quot; and &quot;test-topic-sink&quot;?
How to solve it?</p>
<hr />
<p>I know I can use properties like this:
spring.cloud.stream.function.bindings.stringOperation-in-0=in
spring.cloud.stream.bindings.in.destination=test-topic-source
but if I want to output to two topics?</p>
<p>Thanks!</p>
","<java><spring><spring-boot><spring-cloud-stream><spring-cloud-dataflow>","2021-04-02 09:36:02","161","0","2","66949815","<p>Looks like your custom processor application uses the <code>inbound</code> and <code>outbound</code> names to be <code>in</code> and <code>out</code>. The SCDF expects these names to be <code>input</code> and <code>output</code> respectively. This is the reason you see the explicit binding names based on the names <code>in</code> and <code>out</code>. Please change the names to be <code>input</code> and <code>output</code> and I believe that should fix this issue.</p>
"
"66917316","How to use named destination for custom processor in spring cloud data flow?","<p>I am learning SCDF these days...I hava some questions about named destination.
I create a stream like &quot;:test-topic &gt; log&quot;. I can see the log sink consume data from topic &quot;test-topic&quot;.
But if I add a custom processor to SCDF.Then I create a stream like this:</p>
<p>:test-topic-source &gt; etl-data-transform &gt; :test-topic-sink</p>
<p>I think the &quot;etl-data-transform&quot; processor will consume data from topic &quot;test-topic-source&quot;(Kafka) and product data to &quot;test-topic-sink&quot;, but the log is &quot;Subscribed to topic(s): stringOperation-in-0&quot; and &quot;Using kafka topic for outbound: stringOperation-out-0&quot; (&quot;stringOperation&quot; is my custom function)</p>
<p>Why SCDF do not use the topic &quot;test-topic-source&quot; and &quot;test-topic-sink&quot;?
How to solve it?</p>
<hr />
<p>I know I can use properties like this:
spring.cloud.stream.function.bindings.stringOperation-in-0=in
spring.cloud.stream.bindings.in.destination=test-topic-source
but if I want to output to two topics?</p>
<p>Thanks!</p>
","<java><spring><spring-boot><spring-cloud-stream><spring-cloud-dataflow>","2021-04-02 09:36:02","161","0","2","66962411","<p>Thx! That is my mistake. I add '@EnableBinding(Processor.class)' to my custom processor and then fix it.</p>
"
"66906305","block data flow from a TCP socket","<p>I'm using libsoccr, which allows to checkpoint/restore TCP connection. Here is how I'm trying to use it :</p>
<pre><code>/* some code to initialize and connect the socket */

#define max 10000000
struct libsoccr_sk *so;
struct libsoccr_sk_data data;
int sockfd, dsize, size;
char *buff = (char *)malloc(MAX * sizeof(char));

/* some code to fill the buffer */

write(sockfd, buff, MAX);
so = libsoccr_pause(sockfd);
dsize = libsoccr_save(so, &amp;data, sizeof(data));

/* some code */
</code></pre>
<p>The function <code>libsoccr_save(so, &amp;data, sizeof(data))</code> returns me -6.</p>
<p>I re read libsoccr's documentation, and found out &quot;any data flow for this socket must be blocked by the caller before this call&quot; talking about <code>libsoccr_pause(sockfd)</code>. I looked into the source code and found out that the queue length is got with <code>ioctl(sockfd, SIOCOUTQ, &amp;size)</code> and later, the content of the queue itself is got with <code>ret = recv(sockfd, buf, size+1, MSG_PEEK | MSG_DONTWAIT)</code>. Using gdb, it appears that I always get <code>ret = 0</code> even if the queue is not empty, surely because I don't block the data flow of the socket, but I have no clue on how to do it.
So the question is, how can I block the data flow of a socket ?</p>
","<c><sockets><tcp>","2021-04-01 14:23:24","120","2","1","72295612","<p>A good way to block the data flow of a socket is to use &quot;plug qdisc&quot;. All the documentation can be found here : <a href=""https://www.infradead.org/%7Etgr/libnl/doc/api/group__qdisc__plug.html"" rel=""nofollow noreferrer"">https://www.infradead.org/~tgr/libnl/doc/api/group__qdisc__plug.html</a></p>
"
"66821919","Spring Batch Processing using Spring Cloud data flow","<p>I have spring batch app and I want an admin portal to manage failed jobs and see other job related activities. I saw there was some Spring batch Admin portal package in spring, but it has been deprecated in 2017 and I have to use Spring cloud data flow as mentioned <a href=""https://docs.spring.io/spring-batch-admin/trunk/"" rel=""nofollow noreferrer"">here</a>. I want to know for Spring cloud data flow, is this some dependency we need to add to project as an artifact or is this some separate standalone service that needs to be set up?</p>
<p>My batch has dozen of cron jobs, can I just give my jar to Cloud Data Flow and it will take care of rest or Do I need to configure each and every job there? Any sample for the same are appreciated, as I want to know how big will be an effort to set up all this.</p>
<p>On the side note : My app is a combination of some REST controllers and some batch jobs.So does it make sense to use cloud data flow? If not, then is there better console manager for batch jobs(like restart ,cancel jobs portal) etc.?</p>
","<java><spring><spring-boot><spring-batch><spring-cloud-dataflow>","2021-03-26 17:46:47","965","0","1","67394062","<p>Spring Data Flow requires a server to be running, where you can deploy your jars (register tasks as wrapper over spring batch). This server will be responsible for orchestration and deploying to runtime. If you have massive work load you probably need to go with cluster and Kubernetes, which supports scheduling via cron, but if now you have a single server that handles all together and you don't have performance issues, you may simplify it by using Local mode. But with Local mode you have to manage scheduling anyway by yourself with Quartz for example. <br>
<a href=""https://dataflow.spring.io/docs/feature-guides/batch/scheduling/"" rel=""nofollow noreferrer"">https://dataflow.spring.io/docs/feature-guides/batch/scheduling/</a></p>
<p>So just having SCDF for monitoring may be complicated and probably requires re-think your application design. Also SCDF as I see is good when you have some dependencies between tasks. <br>
Maybe it would be easier for you to write couple of REST endpoint to fetch failed jobs and re-run them - everything depends on what you need and how big your app is.</p>
<p>PS:
I'm currently also thinking between having just spring batch monolith or cloud ready tasks :)</p>
"
"66818414","Issue in incrementing ID dynamically in azure data factory without using data flows","<p>I am working with a table which is in Azure SQL Database, The ID is not in sequential order. I want to copy this data into data lake. Before copying the data, I want to dynamically increment the id either in the same column or add an additional column with auto increment in the same  table. The requirement is not to use any data flows.. this have to be done only by using pipeline activities.</p>
<p>My Execution:
I have tried using until loop by adding two parameters start=0, end=5 and two variables counter and temporary. Firstly, I have set a variable for initializing counter as start parameter and I have sent the counter into the until loop. Inside the loop, I have 2 set variable activities. In first set variable activity, I am trying to increment the counter by 1 and saving it in temporary variable. In the second set variable, I am copying the temporary variable value to the counter.
After this, I am passing the counter into copy activity, and putting it in an additional column , which I am adding in Source tab.
The increment is happening column level. I am trying to make this row level but failing.. Do you have any approach?</p>
<p>I am so sorry for making this complicated.
Please find the screenshots in <a href=""https://github.com/sarvani1929/azure/blob/main/issue.docx"" rel=""nofollow noreferrer"">https://github.com/sarvani1929/azure/blob/main/issue.docx</a></p>
","<azure><azure-sql-database><azure-pipelines><azure-data-factory><azure-data-lake>","2021-03-26 14:02:36","1301","0","1","66818884","<p>You could use the <a href=""https://learn.microsoft.com/en-us/sql/t-sql/functions/row-number-transact-sql?view=sql-server-ver15"" rel=""nofollow noreferrer"">ROW_NUMBER Window Function</a> in a SQL Query to generate the row number as a new column:</p>
<p><a href=""https://i.stack.imgur.com/z3bCy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z3bCy.png"" alt=""enter image description here"" /></a></p>
<p>Then use the Query in the Copy Activity Source.</p>
"
"66816023","Common format to represent a data mapping allowing duplicated column/key names","<p><strong>Context:</strong><br>
I actually have an extracting task in <strong>PHP</strong> which executes a <strong>SQL</strong> query and generates a <strong>CSV</strong> file. The CSV columns names are based on the query column names.<br>
The CSV must contain a lot of columns and <strong>some column names appear multiple times</strong> (2 or 3 times). The team processing this extract need all of the columns because the value can change from a duplicate column to an other, and the order is important.</p>
<p><strong>From SQL to CSV:</strong><br>
I can set aliases in the SQL query <code>SELECT</code> statement with duplicated names:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT
    col_a AS 'duplicate',
    col_b AS 'some_col',
    '' AS 'duplicate',
    col_c AS 'other_col',
...
</code></pre>
<p>The query is executed with <strong>PDO</strong> and fetched with a classic <code>PDO::FETCH_ASSOC</code> which returns an associative array. As the keys of an array are unique, I lose the duplicated columns:</p>
<pre class=""lang-php prettyprint-override""><code>[
    'duplicate' =&gt; 'something',
    'some_col'  =&gt; 'value',
    'other_col' =&gt; 'other value'
    ...
]
</code></pre>
<p><strong>Failing solutions:</strong><br>
Using the <code>PDO::FETCH_NAMED</code> fetch mode would not work because I would lose the order of the columns, requiring much more work to make it work in a reusable way (the task is a generic SQL/CSV extract).</p>
<p>I can rename the aliases in the <code>SELECT</code> clause to be unique but then I wouldn't be able to use the column names for the CSV.<br>
<strong>Except if I have a mapping of the SQL columns to the CSV columns.</strong></p>
<p>I thought about a format like JSON but this won't work as JSON data <em>should not</em> contain duplicated keys, and I would lose them while reading them in PHP anyway:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;CSV_COL&quot;: &quot;SQL_COL&quot;,
    &quot;OTHER_CSV&quot;: &quot;OTHER_SQL&quot;,
    &quot;CSV_COL&quot;: &quot;something_else&quot;
}
</code></pre>
<p>Is there a simple format that could allow for duplicate column/key names that PHP would not delete while reading it ?</p>
","<php><duplicates><export-to-csv><data-representation>","2021-03-26 11:25:29","78","-1","1","67088667","<p>I solved this with a custom format. This is not the ideal solution as I would prefer a more standard one but it works. Here are the details:</p>
<h3>Custom format</h3>
<p>First, a form field accepts text input like the following:</p>
<pre><code>csv_column:sql_column

Name of CSV column   :   &quot;some string&quot;
 csv_column :
</code></pre>
<p>Rules of the format:</p>
<ul>
<li>each line maps a <strong>CSV column</strong> to a <strong>SQL column / value</strong></li>
<li>blank lines are ignored but are fine</li>
<li><strong>CSV</strong> columns are separated from <strong>SQL</strong> columns with a <code>:</code></li>
<li>you can add as many whitespaces as you want around column names</li>
<li>you can define a default string value for the <strong>SQL</strong> side by enclosing the value with <code>'</code> or <code>&quot;</code></li>
<li>if a <strong>CSV</strong> column must be empty, the <strong>SQL</strong> part is optional</li>
</ul>
<h3>Format parsing</h3>
<p>Then I wrote a function which will transform this string into a multidimensionnal array:</p>
<pre class=""lang-php prettyprint-override""><code>/**
 * Parse CSV/SQL column mapping.
 * @param string $textMapping column mapping in text format
 * @return string[][]|null each array has a &quot;CSV&quot; &amp; &quot;SQL&quot; key with corresponding column/value and a &quot;type&quot; key to
 *                         differentiate SQL columns from SQL values
 */
function getColumnMapping(string $textMapping): ?array
{
    // No mapping
    if (trim($textMapping) === '') {
        return null;
    }

    /*
     * RegEx explanation:
     *      \h*(.*\S)   optional whitepaces following by a CSV column name
     *                  (anything but does not end with whitespace)
     *      \h*:\h*     a colon optionally surrounded by whitespaces
     *      ([^\v]*)\h* an optional SQL column/value, optionally followed by whitespaces
     *                  (anything but line breaks)
     *      (?:\R|$)    a line break or the end of the string
     *
     * CSV parts are at index 1 and SQL parts are at index 2
     */
    preg_match_all('~\h*(.*\S)\h*:\h*([^\v]*)\h*(?:\R|$)~', $textMapping, $matches);

    // No mapping found
    if ([] === $matches[0]) {
        return null;
    }

    $mapping = [];

    for ($i = 0; $i &lt; count($matches[0]); $i++) {
        $mappingItem = [
            'CSV' =&gt; $matches[1][$i],
            'SQL' =&gt; $matches[2][$i],
            'type' =&gt; 'column',
        ];

        if ('' === $mappingItem['SQL']) {
            // Empty: empty value
            $mappingItem['type'] = 'value';
        } elseif (1 === preg_match('~^(\'|&quot;)(.*)\1$~', $mappingItem['SQL'], $match)) {
            // Enclosed by quotes: string value
            $mappingItem['SQL'] = $match[2];
            $mappingItem['type'] = 'value';
        }

        $mapping[] = $mappingItem;
    }

    return $mapping;
}
</code></pre>
<p>The previous example would result in the following array:</p>
<pre class=""lang-php prettyprint-override""><code>[
    [
        'CSV'  =&gt; 'csv_column',
        'SQL'  =&gt; 'sql_column',
        'type' =&gt; 'column',
    ],
    [
        'CSV'  =&gt; 'Name of CSV column',
        'SQL'  =&gt; 'some string',
        'type' =&gt; 'value',
    ],
    [
        'CSV'  =&gt; 'csv_column',
        'SQL'  =&gt; '',
        'type' =&gt; 'value',
    ],
]
</code></pre>
<h3>Building the CSV file</h3>
<p>Finally, after executing the SQL query, each result is passed to a function to build the data row:</p>
<pre class=""lang-php prettyprint-override""><code>/**
 * Build a CSV data line based on a column mapping
 * @param array $resultRow SQL result row
 * @param array $mapping CSV/SQL column mapping
 * @return array formatted data
 */
function buildRowFromColumnMapping(array $resultRow, array $mapping): array
{
    $row = [];

    foreach ($mapping as $mappingItem) {
        if ($mappingItem['type'] === 'value') {
            $row[] = $mappingItem['SQL'];
        } elseif (false === array_key_exists($mappingItem['SQL'], $resultRow)) {
            throw new \LogicException(sprintf('Column &quot;%s&quot; not found.', $mappingItem['SQL']));
        } else {
            $row[] = $resultRow[$mappingItem['SQL']];
        }
    }

    return $row;
}
</code></pre>
<p>The result is a unidimensionnal array you can send to anything you want (<a href=""https://www.php.net/manual/en/function.fputcsv"" rel=""nofollow noreferrer"">fputcsv()</a>, specific lib, ...).</p>
"
"66784663","React-table data flow between outside and inside of hooks.visibleColumns.push invocation","<p>I would like to create an array outside <code>useTable</code> invocation, manipulate the array, and based on the array state make checkbox checked or unchecked. After each click on a button, length of the array is increased by adding one element to the array. When length become greater than 3, the <code>input</code> should be checked.</p>
<p>The problem is that the array state is different inside and outside <code>checked</code> attribute of <code>input</code>. Outside it works as expected: the array length increases. Inside, the array length is equal to initial length of 0.</p>
<p>I have attached code with some logging. I think that the relevant part probably ends with the end of <code>useTable</code> invocation (then is some code which I took from <code>react-table</code> docs with button and mock data, columns added). What changes should I introduce to the code to make it work as I expect?</p>
<pre><code>import React, { useMemo, useState } from 'react';
import { useTable } from 'react-table'

function Table({ columns, data }) {

  // neither stateArr nor simpleArr help reach what I would like to
  const [stateArr, setStateArr] = useState([]);
  let simpleArr = [...stateArr];

  const handleOnButtonClick = () =&gt; {
    console.log(&quot;Outside checked: simpleArr, stateArr&quot;);
    console.log(simpleArr);
    console.log(stateArr);
    setStateArr([...stateArr, 1]);
    // in this case unnecessary, since, as I understand, simpleArr is rendered and (re)assigned above
    // simpleArr = [...stateArr];
  };

  const {
    getTableProps,
    getTableBodyProps,
    headerGroups,
    rows,
    prepareRow,
  } = useTable({
    columns,
    data,
  },
  (hooks) =&gt; {
    hooks.visibleColumns.push((columns) =&gt; {
      return [
        {
          id: 'checkedInputs',
          Header: () =&gt; {
            return (&lt;div&gt;
              &lt;input type=&quot;checkbox&quot;
                   // working, not most elegant way to combine logging and computing boolean
                   checked={console.log(&quot;Inside checked: simpleArr, stateArr&quot;) || console.log(simpleArr)
                     || console.log(stateArr) || simpleArr.length &gt; 3 || stateArr.length &gt; 3} /&gt;
            &lt;/div&gt;);
          },
          Cell: () =&gt; {
            return (&lt;div&gt;R&lt;/div&gt;);
          },
        },
        ...columns,
        ];
    });
  }
  );

  return (
    &lt;div&gt;
    &lt;table {...getTableProps()}&gt;
      &lt;thead&gt;
        {headerGroups.map(headerGroup =&gt; (
          &lt;tr {...headerGroup.getHeaderGroupProps()}&gt;
            {headerGroup.headers.map(column =&gt; (
              &lt;th {...column.getHeaderProps()}&gt;{column.render('Header')}&lt;/th&gt;
            ))}
          &lt;/tr&gt;
        ))}
      &lt;/thead&gt;
      &lt;tbody {...getTableBodyProps()}&gt;
        {rows.map((row, i) =&gt; {
          prepareRow(row)
          return (
            &lt;tr {...row.getRowProps()}&gt;
              {row.cells.map(cell =&gt; {
                return &lt;td {...cell.getCellProps()}&gt;{cell.render('Cell')}&lt;/td&gt;
              })}
            &lt;/tr&gt;
          )
        })}
      &lt;/tbody&gt;
    &lt;/table&gt;
    &lt;button type=&quot;button&quot; onClick={handleOnButtonClick}&gt;Click Me!&lt;/button&gt;
    &lt;/div&gt;
  )
}

function App() {
  const columns = useMemo(
    () =&gt; [
      {
        Header: 'Animal Type',
        accessor: 'animalType',
      },
      {
        Header: 'Number of legs',
        accessor: 'numberOfLegs',
      },
    ],
    [],
  );

  const data = useMemo(
    () =&gt; [
      {
        animalType: 'dog',
        numberOfLegs: 4,
      },
      {
        animalType: 'snake',
        numberOfLegs: 0,
      },
    ],
    [],
  );

  return (
    &lt;Table columns={columns} data={data} /&gt;
  )
}

export default App;
</code></pre>
","<javascript><reactjs><react-table>","2021-03-24 15:48:31","2684","1","1","66789268","<h3>Stale Data</h3>
<p>The <code>hooks.visibleColumns.push</code> function is called one time when the table is created.  It creates a <code>Header</code> render component that takes some props and returns a JSX element.  The function which <em>renders</em> the <code>Header</code> based on these props is called every time that the table updates.  The function which <em>creates</em> this <code>Header</code> component is called <strong>once</strong>.</p>
<p>In your example, you create a <code>Header</code> component which prints out some data based on the values of <code>simpleArr</code> and <code>stateArr</code> <strong>at the time that it was created</strong>, not at the time that it was called.</p>
<h3>Table State</h3>
<p>If we want our <code>Header</code> component to render with current data then we should get that data from props.  The <code>Header</code> is called with quite a lot of props but the one that we will use is <code>state</code> which is the state of the table.  We will set the <code>initialState</code> of the table to an object <code>{ stateArr: [] }</code>.  This gets merged with the standard table state <code>{ hiddenColumns: [] }</code>.</p>
<p>The table state is updated through a <code>useReducer</code> hook, so we update it by <code>disptach</code>ing an action.  We need a custom <code>stateReducer</code> to update the table state based on the contents of the <code>action</code>.</p>
<pre><code>import React, { useMemo } from &quot;react&quot;;
import { useTable } from &quot;react-table&quot;;

function Table({ columns, data }) {
  const {
    getTableProps,
    getTableBodyProps,
    headerGroups,
    rows,
    prepareRow,
    dispatch,
    state
  } = useTable(
    {
      columns,
      data,
      initialState: {
        stateArr: []
      },
      stateReducer: (newState, action, prevState) =&gt; {
        console.log(action, newState, newState.stateArr);
        switch (action.type) {
          case &quot;incrementChecks&quot;:
            return {
              ...newState,
              stateArr: [...newState.stateArr, action.payload]
            };
          default:
            return newState;
        }
      }
    },
    (hooks) =&gt; {
      hooks.visibleColumns.push((columns) =&gt; {
        return [
          {
            id: &quot;checkedInputs&quot;,
            Header: (props) =&gt; {
              console.log(&quot;header props&quot;, props); // so you can see all the data you get
              console.log(&quot;stateArr&quot;, props.state.stateArr);
              return (
                &lt;input
                  type=&quot;checkbox&quot;
                  readOnly
                  checked={props.state.stateArr.length &gt; 3}
                /&gt;
              );
            },
            Cell: () =&gt; {
              return &lt;div&gt;R&lt;/div&gt;;
            }
          },
          ...columns
        ];
      });
    }
  );

  const handleOnButtonClick = () =&gt; {
    // payload is the item which we are appending to the array
    dispatch({ type: &quot;incrementChecks&quot;, payload: 1 });
  };

  console.log(&quot;stateArr&quot;, state.stateArr);

  return (
    &lt;div&gt;
      &lt;table {...getTableProps()}&gt;
        &lt;thead&gt;
          {headerGroups.map((headerGroup) =&gt; (
            &lt;tr {...headerGroup.getHeaderGroupProps()}&gt;
              {headerGroup.headers.map((column) =&gt; (
                &lt;th {...column.getHeaderProps()}&gt;{column.render(&quot;Header&quot;)}&lt;/th&gt;
              ))}
            &lt;/tr&gt;
          ))}
        &lt;/thead&gt;
        &lt;tbody {...getTableBodyProps()}&gt;
          {rows.map((row, i) =&gt; {
            prepareRow(row);
            return (
              &lt;tr {...row.getRowProps()}&gt;
                {row.cells.map((cell) =&gt; {
                  return (
                    &lt;td {...cell.getCellProps()}&gt;{cell.render(&quot;Cell&quot;)}&lt;/td&gt;
                  );
                })}
              &lt;/tr&gt;
            );
          })}
        &lt;/tbody&gt;
      &lt;/table&gt;
      &lt;button type=&quot;button&quot; onClick={handleOnButtonClick}&gt;
        Append
      &lt;/button&gt;
    &lt;/div&gt;
  );
}

function App() {
  const columns = useMemo(
    () =&gt; [
      {
        Header: &quot;Animal Type&quot;,
        accessor: &quot;animalType&quot;
      },
      {
        Header: &quot;Number of legs&quot;,
        accessor: &quot;numberOfLegs&quot;
      }
    ],
    []
  );

  const data = useMemo(
    () =&gt; [
      {
        animalType: &quot;dog&quot;,
        numberOfLegs: 4
      },
      {
        animalType: &quot;snake&quot;,
        numberOfLegs: 0
      }
    ],
    []
  );

  return &lt;Table columns={columns} data={data} /&gt;;
}

export default App;
</code></pre>
<p><a href=""https://codesandbox.io/s/storing-data-in-react-table-state-jwufc?file=/src/App.jsx"" rel=""nofollow noreferrer"">CodeSandbox Link</a></p>
"
"66782814","Decompilation of CIL code into some high level code - do I need to introduce new variables during data flow analysis?","<p>I'm writing a compiler from .NET CIL code to some high level language. Process is similar to decompilation. I have done some control flow analysis - detecting loops, ifs, and so on. In terms of data flow analysis i've done simple expression propagation by simulating some instructions involving evaluation stack - I treat evaluation stack as hidden variable, push more complex expressions on it, and whenever there is any assignment instruction to some variable (for example <code>starg</code> or <code>stloc</code>) - I pop and assign propagated expression from stack to this variable and translate this expression into statement in high language code. Of course for now it is so simple that it generates failures. Consider a function written in C#:</p>
<pre><code>int GCD(int n1, int n2)
{
    while(n2 != 0)
    {
        int c = n1 % n2;
        n1 = n2;
        n2 = c;
    }

    return n1;
}
</code></pre>
<p>This function compiles to IL:</p>
<pre><code>.method private hidebysig 
    instance int32 GCD (
        int32 n1,
        int32 n2
    ) cil managed 
{
    .maxstack 8

    IL_0000: br.s IL_000a
        IL_0002: ldarg.1    // load n1 on eval stack
        IL_0003: ldarg.2    // load n2 on eval stack
        IL_0004: rem        // pop n1 and n2 from stack, compute n1 % n2 and push it on stack
        IL_0005: ldarg.2    // load n2 on stack
        IL_0006: starg.s n1 // pop n2 from stack and store it in n1
        IL_0008: starg.s n2 // pop n1 % n2 from stack and store it in n2

        IL_000a: ldarg.2
        IL_000b: brtrue.s IL_0002

    IL_000d: ldarg.1
    IL_000e: ret
}
</code></pre>
<p>With this simple propagation we push <code>n1 % n2</code> on stack, then load <code>n2</code> on stack, then we have <code>starg</code> instruction, so we pop expression from stack and translate assignment to statement. Then we pop again, and do the same. Result code looks like this:</p>
<pre><code>GCD(n1, n2) {
    while (n2) { 
        n1 = n2;
        n2 = (n1 % n2); 
    }
    return n1;
}
</code></pre>
<p>Translated code has changed semantics and isn't doing what it is supposed to do, because <code>n1</code> is changed before computing <code>n1 % n2</code>. There is a need to store intermediate result of <code>n1 % n2</code> in some place, for example local variable, like in original C# code. This indicates that I have to do something inverse to dead code elimination, maybe called like &quot;necessary code introduction&quot;. I searched for some sources about methods to introduce new variables in decompilation, but I did not find any. Do you have any ideas?</p>
","<compilation><compiler-optimization><decompiling><cil><decompiler>","2021-03-24 14:07:43","146","0","1","69069204","<p>You may try to build <a href=""https://en.wikipedia.org/wiki/Static_single_assignment_form"" rel=""nofollow noreferrer"">SSA</a> form from bytecode. SSA form is used by most modern compilers and decompilers as an intermediate representation.</p>
<p>In SSA form every assignment to the variable will produce new variable so you will be always able to refer the correct value.</p>
<p>Here is algorithm description for building SSA: <a href=""https://pp.info.uni-karlsruhe.de/uploads/publikationen/braun13cc.pdf"" rel=""nofollow noreferrer"">https://pp.info.uni-karlsruhe.de/uploads/publikationen/braun13cc.pdf</a></p>
<p>After that you will need to return back to the non-SSA form. Algorithms can be found in still unfinished SSA Book: <a href=""http://ssabook.gforge.inria.fr/latest/book.pdf"" rel=""nofollow noreferrer"">http://ssabook.gforge.inria.fr/latest/book.pdf</a></p>
<p>Look for chapters 3.2 and 17.</p>
"
"66747147","How to sum values are in array using data flow in Azure Data Factory?","<p>I have a requirement to sum values which are in Array separated by | (Pipe). For Examaple [10|20|30]</p>
<p>Output should be: 60</p>
<p>Requesting you to suggest how to achieve it.</p>
<p>Regards,
Ashok</p>
","<arrays><azure-data-factory>","2021-03-22 13:35:37","611","0","1","66750656","<p>reduce(split(pipeCol, '|'), 0, #acc + toInteger(#item))</p>
"
"66684872","Looking for suggestions on performing a complex csv to json data flow transformation","<p>I'm hoping someone might be able to point me in the right direction to perform the following data transformation in Azure Data Factory.</p>
<p>Source Data (CSV) in Blob Storage:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>SKU_ID</th>
<th>TITLE</th>
<th>COLOR</th>
<th>SIZE</th>
<th>PRODUCT_ID</th>
</tr>
</thead>
<tbody>
<tr>
<td>123-1</td>
<td>Shirt</td>
<td>Blue</td>
<td>Small</td>
<td>123</td>
</tr>
<tr>
<td>123-2</td>
<td>Shirt</td>
<td>Blue</td>
<td>Medium</td>
<td>123</td>
</tr>
<tr>
<td>123-3</td>
<td>Shirt</td>
<td>Blue</td>
<td>Large</td>
<td>123</td>
</tr>
<tr>
<td>456-1</td>
<td>Hoodie</td>
<td>Black</td>
<td>Small</td>
<td>456</td>
</tr>
<tr>
<td>456-2</td>
<td>Hoodie</td>
<td>Black</td>
<td>Medium</td>
<td>456</td>
</tr>
<tr>
<td>456-3</td>
<td>Hoodie</td>
<td>Black</td>
<td>Large</td>
<td>456</td>
</tr>
</tbody>
</table>
</div>
<p>Each sku_id should be grouped together in the final JSON output by product_id.</p>
<p>Transformed to JSON for storage in Comos DB:</p>
<pre><code>[
    {
        &quot;product_id&quot;: &quot;123&quot;,
        &quot;title&quot;: &quot;Shirt&quot;,
        &quot;variants&quot;: [
            {
                &quot;sku_id&quot;: &quot;123-1&quot;,
                &quot;color&quot;: &quot;Blue&quot;,
                &quot;size&quot;: &quot;Small&quot;
            },
            {
                &quot;sku_id&quot;: &quot;123-2&quot;,
                &quot;color&quot;: &quot;Blue&quot;,
                &quot;size&quot;: &quot;Medium&quot;
            },
            {
                &quot;sku_id&quot;: &quot;123-3&quot;,
                &quot;color&quot;: &quot;Blue&quot;,
                &quot;size&quot;: &quot;Large&quot;
            }
        ]
    },
    {
        &quot;product_id&quot;: &quot;456&quot;,
        &quot;title&quot;: &quot;Hoodie&quot;,
        &quot;variants&quot;: [
            {
                &quot;sku_id&quot;: &quot;456-1&quot;,
                &quot;color&quot;: &quot;Black&quot;,
                &quot;size&quot;: &quot;Small&quot;
            },
            {
                &quot;sku_id&quot;: &quot;456-2&quot;,
                &quot;color&quot;: &quot;Black&quot;,
                &quot;size&quot;: &quot;Medium&quot;
            },
            {
                &quot;sku_id&quot;: &quot;456-3&quot;,
                &quot;color&quot;: &quot;Black&quot;,
                &quot;size&quot;: &quot;Large&quot;
            }
        ]
    }
]
  
</code></pre>
<p>Is this too complex to handle with Azure Data Factory?</p>
<p>Any suggestion on how to approach this would be greatly appreciated.</p>
<p>Thank you,</p>
<p>Scott</p>
","<azure-data-factory>","2021-03-18 04:40:03","51","0","1","66685553","<p>This is a rough DSL script snippet for dataflow before you write it to cosmosdb</p>
<p>products aggregate(
groupBy(productId, title),
variants = collect(@(skuId = skuId, color = color, size = size))
) ~&gt; productAggregate</p>
"
"66636219","SSIS data flow package connected to KISAM ODBC database unable to export yesterdays changes","<p>I have an existing SSIS Package that is truncating SQL Server database tables before exporting all rows from selected tables in a KISAM database using KODBC 32bit drivers and a SQL Command in the data flow of:</p>
<pre><code>select * from table_a
select * from table_b
etc
</code></pre>
<p>For the largest table, this is over 3.3 million rows with over 100 columns.  But less than 5,000 rows will change on any given day.</p>
<p>Due to business requirements, I can only run the package between 1am and 4am.  I have a working test package that is returning the expected 5,000 rows using:</p>
<pre><code>Select * from table_a where table_a.date_column1=curdate()
</code></pre>
<p>That is giving me today's changed rows, but I need to be able to return yesterday's changed rows.</p>
<p>I have tried:</p>
<pre><code>Select * from table_a where table_a.date_column1=(curdate(), INTERVAL -1, DAY)
</code></pre>
<p>,but get a syntax error as I think this may be MySQL or other ODBC syntax.</p>
<p>What is the correct syntax to get yesterday for Kerridge/Kisam ODBC?</p>
","<ssis><odbc>","2021-03-15 10:28:21","387","0","1","66651496","<p>Use the <code>DATE_ADD()</code> function wich adds a time/date interval to a date and then returns the date :</p>
<pre><code> SELECT * FROM table_a WHERE table_a.date_column1=DATE_ADD(CURDATE(), INTERVAL -1 DAY)
</code></pre>
"
"66598433","How to dynamically convert source folder structure to delta table partition using ADF data flow?","<p>Source (CSV) - In <strong>myfile.csv</strong> I have country column that needs to be created as partition column in target side.</p>
<pre><code>/raw/myfolder/myfile.csv
</code></pre>
<p>Target (Delta)</p>
<pre><code>/raw/myfolder/country=&lt;value&gt;/delta_log
-----------------------------/part*.parquet
</code></pre>
<p>This seems possible using optimize partitions to use key partition type and this needs to be done MANUALLY. However, in my case <strong>I want to pass partition column (in this case country) as a parameter dynamically.</strong></p>
<p><a href=""https://i.stack.imgur.com/BC0Ae.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BC0Ae.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2021-03-12 10:45:15","203","0","1","66607075","<p>Since the key column has to be part of the schema <strong>before runtime</strong>, and schema can only be dynamically inferred <strong>at runtime</strong>, I don't think you can do this with partitioning by keys</p>
<p>You can, however, pass the key column in from the executing pipeline and create the folder structure that way, since you can specify the folder as a parameter.</p>
"
"66596435","Trying to run a data flow job in us central 1 region but source and target is in asia-south1","<p>Wanted to check on the similar error that is also mentioned in the post &quot;https://stackoverflow.com/questions/37298504/google-dataflow-job-and-bigquery-failing-on-different-regions?rq=1&quot;</p>
<p>I am facing a similar issue in my data flow job where I am getting error as below</p>
<pre><code>2021-03-10T06:02:26.115216545ZWorkflow failed. Causes: S01:Read File from GCS/Read+String To BigQuery Row+Write to BigQuery/NativeWrite failed., BigQuery import job &quot;dataflow_job_15712075439082970546-B&quot; failed., BigQuery job &quot;dataflow_job_15712075439082970546-B&quot; in project &quot;whr-asia-datalake-prod&quot; finished with error(s): errorResult: Cannot read and write in different locations: source: US, destination: asia-south1, error: Cannot read and write in different locations: source: US, destination: asia-south1
</code></pre>
<p>This is the error when I try to run the code using a cloud function trigger. Please find below the cloud function code. both my source data and target big query dataset resides in asia south 1</p>
<pre><code>&quot;&quot;&quot;
Google cloud funtion used for executing dataflow jobs.
&quot;&quot;&quot;

from googleapiclient.discovery import build
import time

def df_load_function(file, context):
    filesnames = [
        '5667788_OPTOUT_',
        'WHR_AD_EMAIL_CNSNT_RESP_'
    ]

    # Check the uploaded file and run related dataflow jobs.
    for i in filesnames:
        if 'inbound/{}'.format(i) in file['name']:
            print(&quot;Processing file: {filename}&quot;.format(filename=file['name']))

            project = '&lt;my project&gt;'
            inputfile = 'gs://&lt;my bucket&gt;/inbound/' + file['name']
            job = 'df_load_wave1_{}'.format(i)
            template = 'gs://&lt;my bucket&gt;/template/df_load_wave1_{}'.format(i)
            location = 'us-central1'
           
            dataflow = build('dataflow', 'v1b3', cache_discovery=False)
            request = dataflow.projects().locations().templates().launch(
                projectId=project,
                gcsPath=template,
                location=location,
                body={
                    'jobName': job,
                    &quot;environment&quot;: {
                    &quot;workerZone&quot;: &quot;us-central1-a&quot;
                }
                }
            )

            # Execute the dataflowjob
            response = request.execute()
            
            job_id = response[&quot;job&quot;][&quot;id&quot;]
</code></pre>
<p>I have kept both location and workerzone as us-central1 and us-central1-a respectively. I need to run my data flow job in us central 1 due to some resource issues but read and write data from asia-south1. what else do I need to add in cloud function so that region and zone are both us-central1 but data is read and written from asia south 1.</p>
<p>However, when I run my job manually using cloud shell using below commands it works fine and data is loaded. here both region and zone is us-central1</p>
<pre><code>python -m &lt;python script where the data is read from bucket and load big query&gt; \

--project &lt;my_project&gt; \

--region us-central1 \

--runner DataflowRunner \

--staging_location gs://&lt;bucket_name&gt;/staging \

--temp_location gs://&lt;bucket_name&gt;/temp \

--subnetwork https://www.googleapis.com/compute/v1/projects/whr-ios-network/regions/us-central1/subnetworks/&lt;subnetwork&gt; \

--network projects/whr-ios-network/global/networks/&lt;name&gt; \

--zone us-central1-a \

--save_main_session
</code></pre>
<p>Please help anyone. Have been struggling with this issue.</p>
","<google-bigquery><google-cloud-functions><google-cloud-dataflow>","2021-03-12 08:21:09","218","0","1","66634965","<p>I was able to fix the below error
&quot;2021-03-10T06:02:26.115216545ZWorkflow failed. Causes: S01:Read File from GCS/Read+String To BigQuery Row+Write to BigQuery/NativeWrite failed., BigQuery import job &quot;dataflow_job_15712075439082970546-B&quot; failed., BigQuery job &quot;dataflow_job_15712075439082970546-B&quot; in project &quot;whr-asia-datalake-prod&quot; finished with error(s): errorResult: Cannot read and write in different locations: source: US, destination: asia-south1, error: Cannot read and write in different locations: source: US, destination: asia-south1&quot;</p>
<p>I just changed by cloud function to add the temp location of my asia-south bucket location because though I was providing the tmp location as of asia-south1 while creating the template, bigquery IO in my data flow job was trying to use the temp location of us-central1 and not asia-south1 and hence the above error.</p>
"
"66581210","Need to access storage bucket of project B in project A through a data flow job in google cloud (data flow job failing)","<p>I am running a data flow job in project A where I have a requirement of accessing storage bucket in project B.  I have requested the admin to add the service accounts of project A in project B and have given the required permission
As you can see below, I have provided the screen shot of project B whose storage bucket I need to access in project A and run data flow in project A and load BIG QUERY table in project B.</p>
<p>Project A service account (as highlighted by A) as shown is added in project B and given big query admin and storage admin role.</p>
<p>Data flow runner service account is also added as you can see but looks like admin provided big query admin and storage admin role there and not compute network user. Not sure if I need to add data flow runner service account of A in B but I am getting below error while running the data flow job.</p>
<p>It mainly says &quot;-compute@developer.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.&quot;,\n &quot;domain&quot;: &quot;global&quot;,\n &quot;reason&quot;: &quot;forbidden&quot;\n }\n ]\n }\n}\n&gt;')}')}&quot;</p>
<p>I have tried to provide project name as A AND B in the screen shot below for better understanding. Was not sure of any better way to explain.</p>
<p>Do I need to add .compute@developer.gserviceaccount.com of A in project B too??
Please advise.</p>
<p><a href=""https://i.stack.imgur.com/gNCFu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gNCFu.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/yn92r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yn92r.png"" alt=""enter image description here"" /></a></p>
","<google-cloud-platform><google-cloud-dataflow><google-cloud-iam>","2021-03-11 11:01:15","568","0","1","66586138","<p>When you run a dataflow job, you have workers. If you have a closer look to your project, worker are simple Compute Engine. And when you deploy compute engine, by default, the service account used is -compute@ . Thus it's this identity which try to access to your different component (GCS and BigQuery here).</p>
<p>So, grant the required permission on the correct resources (don't do this at project level, it's better to do this at bucket or dataset level. If you don't know how to do, let me know).</p>
<p>The other solution is to specify a custom service account at runtime in your Dataflow job to create workers, not with the Compute Engine default service account, but with the provided one. You can do this with the <a href=""https://cloud.google.com/sdk/gcloud/reference/dataflow/jobs/run#--service-account-email"" rel=""nofollow noreferrer"">gcloud CLI for example</a></p>
<hr />
<p>About Dataflow permission, you can find more details <a href=""https://cloud.google.com/dataflow/docs/concepts/security-and-permissions"" rel=""nofollow noreferrer"">here</a></p>
"
"66551218","Is it possible to merge Azure Data Factory data flows","<p>I have two separate Data flows in Azure Data Factory, and I want to combine them into a single Data flow.</p>
<p>There is a technique for copying elements from one Data flow to another, as described in this video: <a href=""https://www.youtube.com/watch?v=3_1I4XdoBKQ"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=3_1I4XdoBKQ</a></p>
<p>This does not work for Source or Sink stages, though. The Script elements do not contain the Dataset that the Source or Sink is connected to, and if you try to copy them, the designer window closes and the Data flow is corrupted. The details are in the JSON, but I have tried copying and pasting into the JSON and that doesn't work either - the source appears on the canvas, but is not usable.</p>
<p>Does anyone know if there is a technique for doing this, other than just manually recreating the objects on the canvas?</p>
","<azure-data-factory>","2021-03-09 16:46:29","313","0","1","66566462","<p>Thanks Leon for confirming that this isn't supported, here is my workaround process.</p>
<ol>
<li>Open the Data Flow that will receive the merged code.</li>
<li>Open the Data Flow that contains the code to merge in.</li>
<li>Go through the to-be-merged flow and change the names of any transformations that clash with the names of transformations in the target flow.</li>
<li>Manually create, in the target flow, any Sources that did not already exist.</li>
<li>Copy the entire script out of the to-be-merged flow into a text editor.</li>
<li>Remove the Sources and Sinks.</li>
<li>Copy the remaining transformations into the clipboard, and paste them in to the target flow's script editor.</li>
<li>Manually create the Sinks, remembering to set all properties such as &quot;Allow Update&quot;.</li>
</ol>
<p>Be prepared that, if you make a mistake and paste in something that is not correct, then the flow editor window will close and the flow will be unusable. The only way to recover it is to refresh and discard all changes since you last published, so don't do this if you have other unpublished changes that you don't want to lose!</p>
<p>I have already established a practice in our team that no mappings are done in Sinks. All mappings are done in Derived Column transformations, and any column name ambiguity is resolved in a Select transformations, so the Sink is always just auto-map. That makes operations like this simpler.</p>
<p>It should be possible to keep the Source definitions in Step 6, remove the Source elements from the target script, and paste the new Sources in to replace them, but that's a little more complex and error-prone.</p>
"
"66542187","How to pass LinkedService name to data flow from ADF pipeline?","<p>I am working on a migration project where the source data resides in 4 different ADLS Gen 2 account and needs to be migrated to some other data lake.
For this, I have created 4 LinkedService to connect to these source Gen 2 account.</p>
<p>Now I want to pass LinkedService name to DataFlow at run time depending on the data source I want to run data flow for, to connect to respective ADLS Gen2 account.</p>
<p>I am able to pass other parameter from pipeline to dataflow. But passing LinkedService name is not working.</p>
<p><a href=""https://i.stack.imgur.com/psWGW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/psWGW.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2021-03-09 06:58:39","86","0","1","66544519","<p>I'm afraid to say it's not supported.</p>
<p>Data Factory doesn't support pass parameter(pipeline parameter or Data Flow parameter) to linked service. Like you said, We can't pass parameter from Data Flow to linked service.</p>
<p>HTH.</p>
"
"66429258","SSIS Cache Mamnager","<p>Is it possible to use an SSIS Cache manger with anything other than a Lookup?  I would like to use similar data across multiple data flows.</p>
<p>I haven't been able to find a way to cache this data in memory in a cache manager and then reuse it in a later flow.</p>
","<ssis><data-integration>","2021-03-01 20:33:46","115","0","1","66429889","<p>Nope, a cache connection manager was specific to solving lookup tasks originally only allowing an OLE DB Connection to be used.</p>
<p>However, if you have a set of data you want to be static for the life of a package run and able to be used across data flows, or even other packages, as a table-like entity, perhaps you're looking for a <a href=""https://www.timmitchell.net/post/2019/09/03/using-raw-files-in-ssis/"" rel=""nofollow noreferrer"">Raw File</a>. It's a tight, binary implementation of the data stored to disk. Since it's stored to disk, you will pay a write and subsequent read performance penalty but it's likely that the files are right sized such that any penalty is offset by the specific needs.</p>
<p>The first step you will need to do is define the data that will go into a Raw file and connect a <a href=""https://learn.microsoft.com/en-us/sql/integration-services/data-flow/raw-file-destination?view=sql-server-ver15"" rel=""nofollow noreferrer"">Raw File Destination</a>. Which is going to involve creating a Raw File Connection Manager where you will define where the file lives and the rules about the data in there (recreate, append, etc). At this point, run the data flow task so the file is created and populated.</p>
<p>The next step is everywhere you want to use the data, you'll patch in a <a href=""https://learn.microsoft.com/en-us/sql/integration-services/data-flow/raw-file-source?view=sql-server-ver15"" rel=""nofollow noreferrer"">Raw File Source</a>. It's going to behave much as any other data source in your toolkit at this point.</p>
"
"66376195","Is the flow of data inside the data flow tasks synchronous?","<p>In Data flow task suppose I have source, couple of transforms and destination.</p>
<p>Say there are 1 million records to be read by source. Say it has reached row number 10000. Will the read rows (10000) get passed to the next transform or will the subsequent tasks wait for the previous task to completely process the rows? So only run transform when all 1 million have been read.</p>
","<ssis>","2021-02-25 20:46:19","99","1","1","66376803","<p>It depends!</p>
<p>Quick definitions:</p>
<ul>
<li>Synchronous: One row in, one row out. Input lineage id is the same as output lineage id</li>
<li>Asynchronous: N row(s) in, M row(s) out.</li>
<li>Async, Fully blocking: All data must arrive before new data can leave</li>
<li>Async, Partially blocking: Enough data must arrive before new data can leave</li>
</ul>
<h2>All synchronous</h2>
<pre><code>OLE DB Source -&gt; Derived Column -&gt; OLE DB Destination
</code></pre>
<p>All synchronous components. 1M source rows, 10k rows flow from source, to column to destination. Lather-rinse-repeat</p>
<h2>Asynchronous, fully blocking</h2>
<pre><code>OLE DB Source -&gt; 
                 Aggregate -&gt; OLE DB Destination
</code></pre>
<p>Aggregate is an asynchronous, fully-blocking, component. 1M source rows, 10k rows flow from source to Aggregate (let's assume we're getting the Maximum sales grouped by sales id). It computes the maximum amount for the 10k it has, but it can't release them downstream because the 10k+1 row might have a larger value so it holds and stores the values until it has received an end of buffer signal from the source.</p>
<p>Only then, can the Aggregate component release the results to the downstream consumers of data.</p>
<p>I show the Aggregate not being &quot;in line&quot; with the source because at this point in a data flow, there is a rift between the data before and the data after. If it had been Source -&gt; Derived column -&gt; Aggregate, the Derived component is going to work on the same memory address (yay pointers) as the Source. Asynchronous components do an in memory copy of the data into a separate memory space. So, instead of being able to allocate 1GB to your data flow, it has to spend .5GB to the first half and .5 to the last half.</p>
<p>If you rename a column &quot;upstream&quot; from an asynchronous component, you can tell where the &quot;break&quot; in data lineage is because that column won't be available to the final destination until you modify all the async components between the source to the destination.</p>
<h2>Asynchronous, partially blocking</h2>
<pre><code>OLE Source DB 1 --&gt;
                    Merge Join -&gt; OLE DB Destination
OLE Source DB 2 --&gt;
</code></pre>
<p>Merge join is an asynchronous, partially blocking component. You can usually tell the partially blocking components as they have a requirement of Sorted input. For an aggregate, we have to have <strong>all</strong> the data before we can say <em>this</em> is the maximum value.  With a merge join, since we know that both streams are sorted on key, we can release once the match key is out of matches. Assume I have the merge in an INNER JOIN configuration. If I have the rows with value of <code>A,B,C</code> from db1 feed and <code>A,C</code> from db2. While <code>A</code> matches <code>A</code>, we'll release rows to the destination. We exhaust As and go to the next. Source 1 provides <code>B</code>, Source 2 provides <code>C</code>. They don't match so <code>B</code> is discarded and the next Source 1 is retrieved. <code>C</code> matches so it continues on.</p>
<h2>It Depends(again)</h2>
<pre><code>OLE DB Source -&gt; Script Component -&gt; OLE DB Destination
OLE DB Source -&gt; 
                 Script Component -&gt; OLE DB Destination
</code></pre>
<p>A script component operates the way you define it. The default is synchronous but you can make it async.</p>
<p>Jorg has a good table flagging the components into the various buckets: <a href=""https://jorgklein.com/2008/02/28/ssis-non-blocking-semi-blocking-and-fully-blocking-components/comment-page-2/"" rel=""nofollow noreferrer"">https://jorgklein.com/2008/02/28/ssis-non-blocking-semi-blocking-and-fully-blocking-components/comment-page-2/</a></p>
<h2>The comment asks &quot;What about a lookup transform?&quot;</h2>
<p>As the referenced article shows, Lookup is in Synchronous column. However, if one if looking for performance bottlenecks (async components are usually the first place I look), we often point out that the default lookup will cache all data in the table in the PreExecute phase. If your reference table has 10, 100, 1000000 rows, who cares. However long it takes to run <code>SELECT * FROM MyTable</code> and stream that from the database source to the machine SSIS is running on is the performance penalty you pay.</p>
<p>However, if you work at a mutual fund company and have a trade table that records prices for all of your stocks for all time, maybe <em>don't</em> try and pull that data back for a lookup transform, hypothetically speaking of course. Maybe you only needed to get the most recent settlement price so don't be lazy and pull more data than you'll ever need and/or crash the machine.</p>
"
"66282937","Exclamation sign in data flow execution","<p>Does anyone know what the exclamation sign means next to data flow execution?</p>
<p>However, main execution result is ok.</p>
<p>I appreciate your help in advance.</p>
<p>See screenshot</p>
<p><a href=""https://i.stack.imgur.com/2AGa6.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","<azure><azure-data-factory><dataflow>","2021-02-19 18:17:30","39","0","1","66311730","<p>There are no documents talk about the icon. But if the pipeline/data flow execute state is &quot;Succeeded&quot;, it will be &quot;√&quot; icon.</p>
<p><a href=""https://i.stack.imgur.com/IyqPm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IyqPm.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/abwDt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/abwDt.png"" alt=""enter image description here"" /></a></p>
<p>Not very sure why you get the exclamation icon. Just from your screenshot, it seams that you pipeline/data flow works well.</p>
<p>Maybe the exclamation icon it to mention you that your steps could be optimized.</p>
<p>If your want to get exactly meaning of the icon, the best way is that ask <a href=""https://learn.microsoft.com/en-us/azure/azure-portal/supportability/how-to-create-azure-support-request"" rel=""nofollow noreferrer"">Azure support</a>. It takes some money and time to get the answer.</p>
<p>As normal users, If the pipeline/data flow works well, all the data is transferred correctly, we could ignore it.</p>
<p>HTH.</p>
"
"66209622","Task execution is not working after lunching the task in spring cloud data flow","<p>I have created one Spring boot application with <code>@EnablesTask</code> annotation and try to print the arguments in log.</p>
<pre><code>package com.custom.samplejob;

import org.springframework.boot.CommandLineRunner;
import org.springframework.cloud.task.configuration.EnableTask;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
@EnableTask
public class TaskConfiguration {

    @Bean
    public CommandLineRunner commandLineRunner() {
        return args -&gt; {
            System.out.println(args);
        };
    }
}
</code></pre>
<p>After I have run that <code>mvn clean install</code> to have the jar in local maven repo.</p>
<pre><code>com.custom:samplejob:0.0.1-SNAPSHOT
</code></pre>
<p>Using custom docker-compose to run spring cloud data flow locally on windows using the below parameters</p>
<pre><code>set HOST_MOUNT_PATH=C:\Users\user\.m2 (Local maven repository mounting)

set DOCKER_MOUNT_PATH=/root/.m2/

set DATAFLOW_VERSION=2.7.1

set SKIPPER_VERSION=2.6.1

docker-compose up
</code></pre>
<p>Using the below commend to register the app</p>
<pre><code>app register --type task --name custom-task-trail-1 --uri maven://com.custom:samplejob:0.0.1-SNAPSHOT
</code></pre>
<p>Created task using UI(below URL) and lunch the task. Task was successfully launched.</p>
<pre><code>http://localhost:9393/dashboard/#/tasks-jobs/tasks
</code></pre>
<p>These are the logs I can see in the docker-compose up terminal,</p>
<pre><code>dataflow-server    | 2021-02-15 13:20:41.673  INFO 1 --- [nio-9393-exec-9] o.s.c.d.spi.local.LocalTaskLauncher      : Preparing to run an application from com.custom:samplejob:jar:0.0.1-SNAPSHOT. This may take some time if the artifact must be downloaded from a remote host.
dataflow-server    | 2021-02-15 13:20:41.693  INFO 1 --- [nio-9393-exec-9] o.s.c.d.spi.local.LocalTaskLauncher      : Command to be executed: /usr/lib/jvm/jre-11.0.8/bin/java -jar /root/.m2/repository/com/custom/samplejob/0.0.1-SNAPSHOT/samplejob-0.0.1-SNAPSHOT.jar --name=dsdsds --spring.cloud.task.executionid=38
dataflow-server    | 2021-02-15 13:20:41.702  INFO 1 --- [nio-9393-exec-9] o.s.c.d.spi.local.LocalTaskLauncher      : launching task custom-task-trail-1-48794885-9a0a-4c46-a2a1-299bf91763ad
dataflow-server    |    Logs will be in /tmp/4921907601400/custom-task-trail-1-48794885-9a0a-4c46-a2a1-299bf91763ad
</code></pre>
<p>But in task execution list, it's doesn't show the status and start date and end date of that task executions,</p>
<p><a href=""https://i.stack.imgur.com/A6dql.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A6dql.png"" alt=""enter image description here"" /></a></p>
<p>can some one help me to resolve this or am I missing anything here in local installation or task spring boot implementation wise?</p>
","<spring-boot><docker-compose><spring-batch><spring-cloud-dataflow><spring-cloud-task>","2021-02-15 14:14:10","647","2","1","66258761","<p>I have enabled kubernetes on docker desktop and installed spring data flow server top of that.
And I tried with docker uri to register app and generate docker image using the jib-maven-plugin.</p>
<p>Now its works the sample task application in my case.</p>
"
"66170921","Google cloud build python apache beam data flow yaml file","<blockquote>
<p>I am trying to deploy an apache beam Data Flow pipeline built-in python in google cloud build. I don't find any specific details about constructing the cloud build.YAML file.</p>
</blockquote>
<p>I found a link <a href=""https://medium.com/@zhongchen/dataflow-ci-cd-with-cloudbuild-1ad503c1c81"" rel=""nofollow noreferrer"">dataflow-ci-cd-with-cloudbuild</a>, but this seems JAVA based, tried this too but did not work as my starting point is main.py</p>
<p><a href=""https://i.stack.imgur.com/Uf7TU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Uf7TU.png"" alt=""enter image description here"" /></a></p>
","<google-cloud-dataflow><apache-beam><google-cloud-build><cloudbuild.yaml>","2021-02-12 11:19:10","785","0","1","66196426","<blockquote>
<p>It requires a container registry. Step to build &amp; deploy is explained in the below link</p>
</blockquote>
<p><a href=""https://github.com/vivekkranjan/docker-dataflowrunner-python3"" rel=""nofollow noreferrer"">Github link</a></p>
"
"66122263","Xamarin IOS needs to redirect to BB Access for OpenID connection","<p>A client wants to open the BB Access browser when logging in with OpenID (They are using Azure) and i'm having trouble finding docs about this specific issue.</p>
<p>We only want to open BB Access as a browser and let our backend handle the rest of the stuff(Authentication scheme, tokens, etc...). We have no problem on android since we can setup BB Access app as a default browser but it is not possible to do so on IOS.</p>
<p>What is the best way to proceed to resolve this issue? Do I need to implement the whole Dynamics SDK within my app just to open the browser, is there examples of what i'm trying to do anywhere or is there an easier method to do so?</p>
","<xamarin.ios><blackberry><mdm><blackberry-dynamics>","2021-02-09 15:42:45","125","1","1","66123738","<p>For BlackBerry Access for iOS, links in third-party, non-BlackBerry Dynamics apps can open in BlackBerry Access if they launch with the following URL scheme: access://open?url= (for example, access://open?url=http://www.blackberry.com).  This does require the UEM Admin to enable the &quot;Allow external apps to open HTTP/HTTPS URLs through BlackBerry Access&quot; app policy for BlackBerry Access.</p>
<p>If your app makes use of a custom URL scheme to get the token back from BlackBerry Access, ensure you also enable that in the BlackBerry Access app policy.  The <a href=""https://docs.blackberry.com/en/blackberry-dynamics-apps/blackberry-access/3_1/blackberry-access-administration-guide/lnb1484165962504/ndf1494880538789/bcp1488469362943"" rel=""nofollow noreferrer"">BlackBerry Access app configuration settings doc</a> lists all of these app configuration options.  Specifically, refer to the &quot;Enable 3rd Party Applications&quot; option.</p>
<p>Alternatively, if you do integrate with the BlackBerry Dynamics SDK you could open BlackBerry Access using the <a href=""https://marketplace.blackberry.com/services/795018"" rel=""nofollow noreferrer"">Open HTTP URL Service</a>.</p>
"
"66116313","How to check column that contain letter and number in Talend","<p>My columns must contains <strong>2 letter</strong> and <strong>4 number</strong> like this (AV1234)</p>
<p>How can i check this ?</p>
","<talend><multiplication><division><tmap><data-integration>","2021-02-09 09:37:16","678","0","2","66116545","<p>You can use sql templates as mentioned in talend documentation <a href=""https://help.talend.com/r/JdTBzKszzXoWvjpEJD3EBA/TDxH0u3RnBV0zc3ecpoTuA"" rel=""nofollow noreferrer"">here</a> and you can check your column that contain letter and number using regular expressions.</p>
<p>Use this <code>[a-zA-Z]{2}[0-9]{6}</code></p>
<p>Use this If you want only uppercase letters <code>[A-Z]{2}[0-9]{6}</code></p>
<pre><code>[a-zA-Z]    # Match a single character present in the list below
            # A character in the range between “a” and “z”
            # A character in the range between “A” and “Z”
  {2}       # Exactly 2 times
[0-9]       # Match a single character in the range between “0” and “9”
  {6}       # Exactly 6 times

</code></pre>
"
"66116313","How to check column that contain letter and number in Talend","<p>My columns must contains <strong>2 letter</strong> and <strong>4 number</strong> like this (AV1234)</p>
<p>How can i check this ?</p>
","<talend><multiplication><division><tmap><data-integration>","2021-02-09 09:37:16","678","0","2","66117194","<p>Thank you for your answer ! <strong>it Works</strong></p>
<p><strong>My routine code:</strong></p>
<pre><code>public static Boolean MyPattern(String str) {


String stringPattern = &quot;[A-Z]{2}[0-9]{4}&quot;; 

boolean match = Pattern.matches(stringPattern, str);


return match ;

  }
</code></pre>
"
"66112647","Register App in spring cloud data flow from docker hub when there is no internet","<p>How to get application Properties on registering app in spring cloud data flow from docker hub when there is no internet</p>
","<docker-registry><spring-cloud-dataflow>","2021-02-09 03:40:53","190","-1","1","66122857","<p>For air-gapped environments, you may want to pull the images on the connected machine and then relocate them to the air-gapped boxes that have a private registry in them. There are a few tools out there that can help you with the relocation.</p>
<p>For SCDF on K8s, we ship the <a href=""https://docs.pivotal.io/scdf-k8s/1-2/relocating-ootb-apps.html"" rel=""nofollow noreferrer"">relocation tool</a> to help with it, as well.</p>
"
"66107090","Amazon S3 to SSIS data flow","<p>I'm trying to download a file from amazon S3 with a particular file name via SSIS</p>
<p>Can anyone please suggest me which transformation will help me do that , Below are the details I have in hand from Amazon s3</p>
<ol>
<li>access_key_id</li>
<li>Secret_access_key</li>
<li>Bucket_name</li>
<li>region</li>
</ol>
<p>sorry cannot share values right now.</p>
","<amazon-s3><ssis>","2021-02-08 18:13:31","3965","0","1","66115947","<p><strong>Amazon S3 configuration :</strong></p>
<p>By default, the SSIS package does not allow you to connect with the AWS S3 bucket. It does not mean that we do have any mechanism to do so.
But you can browse Visual studio Marketplace and use <a href=""https://marketplace.visualstudio.com/items?itemName=KingswaySoft.ssisamazons3"" rel=""nofollow noreferrer"">Amazon S3 SSIS Components</a> (SSIS Productivity Pack).
It enables SSIS package and Amazon S3 SSIS bucket integration without writing any specific code. You can easily use this SSIS productivity pack to download complete the task effectively and efficiently.
Download the appropriate 64-bit or 32-bit. Double-click on the installer and follow up the installation wizard for installing SSIS productivity pack with default configurations.</p>
<p><strong>S3 bucket connection :</strong></p>
<p>The first step is to create a connection pointing to the Amazon S3 bucket. Right-click on the connection manager and choose New connection:</p>
<p>Create a connection pointing to the Amazon S3 bucket. Right-click on the connection manager and choose <code>New connection</code>.</p>
<p>You need to connect with the <code>AWS S3 bucket</code> so select the <code>Amazon S3</code> and click on Add. It opens the <code>Amazon S3 Connection Manager</code>.</p>
<p>In General page of the connection manager, you need to specify connection properties and login credentials to the AWS S3 bucket:</p>
<p>You need to store the <code>access key</code> and <code>secret key</code> in a secured location. Specify it in the Amazon S3 Connection Manager. It authenticates user account in Amazon services using these keys, and you can see the bucket name in the drop-down list :</p>
<p><a href=""https://i.stack.imgur.com/F7pvC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F7pvC.png"" alt=""enter image description here"" /></a></p>
<p><strong>Add a Data Flow task :</strong>
Add a data flow task in the SSIS package for the Amazon S3 SSIS bucket.</p>
<p>Double-click on the task, and it takes you to the data flow screen. In the SSIS toolbox on the data flow tab, you can see options for Amazon S3 bucket:</p>
<ul>
<li>Amazon S3 Destination</li>
<li>Amazon S3 Source</li>
<li>Premium Flat File source</li>
<li>Premium Flat File destination</li>
</ul>
<p>Drag <code>Premium Flat File Source</code> in the data flow task from the SSIS toolbox:</p>
<p><a href=""https://i.stack.imgur.com/5jffw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5jffw.png"" alt=""enter image description here"" /></a></p>
<p>Double-click on Premium Flat File Source and it opens the editor window for configuration:</p>
<ul>
<li><p>Connection Manager: Select the existing connection to the S3 bucket from the drop-down. If you do not have an existing connection to the S3 bucket, click on New. You require specifying keys (access and security key) information, as shown in previous steps.</p>
</li>
<li><p>Source File Path: It is the CSV file path in the S3 bucket. Click on eclipse and browse to the CSV file path. You might have multiple files in the S3 bucket. The source file path should point to a valid CSV file.</p>
</li>
</ul>
<p>Click on eclipse (…) on the source file path and browse to the CSV file path:</p>
<p><a href=""https://i.stack.imgur.com/MaOf0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MaOf0.png"" alt=""enter image description here"" /></a></p>
<p>You can see the CSV file in the Source item path column.</p>
<ul>
<li><p>The first row has the header: My CSV file first column contains column headers. Remove the check from here if the CSV file does not contains headers</p>
</li>
<li><p>Skip Empty Rows: CSV file might contain empty rows. Put a check on this to skip the empty rows</p>
</li>
</ul>
<p><a href=""https://i.stack.imgur.com/GE40O.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GE40O.png"" alt=""enter image description here"" /></a></p>
<p>Click on columns tab to view excel file columns. You can uncheck the columns if you do not want to import them into SQL Server tables. It also shows the data type and length automatically.</p>
<p>You can change the column data types and their properties in this window. For this demonstration, let’s go with default columns properties in the Amazon S3 SSIS package:</p>
<p><a href=""https://i.stack.imgur.com/LL2VK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LL2VK.png"" alt=""enter image description here"" /></a></p>
<p>Click OK and add an <code>OLE DB destination connection</code>. This <code>OLE DB destination</code> should point to the <code>SQL Server instance</code> for example. Right-click and configure the destination with the <code>SQL Server connection</code> and <code>SQL Server table</code>. You also need to do the mapping between source and destination columns:</p>
<p><a href=""https://i.stack.imgur.com/j6fHt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j6fHt.png"" alt=""enter image description here"" /></a></p>
<p>Once the <code>OLE DB destination</code> connection is successful, you can see the package as shown below:</p>
<p><a href=""https://i.stack.imgur.com/0KOKy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0KOKy.png"" alt=""enter image description here"" /></a></p>
<p>Execute the SSIS package, and it transfers 100 rows from the source CSV file to SQL Server table.</p>
<p><a href=""https://i.stack.imgur.com/v89CT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v89CT.png"" alt=""enter image description here"" /></a></p>
"
"66049596","Update another table when SSIS data flow finishes","<p>I have a simple package where I pull down every table from a remote source DB into my local server DB.<br />
Every data flow task is simple source to destination.<br />
The problem I have is occasionally the download will stop and I won't get all the tables, or sometimes tables won't fully pull down all data.</p>
<p>All I want to do is have a table with all table names that I need to pull down.</p>
<p>After each table in my data flow completes I need to update a flag in my new table of table names so that there is a 1 for every table that fully downloads from the source to the destination.  Any help would be appreciated.</p>
","<sql><ssis>","2021-02-04 16:18:42","137","0","1","66052982","<p>A simple approach is to add an <code>Execute SQL Task</code> after each <code>Data Flow Task</code> that updates the table containing a <code>LastOperationDate</code> which will have the last execution time.</p>
<pre><code>UPDATE AuditTable
SET  LastExecutionDate=GETDATE()
WHERE TableName LIKE @param
</code></pre>
<p>The @param is the name of the table in the data destination of the concerned data flow.</p>
"
"66046328","How to get the name of mapping data flow inside the mapping data flow?","<p>I want to get the names of running Mapping data flow in order to track the error details when any pipeline or Data flow fails just same like we track for running pipelines in Azure Data Factory ?</p>
<p>How we can achive this ?</p>
","<azure-data-factory>","2021-02-04 13:09:58","277","1","1","66056996","<ol>
<li><p>We can declare a variable <code>FlowName</code>, type in the data flow name, here is <code>myDataflow</code>.
<a href=""https://i.stack.imgur.com/zfrYA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zfrYA.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Inside the data flow, we can declare a parameter named <code>parameter1</code> and temporarily assign double quotes.
<a href=""https://i.stack.imgur.com/u4Wp0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u4Wp0.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Then outside the data flow, we can see <code>paramter1</code> , select <code>Pipeline expression</code>, add dynamic content <code>@variables('FlowName')</code>.
<a href=""https://i.stack.imgur.com/vtumk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vtumk.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Inside the data flow, <code>paratmer1</code> will be assigned the value <code>myDataflow</code> at runtime. So we can use <code>DerivedColumn1</code> activity to generate a new column with the value <code>myDataflow</code>.
<a href=""https://i.stack.imgur.com/VIUG5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VIUG5.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>There may be problems with the data preview, but the debugging result is correct. Because this column will assigned at runtime.<br />
<a href=""https://i.stack.imgur.com/qOLF6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qOLF6.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>
<p>The data preview is as follows, the column is null value:
<a href=""https://i.stack.imgur.com/xDFVG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xDFVG.png"" alt=""enter image description here"" /></a></p>
"
"66038559","Azure Data Factory - Can we use local variables inside derived column in the data flow?","<p>Experts,</p>
<p>I have a simple requirement where I need to store the value of a variable inside a column in my destination SQL Table (Sink).</p>
<p>Here's what I am doing:</p>
<ol>
<li>Created a new pipeline and created a variable &quot;createDate&quot; with value &quot;@utcNow&quot;</li>
<li>I created a &quot;data flow&quot; task, where I configured my source (a simple CSV file) and added a &quot;Derived Column&quot; task because I want to store the date when the data was loaded in to my destination table</li>
<li>In my &quot;Derived Column&quot; task I added a new column &quot;der_createDate&quot;, BUT, i don't know how can I assign the value of the &quot;createDate&quot; variable to this derived column, I tried several expressions like - @variables('createDate'), but, the expression validation fails.</li>
</ol>
<p>How can I use the value of a variable created in the pipeline in any of &quot;Data Flows&quot;? Is it even possible? I have seen several use cases of variables in iterables or even in the &quot;Copy Data&quot; task, but, I am using Data Flows and would want to refer to those local variables i created in my pipeline. Let me know what you'd suggest.</p>
","<azure><variables><azure-data-factory>","2021-02-04 02:30:26","1395","0","2","66040045","<p>Variables are not supported in dataflow. You have to pass the value of this variable as a parameter(createDate) into dataflow and reference it as $createDate</p>
"
"66038559","Azure Data Factory - Can we use local variables inside derived column in the data flow?","<p>Experts,</p>
<p>I have a simple requirement where I need to store the value of a variable inside a column in my destination SQL Table (Sink).</p>
<p>Here's what I am doing:</p>
<ol>
<li>Created a new pipeline and created a variable &quot;createDate&quot; with value &quot;@utcNow&quot;</li>
<li>I created a &quot;data flow&quot; task, where I configured my source (a simple CSV file) and added a &quot;Derived Column&quot; task because I want to store the date when the data was loaded in to my destination table</li>
<li>In my &quot;Derived Column&quot; task I added a new column &quot;der_createDate&quot;, BUT, i don't know how can I assign the value of the &quot;createDate&quot; variable to this derived column, I tried several expressions like - @variables('createDate'), but, the expression validation fails.</li>
</ol>
<p>How can I use the value of a variable created in the pipeline in any of &quot;Data Flows&quot;? Is it even possible? I have seen several use cases of variables in iterables or even in the &quot;Copy Data&quot; task, but, I am using Data Flows and would want to refer to those local variables i created in my pipeline. Let me know what you'd suggest.</p>
","<azure><variables><azure-data-factory>","2021-02-04 02:30:26","1395","0","2","66040106","<ol>
<li>We can declare a variable <code>createDate</code> in the pipeline.
<a href=""https://i.stack.imgur.com/ptN3y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ptN3y.png"" alt=""enter image description here"" /></a></li>
</ol>
<p>2.Inside the dataflow we can declare a parameter <code>parameter1</code> and temporarily assign double quotes.
<a href=""https://i.stack.imgur.com/sFkjy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sFkjy.png"" alt=""enter image description here"" /></a></p>
<ol start=""3"">
<li><p>Then in the pipeline, select <code>Pipeline expression</code>, add dynamic value <code>@variables('createDate')</code>. So we can assign value of <code>createDate</code> to <code>parameter1</code>.
<a href=""https://i.stack.imgur.com/Qt1L0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qt1L0.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>In <code>DerivedColumn1</code> activity, we can generate a new column with the value of <code>parameter1</code>. There may be problems with the data preview, but the debugging result is correct. Because this column will assigned at runtime.
<a href=""https://i.stack.imgur.com/mDHDh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mDHDh.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>The debugging result is as follows:
<a href=""https://i.stack.imgur.com/FFbBR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FFbBR.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>
"
"66030323","SSIS How to pass variable/row count from one data flow process to another in the same control flow","<p>Sorry I am new to SSIS. Using Visual Studio 2017.  In my SSIS package, in my Control Flow screen, I have just one Data Flow Task rectangle, that I labeled Appointments. When you click on that, you are taken to the Data Flow tab, where really everything happens here. I have two different processes or paths here. So for Path 1, I have a OLE DB Source. This runs a SQLSelect statement that pulls data from some tables off a SQL Server.  Then there is a green connector from this to a Row Count transformation rectangle, and then there is a green connector from that to a Flat File Destination rectangle. The Flat File destination is set up to get the output from the SQL statement, create a file on my hard drive, and put the results of the Select statement into that file. I call this file a “data” file. That is it for this process. My hope is to somehow save the record count of the Select statement. Now, on the same tab, I have another process we will call Path 2. I have an OLE DB Source. In this source, I have a SQL Select statement that just creates one record with 6 fields. All the fields are hard coded except one. This field is called RecordCount. So what needs to dynamically go into this field, is the row count from Path 1 (for now, I just have it hard coded to zero). I have a green connector from this OLE DB Source that goes to a Flat File Destination rectangle. This destination is set up to get the output from the (mostly) hard coded SQL statement, create a file on my hard drive, and put that one record into it. I call this file a “control” file. I have 3 connection managers created, one that connects to the SQL Server, and one for each Path on this tab. How can I get the row count from the first Path into the field in the file created by the second Path? I have tried several possibilities and have googled until I am blue in the face, but I cannot get anything to work. Being a newbie, I hesitate to tell you what all I have tried – so feel free to suggest anything, assume I know nothing. Keep in mind that I may in the future create a Path 3 and a Path 4 on the same tab, where the row count from Path 3 goes to Path 4 (don’t want to be using here the rowcount value from Path 1) – how will this affect things? Or would I need Path 3 and Path 4 to be related to a new Data Flow Task on the Control Flow screen?</p>
<p><img src=""https://i.stack.imgur.com/QBa9B.png"" alt=""error"" /></p>
","<variables><ssis><rowcount>","2021-02-03 15:12:36","1079","0","1","66031507","<p>The Row Count transformation requires an SSIS Variable. Double click it and see what value you put in there, something like <code>@User::Variable</code></p>
<p>When the data flow finishes, the value of that variable will contain the number of rows that passed through the component.</p>
<p>That is all that you want in your data flow. The secondary path you describe, you will want to add that to a <em>new</em> data flow. Draw an On Completion arrow from Data Flow 1 to Data Flow 2. And the same for your future steps 3 and 4. Make the Dataflow focus on solving a single business problem (Export Sales to Text file).</p>
<p>Now that the first data flow has completed, the value of the SSIS variable will contain a value so you're going to use that in Data Flow 2 to write your one row of data to a text file.</p>
<p>In your OLE DB Source, you'll use a parameter like</p>
<pre><code>DECLARE @RowCount int = ?;
SELECT GETDATE() AS CurrentDate, 'SalesExport' AS ExportType, @RowCount AS ExportCount;
</code></pre>
<p>That <code>?</code> is the placeholder for an OLE DB Parameter and if you click the Parameters button, you'll be able to specify that parameter <code>0</code> maps to <code>@User::Variable</code></p>
<p><a href=""https://i.stack.imgur.com/7tG9K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7tG9K.png"" alt=""enter image description here"" /></a></p>
<p>When that runs, you'll get a flat file with your one row of data and the actual value computed in data flow 1.</p>
<p>Lather, rinse, repeat the pattern for your exports. Do create an SSIS variable for each row count you need to keep track of so <code>@User::CountSales</code>, <code>@User::CountConsumers</code>, <code>@User::CountProviders</code> so that way you don't run into trouble later by re-using a variable before you were ready.</p>
"
"66025244","“IndexError: tuple index out of range” as part of data flow template job","<p>Below is my python code which is absolutely working fine.</p>
<pre><code>from __future__ import absolute_import
import apache_beam as beam
import argparse
import pickle
import logging
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.io.gcp.internal.clients import bigquery
from datetime import date
today = date.today()
current_date = today.strftime(&quot;%Y%m%d&quot;)
def run(argv=None):
    parser = argparse.ArgumentParser()
    known_args, pipeline_args = parser.parse_known_args(argv)
    p = beam.Pipeline(options=PipelineOptions(pipeline_args))
    (p | 'ReadTable' &gt;&gt; beam.io.Read(beam.io.BigQuerySource(query='select DISTINCT(REPLACE(MOBILE,&quot;+91 &quot;,&quot;91&quot;)) from `whr-asia-datalake-nonprod.WHR_DATALAKE.C4C_CONSUMER_RAW`',use_standard_sql=True))
       | 'read values' &gt;&gt; beam.Map(lambda x: x.values())
       | 'CSV format' &gt;&gt; beam.Map(lambda row:'|'.join (&quot;WHIRLPOOL|WHR|&quot;+ str(column) +'|&quot;'+&quot;Hi, This msg is from Whirlpool DL&quot; + '&quot;' for column in row))
       | 'Write_to_GCS' &gt;&gt; beam.io.WriteToText('gs://whr-asia-datalake-dev-standard/outbound/Valuefirst/WHR_MOBILE_CNSNT_REQ'+''+ str(current_date),file_name_suffix='.csv',header='SENDER_ID|SHORTCODE|MOBILE_NUM|CONSENT_MSG'))
    p.run().wait_until_finish()
if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    run()
</code></pre>
<p>I modified the above code to add a new requirement of creating and empty &quot;.done&quot; file with every file created above. We added below function in our job to create an empty file</p>
<pre><code>today = date.today()
current_date = today.strftime(&quot;%Y%m%d&quot;)

def create_done(argv=None):
    parser = argparse.ArgumentParser()
    known_args, pipeline_args = parser.parse_known_args(argv)
    p = beam.Pipeline(options=PipelineOptions(pipeline_args))
    (p | 'Create .done File' &gt;&gt; beam.io.WriteToText('gs://whr-asia-datalake-dev-standard/outbound/Valuefirst/Valuefirst'+''+str(current_date),file_name_suffix='.done'))
    p.run().wait_until_finish()
if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    create_done()
</code></pre>
<p>However,as soon we add this new code for creating empty .done file, the script is failing with error as</p>
<p>input_tag = transform_node.inputs[0].tag
IndexError: tuple index out of range</p>
<p>I am not able to paste the full traceback of the error. Please let me know of this helps.</p>
","<python><google-cloud-dataflow>","2021-02-03 10:05:08","351","0","1","66035922","<p>WriteToText requires an input PCollection, but you are applying it directly to the Pipeline object.  You need a data source, like ReadFromText or Create, in order to run your pipeline.</p>
<p>See <a href=""https://beam.apache.org/documentation/programming-guide/"" rel=""nofollow noreferrer"">https://beam.apache.org/documentation/programming-guide/</a> for more info about Pipelines and PCollections, as well as some examples of simple pipelines</p>
"
"65988825","Is there any way to capture 'Error Details' from data flow activity?","<p>I need to capture Error Details from Data Flow Activity in case of any error and use it to log into database.</p>
","<azure-data-factory>","2021-02-01 07:44:59","125","0","1","65989403","<p>You can use this expression to get error message from Data Flow:</p>
<pre><code>@activity('Data Flow name').error.message
</code></pre>
<p><a href=""https://i.stack.imgur.com/JkHaP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JkHaP.png"" alt=""enter image description here"" /></a></p>
"
"65949690","Look for best approach replicating oracle table in S3 bucket","<p>My problem:</p>
<p>I need a data pipeline created from my organization’s Oracle DB (Oracle Cloud Infrastructure) to an AWS S3 bucket. Ideally, I would love for there to be some mechanism for oracle to push new data that has entered the database to be pushed to an S3 bucket as it is added (in whatever format).</p>
<p>Question:</p>
<p>Is this possible with Oracle native, specifically Oracle Cloud Infrastructure?</p>
<p>Or would is there a better solution you have seen?</p>
<p>Note:
I have seen AWS has the Data Sync product, this seems like it could facilitate with this problem, however I am not sure if it is suitable for this specific problem.</p>
","<oracle><amazon-s3><data-synchronization><oracle-cloud-infrastructure>","2021-01-29 06:16:59","791","1","1","65953426","<p>An S3 bucket is object storage; it can only hold complete files. You cannot open and update an existing file like you would in a normal file system, even just to add new rows. You will need to construct your whole file outside of Oracle and then push it to S3 with some other mechanism.</p>
<p>You may want to consider the following steps:</p>
<ul>
<li>Export your data from Oracle Cloud into Oracle Object Storage (similar to S3) using the Oracle Cloud's integration with their object storage. (<a href=""https://blogs.oracle.com/datawarehousing/the-simplest-guide-to-exporting-data-from-autonomous-database-directly-to-object-storage"" rel=""nofollow noreferrer"">https://blogs.oracle.com/datawarehousing/the-simplest-guide-to-exporting-data-from-autonomous-database-directly-to-object-storage</a>)</li>
</ul>
<p>THEN:</p>
<ul>
<li>Let the customer access the Oracle Object Store as they normally would access S3, using Oracle's Amazon S3 Compatibility API. (<a href=""https://docs.oracle.com/en-us/iaas/Content/Object/Tasks/s3compatibleapi.htm"" rel=""nofollow noreferrer"">https://docs.oracle.com/en-us/iaas/Content/Object/Tasks/s3compatibleapi.htm</a>)</li>
</ul>
<p>OR:</p>
<ul>
<li>Use an externally driven script to download the data - either from Oracle Object Store or directly from the database - to a server, then push the file up to Amazon S3. The server could be local, or hosted in either Oracle OCI or in AWS, as long as it has access to both object stores. (<a href=""https://blogs.oracle.com/linux/using-rclone-to-copy-data-in-and-out-of-oracle-cloud-object-storage"" rel=""nofollow noreferrer"">https://blogs.oracle.com/linux/using-rclone-to-copy-data-in-and-out-of-oracle-cloud-object-storage</a>)</li>
</ul>
<p>OR:</p>
<ul>
<li>You may be able to use AWS Data Sync to move data directly from Oracle Object Storage to S3, depending on networking configuration requirements. (<a href=""https://aws.amazon.com/blogs/aws/aws-datasync-adds-support-for-on-premises-object-storage/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/aws/aws-datasync-adds-support-for-on-premises-object-storage/</a>)</li>
</ul>
"
"65939577","Kubernetes deployment name changes after every deployment of stream from spring cloud data flow","<p>i have a stream source application Name : app1
Stream Name : Stream1
After First deployment, deployment name is Stream-app1-v1
After second deployment, deployment name is Stream-app1-v2</p>
<p>can one please explain how the names are changing?
i'm trying to autoscale the source based on deployment kind.</p>
","<kubernetes><spring-cloud-dataflow>","2021-01-28 14:53:24","79","0","2","65946505","<p>If you need Pods to have consistent names, then you should use a StatefulSet.</p>
<p>See: <a href=""https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/</a></p>
<p>They're pretty much like Deployment or DeploymentConfigs, though there is no object versioning, no rollback possible. StatefulSets are more suited for stateful workloads such as clustered databases.</p>
"
"65939577","Kubernetes deployment name changes after every deployment of stream from spring cloud data flow","<p>i have a stream source application Name : app1
Stream Name : Stream1
After First deployment, deployment name is Stream-app1-v1
After second deployment, deployment name is Stream-app1-v2</p>
<p>can one please explain how the names are changing?
i'm trying to autoscale the source based on deployment kind.</p>
","<kubernetes><spring-cloud-dataflow>","2021-01-28 14:53:24","79","0","2","66005649","<p>SCDF uses Skipper server to manage the continuous delivery of the streaming applications. Every time you upgrade/rollback an existing application of the stream, that app gets updated with an incremental version name in the suffix. There is a related discussion around this here: <a href=""https://github.com/spring-cloud/spring-cloud-skipper/issues/953"" rel=""nofollow noreferrer"">https://github.com/spring-cloud/spring-cloud-skipper/issues/953</a></p>
"
"65932762","Is there any way we can capture ADF data flow activity logs?","<p>I was trying to log the data flow activity information by using the data flow activity's output object which has a key named 'metrics' which contains 'RowsRead' and 'RowsWritten' but sometimes when data flow activity insert data into database, the metrics show 'NULL' value although records were read and written into database.</p>
","<azure-data-factory>","2021-01-28 07:35:07","478","0","2","65932852","<p>Yes you can do that in ADF</p>
<p>You can create a parameterized stored procedure to do this activity.</p>
<p>Then call that Stored Procedure in ADF pipeline to store the logs in Database.</p>
<p>You can follow <a href=""https://www.mssqltips.com/sqlservertip/6320/logging-azure-data-factory-pipeline-audit-data/"" rel=""nofollow noreferrer"">this</a> link to know how to develop that Stored Procedure and invoke the same in your ADF pipeline</p>
<p>Thanks!</p>
"
"65932762","Is there any way we can capture ADF data flow activity logs?","<p>I was trying to log the data flow activity information by using the data flow activity's output object which has a key named 'metrics' which contains 'RowsRead' and 'RowsWritten' but sometimes when data flow activity insert data into database, the metrics show 'NULL' value although records were read and written into database.</p>
","<azure-data-factory>","2021-01-28 07:35:07","478","0","2","65943731","<p>Can you make sure setting for logging level in 'dataflow activity' is set to verbose/detailed.</p>
"
"65905743","Send info from third app to mdm server ios","<p>I would like to send some info from my ios app to mdm server. It would be simple info, like lastDatabase loaded, etc..</p>
<p>Searching apple documentation about mdm I found that the mdm server can sendo info to a app using the command ManagedApplicationConfiguration. The app can acesss this information using the NSUserDefaults and the key com.apple.configuration.managed.</p>
<p>My problem is to do the inverse way. I thought to use the ManagedApplicationFeedback and then store some keys in the NSUserDefaults, using the key com.apple.feedback.managed, but it seems not correct because this dictionary, by the name, has the purpose of send feedback info, not general info like lastDatabase loaded, etc..</p>
<p>I would appreciate some help. Thank you</p>
","<ios><mdm>","2021-01-26 16:59:12","77","0","1","73862389","<p>You are not sending the info via &quot;com.apple.feedback.managed&quot;, but you are saving the info with &quot;com.apple.feedback.managed&quot; that can be query by mdm server.</p>
<p>For the MDM server query, you have to use the api:</p>
<blockquote>
<p>PUT <a href=""https://yourmdmhost.example.com/mdm"" rel=""nofollow noreferrer"">https://yourmdmhost.example.com/mdm</a></p>
</blockquote>
<p>For the api details, check this:
<a href=""https://developer.apple.com/documentation/devicemanagement/get_app_feedback"" rel=""nofollow noreferrer"">https://developer.apple.com/documentation/devicemanagement/get_app_feedback</a></p>
"
"65873399","Lock Master Data for Personnel Number in ABAP for a long time","<p>Because of the <a href=""https://de.wikipedia.org/wiki/Datenschutz-Grundverordnung"" rel=""nofollow noreferrer"">DSGVO</a> we need to delete personnel data but first we need to lock the personnel data (Infotypes).</p>
<p>Every function module I use works only while the reports is in execution, after that the personnel data is unlocked again.</p>
<pre><code>  DATA: LS_RETURN TYPE BAPIRETURN1. &quot;Return Struktur
  DATA: LV_ERROR_MESSAGE TYPE STRING. &quot;Error Message

* Sperrung der Personalnummer
  CALL FUNCTION 'HR_EMPLOYEE_ENQUEUE'
    EXPORTING
      NUMBER = GV_PERNR &quot;Personalnummer
    IMPORTING
      RETURN = LS_RETURN.
</code></pre>
<p>Does somebody know a way to do it ?</p>
<p>Greets</p>
","<abap><master-data-management>","2021-01-24 16:55:05","242","0","1","65884726","<p>As József Szikszai pointed out in a comment, the <em>proper</em> solution for this would be to install and configure <a href=""https://www.sap.com/products/information-lifecycle-management.html"" rel=""nofollow noreferrer"">Information Lifecycle Management</a>.</p>
<p>But introducing ILM can be a rather large project. So if you need a &quot;good enough&quot; stopgap solution now, then you could solve this via permissions.</p>
<p>I don't have much experience with permissions in HR, but it seems to me like the permission object P_ORGIN should do the job. Simply assign those employees to a separate Personnel Area, Employee Group, Employee Subgroup or Organizational Key (depending on what fits best in your HR permission concept) and then adjust your permission roles so that nobody has the permission to edit these personnel records.</p>
<p>More information on permissions in the module HR can be found in <a href=""https://help.sap.com/erp_hcm_ias2_2015_02/helpdata/en/c9/03dd5321e8424de10000000a174cb4/frameset.htm"" rel=""nofollow noreferrer"">this document from the documentation</a>.</p>
"
"65815791","Calculate the date of the previous month and the date before previous month using current date (data flow, ADF)","<p>I have a date column which is the first day of each month (eg, 2020-01-01), I want to calculate the date of the previous month (eg, 2019-12-01) and the date before the previous month (eg, 2019-11-01). In the expression builder, do we have any function to handle it? Thank you</p>
","<azure-data-factory>","2021-01-20 18:51:08","733","1","1","65819326","<p>You can use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-expression-functions#submonths"" rel=""nofollow noreferrer"">subMonths</a> function, something like:</p>
<pre><code>subMonths(toDate('2020-01-01'), 1)

subMonths(toDate('2020-01-01'), 2)
</code></pre>
"
"65795618","Azure Data Factory - different copy data mappings views in the same pipeline?","<p>I am trying to set up a pipeline with copy data activity in Azure Data Factory and I am confused by the different view of mapping in the copy activity. I have created the pipeline from the template &quot;Copy data from on premise SQL Server to SQL Azure&quot; and I am cloning the activity so there shouldn't be any differences. The source is the same in both activities and I use query against the source database.</p>
<p>Here's how I see it:</p>
<p>Original copy activity:</p>
<p><a href=""https://i.stack.imgur.com/q6rJd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q6rJd.png"" alt=""enter image description here"" /></a></p>
<p>Cloned copy activity:</p>
<p><a href=""https://i.stack.imgur.com/801Jk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/801Jk.png"" alt=""enter image description here"" /></a></p>
<p>I would like to understand why I see different views of mapping.</p>
<p>Thanks in advance!</p>
","<azure><azure-data-factory>","2021-01-19 16:31:04","124","0","1","65822226","<p>I don't that's an problem. When we clone a copy active, before we debug or run the pipeline, we need to check all the settings manually.</p>
<p>From your screenshots, original copy activity miss the source schema during mapping. Just import schema will be solved it.
<a href=""https://i.stack.imgur.com/5H0Sj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5H0Sj.png"" alt=""enter image description here"" /></a></p>
<p>And the cloned copy active seams import schema automatically. Not sure if all the columns are mapped(I think not). Some suggestions:</p>
<ol>
<li>Please import the schema in source dataset firstly, and fully set
one copy active.</li>
<li>Then clone the copy active which may avoid the problem.</li>
</ol>
<p>Data Factory may not very smartly and even we clone active, we still should check all the settings in each actives.</p>
<p><strong>Update:</strong></p>
<p>Like @JeffRamos said, sinks are different then the mapping will be different.</p>
<p>We are glad to hear that you have figured it out:</p>
<ul>
<li>&quot;I have figured it out - I was using the query that contained the
&quot;count(*)&quot; aggregate. Removing it and the &quot;group by&quot; clause made the
mapping view the same as for the original Copy activity.&quot;</li>
</ul>
<p>Thanks for @JeffRamos's useful comment again.</p>
<p>HTH.</p>
"
"65741675","Split a json string column or flatten transformation in data flow (ADF)","<p>I copy the following csv file to a data flow in ADF.</p>
<p><a href=""https://i.stack.imgur.com/6ynV8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6ynV8.png"" alt=""enter image description here"" /></a></p>
<p>The column Data has json format, but it is considered string. I want to flatten Data column into individual rows. I tried the flatten transformation, it did not work as Data column is not json. How do I deal with it? I also tried split expression, and it did not work either. Thank you</p>
","<azure-data-factory>","2021-01-15 18:28:37","1678","0","1","65770042","<p>Just from your screenshot, We can find that :</p>
<ol>
<li>The data in <code>Data</code> are not JSON format.</li>
<li>Data most look like an Array.</li>
<li>The 'array' has 9 elements.</li>
</ol>
<p>Me must consider it as the &quot;Array&quot; then we could using Data Flow <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"" rel=""nofollow noreferrer"">Derived Column</a> to flatten the Data. Please ref my steps bellow:</p>
<p><strong>Source data:</strong>
<a href=""https://i.stack.imgur.com/MkJRu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MkJRu.png"" alt=""enter image description here"" /></a></p>
<p><strong>Derived Column expressions and settings:</strong></p>
<p>The expression to make data as string and using index to get the value:</p>
<pre><code>Data 1: split(substring(Data, 2, length(Data)-2), &quot;,&quot;)[1]
Data 2: split(substring(Data, 2, length(Data)-2), &quot;,&quot;)[2]
Data 3: split(substring(Data, 2, length(Data)-2), &quot;,&quot;)[3]
Data 4: split(substring(Data, 2, length(Data)-2), &quot;,&quot;)[4]
Data 5: split(substring(Data, 2, length(Data)-2), &quot;,&quot;)[5]
Data 6: split(substring(Data, 2, length(Data)-2), &quot;,&quot;)[6]
Data 7: split(substring(Data, 2, length(Data)-2), &quot;,&quot;)[7]
Data 8: split(substring(Data, 2, length(Data)-2), &quot;,&quot;)[8]
Data 9: split(substring(Data, 2, length(Data)-2), &quot;,&quot;)[9]
</code></pre>
<p><a href=""https://i.stack.imgur.com/lw8BB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lw8BB.png"" alt=""enter image description here"" /></a></p>
<p><strong>Derived Column output:</strong>
<a href=""https://i.stack.imgur.com/3bvL6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3bvL6.png"" alt=""enter image description here"" /></a></p>
<p>If the <code>Data</code> are standard JSON format, we need convert the string to JSON first, and then use the key to get the value.</p>
<p>HTH.</p>
"
"65739548","Flatten transformation for the json-string column (data flow in ADF)","<p>I copy a csv file with a json-string column to the data flow.</p>
<p><a href=""https://i.stack.imgur.com/wy0OF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wy0OF.png"" alt=""enter image description here"" /></a>
I want to flatten it by the json-string column, but the column is not recognized as a json format.</p>
<p>How do I convert it to json-format column, or do you have other ways to deal with it? Thank you</p>
","<azure-data-factory>","2021-01-15 16:11:05","153","0","1","65770083","<p>You could ref my answer here: <a href=""https://stackoverflow.com/a/65770042/10549281"">https://stackoverflow.com/a/65770042/10549281</a></p>
<p>If you have any other concerns, please feel free to let me know.</p>
<p>HTH.</p>
"
"65721277","How to Modify App Icon and App Name for MDM Distribution – Rebranding of Native App","<p>We leverage third-party services for our organization which provides employees services. The native Android and iOS apps are part of these services and distrusted in our organization by using MDM (AirWatch) solution. The vendor provides only binaries and NOT Source Code, as they share the same solution with many different organizations.</p>
<p>Currently, the vendor unable to provide customized binaries with our brand icon and app name. We would like to create and distribute a fully branded version of the app for our organization.</p>
<p>Is there any way to <strong>modify App Icon and App Name</strong> (which appear on device) for Binaries of native Android and iOS Apps?</p>
<p>Or is there any custom build tool from which we can re-bundle the binaries with our app icon and name? It would be okay even its a repeatable process with each release.</p>
","<android><ios><iphone><icons><mdm>","2021-01-14 14:51:29","344","0","1","65727650","<p>I don't know about Android but for iOS you would need the provisioning profile, certificate and key. (I imagine you don't have these)</p>
<p>You can then modify the app payload and then re-sign the ipa. I have done this a number of times to update an expiring profile on some of our enterprise apps.</p>
<p>Here is a useful tool for re-signing .ipa files <a href=""https://github.com/xndrs/XReSign"" rel=""nofollow noreferrer"">https://github.com/xndrs/XReSign</a></p>
"
"65673814","Moving YTD MAX by group in Visual expression builder (data flow in ADF)","<p>I am new to visual expression builder language in ADF. I am not sure if I can calculate the moving YTD MAX (or average) by group for the following case in expression language:</p>
<p>I have sales data for different products in different periods, and I want to know the moving YTD (year to date) max sales amount by product.</p>
<p><a href=""https://i.stack.imgur.com/Y6bxp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Y6bxp.png"" alt=""enter image description here"" /></a></p>
<p>I appreciate any help.</p>
","<expression><azure-data-factory>","2021-01-11 20:08:42","68","1","1","65677131","<p>You can use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-window"" rel=""nofollow noreferrer"">Window transformation</a> in Data Flow.</p>
<p>My test:</p>
<ol>
<li>Data of my source
<a href=""https://i.stack.imgur.com/5wdk1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5wdk1.png"" alt=""enter image description here"" /></a></li>
</ol>
<p>2.Setting of Window transformation
<a href=""https://i.stack.imgur.com/1CbbB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1CbbB.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/sagKI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sagKI.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/dE5Fe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dE5Fe.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/Ok3Uc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ok3Uc.png"" alt=""enter image description here"" /></a></p>
<p>Data preview of Window transformation
<a href=""https://i.stack.imgur.com/zRIDO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zRIDO.png"" alt=""enter image description here"" /></a></p>
"
"65665161","Running apple script from bash","<p>I have this apple script code that works fine from the Script Editor app:</p>
<p><code>tell application &quot;Finder&quot; to delete ((POSIX file &quot;/Applications/Symantec Solutions/Symantec Endpoint Protection.app&quot;) as alias)</code></p>
<p>Now, I need to run that from my MDM solution (JAMF), so I'm trying to get it into a bash file as this:</p>
<p><code>osascript -e &quot;tell application \&quot;Finder\&quot; to delete (POSIX file \&quot;/Applications/Symantec Solutions/Symantec Endpoint Protection.app\&quot;) as alias&quot;</code></p>
<p>but when run it from my editor (CodeRunner) it fails with: execution error: Not authorised to send Apple events to System Events. (-1743)</p>
<p>I think it's related with the Privacy Preferences Policy Control but I cannot make it work.</p>
<p>What I want to accomplish ultimately is that by running a script from my MDM I can get the same dialog that when I drag the app to the bin: &quot;The application “Symantec Endpoint Protection” is hosting system extensions. These extensions will be removed if you continue.&quot; which I don't get if I just <code>rm -rf</code> the whole app.</p>
<p>Sorry if I can't be any clearer. Any clues?</p>
","<applescript><mdm><privacy-policy>","2021-01-11 10:32:02","459","0","3","65673638","<p>Script Editor is (by default) authorized to apple events to other applications. If you want to run an AppleScript that controls other apps from some different context, you have to grant permission to that new context to control other apps. You do this by adding the controlling application to the 'Accessibility' section of System Preferences ➦ Security &amp; Privacy ➦ Privacy.</p>
<p>This gets a bit dicy when you shift contexts a lot. For instance, you seem to be using CodeRunner to execute a shell script to execute OSAScript, which might get a security violation at any level. Try giving permissions to CodeRunner and see if that clears things up.</p>
"
"65665161","Running apple script from bash","<p>I have this apple script code that works fine from the Script Editor app:</p>
<p><code>tell application &quot;Finder&quot; to delete ((POSIX file &quot;/Applications/Symantec Solutions/Symantec Endpoint Protection.app&quot;) as alias)</code></p>
<p>Now, I need to run that from my MDM solution (JAMF), so I'm trying to get it into a bash file as this:</p>
<p><code>osascript -e &quot;tell application \&quot;Finder\&quot; to delete (POSIX file \&quot;/Applications/Symantec Solutions/Symantec Endpoint Protection.app\&quot;) as alias&quot;</code></p>
<p>but when run it from my editor (CodeRunner) it fails with: execution error: Not authorised to send Apple events to System Events. (-1743)</p>
<p>I think it's related with the Privacy Preferences Policy Control but I cannot make it work.</p>
<p>What I want to accomplish ultimately is that by running a script from my MDM I can get the same dialog that when I drag the app to the bin: &quot;The application “Symantec Endpoint Protection” is hosting system extensions. These extensions will be removed if you continue.&quot; which I don't get if I just <code>rm -rf</code> the whole app.</p>
<p>Sorry if I can't be any clearer. Any clues?</p>
","<applescript><mdm><privacy-policy>","2021-01-11 10:32:02","459","0","3","65683006","<p>Thanks a lot Ted, that was the issue, having a lot of contexts chained. I had to tick Finder under CodeRunner in the Automation section of PPPC.</p>
"
"65665161","Running apple script from bash","<p>I have this apple script code that works fine from the Script Editor app:</p>
<p><code>tell application &quot;Finder&quot; to delete ((POSIX file &quot;/Applications/Symantec Solutions/Symantec Endpoint Protection.app&quot;) as alias)</code></p>
<p>Now, I need to run that from my MDM solution (JAMF), so I'm trying to get it into a bash file as this:</p>
<p><code>osascript -e &quot;tell application \&quot;Finder\&quot; to delete (POSIX file \&quot;/Applications/Symantec Solutions/Symantec Endpoint Protection.app\&quot;) as alias&quot;</code></p>
<p>but when run it from my editor (CodeRunner) it fails with: execution error: Not authorised to send Apple events to System Events. (-1743)</p>
<p>I think it's related with the Privacy Preferences Policy Control but I cannot make it work.</p>
<p>What I want to accomplish ultimately is that by running a script from my MDM I can get the same dialog that when I drag the app to the bin: &quot;The application “Symantec Endpoint Protection” is hosting system extensions. These extensions will be removed if you continue.&quot; which I don't get if I just <code>rm -rf</code> the whole app.</p>
<p>Sorry if I can't be any clearer. Any clues?</p>
","<applescript><mdm><privacy-policy>","2021-01-11 10:32:02","459","0","3","71338862","<p>I had a very similar issue with a Python script executing AppleScript in CodeRunner. I just couldn't figure out how to manually add CodeRunner to the <code>Automation</code> section of <code>Security &amp; Privacy</code> in System Preferences.</p>
<p>What ended up working for me, might work for others as well. I had to manually trigger the <code>osascript</code> command within CodeRunner, in order to get the prompt to allow CodeRunner to control Apple Events. Specifically <code>System Events</code>. I did that by creating a new shell script file in CodeRunner and running it.</p>
<pre><code>#!/bin/bash

osascript -e 'tell App &quot;System Events&quot; to display dialog &quot;Testing&quot;'
</code></pre>
<p>Upon running that, I was prompted to <code>allow</code> CodeRunner to control <code>System Events</code>. For your particular issue, you'd just want to run the <code>osascript</code> command with AppleScript for <code>Finder</code> instead of <code>System Events</code></p>
<pre><code>osascript -e 'tell App &quot;Finder to display dialog &quot;Testing This&quot;'
</code></pre>
<p>Since you're using an MDM, such as Jamf, you should be able to create a Privacy Preferences Policy Control (PPPC) configuration profile and deploy it to the Mac prior to running the script. You'd just want to ensure that <code>Terminal</code> has access to control <code>Finder</code> or <code>System Events</code>.</p>
<p>The <a href=""https://github.com/jamf/PPPC-Utility"" rel=""nofollow noreferrer"">PPPC Utility</a> is a great app for creating those configuration profiles.</p>
"
"65414939","How to see the continuing 8-bit grayscale image data flow from USB port?","<p>So, I have a continuous data flow coming thorugh the COM4 port on Windows. I am reading the data with this code:</p>
<pre><code>import serial
import cv2
from time import sleep

ser = serial.Serial (&quot;COM4&quot;, 115200)    
while True:
    received_data = ser.read()              
    sleep(0.03)
    data_left = ser.inWaiting()             
    a=cv2.imread(received_data,0)
    cv2.imshow(&quot;Image&quot;,a)
    print (received_data)
</code></pre>
<p>the openCV part does not work and spyder gives this error:</p>
<pre><code>TypeError: bad argument type for built-in operation
</code></pre>
<p>If I take the openCV part out I am able to see bytes like this:</p>
<pre><code>b'AA\x00A::\x00A:A\x00AA:\x00A:A\x00A:A\x00:A:\x00A:AAA::AAA\x00AAA\x00A::\x00:AA\x00:::\x00AAA\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00::A\x00A::A::::A::A:A\x00AAA\x00A::\x00AA:\x00:A:\x00::A\x00AAA\x00A:A\x00:A:\x00:A:\x00::AAA::::A:A::\x00AA:\x00:::\x00AAA\x00::A\x00:AA\x00:AA\x00::A\x00:::\x00:A:\x00A::AAAA:A:AAA:\x00A::\x00:AA\x00AAA\x00:::\x00:A:\x00AAA\x00AAA\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00::AA:\x00A::\x00:A:\x00:::\x00:::\x00AAA\x00:::\x00:A:\x00:A:\x00A:3\x00AAAAA:A:::AA::\x00::A\x00A::\x00A:A\x00:::\x00:::\x00::A\x00:::\x00A::\x00:AA\x00AA:AA::AAAA:::\x00::A\x00AA:\x00A::\x00:A:\x00:AA\x00A::\x00AA:\x00AA:\x00:AA\x00:A:\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00A\x00:::\x00AA:\x00:AA\x00:A:\x00AAA\x00:::\x00A::\x00A::\x00AAAA:AAA:A\x00:AA\x00A::\x00:AA\x00:::\x00:A:\x00A:A'
</code></pre>
<p>This data is 8-bit grayscale image data. How can I see the continuous data flow ( kind of like a grayscale video) with python?</p>
<p>This is the raw 8-bit grayscale image data coming from an FPGA. The dimensions are 160*120 and the starting bit is 0 and the stopping bit is 1</p>
","<python><opencv><image-processing>","2020-12-22 19:45:30","236","1","1","65416439","<p>Your code will need to look something like this but it is untested as I don't have your image source.</p>
<pre><code>import cv2
import numpy as np

while True:

    bytesBuffer = read and accumulate 160*120 bytes from serial

    # Make bytes buffer into Numpy array and reshape
    frame =  np.frombuffer(bytesBuffer, dtype=np.uint8).reshape((120,160))

    cv2.imshow(&quot;Frame&quot;, frame)
    cv2.waitKey(30)
</code></pre>
<p>You will need to somehow know when the frame begins so you know when you have the top-left pixel.</p>
<p>You may need to swap the 120 and 160. As it is, it assumes height is 120 pixels and width is 160 pixels.</p>
<p>The problem with your code is that <code>cv2.imread()</code> expects a file that contains a JPEG/PNG encoded image, rather than the raw bytes you are currently giving it.</p>
<p>You could usefully share a buffer of 160x120 bytes of data in your question, or provide some serial code that sends an image in the same way as your source.</p>
"
"65358246","How to pause the Spring cloud data flow Source class from sending data to kafka?","<p>i am working on spring cloud data flow application ,Following is the code snippet</p>
<pre><code>    @Bean
    @InboundChannelAdapter(channel = TbeSource.PR1, poller = @Poller(fixedDelay = &quot;2000&quot;))
    public MessageSource&lt;Product&gt; getProductSource(ProductBuilder dataAccess) {

        return new MessageSource&lt;Product&gt;() {
            @SneakyThrows
            @Override
            public Message&lt;Product&gt; receive() {
                System.out.println(&quot;calling method&quot;);
                return MessageBuilder.withPayload(dataAccess.getNext()).build();
            }
        };
    }
</code></pre>
<p>In above code the <code>getNext()</code> method will get the data from the database and return that object,so if the data is completely  readed then  it will return null</p>
<p>we can't return null to this MessageSource.</p>
<p>so is there any options available to pause and resume this Source connection class whenever we need?</p>
<p>Did any one faced / overcome  this scenario?</p>
","<spring-integration><spring-kafka><spring-cloud-stream><spring-cloud-dataflow>","2020-12-18 13:51:34","106","0","1","65358722","<p>First of all you just can have a <code>Supplier&lt;Product&gt;</code> instead of that <code>MessageSource</code>and your code would be just like this:</p>
<pre><code>return () -&gt; dataAccess.getNext();
</code></pre>
<p>The <code>null</code> result is valid over here and no message is going to be emitted in this case and no error since the framework handles <code>null</code> result properly.</p>
<p>You still can have an idle functionality on that <code>@InboundChannelAdapter</code> when result of the method call is <code>null</code>. For that reason you need to take a look into the <code>SimpleActiveIdleMessageSourceAdvice</code>. See docs for more info: <a href=""https://docs.spring.io/spring-integration/docs/5.3.4.RELEASE/reference/html/core.html#simpleactiveidlereceivemessageadvice"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-integration/docs/5.3.4.RELEASE/reference/html/core.html#simpleactiveidlereceivemessageadvice</a></p>
"
"65355724","How to get database connection status inside Azure data factory's data flow?","<p>I have created a azure data factory inside which I am using data flow.</p>
<p>This data flow contains multiple operations like to read some files from blobs and read some tables data from azure data base.</p>
<p>My issue is that whenever my dataflow activity trying to read tables from database then I got an error because at that time my database is in pause state.</p>
<p>I searched in net and found that I can check status of azure database by using powershell command but for that I need to create a sperate azure function.</p>
<p>My azure database automatically start if any hit to database. So, I want to do multiple retry to connect with database for certain interval of time and do continue with remaining tasks.</p>
<p>Is there a way to do inside a data flow?</p>
","<azure><azure-sql-database><azure-data-factory>","2020-12-18 10:50:48","113","0","1","65359640","<p>You have a couple of options for this:
1 - in general, a serverless SQL DB instance takes up to 2 minutes to recover from a paused state so you can add a wait task to your data factory pipeline to wait for 2 minutes between opening the connection and starting the transfer</p>
<p>2 - you can specify a longer timeout for the connection to SQL, this will allow it to wait till the database comes out of paused state.</p>
<p>for the second option, what you need to do is on the connection for SQL database, you can go and add extra connection property called connection timeout and set this to a value of 120+ <a href=""https://i.stack.imgur.com/tCI0o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tCI0o.png"" alt=""enter image description here"" /></a></p>
"
"65299504","What's the relationship between data flow and data binding?","<p>Data flow and data binding. Often times I read about one of the terms, I find it is interchangeably used with the other.</p>
<p>E.g. <a href=""https://stackoverflow.com/a/47652784/4132182"">source</a>:</p>
<blockquote>
<p>Two-way flow aka data binding binds two pieces of state: in most
cases, one inside the controller (e. g. some variable), and one inside
the view (e. g. contents of textbox). Binding means that, when one
piece changes, the other piece changes as well and gets the same
value, so you can pretend that there's only one piece of state
involved (while there's two actually). Write events are going back and
forth between controllers and views - thus two-way.</p>
</blockquote>
<p>The questions:</p>
<ul>
<li>Are both terms the same thing named differently, or are there important differences?</li>
<li>What is the proper way/context to use either of these terms?</li>
</ul>
<hr />
<p>P.S. I've tagged modern js technologies like react and angular since that's the context I'm coming from but examples without them are fine as well. Feel free to edit the questions above, if you feel like the list is incomplete or could be better.</p>
","<reactjs><angular><redux><data-binding><two-way-binding>","2020-12-15 03:12:31","213","0","2","65299708","<p><strong>Angular's two-way data binding</strong> :
It's made possible by a mechanism that synchronizes the view and the model whenever either change. In Angular, you update a variable and its change detection mechanism will take care of updating the view, and vice versa. What's the problem? You don't control the change detection mechanism. I found myself having to resort to ChangeDetectorRef.detectChanges or NgZone.run to force the view to update.</p>
<p>To not dive too deep into change detection in Angular, you trust it will update what you need when you change a variable, or when it gets changed after an observable resolve, but you'll find you have no idea how and when it runs, and sometimes it will not update your view after variable changes. Needless to say, it can sometimes be a pain to find where and when a problem occurred.</p>
<p><strong>React's one-way data flow</strong> :
It means that the view always gets its state from the model. To update the view, you need to update the model first, and then redraw the view. React makes the view redrawing process extremely efficient because it compares not the actual DOM but a virtual DOM it keeps on memory. But how does change detection work in this dynamic? Well, you trigger it manually.</p>
<p>In React, you set the state's new value, which then causes a ReactDOM.render, which causes the DOM comparing/updating process. In React/Redux you dispatch actions that update the store (single source of truth) and then the rest. Point is, you always know when the stuff changes, and what caused the change. This makes problem-solving quite straight forward. If your app depends on the state, you look at it before and after the action that triggered the change, and you make sure variables have the value they're supposed to.</p>
"
"65299504","What's the relationship between data flow and data binding?","<p>Data flow and data binding. Often times I read about one of the terms, I find it is interchangeably used with the other.</p>
<p>E.g. <a href=""https://stackoverflow.com/a/47652784/4132182"">source</a>:</p>
<blockquote>
<p>Two-way flow aka data binding binds two pieces of state: in most
cases, one inside the controller (e. g. some variable), and one inside
the view (e. g. contents of textbox). Binding means that, when one
piece changes, the other piece changes as well and gets the same
value, so you can pretend that there's only one piece of state
involved (while there's two actually). Write events are going back and
forth between controllers and views - thus two-way.</p>
</blockquote>
<p>The questions:</p>
<ul>
<li>Are both terms the same thing named differently, or are there important differences?</li>
<li>What is the proper way/context to use either of these terms?</li>
</ul>
<hr />
<p>P.S. I've tagged modern js technologies like react and angular since that's the context I'm coming from but examples without them are fine as well. Feel free to edit the questions above, if you feel like the list is incomplete or could be better.</p>
","<reactjs><angular><redux><data-binding><two-way-binding>","2020-12-15 03:12:31","213","0","2","65299787","<p>Let's talk about the React js for an instance.</p>
<p>When we want to bind the data between view and controller, we have to use setState/redux/flux. Here, setState/flux/redux is used for the data-flow. React has a uni-directional data-flow, which means, the changes we make in the view, don't automatically reflect in the controller. You have to explicitly call some method to pass the data.</p>
<p>Also, the data-flow is not only restricted to the binding. If you want to pass the data between components, you can use this mechanism. (Don't confuse it with binding)</p>
<pre><code>class NameForm extends React.Component {
  constructor(props) {
    super(props);
    this.state = {value: ''};

    this.handleChange = this.handleChange.bind(this);
    this.handleSubmit = this.handleSubmit.bind(this);
  }

  handleChange(event) {
    this.setState({value: event.target.value});
  }

  handleSubmit(event) {
    alert('A name was submitted: ' + this.state.value);
    event.preventDefault();
  }

  render() {
    return (
      &lt;form onSubmit={this.handleSubmit}&gt;
        &lt;label&gt;
          Name:
          &lt;input type=&quot;text&quot; value={this.state.value} onChange={this.handleChange} /&gt;
        &lt;/label&gt;
        &lt;input type=&quot;submit&quot; value=&quot;Submit&quot; /&gt;
      &lt;/form&gt;
    );
  }
}
</code></pre>
<p>In the code above, you can see how <code>this.state.value</code> is updating.</p>
<p>In contrast to React, Angular has a two-way data binding. What it means is, you don't have to explicitly change the variable. It gets updated automatically when we change the view.</p>
<pre><code>&lt;script&gt;
  angular.module('changeExample', [])
    .controller('ExampleController', ['$scope', function($scope) {
      $scope.counter = 0;
      $scope.change = function() {
        $scope.counter++;
      };
    }]);
&lt;/script&gt;
&lt;div ng-controller=&quot;ExampleController&quot;&gt;
  &lt;input type=&quot;checkbox&quot; ng-model=&quot;confirmed&quot; ng-change=&quot;change()&quot; id=&quot;ng-change-example1&quot; /&gt;
  &lt;input type=&quot;checkbox&quot; ng-model=&quot;confirmed&quot; id=&quot;ng-change-example2&quot; /&gt;
  &lt;label for=&quot;ng-change-example2&quot;&gt;Confirmed&lt;/label&gt;&lt;br /&gt;
  &lt;tt&gt;debug = {{confirmed}}&lt;/tt&gt;&lt;br/&gt;
  &lt;tt&gt;counter = {{counter}}&lt;/tt&gt;&lt;br/&gt;
&lt;/div&gt;
</code></pre>
<p>Here, you can see, <code>confirmed</code> is two-way bound, therefore, we don't need to update the state manually.</p>
"
"65288849","How does data flow work when using the Fetch API?","<p>Take a simple API fetch call, such as follows:</p>
<pre><code>fetch('https://api.nasa.gov/planetary/apod?api_key=xxxxxxxxx')
.then(res =&gt; res.json())
.then(data =&gt;setPic(data.hdurl))

</code></pre>
<p>I'm still a bit confused about how this works. My understanding is this - information is sent from the web server as JSON, but to displayed on a web page it has to be converted into a normal JS object. Is this correct?</p>
<p>And if so, how does the above method convert JSON to a JS object? Because as I understand it, <code>res.json</code> simply extracts the JSON, it doesn't convert it.</p>
","<javascript><fetch>","2020-12-14 12:12:25","110","0","1","65289188","<blockquote>
<p>[...] how does the above method convert <strong>JSON</strong> to a <strong>JS Object</strong>? Because
as I understand it, <code>res.json()</code> simply extracts the <strong>JSON</strong>, it doesn't
convert it.</p>
</blockquote>
<p>This is what <code>.json()</code> does - it resolves the <strong>JSON string</strong> and parses it into a <strong>JS Object</strong>:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>// Retrieves data from a URL
fetch('data:text/plain;charset=utf-8,%7B%22myJSON%22%3A%20%22myJSON%22%7D')

// Resolve the data retrieved from the URL as JSON and parse into a JS Object
.then(res =&gt; res.json())

// Work with the resolved data
.then(data =&gt; {
  console.log('data has been resolved as: ' + typeof data);
  console.log(data);
});</code></pre>
</div>
</div>
</p>
<p>If you want the <strong>JSON String</strong> to remain a <strong>JSON String</strong>, you can use <code>.text()</code> instead:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>// Retrieves data from a URL
fetch('data:text/plain;charset=utf-8,%7B%22myJSON%22%3A%20%22myJSON%22%7D')

// Resolve the data retrieved from the URL as a string
.then(res =&gt; res.text())

// Work with the resolved data
.then(data =&gt; {
  console.log('data has been resolved as: ' + typeof data);
  console.log(data);
});</code></pre>
</div>
</div>
</p>
"
"65171714","Image loading/caching off the main thread","<p>I am writing a custom image fetcher to fetch the images needed for my collection view. Below is my image fetcher logic</p>
<pre><code>class ImageFetcher {

    /// Thread safe cache that stores `UIImage`s against corresponding URL's
    private var cache = Synchronised([URL: UIImage]())

    /// Inflight Requests holder which we can use to cancel the requests if needed
    /// Thread safe
    private var inFlightRequests = Synchronised([UUID: URLSessionDataTask]())
    
    
    func fetchImage(using url: URL, completion: @escaping (Result&lt;UIImage, Error&gt;) -&gt; Void) -&gt; UUID? {
        /// If the image is present in cache return it
        if let image = cache.value[url] {
            completion(.success(image))
        }
        
        let uuid = UUID()
        
        let dataTask = URLSession.shared.dataTask(with: url) { [weak self] data, response, error in
            guard let self = self else { return }
            defer {
                self.inFlightRequests.value.removeValue(forKey:uuid )
            }
            
            if let data = data, let image = UIImage(data: data) {
                self.cache.value[url] = image
                
                DispatchQueue.main.async {
                    completion(.success(image))
                }
                return
            }
            
            guard let error = error else {
                // no error , no data
                // trigger some special error
                return
            }
            
            
            // Task cancelled do not send error code
            guard (error as NSError).code == NSURLErrorCancelled else {
                completion(.failure(error))
                return
            }
        }
        
        dataTask.resume()
        
        self.inFlightRequests.value[uuid] = dataTask
        
        return uuid
    }
    
    func cancelLoad(_ uuid: UUID) {
        self.inFlightRequests.value[uuid]?.cancel()
        self.inFlightRequests.value.removeValue(forKey: uuid)
    }
}

</code></pre>
<p>This is a block of code that provides the thread safety needed to access the cache</p>
<pre><code>/// Use to make a struct thread safe
public class Synchronised&lt;T&gt; {
    private var _value: T
    
    private let queue = DispatchQueue(label: &quot;com.sync&quot;, qos: .userInitiated, attributes: .concurrent)
    
    public init(_ value: T) {
        _value = value
    }
    
    public var value: T {
        get {
            return queue.sync { _value }
        }
        set { queue.async(flags: .barrier) { self._value = newValue }}
    }
}

</code></pre>
<p>I am not seeing the desired scroll performance and I anticipate that is because my main thread is getting blocked when I try to access the cache(<code>queue.sync { _value }</code>). I am calling the <code>fetchImage</code> method from the <code>cellForRowAt</code> method of the collectionView and I can't seem to find a way to dispatch it off the main thread because I would need the request's UUID so I would be able to cancel the request if needed. Any suggestions on how to get this off the main thread or are there any suggestions to architect this in a better way?</p>
","<ios><multithreading><grand-central-dispatch><data-synchronization><barrier>","2020-12-06 18:39:58","322","2","1","65186234","<p>I do not believe that your scroll performance is related to <code>fetchImage</code>. While there are modest performance issues in <code>Synchronized</code>, it likely is not enough to explain your issues. That having been said, there are several issue here, but blocking the main queue does not appear to be one of them.</p>
<p>The more likely culprit might be retrieving assets that are larger than the image view (e.g. large asset in small image view requires resizing which can block the main thread) or some mistake in the fetching logic. When you say “not seeing desired scroll performance”, is it stuttering or just slow? The nature of the “scroll performance” problem will dictate the solution.</p>
<hr />
<p>A few unrelated observations:</p>
<ol>
<li><p><code>Synchronised</code>, used with a dictionary, is not thread-safe. Yes, the getter and setter for <code>value</code> is synchronized, but not the subsequent manipulation of that dictionary. It is also very inefficient (though, not likely sufficiently inefficient to explain the problems you are having).</p>
<p>I would suggest not synchronizing the retrieval and setting of the whole dictionary, but rather make a synchronized dictionary type:</p>
<pre><code>public class SynchronisedDictionary&lt;Key: Hashable, Value&gt; {
    private var _value: [Key: Value]

    private let queue = DispatchQueue(label: &quot;com.sync&quot;, qos: .userInitiated, attributes: .concurrent)

    public init(_ value: [Key: Value] = [:]) {
        _value = value
    }

    // you don't need/want this
    //
    // public var value: [Key: Value] {
    //     get { queue.sync { _value } }
    //     set { queue.async(flags: .barrier) { self._value = newValue } }
    // }

    subscript(key: Key) -&gt; Value? {
        get { queue.sync { _value[key] } }
        set { queue.async(flags: .barrier) { self._value[key] = newValue } }
    }

    var count: Int { queue.sync { _value.count } }
}
</code></pre>
<p>In my tests, in release build this was about 20 times faster. Plus it is thread-safe.</p>
<p>But, the idea is that you should not expose the underlying dictionary, but rather just expose whatever interface you need for the synchronization type to manage the dictionary. You will likely want to add additional methods to the above (e.g. <code>removeAll</code> or whatever), but the above should be sufficient for your immediate purposes. And you should be able to do things like:</p>
<pre><code>var dictionary = SynchronizedDictionary&lt;String, UIImage&gt;()

dictionary[&quot;foo&quot;] = image
imageView.image = dictionary[&quot;foo&quot;]
print(dictionary.count)
</code></pre>
<p>Alternatively, you could just dispatch all updates to the dictionary to the main queue (see point 4 below), then you don't need this synchronized dictionary type at all.</p>
</li>
<li><p>You might consider using <code>NSCache</code>, instead of your own dictionary, to hold the images. You want to make sure that you respond to memory pressure (emptying the cache) or some fixed total cost limit. Plus, <code>NSCache</code> is already thread-safe.</p>
</li>
<li><p>In <code>fetchImage</code>, you have several paths of execution where you do not call the completion handler. As a matter of convention, you will want to ensure that the completion handler is always called. E.g. what if the caller started a spinner before fetching the image, and stopping it in the completion handler? If you might not call the completion handler, then the spinner might never stop, either.</p>
</li>
<li><p>Similarly, where you do call the completion handler, you do not always dispatch it back to the main queue. I would either always dispatch back to the main queue (relieving the caller from having to do so) or just call the completion handler from the current queue, but only dispatching some of them to the main queue is an invitation for confusion.</p>
</li>
</ol>
<hr />
<p>FWIW, you can create Unit Tests target and demonstrate the difference between the original <code>Synchronised</code> and the <code>SynchronisedDictionary</code>, by testing a massively concurrent modification of the dictionary with <code>concurrentPerform</code>:</p>
<pre><code>// this is not thread-safe if T is mutable

public class Synchronised&lt;T&gt; {
    private var _value: T

    private let queue = DispatchQueue(label: &quot;com.sync&quot;, qos: .userInitiated, attributes: .concurrent)

    public init(_ value: T) {
        _value = value
    }

    public var value: T {
        get { queue.sync { _value } }
        set { queue.async(flags: .barrier) { self._value = newValue }}
    }
}

// this is thread-safe dictionary ... assuming `Value` is not mutable reference type

public class SynchronisedDictionary&lt;Key: Hashable, Value&gt; {
    private var _value: [Key: Value]

    private let queue = DispatchQueue(label: &quot;com.sync&quot;, qos: .userInitiated, attributes: .concurrent)

    public init(_ value: [Key: Value] = [:]) {
        _value = value
    }

    subscript(key: Key) -&gt; Value? {
        get { queue.sync { _value[key] } }
        set { queue.async(flags: .barrier) { self._value[key] = newValue } }
    }

    var count: Int { queue.sync { _value.count } }
}

class SynchronisedTests: XCTestCase {
    let iterations = 10_000

    func testSynchronised() throws {
        let dictionary = Synchronised([String: Int]())

        DispatchQueue.concurrentPerform(iterations: iterations) { i in
            let key = &quot;\(i)&quot;
            dictionary.value[key] = i
        }

        XCTAssertEqual(iterations, dictionary.value.count)  //  XCTAssertEqual failed: (&quot;10000&quot;) is not equal to (&quot;834&quot;)
    }

    func testSynchronisedDictionary() throws {
        let dictionary = SynchronisedDictionary&lt;String, Int&gt;()

        DispatchQueue.concurrentPerform(iterations: iterations) { i in
            let key = &quot;\(i)&quot;
            dictionary[key] = i
        }

        XCTAssertEqual(iterations, dictionary.count)        // success
    }
}
</code></pre>
"
"65143233","How to filter records with multiple cases for single column using Azure data flows","<p>I want to filter records where column1 not equals to (6,8,9,10) using Azure dataflows and Filter transformation.</p>
","<azure-data-factory>","2020-12-04 12:04:14","90","0","1","65175004","<p>Please use <code>!(equals(column1,6) || equals(column1,8) || equals(column1,9) || equals(column1,10))</code> expression in the Filter, it will filter records where column1 not equals to (6,8,9,10).</p>
"
"65141369","Flink SQL, how to get the first record and the last record by eventtime in kafka data flow and store it to a DB(such as GP, MySQL)?","<p><code>Flink SQL</code>, how to get the first record and the last record by eventtime in <code>Kafka</code> data flow and store it to a DB(such as <code>MySQL</code>)?</p>
<p>Addition, if new record in <code>Kafka</code> data flow coming, we should update the record in <code>MySQL</code>.</p>
<ol>
<li>Assumption, records in <code>Kafka</code> are as follows:</li>
</ol>
<pre><code>    {'word': 'hello', 'eventtime': '2020-12-04 16:00:00', 'appear_page': 5}
    {'word': 'hello', 'eventtime': '2020-12-04 16:05:00', 'appear_page': 10}
    {'word': 'hello', 'eventtime': '2020-12-04 16:10:00', 'appear_page': 15}
    {'word': 'are', 'eventtime': '2020-12-04 16:05:00', 'appear_page': 15}      
    {'word': 'hello', 'eventtime': '2020-12-04 16:15:00', 'appear_page': 20}
    {'word': 'are', 'eventtime': '2020-12-04 16:00:00', 'appear_page': 12}
    {'word': 'are', 'eventtime': '2020-12-04 16:10:00''appear_page': 18}

</code></pre>
<ol start=""2"">
<li>By <code>Flink SQL</code>, the results I expected are as follows:</li>
</ol>
<pre><code>    {'word': 'hello', 'eventtime': '2020-12-04 16:00:00', 'appear_page': 5}
    {'word': 'hello', 'eventtime': '2020-12-04 16:15:00', 'appear_page': 20}
    {'word': 'are', 'eventtime': '2020-12-04 16:00:00', 'appear_page': 12}
    {'word': 'are', 'eventtime': '2020-12-04 16:10:00', 'appear_page': 18}
</code></pre>
<ol start=""3"">
<li>We store these record in <code>MySQL</code>, assump that the result are as follows,</li>
</ol>
<pre><code>    |    word    |    first_appearance_time    |    first_appearance_page    |    last_appearance_time    |    last_appearance_page    |
    |    hello   |    2020-12-04 16:00:00      |            5                |    2020-12-04 16:15:00     |             20             |
    |    are     |    2020-12-04 16:00:00      |            12               |    2020-12-04 16:10:00     |             18             |
</code></pre>
<ol start=""4"">
<li>If a new record in <code>Kafka</code> is coming,</li>
</ol>
<pre><code>    {'word': 'are', 'eventtime': '2020-12-04 17:18:00', 'appear_page': 30}
</code></pre>
<ol start=""5"">
<li>I hope we can update the record of <code>are</code> in <code>MySQL</code>, the updating result are as follows:</li>
</ol>
<pre><code>    |    word    |    first_appearance_time    |    first_appearance_page    |    last_appearance_time    |    last_appearance_page    |
    |    hello   |    2020-12-04 16:00:00      |            5                |    2020-12-04 16:15:00     |             20             |
    |    are     |    2020-12-04 16:00:00      |            12               |    2020-12-04 17:18:00     |             30             |
</code></pre>
<p>I have some trouble in the 2th and 5th step, can anyone give some idea?</p>
","<apache-kafka><apache-flink><flink-sql>","2020-12-04 09:52:45","1243","0","1","65144244","<p>Deduplication with ordering by rowtime would be the easiest way, but this is supported in 1.12.  <a href=""https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/queries.html#deduplication"" rel=""nofollow noreferrer"">https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/queries.html#deduplication</a></p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE src (
  word STRING,
  eventtime TIMESTAMP(3),
  appear_page INT,
  WATERMARK FOR eventtime AS eventtime - INTERVAL '1' SECOND
) WITH (
  'connector' = 'kafka',
  ...
);

-- get last row by word key
SELECT word, eventtime, appear_page
FROM (
  SELECT *, ROW_NUMBER() OVER (PARTITION BY word ORDER BY eventtime DESC) AS rownum
  FROM src
) WHERE rownum = 1;

</code></pre>
<p>This query should also work in 1.11, but is not optimized into a deduplication, but a TopN operator which is less efficient.</p>
"
"65127434","How to add a private external dependency/jar from a gitlab maven repo to a google cloud data flow job","<p>Using the filesToStage option, can we add a dependency which is available on a private gitlab maven repository? If so, how can the token be specified in this option which will be needed to access the  dependency from the private repo.</p>
<p>If it is not possible to use filesToStage option to add a private external dependency to the data flow job then what are the other approaches that can be used to achieve this.</p>
","<java><google-cloud-dataflow><apache-beam>","2020-12-03 13:58:32","270","0","1","65226065","<p>When you build your pipeline, you would add your extra repository, and your extra dependency via your <code>pom.xml</code> or your <code>build.gradle</code> files.</p>
<p>Once you've added this extra dependency, you can run your pipeline as usual (via <code>mvn exec:java</code> or via <code>gradle run</code> or however you run your application - when the pipeline is constructed, Beam inspects the whole class path, and it uploads all of the files in it to be used in the workers' classpaths.</p>
<p>So you don't need to do anything other than define your dependency in <code>pom.xml</code> or <code>build.gradle</code>, and run your pipeline.</p>
<p>Let me know if this somehow won't fit your use case.</p>
"
"65072879","Blackberry UEM REST API 401 Unauthorised error","<p>Unable to perform any authorized API calls using the Blackberry UEM REST API service. I'm running a trial cloud instance of Blackbery UEM and calls to &quot;ping&quot; and &quot;authorization&quot; works. However, after obtaining the authorization code, using it on any REST calls fails with the error 401 Unauthorized.</p>
<p>The credential used is of an administrator with the &quot;Security Administrator&quot; role which has full permission. I have also tried the above in postman and also the sample PowerShell command, but the results are the same.</p>
<p>Does anyone know what's going on wrong here? Am I missing any configuration? I am going a bit crazy... Thanks.</p>
","<blackberry><mdm>","2020-11-30 11:40:10","267","1","1","65076942","<p>if you get a 401 error, its almost always an issue with the account/authorization your using.
Since your connecting to the cloud instance of UEM, it ONLY support OAuth authentication.  For this to work, you will need an token, not an authorization code.  Here is a devblog.blackberry.com entry on the steps: <a href=""https://devblog.blackberry.com/en/2020/07/new-authentication-option-for-rest-apis-oauth"" rel=""nofollow noreferrer"">https://devblog.blackberry.com/en/2020/07/new-authentication-option-for-rest-apis-oauth</a>.
Give that a try instead.</p>
"
"65025457","Enabling Scheduler for spring cloud data flow server in pcf","<p>We are using PCF to run our applications, To build data pipelines we thought of leveraging the Spring cloud data flow server, which is given as service inside PCF.</p>
<p>We created a DataFlow server by giving SQL server and maven repo details, and for the scheduler, we didn't provide any extra parameters while creating service, so by default, it is disabled.
Got some info from here, how to enable scheduler: <a href=""https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#_enabling_scheduling"" rel=""nofollow noreferrer"">https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#_enabling_scheduling</a></p>
<p>So I tried updating the existing Data Flow service with the below command:</p>
<pre><code>cf updat-service my-service -c '{&quot;spring.cloud.dataflow.features.schedules-enabled&quot;:true}'
</code></pre>
<p>the Data Flow server is restarted, but still the scheduler is not enabled to schedule the jobs.
When I check with this endpoint <code>GET /about</code> from the Data Flow server, I am still getting</p>
<pre><code>&quot;schedulesEnabled&quot;: false
</code></pre>
<p>in response body.</p>
","<spring-cloud-dataflow><pcf>","2020-11-26 16:01:37","263","0","1","65101458","<p>I am not sure why the SCDF service isn't updated with the schedules enabled property even after you update  service (as it is expected to have it enabled).</p>
<p>Irrespective of that you can try setting the following as environment property for SCDF service instance as well:</p>
<pre><code>    SPRING_CLOUD_DATAFLOW_FEATURES_SCHEDULES_ENABLED: true
</code></pre>
<p>Once the schedule is enabled, you need to make sure that you have the following properties set correctly as well:</p>
<pre><code>SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES: &lt;all-the-services-for-tasks-along-with-the-scheduler-service-instance&gt;
SPRING_CLOUD_SCHEDULER_CLOUDFOUNDRY_SCHEDULER_URL: &lt;scheduler-url&gt;
</code></pre>
"
"64983254","Composed Task is giving Error in Spring cloud data flow server","<p>When i create tasks independently they are working fine, but when i add the same tasks to composed task it is giving below error</p>
<p><code>Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataFlowOperations' defined in class path resource [org/springframework/cloud/task/app/composedtaskrunner/DataFlowConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.cloud.dataflow.rest.client.DataFlowOperations]: Factory method 'dataFlowOperations' threw exception; nested exception is java.lang.IllegalArgumentException: URI is not absolute</code></p>
<p>I followed the steps mentioned here : <a href=""https://dataflow.spring.io/docs/2.3.x/feature-guides/batch/composed-task/#registering-composed-task-runner"" rel=""nofollow noreferrer"">https://dataflow.spring.io/docs/2.3.x/feature-guides/batch/composed-task/#registering-composed-task-runner</a></p>
<p>This error i am getting only from SCDF server, hosted in pcf.
I am able to execute the composed task in my local scdf server.</p>
<p>This is the DSL i am trying
<code>timelabel1: timestamp &amp;&amp; timelabel2: timestamp</code></p>
","<spring-cloud-dataflow>","2020-11-24 09:03:56","315","0","1","64985994","<p>I am able to resolve this by passing below argument while launching task</p>
<p><code> --arguments &quot;--dataflow-server-uri=https:&lt;data-flow-serrver-url&gt;&quot;</code></p>
"
"64961874","How to expose my web application REST API using spring cloud data flow local","<p>Looking for help on my below query.</p>
<p>I have a web application with 2 REST API and i want to deploy this application to Spring Cloud Data Flow local server.  When other application in the local server calls this API, i want to execute my methods.<br />
I tried below option and it didn't worked.
a) register my web application as app
b) created a task using the app in step(a)
c) created &quot;HTTP|LOCAL-SERVER&quot; stream and deployed
d) accessed REST URL using postman
e) my REST API call is not initiated as i cannot see any logs neither in the SCDF log nor application logs.</p>
<p>Thank You.</p>
","<spring-cloud-dataflow>","2020-11-23 02:34:47","254","0","1","64963876","<p>Have you checked <a href=""https://github.com/spring-cloud-stream-app-starters/httpclient/blob/master/spring-cloud-starter-stream-processor-httpclient/README.adoc"" rel=""nofollow noreferrer"">http-client</a> processor?
That appears to solve your use case (if I understood your requirement correctly).</p>
"
"64948840","Questions in regard to global state/ structure of data flow","<p>Let's say I have a huge array of questions, answers, and a lot of other data all organized into 30 questions in an object and I want this to be the main source of all data regarding rendered text on screen, the correct answer, and other things but most importantly in this case I want it to keep track of right and wrong answers so that later on at the end of the test the user can see a list of answers they got right or wrong. Sort of like this:</p>
<pre><code>{
    id: nanoid(),
    title: &quot;Question One&quot;,
    correct:false,
    description: &quot;Which is the odd one out?&quot;,
    answer:'C',
    category:'spatial',
    type:'MCphoto',
    word:false,
    questionPhoto:{
      id:nanoid(), value:'./photos/spatialOne.png'
    }, {
    id: nanoid(),
    title: &quot;Question Two&quot;,
    correct:false,
    description: &quot;Which is the even one out?&quot;,
    answer:'C',
    category:'spatial',
    type:'MCphoto',
    word:false,
    questionPhoto:{
      id:nanoid(), value:'./photos/spatialOne.png'
}
</code></pre>
<p>I am making a somewhat basic test with React, and the main problem I keep running into is that this array gets shuffled, the questions are randomized, then they are rendered on the screen primarily through the Question component. in other words the info being passed down from parent to child through props and prop drilling etc should include a record of what were answered right or wrong. after being passed a few times down with MAJOR prop drilling (which is getting messy) I am having problems getting the list of right and wrong questions to be recorded and stored and then available to parent components and NOT just children. the component at the end of my test will be called a  but the problem is that structurally this component is rendered from the App component and therefore does NOT have access to the data that's been altered/updated in the Questions component. (refer to bottom of comment for app/component structure) the breakdown component that shows the final scores, what was right and wrong etc.... is NOT a child of the Questions component. I know this would typically be a time to use global state but how would I go about doing that? do you think that global state is the best approach whenever you are having problems passing data from child to parent? while I would like an answer to this particularly coding problem and I will show you more code... I am also wondering what people approach is to using redux, when to use it, and why I keep having problems with accessing data in my projects. I was reading about how redux isn't really common to use on personal projects but when i read about context it says that you can avoid prop drilling BUT the data STILL CAN ONLY be passed from parent to child, essentially not solving my problem here...My component that shows the score breakdown and lists questions answered right or wrong is NOT a child of the questions/test components therefore its unable to access the record of right and wrong scores etc. is this a structural problem in my projects? should ALL the data be passed down from the App component or a very higher level component so that it's available to every component child? to me that seems like it would make the App.js file rather (or any higher level component) cluttered with so many hooks, prop drilling etc. how do I avoid this?  I'm trying to write an action creator something like this:</p>
<pre><code>export const correctScore=(isCorrect, questions)=&gt;{
  return {
    type:'IS_CORRECT',
    payload:{
      isCorrect:isCorrect,
question:question
  }
}
}
</code></pre>
<p>Here I am making an action creator where the iscorrect value is set to true if the person gets answer right, and thus altering the huge questions file shown at beginning of my post.. the second parameter it takes in is the question itself, so that globally the test can keep track of which question was answered right or wrong</p>
<p>and then a reducer something like this:</p>
<pre><code>const INITIAL_STATE={
  isCorrect:null,
question:null
};

export const correctReducer=(state=INITIAL_STATE, action)=&gt;{
  switch (action.type){
    case &quot;IS_CORRECT&quot;:
    return {...state, isCorrect:action.payload.isCorrect, question:action.payload.question}

    default:
    return state
  }
}
</code></pre>
<p>in my project within the test part of project I have some logic that handles this state (obviously i wired it up to mapstate to props etc to make it available, but it still seems to be giving me problems... This the test part of the project. Essentially where the state is being altered through the setScore reducer. BUT after this is altered (and it doesn't even seem to be altering it properly)</p>
<pre><code>if(props.finalAnswer===props.questions.answer){
        props.correctScore(true, props.question) ****here i am passing true in to the action create and then the current question ****
        console.log(props)
        props.setScore(props.score+9)
        props.handleNextClick()
        reset()
      }
</code></pre>
<p>I guess my questions are do you think redux is the best approach to this project? it is over 2k lines of code at this point. and I'm constantly running into these situations where I can't pass props or pieces of data back UP to components being rendered in the App.js file.</p>
<p>Below is the App component so you can see how the project is structured. the ParentComponent is essentially the landing page of the app. you press a button on that page to &quot;begin the test&quot; and it leads u to the Question component. the question component is where all the data begins. it takes the data, does things with the data using QUITE few hooks. I will show you my questions component (or at least the logic for managing the questions object etc.).</p>
<p>Here is the question component. where all the data is received. Note that the breakdown component is being rendered in the App component therefore it is NOT able to receive any of the data, state, or manipulation of state with hooks BC the breakdown component isn't a child of the question component:</p>
<pre><code>import React, {useState, useEffect} from 'react'
import Answers from './Answers'
import Range from './Range'
import QFIB from './QFIB'
import AFIB from './AFIB'
import './Question.css'
import {BrowserRouter as Router, Switch, Link, Route } from 'react-router-dom'
import {test, tester} from './utils/shuffler'

const qAndA=[{

}]

const Question =({score, setScore})=&gt;{
  const [question, setQuestion]=useState('')
  const [questions, setQuestions]=useState(test)
  const [qIndex, setQIndex]=useState(0)
  const [answer, setAnswer]=useState('')
  const [id, setId]=useState('')
  const myRef=React.createRef()
  const [percentage, setPercentage]=useState(0)

useEffect(()=&gt;{
  window.scrollTo(0, 0);

},[qIndex])

  const [random, setRandom]=useState(null)


  const handleAnswerChange=e=&gt;{
    const currentQuestion=questions[qIndex]

    const answeredQuestion={
      ...currentQuestion,
      finalAnswer:e.target.value
    }
    const newQuestions=[...questions]
    newQuestions.splice(qIndex, 1, answeredQuestion);
    setQuestions(newQuestions);
  }
  let content=''
  const handleBackClick = () =&gt; setQIndex((i) =&gt; (i &gt; 0 ? i - 1 : 0));
  const handleNextClick = () =&gt;{
    setPercentage(percentage+(100/29))
    setQIndex((i) =&gt; (i &lt; questions.length - 1 ? i + 1 : i));
  }



  const scoring=e=&gt;{
    e.preventDefault()
  }

  const questionsAndAnswers=qAndA.map(questions=&gt;{
    return {question:questions.question, id:questions.id, answer:questions.answer}
  })

</code></pre>
","<reactjs><redux><react-redux>","2020-11-21 22:41:05","57","0","2","64949253","<pre><code>export const correctScore=(isCorrect, questions)=&gt;{
  return {
    type:'IS_CORRECT',
    payload:{
      isCorrect:isCorrect,
  }
}
}
</code></pre>
<p>here you didn't pass questions to the payload. I usually make separate action for separate payload.</p>
"
"64948840","Questions in regard to global state/ structure of data flow","<p>Let's say I have a huge array of questions, answers, and a lot of other data all organized into 30 questions in an object and I want this to be the main source of all data regarding rendered text on screen, the correct answer, and other things but most importantly in this case I want it to keep track of right and wrong answers so that later on at the end of the test the user can see a list of answers they got right or wrong. Sort of like this:</p>
<pre><code>{
    id: nanoid(),
    title: &quot;Question One&quot;,
    correct:false,
    description: &quot;Which is the odd one out?&quot;,
    answer:'C',
    category:'spatial',
    type:'MCphoto',
    word:false,
    questionPhoto:{
      id:nanoid(), value:'./photos/spatialOne.png'
    }, {
    id: nanoid(),
    title: &quot;Question Two&quot;,
    correct:false,
    description: &quot;Which is the even one out?&quot;,
    answer:'C',
    category:'spatial',
    type:'MCphoto',
    word:false,
    questionPhoto:{
      id:nanoid(), value:'./photos/spatialOne.png'
}
</code></pre>
<p>I am making a somewhat basic test with React, and the main problem I keep running into is that this array gets shuffled, the questions are randomized, then they are rendered on the screen primarily through the Question component. in other words the info being passed down from parent to child through props and prop drilling etc should include a record of what were answered right or wrong. after being passed a few times down with MAJOR prop drilling (which is getting messy) I am having problems getting the list of right and wrong questions to be recorded and stored and then available to parent components and NOT just children. the component at the end of my test will be called a  but the problem is that structurally this component is rendered from the App component and therefore does NOT have access to the data that's been altered/updated in the Questions component. (refer to bottom of comment for app/component structure) the breakdown component that shows the final scores, what was right and wrong etc.... is NOT a child of the Questions component. I know this would typically be a time to use global state but how would I go about doing that? do you think that global state is the best approach whenever you are having problems passing data from child to parent? while I would like an answer to this particularly coding problem and I will show you more code... I am also wondering what people approach is to using redux, when to use it, and why I keep having problems with accessing data in my projects. I was reading about how redux isn't really common to use on personal projects but when i read about context it says that you can avoid prop drilling BUT the data STILL CAN ONLY be passed from parent to child, essentially not solving my problem here...My component that shows the score breakdown and lists questions answered right or wrong is NOT a child of the questions/test components therefore its unable to access the record of right and wrong scores etc. is this a structural problem in my projects? should ALL the data be passed down from the App component or a very higher level component so that it's available to every component child? to me that seems like it would make the App.js file rather (or any higher level component) cluttered with so many hooks, prop drilling etc. how do I avoid this?  I'm trying to write an action creator something like this:</p>
<pre><code>export const correctScore=(isCorrect, questions)=&gt;{
  return {
    type:'IS_CORRECT',
    payload:{
      isCorrect:isCorrect,
question:question
  }
}
}
</code></pre>
<p>Here I am making an action creator where the iscorrect value is set to true if the person gets answer right, and thus altering the huge questions file shown at beginning of my post.. the second parameter it takes in is the question itself, so that globally the test can keep track of which question was answered right or wrong</p>
<p>and then a reducer something like this:</p>
<pre><code>const INITIAL_STATE={
  isCorrect:null,
question:null
};

export const correctReducer=(state=INITIAL_STATE, action)=&gt;{
  switch (action.type){
    case &quot;IS_CORRECT&quot;:
    return {...state, isCorrect:action.payload.isCorrect, question:action.payload.question}

    default:
    return state
  }
}
</code></pre>
<p>in my project within the test part of project I have some logic that handles this state (obviously i wired it up to mapstate to props etc to make it available, but it still seems to be giving me problems... This the test part of the project. Essentially where the state is being altered through the setScore reducer. BUT after this is altered (and it doesn't even seem to be altering it properly)</p>
<pre><code>if(props.finalAnswer===props.questions.answer){
        props.correctScore(true, props.question) ****here i am passing true in to the action create and then the current question ****
        console.log(props)
        props.setScore(props.score+9)
        props.handleNextClick()
        reset()
      }
</code></pre>
<p>I guess my questions are do you think redux is the best approach to this project? it is over 2k lines of code at this point. and I'm constantly running into these situations where I can't pass props or pieces of data back UP to components being rendered in the App.js file.</p>
<p>Below is the App component so you can see how the project is structured. the ParentComponent is essentially the landing page of the app. you press a button on that page to &quot;begin the test&quot; and it leads u to the Question component. the question component is where all the data begins. it takes the data, does things with the data using QUITE few hooks. I will show you my questions component (or at least the logic for managing the questions object etc.).</p>
<p>Here is the question component. where all the data is received. Note that the breakdown component is being rendered in the App component therefore it is NOT able to receive any of the data, state, or manipulation of state with hooks BC the breakdown component isn't a child of the question component:</p>
<pre><code>import React, {useState, useEffect} from 'react'
import Answers from './Answers'
import Range from './Range'
import QFIB from './QFIB'
import AFIB from './AFIB'
import './Question.css'
import {BrowserRouter as Router, Switch, Link, Route } from 'react-router-dom'
import {test, tester} from './utils/shuffler'

const qAndA=[{

}]

const Question =({score, setScore})=&gt;{
  const [question, setQuestion]=useState('')
  const [questions, setQuestions]=useState(test)
  const [qIndex, setQIndex]=useState(0)
  const [answer, setAnswer]=useState('')
  const [id, setId]=useState('')
  const myRef=React.createRef()
  const [percentage, setPercentage]=useState(0)

useEffect(()=&gt;{
  window.scrollTo(0, 0);

},[qIndex])

  const [random, setRandom]=useState(null)


  const handleAnswerChange=e=&gt;{
    const currentQuestion=questions[qIndex]

    const answeredQuestion={
      ...currentQuestion,
      finalAnswer:e.target.value
    }
    const newQuestions=[...questions]
    newQuestions.splice(qIndex, 1, answeredQuestion);
    setQuestions(newQuestions);
  }
  let content=''
  const handleBackClick = () =&gt; setQIndex((i) =&gt; (i &gt; 0 ? i - 1 : 0));
  const handleNextClick = () =&gt;{
    setPercentage(percentage+(100/29))
    setQIndex((i) =&gt; (i &lt; questions.length - 1 ? i + 1 : i));
  }



  const scoring=e=&gt;{
    e.preventDefault()
  }

  const questionsAndAnswers=qAndA.map(questions=&gt;{
    return {question:questions.question, id:questions.id, answer:questions.answer}
  })

</code></pre>
","<reactjs><redux><react-redux>","2020-11-21 22:41:05","57","0","2","64949774","<p>After I combine the reducers and return an object like below:</p>
<pre><code>export default combineReducers({
  info:correctReducer
})

</code></pre>
<p>I'm forgetting to access them in mapStateToprops through info:</p>
<pre><code>const mapStateToProps=(state)=&gt;{
  return {
    isCorrect:state.info.isCorrect,
    question:state.info.question
  }
}
</code></pre>
<p>I was instead thinking I could access it directly and forgot that I had to go through the combine reducers naming (in this case info) here is what I was doing:</p>
<pre><code>const mapStateToProps=(state)=&gt;{
  return {
    isCorrect:state.isCorrect,
    question:state.question
  }
}
</code></pre>
<p>and it kept showing up as undefined! Really glad I sorted that.</p>
"
"64716489","How to prevent sharing files from ""On My iPhone"" folder","<p>I have a xamarin forms app that can download files to &quot;on my iphone&quot; folder. But when I download a file on phone, I can go there and I can share it with another app.</p>
<p><a href=""https://i.stack.imgur.com/C09xW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/C09xW.png"" alt=""enter image description here"" /></a></p>
<p>But I want to prevent this. When I download a file from my app, I want the file not to be uploaded to another device from &quot;on my iphone&quot;.</p>
<p>How to prevent this? Probably I can prevent this with mdm, but how?
Is there a way to prevent it with mdm managed app configuration. Some of my customers said that we can prevent this with the plist file in mdm. But I have very little information about mdm. How to do it with mdm?
I need a solution for ios and android. But especially for ios.</p>
<p>Thank you in advance.</p>
","<xamarin><xamarin.forms><xamarin.android><xamarin.ios><mdm>","2020-11-06 14:31:46","147","0","1","64788835","<p>Thank you for reply Jack.</p>
<p>I looked your sending thread and I tried it. But it didn't work for me.
I created MyUINavigationItem that is inherited from UINavigationItem. And I override SetRightBarButtonItem null. After I override UINavigationItem on PdfPreviewController that is inherited from QLPreviewController. And I set NavigationItem as MyUINavigationItem object. But it doesn't work for me. My codes is like these:</p>
<pre><code>public class DocumentView : IDocumentView
{
    void IDocumentView.DocumentView(string file, string title)
    {
         PdfPreviewController previewController = new PdfPreviewController();
         if (File.Exists(file))
         {
              previewController.NavigationItem.SetRightBarButtonItem(null, false); //I tried this line as both comment and not comment but it didn't work
              previewController.NavigationItem.SetRightBarButtonItems(null, false); //I tried this line as both comment and not comment but it didn't work
              previewController.DataSource = new PDFPreviewControllerDataSource(NSUrl.FromFilename(file), title);
              UIApplication.SharedApplication.KeyWindow.RootViewController.PresentViewController(previewController, true, null);

          }
    }
}
</code></pre>
<pre><code>public class PdfPreviewController : QLPreviewController
{
     MyUINavigationItem item = new MyUINavigationItem();
     public override UINavigationItem NavigationItem =&gt; item;
}
</code></pre>
<pre><code>public class MyUINavigationItem : UINavigationItem
{
    public MyUINavigationItem()
    {

    }

    public override void SetRightBarButtonItem(UIBarButtonItem item, bool animated)
    {
        //base.SetRightBarButtonItem(item, animated); //I tried this line as comment but it didn't work or
        //base.SetRightBarButtonItem(null, animated); //I tried this line but it didn't work
    }

    public override void SetRightBarButtonItems(UIBarButtonItem[] items, bool animated)
    {
        //base.SetRightBarButtonItems(items, animated); //I tried this line as comment but it didn't work or
        //base.SetRightBarButtonItems(null, animated); //I tried this line but it throwed exception
        //base.SetRightBarButtonItems(new UIBarButtonItem[0], animated); //I tried this line but it didn't work
    }
}
</code></pre>
<p>But after your suggestion I looked for enable or hide share button. And I could access child view controller of UINavigationController and I set RightBarButtonItem enable value false.</p>
<pre><code>public class PdfPreviewController : QLPreviewController
{
        public override void ViewDidAppear(bool animated)
        {
            try
            {
                ActionMenuControl();
            }
            catch (Exception ex)
            {
            }
        }

        public override void ViewDidLayoutSubviews()
        {
            try
            {
                ActionMenuControl();
            }
            catch (Exception ex)
            {
            }
        }

        public void ActionMenuControl()
        {
            try
            {
                if (this.ChildViewControllers.Length != 0)
                {
                    var navigationController = this.ChildViewControllers.First() as UINavigationController;
                    if (navigationController.View.Subviews.Length != 0)
                    {
                        var layoutContainerView = navigationController.View.Subviews.FirstOrDefault(x =&gt; x is UINavigationBar) as UINavigationBar;
                        if (layoutContainerView != null) 
                        { 
                            if (layoutContainerView.Items.Length != 0)
                            {
                                var item = layoutContainerView.Items[0];

                                if (item.RightBarButtonItems.Length != 0)
                                    item.RightBarButtonItem.Enabled = false;
                            }
                        }
                    }

                    var toolbar = navigationController.Toolbar;
                    if (toolbar != null) 
                    { 
                        if(toolbar.Items.Length != 0)
                        {
                            toolbar.Items[0].Enabled = false;
                        }
                    }
                }
            }
            catch (Exception ex)
            {
                
            }
        }
}
</code></pre>
<p>It works for right bar. But if opens mp3 files, share button appear on toolbar (on bottom) And I didn't access it. How can I do it for mp3 files? How to access toolbar? (Actually it doesn't work when opens and not move it. But if I open it and move it, it works (because it enters ViewDidLayoutSubviews events))</p>
<p>Sorry for long reply. But I wanted to tell about what I did. Because maybe I did missed something.</p>
"
"64702141","window.navigator properties return null on iPad with MDM-restrictions","<p>I am trying to detect if a client is an iPad, and to do that I'm trying to use <code>window.navigator.platform</code> and <code>window.navigator.maxTouchPoints</code>.</p>
<p>I'm trying to do this from Vue, by setting it as a computed property.</p>
<p>One device I have to test this with is an MDM-device, an iPad running the latest iOS and using Safari as the browser.</p>
<p>This is my computed property:</p>
<pre class=""lang-js prettyprint-override""><code>// ...
computed: {
    isIpad() {
        return navigator.platform === 'MacIntel' &amp;&amp; navigator.maxTouchPoints &gt; 0;
    }
}
</code></pre>
<p>And this is the view:</p>
<pre class=""lang-html prettyprint-override""><code>&lt;div class=&quot;feature&quot;&gt;
    &lt;div v-if=&quot;isIpad&quot; class=&quot;feature-desktop&quot;&gt;This feature is only available on desktops.&lt;/div&gt;
&lt;/div&gt;
</code></pre>
<p>In my vue-inspector on MacOS/Firefox the computed property is correct, returning <code>false</code> as expected. When I try to emulate an iPad through Firefox, it returns <code>true</code>, as expected. However, if I open the page using Safari on my MDM-enabled iPad, this property returns <code>false</code>.</p>
<p>Sadly, I cannot connect with developer tools as the iPad is locked down, but I can manually put javascript on the page to print out the result of both <code>navigator.platform</code> and <code>navigator.maxTouchPoints</code>, and they return the expected <code>MacIntel</code> platform and <code>5</code> touch points.</p>
<pre class=""lang-js prettyprint-override""><code>&lt;script type=&quot;application/javascript&quot;&gt;
    document.getElementById('app').innerHTML = navigator.platform + ' ' + navigator.maxTouchPoints; // Prints MacIntel and 5
&lt;/script&gt;
</code></pre>
<p>Does anyone have any experience with this sort of thing? I've yet to attempt setting a <code>data</code>-property with <code>mounted()</code>.</p>
","<vue.js><ipad><safari><mdm><navigator>","2020-11-05 17:18:47","382","0","1","64772315","<p>Using the <code>mounted</code> event handler to set a <code>data</code>-property solved my problem.</p>
"
"64701878","Disabling and enabling google play protect scanning via intent (or any remote method)","<p>I'm attempting to install some internally developed applications via MDM to a group of Android devices, but because we won't be utilizing the play store for those applications, I get constant &quot;Blocked by Play Protect&quot; and the users have to select &quot;Install Anyway&quot; which is leading to issues as people are not selecting this option.</p>
<p>I've noticed that my MDM supports sending Android &quot;Intents&quot; and I was wondering if there was any possible way to utilize this to disable the play protect scanning option so that I can install the applications/updates and then re-enable the play scanning services at a later date?</p>
<p>Or honestly any other remote method suggestions would be great.</p>
","<android><android-intent><mdm><device-management><google-play-protect>","2020-11-05 17:01:54","411","1","1","64702085","<p>No its not possible . submit your application to google so you don't receive block message
<a href=""https://support.google.com/googleplay/android-developer/contact/protectappeals"" rel=""nofollow noreferrer"">https://support.google.com/googleplay/android-developer/contact/protectappeals</a></p>
"
"64559045","ready/valid vs 2-way handshaking vs 4-way handshaking","<p>I am confused about whether ready/valid handshaking is functionally equivalent to req/ack (2-way) handshaking? By being functionally equivalent, I mean that we can perform data transfers with ready/valid handshaking in all the cases in which we can do with req/ack (2-way) handshaking and vice versa? Are there any scenarios in which one scheme will work while the other will not?</p>
<p>As an extension to the same question, is req/ack (2-way) functionally equivalent to req/ack (4-way) handshaking? Mostly, I have found the difference to be in terms of hardware required and of course speed. Are there any scenarios in which we are bound to use req/ack (4-way) or req/ack (2-way) for that matter.</p>
<p>In summary, I want to build a connection between the three schemes -- where will one scheme fail and the other scheme will work.</p>
<p><strong>The question is in the context of both synchronous and asynchronous designs</strong></p>
<p><a href=""https://www.slideserve.com/kyna/self-timed-and-asynchronous-design"" rel=""nofollow noreferrer"">A reference to handshaking.</a></p>
","<cpu-architecture><handshake><data-synchronization>","2020-10-27 16:41:59","1447","1","1","67226736","<p>An old question but I'll answer anyway for the sake of future similar queries.</p>
<p><strong>1. When are they used?</strong></p>
<p>The 4-phased or 2-phased <code>req/ack</code> protocols are necessary in the absence of clock, in asynchronous logic. The signals are responses to one another: both are responses in the 4-phased protocol, <code>ack</code> is the only response in the 2-phased protocol.</p>
<p>The <code>valid/ready</code> (or equivalent) protocol doesn't need the return to zero phases thanks to the synchronization on the rising edge of the clock, it can only be used in synchronous logic. Furthermore, only one phase is required since the <code>valid</code> and <code>ready</code> are not a response to each other on the current transfer, they are simply updated on the next cycle.</p>
<p><strong>2. How do they compare?</strong></p>
<p>All 3 protocols controls the transfer of data accurately, they can transmit back pressure too. So <strong>yes, they are functionally equivalent</strong>.</p>
<p><em>4-phased <code>req/ack</code> signals switch twice as much, which is not great from a <strong>performance</strong> and <strong>power consumption</strong> point of view. But synchronous circuits have block buffers that may consume a lot when the fan-out is big, which have other consequences like EMI, the need of local capacitors and/or the use of spread spectrum techniques. It all depends on the scale of the circuit and the technology. In general though, it will be harder to maintain the same throughput with a protocol that needs to switch signals at twice the rate.</em></p>
<p>2-phased <code>ack/req</code> provides the same functionality too but the implementation has its disadvantages, the logic to handle high/low may be more complex vs the natural 4-phased protocol. It requires XOR gates and a reference register to output transitions instead of states (changing polarity instead of indicating &quot;I'm ready&quot;). It requires XOR gates to detect if a change of state should occur.</p>
<p><em>From a <strong>performance</strong> point of view, it takes more resources and increases the critical path but reduces the number of phases. It's not clear whether the outcome will improve the performances, to check but it likely depends on the technology. One justification would be the transmission of the protocol over long or loaded lines of limited bandwidth, where it could be interesting to increase the rate over those lines at the expense of local gate area.</em></p>
<p><strong>3. Are they interchangeable?</strong></p>
<p>It is possible to interface two <code>valid/ready</code> blocks with a <code>req/ack</code> protocol in a synchronous circuit. However, in order to interface two <code>req/ack</code> blocks with a <code>valid/ready</code> protocol in an asynchronous circuit, you'd need a third synchronization signal for the notion of transfer cycle.</p>
<p>The question is moot in most cases because there is a penalty and no advantage. <code>valid/ready</code> or equivalent will be used in synchronous circuits. In asynchronous circuits, you have to choose between 2-phased or 4-phased <code>req/ack</code>.</p>
<p>It is possible to interface 2-phased and 4-phased <code>req/ack</code> blocks with XOR and S/R latches in asynchronous circuits.</p>
<p><strong>In summary</strong></p>
<p>They are functionally equivalent when they are used in their respective synchronous / asynchronous implementations, but cannot always be used in the other domain (<code>req/ack</code> can be used in synchronous domain but underperforms, <code>valid/ready</code> cannot be used as such in asynchronous domain). Each implementation has a different impact on performance, power consumption and resource utilization.</p>
"
"64552905","Is there a way of using two authentication schemes for the same controller?","<p>I would like to use my Open Data (OData) Controllers to access data from a third party grid tool in MVC (using cookie authentication), and I would also like to use the same controllers to synchronize data with a mobile Xamarin app (using token authentication).</p>
<p>Here is an extract from my startup file...</p>
<pre><code>            services.AddAuthentication(options =&gt;
        {
            //options.DefaultScheme = CookieAuthenticationDefaults.AuthenticationScheme;
            //options.DefaultChallengeScheme = JwtBearerDefaults.AuthenticationScheme;
            //options.DefaultChallengeScheme = CookieAuthenticationDefaults.AuthenticationScheme;
        })
            .AddCookie(options =&gt; options.SlidingExpiration = true)
            .AddJwtBearer(options =&gt;
            {
                options.TokenValidationParameters = new TokenValidationParameters
                {
                    ValidateIssuer = false,
                    ValidateAudience = false,
                    ValidateLifetime = false,
                    RequireExpirationTime = false,
                    ValidIssuer = authOptions.Issuer,
                    ValidAudience = authOptions.Audience,
                    IssuerSigningKey = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(authOptions.SecureKey))
                };
            });
</code></pre>
<p>and the relevant bits of the controllers look like the following...</p>
<pre><code> [Authorize(AuthenticationSchemes = CookieAuthenticationDefaults.AuthenticationScheme), Authorize(AuthenticationSchemes = JwtBearerDefaults.AuthenticationScheme)]
 public class FooODataController : ODataController {...}
</code></pre>
<p>The problem is that both cookies and bearer tokens are always challenged, so that the user is never authenticated.  Does anyone know a way of implementing this so that the user is authenticated if either of the challenges are successful?</p>
","<asp.net-core><authentication><odata><syncfusion><data-synchronization>","2020-10-27 10:54:37","282","0","1","64726167","<p>It's because there is two [Authorize(..), Authorize(..)] attributes, instead you should have one with multiple schemas: (see <a href=""https://learn.microsoft.com/en-us/aspnet/core/security/authorization/limitingidentitybyscheme?view=aspnetcore-3.1"" rel=""nofollow noreferrer"">this</a>)</p>
<pre class=""lang-cs prettyprint-override""><code>[Authorize(AuthenticationSchemes = CookieAuthenticationDefaults.AuthenticationScheme + &quot;,&quot; + JwtBearerDefaults.AuthenticationScheme)]
public class FooODataController : ODataController 

</code></pre>
"
"64490522","How to post a message to a destination from a spring cloud data flow task?","<p>Is there a correct/preferred way to send a message from a Task to a Destination using Spring Cloud Data Flow?</p>
<p>We have an existing stream with destinations, and would like a scheduled task to also feed messages into the stream via one of the destinations.</p>
<p>Options we have considered so far:</p>
<ol>
<li>Create a new stream with an http source application to feed into the destination, and then use a rest template in the task to post data to the http source. Concern with this approach is that the ip address for the http source could be ephemeral if stream/pod is redeployed.</li>
<li>Manually configure rabbit in the task to send messages directly to the destination (exchange). Concern with this approach is that feels a bit dirty and may be misuse of the underlying messaging system that data flow/streams are using.</li>
</ol>
","<spring-cloud-stream><spring-cloud-dataflow><spring-rabbit><spring-cloud-task>","2020-10-22 21:03:23","268","0","1","64490926","<p>I don't see any real issues with #2 but you could also configure an output binding and use the <code>StreamBridge</code> to send to it.</p>
<p><a href=""https://docs.spring.io/spring-cloud-stream/docs/3.0.8.RELEASE/reference/html/spring-cloud-stream.html#_sending_arbitrary_data_to_an_output_e_g_foreign_event_driven_sources"" rel=""nofollow noreferrer"">Sending arbitrary data to an output (e.g. Foreign event-driven sources)</a></p>
"
"64360419","Source-to-target mapping document","<p>When we say source-to-target mapping document, does it typically include all the mappings between the different layers?</p>
<p>For example, given the following lineage:</p>
<p>source systems -&gt; staging tables -&gt; EDW -&gt; data marts</p>
<p>Would there be 3 separate mapping documents?
(i.e., 1. source systems to staging tables 2. staging tables to EDW and 3. EDW to data marts)</p>
","<mapping><data-modeling><datamodel><data-management><master-data-management>","2020-10-14 19:38:48","283","0","1","65454072","<p>It depends on how you manage your documentation but general practice is 2 different source 2 target document:</p>
<ol>
<li>Source System --&gt; EDW</li>
<li>EDW --&gt; Data Mart</li>
</ol>
"
"64332170","How to support my app in Citrix MDM solution","<p>I have application on App store. I want to add support for Citrix MDM solution, which will allow end user to install application as in-office app.</p>
<p>Want to know what are the steps I need to perform in application?</p>
<p>I have gone through the details on <a href=""https://www.citrix.com/"" rel=""nofollow noreferrer"">citrix</a> and got following information in bits and pieces and have some question on top of information</p>
<ol>
<li>Need to use the <a href=""https://developer.cloud.com/device-management"" rel=""nofollow noreferrer"">MAM SDK</a> in iOS app. From where I should Download the SDK? Is it freely available for POC purpose?</li>
<li>If I integrate the SDK what are the changes I need to do in application? any sample application available?</li>
<li>Need to <a href=""https://www.youtube.com/watch?v=RZrMzMH4O1E"" rel=""nofollow noreferrer"">wrap the iOS application</a> . Is it required if I use MAM SDK?</li>
<li>My application communicate with my server and has user authentication. Do I need to change anything on my server?</li>
<li>Is dummy/temporary server setup of Citrix is available to test my application?</li>
</ol>
","<ios><mdm><citrix>","2020-10-13 09:12:07","100","0","1","66147806","<p>Let me tackle each question.</p>
<ol>
<li><p>You'll want to download the <a href=""https://www.citrix.com/downloads/citrix-endpoint-management/product-software/mdx-toolkit.html"" rel=""nofollow noreferrer"">MAM SDK</a> from the citrix.com web site.  To do so though you'll need to first create a user account.  It is free to download.</p>
</li>
<li><p>The changes to your iOS app are fairly straightforward.  I won't repeat them here, but in the MAM SDK download you'll find step-by-step instructions on what you need to do.  There is a sample app with source code provided.</p>
</li>
<li><p>You no longer need to MDX Wrap your iOS application.  That's the older way of getting your apps managed by Citrix Endpoint Management.  The new MAM SDK approach replaces that mechanism.</p>
</li>
<li><p>There are no changes needed to your authentication.</p>
</li>
<li><p>There's no dummy server available, but you can request a trial site.  Create an account at <a href=""https://onboarding.cloud.com/"" rel=""nofollow noreferrer"">Citrix Cloud</a> and request a trial of Endpoint Management.</p>
</li>
</ol>
<p>Source: I'm an engineer for the Citrix Endpoint Management product.</p>
"
"64259415","How to identify unsynced entities? Is the client the one who generates the Ids?","<p>Let's say I want to synchronize (using HTTP protocol) an entity called <code>Person</code>. So, the persons in client (mobile/desktop/whatever) is a mirror-replicate of persons exist in server's database. Obviously, server owns all persons and client owns only the specific user's persons.</p>
<p>Consider the following case.</p>
<p>Client is offline. While he is offline, he creates a <code>Person</code> and because he can't connect to the  server, he keeps this person to a local storage. Let's say a local database (SQLite or whatever). <strong>The moment this happens, how is/should this person identified?</strong></p>
<p>Before I started implementing the whole thing, I thought the server should be the one that generates the IDs of persons coming to him. However, when I started implementing, I start facing this problem.</p>
<p>In case the server generates the IDs, since the person is never seen by the server, client must give it an ID in order to be able to find the person and obviously use this ID to store the person into his local storage. Now, when client comes online, he will send the person to server. Server, gives it an ID and stores it in his own database. After that, client will request for any kind of person changes that happened after his last time of synced and server will return this specific person.</p>
<p>Lets make an example. Client is offline, creates 100 Persons and stores them to his local storage. Person 1, Person 2, Person 3, etc... Now, he gets connected to the server and he sends all 100 persons. Since the connection happens over HTTP, client makes a post request to <code>post-persons</code> endpoint. Then, server generates IDs (either incremental or UUIDs) and probably change some other properties as well. Now, client access <code>get-persons</code> endpoint and he sees 100 updated persons, each one of them having a new ID that he could not know about. <strong>How does the client know which of these persons correspond to persons that client already has?</strong> Removing the old client's 100 persons, and inserting 100 new with server ID seems unorthodox. With other words, Person 1 known by the client, is stored as Person [uuid] in server, and server returns it as Person [uuid]. How client knows that his Person 1 corresponds to Person [uuid]? A solution might be, to send client's IDs to server, and server will respond like Person [uuid] 1. Now client knows, his 1 is this one. And to me, this seems even more unorthodox.</p>
<p>Second option is to have the clients generate UUIDs either they are offline, either online. This solution seems the &quot;simplest&quot; approach when it comes to implementation by my side. Client creates Person [uuid]. When he comes online, he sends it to server. After that client accesses <code>get-persons</code> and he gets as respond an update Person [uuid]. He easily identifies &amp; stores it in his local storage. The server does not generate any kind of ID for persons.</p>
<p>Is there anything I am missing? Till now, I thought servers are the ones that generate the IDs of syncable entities, but I think the second approach is easier to implement and more comprehensive. But does it introduce any kind of &quot;danger&quot; for later?</p>
<p>There are no explicit requirements when it comes to what kind of ID I will use to client, or the server. However, I am aware of the trade-offs using UUIDs over simple increment numbers.</p>
<p>The stack (even though I consider it irrelevant):</p>
<ul>
<li>Spring boot as server among Hibernate and MySQL</li>
<li>Client with Hibernate and H2 standalone as local storage</li>
<li>Everything Java 8</li>
</ul>
","<java><database><synchronization><primary-key><data-synchronization>","2020-10-08 09:17:00","137","0","1","64260362","<p>There are other possible solutions, and the best one depends on your particular situation - whether you need to maintain local relations, etc.</p>
<hr />
<p><strong>The first solution keeps using IDs with no logic around it on the server.</strong></p>
<p>Don't generate ID at all and store persons without it (if possible), and when synced, the server returns IDs, and you change them locally.</p>
<p>Thus, it's simple to know which persons are already in sync with the server (those with ID) and which are not yet known to the server.</p>
<p>It's also simple to implement INSERT/UPDATE logic.</p>
<p>However, this solution may be a problem if you need to maintain local relations.</p>
<p>When you need local relations, you can generate temporary IDs. Let's say that you want a numeric ID, and so all positive IDs are those that are already synced, and negative IDs are those that are temporary.</p>
<p>When synced with the server, you obtain new IDs from the server (positive ones), and you simply rewrite all IDs in your local database. That's, however, a bit error-prone as you have to be sure to update all relations.</p>
<p>You can also have localId and serverId. Technically, you only need serverId to be globally unique to store persons on your server. You keep serverId empty and fill it once the entity is synchronized with the server (the server returns it). And you can generate localId as you want and use it for local relations. However, this can be a problematic approach if you need to have several clients in sync, and each of them would like to generate its own local IDs.</p>
<hr />
<p><strong>The second approach is to use ID pools. It needs a bit of work on the server and storing assigned pools.</strong></p>
<p>You can allocate a pool of IDs for the given client. The client must be online at least once, and the server sends its unique offset.</p>
<p>Depending on your needs, you can have, for example, an 8-byte identifier (64 bits). The first 36 bits can be used for the offset, so your server can manage 2^36 clients. And the last 28 bits are free to use by the client.</p>
<p>So the client just increments the internal counter whenever it needs a new ID and adds the unique offset =&gt; gets a globally unique identifier.</p>
"
"64235654","Is there where to do line breaks in the expression builder of azure data flow?","<p>I am using the Expression Builder of Derived Column setting of the Azure Data Flow Activity. I am trying to code ac line break. So it looks like:</p>
<p>Value1
Value2</p>
<p>What is the syntax to code this?</p>
","<line-breaks><azure-data-factory><expressionbuilder>","2020-10-07 00:44:52","1668","0","1","64236203","<p>You can use <code>'\n'</code> to code ac line break in the DerivedColumn.</p>
<p>In my case, I use <code>concat(Column1,concat('\n',Column2))</code> epression to concat two columns.
<a href=""https://i.stack.imgur.com/6IjZ0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6IjZ0.png"" alt=""enter image description here"" /></a></p>
<p>So I get the results, the Column3 looks like:<br />
Value1 Value2</p>
<p><a href=""https://i.stack.imgur.com/Of9mn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Of9mn.png"" alt=""enter image description here"" /></a></p>
"
"64218004","Azure Data Sync Error - Member Database is Read Only","<p>I have configured Azure Data Sync between our SQL on-prem and Azure SQL. Sync direction is to Hub (Azure) and conflict resolution is set to Member Win. Sync group has been configured successfully, and I have selected the tables I want to sync in the Hub (I synced schema previously using Data Migration). I have 83 tables that are selected for synchronization.</p>
<p><strong>Problem:</strong> When starting the synchronization, I receive the following error:</p>
<blockquote>
<p>Database provisioning failed with the exception &quot;SqlException Error Code: -2146232060 - SqlError Number:3906, Message: Failed to update database &quot;*****&quot; because the database is read-only. SqlError Number:2759, Message: CREATE SCHEMA failed due to previous errors.</p>
</blockquote>
<p>Why would Data Sync be attempting to <code>CREATE SCHEMA</code> on my read-only on-prem instance, when I have configured it to synchronize in the other direction?</p>
<p>Appreciate any insight you can offer,</p>
<p>Cheers,</p>
","<sql><azure><data-synchronization><azure-data-sync>","2020-10-06 01:04:32","307","2","1","67356253","<p>You need to give dbowner access to role you are using with agent so that it can create a schema initially. It will create schema and tables with datasync.tablename that will have sync logs for respective table. If you sync 10 tables then 10 such tables will be created.</p>
"
"64215085","SSIS Truncation may occur due to inserting data from data flow column","<p>I built Foreach Loop Container in SSIS that extracts data from multiple excel (*.xlsm) files in a directory, and then writes the results in another location.</p>
<p>[Final BCS Description] is causing me problems. I tried using data conversion DT_WSTR.</p>
<p><a href=""https://i.stack.imgur.com/E1CjN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E1CjN.png"" alt=""enter image description here"" /></a></p>
<p>I am getting error:</p>
<blockquote>
<p>Truncation may occur due to inserting data from data flow column
&quot;Copy of Final BSC Description&quot; with a length of 2000 to database
column &quot;Final BSC Description&quot; with a length of 255.</p>
</blockquote>
<p>This doesn't make sense. &quot;Final BSC Description&quot; in the sources is greater than 255 and there is no cap in destination. Could you someone provide a solution or a work around.</p>
<p>Error message below.</p>
<pre><code>SSIS package  starting.
Information: 0x4004300A at Load Excel Files, SSIS.Pipeline: Validation phase is beginning.
Warning: 0x802092A7 at Load Excel Files, Excel Destination [12]: Truncation may occur due to inserting data from data flow column &quot;Copy of Final BSC Description&quot; with a length of 2000 to database column &quot;Final BSC Description&quot; with a length of 255.
Information: 0x4004300A at Load Excel Files, SSIS.Pipeline: Validation phase is beginning.
Warning: 0x802092A7 at Load Excel Files, Excel Destination [12]: Truncation may occur due to inserting data from data flow column &quot;Copy of Final BSC Description&quot; with a length of 2000 to database column &quot;Final BSC Description&quot; with a length of 255.
Warning: 0x80049304 at Load Excel Files, SSIS.Pipeline: Warning: Could not open global shared memory to communicate with performance DLL; data flow performance counters are not available.  To resolve, run this package as an administrator, or on the system's console.
Information: 0x40043006 at Load Excel Files, SSIS.Pipeline: Prepare for Execute phase is beginning.
Information: 0x40043007 at Load Excel Files, SSIS.Pipeline: Pre-Execute phase is beginning.
Error: 0xC0202009 at Load Excel Files, Excel Destination [12]: SSIS Error Code DTS_E_OLEDBERROR.  An OLE DB error has occurred. Error code: 0x80040E21.
An OLE DB record is available.  Source: &quot;Microsoft Access Database Engine&quot;  Hresult: 0x80040E21  Description: &quot;Multiple-step OLE DB operation generated errors. Check each OLE DB status value, if available. No work was done.&quot;.
Error: 0xC002F445 at Load Excel Files, Excel Destination [12]: An error occurred while setting up a binding for the &quot;Control Design Rationale&quot; column. The binding status was &quot;DT_NTEXT&quot;.
Error: 0xC0202025 at Load Excel Files, Excel Destination [12]: Cannot create an OLE DB accessor. Verify that the column metadata is valid.
Error: 0xC004701A at Load Excel Files, SSIS.Pipeline: Excel Destination failed the pre-execute phase and returned error code 0xC0202025.
Information: 0x4004300B at Load Excel Files, SSIS.Pipeline: &quot;Excel Destination&quot; wrote 0 rows.
Information: 0x40043009 at Load Excel Files, SSIS.Pipeline: Cleanup phase is beginning.
Task failed: Load Excel Files
Warning: 0x80019002 at Foreach Loop Container: SSIS Warning Code DTS_W_MAXIMUMERRORCOUNTREACHED.  The Execution method succeeded, but the number of errors raised (4) reached the maximum allowed (1); resulting in failure. This occurs when the number of errors reaches the number specified in MaximumErrorCount. Change the MaximumErrorCount or fix the errors.
Warning: 0x80019002 at Package: SSIS Warning Code DTS_W_MAXIMUMERRORCOUNTREACHED.  The Execution method succeeded, but the number of errors raised (4) reached the maximum allowed (1); resulting in failure. This occurs when the number of errors reaches the number specified in MaximumErrorCount. Change the MaximumErrorCount or fix the errors.
SSIS package  finished: Failure.
</code></pre>
","<excel><ssis><ssis-2012>","2020-10-05 19:36:00","1472","0","2","64232469","<p>I don't know why this occurred but the length of the field must be matched  I mostly use Derived Column and set expression to change the length of string</p>
"
"64215085","SSIS Truncation may occur due to inserting data from data flow column","<p>I built Foreach Loop Container in SSIS that extracts data from multiple excel (*.xlsm) files in a directory, and then writes the results in another location.</p>
<p>[Final BCS Description] is causing me problems. I tried using data conversion DT_WSTR.</p>
<p><a href=""https://i.stack.imgur.com/E1CjN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E1CjN.png"" alt=""enter image description here"" /></a></p>
<p>I am getting error:</p>
<blockquote>
<p>Truncation may occur due to inserting data from data flow column
&quot;Copy of Final BSC Description&quot; with a length of 2000 to database
column &quot;Final BSC Description&quot; with a length of 255.</p>
</blockquote>
<p>This doesn't make sense. &quot;Final BSC Description&quot; in the sources is greater than 255 and there is no cap in destination. Could you someone provide a solution or a work around.</p>
<p>Error message below.</p>
<pre><code>SSIS package  starting.
Information: 0x4004300A at Load Excel Files, SSIS.Pipeline: Validation phase is beginning.
Warning: 0x802092A7 at Load Excel Files, Excel Destination [12]: Truncation may occur due to inserting data from data flow column &quot;Copy of Final BSC Description&quot; with a length of 2000 to database column &quot;Final BSC Description&quot; with a length of 255.
Information: 0x4004300A at Load Excel Files, SSIS.Pipeline: Validation phase is beginning.
Warning: 0x802092A7 at Load Excel Files, Excel Destination [12]: Truncation may occur due to inserting data from data flow column &quot;Copy of Final BSC Description&quot; with a length of 2000 to database column &quot;Final BSC Description&quot; with a length of 255.
Warning: 0x80049304 at Load Excel Files, SSIS.Pipeline: Warning: Could not open global shared memory to communicate with performance DLL; data flow performance counters are not available.  To resolve, run this package as an administrator, or on the system's console.
Information: 0x40043006 at Load Excel Files, SSIS.Pipeline: Prepare for Execute phase is beginning.
Information: 0x40043007 at Load Excel Files, SSIS.Pipeline: Pre-Execute phase is beginning.
Error: 0xC0202009 at Load Excel Files, Excel Destination [12]: SSIS Error Code DTS_E_OLEDBERROR.  An OLE DB error has occurred. Error code: 0x80040E21.
An OLE DB record is available.  Source: &quot;Microsoft Access Database Engine&quot;  Hresult: 0x80040E21  Description: &quot;Multiple-step OLE DB operation generated errors. Check each OLE DB status value, if available. No work was done.&quot;.
Error: 0xC002F445 at Load Excel Files, Excel Destination [12]: An error occurred while setting up a binding for the &quot;Control Design Rationale&quot; column. The binding status was &quot;DT_NTEXT&quot;.
Error: 0xC0202025 at Load Excel Files, Excel Destination [12]: Cannot create an OLE DB accessor. Verify that the column metadata is valid.
Error: 0xC004701A at Load Excel Files, SSIS.Pipeline: Excel Destination failed the pre-execute phase and returned error code 0xC0202025.
Information: 0x4004300B at Load Excel Files, SSIS.Pipeline: &quot;Excel Destination&quot; wrote 0 rows.
Information: 0x40043009 at Load Excel Files, SSIS.Pipeline: Cleanup phase is beginning.
Task failed: Load Excel Files
Warning: 0x80019002 at Foreach Loop Container: SSIS Warning Code DTS_W_MAXIMUMERRORCOUNTREACHED.  The Execution method succeeded, but the number of errors raised (4) reached the maximum allowed (1); resulting in failure. This occurs when the number of errors reaches the number specified in MaximumErrorCount. Change the MaximumErrorCount or fix the errors.
Warning: 0x80019002 at Package: SSIS Warning Code DTS_W_MAXIMUMERRORCOUNTREACHED.  The Execution method succeeded, but the number of errors raised (4) reached the maximum allowed (1); resulting in failure. This occurs when the number of errors reaches the number specified in MaximumErrorCount. Change the MaximumErrorCount or fix the errors.
SSIS package  finished: Failure.
</code></pre>
","<excel><ssis><ssis-2012>","2020-10-05 19:36:00","1472","0","2","64269638","<p>Found a work around. From <em>Advanced Editor for Excel Source</em> change the data type to <em>Unicode string [DT_WSTR]</em> Length = 2000</p>
<p><a href=""https://i.stack.imgur.com/B1E9P.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B1E9P.png"" alt=""enter image description here"" /></a></p>
<p>In the cell &quot;Final BSC Description&quot; of your destination excel file add dummy value of length greater than 255 characters. Then hide the entire row.</p>
<p><a href=""https://i.stack.imgur.com/ibXLD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ibXLD.png"" alt=""enter image description here"" /></a></p>
<p>Got the idea from a post in <a href=""https://dba.stackexchange.com/questions/73060/ssis-excel-destination-does-not-allow-characters-more-than-255/172155"">stackExchange</a>, option 2</p>
"
"64213177","Why is Spring cloud data flow tutorial not showing expected result","<p>I'm trying to complete the first spring cloud dataflow tutorial and I'm not getting the result in the tutorial.</p>
<p><a href=""https://dataflow.spring.io/docs/stream-developer-guides/streams/"" rel=""nofollow noreferrer"">https://dataflow.spring.io/docs/stream-developer-guides/streams/</a></p>
<p>The tutorial has me use curl to a http source and see the result in the log sink with a tail of a file of stdout.</p>
<p>I do not see the result. I see the startup in the log.</p>
<p>I tail the log
docker exec -it skipper tail -f /path/from/stdout/textbox/in/dashboard</p>
<p>I enter
curl http://localhost:20100 -H &quot;Content-type: text/plain&quot; -d &quot;Happy streaming&quot;</p>
<p>all I see is</p>
<pre><code>2020-10-05 16:30:03.315  INFO 110 --- [           main] o.a.kafka.common.utils.AppInfoParser     : Kafka version : 2.0.1
2020-10-05 16:30:03.316  INFO 110 --- [           main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId : fa14705e51bd2ce5
2020-10-05 16:30:03.322  INFO 110 --- [           main] o.s.s.c.ThreadPoolTaskScheduler          : Initializing ExecutorService
2020-10-05 16:30:03.338  INFO 110 --- [           main] s.i.k.i.KafkaMessageDrivenChannelAdapter : started org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter@106faf11
2020-10-05 16:30:03.364  INFO 110 --- [container-0-C-1] org.apache.kafka.clients.Metadata        : Cluster ID: 2J0QTxzQQmm2bLxFKgRwmA
2020-10-05 16:30:03.574  INFO 110 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 20041 (http) with context path ''
2020-10-05 16:30:03.584  INFO 110 --- [           main] o.s.c.s.a.l.s.k.LogSinkKafkaApplication  : Started LogSinkKafkaApplication in 38.086 seconds (JVM running for 40.251)
2020-10-05 16:30:05.852  INFO 110 --- [container-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-3, groupId=http-ingest] Discovered group coordinator kafka-broker:9092 (id: 2147482646 rack: null)
2020-10-05 16:30:05.857  INFO 110 --- [container-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-3, groupId=http-ingest] Revoking previously assigned partitions []
2020-10-05 16:30:05.858  INFO 110 --- [container-0-C-1] o.s.c.s.b.k.KafkaMessageChannelBinder$1  : partitions revoked: []
2020-10-05 16:30:05.858  INFO 110 --- [container-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-3, groupId=http-ingest] (Re-)joining group
2020-10-05 16:30:08.943  INFO 110 --- [container-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-3, groupId=http-ingest] Successfully joined group with generation 1
2020-10-05 16:30:08.945  INFO 110 --- [container-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-3, groupId=http-ingest] Setting newly assigned partitions [http-ingest.http-0]
2020-10-05 16:30:08.964  INFO 110 --- [container-0-C-1] o.a.k.c.consumer.internals.Fetcher       : [Consumer clientId=consumer-3, groupId=http-ingest] Resetting offset for partition http-ingest.http-0 to offset 0.
2020-10-05 16:30:08.981  INFO 110 --- [container-0-C-1] o.s.c.s.b.k.KafkaMessageChannelBinder$1  : partitions assigned: [http-ingest.http-0]
</code></pre>
<p>No Happy streaming</p>
<p>Any suggestions?</p>
","<spring><spring-cloud-dataflow>","2020-10-05 17:06:04","291","0","1","64217392","<p>Thank you for trying out the developer guides!</p>
<p>From what I can tell, it appears the <code>http | log</code> stream definition in SCDF is submitted without an explicit port. When that is the case, a port gets randomly assigned by Spring Boot when the <code>http-source</code> and <code>log-sink</code> applications start.</p>
<p>If you navigate to your <code>http-source</code> application logs, you will see the application port listed, and that is the port you'd use on the CURL command.</p>
<p>There's this following note about this in the guide for your reference.</p>
<blockquote>
<p>If you use the local Data Flow Server, add the following deployment property to set the port to avoid a port collision.</p>
</blockquote>
<p>Alternatively, you can deploy the stream with an explicit port in the definition. For example: <code>http --server.port=9004 | log</code>. With that, your CURL would then be:</p>
<blockquote>
<p>curl http://localhost:9004 -H &quot;Content-type: text/plain&quot; -d &quot;Happy streaming&quot;</p>
</blockquote>
"
"64178247","Using Sankey plot to see data flow in R ggalluvial and cosmetics by ggplot","<p>I have a data table of patient clusters before (consensus) and after treatments (single drug) and I want to show how patients flows into different clusters before and after treatment. In this case the actual cluster number doesn't mean much, the important bit is that for most patients cluster together before treatment also end up together after the treatment. Some moves around.</p>
<p>Here is a screenshot of the data
<a href=""https://i.stack.imgur.com/6AiBn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6AiBn.png"" alt=""enter image description here"" /></a></p>
<pre><code>dummy dataset 

structure(list(Stimulation = c(&quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, 
&quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, 
&quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, 
&quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, 
&quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, 
&quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, 
&quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;, &quot;3S&quot;), Patient.ID =       c(&quot;S3077497&quot;, 
&quot;S1041120&quot;, &quot;S162465&quot;, &quot;S563275&quot;, &quot;S2911623&quot;, &quot;S3117192&quot;, &quot;S2859024&quot;, 
&quot;S2088278&quot;, &quot;S3306185&quot;, &quot;S190789&quot;, &quot;S12146451&quot;, &quot;S2170842&quot;, &quot;S115594&quot;, 
&quot;S2024203&quot;, &quot;S1063872&quot;, &quot;S2914138&quot;, &quot;S303984&quot;, &quot;S570813&quot;, &quot;S2176683&quot;, 
&quot;S820460&quot;, &quot;S1235729&quot;, &quot;S3009401&quot;, &quot;S2590229&quot;, &quot;S629309&quot;, &quot;S1208256&quot;, 
&quot;S2572773&quot;, &quot;S3180483&quot;, &quot;S3032079&quot;, &quot;S3217608&quot;, &quot;S5566943&quot;,     &quot;S5473728&quot;, 
&quot;S104259&quot;, &quot;S2795346&quot;, &quot;S2848989&quot;, &quot;S2889801&quot;, &quot;S2813983&quot;, &quot;S2528246&quot;, 
&quot;S3151923&quot;, &quot;S2592908&quot;, &quot;S2603793&quot;, &quot;S5565867&quot;, &quot;S3127064&quot;, &quot;S675629&quot;, 
&quot;S834679&quot;, &quot;S3011944&quot;, &quot;S5011583&quot;, &quot;S2687896&quot;, &quot;S2998620&quot;, &quot;S651963&quot;, 
&quot;S2104595&quot;, &quot;S2433454&quot;, &quot;S2565220&quot;, &quot;S3307762&quot;, &quot;S294778&quot;, &quot;S995510&quot;, 
&quot;S2476822&quot;, &quot;S140868&quot;, &quot;S1018263&quot;, &quot;S2990223&quot;, &quot;S5524130&quot;, &quot;S1042529&quot;, 
&quot;S999706&quot;, &quot;S363003&quot;, &quot;S2303087&quot;, &quot;S868213&quot;, &quot;S5568359&quot;, &quot;S3174542&quot;, 
&quot;S521782&quot;, &quot;S3294727&quot;), `Cluster assigned consensus` = c(2, 2, 
2, 2, 2, 5, 5, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 4, 3, 7, 4, 4, 4, 
4, 4, 4, 8, 8, 4, 7, 4, 1, 1, 1, 1, 1, 1, 1, 8, 8, 8, 8, 7, 7, 
7, 7, 7, 3, 7, 6, 6, 6, 6, 6, 8, 7, 7, 5, 7, 5, 7, 7, 7, 8, 8, 
4, 7, 4, 7), `Cluster assigned single drug` = c(&quot;1&quot;, &quot;1&quot;, &quot;1&quot;, 
&quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, 
&quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;3&quot;, &quot;3&quot;, &quot;3&quot;, &quot;3&quot;, &quot;3&quot;, &quot;3&quot;, &quot;3&quot;, &quot;4&quot;, &quot;4&quot;, 
&quot;4&quot;, &quot;4&quot;, &quot;5&quot;, &quot;5&quot;, &quot;5&quot;, &quot;5&quot;, &quot;5&quot;, &quot;5&quot;, &quot;5&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6&quot;, 
&quot;6&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6&quot;, &quot;7&quot;, &quot;7&quot;, &quot;7&quot;, &quot;7&quot;, &quot;7&quot;, &quot;7&quot;, 
&quot;7&quot;, &quot;7&quot;, &quot;8&quot;, &quot;8&quot;, &quot;8&quot;, &quot;8&quot;, &quot;8&quot;, &quot;8&quot;, &quot;8&quot;, &quot;8&quot;, &quot;8&quot;, &quot;8&quot;, &quot;8&quot;, 
&quot;8&quot;), count = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)), row.names = c(NA, -69L), class =     c(&quot;tbl_df&quot;, 
&quot;tbl&quot;, &quot;data.frame&quot;))
</code></pre>
<p>I'm first time getting to sankey plot so I 'm no expert. I added the count column, so each patient has a count of 1, the flow thickness can be then added by the count.</p>
<p>I modified from R tutorial and the code to visualise is here</p>
<pre><code>library(ggplot2)
library(ggalluvial)

ggplot(data = CLL3S,
       aes(axis1 = `Cluster assigned consensus`, axis2 = `Cluster assigned single drug`, y = count)) +
  scale_x_discrete(limits = c(&quot;Consensus cluster&quot;, &quot;Single-drug cluster&quot;), expand = c(.1, .1)) +
  xlab(&quot;Clusters&quot;) +
  geom_alluvium(aes(fill = `Cluster assigned consensus`)) +
  geom_stratum() +
  geom_text(stat = &quot;stratum&quot;, aes(label = after_stat(stratum))) +
  theme_minimal() +
  ggtitle(&quot;Patient flow between the Consensus clusters and Single-drug treated clusters&quot;,
          &quot;3S stimulated patients&quot;)
</code></pre>
<p>This kind of works but the figure isn't pretty:</p>
<p><a href=""https://i.stack.imgur.com/4WAqj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4WAqj.png"" alt=""enter image description here"" /></a></p>
<p>You see the cluster numbers are surrounded by huge white empty boxes. How can I change that to something smaller? And how do I color code the box into different colors and make sure the if I change the geom_alluvium (fill) so the flow of the data matches the color of the boxes(consensus boxes)?</p>
","<r><ggplot2><visualization><sankey-diagram>","2020-10-02 21:04:30","853","3","1","64181095","<p>You control that in geom_stratum.  Try this</p>
<pre><code>library(ggplot2)
library(ggalluvial)
library(RColorBrewer)

# Define the number of colors you want
nb.cols &lt;- 10
mycolor1 &lt;- colorRampPalette(brewer.pal(8, &quot;Set2&quot;))(nb.cols)
mycolor2 &lt;- colorRampPalette(brewer.pal(2, &quot;Set2&quot;))(nb.cols)

mycolors &lt;- c(&quot;red&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;orange&quot;)

ggplot(data = CLL3S,
       aes(y = count, axis1 = `Cluster assigned consensus`, axis2 = `Cluster assigned single drug` 
           )) +
  scale_x_discrete(limits = c(&quot;Consensus cluster&quot;, &quot;Single-drug cluster&quot;), expand = c(.1, .1)) +
  labs(x=&quot;Clusters&quot;) +
  geom_alluvium(aes(fill = `Cluster assigned consensus`)) +
  geom_stratum(width = 1/4, fill = c(mycolor1[1:8],mycolor1[1:8]), color = &quot;red&quot;) +
  #geom_stratum() +
  geom_text(stat = &quot;stratum&quot;, aes(label = after_stat(stratum))) +
  #scale_fill_manual(values = mycolors) +
  theme_minimal() +
  guides(fill=guide_legend(override.aes = list(color=mycolors)))+
  ggtitle(&quot;Patient flow between the Consensus clusters and Single-drug treated clusters&quot;,
          &quot;3S stimulated patients&quot;)
</code></pre>
<p><a href=""https://i.stack.imgur.com/RSJlf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RSJlf.png"" alt=""output"" /></a></p>
"
"64129194","Set row as a header Azure Data Factory [mapping data flow]","<p>Currently, I have an Excel file that I'm processing using a mapping data flow to remove some null values.</p>
<p>This is my input file:
<a href=""https://i.stack.imgur.com/UFt05.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UFt05.png"" alt=""enter image description here"" /></a></p>
<p>and after remove the null values I have:</p>
<p><a href=""https://i.stack.imgur.com/77fd2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/77fd2.png"" alt=""enter image description here"" /></a></p>
<p>I'm sinking my data into a Cosmos DB but I need to change the names of the columns and set my first row as headers...</p>
<p>I need to do this (First row as a header) in the previous step before Sink and y Can't use the mapping option a set manually the names of the columns because maybe some of these position of columns can change</p>
<p>Any idea to do this?</p>
<p>Thanks</p>
","<azure-data-factory>","2020-09-30 00:01:17","2467","0","1","64149685","<p>First row as a header can only check in dataset connection.</p>
<p>As a work around, you can save your excel to blob(csv format) after removing null value.</p>
<p>Then create a copy data activity or data flow, use this csv file as source(check first row as header), Cosmos DB as sink.</p>
<hr />
<p><strong>Update</strong></p>
<p>Setting of sink in data flow:</p>
<p><a href=""https://i.stack.imgur.com/XIGue.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XIGue.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/GcG4q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GcG4q.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/lCRIy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lCRIy.png"" alt=""enter image description here"" /></a></p>
<p>Data preview of sink:</p>
<p><a href=""https://i.stack.imgur.com/AGR2l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AGR2l.png"" alt=""enter image description here"" /></a></p>
<p>Result:</p>
<p><a href=""https://i.stack.imgur.com/Rsawh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Rsawh.png"" alt=""enter image description here"" /></a></p>
"
"64118489","ADF data flow not giving cosmos query results with the parameters","<p><strong>Disclaimer : I am very new to Azure Development</strong></p>
<p>In Azure Data factory Dataflow in source option when I have hardcoded the date string and  used below query it gives the results as expected for cosmos DB.</p>
<pre><code>“select c.column1,c.column2 from c where c.time_stamp &gt;= '2010-01-01T20:28:45Z' and c.time_stamp &lt;= '2020-09-11T20:28:45Z'”
</code></pre>
<p>When I have passed the parameters which I have mapped in pipeline and use the query with parameters I am not getting any result.</p>
<pre><code>&quot;oldwatermark&quot;: &quot;'2010-01-01T20:28:45Z'&quot;,
&quot;newwatermark&quot;: &quot;'2020-09-11T20:28:45Z'&quot;


“select c.column1,c.column2 from c where c.time_stamp &gt;= ‘$oldwatermark’ and c.time_stamp &lt;= ‘$oldwatermark’”
</code></pre>
<p>Could you please suggest what am I doing wrong here as my parameter values and hardcoded values are same.</p>
","<azure><azure-cosmosdb><azure-data-factory>","2020-09-29 11:16:32","370","0","1","64129981","<p>Just from your worked statements, your query should be:</p>
<pre><code>select c.column1,c.column2 from c where c.time_stamp &gt;= $oldwatermark and c.time_stamp &lt;= $newwatermark
</code></pre>
<p>not <code>where c.time_stamp &gt;= $oldwatermark and c.time_stamp &lt;= $oldwatermark</code>.</p>
<p><strong>Please don't use the quotes for the parameter in the query</strong>.</p>
<p>Please try this query:</p>
<pre><code>concat('select c.column1,c.column2 from c where c.time_stamp &gt;= ',$oldwatermark,'and c.time_stamp &lt;= ',$newwatermark)
</code></pre>
<p><a href=""https://i.stack.imgur.com/gPeGC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gPeGC.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/AJD8T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AJD8T.png"" alt=""enter image description here"" /></a></p>
"
"64109538","CosmosDB sink in data flow does not conform to target dataset schema","<p>I have defined 1 pipeline with 1 data flow.The data flow does the following:</p>
<ol>
<li>Reads a document from Blob source with av JSON schema</li>
<li>Lookup over a second source from CosmosDB with the same schema. Simple lookup for equality on 1 field.</li>
<li>Merge an array property that comes from the database with the one that comes from the blob</li>
<li>Upsert the resulting document back to the same CosmosDB collection through a corresponding sink.</li>
</ol>
<p>Even though the document schemas are the same for the different sources and the sink, i have defined 2 different schemas - 1 for the blob and 1 for the CosmosDB, used by both source and sink.</p>
<p>The JSON doc schema itself is nothing complex - few properties under the root and an array property of flat document with few properties, that is getting merged. Some of these properties are ints or doubles, rest are strings. Documents are getting correctly processed with the content in the array correctly merged, and then either udpated or inserted in the CosmosDB collection.</p>
<p>However none of the int fields are written as such - they are all converted to strings. Doubles seem to be handled correctly. The schema is correct throughout the data flow. Tried even with adding explicit transformation in order to set the type to int prior to the sink, yet still the same outcome.</p>
<p>I looked then a bit under the hood and found out that the script that is created behind the scenes contains a sink definition with the wrong field types - instead of ints, the fields are all strings. Then i decided to outsmart the ADF and edited the script manually. After running a Publish though i was proven that ADF is smarter than me. In the publish branch the script was magically reverted back to its original state - strings instead of int for the fields in the sink. While at the same time the Dev branch clearly contains the correctly defined types (though manually).Very annoying indeed!</p>
<p>ADF has taken a long way since v1 and resembles a lot the dev experience for SSIS (and even better), yet lack of control of data types of the fields/columns, at least at the source/sink points seems somewhat childish. And in addition this magic transformation of the types from int to strings during publish (!?!) adds 2 more points in direction south for the time being :(</p>
<p>Any idea if this is a known issue, and moreover, if there is a known workaround will be highly appreciated!</p>
","<azure-data-factory>","2020-09-28 20:58:07","127","0","1","64116857","<p>You can create a <code>DerivedColumn</code> after step 3, and use this expression <code>toInteger(your column)</code>.</p>
<p>I think this is a similar <a href=""https://stackoverflow.com/questions/63930814/migrate-json-data-from-azure-sql-db-to-cosmos-db-results-in-string-values/63985596#63985596"">question</a>, you can refer to it.</p>
"
"64105436","Linked Service with self-hosted integration runtime is not supported in data flow in Azure Data Factory","<p>Step to reproduce:</p>
<ol>
<li><p>I created a Copy Data first in the pipeline to simple transfer CSV files frol Azure VM to Azure Blob storage. I always use IRPOC1 as a connection via integration runtime and connect using SAS URI and SAS Token to my Blob Storage</p>
</li>
<li><p>After validate and run my first Copy Data, I successfully have CSV file transfer from my VM to Blob storage</p>
</li>
<li><p>I tried to add a new Data Flow after the Copy Data activity</p>
</li>
<li><p>In my Data Flow, my source is the Blob storage containing the CSV files transferred from VM, my Sink is my Azure SQL Database with successful connection</p>
</li>
<li><p>However, when I ran validation, I got the error message on my Data Flow Source:</p>
<p>Linked Service with self-hosted integration runtime is not supported in data flow.</p>
</li>
</ol>
<p>I saw someone replied on Microsoft Azure Document issue Github that I need to use Copy Data to transfer data to Blob first. Then use the source from this blob with data. This is what I did but I still have the same error. Could you please let me know how I can fix this?</p>
","<azure-data-factory>","2020-09-28 15:47:00","12749","4","1","64108509","<p>The Data Flow source dataset must use a Linked Service that uses an Azure IR, not a self-hosted IR.</p>
<p>Go to the dataset in your data flow Source, click &quot;Open&quot;. In the dataset page, click &quot;Edit&quot; next to Linked Service.</p>
<p>In the Linked Service dialog, make sure you are using an Azure Integration Runtime, not a Self-hosted IR.</p>
<p><a href=""https://i.stack.imgur.com/F1Pmd.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/F1Pmd.png"" alt=""enter image description here"" /></a></p>
"
"64076132","Javascript data mapping","<p>Can someone explain to me how can I implement in a clean way a solution to get a status (string) from combining two other ones?
I need to declare a function which takes two params (two strings) and needs to return another one based on the combination of those two strings.
For ex:</p>
<pre><code>carStatus(status, secondaryStatus) =&gt; string
</code></pre>
<p>where secondaryStatus can have multiple options.
I'm thinking using an if/else if statement which returns a third status which I need.
For ex when status is 'OPEN' and secondaryStatus is 'payment1' or 'payment2' or 'payment3', the function must return a new string (status) like 'CONFIRMED'.
So, an example of how I'm thinking to implement at this moment would be something like this:</p>
<pre><code>carStatus = (status, secondaryStatus) =&gt; {
  if(status === 'OPEN' &amp;&amp; (secondaryStatus === 'payment1' || 'payment2' || 'payment3')){
    return 'CONFIRMED';
  } else if(status === 'CANCELLED' &amp;&amp; (secondaryStatus === 'payment4' || 'payment5' || 'payment6')){
    return 'REMOVED';
  } else if(status === 'REVIEW' &amp;&amp; (secondaryStatus === 'payment2' || 'payment5' || 'payment5')){
    return 'CHECKED';
  }
}

&lt;div&gt;carStatus('OPEN', 'payment1')&lt;/div&gt;
</code></pre>
<p>In div must be rendered 'CONFIRMED'.</p>
<p>In my implementation, I'll have to write I think other 5 else if statements.. so maybe there is a cleaner way to implement this.</p>
<p>My project is written in React, but I'm thinking to put this function in utils folder. Perhaps a solution written in React could be more clean? I don't know.</p>
<p>Any help will be appreciated.</p>
","<javascript><reactjs>","2020-09-26 09:44:26","51","-1","1","64076333","<p>You can probably use a lookup object for the various options. Since criteria given is very vague following is a very basic example</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>const carStatus = (carState, carSecondaryState) =&gt; {
  const secondaries = {
    ORDERED: 'CLIENT'
  }
  return `${secondaries[carSecondaryState]}_${carState}`
}

console.log(carStatus('NEW', 'ORDERED'))</code></pre>
</div>
</div>
</p>
"
"64033915","Using existing data sets withing Azure data factory in a new data flow","<p>I'm having trouble using existing data sets withing Azure data factory when I want to create a new data flow. under data set combo box in data factory doesn't load the existing data sets.
Also when I want to create a new data set most of the data sources such as SQL server is disable.</p>
<p>Is there any idea?</p>
<p>please see this screen shot</p>
<p><a href=""https://i.stack.imgur.com/HAs53.png"" rel=""nofollow noreferrer"">I Can not select the data sets which was built before</a>
When I press new data set sql server is disable and I can not select it.</p>
<p><a href=""https://i.stack.imgur.com/hmoRC.png"" rel=""nofollow noreferrer"">SQL server in this list is disabled and can not be selected</a></p>
","<azure><azure-data-factory>","2020-09-23 18:10:25","228","0","2","64038159","<p>Data flow supports seven type source by now, SQL server is not supported. If your exist dataset isn't within these types, you can't choose them. More Details please refer to <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-source"" rel=""nofollow noreferrer"">this documentation</a>.</p>
<p><a href=""https://i.stack.imgur.com/PuuBD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PuuBD.png"" alt=""enter image description here"" /></a></p>
"
"64033915","Using existing data sets withing Azure data factory in a new data flow","<p>I'm having trouble using existing data sets withing Azure data factory when I want to create a new data flow. under data set combo box in data factory doesn't load the existing data sets.
Also when I want to create a new data set most of the data sources such as SQL server is disable.</p>
<p>Is there any idea?</p>
<p>please see this screen shot</p>
<p><a href=""https://i.stack.imgur.com/HAs53.png"" rel=""nofollow noreferrer"">I Can not select the data sets which was built before</a>
When I press new data set sql server is disable and I can not select it.</p>
<p><a href=""https://i.stack.imgur.com/hmoRC.png"" rel=""nofollow noreferrer"">SQL server in this list is disabled and can not be selected</a></p>
","<azure><azure-data-factory>","2020-09-23 18:10:25","228","0","2","64068611","<p>Since you can't use SQL Server within a Data Flow, prior to the Data Flow you need to run a Copy activity to stage your data from SQL Server into a location reachable by the subsequent Data Flow.  The easiest option to use is a Delimited text file (CSV) in Azure Blob Storage.</p>
"
"64027495","How to use Pentaho Denormalizer Step with Metadata Injection","<p>I want to denormalize the below data.
Input
<a href=""https://i.stack.imgur.com/qWUie.png"" rel=""nofollow noreferrer"">Input</a></p>
<p>Required output</p>
<pre><code>col1   col2   col3   col4

aaa    bbb    ccc    ddd
</code></pre>
<p>I think in Pentaho we can use Metadata Injection step with Denormalizer step to dynamically denormalize all row values to columns</p>
","<pentaho><kettle><pentaho-data-integration><pdi><data-integration>","2020-09-23 12:00:24","326","0","1","64074460","<p>Yes, its possible. One constraint is you need to provide your input data twice.
I have prepared a solution , you can see dynamic denormalize from <a href=""https://drive.google.com/drive/folders/181PxOT11Jw4IB7v1ilJOeHdnrQSvuE9m?usp=sharing"" rel=""nofollow noreferrer"">Here</a></p>
"
"64002859","jQuery AJAX call returning images in wrong order - data synchronization","<p>I use a JQuery AJAX call to return image, name, address, country and score of a tourist attraction in a given city (by latitude). I save the returned data into 5 arrays: for images, names, addresses, country, and score.
Function &quot;tom1&quot; gets all data except images, it triggers function &quot;tom2&quot; that get image name.
Function &quot;tom2&quot; triggers function &quot;callme&quot;.</p>
<p>When function call me appends all 5 values to the page, images often don't match the other info.</p>
<p>Is it related to the AJAX's asynchronous request?</p>
<p>Can you help me solve it? Here is my code:</p>
<pre class=""lang-js prettyprint-override""><code>function tom1(lat, lon) {
  var queryURL = &quot;https://api.tomtom.com/search/2/search/museum.json?key=&quot; + tomAPI + &quot;&amp;lat=&quot; + lat + &quot;&amp;lon=&quot; + lon;

  $.ajax({
    url: queryURL,
    method: &quot;GET&quot;
  }).then(function(response) {

    //only select POIs which possess &quot;dataSources&quot; key = images
    for (var i = 0; i &lt; 20; i++) {
      if (response.results[i].dataSources !== undefined) {
        poiId.push(response.results[i].dataSources.poiDetails[0].id);
        placeName.push(response.results[i].poi.name);
        address.push(response.results[i].address.freeformAddress);
        country.push(response.results[i].address.country);
        rank.push(response.results[i].score.toFixed(1));
      } else {
        console.log(&quot;not lucky today&quot;)
      }
    }
    tom2(poiId);
  });
}

// get POI details and images after you know POI's ID
function tom2(poiId) {

  for (i = 0; i &lt; poiId.length; i++) {
    var queryURL = &quot;https://api.tomtom.com/search/2/poiDetails.json?key=&quot; + tomAPI + &quot;&amp;id=&quot; + poiId[i];

    $.ajax({
      url: queryURL,
      method: &quot;GET&quot;
    }).then(function(response) {
      imgId.push(response.result.photos[0].id);
      callme(imgId);
    });
  }

}
</code></pre>

","<javascript><jquery><ajax><api><asynchronous>","2020-09-22 03:52:12","84","0","1","64003259","<p>Yes, tom2 makes a number of asynchronous calls, and the callback function of those can typically be executed in a different order. So the order of the elements in imgId will be mixed up.</p>
"
"63872347","Is there a pattern for data synchronization in two independent services?","<p>I need the data in the two services to be identical. Changes can be made to each of the services at the same time. I made integration with each service using the REST API. But here's what and how to track and what are the options for resolving data conflicts. For example, Google calendar events and some task Manager, such as Todoist (I know that there is a built-in integration :) )</p>
","<java><synchronization>","2020-09-13 15:14:59","192","-1","1","63872799","<p>When <code>Service1</code> has an update, then <code>Service1</code> publishes a message to a topic.  <code>Service2</code> is subscribed to the topic, receives the message, and <code>Service2</code> updates its data. <code>Service2</code> doesn't know that the message originated from <code>Service1</code>.</p>
<p>Similarly, when <code>Service2</code> has an update, then <code>Service2</code> publishes a message to the same topic.  <code>Service1</code> is subscribed to the topic, receives the message, and <code>Service1</code> updates its data. <code>Service1</code> doesn't know that the message originated from <code>Service2</code>.</p>
<p>This is referred to as a publish-subscribe pattern, and is near real-time, where synchronization depends on the backlog of messages to be processed.  It depends on the app, but a few milliseconds of being out of sync is usually acceptable for a human user.</p>
"
"63856422","Json data synchronization in database","<p>I keep json data from database. I want to sync with the name that comes with the post and get the price of the data I sync. How can I do that?</p>
<pre class=""lang-php prettyprint-override""><code>$video_tkn      = &quot;3D&quot;;
$reklam_json    = json_decode($siteayar-&gt;reklam_json);
$video          = json_decode($reklam_json-&gt;video);

$video_arr      = array();

foreach ($video as $v) {
    if ($v-&gt;video_tur==$video_tkn) {
        $video_arr[] = $v-&gt;fiyat;
    }else{
        $video_arr[] = 0;
    }
}

print_r($video_arr[0]);

//Output : 0

</code></pre>
<pre class=""lang-php prettyprint-override""><code>//$reklam_json-&gt;video
</code></pre>
<pre class=""lang-json prettyprint-override""><code>
[
   {
      &quot;video_tur&quot;:&quot;2D&quot;,
      &quot;fiyat&quot;:&quot;20&quot;
   },
   {
      &quot;video_tur&quot;:&quot;3D&quot;,
      &quot;fiyat&quot;:&quot;80&quot;
   }
]
</code></pre>
","<php><json>","2020-09-12 03:02:07","93","0","1","63856893","<p>You can do something like. The problem with the code that you have written is that in the $video_arr[0] it is always writing 0 as per your else condition because $video_tkn= &quot;3D&quot; value is in second index in the array $video</p>
<pre><code>&lt;?php 
$video_tkn      = &quot;3D&quot;;
$reklam_json    = json_decode($siteayar-&gt;reklam_json);
$video          = json_decode($reklam_json-&gt;video);

$video_arr      = array();

foreach ($video as $v) {
    if ($v-&gt;video_tur==$video_tkn) {
        $video_arr[] = $v-&gt;fiyat;
    }
}

print_r($video_arr[0]);
</code></pre>
"
"63832416","List all data flows in Visual Studio - complete noob","<p>So I am a complete noob when it comes to Visual Studio and only recently started using it as part of a new role.</p>
<p>We have a project in SSIS with 150 packages. Each has a data flow where the data from the table is pushed into our SQL server equivalent table. Both tables have the same name. We discovered that one of the flows was set up with the wrong destination table. This means we now have to check all 150 flows to ensure everything else is correct.</p>
<p>Is there a way that i can list all the data flows and their source and destination details so i can match and compare?</p>
<p>Apologies if using incorrect nomenclature or not providing clear details. Just let me know what you'd need to know.</p>
<p>Any help is greatly appreciated.</p>
<p><a href=""https://i.stack.imgur.com/vubA3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vubA3.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/b63s1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/b63s1.png"" alt=""enter image description here"" /></a></p>
","<sql-server><visual-studio><ssis>","2020-09-10 14:57:50","88","1","1","63833824","<p>If your SSIS project is deployed to SSISDB, you can query tables in it and get source-to-destination mappings for your packages.</p>
<p>I don't have any examples right now, but the following links should give you an idea of which direction to dig to:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/43938001/how-to-query-xml-as-used-in-dtsx-ssis"">How to query XML as used in .dtsx (SSIS)</a></li>
<li><a href=""https://dba.stackexchange.com/questions/211796/finding-table-name-referfences-in-ssis-packages"">Finding table name referfences in SSIS packages</a></li>
</ul>
"
"63831125","How to check whether Spring Cloud data flow microservice is up by using Python","<p>Is there a way to check whether Spring Cloud data flow (streams) microservice is up and running by using Python?
I would also want to (un)deploy microservice.</p>
<p>Is it possible?</p>
","<python-3.x><spring-cloud-dataflow>","2020-09-10 13:48:46","67","0","1","63899587","<p>You can use this <a href=""https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#api-guide"" rel=""nofollow noreferrer"">REST API guide</a> to understand what REST endpoints you can use to specify operations against Spring Cloud Data Flow (start/stop the stream/task for instance).</p>
"
"63725121","ADF Transformation with parameterized dataset in mapping data flow","<p>I'm currently doing an ADF data transformation using .net SDK. I’m using Data Flow for this transformation. I have a CSV file with three columns which are First Name, Last Name and Age. I want to Concatenate First Name and Last Name in to full Name.
But the problem is input file is generated at runtime (Data sets are parameterized). Below is my Data Flow Diagram. I also tried adding parameters for Concat function.</p>
<p><a href=""https://i.stack.imgur.com/Uqd4d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Uqd4d.png"" alt=""enter image description here"" /></a></p>
<p>I was able to do this for an existing file in blob storage and below is that Data Flow Diagram.</p>
<p><a href=""https://i.stack.imgur.com/fsaxs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fsaxs.png"" alt=""enter image description here"" /></a></p>
<p>But I want to do this with a file which is created at runtime. Please assist</p>
","<c#><azure><asp.net-core><azure-data-factory><azure-sdk>","2020-09-03 13:54:19","160","0","1","63730462","<p>This expression will work inside of a Derived Column regardless of the contents of the incoming file, so long as FirstName and LastName exist somewhere in the incoming metadata:</p>
<p>byName('FirstName')+byName('LastName')</p>
"
"63699939","Get Fixed Width file row as single column in Azure Data Factory - Mapping data flow","<p>I have a set of Fixed width files in ADLS.
I am reading the file with the following flow:</p>
<p>MetadataActivity --&gt; ForEach.</p>
<p>Inside ForEach, I have a Mapping Data Flow with this source settings:</p>
<p><a href=""https://i.stack.imgur.com/qpT9W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qpT9W.png"" alt=""enter image description here"" /></a></p>
<p>My projection looks like :</p>
<p><a href=""https://i.stack.imgur.com/9jryV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9jryV.png"" alt=""enter image description here"" /></a></p>
<p>My expected projection is :</p>
<p><a href=""https://i.stack.imgur.com/6p6qF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6p6qF.png"" alt=""enter image description here"" /></a></p>
<p>Please let me know what changes I have to do to get the  above projection. Thank you.</p>
","<azure><azure-data-factory>","2020-09-02 06:33:30","470","-1","2","63700370","<p>I've tried serval times to  reproduce the problem.</p>
<p>1.Have you set any value to the <strong>Column delimiter</strong> at the <strong>Source dataset</strong>?</p>
<p><a href=""https://i.stack.imgur.com/ORrfl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ORrfl.png"" alt=""enter image description here"" /></a></p>
<p>2.In my source data, it contains the ',' as follows
<a href=""https://i.stack.imgur.com/kMrt5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kMrt5.png"" alt=""enter image description here"" /></a></p>
<p>3.ADF will divide this column into three columns automatically.
<a href=""https://i.stack.imgur.com/c6MdT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c6MdT.png"" alt=""enter image description here"" /></a></p>
<p>4.If so, you should select <code>No delimiter</code>.
<a href=""https://i.stack.imgur.com/Afuhh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Afuhh.png"" alt=""enter image description here"" /></a></p>
<p>5.After select <code>No delimiter</code>, there will be just one column.
<a href=""https://i.stack.imgur.com/eI2y6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eI2y6.png"" alt=""enter image description here"" /></a></p>
<hr />
<p><strong>Additional</strong><br />
My Wildcard is as follows:
<a href=""https://i.stack.imgur.com/DGsCe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DGsCe.png"" alt=""enter image description here"" /></a>
I guess your source data contains any other delimiter?  Please correct me if I understand you wrong in the answer.</p>
"
"63699939","Get Fixed Width file row as single column in Azure Data Factory - Mapping data flow","<p>I have a set of Fixed width files in ADLS.
I am reading the file with the following flow:</p>
<p>MetadataActivity --&gt; ForEach.</p>
<p>Inside ForEach, I have a Mapping Data Flow with this source settings:</p>
<p><a href=""https://i.stack.imgur.com/qpT9W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qpT9W.png"" alt=""enter image description here"" /></a></p>
<p>My projection looks like :</p>
<p><a href=""https://i.stack.imgur.com/9jryV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9jryV.png"" alt=""enter image description here"" /></a></p>
<p>My expected projection is :</p>
<p><a href=""https://i.stack.imgur.com/6p6qF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6p6qF.png"" alt=""enter image description here"" /></a></p>
<p>Please let me know what changes I have to do to get the  above projection. Thank you.</p>
","<azure><azure-data-factory>","2020-09-02 06:33:30","470","-1","2","63702153","<p>The issue got resolved after changing the schema in source dataset.</p>
<p>It was csv before:</p>
<p><a href=""https://i.stack.imgur.com/cpOla.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cpOla.png"" alt=""enter image description here"" /></a></p>
<p>Issue was fixed after change the schema to &quot; From Files with &quot;*&quot;&quot;</p>
<p><a href=""https://i.stack.imgur.com/kj9kG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kj9kG.png"" alt=""enter image description here"" /></a></p>
"
"63634415","ADF Mapping Data Flows - Reuse single running spark cluster for parallel execution of mapping data flows","<p>We have a complex ETL in ADF running multiple pipelines with data flow activities to load several tables in a data-warehouse based on table dependencies.</p>
<p>As a result of running multiple pipelines with inter-dependencies, several data flows are executed as a mix of some running sequentially and some running in parallel. It looks like each data flow running in parallel spins up a new spark cluster, which is causing our daily ETL run cost to skyrise!</p>
<p>Ideally we would like the spark cluster to be reused for all parallel data flow execution, if possible. Is there a way to specify an upper limit for the number of spark clusters that should be created for parallel data flow execution?</p>
<p>We already have TTL enabled for 10 mins.</p>
","<azure><apache-spark><azure-data-factory>","2020-08-28 13:08:34","813","0","1","63638474","<p>When you have TTL enabled, make sure to execute data flows using that Azure IR in sequence so that you don't spin-up multiple cluster pools.</p>
<p>To execute in parallel, use Azure IR without TTL.</p>
<p>We are working on the &quot;max concurrency&quot; feature you mention above, hope to land that soon.</p>
"
"63625287","Azure data factory data flow silently NULLing date column","<p>I'm trying to use Azure Data Factory to upsert a CSV into an Azure SQL table. All seemed well until I checked the results. One of the columns is a nullable date. The CSV contains a value like so <code>1/2/2020 12:00:00 AM</code>. The data flow silently inserts a <code>NULL</code> instead of throwing an error because it didn't like the input. So how can I get my data flow to convert the string to a datetime properly, and then to error out on issues like this in the future? I really don't want silent failures and bad data.</p>
","<csv><azure-sql-database><azure-data-factory>","2020-08-27 23:07:56","3771","1","1","63626648","<p>The null value is due to incompatible date formats in ADF. You need to do date format conversion.</p>
<p>Is your source date format like this <strong>MM/dd/yyyy HH:mm:ss</strong>?<br />
If so, you can use <a href=""https://learn.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"" rel=""nofollow noreferrer"">Derived column</a> and add the expression <code>toString(toTimestamp(&lt;Your_Column_Name&gt;,'MM/dd/yyyy HH:mm:ss'),'yyyy-MM-dd HH:mm:SS')</code> to format this column to String. It solved the <code>NULL</code> value. Of course you can choose what the date format you want.</p>
<p>I made a test as follows:</p>
<ol>
<li><p>My data source is from a csv file and the <code>EmpDate</code> is a date type like yours and last row contains a null value.
<a href=""https://i.stack.imgur.com/hcWdq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hcWdq.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Then I add the expression <code>toString(toTimestamp(EmpDate,'MM/dd/yyyy HH:mm:ss'),'yyyy-MM-dd HH:mm:SS')</code> in the Derived column activity. Here you can choose the date format what you want.
<a href=""https://i.stack.imgur.com/cg2AZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cg2AZ.png"" alt=""enter image description here"" /></a></p>
</li>
</ol>
<p>3.According to Mark Kromer's suggestion, I Add Conditional Split directly after the Derived Column and check for isNull(EmpDate). Here I use <code>not(isNull(EmpDate))</code> expression.
<a href=""https://i.stack.imgur.com/t0y9u.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t0y9u.png"" alt=""enter image description here"" /></a></p>
<ol start=""4"">
<li>In the end, if the <code>EmpDate</code> contains null value, it will go to sink2 else go to sink1.
<a href=""https://i.stack.imgur.com/61v9E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/61v9E.png"" alt=""enter image description here"" /></a>
The row contains null value:
<a href=""https://i.stack.imgur.com/QXrIC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QXrIC.png"" alt=""enter image description here"" /></a></li>
</ol>
"
"63619028","Twilio API to SSIS data flow","<p>Attempting to read records from the Twilio API with the aim then to use this in a SSIS Data Flow and save the records in a SQL Server database. Using the API documentation from <a href=""https://www.twilio.com/docs/sms/api/message-resource#read-multiple-message-resources"" rel=""nofollow noreferrer"">https://www.twilio.com/docs/sms/api/message-resource#read-multiple-message-resources</a> I've been able to read the messages but a bit stuck with what to do next, I just want to use this as a source for the Data Flow so not trying to do anything too fancy in .net, more of a SQL guy so not very experienced with C#.</p>
<p>have I been thinking this would be too straight forward? Was assuming i'd be able to read the messages (doesn't look like this comes down as JSON, individual fields have already been split out using the Twilio.dll) put this into a variable which then i loop through each row and pass to the output buffer.</p>
<p>Essentially, was trying something like this:</p>
<pre><code>

    public override void CreateNewOutputRows()
    {
         string accountSid = &quot;AAAA&quot;;

         string authToken = &quot;1111&quot;;

        TwilioClient.Init(accountSid, authToken);

        var response = MessageResource.Read();

        foreach (var msg in response)
        {
            Output0Buffer.AddRow();
            Output0Buffer.ID = msg.Sid.ToString();
            Output0Buffer.Message = msg.Body.ToString();
        }
    }
```
</code></pre>
","<c#><.net><ssis><twilio-api>","2020-08-27 15:11:45","362","0","2","63637808","<p>Here's an example not using the Twilio library and not using a data flow.</p>
<p>We're basically looking at something like:</p>
<p><a href=""https://i.stack.imgur.com/TGbjT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TGbjT.png"" alt=""enter image description here"" /></a></p>
<ol>
<li>Truncate the table we're loading</li>
<li>We'll use a For Loop Container because we have to account for paging.</li>
<li>C# script task to make the REST call</li>
<li>Stored procedure to parse the JSON, insert into a table and evaluate the next url value and return that back that out to see if we need to go around again.</li>
</ol>
<p>Twilio has a default of 50 records, a max of 1000, returned in the response.  You can change that by using URL parameter &quot;?PageSize=60&quot; if you want more than the default.  If there are more records then the specific Page Size the response will include the next url to go get the next set of records.</p>
<p>Two variables:</p>
<p><a href=""https://i.stack.imgur.com/mjGxy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mjGxy.png"" alt=""enter image description here"" /></a></p>
<p>NextUrl set as</p>
<blockquote>
<p><a href=""https://api.twilio.com/2010-04-01/Accounts/%7BAccountSid%7D/Messages.json"" rel=""nofollow noreferrer"">https://api.twilio.com/2010-04-01/Accounts/{AccountSid}/Messages.json</a></p>
</blockquote>
<p>For Loop Container</p>
<p><a href=""https://i.stack.imgur.com/kDy1K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kDy1K.png"" alt=""enter image description here"" /></a></p>
<p>Once NextUrl = STOP the Loop will stop.  We handle that in the stored procedure.  After we insert the data, we check that value and if it is NULL we set it to STOP.</p>
<p>Script Task
<a href=""https://i.stack.imgur.com/wV6xM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wV6xM.png"" alt=""enter image description here"" /></a></p>
<p>Pass in the URL, in the script we set apiResponse so it comes back out.</p>
<p>You will need to include these::
<a href=""https://i.stack.imgur.com/18ypH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/18ypH.png"" alt=""enter image description here"" /></a></p>
<p>Code for Main()</p>
<pre><code>try
            {
                string apiUrl = Dts.Variables[&quot;NextUrl&quot;].Value.ToString();

                string userName = &quot;&quot;; //accountSid found in Twilio account console
                string passwd = &quot;&quot;; //authToken found in Twilio account console

                ServicePointManager.SecurityProtocol = SecurityProtocolType.Tls12;

                HttpClient client = new HttpClient();

                client.DefaultRequestHeaders.Accept.Clear();

                byte[] authToken = Encoding.ASCII.GetBytes($&quot;{userName}:{passwd}&quot;);
                client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(&quot;Basic&quot;, Convert.ToBase64String(authToken));

                HttpResponseMessage response;

                // Execute the REST GET
                response = client.GetAsync(apiUrl).GetAwaiter().GetResult();

                // Get the JSON response.
                string contentString = response.Content.ReadAsStringAsync().GetAwaiter().GetResult();

                Dts.Variables[&quot;apiResponse&quot;].Value = contentString; //Passing the response back out.

                Dts.TaskResult = (int)ScriptResults.Success;
            }

            catch (Exception ex)
            {
                //This code will cause the ssis to fail and bubble back out the error from this code
                Dts.Events.FireError(-1, &quot;Error&quot;, ex.Message, String.Empty, 0);
                Dts.TaskResult = (int)ScriptResults.Failure;

            }
</code></pre>
<p>Create a stored procedure with a NVARCHAR(MAX) parameter to accept the apiResponse and to return out the NextUrl value.  The task would be something like:</p>
<p><a href=""https://i.stack.imgur.com/RJVNq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RJVNq.png"" alt=""enter image description here"" /></a></p>
<p>Parameter Mapping:</p>
<p><a href=""https://i.stack.imgur.com/tAVEQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tAVEQ.png"" alt=""enter image description here"" /></a></p>
<p>Result Set:</p>
<p><a href=""https://i.stack.imgur.com/I3xEW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/I3xEW.png"" alt=""enter image description here"" /></a></p>
<p>Then the stored procedure itself would be something like:</p>
<pre><code>CREATE PROCEDURE [dbo].[InsertTwilioMessage]
    @apiResponse NVARCHAR(MAX)
AS
    DECLARE @NextUrl NVARCHAR(2000);

    INSERT INTO [dbo].[TwilioMessage] (
                                          [body]
                                        , [num_segments]
                                        , [direction]
                                        , [from]
                                        , [date_updated]
                                        , [price]
                                        , [error_message]
                                        , [uri]
                                        , [account_sid]
                                        , [num_media]
                                        , [to]
                                        , [date_created]
                                        , [status]
                                        , [sid]
                                        , [date_sent]
                                        , [messaging_service_sid]
                                        , [error_code]
                                        , [price_unit]
                                        , [api_version]
                                      )
                SELECT [rsp].[body]
                     , [rsp].[num_segments]
                     , [rsp].[direction]
                     , [rsp].[from]
                     , [rsp].[date_updated]
                     , [rsp].[price]
                     , [rsp].[error_message]
                     , [rsp].[uri]
                     , [rsp].[account_sid]
                     , [rsp].[num_media]
                     , [rsp].[to]
                     , [rsp].[date_created]
                     , [rsp].[status]
                     , [rsp].[sid]
                     , [rsp].[date_sent]
                     , [rsp].[messaging_service_sid]
                     , [rsp].[error_code]
                     , [rsp].[price_unit]
                     , [rsp].[api_version]
                FROM OPENJSON(@apiResponse, '$.messages') --data is in the messages array
                           WITH (
                                    [body] NVARCHAR(255) '$.body'
                                  , [num_segments] NVARCHAR(255) '$.num_segments'
                                  , [direction] NVARCHAR(255) '$.direction'
                                  , [from] NVARCHAR(255) '$.from'
                                  , [date_updated] NVARCHAR(255) '$.date_updated'
                                  , [price] NVARCHAR(255) '$.price'
                                  , [error_message] NVARCHAR(255) '$.error_message'
                                  , [uri] NVARCHAR(255) '$.uri'
                                  , [account_sid] NVARCHAR(255) '$.account_sid'
                                  , [num_media] NVARCHAR(255) '$.num_media'
                                  , [to] NVARCHAR(255) '$.to'
                                  , [date_created] NVARCHAR(255) '$.date_created'
                                  , [status] NVARCHAR(255) '$.status'
                                  , [sid] NVARCHAR(255) '$.sid'
                                  , [date_sent] NVARCHAR(255) '$.date_sent'
                                  , [messaging_service_sid] NVARCHAR(255) '$.messing_serivce_sid'
                                  , [error_code] NVARCHAR(255) '$.error_code'
                                  , [price_unit] NVARCHAR(255) '$.price_unit'
                                  , [api_version] NVARCHAR(255) '$.api_version'
                                ) AS [rsp];


    SELECT @NextUrl = [next_page_uri]
    FROM
           OPENJSON(@apiResponse)
               WITH (
                        [next_page_uri] NVARCHAR(2000) '$.next_page_uri'
                    );

                    -- we check for null of NextUrl and return STOP to stop the for loop
                    --if not we return out the Url to go back around and get more data
    SELECT CASE WHEN @NextUrl IS NOT NULL THEN CONCAT('https://api.twilio.com', @NextUrl)
                ELSE 'STOP'
           END AS [NextUrl];
</code></pre>
<p>The SSIS variable NextUrl get's overwritten with what is returned back out by the stored procedure.  If that does not equal STOP that continues the for loop, now passing that URL into the script task to go get the next set of data and so on...</p>
"
"63619028","Twilio API to SSIS data flow","<p>Attempting to read records from the Twilio API with the aim then to use this in a SSIS Data Flow and save the records in a SQL Server database. Using the API documentation from <a href=""https://www.twilio.com/docs/sms/api/message-resource#read-multiple-message-resources"" rel=""nofollow noreferrer"">https://www.twilio.com/docs/sms/api/message-resource#read-multiple-message-resources</a> I've been able to read the messages but a bit stuck with what to do next, I just want to use this as a source for the Data Flow so not trying to do anything too fancy in .net, more of a SQL guy so not very experienced with C#.</p>
<p>have I been thinking this would be too straight forward? Was assuming i'd be able to read the messages (doesn't look like this comes down as JSON, individual fields have already been split out using the Twilio.dll) put this into a variable which then i loop through each row and pass to the output buffer.</p>
<p>Essentially, was trying something like this:</p>
<pre><code>

    public override void CreateNewOutputRows()
    {
         string accountSid = &quot;AAAA&quot;;

         string authToken = &quot;1111&quot;;

        TwilioClient.Init(accountSid, authToken);

        var response = MessageResource.Read();

        foreach (var msg in response)
        {
            Output0Buffer.AddRow();
            Output0Buffer.ID = msg.Sid.ToString();
            Output0Buffer.Message = msg.Body.ToString();
        }
    }
```
</code></pre>
","<c#><.net><ssis><twilio-api>","2020-08-27 15:11:45","362","0","2","64441512","<p>Was able to get this working in C# eventually. Issue was with the versions of dlls being used where the most recent versions weren't working with each other.</p>
"
"63577687","How to add a date range in Azure Data Factory data flow","<h2>Working info</h2>
<p>I have two different sources of data set so I have created a dataflow in data factory in which for first data(A) set I am doing some transformation and loading into sink,in another data set(B) similarly am performing some transformation and loading into another sink.</p>
<h2>Issue</h2>
<p>Now I have some requirements in which there is date column DT_COLUMN_A(11-04-2020  01:17:40) in first data set(A)which needs to be compared with a date column DT_COLUMN_B(01-01-2020  16:32:00) in second data set (B) and store the compared output as a column in second dataset(B).</p>
<p>So I need the min and max(date range) of date column from dataset A ,apply it to min and max of date column to dataset B and find the dates which are matching in A and B and store it as YES if not matching NO.</p>
<h2>Code approach thought</h2>
<p>Logic needed:</p>
<p><code>if(min(DT_COLUMN_A) and max(DT_COLUMN_A) == min(DT_COLUMN_B) and max(DT_COLUMN_B) then YES else No.</code></p>
<p>I am trying to achieve this in ADF data flow but unable to do it.</p>
","<date><azure-functions><date-range><azure-data-factory>","2020-08-25 11:18:12","1269","0","2","63583234","<p>To get MIN and MAX of a dataset in ADF, you will need the Aggregate transformation. Create new columns called MinA, MinB, MaxA, MaxB from each of the relative streams in your data flow using Aggregate. Set the aggregate function to MIN and MAX appropriately for each. Then, you'll be able to set an iif() expression afterward, or use a Filter or Conditional Split transformation that uses those stored min &amp; max values.</p>
"
"63577687","How to add a date range in Azure Data Factory data flow","<h2>Working info</h2>
<p>I have two different sources of data set so I have created a dataflow in data factory in which for first data(A) set I am doing some transformation and loading into sink,in another data set(B) similarly am performing some transformation and loading into another sink.</p>
<h2>Issue</h2>
<p>Now I have some requirements in which there is date column DT_COLUMN_A(11-04-2020  01:17:40) in first data set(A)which needs to be compared with a date column DT_COLUMN_B(01-01-2020  16:32:00) in second data set (B) and store the compared output as a column in second dataset(B).</p>
<p>So I need the min and max(date range) of date column from dataset A ,apply it to min and max of date column to dataset B and find the dates which are matching in A and B and store it as YES if not matching NO.</p>
<h2>Code approach thought</h2>
<p>Logic needed:</p>
<p><code>if(min(DT_COLUMN_A) and max(DT_COLUMN_A) == min(DT_COLUMN_B) and max(DT_COLUMN_B) then YES else No.</code></p>
<p>I am trying to achieve this in ADF data flow but unable to do it.</p>
","<date><azure-functions><date-range><azure-data-factory>","2020-08-25 11:18:12","1269","0","2","73453379","<p>I managed to get something similar to work using using a mapLoop() expression to first build an array of dates in a derived column transformation followed by a flatten transformation</p>
<p><a href=""https://stackoverflow.com/a/73453351/12592985"">https://stackoverflow.com/a/73453351/12592985</a></p>
"
"63514310","Syncs of RDBMS to Redis","<p>I have RDS that serves as the source of truth. A challenge that I have is to have this database partially synced to Redis to make it available for a server app to use. This would be a one way sync always going in one direction, but I can't wrap my head around what tools do I use to make these syncs happen preferably in an optimized way. In other words, rather than loading the entire data set it would be great if deltas are synced only.</p>
<p>I hope someone can give some insight on how this can be done.
Thank you!</p>
","<caching><redis><data-synchronization>","2020-08-20 23:09:55","1612","3","1","63519400","<p>Most RDBMS provide a way to subscribe to the transaction allowing you to put in place a &quot;change data capture&quot; event streams.</p>
<p>In this case you can subscribe to the databases events, and put the change or updated record inside Redis.</p>
<p>You can for example use <a href=""https://debezium.io/"" rel=""noreferrer"">Debezium</a> to capture the event, as you can see Debezium community has connectors for various datasources.</p>
<pre><code>---------          ------------         --------- 
| RDBMS |========&gt;&gt;| Debezium | ======&gt; | Redis |
---------          ------------         ---------

</code></pre>
<p>This <a href=""https://github.com/tgrall/redis-microservices-demo"" rel=""noreferrer"">demonstration</a> (mostly Java) shows this (a little richer since it is using Redis Streams and an intermediate state), the event is capture by this <a href=""https://github.com/tgrall/redis-microservices-demo/blob/18ae54e17adad1e4edbfaa1cdd08df3f69c64301/db-to-streams-service/src/main/java/io/redis/demos/services/db/events/streams/listener/CDCEventListener.java#L87"" rel=""noreferrer"">method</a> so capture Insert/Update/Delete in MySQL and sending the information to Redis.</p>
<p>Another option, that does not match you need but interesting, is to do a &quot;write behind&quot; Cache. In this case you update the Cache in Redis and Redis push the update in RDBMS using <a href=""http://redisgears.io/"" rel=""noreferrer"">Gears</a>.</p>
<p>You can find more information about this &quot;write behind&quot; with Redis Gears in this <a href=""https://github.com/RedisGears/rgsync"" rel=""noreferrer"">GitHub repo</a>.</p>
<pre><code>---------          ---------         --------- 
| Redis |========&gt;&gt;| Gears | ======&gt; | RDBMS |
---------          ---------         ---------

</code></pre>
"
"63483689","which command spring cloud data flow uses to run tasks?","<p>Currently I am not using spring cloud dataflow, though I am creating tasks in my application and running them through command line. Currentlt I am thinking to move to spring cloud data flow via kubernetes. So my concern is first, how can I reuse my current code? and which command does data flow deployer use to run tasks ?</p>
","<spring-cloud-dataflow><spring-cloud-task>","2020-08-19 09:11:48","186","0","1","63491588","<p>This <a href=""https://dataflow.spring.io/docs/batch-developer-guides/getting-started/task/"" rel=""nofollow noreferrer"">guide</a> will get you started on how to setup a task on Spring Cloud Data Flow.</p>
<p>In Summary, this is what you need to do:</p>
<ul>
<li>Register your task as a <code>task</code> application type in SCDF</li>
<li>Create a task definition</li>
<li>Launch your task with arguments and properties</li>
</ul>
<p>You can either use Spring Cloud Data Flow Dashboard or Shell to perform the above operations.</p>
"
"63470756","getIMEI() is returning null in Android (device owner)","<p>My app is a device owner but still getIMEI() is returning null in Android 9 and I have provided &lt;READ_PHONE_STATE&gt; permission also.
</p>
<p>As per the Android Developers Documentation : (<a href=""https://developer.android.com/about/versions/10/privacy/changes#non-resettable-device-ids"" rel=""nofollow noreferrer"">https://developer.android.com/about/versions/10/privacy/changes#non-resettable-device-ids</a>)</p>
<p>Note: If your app is the device or profile owner app, you need only the READ_PHONE_STATE permission to access non-resettable device identifiers, even if your app targets Android 10 or higher. Also, if your app has special carrier permissions, you don't need any permissions to access the identifiers.</p>
","<android><mdm><device-owner>","2020-08-18 14:26:25","328","0","1","63479027","<p>If your targetSdkVersion is 29 (Android 10) or later, then you will get a null value when requesting IMEI (or one of several other identifiers) if you're running Android 9 or earlier. See the doc about <a href=""https://developer.android.com/about/versions/10/privacy/changes"" rel=""nofollow noreferrer"">Privacy Changes in Android 10</a> for more info.</p>
"
"63360702","logstash vs spring cloud data flow, which one is suitable for data preprocessing?","<p>I'm using spring boot along with elasticsearch to make a search system on my website.</p>
<p>I've some data that i need to push in elastic search, this data ( a product for example )  must be processed before ( passed to another micro-service that filters the JSON, adds some fields for a better search result do some calculations and return the object i want to store ). is it possible to do so with log stash, or do i need to use Spring Cloud Data Flow ? thanks in advance.</p>
<p>what i want to do:</p>
<pre class=""lang-java prettyprint-override""><code>save a product ( product service ) 
log the saved product or stream it.
process it before storage ( another service ) 
save the document ( elastic search server )

</code></pre>
<p>Thanks in advance.</p>
","<spring-boot><elasticsearch><elastic-stack><spring-cloud-dataflow>","2020-08-11 14:47:58","279","0","1","63370582","<p>Obviously it depends on various factors but I can try to provide some insights on Spring Cloud Data Flow from the technical standpoint.</p>
<p>If you want to construct a streaming pipeline where your filtering apps are connected via a messaging system that does this flow of data processing, you can checkout Spring Cloud Data Flow.</p>
<p>Spring Cloud Data Flow (and the underlying framework supports such as Spring Cloud Stream and Spring Cloud Task) provides the operational benefits over how you manage your streaming pipelines but it may not make sense if you don't need a data pipeline with a messaging system etc., In those cases, you would just stick to a simple Spring Boot app that does this whole filtering model. As soon as you start exploring the distribution of these applications loosely coupled via messaging system, Spring Cloud Data Flow would be handy.</p>
<p>Please checkout SCDF <a href=""https://dataflow.spring.io"" rel=""nofollow noreferrer"">guide</a> to understand some of the <a href=""https://dataflow.spring.io/docs/feature-guides/"" rel=""nofollow noreferrer"">features</a> and <a href=""https://dataflow.spring.io/docs/recipes/"" rel=""nofollow noreferrer"">recipes</a> to know more about what SCDF can offer and choose what fits in your case.</p>
"
"63288578","Unable to specify parameters to parameterized source data set in ADF data flow","<p>I have a data flow that has a parameter: <code>TableName</code>.  The dataset that is used as a source within the flow is parameterized for a <code>TableName</code> parameter (SQL Server dataset).  When selecting this dataset in source setting within the ADF dataflow, it does not allow me to set the <code>TableName</code> parameter as it does when setting the source within a standard CopyActivity.</p>
<p>So how does one use a parameterized dataset in a dataflow if it never allows you to set the parameters?</p>
<p>UPDATE: The settings are actually on the DataFlow activity itself.</p>
<p><a href=""https://i.stack.imgur.com/CKg6y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CKg6y.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2020-08-06 17:08:47","1383","2","2","63292019","<p>In data flow, you will set the dataset parameter in Debug Settings when designing/debugging your data flow. You can then set the parameter at runtime in the data flow activity settings in the pipeline.</p>
"
"63288578","Unable to specify parameters to parameterized source data set in ADF data flow","<p>I have a data flow that has a parameter: <code>TableName</code>.  The dataset that is used as a source within the flow is parameterized for a <code>TableName</code> parameter (SQL Server dataset).  When selecting this dataset in source setting within the ADF dataflow, it does not allow me to set the <code>TableName</code> parameter as it does when setting the source within a standard CopyActivity.</p>
<p>So how does one use a parameterized dataset in a dataflow if it never allows you to set the parameters?</p>
<p>UPDATE: The settings are actually on the DataFlow activity itself.</p>
<p><a href=""https://i.stack.imgur.com/CKg6y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CKg6y.png"" alt=""enter image description here"" /></a></p>
","<azure-data-factory>","2020-08-06 17:08:47","1383","2","2","63297798","<p>As I understand, you mean that you can set the <code>TableName</code> in Copy Active and can't in Data Flow.</p>
<p>In Copy Active, we could set parameter like this:
<a href=""https://i.stack.imgur.com/BNStP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BNStP.png"" alt=""enter image description here"" /></a></p>
<p>But in Data Flow, the UI looks like:
<a href=""https://i.stack.imgur.com/YBCHA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YBCHA.png"" alt=""enter image description here"" /></a></p>
<p>I have a workaround is that <strong>we could choose the table with <code>Query</code> in <code>Source operations</code></strong>:</p>
<pre><code>'select * from ' +$TableName
</code></pre>
<p><a href=""https://i.stack.imgur.com/DZurA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DZurA.png"" alt=""enter image description here"" /></a></p>
<p>Pipeline parameter:</p>
<p><a href=""https://i.stack.imgur.com/Tdzhh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Tdzhh.png"" alt=""enter image description here"" /></a></p>
<p>Data Flow parameter:
<a href=""https://i.stack.imgur.com/xxwxL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xxwxL.png"" alt=""enter image description here"" /></a></p>
<p>It works well.</p>
"
"63280078","Azure data factory data flow json to SQL","<p>I have a source JSON file with hierarchical data, which I need to sink in two SQL tables(relational).
The JSON is as below</p>
<pre><code>{
&quot;orders&quot;:[
  {
    &quot;orderid&quot;:&quot;30933&quot;,
    &quot;ordername&quot;:&quot;abc&quot;,
    &quot;items&quot;:[
      {
        &quot;itemid&quot;:1,
        &quot;itemstatus&quot;:&quot;Failed&quot;
      },
      {
        &quot;itemid&quot;:2,
        &quot;itemstatus&quot;:&quot;Failed&quot;
      }
    ]
  },
  {
    &quot;orderid&quot;:&quot;308320&quot;,
    &quot;ordername&quot;:&quot;xyz&quot;,
    &quot;items&quot;:[
      {
        &quot;itemid&quot;:5,
        &quot;itemstatus&quot;:&quot;Succeeded&quot;
      }
    ]
  }
]
}
</code></pre>
<p>My SQL holding two tables Order and OrderItem with OrderID primary and foreign key.</p>
<p>Now I have an Azure data factory data flow with source as above JSON and I need to park all data relational in respective tables.</p>
<p>So here I need OrderId(30933,308320) and OrderName(abc,xyz) will go into Order table and respective items data go into OrderItem table(which reference OrderId from Order table). In this case Order table have 2 and OrderItem table have 3 entries.</p>
","<sql><json><azure-data-factory><dataflow>","2020-08-06 08:53:38","727","0","1","63294676","<p>We can not achieve that in one copy active.</p>
<p>We could using two copy actives in one pipeline, I tested and it succeed. You could follow my steps bellow:</p>
<ol>
<li>Copy active1: copy the data from <code>Orders</code>(orderid and ordername) to table <code>Orders</code>.</li>
<li>Copy active2: copy the data from <code>items</code>(itemid and itemstatus) to
table <code>OrderItems</code>.</li>
</ol>
<p><strong>Note:</strong></p>
<ol>
<li><p>Copy active 1 and 2 use the same json file as the source. The
differences are in the Mapping settings.</p>
</li>
<li><p>Copy active 1 sink is Azure SQL database table <code>Orders</code>, Copy active
2 sink is Azure SQL database table <code>OrderItems</code>.</p>
</li>
</ol>
<p>To make you understand it clearly, I made two GIF pictures.</p>
<p><strong>Mapping settings in Copy active 1:</strong>
<a href=""https://i.stack.imgur.com/fcach.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fcach.gif"" alt=""enter image description here"" /></a></p>
<p><strong>Mapping settings in Copy active 2:</strong>
<a href=""https://i.stack.imgur.com/oqchZ.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oqchZ.gif"" alt=""enter image description here"" /></a></p>
<p>Run the pipeline:</p>
<p><a href=""https://i.stack.imgur.com/uDMHW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uDMHW.png"" alt=""enter image description here"" /></a></p>
<p>Check the data in table:</p>
<p><a href=""https://i.stack.imgur.com/INVlG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/INVlG.png"" alt=""enter image description here"" /></a></p>
<p>The limit is that we only could get the first element of <code>items</code>, we can not choose the collection reference both.</p>
<p><a href=""https://i.stack.imgur.com/wJL0S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wJL0S.png"" alt=""enter image description here"" /></a></p>
<p>Update:</p>
<p>Congratulations that you achieved it in another way: flatten/transpose the data using data flow and then maintain it to pour in a relational SQL table.</p>
"
"63259808","Problem with data flow between Angular components","<p>I have a webpage structure like the following:</p>
<p><a href=""https://i.stack.imgur.com/8gBKT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8gBKT.png"" alt=""page layout"" /></a></p>
<ul>
<li>Button should select/unselect all the rows of the table.</li>
<li>Button and Table are separate components injected into the page.</li>
<li>Table is using Material's <code>SelectionModel</code>.</li>
</ul>
<p>From the page I'm reaching <code>selection.selected</code> of the Table through <code>@ViewChild</code> decorator. Its easy to place the selection logic in the page as it's where the table is injected, but this requires to place the same logic in every page where the button needs to be.<br />
I'd like to simplify this by placing the logic inside the Button component so all that should be done to have the functionality is to insert its selector into page's template. But there's a catch I can't overcome:</p>
<p><strong>How to reach <code>selection.selected</code> of the Table from the Button component?</strong></p>
<p>What is the best practice to achieve this?</p>
<p>Here's how does it look like in the Page class</p>
<pre class=""lang-js prettyprint-override""><code>    @ViewChild(TableComponent)
    private table: TableComponent;


    // ...


    checkAll(): void {
        const numSelected = this.table.selection.selected.length;
        const numRows = this.table.dataSource.data.length;
        const result = numSelected === numRows;
        result
            ? this.table.selection.clear()
            : this.table.dataSource.data.forEach(row =&gt; this.table.selection.select(row));
    }
</code></pre>
<p>And here's the template html file:</p>
<pre class=""lang-html prettyprint-override""><code>    &lt;button (click)=&quot;checkAll()&quot;&gt;Check all&lt;/button&gt;
    &lt;app-table&gt;&lt;/app-table&gt;
</code></pre>
","<angular><angular-material>","2020-08-05 06:46:21","69","0","2","63259942","<p>Use a service with a RxJS Subject. The service can be injected both in the button component and the table component. Through the constructor. When the button is
Clicked you trigger the service subject that the table component subscribe to. Google ‘subject service angular’</p>
<p>Alternative you could create a component that includes the button and the table. The button would then not need to be a component. And the button click logic would only be implemented once.</p>
"
"63259808","Problem with data flow between Angular components","<p>I have a webpage structure like the following:</p>
<p><a href=""https://i.stack.imgur.com/8gBKT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8gBKT.png"" alt=""page layout"" /></a></p>
<ul>
<li>Button should select/unselect all the rows of the table.</li>
<li>Button and Table are separate components injected into the page.</li>
<li>Table is using Material's <code>SelectionModel</code>.</li>
</ul>
<p>From the page I'm reaching <code>selection.selected</code> of the Table through <code>@ViewChild</code> decorator. Its easy to place the selection logic in the page as it's where the table is injected, but this requires to place the same logic in every page where the button needs to be.<br />
I'd like to simplify this by placing the logic inside the Button component so all that should be done to have the functionality is to insert its selector into page's template. But there's a catch I can't overcome:</p>
<p><strong>How to reach <code>selection.selected</code> of the Table from the Button component?</strong></p>
<p>What is the best practice to achieve this?</p>
<p>Here's how does it look like in the Page class</p>
<pre class=""lang-js prettyprint-override""><code>    @ViewChild(TableComponent)
    private table: TableComponent;


    // ...


    checkAll(): void {
        const numSelected = this.table.selection.selected.length;
        const numRows = this.table.dataSource.data.length;
        const result = numSelected === numRows;
        result
            ? this.table.selection.clear()
            : this.table.dataSource.data.forEach(row =&gt; this.table.selection.select(row));
    }
</code></pre>
<p>And here's the template html file:</p>
<pre class=""lang-html prettyprint-override""><code>    &lt;button (click)=&quot;checkAll()&quot;&gt;Check all&lt;/button&gt;
    &lt;app-table&gt;&lt;/app-table&gt;
</code></pre>
","<angular><angular-material>","2020-08-05 06:46:21","69","0","2","63263598","<p>You can make use of RxJS's <code>Subject</code>, put it in service which you'll inject via constructor to both the button and table components.<br />
Your service implementation may look like the following:</p>
<pre class=""lang-js prettyprint-override""><code>// service
export class TableService {
    private event: Subject&lt;void&gt; = new Subject&lt;void&gt;();
    public event$ = this.event.asObservable();

    public emitEvent(event?: MouseEvent): void {
        this.event.next();
    }
}
</code></pre>
<p>Then you take this service and inject it in your button component:</p>
<pre class=""lang-js prettyprint-override""><code>// button component
export class Button {

    constructor(private tableService: TableService) {}

    update(): void {
        this.tableService.emitEvent();
    }
}
</code></pre>
<p>you can activate the <em>update()</em> function on click in html template file.<br />
Then implement the selection logic in your table and just use the <code>event$</code> to trigger it:</p>
<pre class=""lang-js prettyprint-override""><code>// table component
export class Table {

    constructor(private tableService: TableService) {
        this.tableService.event$.subscribe(() =&gt; this.checkAll());
    }

    checkAll(): void {
        // your selection logic goes here
    }
}
</code></pre>
"
"63224082","Cannot set the app as profile owner - set-profile-owner command stuck","<p>I'm trying to set the profile owner for one of my application using command on my Android 10 device:</p>
<p>adb shell dpm set-profile-owner com.example.blockcamera/.BlockCameraDeviceAdminReceiver</p>
<p>When I press Enter nothing happens - the command will be stuck with a cursor flashing on new line. I have to Ctrl+C to kill it.</p>
<p>The command works fine on Android Emulator and the app works as expected afterwords.
I am using android:testOnly flag in AndroidManifest.xml</p>
<p>Things tried:</p>
<ol>
<li>Used --user current option but no luck</li>
<li>Also tried set-device-owner command but same result, the command gets stuck.</li>
</ol>
","<android><android-source><mdm><device-policy-manager><dpm>","2020-08-03 05:16:12","445","-2","1","63240051","<p>I reset the device and it worked later.</p>
"
"63211262","Create Dynamic Json on the go in expression functions inside mapping data flow (Azure Data Factory)","<p>I am trying to convert a csv file into json schema based collection. For json conversion, the only way I found was creating subcolumns for a column if you want hierarchy, Or you can use expressions like @(key1=value, @(key2=value2)) for json structures. What I want to do is Either  pick key1 or key2 as parameters from dataflow/pipeline Or use it in a loop over an array such as map(array,@(#item=#item+2)).
But It restricts me from doing that. However, I am able to traverse over values, But keys are something that needs to be only hardcoded. Please suggest a workaround if any.</p>
","<json><mapping><azure-data-factory><dataflow>","2020-08-02 00:25:34","393","0","1","63223186","<p>In my opinion:</p>
<blockquote>
<p>1: You can import the csv file into Azure SQL Database by using <a href=""https://learn.microsoft.com/en-us/azure/data-factory/tutorial-copy-data-portal"" rel=""nofollow noreferrer"">copy activity</a><br />
2: Then you can <a href=""https://learn.microsoft.com/en-us/azure/azure-sql/database/json-features"" rel=""nofollow noreferrer"">work with JSON features in Azure SQL Database</a></p>
</blockquote>
"
"63210891","best solution to sync 2 mysql databases in a laravel project","<p>I have a laravel project that is supposed to run in a localhost.
but we needed to add the ability to do some modification while user of the app is away from his pc that the app runs on it's host.
I know i can deploy the whole project in an online server but this solution is not an option till now.
we have only a weak online server (it's slower a lot than localhost);
so we can use this weak online server for these situations when the user wants to do some modifications remotely which would happen from time to time almost two or three times a day while the localhost will have the heavy work of the rest of the day which may be more than 3 or 4 hundreds processes a day.
i can't make the whole load on the online server while it's very slow like that and we don't need online benefits a lot, just for those two or three times remote modifications that the app user may or may not need, so i can't trade off localhost speed for online benefits which i need only two or three times a day.
what solution can i do.
i knew about master-slave and master-master replication but it's not an option too.</p>
<p>is there any ideas and thank you in advance.</p>
<p>-------------- about the two environments (local and online)------------
local is windows running xamp stack (apache, mysql, php)
server is linux (don't know actually which distro but any way i can't install any tools there ... just php packages with composer)</p>
","<mysql><database><laravel><data-synchronization>","2020-08-01 23:08:57","1870","0","2","63211049","<p>I had the same problem for uploading my laravel project</p>
<p>just use FileZilla to upload your project, even with the worst internet speed you can do it.
and save yourself the trouble.</p>
"
"63210891","best solution to sync 2 mysql databases in a laravel project","<p>I have a laravel project that is supposed to run in a localhost.
but we needed to add the ability to do some modification while user of the app is away from his pc that the app runs on it's host.
I know i can deploy the whole project in an online server but this solution is not an option till now.
we have only a weak online server (it's slower a lot than localhost);
so we can use this weak online server for these situations when the user wants to do some modifications remotely which would happen from time to time almost two or three times a day while the localhost will have the heavy work of the rest of the day which may be more than 3 or 4 hundreds processes a day.
i can't make the whole load on the online server while it's very slow like that and we don't need online benefits a lot, just for those two or three times remote modifications that the app user may or may not need, so i can't trade off localhost speed for online benefits which i need only two or three times a day.
what solution can i do.
i knew about master-slave and master-master replication but it's not an option too.</p>
<p>is there any ideas and thank you in advance.</p>
<p>-------------- about the two environments (local and online)------------
local is windows running xamp stack (apache, mysql, php)
server is linux (don't know actually which distro but any way i can't install any tools there ... just php packages with composer)</p>
","<mysql><database><laravel><data-synchronization>","2020-08-01 23:08:57","1870","0","2","67109245","<p>To answer your question, If I were you, I will create a <strong>sync_data</strong> table in the application. And the sync data table will have the responsibility to record the changes occurring for various entities.</p>
<p>For example, if you change customer data in the localhost, you will save an entry to <strong>sync_data</strong> like <strong>type=customer, id=10, action=update, synced=No</strong>. And using a cron you can push these updates -fetching the customer record by saved id-  to your online server in regular intervals, which will not make your online server busy. Furthermore, your online users will have the latest data at least.</p>
"
"63197465","How to process excel in azure data flow","<p>I have multiple excel files in a blob storage and sheet name in those files are different while loading in the files in azure sql database in azure data flow how the sheet name to be handled.
Note : I have to do it in azure data flow,i tried doing in azure pipeline its working</p>
","<azure><azure-data-factory>","2020-07-31 18:44:05","755","0","1","63222627","<p>Data factory supports Excel connector now, but still couldn't support dynamic choose the sheet index for now.</p>
<p>Others have added comment in <a href=""https://feedback.azure.com/forums/270578-data-factory/suggestions/19807720-add-excel-as-source"" rel=""nofollow noreferrer"">Data Factory feedback</a> but didn't get reply:
<a href=""https://i.stack.imgur.com/ukG6U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ukG6U.png"" alt=""enter image description here"" /></a></p>
<p>I think you also could post a <a href=""https://feedback.azure.com/forums/270578-data-factory"" rel=""nofollow noreferrer"">new feedback</a>, we could help you vote up it and Data Factory Product Team may see it.</p>
"
"63160791","NiFi: Calculate a processor data flow over a period of time (> 5 mins)","<p>Looking for data flow statistics for the recent one week of time (bytesIn, bytesOut). Using NiFi REST API endpoint <code>[GET] /nifi-api/processors/{id}</code>, got below statistics for last five minutes. Is there any existing API end-point to retrieve data flow statistics for one week of time?</p>
<pre><code>{
    &quot;id&quot;: &quot;1234aa-1234-1f23-1f23-123456ed51f1a&quot;,
    &quot;status&quot;: {
        &quot;name&quot;: &quot;MyConsumeKafkaProcessor&quot;,
        &quot;runStatus&quot;: &quot;Running&quot;,
        &quot;statsLastRefreshed&quot;: &quot;14:39:55 EDT&quot;,
        &quot;aggregateSnapshot&quot;: {
            &quot;type&quot;: &quot;ConsumeKafka_0_10&quot;,
            &quot;runStatus&quot;: &quot;Running&quot;,
            &quot;executionNode&quot;: &quot;ALL&quot;,
            &quot;bytesRead&quot;: 0,
            &quot;bytesWritten&quot;: 23948016,
            &quot;read&quot;: &quot;0 bytes&quot;,
            &quot;written&quot;: &quot;22.84 MB&quot;,
            &quot;flowFilesIn&quot;: 0,
            &quot;bytesIn&quot;: 0,
            &quot;input&quot;: &quot;0 (0 bytes)&quot;,
            &quot;flowFilesOut&quot;: 2188,
            &quot;bytesOut&quot;: 23948016,
            &quot;output&quot;: &quot;2,188 (22.84 MB)&quot;,
            &quot;taskCount&quot;: 1179,
            &quot;tasksDurationNanos&quot;: 15974094510,
            &quot;tasks&quot;: &quot;1,179&quot;,
            &quot;tasksDuration&quot;: &quot;00:00:15.974&quot;,
            &quot;activeThreadCount&quot;: 0,
            &quot;terminatedThreadCount&quot;: 0
        }
    }
}
</code></pre>
","<api><rest><apache-nifi>","2020-07-29 19:08:20","449","0","1","63163802","<p>I had a similar issue, trying to see what comes in over a period of time from Kafka.</p>
<p>I used counter with a frequency of 1 min.</p>
<p><strong>so is like this :</strong></p>
<p><strong>1 - ConsumeKafka</strong></p>
<p><strong>2 - Capture Records out of Kafka</strong></p>
<p><strong>3 - UpdateCounter IN (as a cloned success connection) - the deltas will be records count oof the kafka payload</strong></p>
<p><strong>4 - do you stuff with the data (enrich/change bla bla bla)</strong></p>
<p><strong>5 - capture records count before persisting data</strong></p>
<p><strong>6 - Update OUT counter</strong></p>
<p><strong>7 - Persists the data in (DB/S3/ etc)</strong></p>
<p>I then have flow that interogates the <code>https://${hostname(true)}:8443/nifi-api/counters</code> every 60 sec.</p>
<p>I land thiis data in monitoring db repo.</p>
<p>I use this to messue data delivery <strong>IN/OUT</strong> of NiFi and look for dropouts, thruput,etc.</p>
<p>I do the same with my Source Data, where in Kafka case i capture the num of msg generated evey min.</p>
"
"63118171","Connect R to a Power BI Service data flow","<p>Does anyone know if it is possible to connect R to a Power BI Service data flow?</p>
<p>I'd like to do all ETL in Power BI data flow and analytics on R. I know it's not possible to run R scripts in data flows, like in Power BI Desktop Power Query, so I could download the data to R Studio and work from there.</p>
<p>Is this possible?</p>
","<r><powerbi>","2020-07-27 15:07:16","853","1","1","67125151","<p>I'm also interested in this, I don't see any ways to download it via API (<a href=""https://learn.microsoft.com/en-us/rest/api/power-bi/dataflows"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/rest/api/power-bi/dataflows</a>), but I have heard of people being able to download PBIX files from PowerBI, and then theoretically you could read the data from the PBIX file using the PBIXr package (<a href=""https://cran.r-project.org/web/packages/pbixr/vignettes/explore.html"" rel=""nofollow noreferrer"">https://cran.r-project.org/web/packages/pbixr/vignettes/explore.html</a>). I don't know too much about it but I haven't been able to find any threads on this.</p>
"
"63106983","PowerBI report service - data flow questions","<p>This is what I am trying to do: I have various SQL server databases with data. I created views in all of them. All views will need to be imported, and I specify their relationships. I want this to be refreshed nightly. I want to build various reports of the same data source.</p>
<ol>
<li><p>Do I have to use a PowerBI desktop application to import data into PowerBI Report Service? [I have done this so far, but then can create new reports in the cloud on existing data. It would make sense to connect directly from PowerBI report service to my SQL servers.]</p>
</li>
<li><p>Once I uploaded data using a desktop application (as I have done so far), how can I view the data model in the report service once it is uploaded in the cloud?</p>
</li>
<li><p>In order to get routinely refreshed data I need to setup a gateway. Is the local PowerBI desktop application still involved in this process, or could I [in theory] delete the local desktop application that pushed the data in initially?</p>
</li>
</ol>
","<powerbi><powerbi-desktop>","2020-07-26 23:51:00","119","-2","1","63112590","<p>For your questions:</p>
<ol>
<li><p>You have two options, use PBI Desktop to connect to the data using import/direct query, then load it to the service. You can use dataflows to create an import based on your views, but you will then need to create reports from those. Using dataflows, you'll have to set up a refresh schedule, then for the dataset(s) built on top of those, you'll have to set another refresh schedule.
You will be limited to the dataset sizes of 1GB for the workspace if importing data. You cannot use direct query on dataflows (unless you have enhanced compute with PBI premium). Once the dataset is loaded, you can then create new reports in the service or via desktop on top of that dataset. If possible it is recommended to use direct query.</p>
</li>
<li><p>To see the data model, you can use desktop to connect to PBI Service Dataset. This will connect in 'Live Connection' mode, and will be limited to that one dataset, you can't add others to it, Excel, CSV, SQL etc. You can also use Analyse in Excel, a plugin for Excel, that can connect to the data model. You can create new reports in the service for existing data models as well.</p>
</li>
<li><p>When creating the report in PBI Desktop it does not use the Gateway, you connect to your data sources as normal, then once you load the dataset to Power BI it will match the data sources in the file to the ones set up in the Gateway Admin settings. So you will still need PBI Desktop to create reports, but the gateway is there for the refreshing. The Desktop is not used in the process for refreshing. You could delete the workbook or application, but if you have to make changes, what will you refer to? (You could download a copy of the report from the service).+ It is easier to make changes in the desktop app, then the service, as there is a feature difference between dataset creation in the desktop vs service.</p>
</li>
</ol>
"
"63058890","Not able to get logs related to azure data factory mapping data flows from log analytics","<p>We are working on implementing a custom logging solution. Most of the information what we need is already present in log analytics from data factory analytics solution but for getting log info on data flows,  there is a challenge. When querying, we get this error in output. &quot;Too large to parse&quot;. </p>
<p>Since data flows are complex and critical piece in a pipeline, we are in desperate need to get data like rows copied, skipped, read etc of each activities with in data flow. can you pls help how to get those info?</p>
","<azure-data-factory><azure-log-analytics>","2020-07-23 16:26:55","533","0","1","63142696","<p>You can get the same information shown in the ADF portal UI by making a POST request to the below REST endpoint. You can find more information and read about authentication on the following link <code>https://learn.microsoft.com/en-us/rest/api/datafactory/pipelineruns/querybyfactory</code></p>
<p>You can choose to query by factory or for a specific pipeline run id depending on your needs.</p>
<pre><code>https://management.azure.com/subscriptions/&lt;subscription id&gt;/resourcegroups/&lt;resource group name&gt;/providers/Microsoft.DataFactory/factories/&lt;ADF resource Name&gt;/pipelineruns/&lt;pipeline run id&gt;/queryactivityruns?api-version=2018-06-01
</code></pre>
<p>Below is an example of the data you can get from one stage:</p>
<pre><code>{
  &quot;stage&quot;: 7,
  &quot;partitionTimes&quot;: [
    950
  ],
  &quot;lastUpdateTime&quot;: &quot;2020-07-28 18:24:55.604&quot;,
  &quot;bytesWritten&quot;: 0,
  &quot;bytesRead&quot;: 544785954,
  &quot;streams&quot;: {
    &quot;CleanData&quot;: {
      &quot;type&quot;: &quot;select&quot;,
      &quot;count&quot;: 241231,
      &quot;partitionCounts&quot;: [
        950
      ],
      &quot;cached&quot;: false
    },
    &quot;ProductData&quot;: {
      &quot;type&quot;: &quot;source&quot;,
      &quot;count&quot;: 241231,
      &quot;partitionCounts&quot;: [
        950
      ],
      &quot;cached&quot;: false
    }
  },
  &quot;target&quot;: &quot;MergeWithDeltaLakeTable&quot;,
  &quot;time&quot;: 67589,
  &quot;progressState&quot;: &quot;Completed&quot;
}
</code></pre>
"
"63042888","How does MDM restrict access to the public version of application?","<p>I am currently using MDM on a mobile application (React Native/Expo Build) I have a scenario that I would like discussed:</p>
<p>Preface:
Some of the Firms that use our application have an MDM platform (Intune, Mobile Iron, Airwatch) and others do not.
Therefore we will have two application package:</p>
<ol>
<li>Public users that do not MDM software can simply download application from IOS or Android App store</li>
<li>A wrapped application version for those firms that use MDM Software</li>
</ol>
<p>FirmA has Intune applied and the wrapped version of our app
User at FirmA decided to go on to the public IOS/GooglePlay store and download application</p>
<p>I need a way to stop this user at FirmA from using the public version of the app and restrict them to ONLY USE the MDM build of application.</p>
<p>Is this possible?
Does this need to be applied on the MDM side or application side?</p>
","<react-native><mdm><intune><mobileiron>","2020-07-22 20:41:51","263","0","2","63206892","<p>You can use different application IDs for different builds. Then, you can restrict the public build on the MDM side and allow only MDM build.</p>
"
"63042888","How does MDM restrict access to the public version of application?","<p>I am currently using MDM on a mobile application (React Native/Expo Build) I have a scenario that I would like discussed:</p>
<p>Preface:
Some of the Firms that use our application have an MDM platform (Intune, Mobile Iron, Airwatch) and others do not.
Therefore we will have two application package:</p>
<ol>
<li>Public users that do not MDM software can simply download application from IOS or Android App store</li>
<li>A wrapped application version for those firms that use MDM Software</li>
</ol>
<p>FirmA has Intune applied and the wrapped version of our app
User at FirmA decided to go on to the public IOS/GooglePlay store and download application</p>
<p>I need a way to stop this user at FirmA from using the public version of the app and restrict them to ONLY USE the MDM build of application.</p>
<p>Is this possible?
Does this need to be applied on the MDM side or application side?</p>
","<react-native><mdm><intune><mobileiron>","2020-07-22 20:41:51","263","0","2","63790550","<p>So any application downloaded from the google play store is signed by google and can only allowed to install on the device if the previous version has been also installed from the play store application as the signature of both needs to matched.</p>
<p>If you can supply your APP to the FirmA by just signing with your keys, then user cannot upgrade their App from the play store unless you provide them updated version locally signed by you.</p>
<p>Alternatively you can also think of have two different versions. The one you supply to FirmA is way higher version than the updated one on the play store. So your application running on their devices with higher number will never allowed to any application installed from the play store.</p>
"
"63022589","How to use custom processor on spring cloud data flow?","<p>Here is the Stream I intend to implement:</p>
<p><a href=""https://i.stack.imgur.com/5ehza.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5ehza.png"" alt=""Spring Cloud DataFlow Stream"" /></a></p>
<p>It is supposed to read records from jdbc, transform to json and write on another database thru jdbc.</p>
<p>For this I have implemented (using the new functional approach):</p>
<pre><code>@SpringBootApplication
public class StreamAppApplication {
    private static ObjectMapper objectMapper;

    public static void main(String[] args) {
        SimpleModule module = new SimpleModule();
        module.addSerializer(new ResultSetSerializer());

        objectMapper = new ObjectMapper().registerModule(new ParameterNamesModule())
                .registerModule(new Jdk8Module())
                .registerModule(new JavaTimeModule())
                .disable(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS);
        objectMapper.registerModule(module);

        SpringApplication.run(StreamAppApplication.class, args);
    }

    @Bean
    public Function&lt;ResultSet, String&gt; recordToJson() {
        return value -&gt; {
            try {
                return objectMapper.writeValueAsString(value);
            } catch (JsonProcessingException e) {
                throw new IllegalStateException(&quot;Falha conversão json&quot;, e);
            }
        };
    }
}
</code></pre>
<p>On the application.properties</p>
<pre><code>spring.cloud.stream.function.definition=recordToJson
</code></pre>
<p>Then I have imported it on the web UI as app of type TRANSFORM. It appeared on the UI with the transform classification and no parameters.</p>
<p>How do I use it?</p>
","<spring-cloud-dataflow>","2020-07-21 20:29:04","173","0","1","63024716","<p>You may want to review and follow the <a href=""https://dataflow.spring.io/docs/2.6.0.SNAPSHOT/recipes/functional-apps/scst-function-bindings/"" rel=""nofollow noreferrer"">function-bindings recipe</a> from the Microsite to get an understanding of what needs explicitly configured.</p>
<p>From what I can tell, you're likely missing the binding configuration for how your custom processor needs to consume and produce to the relevant channels.</p>
<p>Perhaps even repeat the samples from the recipe on your environment to get an understanding of how it comes together. With that then, you will be able to adapt your custom processor in the same data pipeline to validate it.</p>
"
"63021976","How to configure remote debugger for spring cloud data flow Local server","<p>I am using the spring cloud dataflow server to create a stream. I want to do a remote debugging on the spring cloud dataflow server.
Below is the command which I am trying but not able to connect on 5005 port.
<code>java -jar spring-cloud-dataflow-server-2.5.3.RELEASE.jar -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005</code></p>
","<spring-cloud><spring-cloud-stream><spring-cloud-dataflow>","2020-07-21 19:42:54","172","0","2","63024749","<p>We have a FAQ describing what property to configure to <a href=""https://dataflow.spring.io/docs/2.6.0.SNAPSHOT/resources/faq/#remotedebug"" rel=""nofollow noreferrer"">debug the applications</a> deployed by SCDF.</p>
<p>If you want to debug SCDF itself, you may want to review the <a href=""https://dataflow.spring.io/docs/2.6.0.SNAPSHOT/installation/local/docker-customize/#debug-data-flow-server"" rel=""nofollow noreferrer"">Debug Data Flow Server</a> guide from the Microsite. There's also a specific section to debug the Skipper application, as well.</p>
"
"63021976","How to configure remote debugger for spring cloud data flow Local server","<p>I am using the spring cloud dataflow server to create a stream. I want to do a remote debugging on the spring cloud dataflow server.
Below is the command which I am trying but not able to connect on 5005 port.
<code>java -jar spring-cloud-dataflow-server-2.5.3.RELEASE.jar -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005</code></p>
","<spring-cloud><spring-cloud-stream><spring-cloud-dataflow>","2020-07-21 19:42:54","172","0","2","63026608","<p>Please refer to the guides that Sabby pointed for debugging applications and the SCDF/Skipper server in general. Apart from this, what I notice from your question above is you have the <code>debug</code> parameters as java <code>program arguments</code> which is <strong>incorrect</strong>.</p>
<p>What you actually need is something like this:</p>
<pre><code>java -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005 -jar spring-cloud-dataflow-server-2.5.3.RELEASE.jar 
</code></pre>
<p>The <code>debug</code> parameters need to be passed before <code>-jar</code> argument.</p>
"
"62929519","How can I use Custom Attributes for devices in order to get Version number of Products","<p>My requirement is to get all products from console for Android app along with version name.  If I query <code>deviceExtensive</code> search from <code>Postman</code>, getting information of all <code>Products</code>(apps) in all devices in one go like <code>ProductId</code>, <code>Name</code>, <code>Status</code> but not getting <code>Version</code> of Product.</p>
<p>Same problem with <code>productExtensive</code> search, getting all information of products except version number. I have nearly 20 products.</p>
<p>Is there any <code>service</code> available in <a href=""https://resources.workspaceone.com/view/zv5cgwjrcv972rd6fmml/en"" rel=""nofollow noreferrer"">document</a> where I can get version number of all device available in serve.</p>
<p>OR</p>
<p>Can <a href=""https://docs.vmware.com/en/VMware-Workspace-ONE-UEM/services/WinRugg_Platform/GUID-AWT-CREATECUSTOMATTRIBUTEPAYLOAD.html"" rel=""nofollow noreferrer""><code>Custom attributes</code></a> serve my purpose?</p>
<p>But no idea we can add <code>Custom attributes</code> payload groups level because device level is possible as we thousands of devices.</p>
","<android><flutter><vmware><mdm><airwatch>","2020-07-16 07:21:23","438","2","1","63085539","<p>You only can get <code>version number</code> of applications <code>device</code> level instead of getting server level.</p>
<p>Use <code>loop</code> and pass <code>deviceId</code> in every iteration.</p>
<p>There is no api available in AW rest service document. Below service you can use to do so</p>
<pre><code>https://host/api/mdm/devices/apps?searchby=Serialnumber&amp;id=R10QA7AL&amp;page=1&amp;pagesize=50
https://host/api/mdm/devices/apps?searchby=ImeiNumber&amp;id=YourID&amp;page=1&amp;pagesize=50
https://host/api/mdm/devices/apps?searchby=Udid&amp;id=YourID&amp;page=1&amp;pagesize=50
https://host/api/mdm/devices/apps?searchby=Macaddress&amp;id=YourID&amp;page=1&amp;pagesize=50
</code></pre>
<p><code>Custom attributes</code> can only be used for device level not <code>groups</code> level then again this not going to solve your problem.</p>
"
"62855868","Product database with multiple attributes and not lots of database tables","<p>I have started to create a database for floor-ing products and I am asking question here because I have failed to find a schema design / template that allows the products to have &quot;attributes&quot; such as colour, material, size, price type such as per metre and so on, and my initial design I ended up with database tables for each of the above attributes, which I know can't be the best or even right way to do it.</p>
<p>Is there a way to achieve this without lots of tables?  I suspect it can maybe be done with one or two tables but I am unable to think of the relationships hence my failed search for a design or template.</p>
<p>Barry</p>
","<sql><database><database-design>","2020-07-12 00:14:34","290","0","2","62856020","<p>The simplest method is to keep the attributes in separate columns in the <code>products</code> table.  If a particular product doesn't have an attribute, then the value would be <code>NULL</code>.</p>
<p>Sometimes this approach is not sufficient, often for one of two reasons:</p>
<ul>
<li>There are lots of attributes and they fall into different groupings.</li>
<li>You need to add new attributes on demand.</li>
</ul>
<p>In this case, you basically have two choices:</p>
<ul>
<li>Store the attributes in an entity-attribute-value table (EAV).  This basically has three columns, the product, the attribute name, and the value.  This can be convenient and flexible, but one downside is that the &quot;value&quot; column has only one type in a SQL table.</li>
<li>Store the attributes using a &quot;flexible&quot; storage mechanism, such as JSON or XML.</li>
</ul>
<p>I sometimes use a hybrid approach, where I have a <code>products</code> table with the main attributes.  Then there is a JSON column for more flexible/specific attributes.</p>
"
"62855868","Product database with multiple attributes and not lots of database tables","<p>I have started to create a database for floor-ing products and I am asking question here because I have failed to find a schema design / template that allows the products to have &quot;attributes&quot; such as colour, material, size, price type such as per metre and so on, and my initial design I ended up with database tables for each of the above attributes, which I know can't be the best or even right way to do it.</p>
<p>Is there a way to achieve this without lots of tables?  I suspect it can maybe be done with one or two tables but I am unable to think of the relationships hence my failed search for a design or template.</p>
<p>Barry</p>
","<sql><database><database-design>","2020-07-12 00:14:34","290","0","2","62856044","<p>Since you would have varying numbers of attributes, I would do a table for the product and another for the attributes with a many-to-one relationship [one product has many attributes].</p>
<pre><code>ProductId,
ProductName,
ProductDescription,
etc
</code></pre>
<pre><code>AttributeId,
ProductId,
AttributeName,
AttributeValue,
etc
</code></pre>
"
"62838610","How to improve data synchronization using Unity an multicast UDP socket","<p>I'm teaching myself some simple networking using Unity and Sockets and I'm running into problems synchronizing data between a client and server. I'm aware that there are other options using Unity Networking but, before I move on, I want to understand better how to improve my code using the System libraries.</p>
<p>In this example I'm simply trying to stream my mouse position over a multicast UDP socket. I'm encoding a string into a byte array and sending that array once per frame. I'm aware that sending these values as a string is un-optimal but, unless that is likely the bottleneck, I'm assuming It's ok come back to optimize that later.</p>
<p>Im my setup server is sending values at 60 fps, and the client is reading at the same rate. The problem I'm having is that when the client receives values it typically receives many at once. If I log the values I received with a <code>-----</code> between each frame I typically get output like this:</p>
<pre><code>------
------
------
------
------
------
------
119,396
91,396
45,391
18,379
-8,362
-35,342
-59,314
------
------
------
------
------
------
------
</code></pre>
<p>I would expect unsynchronized update cycles to lead to receiving two values per frame, but I'm not sure what might be accounting for the larger discrepancy.</p>
<p>Here's the Server code:</p>
<pre><code>using UnityEngine;
using System.Net;
using System.Net.Sockets;
using System.Text;

public class Server : MonoBehaviour
{
    Socket _socket;

    void OnEnable ()
    {
        var ip = IPAddress.Parse (&quot;224.5.6.7&quot;);
        var ipEndPoint = new IPEndPoint(ip, 4567);

        _socket = new Socket (AddressFamily.InterNetwork, SocketType.Dgram, ProtocolType.Udp);
        _socket.SetSocketOption (SocketOptionLevel.IP, SocketOptionName.AddMembership, new MulticastOption (ip));
        _socket.SetSocketOption (SocketOptionLevel.IP, SocketOptionName.MulticastTimeToLive, 2);
        _socket.Connect(ipEndPoint);
    }

    void OnDisable ()
    {
        if (_socket != null)
        {
            _socket.Close();
            _socket = null;
        }
    }

    public void Send (string message)
    {
        var byteArray = Encoding.ASCII.GetBytes (message);
        _socket.Send (byteArray);
    }
}
</code></pre>
<p>And the client:</p>
<pre><code>using UnityEngine;
using System.Net;
using System.Net.Sockets;
using System.Text;

public class Client : MonoBehaviour
{
    Socket _socket;
    byte[] _byteBuffer = new byte[16];

    public delegate void MessageRecievedEvent (string message);
    public MessageRecievedEvent messageWasRecieved = delegate {};
    

    void OnEnable ()
    {
        var ipEndPoint = new IPEndPoint(IPAddress.Any, 4567);
        var ip = IPAddress.Parse(&quot;224.5.6.7&quot;);

        _socket = new Socket(AddressFamily.InterNetwork, SocketType.Dgram, ProtocolType.Udp);
        _socket.Bind (ipEndPoint);
        _socket.SetSocketOption (SocketOptionLevel.IP, SocketOptionName.AddMembership, new MulticastOption(ip,IPAddress.Any));
    }

    void Update ()
    {
        while (_socket.Available &gt; 0)
        {
            for (int i = 0; i &lt; _byteBuffer.Length; i++) _byteBuffer[i] = 0;
            _socket.Receive (_byteBuffer);
            messageWasRecieved (Encoding.ASCII.GetString (_byteBuffer));
        }
    }
}
</code></pre>
<p>If anybody could shed light on what I can do to improve synchronization that would be a great help.</p>
","<c#><sockets><unity3d><udp><multicastsocket>","2020-07-10 16:31:37","292","2","1","62840547","<p>Network I/O is subject to a large number of external influences, and TCP/IP as a protocol has few requirements. Certainly none that would provide a guarantee of the behavior you seem to want.</p>
<p>Unfortunately, without a good <a href=""https://stackoverflow.com/help/minimal-reproducible-example"">Minimal, Complete, and Verifiable code example</a>, it's not possible to verify that your server is in fact sending data at the interval you claim. It's entirely possible you have a bug that's causing this behavior.</p>
<p>But if we assume that the code itself is perfect, there are still no guarantees when using UDP that datagrams won't be batched up at some point along the way, such that a large number appear in the network buffer all at once. I would expect this to happen with higher frequency when the datagrams are sent through multiple network nodes (e.g. a switch and especially over the Internet), but it could just as easily happen when the server and client are both on the same computer.</p>
<p>Ironically, one option that <em>might</em> force the datagrams to be spread out more is to pad each datagram with extra bytes. The exact number of bytes required would depend on the exact network route; to do this &quot;perfectly&quot; might require writing some calibration logic that tries different padding amounts until the code sees datagrams arriving at the intervals it expects.</p>
<p>But that would significantly increase the complexity of your network I/O code, and yet still would not <em>guarantee</em> the behavior you'd like. And it has some obvious negative side-effects, including the extra overhead on the network (something people using metered network connections certainly won't appreciate), as well as increasing the likelihood of a UDP datagram being dropped altogether.</p>
<p>It's not clear from your question whether your project actually requires multicast UDP, or if that's just something in your code because that's what some tutorial or other example you're following was using. If multicast is not actually a requirement, another thing you definitely should try is to use direct UDP without multicasting.</p>
<p>FWIW: I would not implement this the way you have. Instead, I would use asynchronous receive operations, so that my client receives datagrams the instant they are available, rather than only checking periodically each frame of rendering. I would also include a sequence number in the datagram, and discard (ignore) any datagrams that arrive out of sequence (i.e. where the sequence number isn't strictly greater than the most recent sequence number already received). This approach will improve (perhaps only slightly) responsiveness, but also will handle situations where the datagrams arrive out of order, or are duplicated (two of the three main delivery issues one will experience with UDP…the third being, of course, failure of delivery).</p>
"
"62834846","SwiftUI data flow and UI update for foreign model object with changing values but same id","<p>I have an array of objects (&quot;things&quot;) of some third party library that I want to display in a SwiftUI View.
These &quot;Thing&quot; objects are Identifiable and Hashable by an id, but when reloading a new set of Things, their content may have changed (let's say a &quot;status&quot; or a &quot;text&quot; of that Thing, although it is the same Thing again). So the id keeps the same, but the content of a Thing can change.
Problem is, that SwiftUI doesn't update the UI when I get a new array of Things. I assume this is because the Things are &quot;identified&quot; as the same Things again by their id.
I can not change Thing, because it is from a third party library.</p>
<p>Now I simply wrapped Thing into another class and suddenly it works! But I want to understand <em>why</em> this works, and if it is defined behaviour and not just coincidence or &quot;luck&quot;.</p>
<p>Can anybody explain what happens here behind the scene? Especially what is the main difference between DirectThingView and WrappedThingView that causes SwiftUI to update the UI for the latter and not for the former?</p>
<p>Or are the any suggestions how to solve this problem in a better way?</p>
<p>Here is sample code that shows everything:
It displays the Things in two columns; first column uses the DirectThingView and second column uses the WrappedThingView. If you hit the &quot;Reload&quot; button, the things array gets filled with changed Things, but only the UI of the right column correctly updates the values; the left column always stays at its initial state.</p>
<pre><code>//
//  TestView.swift
//
//  Created by Manfred Schwind on 10.07.20.
//  Copyright © 2020 mani.de. All rights reserved.
//

import SwiftUI

// The main model contains an array of &quot;Things&quot;,
// every Thing has an id and contains a text.
// For testing purposes, every other time a Thing gets instantiated, its text contains either &quot;A&quot; or &quot;B&quot;.
// Problem here: the &quot;current&quot; text of a Thing with the same id can change, when Things are reloaded.
class TestViewModel: ObservableObject {
    @Published var things = [Thing(id: 1), Thing(id: 2), Thing(id: 3)]
}

struct TestView: View {
    @ObservedObject var viewModel = TestViewModel()
    var body: some View {
        VStack (spacing: 30) {
            HStack (spacing: 40) {
                // We try to display the current Thing array in the UI

                // The views in the first column directly store the Thing:
                // Problem here: the UI does not update for changed Things ...
                VStack {
                    Text(&quot;Direct&quot;)
                    ForEach(self.viewModel.things, id: \.self) { thing in
                        DirectThingView(viewModel: thing)
                    }
                }
                // The views in the second column store the Thin wrapped into another class:
                // In this case, the problem magically went away!
                VStack {
                    Text(&quot;Wrapped&quot;)
                    ForEach(self.viewModel.things, id: \.self) { thing in
                        WrappedThingView(viewModel: thing)
                    }
                }
            }
            Button(action: {
                // change the Thing array in the TestViewModel, this causes the UI to update:
                self.viewModel.things = [Thing(id: 1), Thing(id: 2), Thing(id: 3)]
            }) {
                Text(&quot;Reload&quot;)
            }
        }
    }
}

struct DirectThingView: View {
    // first approach just stores the passed Thing directly internally:
    private let viewModel: Thing

    init(viewModel: Thing) {
        self.viewModel = viewModel
    }

    var body: some View {
        Text(self.viewModel.text)
    }
}

struct WrappedThingView: View {
    // second approach stores the passed Thing wrapped into another Object internally:
    private let viewModel: WrappedThing

    init(viewModel: Thing) {
        // take the Thing like in the first approach, but internally store it wrapped:
        self.viewModel = WrappedThing(childModel: viewModel)
    }

    var body: some View {
        Text(self.viewModel.childModel.text)
    }

    // If type of WrappedThing is changed from class to struct, then the problem returns!
    private class WrappedThing {
        let childModel: Thing
        init(childModel: Thing) {
            self.childModel = childModel
        }
    }

}

// Thing has do be Identifiable and Hashable for ForEach to work properly:
class Thing: Identifiable, Hashable {

    // Identifiable:
    let id: Int

    // The text contains either &quot;A&quot; or &quot;B&quot;, in alternating order on every new Thing instantiation
    var text: String

    init(id: Int) {
        self.id = id
        struct Holder {
            static var flip: Bool = false
        }
        self.text = Holder.flip ? &quot;B&quot; : &quot;A&quot;
        Holder.flip = !Holder.flip
    }

    // Hashable:
    public func hash(into hasher: inout Hasher) {
        hasher.combine(self.id)
    }

    // Equatable (part of Hashable):
    public static func == (lhs: Thing, rhs: Thing) -&gt; Bool {
        return lhs.id == rhs.id
    }
}

#if DEBUG
struct TestView_Previews: PreviewProvider {
    static var previews: some View {
        TestView()
    }
}
#endif
</code></pre>
<p><a href=""https://i.stack.imgur.com/PWBBw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PWBBw.png"" alt=""enter image description here"" /></a></p>
<p>Thank you very much in advance!</p>
","<swift><xcode><user-interface><swiftui>","2020-07-10 12:59:22","207","1","1","62840175","<p>I found the answer for myself. The problem is the implementation of Equatable of Thing here. As implemented above, an old and a new version of the same Thing (but with different content) are considered equal. But SwiftUI distincts between &quot;identity&quot; and &quot;equality&quot; and this has to be properly implemented.</p>
<p>In the code above, Identifiable and Hashable are OK, but Equatable has to be changed to be more precise. So e.g. this fixes the problem:</p>
<pre><code>// Thing has do be Identifiable and Hashable for ForEach to work properly:
class Thing: Identifiable, Hashable {

    // Identifiable:
    let id: Int

    // The text contains either &quot;A&quot; or &quot;B&quot;, in alternating order on every new Thing instantiation
    var text: String

    init(id: Int) {
        self.id = id
        struct Holder {
            static var flip: Bool = false
        }
        self.text = Holder.flip ? &quot;B&quot; : &quot;A&quot;
        Holder.flip = !Holder.flip
    }

    // Hashable:
    public func hash(into hasher: inout Hasher) {
        // we are lazy and just use the id here, &quot;collisions&quot; are then separated by func ==
        hasher.combine(self.id)
    }

    // Equatable (part of Hashable):
    public static func == (lhs: Thing, rhs: Thing) -&gt; Bool {
        // We are lazy again (in reality Thing has many properties) and we consider
        // two Things to be the equal ONLY when they point to the same address.
        // So we get the &quot;same but different&quot; semantic that we want, when we are
        // getting a new version of the Thing.
        // (Same in the sense of identity, different in the sense of equality)
        return lhs === rhs
    }
}
</code></pre>
"
"62787991","Confusion about data flow on HTTP requests - what part is HTTP and what is JSON?","<p>I'm going to use the following code as an example to frame my question. It's basically just the code required to pull a list of to dos from an SQLite3 database:</p>
<p>So, there's an axios request in the front end:</p>
<pre><code>useEffect(() =&gt; {
    axios.get('http://localhost:3001/todo', {})
    .then(res =&gt; {
        setTodoList(res.data)
    })
    .catch(err =&gt; {
        console.log(err)
    })
}, [])
</code></pre>
<p>...which links to the following function in the back end:</p>
<pre><code>server.get('/todo', (req,res) =&gt; {
    // res.json(testData)
    const todos = db('todos') //this is shorthand for 'return everything from the table 'todos''
    .then(todos =&gt; {
        return res.json(todos)
    })
})
</code></pre>
<p>..the data from this GET request is then rendered within a react component, as a list of text.</p>
<p>I'm just confused about the flow of data - when is it HTTP, when is it JSON, what form does the data come out of the database as, and how is it that these different protocol/languages can talk to each other?</p>
<p>I get the overall principle of a GET request and async functions, I just don't get what's going on under the hood. Thanks!</p>
","<node.js><json><express><http>","2020-07-08 05:16:00","80","0","1","62788194","<p>That's a lot of questions about basic issues. But here are some answers. Firstly, you can simplify the server function as:</p>
<pre><code>server.get('/todo', (req, res) =&gt; {
  db('todos').then(todos =&gt; res.json(todos));
}); 
</code></pre>
<p>The data from the db is a Javascript array by the time you are dealing with it in Express. <code>res.json</code> converts it into JSON, which is of course, just a string.</p>
<p>Express creates an HTTP response, which consists of some headers (key value pairs such as Content-Length:  and so on) followed by a body, which in your case is just a JSON blob, a string. That response object is sent over the network via HTTP.</p>
<p>The browser receives the response and axios is kind enough to handle the grunt work of reading the headers and turning your JSON back into a Javascript array/object which can then be handled inside React.</p>
<p>The part I can't answer is &quot;how is it that these different protocol/languages can talk to each other&quot;, because that is very complex and the question is not well defined. There are many network layers involved.</p>
"
"62693698","UDP: How make the data flow rate smooth for UDP packets over a network connection","<p><strong>Scenario:</strong><br />
Using <a href=""https://www.boost.org/doc/libs/1_69_0/doc/html/boost_asio.html"" rel=""nofollow noreferrer"">Boost Asio 1.69</a> I have a C++ server running on Linux Fedora 4.9 and a client running on an Android 10 phone using a peer to peer connection. There is continuous sending of data from server to client and occasionally a packet from client to server.</p>
<p><strong>Problem:</strong><br />
Things work well but due to my Server sending many packets at a high rate, my client is unable to catch up. This causes packets to get dropped. Digging deep and reading about the problem, I learnt that there is something called <a href=""https://blog.cloudflare.com/accelerating-udp-packet-transmission-for-quic/"" rel=""nofollow noreferrer"">UDP packets pacing</a>. Another link <a href=""https://fasterdata.es.net/network-tuning/packet-pacing/"" rel=""nofollow noreferrer"">here</a>. This seems to me as a potential solution to my problem. At least something I want to try to avoid the burst of flow of UDP packets and rather try smoothen the flow.</p>
<p>So I tried the following firstly:</p>
<pre><code>uint32_t bytes_per_second = 1000000;
if(setsockopt(udp_socket, SOL_SOCKET, SO_MAX_PACING_RATE, &amp;bytes_per_second, sizeof(bytes_per_second)) &lt; 0) {
    std::cout &lt;&lt; &quot;Unable to set socket max pacing rate&quot; &lt;&lt; std::endl;
}
</code></pre>
<p>But above does not seem to have any affect. I different numbers set for <code>bytes_per_second</code> with no helpful effect and the problem stayed the same.</p>
<p><strong>Question:</strong><br />
How can I effectively exercise UDP packets pacing? Or how can I ensure a slight gap between the packets I am sending from my Linux server side?</p>
<p>Is it a linux configuration I could do or is it something I could do by calling <a href=""https://linux.die.net/man/3/setsockopt"" rel=""nofollow noreferrer"">setsockopt</a> on the udp socket? Any suggestions towards potential investigations are also welcome!</p>
","<network-programming><udp><asio>","2020-07-02 09:57:18","1147","1","2","62694305","<p>What you're describing is a <a href=""https://en.wikipedia.org/wiki/Flow_control_(data)"" rel=""nofollow noreferrer"">Flow Control</a> problem.
The problem in flow control is not how to pace the rate of packets, but how to determine the right rate in the first place. You've set <code>bytes_per_second = 1000000;</code>, but why? Why not half that rate? Why not one tenth? Perhaps you're not seeing any improvement because the rate is still too high.</p>
<p>Your protocol needs some way of finding an appropriate rate for a specific client. This can only be done using information from the client. For instance you could add sequence numbers to the packets and have the client report its last processed sequence number. If that number is too old then you need to slow down.</p>
<p>TCP has a <a href=""https://en.wikipedia.org/wiki/Transmission_Control_Protocol#Flow_control"" rel=""nofollow noreferrer"">built-in flow control mechanism</a> in which the client reports the amount of free space it has in the receive buffer. This information is reported in every TCP segment, so the sender always knows how much additional data it can send. Perhaps it's worth considering switching to TCP for your application?</p>
"
"62693698","UDP: How make the data flow rate smooth for UDP packets over a network connection","<p><strong>Scenario:</strong><br />
Using <a href=""https://www.boost.org/doc/libs/1_69_0/doc/html/boost_asio.html"" rel=""nofollow noreferrer"">Boost Asio 1.69</a> I have a C++ server running on Linux Fedora 4.9 and a client running on an Android 10 phone using a peer to peer connection. There is continuous sending of data from server to client and occasionally a packet from client to server.</p>
<p><strong>Problem:</strong><br />
Things work well but due to my Server sending many packets at a high rate, my client is unable to catch up. This causes packets to get dropped. Digging deep and reading about the problem, I learnt that there is something called <a href=""https://blog.cloudflare.com/accelerating-udp-packet-transmission-for-quic/"" rel=""nofollow noreferrer"">UDP packets pacing</a>. Another link <a href=""https://fasterdata.es.net/network-tuning/packet-pacing/"" rel=""nofollow noreferrer"">here</a>. This seems to me as a potential solution to my problem. At least something I want to try to avoid the burst of flow of UDP packets and rather try smoothen the flow.</p>
<p>So I tried the following firstly:</p>
<pre><code>uint32_t bytes_per_second = 1000000;
if(setsockopt(udp_socket, SOL_SOCKET, SO_MAX_PACING_RATE, &amp;bytes_per_second, sizeof(bytes_per_second)) &lt; 0) {
    std::cout &lt;&lt; &quot;Unable to set socket max pacing rate&quot; &lt;&lt; std::endl;
}
</code></pre>
<p>But above does not seem to have any affect. I different numbers set for <code>bytes_per_second</code> with no helpful effect and the problem stayed the same.</p>
<p><strong>Question:</strong><br />
How can I effectively exercise UDP packets pacing? Or how can I ensure a slight gap between the packets I am sending from my Linux server side?</p>
<p>Is it a linux configuration I could do or is it something I could do by calling <a href=""https://linux.die.net/man/3/setsockopt"" rel=""nofollow noreferrer"">setsockopt</a> on the udp socket? Any suggestions towards potential investigations are also welcome!</p>
","<network-programming><udp><asio>","2020-07-02 09:57:18","1147","1","2","64940736","<p>You need to add fq discipline to the interface first</p>
<blockquote>
<p>tc qdisc add dev eth0 root fq <a href=""https://github.com/esnet/iperf/issues/325#issuecomment-210643866"" rel=""nofollow noreferrer"">(1)</a></p>
</blockquote>
"
"62668173","Get result data from pentaho data integration metadata injection","<p>In pentaho data integration I am using metadata injection within a stream of a transformation. How can I get the result of the metadata injection back to my stream in order to continue transforming the data outside of the metadata injection. Copy rows to result does not seem to be working here like it does with a transformation within a transformation.</p>
","<pentaho><pentaho-spoon><pentaho-data-integration><pdi>","2020-07-01 01:04:07","348","0","1","62720555","<p>Found it myself. In the Options tab you can select the step within the template to read the data from and below you can set the fields.</p>
<p><a href=""https://i.stack.imgur.com/eYTE0.png"" rel=""nofollow noreferrer"">metadata injection options</a></p>
"
"62662241","Can I split a column text as array using data factory data flow?","<p>Inside my data flow pipeline I would like to add a derived column and its datatype is array. I would like to split the existing column with 1000 characters without breaking words. I think we can use regexSplit,</p>
<pre><code>regexSplit(&lt;string to split&gt; : string, &lt;regex expression&gt; : string) =&gt; array
</code></pre>
<p>But I do not know which regular expression I can use for split the existing column without breaking words.
Please help me to figure it out.</p>
","<regex><azure><azure-data-factory>","2020-06-30 16:42:33","3518","3","2","62748465","<p>I wouldn't use a regex for this, but a truncating function like this one, <a href=""https://stackoverflow.com/a/1614090/119129"">courtesy of TimS</a>:</p>
<pre class=""lang-cs prettyprint-override""><code>public static string TruncateAtWord(this string input, int length)
{
    if (input == null || input.Length &lt; length)
        return input;
    int iNextSpace = input.LastIndexOf(&quot; &quot;, length, StringComparison.Ordinal);
    return string.Format(&quot;{0}…&quot;, input.Substring(0, (iNextSpace &gt; 0) ? iNextSpace : length).Trim());
}
</code></pre>
<p>Translated into expression functions it would look something* like this.</p>
<pre><code>substring(Input, 1, iif(locate(Input, ' ', 1000) &gt; 0, locate(Input, ' ', 1000) , length(Input)) )
</code></pre>
<p>Since you don't have a <code>lastIndexOf</code> available as an expression function, you would have to default to <code>locate</code>, which means that this expression truncates the string at the first space <strong>after</strong> the 1000th character.</p>
<p>*I don't have an environment where I can test this.</p>
"
"62662241","Can I split a column text as array using data factory data flow?","<p>Inside my data flow pipeline I would like to add a derived column and its datatype is array. I would like to split the existing column with 1000 characters without breaking words. I think we can use regexSplit,</p>
<pre><code>regexSplit(&lt;string to split&gt; : string, &lt;regex expression&gt; : string) =&gt; array
</code></pre>
<p>But I do not know which regular expression I can use for split the existing column without breaking words.
Please help me to figure it out.</p>
","<regex><azure><azure-data-factory>","2020-06-30 16:42:33","3518","3","2","62780464","<p>I created a workaround for this and it works fine for me.</p>
<pre><code>filter(split(regexReplace(regexReplace(text, `[\t\n\r]`, ``), `(.{1,1000})(?:\s|$)`, `$1~~`), '~~'), #item !=&quot;&quot;)
</code></pre>
<p>I think, we have a better solution than this.</p>
"
"62571428","pentaho data integration excel file outputs","<p>using the Pentaho ETL tool I am trying to get an SQL query result dumped into an excel file output. I have created an excel file and saved it but the transformation executes but does not update the excel file. I am trying to insert select * from table A into the excel file.</p>
<p>The connection to the database is fine as I have other transformations.
The two steps are execute SQL script which I have tested in isolation and works, the second is the Microsoft excel output which is not working.</p>
<p>Any suggestions?</p>
","<pentaho><pentaho-data-integration>","2020-06-25 08:48:51","802","0","1","62587069","<p>Use Table input instead of Execute SQL script. Execute SQL script doesn't give output to next steps</p>
"
"62527486","A better way to map data in multiple datasets, with multiple data mapping rules","<p>I have three datasets (<code>final_NN</code>, <code>ppt_code</code>, <code>herd_id</code>), and I wish to add a new column called <code>MapValue</code> in <code>final_NN</code> dataframe, and the value to be added can be retrieved from the other two dataframes, the rule is in the bottom after codes.</p>
<pre><code>import pandas as pd

final_NN = pd.DataFrame({
    &quot;number&quot;: [123, 456, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;],
    &quot;ID&quot;: [&quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, 799, 813],
    &quot;code&quot;: [&quot;&quot;, &quot;&quot;, &quot;AA&quot;, &quot;AA&quot;, &quot;BB&quot;, &quot;BB&quot;, &quot;BB&quot;, &quot;CC&quot;, &quot;&quot;, &quot;&quot;]
})

ppt_code = pd.DataFrame({
    &quot;code&quot;: [&quot;AA&quot;, &quot;AA&quot;, &quot;BB&quot;, &quot;BB&quot;, &quot;CC&quot;],
    &quot;number&quot;: [11, 11, 22, 22, 33]
})

herd_id = pd.DataFrame({
    &quot;ID&quot;: [799, 813],
    &quot;number&quot;: [678, 789]
})

new_column = pd.Series([])
for i in range(len(final_NN)):
    if final_NN[&quot;number&quot;][i] != &quot;&quot; and final_NN[&quot;number&quot;][i] != &quot;Unknown&quot;:
        new_column[i] = final_NN['number'][i]

    elif final_NN[&quot;code&quot;][i] != &quot;&quot;:
        for p in range(len(ppt_code)):
            if ppt_code[&quot;code&quot;][p] == final_NN[&quot;code&quot;][i]:
                new_column[i] = ppt_code[&quot;number&quot;][p]

    elif final_NN[&quot;ID&quot;][i] != &quot;&quot;:
        for h in range(len(herd_id)):
            if herd_id[&quot;ID&quot;][h] == final_NN[&quot;ID&quot;][i]:
                new_column[i] = herd_id[&quot;number&quot;][h]

    else:
        new_column[i] = &quot;&quot;

final_NN.insert(3, &quot;MapValue&quot;, new_column)
print(final_NN)
</code></pre>
<p>final_NN:</p>
<pre><code>    number   ID code
0      123          
1      456          
2  Unknown        AA
3  Unknown        AA
4  Unknown        BB
5  Unknown        BB
6  Unknown        BB
7  Unknown        CC
8  Unknown  799     
9  Unknown  813 
</code></pre>
<p>ppt_code:</p>
<pre><code>  code  number
0   AA      11
1   AA      11
2   BB      22
3   BB      22
4   CC      33
</code></pre>
<p>herd_id:</p>
<pre><code>    ID  number
0  799     678
1  813     789
</code></pre>
<p>Expected output:</p>
<pre><code>    number   ID code   MapValue
0      123                  123
1      456                  456
2  Unknown        AA         11
3  Unknown        AA         11
4  Unknown        BB         22
5  Unknown        BB         22
6  Unknown        BB         22
7  Unknown        CC         33
8  Unknown  799             678
9  Unknown  813             789
</code></pre>
<p>The rules is:</p>
<ol>
<li>if <code>number</code> in final_NN is not <code>Unknown</code>, <code>MapValue</code> = <code>number</code> in <code>final_NN</code>;</li>
<li>if <code>number</code> in final_NN is <code>Unknown</code> but <code>code</code> in <code>final_NN</code> is not Null, then search the ppt_code dataframe, and use the <code>code</code> and its corresponding &quot;number&quot; to map and fill in the &quot;MapValue&quot; in <code>final_NN</code>;</li>
<li>if both <code>number</code> and <code>code</code> in <code>final_NN</code> are <code>Unknown</code> and null respectively, but <code>ID</code> in <code>final_NN</code> is not Null, then search <code>herd_id</code> dataframe, and use the <code>ID</code> and its corresponding <code>number</code> to fill in the <code>MapValue</code> in the first dataframe. I applied a loop through the dataframe which is a slow way to achieve this, as above. But I understand there could be a faster way to do this. Just wondering would anyone help me to have a fast and easier way to achieve the same result?</li>
</ol>
","<python><pandas><dataframe><data-analysis><data-mapping>","2020-06-23 04:43:38","852","5","2","62527850","<p>First create a mapping series from the <code>ppt_code</code> and <code>herd_id</code> dataframes, then use <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.replace.html"" rel=""nofollow noreferrer""><code>Series.replace</code></a> to create a new column <code>MapNumber</code> by replacing the <code>Unknown</code> values in <code>number</code> column with <code>np.NaN</code>, then use two consecutive <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.fillna.html"" rel=""nofollow noreferrer""><code>Series.fillna</code></a> along with <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>Series.map</code></a> to fill the missing values in <code>MapNumber</code> column according to the rules:</p>
<pre><code>ppt_map = ppt_code.drop_duplicates(subset=['code']).set_index('code')['number']
hrd_map = herd_id.drop_duplicates(subset=['ID']).set_index('ID')['number']

final_NN['MapNumber'] = final_NN['number'].replace({'Unknown': np.nan})
final_NN['MapNumber'] = (
    final_NN['MapNumber']
    .fillna(final_NN['code'].map(ppt_map))
    .fillna(final_NN['ID'].map(hrd_map))
)
</code></pre>
<p>Result:</p>
<pre><code># print(final_NN)

    number   ID code  MapNumber
0      123                123.0
1      456                456.0
2  Unknown        AA       11.0
3  Unknown        AA       11.0
4  Unknown        BB       22.0
5  Unknown        BB       22.0
6  Unknown        BB       22.0
7  Unknown        CC       33.0
8  Unknown  799           678.0
9  Unknown  813           789.0
</code></pre>
"
"62527486","A better way to map data in multiple datasets, with multiple data mapping rules","<p>I have three datasets (<code>final_NN</code>, <code>ppt_code</code>, <code>herd_id</code>), and I wish to add a new column called <code>MapValue</code> in <code>final_NN</code> dataframe, and the value to be added can be retrieved from the other two dataframes, the rule is in the bottom after codes.</p>
<pre><code>import pandas as pd

final_NN = pd.DataFrame({
    &quot;number&quot;: [123, 456, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;, &quot;Unknown&quot;],
    &quot;ID&quot;: [&quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, 799, 813],
    &quot;code&quot;: [&quot;&quot;, &quot;&quot;, &quot;AA&quot;, &quot;AA&quot;, &quot;BB&quot;, &quot;BB&quot;, &quot;BB&quot;, &quot;CC&quot;, &quot;&quot;, &quot;&quot;]
})

ppt_code = pd.DataFrame({
    &quot;code&quot;: [&quot;AA&quot;, &quot;AA&quot;, &quot;BB&quot;, &quot;BB&quot;, &quot;CC&quot;],
    &quot;number&quot;: [11, 11, 22, 22, 33]
})

herd_id = pd.DataFrame({
    &quot;ID&quot;: [799, 813],
    &quot;number&quot;: [678, 789]
})

new_column = pd.Series([])
for i in range(len(final_NN)):
    if final_NN[&quot;number&quot;][i] != &quot;&quot; and final_NN[&quot;number&quot;][i] != &quot;Unknown&quot;:
        new_column[i] = final_NN['number'][i]

    elif final_NN[&quot;code&quot;][i] != &quot;&quot;:
        for p in range(len(ppt_code)):
            if ppt_code[&quot;code&quot;][p] == final_NN[&quot;code&quot;][i]:
                new_column[i] = ppt_code[&quot;number&quot;][p]

    elif final_NN[&quot;ID&quot;][i] != &quot;&quot;:
        for h in range(len(herd_id)):
            if herd_id[&quot;ID&quot;][h] == final_NN[&quot;ID&quot;][i]:
                new_column[i] = herd_id[&quot;number&quot;][h]

    else:
        new_column[i] = &quot;&quot;

final_NN.insert(3, &quot;MapValue&quot;, new_column)
print(final_NN)
</code></pre>
<p>final_NN:</p>
<pre><code>    number   ID code
0      123          
1      456          
2  Unknown        AA
3  Unknown        AA
4  Unknown        BB
5  Unknown        BB
6  Unknown        BB
7  Unknown        CC
8  Unknown  799     
9  Unknown  813 
</code></pre>
<p>ppt_code:</p>
<pre><code>  code  number
0   AA      11
1   AA      11
2   BB      22
3   BB      22
4   CC      33
</code></pre>
<p>herd_id:</p>
<pre><code>    ID  number
0  799     678
1  813     789
</code></pre>
<p>Expected output:</p>
<pre><code>    number   ID code   MapValue
0      123                  123
1      456                  456
2  Unknown        AA         11
3  Unknown        AA         11
4  Unknown        BB         22
5  Unknown        BB         22
6  Unknown        BB         22
7  Unknown        CC         33
8  Unknown  799             678
9  Unknown  813             789
</code></pre>
<p>The rules is:</p>
<ol>
<li>if <code>number</code> in final_NN is not <code>Unknown</code>, <code>MapValue</code> = <code>number</code> in <code>final_NN</code>;</li>
<li>if <code>number</code> in final_NN is <code>Unknown</code> but <code>code</code> in <code>final_NN</code> is not Null, then search the ppt_code dataframe, and use the <code>code</code> and its corresponding &quot;number&quot; to map and fill in the &quot;MapValue&quot; in <code>final_NN</code>;</li>
<li>if both <code>number</code> and <code>code</code> in <code>final_NN</code> are <code>Unknown</code> and null respectively, but <code>ID</code> in <code>final_NN</code> is not Null, then search <code>herd_id</code> dataframe, and use the <code>ID</code> and its corresponding <code>number</code> to fill in the <code>MapValue</code> in the first dataframe. I applied a loop through the dataframe which is a slow way to achieve this, as above. But I understand there could be a faster way to do this. Just wondering would anyone help me to have a fast and easier way to achieve the same result?</li>
</ol>
","<python><pandas><dataframe><data-analysis><data-mapping>","2020-06-23 04:43:38","852","5","2","62528110","<p>We simply combined the three data frames.</p>
<ol>
<li>The original DF removes the 'Unknown' row.</li>
<li>'ppt_code' changes the column names.</li>
<li>'pandas.concat()' joins them together.</li>
</ol>
<pre><code>final_NN['number'].replace('Unknown', np.NaN, inplace=True)
final_NN.dropna(inplace=True, how='any')
ppt_code.rename(columns={'code':'ID'}, inplace=True)
new_df = pd.concat([final_NN, ppt_code, herd_id], axis=0, ignore_index=True)

new_df
    number  ID  code
0   123.0       
1   456.0       
2   11.0    AA  NaN
3   11.0    AA  NaN
4   22.0    BB  NaN
5   22.0    BB  NaN
6   33.0    CC  NaN
7   678.0   799 NaN
8   789.0   813 NaN
</code></pre>
"
"62514821","Azure Wrangling data flow doesn't appearing","<p>I want to use the <code>Wrangling data flow</code> in <code>Azure Data Factory v2</code>, but this data flow doesn't appearing for me.</p>
<p>I followed this tutorial <a href=""https://learn.microsoft.com/en-us/azure/data-factory/wrangling-data-flow-tutorial"" rel=""nofollow noreferrer"">Prepare data with wrangling data flow</a></p>
<p>We have this image to create the wrangler:
<a href=""https://i.stack.imgur.com/DbEOg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DbEOg.png"" alt=""enter image description here"" /></a></p>
<p>But, in my subscription these options doesn't appearing for me.
<a href=""https://i.stack.imgur.com/Dl6Cq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dl6Cq.png"" alt=""enter image description here"" /></a></p>
<p>I searched in many websites, tutorials and I didn't find anything about this.</p>
","<azure><azure-data-factory>","2020-06-22 12:48:08","93","0","2","62570901","<p>If you have already create the Wrangling data flow, I think you can select it.</p>
<p>In your screenshot, it seems you dont select the 'Use existing data flow':</p>
<p><a href=""https://i.stack.imgur.com/D1J78.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D1J78.png"" alt=""enter image description here"" /></a></p>
"
"62514821","Azure Wrangling data flow doesn't appearing","<p>I want to use the <code>Wrangling data flow</code> in <code>Azure Data Factory v2</code>, but this data flow doesn't appearing for me.</p>
<p>I followed this tutorial <a href=""https://learn.microsoft.com/en-us/azure/data-factory/wrangling-data-flow-tutorial"" rel=""nofollow noreferrer"">Prepare data with wrangling data flow</a></p>
<p>We have this image to create the wrangler:
<a href=""https://i.stack.imgur.com/DbEOg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DbEOg.png"" alt=""enter image description here"" /></a></p>
<p>But, in my subscription these options doesn't appearing for me.
<a href=""https://i.stack.imgur.com/Dl6Cq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Dl6Cq.png"" alt=""enter image description here"" /></a></p>
<p>I searched in many websites, tutorials and I didn't find anything about this.</p>
","<azure><azure-data-factory>","2020-06-22 12:48:08","93","0","2","70255945","<p>The Azure data wrangling in data flow is actually moved below as &quot;<strong>Power Query</strong>&quot;.</p>
<p>for more details watch the official docs video:
<a href=""https://learn.microsoft.com/en-us/azure/data-factory/wrangling-overview#use-cases"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/data-factory/wrangling-overview#use-cases</a></p>
"
"62499434","How to change the data in a column in SAS Data Integration?","<p>I have an existing ETL solution built-in SAS Data Integration, where one of the columns in initially set to have all null values. I want to populate that column with actual data. The original column in that table was set to receive numeric values with specific format and in format. After I am changing the code (that is the easy part), I notice that the column doesn't accept character values (I did not get an error, I just noticed the column still having all NULL values).
Can anyone help ?</p>
","<sas><data-integration><sas-dis>","2020-06-21 13:47:25","632","2","1","62507862","<p>So you have a table that is defined in Data Integration studio <em>(1)</em> and created by running the job <em>(2)</em> a long time ago with a numeric column. Let us call that table <code>THE_TABLE</code> that field <code>the_field</code> and the job, <code>The_Job</code>, that loads data into <code>THE_TABLE</code></p>
<p>You must be aware of the fundamental difference</p>
<ol>
<li>defining a <code>THE_TABLE</code> in DI studio, which creates a description of the table in meta data</li>
<li>creating <code>THE_TABLE</code> by running <code>The_Job</code>, which creates a file in a folder with data</li>
</ol>
<p>If <code>The_Job</code> really creates <code>THE_TABLE</code> from scratch each time (which is typical for ETL jobs), it is sufficient to do change <code>THE_TABLE</code> and <code>The_Job</code> in DI studio. Your edits will only change the meta data, but the next time you run <code>The_job</code>, <code>THE_TABLE</code> wil be created with the the right structure.</p>
<p>However, if <code>The_Job</code> updates <code>THE_TABLE</code> or appends to it, your edits will not change the structure of <code>THE_TABLE</code> and your job will not be fit for the structure of the file <code>THE_TABLE</code> like it still exists in the folder, so you must convert <code>THE_TABLE</code> before running <code>The_Job</code>.</p>
<p>This can be done with a simple program like</p>
<pre><code>data THE_TABLE;
   set THE_TABLE (drop=the_field);            /* forget about the numeric field */
   attrib the_field length=$200 format=$200.; /* and create the character field */
run;
</code></pre>
<p>The correct attrib statement might well be in the code generated for <code>The_Job</code> somewhere.</p>
<p>Mind that in a typical setup with a development, test and production environment, you will need that program once in each environment.</p>
"
"62426021","CouchDB data synchronization","<p>I'm developing a TODO app for myself. It has to be offline first so data synchronization became a problem soon. As to my searches, CouchDB does data sync (replication) very well but I'm having some confusion here.<br>
First of all, I'm using Flutter and Dart programming language and there isn't any couch client on Dart. For javascript, there is <a href=""https://pouchdb.com/"" rel=""nofollow noreferrer"">PouchDB</a> which apparently makes data sync with remote database automatic (<strong>Am I right?</strong>) as far as I know I should be good to go without any third party library since CouchDB comes with native HTTP API, I can just store user data as a JSON file and just sync it with CouchDB server as a document (<strong>Am I right?</strong>)<br>
Another <a href=""https://stackoverflow.com/questions/46351432/per-user-db-pouchdb-couchdb-shared-data-doable"">confusion</a> <a href=""https://www.joshmorony.com/part-2-creating-a-multiple-user-app-with-ionic-2-pouchdb-couchdb/"" rel=""nofollow noreferrer"">for</a> <a href=""https://docs.couchdb.org/en/stable/config/couch-peruser.html"" rel=""nofollow noreferrer"">me</a> is that should user data be stored as a document or should I create 1 database per user(In other words does couch syncs database or document?) It also raises authorizations. A user which has access to a database has access to all documents. I want to restrict each user to its documents without placing another API between couch and end-user application.</p>
","<synchronization><couchdb><replication><pouchdb>","2020-06-17 09:41:16","2380","3","1","62456887","<p>A primary benefit of pouchdb is having local databases that have APIs like couchdb but are present even while offline, etc. So if you were using pouchdb and want to work offline you would almost certainly want per user dbs on couchdb and to do <strong>pouchdb &lt;-> couchdb</strong> syncing, etc. But you can't use pouchdb without JS and depending on it in a webview, etc, would largely undermine the benefit of compiling to native code instead of a webview based app.</p>

<p>Using HTTP APIs directly and storing a local document, it is probably simpler to start with each user storing a few simple things as additional JSON on their own user document in _users. You can always add per-user dbs later. The _users database is a little bit special in that it gives each user read access to their own document without having to give them access to other user docs and relies on a validation design document to protect important fields while otherwise letting them edit their data.</p>

<p>In such a scenerio, the client keeps a copy of the users own document locally that it needs to sync back to the server when it has networking. There are of course conflict possibilities for a user with multiple devices, and they can be resolved either by the client itself comparing local and server versions or with the help of update design documents on the server.</p>

<p>Starting from each user having their private data in their own _users doc, one can still build dbs for groups of users where all users in the group share read access to all documents but can only update ones they created. A per user db is really only a special case of such a database, where the group has been reduced to only one member.</p>
"
"62385711","Block file upload from managed devices Microsoft 365","<p>I have implemented MFA and registered personal devices to access organization data and applications. Now users can access organization apps on personal devices, i want to restrict document uploads from the managed devices.
Users should not be able to upload files from personal devices to application such Onedrive/ SharePoint etc.</p>
","<azure-active-directory><office365><mdm><multi-factor-authentication><microsoft365>","2020-06-15 09:55:31","877","0","1","62392590","<p>here are your options: <a href=""https://learn.microsoft.com/en-us/sharepoint/control-access-from-unmanaged-devices"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/sharepoint/control-access-from-unmanaged-devices</a>
at the bottom under advanced, there is option to limit editing from browser, this may be sufficient to what you are seeking. </p>

<p>According to here. personal devices as long as they are not marked compliant, 
then they are considered unmanaged. 
<a href=""https://learn.microsoft.com/en-us/azure/active-directory/conditional-access/require-managed-devices#managed-devices"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/active-directory/conditional-access/require-managed-devices#managed-devices</a></p>
"
"62371346","Pub/Sub - Push to API when STATE is ACTIVE on Enrollment","<p>I have implemented to <strong>PUSH</strong> a <strong>PUB/SUB</strong> notification to my <strong>POST API</strong> with the device details on <code>ENROLLMENT</code>. But notification comes in while the device <code>STATE</code> is still <code>PROVISIONING</code></p>

<p>Is there a way to get the notification only when the <code>STATE</code> is <code>ACTIVE</code> ? This would help us in getting IMEI of the device only after Enrollment is complete to add it into our DB.</p>

<ol>
<li>My Policy contains absolutely nothing, so Policy is compliant.</li>
</ol>

<pre><code>{
  ""name"": ""enterprises/LC023XXXX/policies/Policy_D-a26fead-4c03-b987-b5692e23ab19"",
  ""version"": ""18""
}
</code></pre>

<ol start=""2"">
<li>When I fetch Device Info, it's <strong>ACTIVE</strong> :</li>
</ol>

<pre><code>{
  ""name"": ""enterprises/LC023XXX/devices/343ee9888eXXXXX"",
  ""managementMode"": ""PROFILE_OWNER"",
  ""state"": ""ACTIVE"",
  ""appliedState"": ""ACTIVE"",
  ""policyCompliant"": true
}
</code></pre>

<ol start=""3"">
<li>But with <strong>ENROLLMENT</strong> status, <code>Pub/Sub</code> never sends with <code>State : ACTIVE</code> :</li>
</ol>

<pre><code>{
 ""name"": ""enterprises/LC021mqgu0/devices/34aeb69ab34355aa"",
 ""managementMode"": ""PROFILE_OWNER"",
 ""state"": ""PROVISIONING"",
 ""enrollmentTime"": ""2020-06-14T10:20:01.713Z"",
 ""lastPolicySyncTime"": ""2020-06-14T10:20:01.716Z""
}
</code></pre>

<ol start=""4"">
<li>Here's how my <code>Enterprise</code> is configured :</li>
</ol>

<pre><code>{
  ""name"": ""enterprises/LC02XXXXX"",
  ""enabledNotificationTypes"": [
    ""ENROLLMENT""
  ],
  ""pubsubTopic"": ""projects/projectName/topics/enrollmentNotification""
}
</code></pre>

<p>I have tried with multiple <code>Enterprises</code> and none of the enterprises send me the expected <code>STATE - ACTIVE</code></p>

<p>Any help on what's causing this?</p>
","<android><google-api><mdm><android-management-api>","2020-06-14 10:38:07","265","3","1","63394823","<p>The STATE - ACTIVE notification will only be sent via status report so you have to set <code>enabledNotificationTypes</code> to also include <code>STATUS_REPORT</code>. You will receive this notification once the enrollment is completed and the device syncs the assigned policy.</p>
<p>Your enterprise should be configured as follows:</p>
<pre><code>{ 
  &quot;name&quot;: &quot;enterprises/LC02XXXXX&quot;, 
  &quot;enabledNotificationTypes&quot;: [ &quot;ENROLLMENT&quot;,&quot;STATUS_REPORT&quot; ], 
  &quot;pubsubTopic&quot;: &quot;projects/projectName/topics/enrollmentNotification&quot; 
}
</code></pre>
"
"62317251","How to Clear WMI MDM_AssignedAccess configuration?","<p>I'm using WMI MDM Bridge to configure Multi-App Kiosk mode in Windows 10.</p>

<pre><code>$nameSpaceName=""root\cimv2\mdm\dmmap""
$className=""MDM_AssignedAccess""
$obj = Get-CimInstance -Namespace $namespaceName -ClassName $className

Add-Type -AssemblyName System.Web

$obj.Configuration = [System.Web.HttpUtility]::HtmlEncode(@""
&lt;?xml version=""1.0"" encoding=""utf-8"" ?&gt;
 ... CONFIGURATOIN HERE ...

Set-CimInstance -CimInstance $obj
</code></pre>

<p>WMI Explorer shows me that MDM_AssignedAccess.Instance class has a new property now called COnfiguration.</p>

<p>Is there any way available to cleanup the configuration and revert Windows to the previous state?</p>

<p>What I tried:</p>

<ul>
<li>Clear-AssignedAccess - no effect</li>
<li>Manually clear configuration property - Error with Set-CimInstance -CimInstance $obj command </li>
<li>Also I tried to find the way to remove new ""configuration"" property with no effect.</li>
</ul>

<p>It looks like a one-way road. I want to be able to revert Multi-App kiosk mode if needed.</p>

<p>I have one more question - Is it possible to provide command line options to the allowed apps in multi-app kiosk? </p>

<p>For me, </p>

<pre><code>rs5:AutoLaunchArguments=""ARGS_HERE"" 
</code></pre>

<p>does not work as well as </p>

<pre><code>rs5:AutoLaunch=""true"" 
</code></pre>

<p>Thank you for any suggestions.</p>
","<powershell><wmi><mdm><kiosk><kiosk-mode>","2020-06-11 04:49:16","682","0","2","62942081","<p>If a user is signed-in or the computer has a PS/2 keyboard, you must restart the computer to apply the changes.</p>
<p><a href=""https://learn.microsoft.com/en-us/powershell/module/assignedaccess/clear-assignedaccess?view=win10-ps"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/powershell/module/assignedaccess/clear-assignedaccess?view=win10-ps</a></p>
"
"62317251","How to Clear WMI MDM_AssignedAccess configuration?","<p>I'm using WMI MDM Bridge to configure Multi-App Kiosk mode in Windows 10.</p>

<pre><code>$nameSpaceName=""root\cimv2\mdm\dmmap""
$className=""MDM_AssignedAccess""
$obj = Get-CimInstance -Namespace $namespaceName -ClassName $className

Add-Type -AssemblyName System.Web

$obj.Configuration = [System.Web.HttpUtility]::HtmlEncode(@""
&lt;?xml version=""1.0"" encoding=""utf-8"" ?&gt;
 ... CONFIGURATOIN HERE ...

Set-CimInstance -CimInstance $obj
</code></pre>

<p>WMI Explorer shows me that MDM_AssignedAccess.Instance class has a new property now called COnfiguration.</p>

<p>Is there any way available to cleanup the configuration and revert Windows to the previous state?</p>

<p>What I tried:</p>

<ul>
<li>Clear-AssignedAccess - no effect</li>
<li>Manually clear configuration property - Error with Set-CimInstance -CimInstance $obj command </li>
<li>Also I tried to find the way to remove new ""configuration"" property with no effect.</li>
</ul>

<p>It looks like a one-way road. I want to be able to revert Multi-App kiosk mode if needed.</p>

<p>I have one more question - Is it possible to provide command line options to the allowed apps in multi-app kiosk? </p>

<p>For me, </p>

<pre><code>rs5:AutoLaunchArguments=""ARGS_HERE"" 
</code></pre>

<p>does not work as well as </p>

<pre><code>rs5:AutoLaunch=""true"" 
</code></pre>

<p>Thank you for any suggestions.</p>
","<powershell><wmi><mdm><kiosk><kiosk-mode>","2020-06-11 04:49:16","682","0","2","75227279","<p>##To remove the existing Multi-App Kiosk configuration settings</p>
<p>$aacsp = Get-CimInstance -Namespace “root\cimv2\mdm\dmmap” -ClassName “MDM_AssignedAccess”</p>
<p>$aacsp.Configuration = $NULL</p>
<p>Set-CimInstance -CimInstance $aacsp</p>
"
"62317209","Problem in accessing global in data flow even save main session is set to true","<p><a href=""https://i.stack.imgur.com/ESqMz.jpg"" rel=""nofollow noreferrer"">enter image description here</a><a href=""https://i.stack.imgur.com/WCuGn.jpg"" rel=""nofollow noreferrer"">enter image description here</a><a href=""https://i.stack.imgur.com/bP2kC.jpg"" rel=""nofollow noreferrer"">enter image description here</a>The problem you have encountered:
I got an error regarding access of the global variable ""regr"" in my project. the previously same program was executed successfully but now it shows an error which I have attached herewith.</p>

<p>What you expected to happen:
Even though ""save main session is set to true"", I am not able to access regr (Global one) </p>
","<google-cloud-dataflow>","2020-06-11 04:43:54","276","1","1","62543829","<p>In this case, <code>regr</code> looks to be defined in the function scope. Can you try defining it in the base scope like so:</p>
<pre class=""lang-py prettyprint-override""><code>regr = None

class MyDoFn(beam.DoFn):
  def process(self, elm):
    yield apply_operation(elm, regr)

def run(...):
  regr = calculate_regr()
  ...
  p.run().wait_until_finish()
</code></pre>
"
"62285280","How to ensure consistency of data between two execution loops","<p>I have been developing control software based on FreeRTOS operating system. From the timing point of view the software can be divided into two loops.</p>

<p>The first execution loop (fast loop (FL)) is invoked with period 100 us at the end of analogue-digital conversion. The second execution loop (slow loop (SL)) is invoked after each fourth execution of fast loop. So the timing of the control software can be described by following timing diagram </p>

<p><a href=""https://i.stack.imgur.com/Q5EbK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q5EbK.jpg"" alt=""enter image description here""></a></p>

<p>I need to ensure consistency of data shared between fast and slow loop. The slow loop has to use during its whole execution time the data values valid at the end of fourth execution of the fast loop. </p>

<p>I have been thinking about how to ensure the above mentioned consistency of data. My first idea is to use some global variable (let´s say <code>mutex</code>) </p>

<pre><code>typedef enum
{
    SLOW_LOOP_RUNNING,
    SLOW_LOOP_WAITING
}MutexState

MutexState mutex; 
</code></pre>

<p>which will be used in atomic manner. For example in case the slow loop starts its calculation it at first executes</p>

<pre><code>mutex = SLOW_LOOP_RUNNING;
</code></pre>

<p>At the end of slow loop execution it executes</p>

<pre><code>mutex = SLOW_LOOP_WAITING;
</code></pre>

<p>The fast loop monitors always the status of the <code>mutex</code> variable and in case it finds out that the mutex contains <code>SLOW_LOOP_RUNNING</code> value it doesn´t overwrite the shared variables. I think that this could work but I don´t like the global variable. Does anybody have better idea how to resolve that? Thanks in advance for any suggestions.</p>
","<c><embedded><task><data-synchronization>","2020-06-09 14:44:48","97","0","1","62286951","<p>If the data is small, I'd just send/copy it. Especially if you're already using <code>fork()</code>, you can use <a href=""https://man7.org/linux/man-pages/man2/pipe.2.html"" rel=""nofollow noreferrer"">pipes</a> (<a href=""https://stackoverflow.com/a/44115862/6069586"">ex</a>). Where your fast loop, on every 4th cycle, can <a href=""https://man7.org/linux/man-pages/man2/write.2.html"" rel=""nofollow noreferrer""><code>write(buf)</code></a> the data to the pipe fd, and the slow loop then <a href=""https://man7.org/linux/man-pages/man2/read.2.html"" rel=""nofollow noreferrer""><code>read(buf)</code></a> from it. Even better, if you only want to run the slow loop cycle after you've received data from the 4th fast loop cycle, you can <a href=""https://man7.org/linux/man-pages/man2/select.2.html"" rel=""nofollow noreferrer""><code>select()</code></a> to only run the slow loop function when there is data to read from the pipe. This approach, vs mutexes, should be cleaner &amp; less error prone, avoiding the data race entirely; allowing you to treat the slow loop task as a purely functional task.</p>
"
"62260007","How can print compress data flow content in Nifi","<p>I am using Attribute Log Message for logging but if data is compress as gzip log not message readable. How  can fix this issue.</p>
","<apache-nifi>","2020-06-08 10:28:53","240","0","1","62268809","<p>The <code>LogAttribute</code> processor will print whatever the flowfile attribute(s) and content (if desired) contain. If the data is compressed, you will first need to uncompress it with the <code>CompressContent</code> processor to have it print in a human-readable form. </p>
"
"62252425","Design - Dynamic data mapping","<p>I am working on an online tool that serves to a number of merchants(e.g. lets say retail merchants).This application takes data from different merchants and provides some data on their retail shop. The solution that I am trying to incorporate here is that any merchant can signup for the tool, send (may be upload through excel or my application can input a json object) their transaction and inventory data and in turn return the result to merchant.</p>

<p>My application consist of domain that is intrinsic to the application and contain all the datapoints that can be used by merchants, e.g</p>

<p>Product {
productId,
productName,
...
}</p>

<p>But the problem that I am facing is that, each merchant will have their own way of representing data, for e.g. merchant <code>x</code> may call <code>product</code> as <code>prod</code> or merchant <code>y</code> may call <code>product</code>
as <code>proddt</code>.</p>

<p>Now I would need to way to convert data represented in merchant format to a way that application understand, i.e each time there is a request from merchant <code>x</code>, application should map <code>prod</code> to <code>product</code> e.t.c e.t.c. </p>

<p>Firstly I was thinking of coding these mappers but then this is not a viable solution as I can't really code these mappings for 1000's of merchants that may join my application.</p>

<p>Another solution I was think was to enable the merchant to map a field from their domain to application domain through UI. And then save this somewhere in DB and on each request from merchant first find the mapping from db and then apply it over any incoming request.(Though I am still confused how this can be done).</p>

<p>Does anyone has faced similar design issue before and know of the better way of solving this problem.</p>
","<database><algorithm><architecture>","2020-06-07 22:21:14","428","4","4","62295282","<p>I appreciate this ""language"" concern, and -in fact- multi-lingual applications do it the way you describe. You need to standardize your terminology at your end, so that each term has only one meaning and only one word/term to describe it. You could even use mnemonics for that, e.g. for ""favourite product"" you use ""Fav_Prod"" in your app and in your DB. Then, when you present data to you customer, your app looks-up their preferred term for it in a look-up-table, and uses ""favourite product"" for customer one, and perhaps the admin, and then ""favr prod"" for customer two, etc...</p>

<p>Look at SQL and DB design, you'll find that this is a form of normalization. </p>

<p>Are you dealing with legacy systems and/or APIs at the customer end? If so, someone will indeed have to type in the data. </p>

<p>If you have 1000s of customers, but there are only 10..50 terms, it may best to let the customer, not you, set the terms.</p>

<p>You might be lucky, and be able to cluster customers together who use similar or close enough terminology. For new customers you could offer them a menu of terms that they can choose from.</p>
"
"62252425","Design - Dynamic data mapping","<p>I am working on an online tool that serves to a number of merchants(e.g. lets say retail merchants).This application takes data from different merchants and provides some data on their retail shop. The solution that I am trying to incorporate here is that any merchant can signup for the tool, send (may be upload through excel or my application can input a json object) their transaction and inventory data and in turn return the result to merchant.</p>

<p>My application consist of domain that is intrinsic to the application and contain all the datapoints that can be used by merchants, e.g</p>

<p>Product {
productId,
productName,
...
}</p>

<p>But the problem that I am facing is that, each merchant will have their own way of representing data, for e.g. merchant <code>x</code> may call <code>product</code> as <code>prod</code> or merchant <code>y</code> may call <code>product</code>
as <code>proddt</code>.</p>

<p>Now I would need to way to convert data represented in merchant format to a way that application understand, i.e each time there is a request from merchant <code>x</code>, application should map <code>prod</code> to <code>product</code> e.t.c e.t.c. </p>

<p>Firstly I was thinking of coding these mappers but then this is not a viable solution as I can't really code these mappings for 1000's of merchants that may join my application.</p>

<p>Another solution I was think was to enable the merchant to map a field from their domain to application domain through UI. And then save this somewhere in DB and on each request from merchant first find the mapping from db and then apply it over any incoming request.(Though I am still confused how this can be done).</p>

<p>Does anyone has faced similar design issue before and know of the better way of solving this problem.</p>
","<database><algorithm><architecture>","2020-06-07 22:21:14","428","4","4","62396521","<p>If merchants were required to input their mapping with their data, your tool would not require a DB.  In JSON, the input could be like the following:</p>

<pre><code>input = {mapping: {ID: ""productId"", name: ""productName""}, data: {productId: 0, productName: """"}}
</code></pre>

<p>Then, you could convert data represented in any merchant's format to your tool's format as follows:</p>

<pre><code>ID = input.data[input.mapping.ID]
name = input.data[input.mapping.name]
</code></pre>
"
"62252425","Design - Dynamic data mapping","<p>I am working on an online tool that serves to a number of merchants(e.g. lets say retail merchants).This application takes data from different merchants and provides some data on their retail shop. The solution that I am trying to incorporate here is that any merchant can signup for the tool, send (may be upload through excel or my application can input a json object) their transaction and inventory data and in turn return the result to merchant.</p>

<p>My application consist of domain that is intrinsic to the application and contain all the datapoints that can be used by merchants, e.g</p>

<p>Product {
productId,
productName,
...
}</p>

<p>But the problem that I am facing is that, each merchant will have their own way of representing data, for e.g. merchant <code>x</code> may call <code>product</code> as <code>prod</code> or merchant <code>y</code> may call <code>product</code>
as <code>proddt</code>.</p>

<p>Now I would need to way to convert data represented in merchant format to a way that application understand, i.e each time there is a request from merchant <code>x</code>, application should map <code>prod</code> to <code>product</code> e.t.c e.t.c. </p>

<p>Firstly I was thinking of coding these mappers but then this is not a viable solution as I can't really code these mappings for 1000's of merchants that may join my application.</p>

<p>Another solution I was think was to enable the merchant to map a field from their domain to application domain through UI. And then save this somewhere in DB and on each request from merchant first find the mapping from db and then apply it over any incoming request.(Though I am still confused how this can be done).</p>

<p>Does anyone has faced similar design issue before and know of the better way of solving this problem.</p>
","<database><algorithm><architecture>","2020-06-07 22:21:14","428","4","4","62413968","<p>if you can find the order of fields then you can easily map data send by your client and you can return result. for example in Excel you client can mention data in this format:</p>

<p>product |  name  | quantity | cost</p>

<p>condition: your ALL client should send data in this format.</p>

<p>then it will be easy for you to map these field and access then with correct DTO and later save and process data.</p>
"
"62252425","Design - Dynamic data mapping","<p>I am working on an online tool that serves to a number of merchants(e.g. lets say retail merchants).This application takes data from different merchants and provides some data on their retail shop. The solution that I am trying to incorporate here is that any merchant can signup for the tool, send (may be upload through excel or my application can input a json object) their transaction and inventory data and in turn return the result to merchant.</p>

<p>My application consist of domain that is intrinsic to the application and contain all the datapoints that can be used by merchants, e.g</p>

<p>Product {
productId,
productName,
...
}</p>

<p>But the problem that I am facing is that, each merchant will have their own way of representing data, for e.g. merchant <code>x</code> may call <code>product</code> as <code>prod</code> or merchant <code>y</code> may call <code>product</code>
as <code>proddt</code>.</p>

<p>Now I would need to way to convert data represented in merchant format to a way that application understand, i.e each time there is a request from merchant <code>x</code>, application should map <code>prod</code> to <code>product</code> e.t.c e.t.c. </p>

<p>Firstly I was thinking of coding these mappers but then this is not a viable solution as I can't really code these mappings for 1000's of merchants that may join my application.</p>

<p>Another solution I was think was to enable the merchant to map a field from their domain to application domain through UI. And then save this somewhere in DB and on each request from merchant first find the mapping from db and then apply it over any incoming request.(Though I am still confused how this can be done).</p>

<p>Does anyone has faced similar design issue before and know of the better way of solving this problem.</p>
","<database><algorithm><architecture>","2020-06-07 22:21:14","428","4","4","62417157","<p>To reacp:</p>

<ul>
<li>You have an application</li>
<li>You want to load client data (merchants in this case) into your application</li>
<li>Your clients “own” and manage this data</li>
<li>There are N ways in which such client data can be managed, where N &lt;= the number of possible clients</li>
<li>You will run out of money and your business will close before you can build support for all N models</li>
<li>Unless you are Microsoft, or Amazon, or Facebook, or otherwise have access to significant resources (time, people, money)</li>
</ul>

<p>This may seem harsh, but it is pragmatic. You should assume NOTHING about how potential clients will be storing their data. Get anything wrong, your product will process and return bad results, and you will lose that client. Unless your clients are using the same data management tools—and possibly even then—their data structures and formats will differ, and could differ significantly.</p>

<p>Based on my not entirely limited experience, I see three possible ways to handle this.</p>

<p>1) Define your own way of modeling data. Require your customers to provide data in this format. Accept that this will limit your customer base.</p>

<p>2) Identify the most likely ways (models) your potential clients will be storing data (e.g. most common existing software systems they might be using for this.) Build import structures, formats to suppor these models. This, too, will limit your customer base.</p>

<p>3) Start with either of the above. Then, as part of your business model, agree to build out your system to support clients who sign up. If you already support their data model, great! If not, you will have to build it out. Maybe you can work the expense of this into what you charge them, maybe not. Your customer base will be limited by how efficiently you can add new and functional means of loading data to your system.</p>
"
"62214981","Spring cloud stream apps created using Function interface not communicating data in Spring cloud data flow","<p>I have created 3 simple Spring cloud stream apps(Source/Processor/Sink) using Spring cloud function approach transferring Flux.</p>

<p>Source-app:</p>

<pre><code>@SpringBootApplication
public class SourceApplication {

    public static void main(String[] args) {
        SpringApplication.run(SourceApplication.class, args);
    }

    @PollableBean
    public Supplier&lt;Flux&lt;String&gt;&gt; stringSupplier() {
        return () -&gt; {
            String v1 = String.valueOf(""abc"");
            String v2 = String.valueOf(""pqr"");
            String v3 = String.valueOf(""xyz"");
            return Flux.just(v1, v2, v3);
        };
    }
}
</code></pre>

<p>Processor-app:</p>

<pre><code>@SpringBootApplication
public class ProcessorApplication {

    @Bean
    public Function&lt;Flux&lt;String&gt;, Flux&lt;String&gt;&gt; uppercase() {
        return flux -&gt; flux.map(value -&gt; value.toUpperCase()).log();
    }

    public static void main(String[] args) {
        SpringApplication.run(ProcessorApplication.class, args);
    }
}
</code></pre>

<p>Sink-app:</p>

<pre><code>@SpringBootApplication
public class SinkApplication {
    public static void main(String[] args) {
        SpringApplication.run(SinkApplication.class, args);
    }

    @Bean
    public Consumer&lt;Flux&lt;String&gt;&gt; log() {
        return flux -&gt; {
            flux.subscribe(f -&gt; System.out.println(""Received data: "" + f));
        };
    }
}
</code></pre>

<p>The dependencies that I have added are:</p>

<pre><code>SpringBoot version = 2.2.6.RELEASE

implementation(platform(""org.springframework.cloud:spring-cloud-dependencies:Hoxton.SR5""))
implementation(platform(""org.springframework.cloud:spring-cloud-stream-dependencies:Horsham.SR5""))
implementation(""org.springframework.cloud:spring-cloud-starter-stream-rabbit"")
implementation(""org.springframework.cloud:spring-cloud-starter-function-webflux:3.0.7.RELEASE"")
implementation(""org.springframework.boot:spring-boot-starter-actuator"")
implementation(""org.springframework.boot:spring-boot-starter-webflux"")
</code></pre>

<p>I have registered these these apps in Spring Cloud Data Flow and deployed in a stream.</p>

<p>I am able to transmit data to these apps and receive output both via HTTP and via RabbitMQ individually. But, the message is not communicated across the apps(Source->Processor->sink).
Am I missing any dependency, annotation or application property.</p>

<p>Currently my application property file is completely empty.</p>
","<spring-boot><spring-cloud-stream><spring-cloud-dataflow><spring-cloud-function>","2020-06-05 12:05:47","150","0","1","62215327","<p>You need to set  spring.cloud.stream.function.bindings.-in-0=input. See <a href=""https://cloud.spring.io/spring-cloud-static/spring-cloud-stream/current/reference/html/spring-cloud-stream.html#_programming_model"" rel=""nofollow noreferrer"">https://cloud.spring.io/spring-cloud-static/spring-cloud-stream/current/reference/html/spring-cloud-stream.html#_programming_model</a>. We’re looking to automate this in a future Dataflow release.</p>
"
"62202159","React Initializing and synchronizing state with backend","<p>I wonder what is the best way/pattern to initialize state and keep it synced with the server. I've read and tried a lot for the last couple of days but haven't found anything that solves my question.</p>

<p>The example is really simple. There is a state - it's a number in the example for the sake of simplicity, although in real life it would be an object - that I need to retrieve from the server. Once retrieved, I want it to be synchronized with the server. The getValueFromServer is a mock that returns a random value after waiting a random amount of time in a setTimeout.</p>

<p>So, to initialize my state I use a useEffect on an empty array as a dependency, and to keep it synched I use a useEffect with the state as a dependency.</p>

<p>The problem is that it is trying to save an undefined value. The log is as follows. </p>

<p>1 -> Initializing for first time</p>

<p>2 -> API call to save in server value:  undefined</p>

<p>3 -> API call to save in server value:  6.026930847574949</p>

<p>What I get:</p>

<p>1: Runs on mounting as expected.</p>

<p>2: This one I didn't expect. I guess it is triggered because of the ""useState"".</p>

<p>3: Runs once we get the response from the server. Kind of obvious but a pain in the ass, because why on earth would I want to save this.</p>

<p>What would the best approach be here? Using something like a ""isInitialized"" flag in the useEffect with dependency feels kind of hacked and not professional.</p>

<p>Code below and you can find it working here too: <a href=""https://codesandbox.io/s/optimistic-rgb-uce9f"" rel=""nofollow noreferrer"">https://codesandbox.io/s/optimistic-rgb-uce9f</a></p>

<pre><code>import React, { useState, useEffect } from ""react"";
import { getValueFromServer } from ""./api"";

export default function App() {
  const [value, setValue] = useState();

  useEffect(() =&gt; {
    async function initialize() {
      console.log(""Initializing for first time"");
      let serverValue = await getValueFromServer();
      setValue(serverValue);
    }
    initialize();
  }, []);

  useEffect(() =&gt; {
    console.log(""API call to save in server value: "", value);
  }, [value]);

  const handleClick = () =&gt; {
    setValue(value + 1);
  };

  return (
    &lt;div className=""App""&gt;
      &lt;h1&gt;Value: {value}&lt;/h1&gt;
      &lt;button onClick={handleClick}&gt;Add 1 to value&lt;/button&gt;
    &lt;/div&gt;
  );
}
</code></pre>
","<reactjs><react-hooks><data-synchronization>","2020-06-04 18:55:35","907","1","2","62202502","<p>your app does other stuff while it's busy <code>await</code>ing something to happen in your first <code>useEffect</code> hook function.  ""other stuff"" in this case is ""executing your second <code>useEffect</code> hook function"".  <code>serverValue</code>, and therefore <code>value</code>, is not defined at this point, so your <code>console.log</code> prints <code>undefined</code>.  once your <code>await</code> promise resolves to a value and <code>setValue</code> gets called, your second <code>useEffect</code> hook function's dependencies change causing your function to be run a second time (and printing out the value you expect).</p>

<p>if you have to have two <code>useEffect</code> hooks, then just bail out of the second one when <code>value</code> is undefined:</p>

<pre class=""lang-js prettyprint-override""><code>  useEffect(() =&gt; {
    if (value === undefined) {
      return;
    }

    console.log('value is:', value);
  }, [ value ]);
</code></pre>
"
"62202159","React Initializing and synchronizing state with backend","<p>I wonder what is the best way/pattern to initialize state and keep it synced with the server. I've read and tried a lot for the last couple of days but haven't found anything that solves my question.</p>

<p>The example is really simple. There is a state - it's a number in the example for the sake of simplicity, although in real life it would be an object - that I need to retrieve from the server. Once retrieved, I want it to be synchronized with the server. The getValueFromServer is a mock that returns a random value after waiting a random amount of time in a setTimeout.</p>

<p>So, to initialize my state I use a useEffect on an empty array as a dependency, and to keep it synched I use a useEffect with the state as a dependency.</p>

<p>The problem is that it is trying to save an undefined value. The log is as follows. </p>

<p>1 -> Initializing for first time</p>

<p>2 -> API call to save in server value:  undefined</p>

<p>3 -> API call to save in server value:  6.026930847574949</p>

<p>What I get:</p>

<p>1: Runs on mounting as expected.</p>

<p>2: This one I didn't expect. I guess it is triggered because of the ""useState"".</p>

<p>3: Runs once we get the response from the server. Kind of obvious but a pain in the ass, because why on earth would I want to save this.</p>

<p>What would the best approach be here? Using something like a ""isInitialized"" flag in the useEffect with dependency feels kind of hacked and not professional.</p>

<p>Code below and you can find it working here too: <a href=""https://codesandbox.io/s/optimistic-rgb-uce9f"" rel=""nofollow noreferrer"">https://codesandbox.io/s/optimistic-rgb-uce9f</a></p>

<pre><code>import React, { useState, useEffect } from ""react"";
import { getValueFromServer } from ""./api"";

export default function App() {
  const [value, setValue] = useState();

  useEffect(() =&gt; {
    async function initialize() {
      console.log(""Initializing for first time"");
      let serverValue = await getValueFromServer();
      setValue(serverValue);
    }
    initialize();
  }, []);

  useEffect(() =&gt; {
    console.log(""API call to save in server value: "", value);
  }, [value]);

  const handleClick = () =&gt; {
    setValue(value + 1);
  };

  return (
    &lt;div className=""App""&gt;
      &lt;h1&gt;Value: {value}&lt;/h1&gt;
      &lt;button onClick={handleClick}&gt;Add 1 to value&lt;/button&gt;
    &lt;/div&gt;
  );
}
</code></pre>
","<reactjs><react-hooks><data-synchronization>","2020-06-04 18:55:35","907","1","2","62202515","<blockquote>
  <p>What would the best approach be here? Using something like a
  ""isInitialized"" flag in the useEffect with dependency feels kind of
  hacked and not professional.</p>
</blockquote>

<p>You can either use a flag or initialize object with default value in <code>useState</code></p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""true"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>const { Fragment, useState, useEffect } = React;

const getValueFromServer = () =&gt; new Promise((resolve, reject) =&gt; setTimeout(() =&gt; resolve(Math.random())), 1000)

const App = () =&gt; {
  const [value, setValue] = useState(null);
  const [isLoading, setLoading] = useState(true);

  useEffect(() =&gt; {
    let isUnmounted = false;
  
    getValueFromServer().then(serverValue =&gt; {
      console.log(""Initializing for first time"");
      if(isUnmounted) {
        return;
      }
      setValue(serverValue);
      setLoading(false);
    })
    
    return () =&gt; {
      isUnmounted = true;
    }
  }, []);

  useEffect(() =&gt; {
    if(!value) {
      return () =&gt; {}
    }
  
    console.log(""API call to save in server value: "", value);
    setTimeout(() =&gt; setLoading(false), 50);
  }, [value]);

  const handleClick = () =&gt; {
    setLoading(true);
    setValue(value + 1);
  };

  return &lt;div className=""App""&gt;
    {isLoading ? &lt;Fragment&gt;
      &lt;span&gt;Loading...&lt;/span&gt;
    &lt;/Fragment&gt; : &lt;Fragment&gt;
      &lt;h1&gt;Value: {value}&lt;/h1&gt;
      &lt;button onClick={handleClick}&gt;Add 1 to value&lt;/button&gt;
    &lt;/Fragment&gt;}
  &lt;/div&gt;
}

ReactDOM.render(
    &lt;App /&gt;,
    document.getElementById('root')
  );</code></pre>
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;script src=""https://unpkg.com/react/umd/react.development.js""&gt;&lt;/script&gt;
&lt;script src=""https://unpkg.com/react-dom/umd/react-dom.development.js""&gt;&lt;/script&gt;
&lt;script src=""https://cdnjs.cloudflare.com/ajax/libs/babel-polyfill/7.10.1/polyfill.js""&gt;&lt;/script&gt;
&lt;script src=""https://unpkg.com/babel-standalone@6/babel.min.js""&gt;&lt;/script&gt;
&lt;div id=""root""&gt;&lt;/div&gt;</code></pre>
</div>
</div>
</p>
"
"62199256","Passing a dataset parameter in a data mapping acitivity in a data flow in Azure Data factory","<p>I have a parametrized dataset, that I used for a copy data activty and it worked fine.
I am trying to replicate that using a mapping dataflow but I cant find where to input the value for the dataset parameter...</p>
","<azure-data-factory>","2020-06-04 16:09:09","2042","0","2","62202562","<p>Parameterized Data Sets work exactly the same way in a Data Flow activity. If a Data Set used in the Data Flow has parameters, they will have configuration points on the Settings tab:</p>

<p><a href=""https://i.stack.imgur.com/iPvDi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iPvDi.png"" alt=""enter image description here""></a></p>
"